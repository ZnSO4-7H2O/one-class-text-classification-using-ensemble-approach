{"title": "A Linear Dynamical System Model for Text", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "Low dimensional representations of words allow accurate NLP models to be trained on limited annotated data. While most representations ignore words' local context, a natural way to induce context-dependent representations is to perform inference in a probabilistic latent-variable sequence model. Given the recent success of continuous vector space word representations, we provide such an inference procedure for continuous states, where words' representations are given by the posterior mean of a linear dynamical system. Here, efficient inference can be performed using Kalman filtering. Our learning algorithm is extremely scalable, operating on simple cooccurrence counts for both parameter initialization using the method of moments and subsequent iterations of EM. In our experiments, we employ our inferred word embeddings as features in standard tagging tasks, obtaining significant accuracy improvements. Finally, the Kalman filter updates can be seen as a linear recurrent neural network. We demonstrate that using the parameters of our model to initialize a non-linear recurrent neural network language model reduces its training time by a day and yields lower perplexity.", "text": "dimensional representations words allow accurate models trained limited annotated data. representations ignore words’ local context natural induce context-dependent representations perform inference probabilistic latent-variable sequence model. given recent success continuous vector space word representations provide inference procedure continuous states words’ representations given posterior mean linear dynamical system. here efﬁcient inference performed using kalman ﬁltering. learning algorithm extremely scalable operating simple cooccurrence counts parameter initialization using method moments subsequent iterations experiments employ inferred word embeddings features standard tagging tasks obtaining signiﬁcant accuracy improvements. finally kalman ﬁlter updates seen linear recurrent neural network. demonstrate using parameters model initialize non-linear recurrent neural network language model reduces training time yields lower perplexity. many applications limited available labeled training data tremendous quantities unlabeled in-domain text. effective semi-supervised learning technique learn word embeddings unlabeled data every word dimensional dense vector features supervised training labeled data furthermore many deep architectures ﬁrst layer maps words low-dimensional vectors parameters initialized unsupervised embeddings methods embed word types i.e. words independent local context opposed work tokens i.e. instances words within context. ideally would different representation token. example depending context bank side river ﬁnancial institution. furthemore would like embeddings come probablistic sequence model allows study transition dynamics text generation dimensional space. present method obtaining context-dependent token embeddings using generative model vectorvalued latent variable token performing posterior inference sentence. speciﬁcally employ gaussian linear dynamical system efﬁcient inference kalman ﬁlter. learn parameters two-stage procedure initializing method moments performing approximate second order statistics technique martens overall taking single pass training corpus runtime approximate maximumlikelihood estimation procedure independent amount training data since operates aggregate co-occurrence counts. furthermore performing inference obtain token embeddings time complexity widely-used discrete ﬁrst-order sequence models. one-hot encoding token input text sequence. therefore mis-speciﬁed generative model since draws proper indicator vectors. however embrace multivariate gaussian model instead continuous-state dynamical system multinomial link function gaussian offers several desirable scalability properties kalman ﬁlter inference simple efﬁcient using asos cost learning iterations scale corpus size initialize using method-of-moments estimator requires single co-occurrence matrix m-step updates simple least-squares problems solvable closed form used multinomial link function would performed inference using extended kalman ﬁltering makes secondorder approximation log-likelihood thus leads gaussian anyway using avoid stochastic-gradient-based optimization requires careful tuning nonconvex problems. naive application method scales large amounts training data high-dimensional observations. response paper contributes variety novel methods scaling learning techniques handle large input vocabularies. employ inferred token embeddings features part speech named entity recognition taggers. obtain relative error reduction applying local classiﬁer context-dependent embeddings rather wordvec context-independent embeddings using token embeddings additional features lexicalized taggers already explicit features test-time inference context-dependence obtain signﬁcant gains baseline performing well using wordvec embeddings. also present experiments demonstrating transition dynamics capture salient patterns transforming ﬁrst names last names. finally functional form kalman ﬁlter update equations identical updates recurrent neural network language model without non-linearities difference however provides natural backwards pass using kalman smoothing token’s embedding depends text right left. drawing parallelism ﬁltering parameters estimated quicky using techniques initialize gradientbased optimization nonlinear rnn. yields signficant decrease perplexity baseline requires many training epochs saving single core. provide continuous analog popular discrete-state generative models used inducing class membership tokens including class-based language models induction tags parlearning algorithm scalable operates aggregate count matrices rather individual tokens. similar algorithms proposed obtaining type-level embeddings matrix factorization however context-independent ignore transition dynamics link tokens’ embeddings. furthermore require careful tuning stochastic gradient methods. previous methods token-level embeddings either rigid prototypes embed token’s context ignoring token itself learning discrete-state latent variable models spectral learning methods also count matrices thus similarly scalable however offers advantages third-order moments difﬁcult estimate perform approximate rather method moments exhibits poor statistical efﬁciency. recently rnns used provide impressive results applications including translation language modeling parsing attempt replace kalman ﬁlter expect non-linearities crucial capturing long-term interactions rigid combinatorial constraints outputs. however rnns training take days even gpus requires careful tuning stochastic gradient step sizes. given scalability parameter-free training algorithm favorable preliminary results using initialize nonlinear encourage work using linear latent variable models gaussian approximations multinomial data develop sophisticated initialization methods. already practitioners started using techniques initializing simple nonlinear deep neural networks using recommendations saxe finally work differs pasa sperduti initialize using spectral techniques perform maximum-likelihood learning. found crucial good performance experiments. latent space completely unobserved could choose coordinate system maintaining likelihood value. therefore without loss generality either furthermore note magnitude maximum eigenvalue must larger system stable. assume data centered case maximum eigenvalue strictly less since implies asymptotically mean zero also asymptotically mean zero. valid assume data mean zero. learning algorithms require input. matrices gathered using single pass data size depend amount data. furthermore constructing matrices accelerated splitting data chunks aggregating separate matrices afterwards. distributed multivariate gaussian prior posterior fully characterized mean variance. mean covariance posterior given computed using kalman ﬁltering considering posterior given data computed using kalman smoothing. appendix provide full ﬁltering smoothing updates compute different means variances every timestep. note updates require inverting matrix every step. employ widely-used ’steady-state’ approximation yields substantially efﬁcient ﬁltering smoothing updates property ﬁltering smoothing updates depend actual observations model’s parameters. furthermore converge quickly time-independent ‘steady-state’ values. deﬁne aymptotic limit variance posterior given history here expectation taken respect time posterior latent variables. satisﬁes unconditional prior covariance model. note data-independent precomputed smoothing matrix σa−. long sequences steady-state ﬁltering provides asymptotically exact inference. however short sequences approximation. expectation taken respect time posterior latent variables. computed using kalman smoothing averaging time. step done closed form since solving least-squares regressions obtain lastly recovered using requires recomputing second order statistics every iteration. computed using kalman smoothing entire training interested datasets billions timesteps. fortunately avoid smoothing employing asos method martens directly performs inference time-averaged second order statistics. relationships underlying dynamical system. namely rather performing posterior inference recursively applying linear operations averaging time switch order operations apply linear operators time-averaged second order statistics. example following equality immediate consequences ﬁltering equation asos uses number recursions along methods estimating covariances time horizon covariances approximated assuming exactly described current estimate model parameters. therefore unlike standard performing asos allows precompute empirical estimate various lags never touch data again. furthermore demonstrates asos approximation consistent. namely error approximating time-averaged second order statistics vanishes inﬁnite data evaluated parameters. overall asos scales linearly cost multiplying initialize using subspace identiﬁcation family method-of-moments estimators spectral decomposition recover parameters rationale combination method moments statistically consistent performing reasonably-sized datasets yield parameters neighborhood global optimum perform local hill climbing local optimum marginal likelihood. combination yields empirical accuracy gains smith related two-stage estimator local search replaced single newton step local likelihood surface known minimax optimal certain local asymptotic normality conditions particular application ssid approximate method statistically consistent mis-speciﬁcation ﬁtting indicator-vector data multivariate gaussian. experiments discuss superiority ssid+em rather ssid. present results using initialized randomly rather ssid since found difﬁcult high dimensional problems generate initial parameters allowed reach high likelihoods. result rank-h empirical estimate recover parameters ﬁrst de∆r submatrix corresponding ﬁrst blocks. similarly deﬁne deﬁnition estimate next read estimate ﬁrst block alternatively since previous step gives value regression problem similar previous step solve invoking block structure finally need recover covariance matrix ﬁrst asymptotic latent covariance using ﬁxedpoint iteration this using similar update uses statistics posterior except unconditional data purely function parameters. text using ssid initialize step performed using asos. summary procedure provided algorithm ssid asos scale extremely large training sets since require matrices small however directly handle high dimensionality text observations section ﬁrst data subspace breaks special structure described above instead work perform projections onto implicitly as-needed. fortunately ssid lies column space data iterations learning algorithm maintain col. appendix describe handle rank deﬁciency computing kalman gain. note could used pretrained type-level embeddings project corpus train lowdimensional dense observations. however vulnerable subspace type-level embeddings trained maximize likelihod sequence model thus might capture proper syntactic semantic information. release code implementation. ssid requires simple scripting sparse linear algebra library. implementation consists small modiﬁcations martens’ public asos code. ssid requires rank-h large block hankel matrix employ randomized approximate algorithm halko factorize matrix requires repeated multiplication submatrices sparse-minus-low-rank handle sparse low-rank terms individually within multiplication subroutines. modeling full-rank noise covariance noise covariance matrix unmanageably large application thus reasonable employ spherical diagonal diag approximation. problem however found approximations performed poorly. property offdiagonal elements critical modeling anticorrelations coordinates. would captured passed logistic multinomial link function. however prevents simple inference using kalman ﬁlter. maintain conjugacy practitioners sometimes employ quadratic upper bound logistic multinomial likelihood introduced b¨ohning hard-codes coordinate-wise anticorrelations however found data= independent estimator performed poorly. instead exploit particular property ssid estimators namely minus low-rank matrix thus diagonal-minuslow-rank structure mostly seek manipulate precision matrix instantiating dense matrix infeasible multiplication evaluation deﬁne indicator vector index word time deﬁne corpus frequency word type mean-zero observations note generate observations structure one-hot vector shifted constant mean cannot directly generative language model. hand still models training data structure perform posterior inference given observations assess likelihood corpus etc. experiments demonstrate usefulness variety applications. have real-world data extremely sparse number nonzeros substantially less length corpus. fact sparse-minuslow-rank diagonal-minus-low-rank critical scaling learning algorithms. first instantiate dense matrices operate directly factorized structure. second sec. show structure allows model full-rank noise covariance matrices implicitly. strictly speaking number nonzeros increase corpus size increases heavy-tailed word co-occurence statistics. however growth sublinear mitigated ignoring rare words. unfortunately rank-deﬁcient. also every zero deﬁne length-v vector ones. then data lives dimensional subspace orthogonal maximum likelihood instead lead degenerate likelihood function since empirical variance direction however projecting done efﬁciently using shermanwoodbury-morrison formula appendix also leverage formula efﬁciently evaluate training likelihood. uses formula differ common usage using steady-state assumption posterior precision matrix needs updated using rank updates covariance. technique particular ﬁtting indicator-vector data multivariate gaussian. whitening highlight similarity parametrization architecture commonly used language modeling kalman ﬁlter. allows novel method initializing parameters non-linear explore sec. following consider network structure fitting maintains data’s sparse-minus-low-rank diagonal-minus-lowrank structures. furthermore unaffected i.e. applying linearly-transformed data equivalent learning original data transforming post-hoc. hand ssid output affected whitening since squared reconstruction loss implicitly minimizes depends coordinate system data. found whitening crucial obtaining high-quality initial parameters. whitening ssid recommended overschee moor solves similar factorization problem canonical correlation analysis words contexts used successfully learn word embeddings identify parameters class-based language models appendix also provide algorithm relies whitening manually ensuring returned ssid without needing factorize matrix. manual correction unnecessary since estimator guaranteed psd. data-dependent term steady-state ﬁltering smoothing equations kwt. since take possible values precompute word-type-level vectors. computational cost ﬁltering/smoothing length sequence identical cost inference discrete ﬁrst-order sequence model. directly usable obtain data’s rank-deﬁciency provide efﬁcient alternative appendix also requires matrix inversion lemma avoid instantiating experiments latent space deﬁne features tokens. however distances space well-deﬁned since likelihood invariant linear transformation latent variables. place consider steady-state kalman ﬁlter online predictor mean prediction given ˆxt. then replace softmax identity kalman ﬁlter parameters corresponds corresponds terms state dynamics provide parameters reasonable nonlinear since sigmoid regime inputs close zero behaves like identity. linear approximation softmax ignores mutual exclusivity. however discuss section using full-rank captures coordinate-wise anti-correlations. also affect state evolution many popular word embedding methods learn word-tovector mappings learn dynamics text’s evolution latent space. using speciﬁc model describe next section employ transition matrix explore properties dynamics. state evolution linear studied easily using spectral decomposition. namely converts left singular vectors right singular vectors. vector words likely generated state. table presents singular vector pairs. reﬂect interpretable transition dynamics. evans anderson harris robinson smith phillips collins murray murphy shares referee suggesting industries testiﬁed insisted adding arguing yesterday table words likely generated singular vector pairs transition operator. operator maps right vectors left pairs syntactically semantically coherent. last block vectors reﬂect strict state transitions. however last block contains topical terms food invariant overall salient structure parameters estimated using ssid. unsupervised learning generative discrete state models text shown capture part-of-speech information response assess ability also capture structure. token embeddings used predict ways applying local classiﬁer token’s embedding including token’s embedding additional features lexicalized tagger. both train tagging model penn treebank train included training. token embeddings obtained kalman smoothing. evaluate tagging accuracy test using ‘universal’ tags original tags. contrast type embeddings wordvec trained data using combination apnews york times newswire corpora tokens total. maintain punctuation casing text replace digits num’ frequent types oov. employ ssid psuedocounts type adding table tagging universal tags tags maximizing local tagging. local classiﬁer two-layer neural network hidden units outperformed linear classiﬁer. best wordvec conﬁguration used cbow architecture window width lexicalized tagger’s hyperparameters also tuned set. local tagging ignored punctuation common words types training. instead classiﬁed directly using majority training data. overall found wordvec took hours train single-core cpu. since wordvec algorithm simple code heavily optimized performs well learning algorithm would substantially faster given larger training since matrices gathered parallel cost ssid asos sublinear corpus size. section training order magnitude faster rnn. results shown table left right compare wordvec ssid initialized ssid baseline lexicalized tagger lexicalized tagger extra features token embeddings lexicalized tagger typelevel wordvec embeddings ﬁrst columns perform local classiﬁcation. first ssid crucial initialization found performed poorly own. however outperforms wordvec substantially. expect explicitly maximizes likelihood text sequences thus forces token embeddings capture transition dynamics syntax. differences statistically significant signiﬁcance level using exact binomial test. appendix demonstrate importance ssid random initialization. ﬁnal columns carefully-engineered tagger. universal tags wordvec contribute statistically-signiﬁcant gain baseline difference significant. tags wordvec achieves signiﬁcant gain not. expect context-dependent embeddings perform well context-independent embeddings since taggers’ features test-time inference capture non-local interactions. clusters used ratinov roth before wordvec provide signiﬁcant accuracy improvements baseline. expect reason outperform wordvec relies mainly performing local pattern matching rather capturing long-range discourse structure. highlighted section rnns provide impressive accuracy various applications. consider simple architecture sec. since permits natural initialization mikolov demonstrate small variants outperform lstms language model note ‘context units’ mikolov could also learned using procedure restricting parametrization leave exploration hierarchical softmax observations alternative architectures future work. evaluate usefulness initializing criteria whether improves perplexity ﬁnal model whether leads faster optimization. standard dataset comparing language models penn treebank ﬁrst train baseline obtaining test perplexity mikolov hidden dimensions. initializes parameters randomly lengthscales tuned mikolov next initialize rnn. order maintain fair comparison baseline train data though practice train substantially larger corpus. table final perplexity language model trained using random parameter initialization initialization. prove decreased geometrically heldperformance fails improve tuned initial value decay rate. initializing small learning rates crucial otherwise optimization jumps started. figure plot perplexity number training epochs. time train minutes inconsequential compared training single core. training faster experiments tokens because small vocabulary fewer iterations order prevent overﬁtting. baseline converged training epochs using initialization allowed converge amounts savings single core. next table compare ﬁnal perplexities test sets. initializing also provides better model. found initializing parameters trained using ssid rather ssid+em performed better baseline. speciﬁcally best performance obtained using high initial learning rate allows gradient descent ignore ssid values. expect method moments requires lots data small. setting trains large corpus possible ssid effective. overall explore initializing using type-level embeddings wordvec since unclear initialize contributed scalable method assigning word tokens context-speciﬁc low-dimensional representation capture useful syntactic semantic structure. algorithm requires single pass training data painful tuning learning rates. next extend asos models improve initialization alternative architectures including hierarchical softmaxes leveraging parameters also posterior training data. work partially completed ﬁrst author intern microsoft research. appreciate helpful comments brendan o’connor marlin andrew mccallum qingqing huang. references anandkumar animashree rong daniel kakade sham telgarsky matus. tensor decompositions learning latent variable models. journal machine learning research bansal mohit gimpel kevin livescu karen. tailoring continuous word representations dependency proceedings annual meeting parsing. association computational linguistics christodoulopoulos christos goldwater sharon steedman mark. decades unsupervised induction come? proceedings conference empirical methods natural language processing association computational linguistics collobert ronan weston jason bottou l´eon karlen michael kavukcuoglu koray kuksa pavel. natural language processing scratch. journal machine learning research dhillon paramveer rodu jordan foster dean ungar lyle. step spectral method proceedings estimating vector models words. international conference machine learning icml’ ghahramani zoubin hinton geoffrey parameter estimation linear dynamical systems. technical report technical report crg-tr-- university totronto dept. computer science halko nathan martinsson per-gunnar tropp joel finding structure randomness probabilistic algorithms constructing approximate matrix decompositions. siam review huang eric socher richard manning christopher andrew improving word representations proglobal context multiple word prototypes. ceedings annual meeting association computational linguistics long papers-volume association computational linguistics mikolov tomas sutskever ilya chen corrado greg dean jeff. distributed representations adwords phrases compositionality. vances neural information processing systems mikolov tomas joulin armand chopra sumit mathieu michael ranzato marc’aurelio. learning internalonger memory recurrent neural networks. tional conference learning representations socher richard chen danqi manning christopher andrew. reasoning neural tensor networks knowledge base completion. advances neural information processing systems stratos karl do-kyum collins michael daniel. spectral algorithm learning class-based ngram models natural language. uncertainty artiﬁcial intelligence turian joseph ratinov bengio yoshua. word representations simple general method semisupervised learning. proceedings annual meeting association computational linguistics association computational linguistics neelakantan arvind shankar jeevan passos alexandre mccallum andrew. efﬁcient nonparametric estimation multiple embeddings word vector space. proceedings emnlp passos alexandre kumar vineet mccallum andrew. lexicon infused phrase embeddings named entity resolution. proceedings eighteenth conference computational natural language learning jeffrey socher richard manning christopher glove global vectors word representation. proceedings empiricial methods natural language processing press william flannery brian teukolsky saul vetterling william numerical recipes scientiﬁc computing volume cambridge university press london ratinov roth dan. design challenges misconceptions named entity recognition. proceedings thirteenth conference computational natural language learning association computational linguistics saxe andrew mcclelland james ganguli surya. exact solutions nonlinear dynamics international learning deep linear neural networks. conference learning representations besides improving empirical performance ssid working whitened coordinate system also simpliﬁes various details used section scaling learning text. transformation diag− form rather simpliﬁes various steps estimators diag whitened coordinates data orthogonal ssid consistent ﬁnite data procedure guaranteed yield positive semideﬁnite estimate required covariance matrix. particular case seek singular span seek avoid computation test time ﬁltering. first precompute second term possible values unwhitened input ˜wt−µ would like precompute every possible value indicator take have expression useful already inverse want efﬁciently compute inverse low-rank perturbation also useful order able linear algebra using without actually instantiating matrix unmanageable terms time space large example matrix compute using carefully placing parentheses matrix required. application diagonal computing inverse trivial. also note used recursively deﬁned another easily invertible matrix rank matrix. along lines additional useful identities follow quantities computed without time storage. here assume computed inexpensively product matrices note compute time figure contrast progress terms log-likelihood training data initializing ssid initializing randomly note initial values ssid random nearly identical. model mispeciﬁcation fact chose lengthscales random parameters post-hoc looking lengthscales ssid parameters. course iterations model initialized ssid climbs quickly begins leveling whereas takes long time random model begin climbing all. truncate iterations since actually ssid-initialized model iteration. that local tagging accuracy diminished.", "year": 2015}