{"title": "BinaryConnect: Training Deep Neural Networks with binary weights during  propagations", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and power-hungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.", "text": "deep neural networks achieved state-of-the-art results wide range tasks best results obtained large training sets large models. past gpus enabled breakthroughs greater computational speed. future faster computation training test time likely crucial progress consumer applications low-power devices. result much interest research development dedicated hardware deep learning binary weights i.e. weights constrained possible values would bring great beneﬁts specialized hardware replacing many multiply-accumulate operations simple accumulations multipliers space powerhungry components digital implementation neural networks. introduce binaryconnect method consists training binary weights forward backward propagations retaining precision stored weights gradients accumulated. like dropout schemes show binaryconnect acts regularizer obtain near state-of-the-art results binaryconnect permutation-invariant mnist cifar- svhn. deep neural networks substantially pushed state-of-the-art wide range tasks especially speech recognition computer vision notably object recognition images recently deep learning making important strides natural language processing especially statistical machine translation interestingly factors enabled major progress advent graphics processing units speed-ups order -fold starting similar improvements distributed training indeed ability train larger models data enabled kind breakthroughs observed last years. today researchers developers designing deep learning algorithms applications often limited computational capability. along drive deep learning systems low-power devices greatly increasing interest research development specialized hardware deep networks computation performed training application deep networks regards multiplication real-valued weight real-valued activation gradient paper proposes approach called binaryconnect eliminate need multiplications forcing weights used forward backward propagations binary i.e. constrained values show state-of-the-art results achieved binaryconnect permutation-invariant mnist cifar- svhn. sufﬁcient precision necessary accumulate average large number stochastic gradients noisy weights quite compatible stochastic gradient descent main type optimization algorithm deep learning. explores space parameters making small noisy steps noise averaged stochastic gradient contributions accumulated weight. therefore important keep sufﬁcient resolution accumulators ﬁrst sight suggests high precision absolutely required. show randomized stochastic rounding used provide unbiased discretization. shown requires weights precision least bits successfully train dnns bits dynamic ﬁxed-point computation. besides estimated precision brain synapses varies bits noisy weights actually provide form regularization help generalize better previously shown variational weight noise dropout dropconnect noise activations weights. instance dropconnect closest binaryconnect efﬁcient regularizer randomly substitutes half weights zeros propagations. previous works show expected value weight needs high precision noise actually beneﬁcial. applying mainly consists convolutions matrix multiplications. arithmetic operation thus multiply-accumulate operation. artiﬁcial neurons basically multiplyaccumulators computing weighted sums inputs. binaryconnect constraints weights either propagations. result many multiply-accumulate operations replaced simple additions huge gain ﬁxed-point adders much less expensive terms area energy ﬁxed-point multiply-accumulators binarized weight real-valued weight. although deterministic operation averaging discretization many input weights hidden unit could compensate loss information. alternative allows ﬁner correct averaging process take place binarize stochastically hard sigmoid rather soft version less computationally expensive yielded excellent results experiments. similar hard tanh non-linearity introduced also piece-wise linear corresponds bounded form rectiﬁer given target compute training objective’s gradient w.r.t. layer’s activations starting layer going layer layer ﬁrst hidden layer. step referred backward propagation backward phase backpropagation. algorithm training binaryconnect. cost function minibatch functions binarize clip specify binarize clip weights. number layers. require minibatch previous parameters forward propagation binarize compute knowing backward propagation initialize output layer’s activations gradient compute parameter update compute dbt− clip parameter update illustrated algorithm keeping good precision weights updates necessary work all. parameter changes tiny virtue obtained gradient descent i.e. performs large number almost inﬁnitesimal changes direction improves training objective picture hypothesize matters noisy estimator ∂cyt) value objective function example previous weights ﬁnal discretized value weights. another conceive discretization form corruption hence regularizer empirical results conﬁrm hypothesis. addition make discretization errors different weights approximately cancel keeping precision randomizing discretization appropriately. propose form randomized discretization preserves expected value discretized weight. hence training time binaryconnect randomly picks values weight minibatch forward backward propagation phases backprop. however update accumulated real-valued variable storing parameter. interesting analogy understand binaryconnect dropconnect algorithm like binaryconnect dropconnect injects noise weights propagations. whereas dropconnect’s noise added bernouilli noise binaryconnect’s noise binary sampling process. cases corrupted value expected value clean original value. since binarization operation inﬂuenced variations real-valued weights magnitude beyond binary values since common practice bound weights order regularize them chosen clip real-valued weights within interval right weight updates algorithm real-valued weights would otherwise grow large without impact binary weights. batch normalization experiments accelerates training reducing internal covariate shift also reduces overall impact weights scale. moreover adam learning rule experiments. last least scale weights learning rates respectively weights initialization coefﬁcients optimizing adam squares coefﬁcients optimizing nesterov momentum table illustrates effectiveness tricks. introduced different ways training on-the-ﬂy weight binarization. reasonable ways using trained network i.e. performing test-time inference examples? considered three reasonable alternatives stochastic case many different networks sampled sampling weight according ensemble output networks obtained averaging outputs individual networks. ﬁrst method deterministic form binaryconnect. stochastic form binaryconnect focused training advantage used second method experiments i.e. test-time inference using real-valued weights. follows practice dropout methods test-time noise removed. table test error rates dnns trained mnist cifar- svhn depending method. spite using single weight propagation performance worse ordinary dnns actually better especially stochastic version suggesting binaryconnect acts regularizer. figure features ﬁrst layer trained mnist depending regularizer. left right regularizer deterministic binaryconnect stochastic binaryconnect dropout. mnist benchmark image classiﬁcation dataset consists training test gray-scale images representing digits ranging permutationinvariance means model must unaware image structure data besides data-augmentation preprocessing unsupervised pretraining. train mnist consists hidden layers rectiﬁer linear units l-svm output layer square hinge loss minimized without momentum. exponentially decaying learning rate. batch normalization minibatch size speed training. typically done figure histogram weights ﬁrst layer trained mnist depending regularizer. cases seems weights trying become deterministic reduce training error. also seems weights deterministic binaryconnect stuck around hesitating figure training curves cifar- depending regularizer. dotted lines represent training costs continuous lines corresponding validation error rates. versions binaryconnect signiﬁcantly augment training cost slow training lower validation error rate would expect dropout scheme. last samples training validation early stopping model selection. report test error rate associated best validation error rate epochs repeat experiment times different initializations. results table suggest stochastic version binaryconnect considered regularizer although slightly less powerful dropout context. cifar- benchmark image classiﬁcation dataset. consists training test color images representing airplanes automobiles birds cats deers dogs frogs horses ships trucks. preprocess data using global contrast normalization whitening. data-augmentation architecture −−sv relu convolution layer max-pooling layer fully connected layer l-svm output layer. architecture greatly inspired square hinge loss minimized adam. exponentially decaying learning rate. batch normalization minibatch size speed training. last samples training validation set. report test error rate associated svhn benchmark image classiﬁcation dataset. consists training test color images representing digits ranging follow procedure used cifar- notable exceptions half number hidden units train epochs instead results table training dnns binary weights subject recent works even though share objective approaches quite different. train backpropagation variant called expectation backpropagation based expectation propagation variational bayes method used inference probabilistic graphical models. compare method ours comparison train binary weights propagations i.e. training procedure could implemented efﬁcient specialized hardware avoiding forward backward propagations multiplications amounts multiplications introduced novel binarization scheme weights forward backward propagations called binaryconnect. shown possible train dnns binaryconnect permutation invariant mnist cifar- svhn datasets achieve nearly state-of-the-art results. impact method specialized hardware implementations deep networks could major removing need multiplications thus potentially allowing speed-up factor training time. deterministic version binaryconnect impact test time could even important getting multiplications altogether reducing factor least memory requirement deep networks impact memory computation bandwidth size models run. future works extend results models datasets explore getting multiplications altogether training removing need weight update computation. thank reviewers many constructive comments. also thank roland memisevic helpful discussions. thank developers theano python library allowed easily develop fast optimized code gpu. also thank developers pylearn lasagne deep learning libraries built theano. also grateful funding nserc canada research chairs compute canada nuance foundation cifar. references geoffrey hinton deng george dahl abdel-rahman mohamed navdeep jaitly andrew senior vincent vanhoucke patrick nguyen tara sainath brian kingsbury. deep neural networks acoustic modeling speech recognition. ieee signal processing magazine nov. christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. technical report arxiv. jacob devlin rabih zbib zhongqiang huang thomas lamar richard schwartz john makhoul. fast robust neural network joint models statistical machine translation. proc. acl’ ilya sutskever oriol vinyals quoc sequence sequence learning neural networks. sang kyun lawrence mcafee peter leonard mcmahon kunle olukotun. highly scalable restricted boltzmann machine fpga implementation. field programmable logic applications international conference pages ieee tianshi chen zidong ninghui wang chengyong yunji chen olivier temam. diannao small-footprint high-throughput accelerator ubiquitous machine-learning. proceedings international conference architectural support programming languages operating systems pages yunji chen shaoli shijin zhang liqiang wang ling tianshi chen zhiwei ninghui dadiannao machine-learning supercomputer. microarchitecture annual ieee/acm international symposium pages ieee alex graves. practical variational inference neural networks. shawe-taylor r.s. zemel p.l. bartlett pereira k.q. weinberger editors advances neural information processing systems pages curran associates inc. nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research collobert. large scale machine learning. thesis universit´e paris glorot bordes bengio. deep sparse rectiﬁer neural networks. aistats’ xavier glorot yoshua bengio. understanding difﬁculty training deep feedforward neural zhiyong cheng daniel soudry zexi zhenzhong lan. training binary multilayer neural networks image classiﬁcation using expectation backpropgation. arxiv preprint arxiv. kyuyeon hwang wonyong sung. fixed-point feedforward deep neural network design using weights+ and. signal processing systems ieee workshop pages ieee jonghong kyuyeon hwang wonyong sung. real-time phoneme recognition vlsi using feed-forward deep neural networks. acoustics speech signal processing ieee international conference pages ieee thomas minka. expectation propagation approximate bayesian inference. uai’ james bergstra olivier breuleux fr´ed´eric bastien pascal lamblin razvan pascanu guillaume desjardins joseph turian david warde-farley yoshua bengio. theano math expression compiler. proceedings python scientiﬁc computing conference june oral presentation. fr´ed´eric bastien pascal lamblin razvan pascanu james bergstra goodfellow arnaud bergeron nicolas bouchard yoshua bengio. theano features speed improvements. deep learning unsupervised feature learning nips workshop goodfellow david warde-farley pascal lamblin vincent dumoulin mehdi mirza razvan pascanu james bergstra fr´ed´eric bastien yoshua bengio. pylearn machine learning research library. arxiv preprint arxiv. sander dieleman schlter colin raffel eben olson sren kaae snderby daniel nouri daniel maturana martin thoma eric battenberg jack kelly jeffrey fauw michael heilman diogo brian mcfee hendrik weideman takacsg peterderivaz instagibbs kashif rasul congliu britefury jonas degrave. lasagne first release. august", "year": 2015}