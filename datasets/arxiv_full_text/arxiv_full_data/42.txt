{"title": "Reasoning with Memory Augmented Neural Networks for Language  Comprehension", "tag": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "abstract": "Hypothesis testing is an important cognitive process that supports human reasoning. In this paper, we introduce a computational hypothesis testing approach based on memory augmented neural networks. Our approach involves a hypothesis testing loop that reconsiders and progressively refines a previously formed hypothesis in order to generate new hypotheses to test. We apply the proposed approach to language comprehension task by using Neural Semantic Encoders (NSE). Our NSE models achieve the state-of-the-art results showing an absolute improvement of 1.2% to 2.6% accuracy over previous results obtained by single and ensemble systems on standard machine comprehension benchmarks such as the Children's Book Test (CBT) and Who-Did-What (WDW) news article datasets.", "text": "hypothesis testing important cognitive process supports human reasoning. paper introduce computational hypothesis testing framework based memory augmented neural networks. approach involves hypothesis testing loop reconsiders progressively reﬁnes previously formed hypothesis order generate hypotheses test. applied proposed approach language comprehension task using neural semantic encoders models achieved state-of-the-art results showing absolute improvement accuracy previous results obtained single ensemble systems standard machine comprehension benchmarks children’s book test who-did-what news article datasets. formulating hypotheses testing cognitive process supports human reasoning intelligence. hypothesis testing process involves selective attention working memory cognitive control attention working memory engaged order maintain manipulate update hypotheses. cognitive control required inspect ignore incorrect hypotheses. inspired hypothesis testing process human brain support dynamic reasoning machines work introduce reasoning approach based memory augmented neural networks hypothesis formed regressing original statement hypothesis tested reality model satisﬁed current test response hypothesis true reasoning process halted answer found. otherwise another hypothesis formulated reﬁning previous process repeated answer found. idea modeling hypothesis testing mann remains generic reasoning framework applicable several tasks apply approach cloze-type using neural semantic encoders ﬂexible mann architecture shown notable success several language understanding tasks ranging sentence classiﬁcation language inference machine translation read compose write modules manipulate external memories introduced concept shared multiple memory accesses shown effective sequence transduction problems. cloze-type question answering clever assess ability human machine comprehend natural language. type tasks attractive natural language processing community test sets datasets generated without requiring expert’s supervision automatic manner typically useful training testing artiﬁcial intelligent systems understand human language. cloze-type setup machine ﬁrst presented text document containing fact asked output answer query related document. recent development large-scale cloze-type datasets deep neural network methods remarkable advances made order solve problem end-to-end fashion. existing neural network approaches broadly divided single-step multi-step comprehension depending documenting reading answer inference processes modeled. mimicking human readers deeper reasoning multi-step comprehension systems shown promising results however current multiturn models designed predeﬁned number computational hops inference difﬁculty document query pairs vary. query-document pairs require shallow reasoning like word sentence level matching deeper document level reasoning complex semantic understanding crucial. proposed comprehension models perform reasoning process called hypothesis-test loop. step hypothesis correct answer formed query regression. hypothesis checked true model halts reasoning process give correct answer. unlike previous methods ﬁxed computation models introduce halting procedure hypothesis-test loop. trained classic back-propagation algorithm models show consistent improvements state-of-the-art baselines cloze-type datasets. recently several large-scale datasets machine comprehension introduced including cloze-type consequently increasing interest developing neural network approaches solve problem end-toend fashion. existing models cloze-type categorized single-step multi-step approach depending comprehension process. singe-step comprehension methods read input document single computational make answer prediction. reading process mainly involves context modeling bi-directional recurrent neural networks selective focusing attention mechanism. hermann introduced cnn/daily news task along baseline models attentive reader impatient reader. attentive reader model reads document query bi-directional lstm networks selects query-relevant context attending document. chen re-designed attentive reader model examined cnn/daily news task. found roughly queries hermann dataset unanswerable recent neural network approaches obtained ceiling performance task. kadlec proposed attention reader model ﬁrst attends document aggregates attention score candidate answer select highest scoring candidate correct answer. however complex document query pairs deeper semantic association machine reading enough multiple reading checking crucial perform deeper reasoning. similar human reader multi-step comprehension methods read document query before making ﬁnal prediction. kind comprehension mostly achieved implementing external memory attention mechanism retrieve query-relevant information memory throughout time steps. hermann impatient reader revisits document states attention mechanism whenever reads next query word time scale. hill extended multi-hop memory networks self-supervision retrieve supporting memory. epireader performs two-stage computation first chooses probable answers model forms queries replacing answer placeholder original query candidate answer words. second epireader runs entailment estimation document query pairs predict correct answer. gated attention reader extends model document-gating iterative reading document query query representation used gating document iterative reading accomplished separate bi-directional gated recurrent unit networks computational hop. iterative alternative attention reader multi-step comprehension model uses network search correct answers document. model network expected collect evidence document query assists prediction last time step. introduced attention-over-attention loss computing word level query-document matching matrix. model provides ﬁne-grained word-level supervision signal seems help model training. proposed model performs multiple computational steps deeper reasoning. unlike previous work model number steps revise document predeﬁned dynamically adapted particular document query pair. furthermore deﬁne novel ways substitute query words word chosen document check hypothesis selected document word actually compliments query hypothesis true model halts reading process outputs word chosen document correct answer. used controller whole process throughout reasoning steps. among aforementioned models epireader seems relevant language comprehension models. however entailment estimation introduces constraint epireader limits application. model generic useful different tasks machine comprehension language-based conversational tasks knowledge inference link prediction. epireader tightly integrated two-stage neural network modules performance directly depends ﬁrst stage. ﬁrst module misses fails choose enough candidates correct answer found. model issue constrained forming queries. recently idea dynamic termination context language comprehension proposed shen independently. reinforcement learning explore different approaches adaptive computation query gating fully trained end-to-end back-propagation. dataset consists tuples document serving fact query candidate answers true answer. document query sequences tokens drawn vocabulary train model predict correct answer candidate given pair query document. main components proposed model shown figure first query document memory initialized context embedding memories processed memory read write operations throughout hypothesis-test loop. step loop read module formulates hypothesis updating query memory relevant content document memory. write module tests whether hypothesis true inspecting current query document states. selecting relevant content document regress query essentially inference model. intuitively input query regressed toward complete query containing correct answer word within write module thinks query complete correct answer found halts hypothesis-test loop. write module also supervises query state transitions retains right roll back query changes reasoning process. avoid overconﬁdent prediction halt reasoning process explore different strategies query gating adaptive computation. like human reader model reads document multiple times formulates hypothetical answer query tests story document throughout hypothesis-test steps. satisﬁed response current hypothesis model outputs response correct answer. instead using word embeddings initialize document query memories context embedding order inform memory slots contextual information text passages. figure high-level architectures proposed models query gating model adaptive computation model query memory former gated next step write module whereas query memory latter updated passed next step without gating write module trained halt hypothesis-test loop. read compose write module. note model context embedding network network accepting word embeddings input multilayer perceptron convolutional neural network. choose bilstm able learn word-centered context representation effectively reading text lstms left right concatenating resulting hidden vectors. query document memories processed iterative process called hypothesis-test loop. step loop query memory updated content document memory form query query checked document facts used make answer prediction read compose write modules collectively perform following overall process. intuitively depending previously retrieved document content well previous query states read module retrieves document word relocated query computes positions word query. since document word located multiple different positions query sigmoid function used normalize alignment vector document deﬁnes position state query memory. decision made sequentially every step equip read module lstm network initialize document query states compose compose module combines current query document states current hidden state read module resulting single vector passed write module subsequent process. compose module viewed feature extractor current document query pair. taking hidden state compose module also informs write module read module’s current decision. write module accepts outputs read module updates query memory follows write module also responsible checking hypothesis order decide whether halt hypothesis-test loop ﬁnal answer continue. explore different strategies discussed below. methods take output compose module employs lstm make sequential decision. figure shows overall architecture model query gating mechanism. model instead making hard decision halting loop write module performs word-level query gating seen memory gating process prevents model forgetting query information. note even query memory updated document content given read module write module makes ﬁnal decision based features extracted compose module. words write module decides keep query information changes query simply ignored query previous time step passed along next step. number steps hypothesis-test loop hyperparameter model. therefore setup write module expected lock query state gating mechanism soon hypothesis true. model write module equipped termination head shown figure particularly write module termination head decides willingness continue ﬁnish computation step. deﬁne probabilistic framework halting. approach similar input output handling mechanism neural random-access machines time step write module outputs termination score follows also introduce hyperparameter maximum number permitted steps. model runs time without halting process force model output ﬁnal answer step case probability stop reading used compute probability step query-to-document alignment score answer correct given document query. particular adapt pointer attention mechanism model human lstms memnns reader reader epireader reader reader memnn reader epireader reader query gating query gating query gating query gating adaptive computation adaptive computation evaluated models large-scale datasets childrens book test who-did-what focused tasks still large human machine performances contrast cnn/daily news datasets covered section dataset constructed children book domain whereas corpus built news article domain therefore think datasets quite representative evaluation models. furthermore dataset comes difﬁcult tasks depending type answer words predicted named entity common nouns training different setups strict relaxed baseline suppression. table summarizes important statistics datasets. chose one-layer lstm networks read write modules singlelayer composition module. used stochastic gradient descent adam optimizer train models. initial learning rate cbt-cn tasks. pre-trained glove vectors used initialize word embedding layer; therefore embedding layer size hidden layer size context embedding bilstm nets embeddings out-of-vocabulary words model parameters randomly initialized uniform distribution gradient model human attentive reader reader reader stanford attentive reader query gating query gating query gating query gating adaptive computation adaptive computation clipping threshold models regularized applying dropouts embedding layer. used batch size dataset dataset early stopping patience epoch. dataset anonymized answer candidates following work onishi hermann used following batching heuristic order speedup training. created temporary example pool randomly sampling training sorted according length document. ﬁrst examples ordered pool batch without replacement rest pool replaced back training set. performed enough training examples create pool. finally documents batch padded special symbol <pad> length. batches regenerated every epoch prevent model learning simple mapping function. also padded queries batch special symbol equal length. model models update memory enough time read back thus reduced model called typically greater proposed halting strategies result different models. query gating model achieves accuracy tasks outperforming previous baselines. performance varies different number steps. larger number allowed steps query gating model tends overﬁt dataset performance test sets increases. adaptive computation model sets best score cbt-ne task accuracy. overall models bring modest improvements cbt-ne cbt-cn tasks performance differences human machines task single model adaptive computation scores higher previous best result stanford attentive reader. query gating model obtains best result comparing proposed variations adaptive computation model robust sets state-of-the-art performance three four tasks effectively deciding halt reasoning processing termination head. memory locking show effective termination-based approach task. appendix visualize query regression process models observe queries generated query words relevant correct answer overwritten memory write module. performance model matches previous single systems. general given small number permitted steps proposed models tend less overﬁt ﬁnal performance high. number permitted steps increases test accuracy improves yielding overall higher performance. however holds certain point large permitted steps longer observe signiﬁcant performance improvement terms testing accuracy. example query gating model achieved accuracy test accuracy cbt-ne task showing highest accuracy massively overﬁtting test set. furthermore becomes expensive train model large number allowed steps. worth noting even proposed models work trained using classic end-toend back-propagation easily trained reinforcement learning methods reinforce algorithm evaluation. adaptation particularly straightforward adaptive computation model already incorporated probabilistic termination head inspired cognitive process hypothesis testing human brain proposed reasoning approach based memory augmented neural networks applied language comprehension. proposed models dynamic reasoning achieved state-of-the-art results machine comprehension tasks. order halt reasoning process explored different strategies fully trained classic back-propagation algorithm query memory gating prevents forgetting query adaptive computation termination head. adaptive computation model shown effective experiments. proposed models trained using reinforcement learning. plan apply approach tasks language-based conversational tasks link prediction knowledge inference. would like thank abhyuday jagannatha jesse lingeman reviewers insightful comments suggestions. work supported part grant national institutes health grant ihx- supported health services research development department veterans affairs investigator initiated research. opinions ﬁndings conclusions recommendations expressed material authors necessarily reﬂect sponsor. felix hill antoine bordes sumit chopra jason weston. goldilocks principle reading children’s books explicit memory representations. arxiv preprint arxiv. figure depicts memory states query gating model input document shown top. noted previously model updates query memory position lower memory value rolls back ignores query change gating value position. attended words resulted memory read operation deﬁne information update. listed top- attended words figure already includes correct answer. note memory gating states changed regression steps. memory values varied across query positions. overall query tokens ’he’ ’meadows’ pointed updated. proceeding query regression model effectively adjusts gates. ﬁrst steps model willing accept updates occurred certain positions ’he’ ’meadows’. however gating value reaches gates closed later steps. model longer accepts query updates step ready output correct answer. analysis showed model rarely updates query place holder position desirable iterative query regression process. placeholder information remains unchanged order used subsequent steps inform model part query needs completed. figure showed memory termination head states input pair set. model actively performs query regression ﬁrst steps halts process termination head reaches third step. interestingly memory points query positions update ﬁrst step values gradually increase later steps implying information exchange document query. answer word ’johnny’ place holder information overwritten loop halted. figure visualization query regression process query gating model. input document along correct answer shown. top- attended words throughout hypothesis-testing loop highlighted. bottom visualize states memory query gate hypothesis-testing steps ranging shown space limitation. gating value near ignore query update memory value closer zero indicates query update position. figure visualization query regression process adaptive computation model. input document along correct answer shown. top- attended words throughout hypothesis-testing loop highlighted. bottom visualize states memory termination head. memory value closer zero indicates query update position termination score closer halt hypothesis testing loop.", "year": 2016}