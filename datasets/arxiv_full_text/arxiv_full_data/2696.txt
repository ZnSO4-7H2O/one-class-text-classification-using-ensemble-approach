{"title": "Temporal-related Convolutional-Restricted-Boltzmann-Machine capable of  learning relational order via reinforcement learning procedure?", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "In this article, we extend the conventional framework of convolutional-Restricted-Boltzmann-Machine to learn highly abstract features among abitrary number of time related input maps by constructing a layer of multiplicative units, which capture the relations among inputs. In many cases, more than two maps are strongly related, so it is wise to make multiplicative unit learn relations among more input maps, in other words, to find the optimal relational-order of each unit. In order to enable our machine to learn relational order, we developed a reinforcement-learning method whose optimality is proven to train the network.", "text": "time dependent learning tasks. traditionally recurrent neural network shown efficiency time-dependent recognition problems. instance widely used elegant framework manipulate audio model based time-related inputs. recent researches also combined power convolutional restricted boltzmann machine gated autocoder factorized crbm later attention driven special form restricted boltzmann machine built learn feature transformations describe relations time-related input maps. model similar conventional hidden stack layers constructed describe conditional probabilistic distributions inputs. difference traditional model hidden layers take account several input maps different time moment. hidden layers able extract features combination several time-related maps. extract features explained hidden layers represent correlation inputs maps words describe matrix transformation another. previous works noted multiplicative interaction effective correlate input maps. method model combine related inputs. learned hidden features transformation machine predict later inputs based conditional distributions learned carried hidden layers rbm. also power high-order temporal dependencies describe learn features even abstract. orther words learn features transformations input maps. achieved hidden layers constructed learning efficiency brought deep learning architecture. limitations higher order temporal model. since conventional multiplicative interaction takes account related maps lacks ability correlate input maps therefore learn features related inputs. theoretically learn temporal dependence among inputs matter high-order training process. thus combining input maps achievable efficient. practice however cause number layers parameters needed learned explode. moreover cases large amount similar strongly correlated bstract— recent works recurrent neural network deep learning architecture shown power input deep learning modeling time dependent sequences. gradient-based higher-order learning manifold recurrent grammar cell reveal ability learn feature transformation related input maps perform well time-related learning prediction tasks higher order cases. article conventional convolutional-restricted-boltzmann-machine learn highly abstract features among abitrary number time related input maps layer multiplicative units capture relations among inputs. cases care transforms another multiplicative unit takes features maps. cases however maps strongly related reasonable make multiplicative unit learn relations among input maps words find optimal relational-order unit. order enable machine learn relational order developed reinforcement-learning method whose optimality proven train network. keywords artificial neural network; convolutional-restricted-boltzmann-machine; reinforcement learning; deep learning; temporal-related; relational-order unsupervised learning combined deep architecture unveiled part mystery artificial intelligence. learning techniques boarder applications areas like visual recognition natural language processing audio detection growing computational capabilities deep learning framework like convolutional deep belief network bring contribution cognitive science. recently researchers begin take next step trying develop model handle input maps sequence wise combine together stack hidden layers describe correlation saving space parameters time learning. this define term relational order number maps stack hidden layers learn features from words number maps correlate using multiplicative interactions. finally developed reinforcement-learning based method learn relational order time minimizing reconstruction error. proved satisfies sub-problem structure dynamic programming therefore optimized reconstruction error group related input maps globally optimal solution entire input sequence. traditional temporal-dependent widely used model language processing audio recognition time-evolutional learning tasks deep belief network revealed advantages gaussian mixture models automatic speech recognition. recent works image recognition show convolutional network dramatically enhance power artificial neural network. basically architecture includes gained convoluting high hidden layers dimensional inputs kernels pooling layer exclude noise order gain highly abstract features. constructing convolutional layer takes advantage local invariance parameter sharing image classification tasks since objects need recognized image input show different locations angles crucial equip model translational invariance property. sharing parameters hand reduce parametric redundancy therefore save space. thus instead defining independent hidden units learning features different parts input space make hidden units share combination weights extract features come different locations. advantages make extremely efficient. therefore article developed model based convolutional deep neural network takes temporal dependency account. model combines power deep convolution capability time-based model handle temporal related inputs. showed efficiency tasks learning matrix transformations related inputs. recently convolutional restricted boltzmann machine used extract features high dimensional highly abstract dataset case image processing extract features image convoluting kernels construct layer updating kernels using cd-k algorithm able perform task image recognition efficiently sending outputs classical discriministic layer generate image sampling joint distribution latent variable described hidden layers c-rbm conventional recurrent network proven useful model handle time dependent learning tasks recognition prediction future outcomes based previous inputs. order combine power deep learning ability extract features time sequence researchers developed structures collaborate rnn. example stacks hidden units used model time evolution features hidden pooling layer computes hidden units sigmoid function finally update weights minimizing distance function real visible inputs generative visible variables sampled conditional distribution model provides basic time-evolved learning tasks. however unlike model deep audio recognition framework input time-related sequence machine constructs distribution hidden layer capture time dependent features sequenced audio signals c-rbm lacks ability grab relations time related sequence images. therefore article different framework combines structure high convolutional-neural-network restricted boltzmann machine constructing stack hidden variables. idea instead building stacks hidden layers model constructs multiplicative units relating present input previous ones. model hidden variables viewed layer portrays correlation among observations different time prediction hidden layer generates future maps based many cases multiplicative unit takes account input maps. major drawback framework model takes correlated features among observations specified length time range. however natural brain system cognitive processes affected wider range inputs taken sensors. therefore order make machine capable learning optimal range input maps multiplicative unit takes features from reinforcement learning model specified time range help machine take different length inputs sequences solving optimal value reward function defined model. bi-related feature learning machine extracts features related input maps. order combine pair maps multiplicative unit constructed take matrix multiplication maps input connected stack hidden layers different kernels. hidden layer constructed sampling probabilistic distribution gained convoluting input kernels separately. parameters model include kernels biases associated hidden layers multiplicative unit respectively. ideally want learn kernel bias maximizing likelihood gradient ascent fashion given conditional distributions gradient likelihood crbm based energy model described novel namely called contrastive divergence approximate gradient update parameters gradient approximated gradient ascent procedure. every iteration updates multiplicative unit sampled hidden layers times. take kernel matrix example gradient approximated upper index input layers means original input maps sampled times respectively. m-step procedure converge shown hinton similarly gradients associated bias written gradients matrix form want. denote dimension multiplicative unit kernel hidden layers definition convolution nh=nv-nw+. according gradient kernel gained take difference convolution hidden layer acts kernel. therefore definition dimension gradient ng=nv-+=nw. similarly easy show dimension bias’s gradients equal bias. thus verified gradients correct form. used notation represent valid convolution last term viewed convolutional kernel. also write matrix form refine sigmoid operation matrix’s element similarly conditional distribution multiplicative unit concating original full generative multiplicative-unit. denote ginal generative multiplicative unit approximating generative multiplicative unit least squares find proximated version generative input maps. used lstsq notation least square method solves equation lstsq combining related maps together boosting learning efficiency reducing unnecessary waste learning space parameters hidden layers constructed training process. specifically typical tri-relational model multiplicative units takes account three subsequent maps instead stack hidden layers constructed learn correlation among three maps. since energy function conditional distribution bi-related model hold tr-related model need slightly change form able train model algorithm m-step constrastive divergence step input maps sampled conditional distributions figure. shows structural difference bi-related tri-related model. tr-temporal-related crbm higher-temporal-related crbm relational-order higher three. assume using n-relational-order crbm write conditional distributions input maps discussed bi-related model sufficient handle temporal learning task advantage brought deep crbm. however practice need extend bi-multiplicative unit learn correlation maps. reason making extension many image recognition problems maps correlated. therefore constructing hidden layers extract relation maps space-consuming. also cases rate frames sent unit might fast model manipulate. example cases want predict objects’ locations video based features captured temporal crbm inputs gained cutting video input pieces frames represents input model enough time train network completely two-related input maps next pair related maps shows therefore options keep rate frames down. words need slow rate video input frames model give machine time learn features pair maps. however extremely time-consuming. section introduce general structure address problems. recall defined term relational-order number input maps hidden layers capture features from want machine break constrain extracting features limited number maps determines relative value versus immediate reward. specifically importance reward received time step decreased exponentially factor based definitions above best policy want model learn policy gives biggest cumulative reward. that model take best action following optimal policy defined above. alternatively base cumulative reward function choose among actions well. case redefine best policy recursive form value function provides basis algorithm iteratively approximates value efficiently shows value gained recursive method converge optimal value. upper index represents original represent constrastive divergence. function dominator normalize reconstruction error relational-order scale. since we’d like make model choose higher-order model sequence inputs fewer number multiplicative units thereby reduce number parameters multiply monotone increasing function respect relational-order increase also increase therefore reconstruction error input maps change much value would decrease. define reward function maximizing reward minimizing reconstruction error. general procedure initialize value choose first relational order according gaussian distribution. taking action based probabilistic distribution defined discussed section major problem faced specific relational-order crbm lack generality. relational-order determined model constrained learn features specific number maps training process. however many cases sequences input maps correlations same. example input sequences number strongly related maps might three change later therefore reasonable make relational order evolve time. this first proved sub-sequence given sequence input maps satisfies sub-problem structure dynamic programming. develop reinforcement learning procedure learn relational-order multiplicative unit minimizing reconstruction error inputs recursively dynamic programming method minimizes reconstruction error sub-sequence input maps. finally proved optimality hold method. first denote markov decision process state-space action-space represents reward function defined reward returned taking action state task learn policy maps current state action. obvious approach determine overall value policy evaluate cumulative reward policy time. formally cumulative reward section focus experiments demonstrate performance temporal-related crbm practice. famous minist-dataset. training process give model sequence digit input maps. apply td-crbm learn matrix transformations among maps. finally matrix transformations learned produce generative maps digit inputs measure reconstruction error show model performance. test model’s ability capture simple transformation shifts rotation single digit. construct bi-temporal model consists multiplicative unit relational-order connects stack hidden layers. give model sequences input maps length consists digit maps different angles towards axis. train model following algorithm store stack hidden layers feature maps describe correlation pair digit inputs. training epoch finished calculate reconstruction error generative digit inputs original ones. reconstruction error drops exponentially confirms efficiency ability bi-related model. figure. shows performance bi-related model task learning transformation pair minist-digit maps. discussion previous sections higher-related practical reduces amount model parameters must learned. section implemented experiment shows different performance higher bi-temporal-related models. current state model take action along procedure model stores table value representing reward relational-order state. based table model choose least reconstruction error training step taking actions according distribution also brings advantage short-term memory since previously accumulative rewards added influence state current training step. also confirms previous assumption relational-orders nearby multiplicative units also correlated. relational-order current unit next unit would probably around instead changing suddenly. multiplicative unit find optimal relation-order along minimizes reconstruction error input maps. would minimizing sub-sequences minimizes reconstruction error whole sequence input maps? show indeed assume sequence input maps length construct multiplicative units relational-order defined confirms statement optimal whole sequence gained finding optimal sub-sequence. core dynamic programming artificial called reinforcement learning introduced earlier. thus reinforcement-learning procedure train high-temporal-related model learning epoch. again give model sequences maps consists digit maps number digit. constraint amplitude transformation pair inputs. specifically rotation last exceed degrees shift centroid exceed algorithm train bi-related model algorithm relational-order train tri-related model algorithm initial order train higher-related model first analyze bi-related model. figure. figure. performance bi-related model case. relational-order multiplicative unit basic one. feature learned unit well describe relationship pair input maps matter much transform another. tri-related model however even though roughly training epoch bi-related perform well constraint transformation pair maps. reconstruction error still even training epochs. true limitation transformation least number highly related maps two. multiplicative unit learn features maps transformation. learn transformation three maps different transformation error goes example consider sequence three maps first rotates degrees clockwise second second rotates degrees counterclockwise three. case bi-related model constructs multi-units learns feature rotation. tri-related model however perform well multi-unit learn different transformations learning epoch. reinforcement--learning procedure flexible. choose optimal relational-order multi-units give least reconstruction error. figure. highly related higher-order model constructs multi-units relational-order higher thereby reducing number parameters dramatically. figure constraint transformation even though error early steps training higher models high-order model adjust relational-order optimal gives least reconstruction error. training epochs outperforms tri-related model. training epochs reduce reconstruction error roughly scale bi-related model. article introduced crbm structure models temporal-dependent input sequences constructing multiplicative units hidden layers learn features correlation among related input maps. combined model reinforcement--learning procedure. made model capable learning optimal relational-order multiplicative units give least amount errors. experiment model reinforcement--learning procedure flexible choosing relational-order. generative maps accuracy bi-related model long-term. reinforcement--learning strategy increases complexity computation amount training time dramatically. takes training epochs model reduce reconstruction error performance catches bi-related model long-term. therefore researches reduce training period needed. michalski memisevic konda k..modeling deep temporal dependencies recurrent \"grammar cells.advances neural information processing systems learning introduction.neural networks ieee transactions howard dynamic programming.management science stochastic dynamic programming.technometrics control.athena scientific sainath t，n. kingsbury ramabhadran fousek p.making deep belief networks effective large vocabulary continuous speech recognition.automatic speech recognition understanding envelopes using restricted boltzmann machines deep belief networks statistical parametric speech synthesis.ieee transactions audio speech language processing alpaydin introduction machine learning. zizhuang wang chinese high school student entering grade year finishing school courses perfect multiple school scientific competitions grade started focus researches artificial intelligence mathematics. wang’s major research interest lies machine learning neural networks bayesian inference applied mathematics quantum computation. http//kingofspacewzz.github.io/ convolutional deep belief networks scalable unsupervised learning hierarchical representations.international conference machine learning pham unsupervised feature learning audio classification using convolutional deep belief networks. .advances neural information processing systems zhang johnson t..rnn language model word clustering class-based output layer.eurasip journal audio speech music processing taylor factored conditional restricted boltzmann machines modeling motion style. ..international conference machine learning chen，cy zhang，l chen，m gan.fuzzy restricted boltzmann machine enhancement deep learning.ieee transactions fuzzy systems fischer igel c..training restricted boltzmann machines introduction.pattern recognition sejnowski terrence higher-order boltzmann machines.aip conference neural networks computing memisevic hinton .learning represent spatial transformations factored higher-order boltzmann machines .neural computation memisevic r.learning relate images.pattern analysis machine intelligence ieee transactions", "year": 2017}