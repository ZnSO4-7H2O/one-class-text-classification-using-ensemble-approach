{"title": "No bad local minima: Data independent training error guarantees for  multilayer neural networks", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "We use smoothed analysis techniques to provide guarantees on the training loss of Multilayer Neural Networks (MNNs) at differentiable local minima. Specifically, we examine MNNs with piecewise linear activation functions, quadratic loss and a single output, under mild over-parametrization. We prove that for a MNN with one hidden layer, the training error is zero at every differentiable local minimum, for almost every dataset and dropout-like noise realization. We then extend these results to the case of more than one hidden layer. Our theoretical guarantees assume essentially nothing on the training data, and are verified numerically. These results suggest why the highly non-convex loss of such MNNs can be easily optimized using local updates (e.g., stochastic gradient descent), as observed empirically.", "text": "smoothed analysis techniques provide guarantees training loss multilayer neural networks differentiable local minima. speciﬁcally examine mnns piecewise linear activation functions quadratic loss single output mild over-parametrization. prove hidden layer training error zero every differentiable local minimum almost every dataset dropout-like noise realization. extend results case hidden layer. theoretical guarantees assume essentially nothing training data veriﬁed numerically. results suggest highly non-convex loss mnns easily optimized using local updates observed empirically. multilayer neural networks achieved state-of-the-art performances many areas machine learning success typically achieved training complicated models using simple stochastic gradient descent method variants. however guaranteed converge critical points gradient expected loss zero speciﬁcally stable local minima since loss functions parametrized weights non-convex long mystery work well rather converging local minima training error high previous results suggest training error local minima mnns extremely wide layers. however wide mnns would also extremely large number parameters serious overﬁtting issues. moreover current state results typically achieved deep mnns rather wide. therefore interested provide training error guarantees practical number parameters. common rule-of-the-thumb multilayer neural network least many parameters training samples regularization dropout reduce overﬁtting. example alexnet million parameters trained using million examples. over-parametrization regime continues recent works achieve state-of-the-art performance deep networks networks typically under-ﬁtting suggests training error main bottleneck improving performance. work focus mnns single output leaky rectiﬁed linear units. provide guarantee training error zero every differentiable local minimum mild over-parametrization essentially every data set. hidden layer show training error zero dlms whenever number weights ﬁrst layer larger number samples i.e. width activation l-th layer. mnns layers show that dl−dl− convergence potentially dlms averted using small perturbation mnn’s weights ﬁxing weights except last weight layers aspect approach presence multiplicative dropout-like noise term mnns model. formalize notion validity essentially every dataset showing results hold almost everywhere respect lebesgue measure data noise term. approach commonly used smoothed analysis algorithms often affords great improvements worst-case guarantees intuitively rare cases results hold almost inﬁnitesimal perturbation input activation functions this. thus results assume essentially structure input data unique sense. ﬁrst seem hopeless training error guarantee mnns. since loss mnns highly non-convex multiple local minima seems reasonable optimization would stuck local minimum. moreover many theoretical hardness results proven mnns hidden layer. despite results easily achieve zero training error mnn’s last hidden layer units training samples case useful since results huge number weights leading strong over-ﬁtting. however wide networks easy optimize since training last layer global minimum almost every random initialization qualitatively similar training dynamics observed also standard mnns. speciﬁcally training error usually descends single smooth slope path barriers training error local minima seems similar error global minimum latter explained analogy high-dimensional random gaussian functions critical point high global minimum probability local minimum. different explanation phenomenon suggested there mapped spin-glass ising model local minima limited ﬁnite band global minimum. however clear relevant statistical mechanics results actual mnns realistic datasets. first analogy qualitative mapping requires several implausible assumptions second statistical mechanics results become exact limit inﬁnite parameters ﬁnite number layers layer inﬁnitely wide. however extremely wide networks serious over-ﬁtting issues explained before. previous works shown that given several limiting assumptions dataset possible training error hidden layer proved convergences linearly separable datasets; either required clustering classes. going beyond training error showed mnns hidden layer learn order polynomials product gaussians distributional assumption input. also devised tensor method instead standard method mnns hidden layer guaranteed approximate arbitrary functions. note however last works require rather large good guarantees. recover common leaky rectiﬁed linear unit nonlinearity ﬁxed slope matrix viewed realization dropout noise implementations distributed discrete competitive performance obtained continuous distributions results apply directly latter case. inclusion innovative part model performing smoothed analysis jointly able derive strong training error guarantees. however dropout purely proof strategy; never expect dropout reduce training error realistic datasets. discussed sections measure-theoretic terminology throughout paper make extensive term -almost everywhere a.e. short. taken mean almost everywhere respect lebesgue measure entries .... property hold a.e. respect measure objects doesn’t hold measure particular results hold probability whenever taken i.i.d. gaussian entries arbitrarily small gaussian i.i.d. noise used smooth input mnns typically trained minimizing loss training using stochastic gradient descent variants section next guarantee zero training loss common case over-parametrized mnn. analyzing properties differentiable local minima focus dlms since rather mild conditions asymptotically converges dlms loss note simpliﬁed notation actually change weights equations activation slopes remain same i.e. exists inﬁnitesimal perturbation reduces exists corresponding inﬁnitesimal perturbation reduces therefore must also clearly dlms value. therefore proceed assuming constraint derive automatically apply derivative equal zero. calculate derivative rely facts. first always switch order differentiation expectation since average ﬁnite training set. second differentiable point derivative respect weights zero. thus that examine implications approach mnns hidden layer. dlms general need differentiate equate zero. section exchange order expectation differentiation fact piecewise constant. differentiating near respect vectorized version obtain constraints corresponding parameters mnn. previous section rank must however generally difﬁcult rank since need whether different linearly dependent rows. therefore focus last hidden layer condition rank ensures however since depends weights cannot results previous section possible rank example differentiable critical point generally global minimum. intuitively cases seem fragile since give random perturbation would expect typically would rank establish idea ﬁrst proving following stronger result theorem dl−dl− ﬁxed values differentiable local minimum function also global minimum almost everywhere. theorem means random weights ﬁrst layers every respect weights last layers also global minimum loss note condition dl−dl− implies weights contrast instead allowed adjust last layer random training error ensured extremely wide layers require much parameters theorem easily extended types neural networks beyond basic formalism introduced section example replace layers convolutional layers types architectures. additionally proof theorem holds ﬁxed identical nonzero entries dropout turned except last hidden layers. result continues hold even ﬁxed well condition dl−dl− weakened minl≤l− next formalize intuition dlms deep mnns must zero loss fragile sense following immediate corollary theorem corollary dl−dl− differentiable local minimum consider weight vector i.i.d. gaussian entries arbitrarily small variance. then almost everywhere probability w.r.t. ˜wl− held ﬁxed differentiable local minima function also global minima note result different classical notion linear stability differentiable critical points based analysis eigenvalues hessian mse. hessian written symmetric block matrix blocks rdm−dm×dl−dl corresponds layers speciﬁcally using block written components interestingly positive semi-deﬁnite nature hessian dlms imposes additional constraints error. note matrix symmetric positive semi-deﬁnite relatively small rank however potentially high rank thus many negative eigenvalues zero eigenvalues also zero). therefore intuitively expect positive semi-deﬁnite become small generically indeed observed empirically section examine numerically main results paper theorems hold almost everywhere respect lebesgue measure data dropout realization. however without dropout analysis guaranteed hold. example results hold mnns weights negative constant entries therefore cannot full rank. nonetheless activations sufﬁciently variable expect results hold even without dropout noise leaky relu’s replaced basic relu’s tested numerically present result figure performed binary classiﬁcation task synthetic random dataset subsets mnist dataset show mean classiﬁcation error commonly used tasks. note mnist dataset contains redundant information training samples much easier completely random synthetic data. thus performance random data representative typical worst case smoothed analysis approach aimed uncover. hidden layer error goes zero number non-redundant parameters greater number samples predicted theorem theorem predicts similar behavior hidden layers prediction also seems hold less tightly. reasonable analysis section suggests typically error would zero total number parameters larger number training samples though proven. note repetitions figure matrix always full rank. however smaller mnns shown figure sometimes full rank. recall theorems give guarantees training error dlm. however ﬁnite since loss non-differentiable points clear dlms actually exist converge them. check indeed case performed following experiment. trained many epochs using batch gradient steps. then figure final training error over-parametrized regime predicted results trained standard mnns hidden layers single output relu activations loss dropout datasets synthetic random dataset drawn normal distribution probability binary classiﬁcation sized subsets mnist dataset value data point average mean classiﬁcation error repetitions. ﬁgure mean reached zero zero repetitions. started gradually decrease learning rate. activation inputs converge distinctly non-zero value demonstrated figure ﬁgure tested small synthetic data neural inputs seem remain constant non-zero value keeps decreasing. typical case experiments. however instances would converge value indicate convergence non-differentiable points possible well. implementation details weights initialized uniform mean zero variance suggested epoch randomly permuted dataset used adam optimization method figure training done epochs different learning rates mini-batch sizes selected dataset architecture. work provided training error guarantees mildly over-parameterized mnns differentiable local minima single hidden layer proof surprisingly simple. show near locally similar linear regression allows prove that almost everywhere number nonredundant parameters larger number samples dlms global minima linear regression. hidden layers theorem states dl−dl− always perturb weights dlms would global minima note realistic setting zero training error necessarily intended objective training since encourage overﬁtting. main goal show essentially dlms provide good training error however decrease size model artiﬁcially increase number samples convergence gradient descent then starting epoch gradually decreased learning rate activation inputs converged values ﬁnal magnitudes numbers fact neuronal inputs keep decreasing learning rate indicate converged differentiable local minimum equal predicted. augmentation re-sampling dropout noise) mildly under-parameterized regime relatively small error seen figure example alexnet weights larger required theorem however without data augmentation dropout alexnet exhibit severe overﬁtting. analysis non-asymptotic relying fact that near differentiable points mnns piecewise linear activation functions differentiated similarly linear mnns smoothed analysis approach examine error slight random perturbations worst-case input dropout. experiments suggest results describe typical performance mnns even without dropout. note claim dropout merit reducing training loss real datasets used practice dropout typically trades training performance favor improved generalization. thus role dropout results purely theoretical. particular dropout ensures gradient matrix full column rank. would interesting direction future work sufﬁcient conditions full column rank. many directions remain future work. example believe possible extend work multi-output mnns and/or convex loss functions besides quadratic loss. results might also extended stable non-differentiable critical points using necessary condition sub-gradient contains zero critical point another important direction improving results theorem would make efﬁcient parameters mnns last weight layers. results might used guideline architecture design training error major bottleneck last least work focused empirical risk dlms. guarantees might combined generalization guarantees obtain novel excess risk bounds beyond uniform convergence analysis. authors grateful barak carmon han. harel meir meirom paninski rubin stern sümbül wolf helpful discussions. research partially supported gruss lipper charitable foundation intelligence advanced research projects activity department interior/ interior business center contract number dpc. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright annotation thereon. disclaimer views conclusions contained herein authors interpreted necessarily", "year": 2016}