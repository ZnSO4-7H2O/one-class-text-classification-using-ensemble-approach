{"title": "Generalized Majorization-Minimization", "tag": ["cs.CV", "cs.IT", "cs.LG", "math.IT", "stat.ML"], "abstract": "Non-convex optimization is ubiquitous in machine learning. The Majorization-Minimization (MM) procedure systematically optimizes non-convex functions through an iterative construction and optimization of upper bounds on the objective function. The bound at each iteration is required to \\emph{touch} the objective function at the optimizer of the previous bound. We show that this touching constraint is unnecessary and overly restrictive. We generalize MM by relaxing this constraint, and propose a new framework for designing optimization algorithms, named Generalized Majorization-Minimization (G-MM). Compared to MM, G-MM is much more flexible. For instance, it can incorporate application-specific biases into the optimization procedure without changing the objective function. We derive G-MM algorithms for several latent variable models and show that they consistently outperform their MM counterparts in optimizing non-convex objectives. In particular, G-MM algorithms appear to be less sensitive to initialization.", "text": "non-convex optimization ubiquitous machine learning. majorization-minimization procedure systematically optimizes non-convex functions iterative construction optimization upper bounds objective function. bound iteration required touch objective function optimizer previous bound. show touching constraint unnecessary overly restrictive. generalize relaxing constraint propose optimization framework named generalized majorization-minimization ﬂexible compared instance incorporate application-speciﬁc biases optimization procedure without changing objective function. derive g-mm algorithms several latent variable models show empirically consistently outperform counterparts optimizing non-convex objectives. particular g-mm algorithms appear less sensitive initialization. keywords majorization-minimization non-convex optimization latent variable models expectation maximization non-convex optimization ubiquitous machine learning. example data clustering training classiﬁers latent variables training visual object detectors weakly labeled data lead non-convex optimization problems. majorization-minimization optimization framework designing well-behaved optimization algorithms non-convex functions. algorithms work iteratively optimizing sequence easy-to-optimize surrogate functions bound objective. successful instances algorithms expectation-maximization concave-convex procepropose procedure generalized majorization-minimization optimizing non-convex objective functions. approach inspired generalize bound construction process. speciﬁcally relax strict bound construction method allow valid bounds used still maintaining algorithmic convergence. relaxation gives freedom bound selection used design better optimization algorithms. training latent variable models clustering problems algorithms cccp k-means known sensitive initial values latent variables cluster memberships. refer problem stickiness algorithm initial latent values. experimental results show g-mm leads methods tend less sticky initialization. demonstrate beneﬁt using g-mm multiple problems including k-means clustering applications latent structural svms image classiﬁcation latent variables. perhaps famous iterative algorithm non-convex optimization machine learning statistics algorithm best understood context maximum likelihood estimation presence missing data latent variables. bound optimization algorithm e-step lower bound likelihood constructed m-step maximizes bound. countless eﬀorts made extend algorithm since introduction. shown that steps involve optimizing functions necessary fully optimize functions step; fact step needs make progress. relaxation potentially avoid sharp local minima even speed convergence. another attempt hunter proposed majorization-minimization framework. generalizes methods like transferring optimization sequence surrogate functions original objective function. concaveconvex procedure another widely-used instance surrogate function obtained linearizing concave part objective function. many successful machine learning algorithms employ cccp e.g. latent instances algorithms include iterative scaling non-negative matrix factorization another related line research concerns diﬀerence-of-convex programming shown reduce cccp certain conditions. convergence properties general bound optimization algorithms discussed salakhutdinov despite widespread success number drawbacks motivated work. practice cccp often exhibits stickiness initialization necessitates expensive initialization multiple trials optimizing latent variable models cccp lacks ability incorporate application-speciﬁc information without making modiﬁcations objective function prior knowledge side information latent variable uncertainty posterior regularization framework deal drawbacks. observation relax constraint enforced requires bounds touch objective function relaxation gives ability better avoid sensitivity initialization incorporate side information. closely related work pseudo-bound optimization framework tang generalizes cccp using pseudo-bounds intersect objective function. contrast framework still uses valid bounds relaxes touching requirement. also pseudo-bound optimization framework general designed speciﬁcally optimizing binary energies mrfs restricts form surrogate functions parametric max-ﬂow. generalized variants proposed analyzed neal hinton gunawardana byrne related work restrict attention probabilistic models algorithm. viewed bound optimization procedure likelihood function involves model parameters distribution latent variables denoted choosing posterior leads lower bound tight current estimate generalized versions given neal hinton distributions posterior alternating optimization framework exact objective function change bound construction step propose stochastic deterministic strategies bound construction demonstrate lead higher quality solutions less sensitivity initialization em-like methods. converges. solution iteration obtained minimizing upper bound objective function i.e. argminw bound iteration chosen valid bounds practice assume members come family functions upper bound measures progress respect objective values. guarantee progress time requires bound iteration must touch objective function previous solution leading following constraint touching requirement restrictive practice make hard avoid local minima. particular make algorithms cccp sensitive initialization touching constraint also eliminates possibility using bounds touch objective function desirable properties. valid subset upper bounds satisfy denote valid bounds iteration consider scenarios constructing bounds g-mm. ﬁrst scenario bias function valid bounds. function takes bound current solution returns scalar second scenario propose choose valid bounds random. thus deterministic stochastic bound construction mechanism. algorithms cccp fall ﬁrst scenario. maximizes amount progress respect previous bound choosing bounds maximize progress algorithms tend rapidly converge nearby local minimum. instance iteration cccp bound latent svms obtained ﬁxing latent variables concave part objective function best values according previous solution making solution argminw attracted wt−. similarly e-step sets posterior distribution latent variables conditioned data according model previous iteration. thus next maximization step model updated match ﬁxed posterior distributions. explains reason algorithms observed sticky initialization. g-mm oﬀers ﬂexible bound construction scheme section show empirically picking bounds uniformly random valid bounds less sensitive initialization leads better results compared cccp also show using good bias functions improve performance learned models. iterations using progress coeﬃcient ensures making least progress bt−f bound true objective value note valid bounds touch corresponding requirement. small values allow gradual exploratory progress step large values greedily select bounds guarantee immediate progress. g-mm bounds true objective converges zero minimizers bounds moreover show mild conditions converges solution local extremum saddle point convergence properties g-mm depend structure global minimizer exist assumption bounded replace value lower bound. theorem family m-strongly convex functions s.t. limt→∞ i.e. g-mm converges solution. simplicity ease exposition primarily focus demonstrating g-mm latent variable models bound construction naturally corresponds imputing latent variables model. section derive g-mm algorithms models widely used machine learning namely k-means clustering latent structural belonging category latent variable models. worth mentioning g-mm fully capable handling general non-convex problems. bound construction obtain convex upper bound ﬁxing latent variables certain values instead minimizing variables. bounds constructed quadratic convex functions algorithm. also deﬁne diﬀerently obtain g-mm algorithm exhibits desired properties. instance common issue clustering cluster starvation. design bias function encourages balanced clusters selecting appropriately. uniformly conﬁgurations leading valid bounds. speciﬁcally start valid initial conﬁguration random walk graph whose nodes latent conﬁgurations deﬁning valid bounds. neighbors latent conﬁguration latent conﬁgurations obtained changing value latent variables clustering. perform random walk graph nodes latent conﬁgurations leading valid bounds edges connect latent conﬁgurations diﬀer single latent variable. multi-fold algorithm introduced training latent svms weakly supervised object localization deal stickiness issues training cccp. modiﬁes latent variables updated training. cinbis divide training folds updates latent variables fold using sample indices. also denote latent variable associated training example denote subsequence also denote ﬁxed latent variable values training examples iteration model trained latent variables ﬁxed conduct experiments four diﬀerent clustering datasets norm- cloud gmm-. norm- gmm- synthetic cloud real data. references details datasets. gmm- created gaussian mixture model data mixture components. component gaussian compare results three diﬀerent initializations forgy selects training examples uniformly random without replacement deﬁne initial cluster centers random partition assigns training samples cluster centers randomly k-means++ uses algorithm experiment algorithm times report mean standard deviation best objective value table shows results using k-means g-mm. note variance solutions found g-mm typically smaller k-means. moreover best average solutions found g-mm always better found table comparison g-mm k-means multiple clustering datasets. three diﬀerent initialization methods compared; forgy initializes cluster centers random examples random partition assigns data point random cluster center k-means++ implements algorithm mean standard deviation best objective values random trials reported. k-means g-mm exact initialization trial. g-mm consistently converges better solutions. although random partition seems initialization k-means datasets g-mm recovers fact gmm- datasets g-mm initialized random partition performs better initialized methods also variance best solutions g-mm smaller k-means. suggest g-mm optimization less sticky initialization k-means. figure visualizes result k-means g-mm dataset initialization. g-mm ﬁnds near perfect solution k-means many clusters merged incorrectly dead clusters points assigned them. update rule equation collapses dead clusters origin. figure shows eﬀect progress coeﬃcient quality solution found g-mm. diﬀerent colors correspond diﬀerent initialization schemes. solid line indicates average objective iterations shaded area covers standard deviation average dashed line indicates best solution trials. smaller progress coeﬃcients allow extensive exploration hence smaller variance quality solutions. hand progress coeﬃcient large g-mm sensitive initialization thus quality solutions multiple runs diverse. however despite greater diversity best solution worse progress coeﬃcient large. g-mm reduces k-means progress coeﬃcient figure visualization clustering solutions dataset identical initializations. random partition initialization scheme used. color-coded ground-truth clusters. solution k-means. solution g-mm. white crosses indicate location cluster centers. color codes match permutation. figure eﬀect progress coeﬃcient quality solutions found g-mm clustering datasets. quality measured objective function lower values better. average best variance trials shown plots diﬀerent initializations color coded. consider problem training ls-svm classiﬁer mammals dataset dataset contains images mammal categories image-level annotation. locations objects images provided therefore treated latent variables model. speciﬁcally image class label g-mm random table ls-svm results mammals dataset report mean standard deviation training objective test error folds. three strategies initializing latent object locations tried image center top-left corner random location. g-mm random uses random bounds g-mm bias uses bias function inspired multi-fold variants consistently signiﬁcantly outperform cccp baseline. experiment setup similar histogram oriented gradients image feature classiﬁcation loss equation report -fold cross-validation performance. three initialization strategies considered latent object locations image center top-left corner random locations. ﬁrst reasonable initialization since objects center dataset; second initialization strategy somewhat adversarial. stochastic well deterministic bound construction method. stochastic method iteration uniformly sample subset examples training update latent variables using latent variables kept previous iteration. increase size across iterations. deterministic method bias function described section inspired multi-fold idea shown reduce stickiness initialization especially high dimensions. number folds experiments. table shows results mammals dataset. variants g-mm consistently outperform cccp terms training objective test error. observed cccp rarely updates latent locations initializations. hand variants g-mm signiﬁcantly alter latent locations thereby avoiding local minima close initialization. figure visualizes top-left initialization. since objects rarely occur top-left corner mammals dataset good model expected signiﬁcantly update latent locations. averaged cross-validation folds latent variables updated g-mm training whereas measure cccp. consistent better training objectives test errors g-mm. provide additional experimental results mammals dataset. figure shows example training images ﬁnal imputed latent object locations three algorithms cccp g-mm random g-mm biased initialization top-left. figure latent location changes learning relative image coordinates cross-validation folds top-left initialization mammals dataset. left right cccp g-mm random g-mm biased cross represents training image; cross-validation folds color coded diﬀerently. averaged folds cccp alters latent locations leading performance. g-mm random g-mm biased alter average respectively perform much better. cases cccp fails update latent locations given initialization. g-mm variants however able update signiﬁcantly often localize objects training images correctly. achieved image-level object category annotations initialization. image regions. train model shared parts. parts used describe data image region. pre-trained hybrid convnet zhou extract features image regions. record neurons penultimate layer network reduce dimensionality features reconﬁgurable model instance ls-svm models. latent variables assignments parts image regions output structure multi-valued category label predictions. ls-svms known sensitive initialization parizi cope issue training generative version model ﬁrst using initialize discriminative model. generative models typically less sticky perform worse practice. validate hypothesis regarding stickiness ls-svms training using several initialization strategies. initializing training entails assignment parts image regions i.e. setting zi’s equation deﬁne ﬁrst bound. ﬁrst discover parts capture discriminative features training data. graph training image graph. nodes graph connected corresponding cells image grid next other. unary terms graph product scores feature vector extracted image region part ﬁlter plus corresponding region-to-part assignment score. pairwise terms graph implement figure example training images mammals dataset shown ﬁnal imputed latent object locations three algorithms cccp g-mm random g-mm biased initialization top-left. table performance ls-svm trained cccp g-mm mit-indoor dataset. report classiﬁcation accuracy training objective value columns correspond diﬀerent initialization schemes. random assigns random parts regions. controls coherency initial part assignments corresponds coherent case. g-mm random uses random bounds g-mm biased uses bias function equation experiments. coherent initializations lead better models general require discovering good initial parts. g-mm outperforms cccp especially random initialization. g-mm biased performs best. initial assignments. experiment using also experiment random initialization corresponds assigning zi’s randomly. simplest form initialization require discovering initial part ﬁlters. labeling image regions. denote labeling corresponding bound speciﬁes part assignments regions i-th image. also denote function measures coherence labeling fact potts energy function graph whose nodes graph respects -connected neighborhood system neighboring nodes diﬀerent labels energy increases biased bounds following bias function favors bounds correspond coherent labelings table compares performance models trained using cccp g-mm random biased bounds. g-mm random bounds repeat experiment times report average trials. also random initialization trials using diﬀerent random seeds report mean standard deviation results. g-mm better cccp initializations. also converges solution lower training objective value cccp. results show picking bounds uniformly random valid bounds slightly better committing cccp bound. remarkable boost performance reasonable prior bounds cccp attains table comparison number iterations takes g-mm converge scene recognition data clustering experiment diﬀerent initializations and/or datasets. numbers reported clustering experiment average standard deviation trials. g-mm bounds make fraction progress made bound construction step compared bound. therefore would expect g-mm take steps converges. report number iterations g-mm take converge table results g-mm depend value progress coeﬃcient match experiments paper; clustering experiment scene recognition experiment overhead bound construction step depends application. example scene recognition experiment optimizing bounds takes orders magnitude sampling clustering experiment however optimization step solved closed form whereas sampling bound involves performing random walk large graph take couple minutes run. introduced generalized majorization-minimization generic iterative bound optimization framework generalizes upon majorization-minimization observation enforces overly-restrictive touching constraint bound construction mechanism inﬂexible lead sensitivity initialization. adopting diﬀerent measure progress g-mm constraint relaxed allow freedom bound construction. speciﬁcally propose deterministic stochastic ways selecting bounds valid ones. generalized bound construction process tends less sensitive initialization enjoys ability directly incorporate rich application-speciﬁc priors constraints without modiﬁcations objective function. experiments several latent variable models g-mm algorithms shown signiﬁcantly outperform counterparts. future work includes applying g-mm wider range problems theoretical analysis convergence rate. also note that although g-mm conservative moving towards nearby local minima still requires making progress every step. another interesting research direction enable g-mm occasionally pick bounds make progress respect solution previous bound thereby making possible local minima still maintaining convergence guarantees method. appendix show mild conditions g-mm framework converges local extremum saddle point objective function. theorem roughly states that continuously diﬀerentiable family bounds smooth prove theorem contradiction showing construct implies s.t. contradicts assumption upper bound proof three steps constructing lower bound constructing upper bound ﬁnding point s.t. details presented below. figure show non-convex function gradient direction particular point also shown ﬁgure. figure shows function view together gradient direction figure shows value black white line. shows view; also marks point circle shows direction gradient vector length vector arbitrarily visualization purposes. shows value along tangent line dimensional function also shows gradient direction linear function equation lower bounds guarantees quadratic function distinct roots. constant aﬀected shifts quadratic function control drive constant arbitrarily close causing arbitrarily close zero thus fall within interval therefore satisﬁed. summarize assume lower bound along gradient direction along direction upper bounded suﬃciently large s.t. finally leading however violates assumption upper bound therefore ||∇f", "year": 2015}