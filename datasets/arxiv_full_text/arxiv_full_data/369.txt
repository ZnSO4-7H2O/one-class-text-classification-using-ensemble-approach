{"title": "Stochastic Video Generation with a Learned Prior", "tag": ["cs.CV", "cs.AI", "cs.LG", "stat.ML", "I.2.6; I.4"], "abstract": "Generating video frames that accurately predict future world states is challenging. Existing approaches either fail to capture the full distribution of outcomes, or yield blurry generations, or both. In this paper we introduce an unsupervised video generation model that learns a prior model of uncertainty in a given environment. Video frames are generated by drawing samples from this prior and combining them with a deterministic estimate of the future frame. The approach is simple and easily trained end-to-end on a variety of datasets. Sample generations are both varied and sharp, even many frames into the future, and compare favorably to those from existing approaches.", "text": "generating video frames accurately predict future world states challenging. existing approaches either fail capture full distribution outcomes yield blurry generations both. paper introduce unsupervised video generation model learns prior model uncertainty given environment. video frames generated drawing samples prior combining deterministic estimate future frame. approach simple easily trained end-to-end variety datasets. sample generations varied sharp even many frames future compare favorably existing approaches. learning generate future frames video sequence challenging research problem great relevance reinforcement learning planning robotics. although impressive generative models still images demonstrated karras techniques extend video sequences. main issue inherent uncertainty dynamics world. example bouncing ball hits ground unknown effects surface imperfections ball spin ensure future trajectory inherently random. consequently pixel-level frame predictions event degrade deterministic loss function used e.g. ball blurring accommodate multiple possible futures. recently loss functions impose distribution instead explored. approach adversarial losses training difﬁculties mode collapse often mean full distribution captured well. dependent stochastic latent variables. propose variants model ﬁxed prior latent variables another learned prior insight leverage learned-prior model majority ball’s trajectory deterministic model sufﬁces. point contact modeling uncertainty become important. learned prior interpreted predictive model uncertainty. trajectory prior predict uncertainty making frame estimates deterministic. however instant ball hits ground predict high variance event causing frame samples differ signiﬁcantly. train model introducing recurrent inference network estimate latent distribution time step. novel recurrent inference architecture facilitates easy end-to-end training svg-fp svg-lp. evaluate svg-fp svg-lp real world datasets stochastic variant moving mnist dataset. sample generations varied sharp even many frames future. several models proposed prediction within video learn deep feature representations appropriate high-level tasks object detection. wang gupta learn embedding patches taken object video tracks. similar principles learn features exhibit range complex invariances. lotter propose predictive coding model learns features effective recognition synthetic faces well predicting steering angles kitti benchmark. criterions related slow feature analysis proposed linearity representations equivariance ego-motion agrawal learn representation predicting transformations obtained ego-motion. range deep video generation models recently proposed. srivastava lstms trained prelearned dimensional image representations. ranzato adopt discrete vector quantization approach inspired text models. video pixel networks probabilistic approach generation whereby pixels generated time raster-scan order oord approach differs uses continuous representations throughout generates frame directly rather sequential process scale location. train adversarial loss. approaches handle uncertainty pixel-space introduces associated difﬁculties gans i.e. training instability mode collapse. contrast approach relies loss pixel-space reconstruction having gans adversarial terms. finn lstm framework model motion transformations groups pixels. works predict optical ﬂows ﬁelds used extrapolate motion beyond current frame e.g. although directly generate pixels model also computes local transformations implicit fashion. skip connections encoder decoder allow direct copying previous frame allowing rest model focus changes. however approach able handle stochastic information principled way. another group approaches factorize video static dynamic components learning predictive models latter. denton birodkar decomposes frames content pose representations using form adversarial loss give clean separation. lstm applied pose vectors generate future frames. villegas pixel level prediction using lstm separates motion content video sequences. reconstruction term used combines mean squared error gan-based loss. although approach also factorizes video deterministic stochastic components rather static/moving ones. important distinction since difﬁculty making accurate predictions stems much motion itself uncertainty motion. villegas propose hierarchical model ﬁrst generates high level structure video generates pixels conditioned high level structure. method able successfully generate complex scenes unlike unsupervised approach requires annotated pose information training time. chiappa focus actionconditional prediction video game environments known actions frame assumed. models produce accurate long-range predictions. contrast works utilize action information. several video prediction approaches proposed focus handling inherent uncertainty predicting future. mathieu demonstrate loss based gans produce sharper generations traditional -based losses. vondrick train generative adversarial network separates foreground background generation. vondrick torralba propose model based transforming pixels past approaches address uncertainty predicting future introducing latent variables prediction model. henaff disentangle deterministic stochastic components video encoding prediction errors deterministic model dimensional latent variable. approach broadly similar ours differs latent variables inferred training sampled test time. closest work babaeizadeh propose variational approach stochastic videos sampled. discuss relationship model depth section stochastic temporal models also explored outside domain video generation. bayer osendorfer introduce stochastic latent variables recurrent network order model music motion capture data. method utilizes recurrent inference network similar approach time-independent gaussian prior ﬁxed-prior model. several additional works train stochastic recurrent neural networks model speech handwriting natural language perform counterfactual inference anomaly detection work methods optimize bound data likelihood using approximate inference network. differ primarily parameterization approximate posterior choice prior model. start explaining model generates video frames detailing training procedure. model distinct components prediction model generates next frame based previous ones sequence latent variable prior distribution sampled time step prior distribution ﬁxed learned intuitively latent variable carries stochastic information next frame deterministic prediction model cannot capture. conditioning short series real frames model generate multiple frames future passing generated frames back input prediction model case svg-lp model prior also. model takes input frame i.e. target prediction model previous frames xt−. computes distribution sample prevent copying force close prior distribution using kl-divergence term. constrains information carry forcing capture information present previous frames. second term loss penalizes reconstruction error fig. shows inference procedure svg-fp svg-lp. generation procedure svg-fp svg-lp shown fig. fig. respectively. fig. drawback samples time step drawn randomly thus ignore temporal dependencies present frames. learned prior sophisticated approach learn prior varies across time function past frames including frame predicted speciﬁcally time prior network observes frames output parameters conditional gaussian distribution σψ). prior network trained jointly rest model maximizing test time frame time generated ﬁrst sampling prior. svg-fp draw svg-lp draw then frame generated conditioning short series real frames model begins pass generated frames back input prediction model case svg-lp model prior. sampling procedure svg-lp illustrated fig. architectures generic convolutional lstm frames input lstms feedforward convolutional network shared across three parts model. convolutional frame decoder maps output frame predictor’s recurrent network back pixel space. explain model adopt formalism variational auto-encoders. recurrent frame predictor speciﬁed ﬁxed-variance conditional gaussian distribution practice i.e. mean distribution rather sampling. note time step frame predictor receives input. dependencies previous stem recurrent nature model. since true distribution latent variables intractable rely time-dependent inference network approximates conditional gaussian distribution σφ). model trained optimizing variational lower bound given form likelihood term reduces penalty train model using re-parameterization trick estimating expectation single sample. appendix full derivation loss. hyper-parameter represents trade-off minimizing frame prediction error ﬁtting prior. smaller increases capacity inference network. small inference network learn simply copy target frame resulting prediction error training poor performance test time mismatch posterior prior large model under-utilize completely ignore latent variables reduce deterministic predictor. practice found easy tune particularly learned-prior variant discuss below. discussion hyperparameter context variational autoencoders higgins fixed prior simplest choice ﬁxed gaussian typically used variational auto encoder models. refer svg-fp model shown figure proposed video generation model. training ﬁxed prior training learned prior generation learned prior model. boxes show loss functions used training. text details. model related recent stochastic variational video prediction model babaeizadeh although variational framework broadly similar difference work latent variables estimated training sampled test time. inference network babaeizadeh encodes entire video sequence feed forward convolutional network estimate propose different models distribution. time-invariant version single sampled entire video sequence. time-variant model different sampled every time step samples coming distribution. contrast ﬁxed-prior learned-prior models utilize ﬂexible inference network outputs different posterior distribution every time step given test time ﬁxed-prior model time-variant model babaeizadeh sample ﬁxed differences manifest ways. first generated frames signiﬁcantly sharper models figure fig. second training model much easier. despite prior distribution used ﬁxed-prior model babaeizadeh time variant posterior distribution introduced model appears crucial successfully training model. indeed babaeizadeh report difﬁculties training model naively optimizing variational lower bound noting model simply ignores latent variables. instead propose scheduled three phase training procedure whereby ﬁrst deterministic element model trained latent variables introduced loss turned ﬁnal stage model trained full loss. contrast ﬁxed-prior learned-prior models easily trainable end-to-end single phase using uniﬁed loss function. evaluate svg-fp svg-lp model synthetic video dataset real ones bair robot show quantitative comparisons computing structural similarity peak signal-to-noise ratio scores ground truth generated video sequences. since neither metrics fully captures perceptual ﬁdelity generated sequences also make qualitative comparison samples figure qualitative comparison svg-lp purely deterministic baseline. deterministic model produces sharp predictions ones digits collides wall point prediction blurs account many possible futures. contrast samples svg-lp show digit bouncing different plausible directions. figure qualitative comparison svg-lp purely deterministic baseline. models conditioned ﬁrst frames test sequences. deterministic model produces plausible predictions future frames frequently mispredicts precise limb locations. contrast different samples svg-fp reﬂect variability persons pose future frames. picking sample best psnr svg-fp closely matches ground truth sequence. layer lstms cells layer. single layer lstms cells layer. network linear embedding layer fully connected output layer. output passed tanh nonlinearity going frame decoder. stochastic moving mnist frame encoder dcgan discriminator architecture output dimensionality similarly decoder uses dcgan generator architecture sigmoid output layer. output dimensionalities lstm networks |µφ| |µψ| bair datasets frame encoder uses architecture ﬁnal pooling layer output dimensionality decoder mirrored version encoder pooling layers replaced spatial up-sampling sigmoid output layer. output dimensionalities lstm networks |µφ| |µψ| |µφ| |µψ| bair. datasets skip connections encoder last ground truth frame decoder enabling model easily generate static background features. figure three examples svg-lp model accurately capturing distribution mnist digit trajectories following collision wall. right show trajectory digit prior collision. ground truth sequence angle speed immediately impact drawn random uniform distributions. sub-plots shows distribution time step. lower ground truth sequence trajectory deterministic collision corresponding delta-function. following collision distribution broadens approximate uniform distribution reshaped subsequent collisions. upper shows distribution estimated svg-lp model note model accurately captures correct distribution many time steps future despite complex shape. distribution computed drawing many samples model well averaging different digits sharing trajectory. examples show different trajectories correspondingly different impact times introduce stochastic moving mnist dataset consists sequences frames size containing mnist digits moving bouncing edge frame original moving mnist dataset digits move constant velocity bounce walls deterministic manner. contrast sm-mnist digits move constant velocity along trajectory wall point bounce random speed direction. dataset thus contains segments deterministic motion interspersed moments uncertainty i.e. time digit hits wall. figure learned prior svg-lp accurately predicts collision points sm-mnist. five hundred test video sequences different mnist test digits synchronized motion learned prior. mean plotted true points uncertainty video sequences i.e. digits hits wall marked vertical lines colored blue digit respectively. figure quantitative evaluation svg-fp svg-lp video generation quality sm-mnist models conditioned ﬁrst frames sm-mnist frames kth. vertical indicates frame number models trained predict generations indicate generalization ability. mean ssim test videos plotted conﬁdence interval shaded. figure quantitative comparison models babaeizadeh bair robot dataset. models conditioned ﬁrst frames generate subsequent frames. models trained predict frames future indicated vertical bar; generations indicate generalization ability. mean ssim psnr test videos plotted conﬁdence interval shaded. sm-mnist conditioning frames training model predict next frames sequence. compute ssim svg-fp svg-lp drawing samples model test sequence picking best score respect ground truth. fig. plots average ssim unseen test videos. svg-fp svg-lp outperform deterministic baseline svg-lp performs best overall particularly later time steps. fig. shows sample generations deterministic model svg-lp. generations deterministic model sharp several time steps model rapidly degrades digit collides wall since subsequent trajectory uncertain. hypothesize improvement svg-lp svg-fp model deterministic stochastic movement dataset. svg-fp frame predictor must determine latent variables given time step integrated prediction. svg-lp burden predicting points high uncertainty ofﬂoaded prior network. empirically measure fig. five hundred different video sequences constructed different test digits whose trajectories synchronized. plot shows mean i.e. variance distribution predicted learned prior time steps. superimposed blue time instants respective digits wall. learned prior able accurately predict collisions result signiﬁcant randomness trajectory. major challenge evaluating generative video models assessing accurately capture full distribution possible outcomes mainly high dimensionality space samples drawn. however synthetic nature single digit sm-mnist allows investigate principled way. point note figure qualitative comparison svg-lp model babaeizadeh models conditioned ﬁrst frames unseen test videos. svg-lp generates crisper images predicts plausible movement robot arm. sequence digit appearance remains constant randomness coming trajectory hits image boundary. thus sequence generated model establish digit trajectory taking pair frames time step cross-correlating digit used initial conditioning frames. maxima frame reveal location digit difference gives velocity vector time. taking expectation many samples model compute empirical distribution figure additional examples generations svg-lp showing crisp varied predictions. large segment background occluded conditioning frames preventing svg-lp directly copying background pixels generated frames. addition crisp robot movement svg-lp generates plausible background objects space occluded robot initial frames. figure long range generations svg-lp. robot remains crisp time steps object motion seen generated video frames. additional videos viewed https//sites.google.com/view/svglp/. trajectories produced model. perform operation validation ground truth sequences produce true distribution digit trajectories compare produced model. fig. shows svg-lp accurately capturing distribution mnist digit trajectories many time steps. digit trajectory deterministic collision. accurately reﬂected highly peaked distribution velocity vectors svg-lp time steps leading collision. following collision distribution broadens approximately uniform reshaped subsequent collisions. crucially svg-lp accurately captures complex behavior many time steps. temporally varying nature true trajectory distributions supports need learned prior action dataset consists real-world videos people performing actions fairly uniform backgrounds. human motion video sequences fairly regular however still uncertainty regarding precise locations person’s joints subsequent time steps. trained svg-fp svg-lp deterministic baseline video sequences conditioning frames training model predict next frames sequence. compute ssim svg-fp svg-lp drawing samples model test sequence picking best score respect ground truth. fig. plots average ssim unseen test videos. svg-fp svg-lp perform comparably dataset outperform deterministic baseline. fig. shows generations deterministic baseline svg-fp. deterministic model predicts plausible future frames inherent uncertainty precise limb locations often deviates ground truth. contrast different samples stochastic model reﬂect variability future frames indicating latent variables utilized even simple dataset. bair robot pushing dataset contains videos sawyer robotic pushing variety objects around table top. movements highly stochastic providing good test model. although dataset contain actions given discard training make frame predictions based solely video input. following babaeizadeh train svg-fp svg-lp deterministic baseline conditioning ﬁrst frames sequence predicting subsequent frames. compute ssim svg-fp svg-lp drawing samples model test sequence picking best score respect ground truth. fig. plots average ssim psnr scores held test sequences comparing state-of-the-art approach babaeizadeh evaluation consists conditioning frames generating subsequent ones i.e. longer train time demonstrating generalization capability svgfp svg-lp. svg-fp svg-lp outperform babaeizadeh terms ssim. svg-lp outperforms remaining models terms psnr ﬁrst steps babaeizadeh marginally better. qualitatively svg-fp svg-lp produce signiﬁcantly sharper generations babaeizadeh illustrated fig. psnr biased towards overly smooth results might explain slightly better psnr scores obtained babaeizadeh later time steps. svg-fp svg-lp produce crisp generations many time steps future. fig. shows sample generations time steps alongside ground truth video frames. also svg-lp forward time steps continue crisp motion robot introduced novel video prediction model combines deterministic prediction next frame stochastic latent variables drawn time-varying distribution learned training sequences. recurrent inference network estimates latent distribution time step allowing easy end-to-end training. evaluating model real-world sequences demonstrate high quality generations comparable better than existing approaches. synthetic data possible characterize distribution samples able match complex distributions futures. framework sufﬁciently general readily applied complex datasets given appropriate encoder decoder modules. bowman samuel vilnis luke vinyals oriol andrew jozefowicz rafal bengio samy. generating sentences continuous space. proceedings signll conference computational natural language learning chung junyoung kastner kyle dinh laurent goel kratarth courville aaron bengio yoshua. recurrent latent variable model sequential data. advances neural information processing systems fraccaro marco snderby sren kaae paquet ulrich winther ole. sequential neural models stochastic layers. advances neural information processing systems mathieu micha¨el couprie camille lecun yann. deep multi-scale video prediction beyond mean square error. proceedings international conference learning representations goodfellow pouget-abadie jean mirza mehdi bing warde-farley david ozair sherjil courville aaron bengio yoshua. generative adversarial nets. advances neural information processing systems higgins irina matthey loic arka burgess christopher glorot xavier botvinick matthew mohamed shakir lerchner alexander. early visual concept learning unsupervised deep learning. proceedings international conference learning representations radford alec metz luke chintala soumith. unsupervised representation learning deep convolutional generative adversarial networks. proceedings international conference learning representations ranzato marc’aurelio szlam arthur bruna joan mathieu micha¨el collobert ronan chopra sumit. video modeling baseline generative models natural videos. arxiv reed scott oord aaron kalchbrenner colmenarejo sergio gomez wang ziyu belov freitas nando. parallel multiscale autoregressive density estimation. proceedings international conference machine learning schuldt christian laptev ivan caputo barbara. recognizing human actions local approach. proceedings international conference pattern recognition s¨olch maximilian bayer justin ludersdorfer marvin smagt patrick. variational inference online anomaly detection high-dimensional time series. arxiv. srivastava mansimov salakhutdinov unsupervised learning video representations using lstms. proceedings international conference machine learning villegas yang hong decomposing motion content natural video sequence prediction. proceedings international conference learning representations villegas ruben yang jimei yuliang sohn sungryull xunyu honglak. learning generate long-term future hierarchical prediction. proceedings international conference machine learning tianfan jiajun bouman katherine freeman william visual dynamics probabilistic future frame synthesis cross convolutional networks. advances neural information processing systems recall frame predictor parameterized recurrent neural network. time step model takes input recurrence model also depends zt−. then simplify bound with recall inference network used svg-fp svg-lp parameterized recurrent neural network outputs different distribution every time step denote collection latent variables across time steps denote distribution independence across time", "year": 2018}