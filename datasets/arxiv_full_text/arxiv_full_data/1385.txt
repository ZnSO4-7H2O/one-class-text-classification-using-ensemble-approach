{"title": "Parametric Exponential Linear Unit for Deep Convolutional Neural  Networks", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Object recognition is an important task for improving the ability of visual systems to perform complex scene understanding. Recently, the Exponential Linear Unit (ELU) has been proposed as a key component for managing bias shift in Convolutional Neural Networks (CNNs), but defines a parameter that must be set by hand. In this paper, we propose learning a parameterization of ELU in order to learn the proper activation shape at each layer in the CNNs. Our results on the MNIST, CIFAR-10/100 and ImageNet datasets using the NiN, Overfeat, All-CNN and ResNet networks indicate that our proposed Parametric ELU (PELU) has better performances than the non-parametric ELU. We have observed as much as a 7.28% relative error improvement on ImageNet with the NiN network, with only 0.0003% parameter increase. Our visual examination of the non-linear behaviors adopted by Vgg using PELU shows that the network took advantage of the added flexibility by learning different activations at different layers.", "text": "abstract—object recognition important task improving ability visual systems perform complex scene understanding. recently exponential linear unit proposed component managing bias shift convolutional neural networks deﬁnes parameter must hand. paper propose learning parameterization order learn proper activation shape layer cnns. results mnist cifar-/ imagenet datasets using overfeat all-cnn resnet networks indicate proposed parametric better performances non-parametric elu. observed much relative error improvement imagenet network parameter increase. visual examination non-linear behaviors adopted using pelu shows network took advantage added ﬂexibility learning different activations different layers. recognizing objects using light visible spectrum essential ability performing complex scene understanding visual system. vision-based applications face veriﬁcation robotic grasping autonomous driving require fundamental skill object recognition carrying tasks. must ﬁrst identify different elements surrounding environment order create highlevel representation scene. since scene understanding performed analyzing spatial relations taxonomy representation overall performance visual system depends capability recognizing objects. integrating novel object recognition advances building fully-automated vision systems ﬁrst steps towards general visual perception. past years convolutional neural networks become leading approach computer vision series non-linear transformations cnns process high-dimensional input observations simple low-dimensional concepts. principle cnns features layer composed features layer below creates hierarchical organization increasingly abstract concepts. since levels organization often seen complex biological structures cnns particularly well-adapted capturing high-level abstractions real-world observations. activation function plays crucial role learning representative features. recently proposed exponential linear unit interesting property reducing bias shift deﬁned change neuron’s mean value weight update bias shift lead oscillations impede learning taken account clevert shown either centering neuron values batch normalization layer using activation functions negative values helps manage problem. deﬁned identity positive arguments negative ones elu’s negative values negative inputs make activation function well-suited candidate reducing bias shift. choosing proper parameterization however relatively cumbersome considering certain parameterizations suitable networks others. objective paper alleviate limitation learning parameterization activation function refer parametric contribute following ways deﬁne parameters controlling different aspects function show learn backpropagation. parameterization preserves differentiability acting positive negative parts function. computational complexity adds additional parameters number layers. perform experimental evaluation mnist cifar-/ imagenet tasks using resnet network network all-cnn overfeat networks. results indicates pelu better performances elu. rest paper organized follows. present related works section described proposed approach section iii. detail experimentations section discuss results section conclude paper section proposed pelu activation function related parametric approaches literature. parametric relu learns parameterization leaky relu activation deﬁned max{h min{h prelu learns leak parameter order proper positive slope negative inputs. prevents negative neurons dying i.e. neurons always equal zero caused null derivative blocks back-propagated error signal. based empirical evidence learning leak parameter rather setting predeﬁned value improves performance goal improving performance learning proper parameterization function. adaptive piecewise linear unit aims learning weighted parametrized hinge functions drawback number points function non-differentiable increase linearly differentiable activation functions usually give better parameter updates back-propagation activation functions non-differentiable points moreover although ﬂexibility either convex non-convex function rightmost linear function forced unit slope zero bias. inappropriate constraint could affect ability learn representative features. another activation function maxout outputs maximum afﬁne functions input neuron main drawback maxout multiplies amount weights learned layer. context cnns operator applied feature maps convolutional layers increased computational burden demanding deep network. unlike maxout pelu adds parameters number layers makes activation computationally demanding original function. s-shaped relu imitates webner-fechner stevens learning combination three linear functions although parametric function either convex non-convex srelu points non-differentiable. unlike srelu pelu fully differentiable since parameterization acts positive negative sides function. turn improves back-propagation weight bias updates. fig. effects parameters exponential linear unit activation function. original shown show effect second effect third effect fourth row. saturation point decreases increases function saturates faster decreases slope linear part increases increases. arguments although parameter positive value clevert proposed using fully differentiable function. values function non-differentiable directly learning parameter would break differentiability could impede back-propagation positive arguments negative arguments original recovered shown figure parameter controls different aspects activation. parameter changes slope linear function fig. auto-encoder results mnist task. compare pelu include bn-relu additional reference. compared pelu obtained lower test mean squared error. positive quadrant parameter affects scale exponential decay acts saturation point negative quadrant constraining parameters positive quadrant forces activation monotonic function reducing weight magnitude training always lowers neuron contribution. using parameterization network control nonlinear behavior throughout course training phase. increase slope decay lower saturation point however standard gradient update parameters would make function nondifferentiable impair back-propagation. instead relying projection operator restore differentiability update constrain parameterization always differentiability equaling derivatives sides zero solving gives solution. proposed parametric follows parameterization addition changing saturation point exponential decay respectively adjust slope linear function positive part ensure differentiability ﬁrst experiment performed unsupervised learning task learning feature representations unlabeled observations. unsupervised learning useful cases like deep learning data fusion evaluating proposed pelu activation trained deep auto-encoder unlabeled mnist images refer network daa-net. encoder four fully connected layers sizes decoder symmetrical encoder used dropout probability activation relu batch normalization layer activation. trained daa-net rmsprop learning rate smoothing constant batch size fig. residual network building block structure. left main basic block structure right transition block structure reducing input spatial dimensions increasing number ﬁlters. cifar experiments opted sub-sampling followed zero concatenation transition block imagenet experiments opted strided convolution followed batch normalization transition block. figure presents progression test mean squared error averaged tries daa-net mnist dataset. results show pelu outperformed relu convergence speed reconstruction error. pelu converged approximatively epoch converged epoch relu epoch .e−. basic blocks transition blocks reducing spatial dimensions input image increasing number ﬁlters. resnet cifar experiments transition block structure spatial sub-sampling zero concatenation resnet imagenet experiments transition block structure strided convolution followed batch normalization. train network used stochastic gradient descent weight decay momentum mini batch-size learning rate starts divided epoch epoch performed standard center crop horizontal data augmentation four pixels added side image random crop extracted randomly ﬂipped horizontally. color-normalized images used test phase. figure presents resnet test error medians tries cifar datasets. resnet obtained minimum median error rate cifar- bn+relu pelu bnprelu resnet obtained minimum median error rate cifar- bn+relu pelu bn+prelu. comparison pelu obtained relative improvement cifar- cifar- respectively. interesting note pelu adds additional parameters negligible increase total number parameters. observed pelu better convergence behavior elu. shown figure large test error rate increase second stage training phase cifar- cifar- datasets. although pelu also test error rate increase second stage increase high elu. observe small test error rate increase training phase pelu converges steady without test error rate increase. results show training resnet parameterization improve performance convergence behavior resnet activation. compared relu prelu obtained smaller minimum median error rate cifar- smaller average median error rate cifar-. shown table prelu obtained minimum median error rate compared cifar- average median error rate compared cifar-. although prelu obtained minimum median error rate pelu cifar signiﬁcantly higher cifar-. note main contribution showing performance improvement prelu additional reference. nonetheless observe pelu parameterization obtains higher relative improvements prelu parameterization relu. fig. resnet layers test error medians tries cifar- cifar- datasets. compare pelu also include bn-relu bn-prelu additional references. pelu better convergence lower recognition errors elu. building block structure network shown figure show basic block structure left figure transition block structure right figure module pelu relu prelu without network contains mainly fig. effect using pelu activations resnet layers cifar- cifar- datasets. show convergence behavior median test error tries. cases worsen performance pelu. note still second conv layer seen figure section show using pelu activation detrimental effect performance. figure presents inﬂuence using pelu resnet layers cifar- cifar- datasets. trained networks using framework section iv-b added activate. note cases second convolutional layer basic block results show large error rate increase cifar cifar- dataset pelu activation. minimum median test error increases cifar- cifar respectively pelu increases also observe relative error rate increase pelu smaller elu. indeed relative minimum test error rate increase cifar- cifar respectively pelu although shows pelu parameterization reduces detrimental effect using activation pelu preceded tested proposed pelu imagenet task using four different network architectures resnet network network all-cnn overfeat resnet building block structure shown figure order favor pelu detriment bn+relu performed minimal changes replacing activation function. used either pelu bn+relu activation module. network trained following chintala’s torch implementation imagenet-multigpu.torch training regimes figure presents top- error rate four networks imagenet validation dataset. cases networks using pelu outperformed networks using elu. obtained best result pelu corresponds relative improvement compared since additional parameters added network performance improvement indicates pelu’s parameterization acts different weights biases. adding additional weights throughout network would sufﬁcient increase representative ability enough observed performance improvement. since number weights cannot signiﬁcantly increase expressive power network results indicate networks beneﬁt pelu. fig. top- error rate progression resnet overfeat all-cnn imagenet validation set. resnet used training regime all-cnn overfeat used training regime pelu lowest error rates networks. regime shows greater performance pelu regime also observe error rates all-cnn overfeat pelu increase small amount starting epoch stay steady relu. results suggest training regimes larger learning rates decays help pelu obtain better performance improvement. eterization. note conﬁgurations reciprocal weight decay favors weight magnitude. instance favoring magnitude parameter conﬁguration favors high pelu slope a/b. contrary favoring magnitude parameter favors pelu slope order better understand difference conﬁguration performed experimental evaluation cifar- cifar- datasets using -layers resnet deﬁned section iv-b. shown table proposed parameterization obtained best accuracy. parameterization obtained minimum test error medians cifar- cifar- respectively obtained obtained obtained results also show parameterizations obtained signiﬁcantly lower error rate parameterizations convergence behavior figure parameterizations larger error increase second stage training phase parameterizations converge lower error rates. results concur observations section effect parameters. since weight decay pushes weight magnitude fig. pelu parameter progression layer trained cifar. present variation slope negative saturation network adopted different non-linear behaviors throughout training phase. converging relu believe increasing slope helps early training disentangle redundant neurons. since peak activations scatter inputs ones spreading values allow network declutter fig. experimenting pelu parameter conﬁguration resnet layers cifar- cifar- datasets. show convergence behavior median test error tries. proposed parameterization obtained best performance. towards zero function saturates slower decreases parameterizations using encourages function almost linear shape input values removes non-linear characteristic activation. contrary function saturates faster decreases parameterizations using helps activation keep non-linear nature explains observed performance gaps perform visual evaluation non-linear behaviors adopted network training cifar dataset figure shows progression slope negative saturation point pelu layer vgg. different behaviors. layers slope quickly increased large value decreased converged value near parameter quickly converged value near slope near negative saturation near indicates network learned activations shape relu. interesting result relu important effect promoting activation sparsity although clear understanding network increases slope decreases another interesting observation that apart relu layers negative saturations layers converged values instance parameter converges value near layer converges value near layer negative saturation zero indicates learned pelu activations outputs negative values negative arguments. network possibility learn activation functions zero negative saturation opted majority activations non-zero negative saturation. activation functions negative values previously analyzed context standard activation proposed helps manage bias shift results constitute additional experimental evidence characteristic important network. experiments pelu batch normalization activations. detrimental effect preceding pelu observed section iv-c resnet experiments cifar- cifar-. although detrimental effect also previously observed clevert coworkers unclear pelu increases error rate reduces error rate before relu. important difference relu positively scale invariant not. indeed relu max{ max{ expressed max{ min{ exp{x} min{ exp{kx} min{ exp{x} fact relu positively scale invariant part reason relu helps harms elu. given performs mean standard deviation scaling followed afﬁne transformation using positively scale invariant activation function essential properly reduce internal covariate shift manage bias shift could validate hypothesis experimenting positively scale invariant activation function observing whether helps not. leave idea future work. object recognition essential ability improving visual perception automated vision systems performing complex scene understanding. recent work exponential linear unit proposed element convolutional neural networks reducing bias shift inconvenience deﬁning parameter must hand. paper proposed parametric alleviates limitation learning parameterization activation function. results cifar-/ imagenet datasets using resnet all-cnn overfeat networks show cnns pelu better performance cnns elu. experiments shown network uses added ﬂexibility provided pelu learning different activation shapes different locations network. parameterizing activation functions softplus sigmoid tanh could worth investigating. hosang benenson doll´ar schiele what makes effective detection proposals? pami vol. d.-a. clevert unterthiner hochreiter fast accurate deep network learning exponential linear units arxiv preprint arxiv. sermanet eigen zhang mathieu fergus lecun overfeat integrated recognition localization detection using convolutional networks arxiv preprint arxiv. maas hannun rectiﬁer nonlinearities improve neural network acoustic models icml workshop deep learning audio speech language processing. citeseer agostinelli hoffman sadowski baldi learning activation functions improve deep neural networks arxiv preprint arxiv.", "year": 2016}