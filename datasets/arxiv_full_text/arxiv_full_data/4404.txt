{"title": "Decorrelation of Neutral Vector Variables: Theory and Applications", "tag": ["cs.CV", "stat.ML"], "abstract": "In this paper, we propose novel strategies for neutral vector variable decorrelation. Two fundamental invertible transformations, namely serial nonlinear transformation and parallel nonlinear transformation, are proposed to carry out the decorrelation. For a neutral vector variable, which is not multivariate Gaussian distributed, the conventional principal component analysis (PCA) cannot yield mutually independent scalar variables. With the two proposed transformations, a highly negatively correlated neutral vector can be transformed to a set of mutually independent scalar variables with the same degrees of freedom. We also evaluate the decorrelation performances for the vectors generated from a single Dirichlet distribution and a mixture of Dirichlet distributions. The mutual independence is verified with the distance correlation measurement. The advantages of the proposed decorrelation strategies are intensively studied and demonstrated with synthesized data and practical application evaluations.", "text": "paper propose novel strategies neutral vector variable decorrelation. fundamental invertible transformations namely serial nonlinear transformation parallel nonlinear transformation proposed carry decorrelation. neutral vector variable multivariate gaussian distributed conventional principal component analysis cannot yield mutually independent scalar variables. proposed transformations highly negatively correlated neutral vector transformed mutually independent scalar variables degrees freedom. also evaluate decorrelation performances vectors generated single dirichlet distribution mixture dirichlet distributions. mutual independence veriﬁed distance correlation measurement. advantages proposed decorrelation strategies intensively studied demonstrated synthesized data practical application evaluations. many pattern recognition machine learning areas gaussian distributions among probability distributions ubiquitously applied describe data distribution assumption data gaussian distributed however many applications distribution data asymmetric constrained example pixel values color grey image ratings assigned item collaborative ﬁltering epigenetic mark values epigenome-wide-association studies strictly bounded support speech enhancement spectrum coefﬁcients semi-bounded norms spatial fading correlation yeast gene expressions equal data convey directional property common property aforementioned data that data speciﬁc support range also non-bell distribution shape. apparently properties match natural properties gaussian distribution therefore data non-gaussian distributed demonstrated many recent studies explicitly utilizing non-gaussian characteristics signiﬁcantly improve performance practice typical type non-gaussian distributed data among others represents proportions. frequently used mixture modeling technique weighting factors denote proportions mixture component whole mixture model. text mining area dirichlet distribution used model topic relations i.e. proportions speciﬁc topic appears total documents analyzing color images normalized space often used pure color space discarding illuminance represents proportions channels whole color space. time series signal processing difference adjacent line spectral frequencies conveys proportion frequency distance half unit circle’s circumference. lsfs less sensitive quantization noise representations widely used speech coding also parameters multinomial distribution represent probabilities particular event happen trial sequence. novel online kernel learning algorithm called qkrls developed computationally efﬁcient used online regression classiﬁcation. data representing proportions denoted dimensional vector degrees freedom. element nonnegative elements constant connor introduced concept neutrality investigate particular type independence elements even though resulting neutral vector represents particular type independence substraction-normalization operation elements neutral vector mutually highly correlated rather negatively correlated. intuitively pattern recognition intelligent system lab. beijing university posts telecommunications beijing china. j.-h. department statistical science university college london london united kingdom. leijon school electrical engineering royal institute technology stockholm sweden. z.-h. department electronic systems aalborg university aalborg denmark. yang college computer science beijing university technology beijing china. corresponding author email mazhanyubupt.edu.cn correlated random vector variables principal component analysis popular technique used applications data decorrelation dimension reduction lossy data compression feature extraction also known karhunen-lo`eve transform transform coding considered orthogonal transformation correlated variables uncorrelated scalar variables named principal components. transformation linear invertible. optimal decorrelation strategy multivariate-gaussian distributed data data multivariate-gaussian distribution resulting transformed scalar variables mutually uncorrelated also mutually independent. data sources guarantee scalar variables mutually uncorrelated. independent component analysis computational method applied separate multivariate vector variable additive mutually independent scalar variables assumption source signals independent source signals non-gaussian distributed attempts decorrelate multivariate vector variable mutually independent non-gaussian scalar variables. applied several ﬁelds face recognition blind source separation wireless communications neutral vector bounded support negatively correlated thus cannot follow multivariate-gaussian distribution. case applying neutral vector yield mutually uncorrelated mutually independent scalar variables. although yield mutually independent non-gaussian scalar variables cannot preserve bounded support property. considering neutrality highly correlated variables neutral vector decorrelated independent variables nonlinear transformation. moreover procedure depend eigenvalue decomposition covariance matrix. paper propose fundamental transformation strategies namely serial nonlinear transformation parallel nonlinear transformation decorrelate neutral vectors. invertible nonlinear transformations take advantages completely neutrality. prove mentioned nonlinear transformations decorrelate neutral vector variable mutually independent variables. particularly neutral vector variable dirichlet distributed transformed variables follows beta distribution actually special case dirichlet distribution parameters. although nonlinear kernel functions introduced carry kernel kernel vector variable decorrelation implemented nonlinear manner proposed nonlinear transformation strategies different ones. kernel input vectors ﬁrstly mapped feature space kernel function standard applied conduct decorrelation similar approaches applied kernel ica. therefore kernel kernel contain stages nonlinear kernel mapping linear decorrelation contrast this proposed nonlinear transformation strategies require kernel mapping. one-stage nonlinear operation decorrelation implementation. neutral random vector decorrelation strategies based observed vector require statistical information whole observation set. words decorrelation strategies model independent. therefore proposed decorrelation strategies reduce computational complexity compared requires eigenvalue decomposition covariance matrix. even higher computational costs pca. decorrelation vector variable important helpful many applications hence proposed decorrelation strategies novel useful data neutrality. rest paper organized follows review concept neutrality sec. proposed transformation strategies introduced sec. proof mutually independence also provided. sec. take dirichlet distribution example neutral vectors. comprehensive evaluations proposed strategies synthesized real data presented sec. draw conclusions sec. idea neutrality introduced connor describing constrained variables property mentioned above. originally developed biological applications. according deﬁnition neutral vector conveys particular type independence among elements even though element variables mutually negatively correlated. complete neutral vector variable properties list used paper here usually dimensions completely neutral vector equally treated. words positions dimensions affect properties vector. order explicitly convey fact make following deﬁnition ﬁeld statistical analysis typical variable mentioned properties dirichlet variable. bayesian analysis mixture models weighting factors mixture components usually modelled dirichlet distribution. recently dirichlet process applied nonparametric bayesian analysis. represent dirichlet process so-called stick-breaking process independence among different generating steps expressed explicitly neutral vector inﬁnite dimensionality. dirichlet process cornerstone non-parametric bayesian analysis applied variety practical signal feature analysis problems. thus concept neutral vectors useful many signal processing pattern recognition practical applications. signal processing applications transformations linear linear according nonlinear kernel functions. even though could apply directly neutral random vector variable linear transformation could decorrelate data cannot guarantee independence data gaussian. furthermore exploit neutrality case optimal decorrelating neutral vectors. considering exchangeably complete neutrality propose nonlinear invertible transformations namely serial nonlinear transformation parallel nonlinear transformation proposed nonlinear transformations decorrelate vector variable mutually independent variables. contrast transformations require statistical information observed vector set. thus avoids eigenvalue analysis therefore computational complexity reduced. exchangeable completely neutral vector variable degrees freedom process vector variable strategy described algorithm dimensional vector variable transformed vector variables. scalar variables mutually independent. according property shows variables mutually independent. since invertible degrees freedom contains amount information example illustrated fig. algorithm needs rounds iterations ﬁnalize transformation. order facilitate operation also carry nonlinear transformation parallel referred parallel nonlinear transformation scheme introduced alg. iteration dimension processed vector reduced half. finally still vector variable mutually independent element variables. example shown fig. proofs independence follows. independence subvectors algorithm iteration yields subvector based taking arbitrary subvectors selecting arbitrary elements subvector respectively following transformation fast parallel nonlinear transformation according alg. implementation needs check length iteration even odd. fact number elements always equal power inspired fast fourier transform design fast algorithm facilitate practical computation zero-padding. zero-padding technique usually employed make length vector equal power adding zeros vector total number elements equals next higher power vector expanded zero-padding next higher power iteration transformation vector length reduces half length vector reduces two. algorithm skips check parity therefore practical computational time reduced. convenient implement practice. worthy note fpnt algorithm similar computational complexity chart shown alg. fpnt algorithm introduced algorithm nonlinear transformations assign explicit distribution neutral vector variable. indeed transformation require know speciﬁc distribution vector variable assumption vector variable exchangeably completely neutral. section take dirichlet variable intuitive example. showed dirichlet distribution characterized neutrality vector drawn dirichlet distribution completely neutral. moreover permutation vector also completely neutral vector note that completely neutral vector permutation property beta density function exactly dirichlet density function parameters thus dirichlet variable neutral vector. furthermore dirichlet variable aggregation property considering xli− neutral variable normalized version remaining variables dirichlet distributed parameters. equivalent beta distribution. thus obtained coefﬁcient xi−/ follows beta distribution based reasoning show uli− also beta distributed. thus dirichlet variable decorrelated vector degrees freedom. complete neutrality element variables transformed vector mutually independent element variable beta distributed. importance independence arises many applications. proposed nonlinear transformation methods decorrelate neutral vector variable mutually independent scalar variables. order illustrate decorrelation performance distance correlation measures statistical dependence random variables applied evaluate mutual independence scalar variables transformation. unlike commonly used pearson correlation coefﬁcient zero random variables statistically mutually independent given paired samples pairwise euclidean distances calculated null hypothesis case variables involved independent corresponding p-value smaller null-hypothesis rejected variables independent hence p-value greater indicates independence. choose signiﬁcance level remaining parts paper. section ﬁrstly compare pnt/snt evaluation decortication performance. next demonstrate decorrelation performance synthesized real data. afterwards apply proposed strategy real-life applications improve corresponding practical performance. null hypothesis related dimensions independent first p-values generated data. second p-values decorrelated data pnt. third p-values decorrelated data pca. fourth p-values decorrelated data ica. p-values smaller marked computational complexity practical applications computational complexity decorrelation usually concern. analyze computational complexities respectively compare conventionally used strategies. described algorithm iteration yields element target vector hence decorrelating neutral vector variable independent scalar variables iterations required. iteration summation division operated purpose normalization number elements intermediate vector therefore treat summation ﬂoating-point operation division eight times that computational complexity applying algorithm decorrelate neutral vector parallel manner ⌈log iterations required. within iteration summations divisions even summations divisions needed. therefore consideration ﬂoating-point operation above computational complexity since ﬁrst iteration reduce half consequent iteration. analysis conclude algorithm efﬁcient algorithm preferable practice although algorithms nonlinearly transform neutral vector mutually independent scalars. operation includes parts eigenvalue analysis covariance matrix decorrelation vector. many approaches exist eigenvalue analysis. best knowledge fastest method so-far method proposed computational cost covariance matrix. decorrelation multiplying source vector eigenvector matrix computational cost around therefore computational cost average hence proposed sntpnt-based decorrelation methods efﬁcient pca-based method. although robust source separation performance achieved drawback algorithms carrying high computational complexity typical algorithms requires centering whitening dimension reduction preprocessing steps facilitate calculation. unlike pnt/snt converges fast convergence also depends number iterations. hence analytically tractable solution exist. introduced computational cost iterations decorrelation performance generated different amounts samples single dirichlet distribution parameters chosen proposed method shown efﬁcient method applied decorrelate generated samples. different amounts data possible pairs transformed variables evaluated corresponding p-values listed tab. respectively. make extensive comparison also applied pca-based decorrelation method ica-based decorrelation method respectively generated data summarized decorrelation performance tab. i-tab. amount samples small generated data cannot reveal neutrality completely p-value larger indicates variables independent other conﬂict deﬁnition neutrality.) methods decorrelate semineutral vector variable mutually independent scalar variables. amount sample increases neutrality data becomes clear observed algorithms yield mutually independent variables cases contrast algorithm lead partially mutual independence. summary proposed strategy nonlinearly transform highly negatively correlated neutral vector variable mutually independent scalar variables. compared show better decorrelation performance data neutral property wide range amounts samples. order remove effect randomness rounds simulations mean values reported tab. round simulation includes data generation decorrelation decorrelation decorrelation calculation. discussions compared computational complexities sec. v-a. proposed methods less computational complexity compared ica. methods least computational complexity. largest computational complexity meantime analytically tractable solution needs many iterations converge. evaluating methods decorrelation performance used represent proposed nonlinear transformation strategies. observed good decorrelatoin performance neutral vector variables wide range data amounts. perform well neutral vector variables increases. summary neutral vector variable performs better terms decorrelation computational complexity. comparing analytically tractable solution. therefore algorithms typically resort iterative procedures either difﬁculties high computational load. hence compare following experiments. mixture dirichlet distributions real applications data obtained usually multimodally distributed. neutral vector variable however uni-modally distributed deﬁnition. hence sufﬁcient interest study decorrelation performance proposed method data sampled mixture dirichlet distributions. section generated data mixture dirichlet distributions evaluate decorrelation performance. chosen model contains mixture components mixture coefﬁcients component parameters table shows decorrelation performance whole data set. upper illustrates decorrelation performance data samples. mentioned previous section small amount data single component cannot completely reveal neutrality. hence data generated mixture dirichlet distributions still mutual independence pairs dimensions p-value larger indicates mutual independence.) case applying algorithm whole data yields partially mutual independence data cluster algorithm works well expected ii). large amount data data generated mixture component strong neutral property whole data highly correlated neutral case algorithm work proposed decorrelation strategy based assumption neutrality work data neutral. however partition data clusters cluster contains data vectors neutral algorithm perfectly leads mutual independence possible pairs decorrelated dimensions ii). coding gain/removal memory advantage advantage proposed nonlinear transformation strategy occurs high rate quantization vectors. application source coding source vectors usually highly correlated. hence natural decorrelate vector mutually independent scalars vector quantization replaced scalar quantization without losing memory advantage quantiﬁed so-called coding gain measurement different quantization methods coding gain measured ratio quantization distortions given number bits quantization. dimensionality denotes expectation operation. equation denote distortion incurred quantization domain. assuming dirichlet distributed known parameters apply algorithm transform beta distributed high rate theory entropy constrained quantization derive that bits probability density function -optimized allocation strategy distortion domain incurred quantizing equation ratio indicates less distortion achieved proposed nonlinear transformation. larger ratio beneﬁt obtain transformation. order evaluate coding gain extensively evaluated coding gain different different dimensionalities. give example inverse nonlinear transformation elements listed tab. iii. expectation term denominator calculated closed-form expression fact beta distributed parameters calculated original dirichlet parameters details). coding gains plotted fig. randomly generated elements total rounds simulations conducted observed proposed nonlinear transformation yield coding gain greater different dimensions. memory advantage removed. fig. coding gains different shown plot. central mark median blue star mark mean edges percentiles. outliers marked crosses. mean values listed bottom. discussion synthesized data experiments demonstrated superior performance proposed nonlinear transformation strategy neutral data. data generated mixture dirichlet distributions multimodally distributed neutral. case partition data different clusters. assuming data assigned cluster generated single dirichlet distribution proposed method applied data results promising decorrelation performance. decorrelation highly negatively correlated vector plays important role many applications. next section apply idea real data applications. decorrelation highly correlated vector variable mutually independent variables leads many advantages real applications section evaluate decorrelation performance proposed strategy real life data deﬁnition neutral vector assume neutral-like data neutral property apply algorithm nonlinearly transform them. performance improvement practical applications also presented. vector quantization line spectral frequency parameters quantization parameters linear predictive coding model essential part speech transmission parameters usually -dimensional narrow band speech -dimensional wide band speech. hence vector quantization required. generally speaking memory shape space-ﬁlling advantages scalar quantization however impractical design full vector quantizer size codebook increases exponentially dimension data leads high storage complexity; effort training codebook searching index codebook also exponentially increased data’s dimension computationally costly. especially dimension high e.g. feasible. practical implementation frequently used method decorrelate parameters mutually independent scalars memory advantage removed then employed replace design pdf-optimized gaussian distribution corresponding gaussian mixture model intensively applied model distribution parameters however since parameters interval strictly ordered gaussian distributed. purpose efﬁcient modeling parameters converted so-called ∆lsf parameters ∆lsf parameters nonnegative summation equals ∆lsf parameters deﬁnition suppose follow dirichlet distributions apply dirichlet mixture model describe underlying distribution data. data generated dirichlet distribution neutral property proposed nonlinear strategy applied decorrelate ∆lsf parameters. practical carried based neutrality. ∆lsf parameters -dimensional wide band speech data. space consuming list mutual independence p-value table. thus calculated independence coefﬁcient deﬁned proportion number mutually independent pairs number possible pairs measure decorrelation performance. higher proportion better decorrelation performance described sec. ﬁrstly applied algorithm ∆lsf parameters. shown fig. original data small means decorrelation performance signiﬁcant. fact ∆lsf parameters multimodally distributed. applied algorithm partition ∆lsf hereby name vector contains nonnegative elements unit/constant l-norm neutral-like data. strictly speaking summation ∆lsf parameters equals scaled summation equals scaled ∆lsf parameters show results wide band data here. similar performance also obtained narrow band data. matrix number possible pair largest ratio means possible pairs mutually independent. parameters different clusters. assumption data cluster dirichlet distributed applied algorithm data cluster respectively. cluster also plotted fig. clearly shown pairs mutually independent. hence mutual correlation cluster signiﬁcantly removed pnt. motivated coding gain advantage sec. designed implemented dmm-based based neutral properties. parameters partitioned clusters contains mixture components introduced procedure algorithm applied realize decorrelation cluster mutually independent scalar elements obtained. memory advantage removed explicitly using neutrality carried pdf-optimized parameters. beneﬁts fold saving storage training searching costs. average rate bits spent indexing mixture component bits spent hence assuming components identical other codebook codewords required mixture component. case cluster placed dimension based differential entropy. average needed component. usually number hence required number codewords signiﬁcantly reduced storage cost saved. well-known lloyd algorithm linde-buzo-gray algorithm usually utilized obtaining codebook. case training carried -dimensional space. meanwhile training executed one-dimensional space obviously training codebook dimensional space computationally costly one-dimensional space therefore training cost saved. reasoning searching cost also signiﬁcantly reduced replacing saving rates. ultimate goal pdf-optimized spend less bits possible satisfying quantization distortion requirement. practical parameters based modeling proposed nonlinear transformation strategy introduced transparent coding criterion evaluated spectral distortion obtained dmm-based compared state-of-the-art gmm-based gmm-based partitioned parameters clusters algorithm gmm. next parameters decorrelated pca. finally pdf-optimized gmmbased carried well. fig. shows designs dmm-based gmm-based performance comparisons summarized tab. clearly demonstrated dmm-based improves performance bits/vector. fact proposed nonlinear transformation strategy removes memory advantage makes implementation practical feasible. details found signal classiﬁcation persons suffer neuromuscular diseases brain-computer interface connects computers recording analyzing brain signals. non-invasively acquired signal electroencephalogram signal studied applied design system signal obtained channel various types features extracted signal purpose classiﬁcation. marginal discrete wavelet transform vector among others typical feature widely adopted elements vector reveal features related transient nature signal. marginalization operation yields mdwt vector makes vector insensitive time alignment data used paper competition signal trial recording subject perform imagined movements either left small ﬁnger tongue. data contains trials training trials test. trials training test sets evenly distributed labeled respectively. trial channel data length samples provided. mdwt vector contains nonnegative elements unit l-norm. hence applied nonlinear transformation method decorrelate mdwt vector purposed classiﬁcation accuracy improvement. previous work successfully applied proposed method signal classiﬁcation. so-called multivariate beta distribution -based classiﬁer introduced based feature selection strategy transformed feature domain applied classify signals. paper make thorough study show obtained gain classiﬁcation accuracy indeed application method mdwt vectors. channels closely relevant classiﬁcation task. conducting classiﬁcation task importance select relevant channels classiﬁcation accuracy improved. fisher ratio generalization error estimation applied select channels. channels ranked according gees respectively. classiﬁcation stage exploit mdwt vectors channels. feature selection important problem signal classiﬁcation selected channel dimension mdwt vector applied algorithm decorrelate mdwt vectors training set. -dimensional vectors contains mutually independent elements obtained. sorted dimensions according variances descending order. mdwt vectors test also decorrelated pnt. dimension reordering carried based variance order training set. according reordered dimensions selected relevant dimensions classiﬁcation. binary classiﬁcation task support vector machine classic widely applied classiﬁer evaluated introduced feature selection strategy comparing classiﬁcation accuracies. channel selection method radial basis function kernel trained benchmark respectively. libsvm toolbox adjusted parameters rbf-svm cross validation training accuracy highest. mdwt vectors training used parameter adjustment. make fair comparisons also applied decorrelate mdwt vectors. mdwt vectors test transformed eigenvectors obtained training set. relevant dimensions also selected according variances. classiﬁcation results obtained channels channel relevant features selected. total obtained -dimensional feature vector train rbf-svm. observed rbf-svm+pnt yields highest recognition accuracies case case respectively. figure illustrates classiﬁcation results channels highest classiﬁcation rates obtained indicates feature selection variance indeed beneﬁts classiﬁcation. rbf-svm+pnt yields highest recognition accuracy case case respectively. discussion parameters model mdwt parameters signal contain nonnegative elements unit/constant l-norm respectively. although difﬁcult prove neutrality neutral-like data still exploit neutrality apply pnt-based nonlinear transformation strategy purpose decorrelation improve practical performance. compared pca-based linear transformation strategy pnt-based nonlinear transformation showed advantages applications. nonlinear transformations neutral vector variable proposed studied paper. explicitly utilizing neutrality neutral vector variables introduced serial nonlinear transformation parallel nonlinear transformation methods decorrelate neutral vector variable mutually independent element variables. mutual independence theoretically proved. computational costs proposed decorrelation methods analyzed compared pca-based ica-based approaches. shown computational costs proposed methods smallest. typical case vector variable following dirichlet distribution completely neutral vector. transformed element variables beta distributed. distance correlation metric decorrelation performance proposed nonlinear transformation demonstrated superior synthesized real life data. moreover applied proposed nonlinear transformation applications i.e. quantization line spectral frequency parameters speech linear predictive model signal classiﬁcation. extensive experimental results showed that carrying decorrelation feature selection neutral-like data proposed parallel nonlinear transformation -based nonlinear transformation achieve better practical performance preferable conventionally applied pca-based linear transformation. non-gaussian statistical models applications ph.d. dissertation royal institute technology jung park capacity error probability analysis diversity reception schemes generalized-k fading channels bishop pattern recognition machine learning. springer mclachlan peel finite mixture models. wiley blei probabilstic model text images ph.d. dissertation university california berkeley blei lafferty correlated topic models advances neural information processing systems blei probabilistic topic models communications vol. wyszecki stiles color science concepts methods quantitative data formulae. york wiley bouguila ziou high-dimensional unsupervised selection estimation ﬁnite generalized dirichlet mixture model based minimum forbes evans hastings peacock statistical distributions. wiley chen buntine ding differential topic models ieee transactions pattern analysis machine intelligence vol. chen zeng transforms tightly bounded ieee sig. proc. lett. vol. torun akansu efﬁcient method derive explicit kernel ﬁrst-order autoregressive discrete process ieee transactions stone independent component analysis tutorial introduction. press nguyen zheng binary independent component analysis mixtures ieee transactions signal processing vol. bach jordan kernel independent component analysis journal machine learning research vol. xiao zhao kernel reconstruction sparse representation ieee transactions neural networks hankin generalization dirichlet distribution journal statistical software vol. blei jordan variational inference dirichlet process mixtures bayesian analysis vol. brigham fast fourier transfor. prentice-hall frigyik kapila gupta introduction dirichlet distribution related processes department electrical engineering sz´ekely rizzo brownian distance covariance annals applied statistics vol. pearson notes regression inheritance case parents proceedings royal society london wilcox introduction robust estimation hypothesis testing. academic press sz´ekely rizzo uniqueness distance covariance statistics probability letters vol. minka lightspeed matlab toolboxes. available http//research.microsoft.com/en-us/um/people/minka/software/lightspeed/ qiao fast singular value algorithm hankel matrices. boston american mathematical society shwartz zibulevsky schechner using kernel entropy estimation nlogn complexity. springer berlin heidelberg kleijn basis source coding lecture notes. gersho gray vector quantization signal compression. kluwer academic publishers gardner theoretical analysis high-rate vector quantization parameters ieee transactions speech audio leijon modeling speech line spectral frequencies dirichlet mixture models proceedings interspeech jung gmm-based klt-domain switched-split vector quantization coding ieee signal processing letters vol. chatterjee sreenivas predicting performance bound coding ieee signal processing letters vol. ramirez intra-predictive switched split vector quantization speech spectra ieee signal processing letters vol. lloyd least squares quantization ieee transactions information theory vol. mar. ortega quantizer design energy-based source localization sensor networks ieee transactions signal processing vol. darpa-timit acoustic-phonetic continuous speech corpus nist speech disc cortes vapnik support-vector networks machine learning vol. huang suykens support vector machine classiﬁer pinball loss ieee transactions pattern analysis machine", "year": 2017}