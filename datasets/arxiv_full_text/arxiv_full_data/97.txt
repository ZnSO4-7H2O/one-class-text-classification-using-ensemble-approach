{"title": "Learning Phrase Representations using RNN Encoder-Decoder for  Statistical Machine Translation", "tag": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.", "text": "paper propose novel neural network model called encoder– decoder consists recurrent neural networks encodes sequence symbols ﬁxedlength vector representation decodes representation another sequence symbols. encoder decoder proposed model jointly trained maximize conditional probability target sequence given source sequence. performance statistical machine translation system empirically found improve using conditional probabilities phrase pairs computed encoder–decoder additional feature existing log-linear model. qualitatively show proposed model learns semantically syntactically meaningful representation linguistic phrases. deep neural networks shown great success various applications objection recognition speech recognition furthermore many recent works showed neural networks successfully used number tasks natural language processing include limited language modeling paraphrase detection word embedding extraction ﬁeld statistical machine translation deep neural networks begun show promising results. summarizes successful usage feedforward neural networks framework phrase-based system. along line research using neural networks paper focuses novel neural network architecture used part conventional phrase-based system. proposed neural network architecture refer encoder–decoder consists recurrent neural networks encoder decoder pair. encoder maps variable-length source sequence ﬁxed-length vector decoder maps vector representation back variable-length target sequence. networks trained jointly maximize conditional probability target sequence given source sequence. additionally propose rather sophisticated hidden unit order improve memory capacity ease training. proposed encoder–decoder novel hidden unit empirically evaluated task translating english french. train model learn translation probability english phrase corresponding french phrase. model used part standard phrase-based system scoring phrase pair phrase table. empirical evaluation reveals approach scoring phrase pairs encoder–decoder improves translation performance. qualitatively analyze trained encoder–decoder comparing phrase scores given existing translation model. qualitative analysis shows encoder–decoder better capturing linguistic regularities phrase table indirectly explaining quantitative improvements overall translation performance. analysis model reveals encoder– decoder learns continuous space representation phrase preserves semantic syntactic structure phrase. encoder reads symbol input sequence sequentially. reads symbol hidden state changes according reading sequence hidden state summary whole input sequence. decoder proposed model another trained generate output sequence predicting next symbol given hidden state however unlike described sec. also conditioned summary input sequence. hence hidden state decoder time computed preliminary recurrent neural networks recurrent neural network neural network consists hidden state optional output operates variablelength sequence time step hidden state updated learn probability distribution sequence trained predict next symbol sequence. case output timestep conditional distribution example multinomial distribution output using softmax activation function encoder–decoder paper propose novel neural network architecture learns encode variable-length sequence ﬁxed-length vector representation decode given ﬁxed-length vector representation back variable-length sequence. probabilistic perspective model general method learn conditional distribution variable-length sequence conditioned another variable-length sequence e.g. encoder–decoder trained model used ways. model generate target sequence given input sequence. hand model used score given pair input output sequences score simply probability eqs. hidden unit adaptively remembers addition novel model architecture also propose type hidden unit motivated lstm unit much simpler compute implement. fig. shows graphical depiction proposed hidden unit. lstm unit shown impressive results several applications speech recognition memory cell four gating units adaptively control information inside unit compared gating units proposed hidden unit. details lstm networks e.g. figure illustration proposed hidden activation function. update gate selects whether hidden state updated hidden state reset gate decides whether previous hidden state ignored. eqs. detailed equations hand update gate controls much information previous hidden state carry current hidden state. acts similarly memory cell lstm network helps remember longterm information. furthermore considered adaptive variant leaky-integration unit hidden unit separate reset update gates hidden unit learn capture dependencies different time scales. units learn capture short-term dependencies tend reset gates frequently active capture longer-term dependencies update gates mostly active. preliminary experiments found crucial unit gating units. able meaningful result oft-used tanh unit without gating. ﬁrst term right hand side called translation model latter language model practice however systems model loglinear model additional features correpairs original corpus. ﬁxed capacity encoder–decoder ensure capacity model focused toward learning linguistic regularities i.e. distinguishing plausible implausible translations learning manifold plausible translations. encoder–decoder trained score phrase pair existing phrase table. allows scores enter existing tuning algorithm minimal additional overhead computation. schwenk pointed possible completely replace existing phrase table proposed encoder– decoder. case given source phrase encoder–decoder need generate list target phrases. requires however expensive sampling procedure performed repeatedly. paper thus consider rescoring phrase pairs phrase table. schwenk proposed similar approach scoring phrase pairs. instead rnn-based neural network used feedforward neural network ﬁxed-size inputs ﬁxed-size outputs used speciﬁcally scoring phrases system maximum phrase length often chosen small. however length phrases increases apply neural networks variable-length sequence data important neural network handle variable-length input output. proposed encoder–decoder well-suited applications. devlin proposed feedforward neural network model translation model however predicting word target phrase time. reported impressive improvement approach still requires maximum length input phrase ﬁxed priori. framework introduced translation model factorized translation probabilities matching phrases source target sentences. probabilities considered additional features log-linear model weighted accordingly maximize bleu score. since neural language model proposed neural networks used widely systems. many cases neural networks used rescore translation hypotheses recently however interest training neural networks score translated sentence using representation source sentence additional input. e.g. train encoder–decoder ignore frequencies phrase pair original corpora. measure taken order reduce computational expense randomly selecting phrase pairs large phrase table according normalized frequencies ensure encoder– decoder simply learn rank phrase pairs according numbers occurrences. underlying reason choice existing translation probability phrase table already reﬂects frequencies phrase although exactly neural network train authors proposed learn bilingual embedding words/phrases. learned embedding compute distance pair phrases used additional score phrase pair system. feedforward neural network trained learn mapping bag-of-words representation input phrase output phrase. closely related proposed encoder–decoder model proposed except input representation phrase bag-of-words. similar approach using bag-of-words representations proposed well. earlier similar encoder–decoder model using recursive neural networks proposed model restricted monolingual setting i.e. model reconstructs input sentence. recently anencoder–decoder model using proposed decoder conditioned representation either source sentence source context. important difference proposed encoder–decoder approaches order words source target phrases taken account. encoder–decoder naturally distinguishes sequences words different order whereas aforementioned approaches effectively ignore order information. closest approach related proposed encoder–decoder recurrent continuous translation model proposed paper proposed similar model consists encoder decoder. difference model used convolutional n-gram model encoder hybrid inverse recurrent neural network decoder. they however evaluated model rescoring n-best list proposed conventional system computing perplexity gold standard translations. large amounts resources available build english/french system framework wmt’ translation task. bilingual corpora include europarl news commentary crawled corpora words respectively. last corpora quite noisy. train french language model words crawled newspaper material available addition target side bitexts. word counts refer french words tokenization. commonly acknowledged training statistical models concatenation data necessarily lead optimal performance results extremely large models difﬁcult handle. instead focus relevant subset data given task. done applying data selection method proposed extension bitexts means selected subset words words language modeling subset words training encoder–decoder. used test newstest data selection weight tuning mert newstest test set. thousand words single reference translation. training neural networks including proposed encoder–decoder limited source target vocabulary frequent words english french. covers approximately dataset. out-of-vocabulary words mapped special token encoder–decoder encoder–decoder used experiment hidden units proposed gates encoder decoder. input matrix input symbol hidden unit approximated lower-rank matrices output matrix approximated table bleu scores computed development test sets using different combinations approaches. denotes word penalty penalizes number unknown words neural networks. similarly. used rank- matrices equivalent learning embedding dimension word. activation function used hyperbolic tangent function. computation hidden state decoder output implemented deep neural network single intermediate layer maxout units pooling inputs weight parameters encoder– decoder initialized sampling isotropic zero-mean gaussian distribution standard deviation ﬁxed except recurrent weight parameters. recurrent weight matrices ﬁrst sampled white gaussian distribution used left singular vectors matrix following used adadelta stochastic gradient descent train encoder–decoder hyperparameters update used randomly selected phrase pairs phrase table model trained approximately three days. neural language model order assess effectiveness scoring phrase pairs proposed encoder– decoder also tried traditional approach using neural network learning target language model especially comparison system using cslm using proposed approach phrase scoring encoder–decoder clarify whether contributions multiple neural networks different parts systrained cslm model -grams target corpus. input word projected embedding space concatenated form dimensional vector. concatenated vector rectiﬁed layers output layer simple softmax layer weight parameters initialized uniformly between model trained validation perplexity improve epochs. training language model achieved perplexity validation random selection corpus. model used score partial translations during decoding process generally leads higher gains bleu score n-best list rescoring address computational complexity using cslm decoder buffer used aggregate n-grams stacksearch performed decoder. full stack buffer pruned n-grams scored cslm. allows perform fast matrixmatrix multiplication using theano table scoring target phrases small source phrases according translation model encoder–decoder. source phrases randomly selected phrases words. denotes incomplete character. cyrillic letter ghe. best performance achieved used cslm phrase scores encoder–decoder. suggests contributions cslm encoder– decoder correlated expect better results improving method independently. furthermore tried penalizing number words unknown neural networks simply adding number unknown words additional feature loglinear model however case order understand performance improvement comes from analyze phrase pair scores computed encoder–decoder corresponding translation model. since existing translation model relies solely statistics phrase pairs corpus expect scores better estimated frequent phrases badly estimated rare phrases. also mentioned earlier sec. expect encoder– decoder trained without frequency information score phrase pairs based rather linguistic regularities statistics occurrences corpus. result probability words shortlist always overestimated. possible address issue backing existing model contain non-shortlisted words paper however introducing word penalty instead counteracts word probability overestimation. figure embedding learned word representation. left shows full embedding space right shows zoomed-in view region plots supplementary material. frequent. source phrase look target phrases scored high either translation probability encoder–decoder. similarly perform procedure pairs whose source phrase long rare corpus. table lists top- target phrases source phrase favored either translation model encoder–decoder. source phrases randomly chosen among long ones words. phrase pairs scored radically different could arise proposed approach training encoder– decoder unique phrase pairs discouraging encoder–decoder learning simply frequencies phrase pairs corpus explained earlier. furthermore table show source phrases table generated samples encoder–decoder. source phrase generated samples show top-ﬁve phrases accordingly scores. encoder–decoder able propose well-formed target phrases withlooking actual phrase table. importantly generated phrases overlap completely target phrases phrase table. encourages investigate possibility replacing whole part phrase table time continuous using neural networks able learn semantically meaningful e.g. since proposed encoder–decoder also projects maps back sequence words continuous space vector expect similar property proposed model well. left plot fig. shows embedding words using word embedding matrix learned encoder–decoder. projection done recently proposed barneshut-sne clearly semantically similar words clustered proposed encoder–decoder naturally generates continuous-space representation phrase. representation case -dimensional vector. similarly word representations visualize representations phrases consists four words using barnes-hut-sne fig. visualization clear encoder–decoder captures semantic syntactic structures phrases. instance bottom-left plot phrases duration time phrases syntactically similar clustered together. bottom-right plot shows cluster phrases semantically similar hand top-right plot shows phrases syntactically similar. arbitrary length another sequence possibly different arbitrary length. proposed encoder–decoder able either score pair sequences generate target sequence given source sequence. along architecture proposed novel hidden unit includes reset gate update gate adaptively control much hidden unit remembers forgets reading/generating sequence. evaluated proposed model task statistical machine translation used encoder–decoder score phrase pair phrase table. qualitatively able show model able capture linguistic regularities phrase pairs well also encoder–decoder able propose well-formed target phrases. scores encoder–decoder found improve overall translation performance terms bleu scores. also found contribution encoder– decoder rather orthogonal existing approach using neural networks system improve performance using instance encoder– decoder neural language model together. qualitative analysis trained model shows indeed captures linguistic regularities multiple levels i.e. word level well phrase level. suggests natural language related applications beneﬁt proposed encoder– decoder. proposed architecture large potential improvement analysis. approach investigated replace whole part phrase table letting encoder–decoder propose target phrases. also noting proposed model limited used written language important future research apply proposed architecture applications speech transcription. references michael auli michel galley chris quirk geoffrey zweig. joint language translation modeling recurrent neural netproceedings conference works. empirical methods natural language processing pages amittai axelrod xiaodong jianfeng gao. domain adaptation pseudo in-domain data selection. proceedings conference empirical methods natural language processing pages fr´ed´eric bastien pascal lamblin razvan pascanu james bergstra goodfellow arnaud bergeron nicolas bouchard yoshua bengio. theano features speed improvements. deep learning unsupervised feature learning nips workshop. boulangerlewandowski pascanu. advances proceedings optimizing recurrent networks. international conference acoustics speech signal processing may. james bergstra olivier breuleux fr´ed´eric bastien pascal lamblin razvan pascanu guillaume desjardins joseph turian david wardefarley yoshua bengio. theano math expression compiler. proceedings python scientiﬁc computing conference june. oral presentation. sarath chandar stanislas lauly hugo larochelle mitesh khapra balaraman ravindran vikas raykar amrita saha. autoencoder approach learning bilingual word representations. arxiv. february. george dahl dong deng alex acero. context-dependent pretrained deep neural networks large vocabulary ieee transactions audio speech recognition. speech language processing zbib zhongqiang huang thomas lamar richard schwartz john makhoul. fast robust neural network joint models statistical proceedings machine translation. conference pages kalchbrenner phil blunsom. recurrent continuous translation models. proceedings conference empirical methods natural language processing pages philipp koehn franz josef daniel marcu. statistical phrase-based translation. proceedings conference north american chapter association computational linguistics human language technology volume naacl pages daniel marcu william phrase-based joint probability wong. promodel statistical machine translation. ceedings acl- conference empirical methods natural language processing volume emnlp pages tomas mikolov ilya sutskever chen greg corrado jeff dean. distributed representations words phrases compositionality. advances neural information processing systems pages robert moore william intelligent selection language lewis. proceedings model training data. conference short papers aclshort pages stroudsburg usa. pascanu gulcehre bengio. construct deep recurrent neural networks. proceedings second international conference learning representations april. andrew saxe james mcclelland surya ganguli. exact solutions nonlinear dynamics learning deep linear neural networks. proceedings second international conference learning representations april. holger schwenk. continuous space translation models phrase-based statistical machine translation. martin christian boitet editors proceedings international conference computational linguistics pages richard socher eric huang jeffrey pennington andrew christopher manning. dynamic pooling unfolding recursive autoencoders paraphrase detection. advances neural information processing systems alexandre allauzen franc¸ois yvon. continuous space translation models neural networks. proceedings conference north american chapter association computational linguistics human language technologies naacl pages stroudsburg usa. ashish vaswani yinggong zhao victoria fossum david chiang. decoding large-scale neural language models improves translation. proceedings conference empirical methods natural language processing pages socher daniel christopher manning. bilingual word embeddings phrase-based proceedings machine translation. conference empirical methods natural language processing pages denote source phrase target phrase phrase sequence k-dimensional one-hot vectors element vector others index active element indicates word represented vector.", "year": 2014}