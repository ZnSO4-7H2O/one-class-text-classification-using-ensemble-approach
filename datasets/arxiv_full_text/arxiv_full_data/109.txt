{"title": "BlackOut: Speeding up Recurrent Neural Network Language Models With Very  Large Vocabularies", "tag": ["cs.LG", "cs.CL", "cs.NE", "stat.ML"], "abstract": "We propose BlackOut, an approximation algorithm to efficiently train massive recurrent neural network language models (RNNLMs) with million word vocabularies. BlackOut is motivated by using a discriminative loss, and we describe a new sampling strategy which significantly reduces computation while improving stability, sample efficiency, and rate of convergence. One way to understand BlackOut is to view it as an extension of the DropOut strategy to the output layer, wherein we use a discriminative training loss and a weighted sampling scheme. We also establish close connections between BlackOut, importance sampling, and noise contrastive estimation (NCE). Our experiments, on the recently released one billion word language modeling benchmark, demonstrate scalability and accuracy of BlackOut; we outperform the state-of-the art, and achieve the lowest perplexity scores on this dataset. Moreover, unlike other established methods which typically require GPUs or CPU clusters, we show that a carefully implemented version of BlackOut requires only 1-10 days on a single machine to train a RNNLM with a million word vocabulary and billions of parameters on one billion words. Although we describe BlackOut in the context of RNNLM training, it can be used to any networks with large softmax output layers.", "text": "nadathur satish michael anderson pradeep dubey parallel computing intel {nadathur.rajagopalan.satishmichael.j.andersonpradeep.dubey}intel.com propose blackout approximation algorithm efﬁciently train massive recurrent neural network language models million word vocabularies. blackout motivated using discriminative loss describe weighted sampling strategy signiﬁcantly reduces computation improving stability sample efﬁciency rate convergence. understand blackout view extension dropout strategy output layer wherein discriminative training loss weighted sampling scheme. also establish close connections blackout importance sampling noise contrastive estimation experiments recently released billion word language modeling benchmark demonstrate scalability accuracy blackout; outperform state-of-the achieve lowest perplexity scores dataset. moreover unlike established methods typically require gpus clusters show carefully implemented version blackout requires days single machine train rnnlm million word vocabulary billions parameters billion words. although describe blackout context rnnlm training used networks large softmax output layers. statistical language models crucial component speech recognition machine translation information retrieval systems. order handle data sparsity problem associated traditional n-gram language models neural network language models represent history context continuous vector space learned towards error rate reduction sharing data among similar contexts. instead using ﬁxed number words represent context recurrent neural network language models recurrent hidden layer represent longer variable length histories. rnnlms signiﬁcantly outperform traditional n-gram therefore becoming increasingly popular choice practitioners consider standard rnnlm depicted figure network input layer hidden layer recurrent connection itself output layer typically time step network input denotes vocabulary size previous state. produces hidden state size hidden layer turn transformed output different layers fully connected weight matrices denoted language modeling applications input sparse vector -of-v encoding element corresponding input word rest components state network dense vector summarizing history context {wt−··· immediately uses -of-v encoding computations equation relatively inexpensive computations equation expensive similarly back propagating gradients output layer hidden layer expensive. consequently training times largest models reported literature order weeks paper following question design approximate training scheme rnnlm improve state models using signiﬁcantly less computational resources? towards propose blackout approximation algorithm efﬁciently train massive rnnlms million word vocabularies. blackout motivated using discriminative loss describe weighted sampling strategy signiﬁcantly reduces computation improving stability sample efﬁciency rate convergence. also establish close connections blackout importance sampling noise contrastive estimation demonstrate blackout mitigates limitations previous methods. experiments recently released billion word language modeling benchmark demonstrate scalability accuracy blackout; outperform state-of-the achieving lowest perplexity scores dataset. moreover unlike established methods typically require gpus clusters show carefully implemented version blackout requires days single machine train rnnlm million word vocabulary billions parameters billion words. understand blackout view extension dropout strategy output layer wherein discriminative training loss weighted sampling scheme. connection dropout mainly operate model training model evaluation. similar dropout blackout training subset output layer sampled trained training batch evaluating full network participates. also like dropout regularization technique experiments show models trained blackout less prone overﬁtting. primary difference dropout routinely used input and/or hidden layers deep neural networks blackout operates output layer. chose name blackout light similarities method dropout complementary offer train deep neural networks. primarily focus estimation matrix wout. simplify notation sequel denote wout denote j-th wout. moreover denote product vectors. given notations rewrite equation rnnlms softmax output layer typically trained using cross-entropy loss function equivalent maximum likelihood estimation model parameter maximizes log-likelihood target word given history context gradient log-likelihood expensive evaluate cost computing summation takes time linear vocabulary size alleviate computational bottleneck computing gradient propose following discriminative objective function training rnnlm indices words drawn vocabulary typically tiny fraction experiments generate sample words vocabulary using easy sample distribution order compute equation cost function standard logistic regression classiﬁer discriminates positive sample negative samples wj∀j ﬁrst term corresponds traditional maximum likelihood training second term explicitly pushes probability negative samples addition implicit shrinkage enforced denominator experiments found discriminative training outperforms maximum likelihood training cases varying degree accuracy improvement depending weighted softmax function considered stochastic version standard softmax different base measure. standard softmax uses base measure gives equal weights words support entire vocabulary base measure used support words target word samples noise portion motivation sampling scheme term target word introduced mainly balance contributions target word noisy sample words. justiﬁcations discussed sec. sec. establish close connections blackout importance sampling noise contrastive estimation. weighted sampling property blackout words might sampled multiple times according proposal distribution thus indices appear multiple times target word assumed included computing therefore explicitly. chain rule derivatives propagate errors backward previous layers compute gradients respect full model parameters contrast eqs. much cheaper evaluate cost computing summation takes hence roughly times speed-up. next turn attention proposal distribution past uniform distribution unigram distribution advocated promising candidates sampling distributions experiments neither suitable wide range datasets power-raised unigram distribution mikolov important context note generalization uniform distribution unigram distribution rationale behind choice tuning interpolate smoothly sampling popular words advocated unigram distribution sampling words equally. best typically dataset and/or problem dependent; experiments holdout best value it’s worth noting sampling strategy used mikolov similar context word embedding explore effect language modeling applications. blackout training evaluate predictive performance rnnlm perplexity. calculate perplexity explicitly normalize output distribution using exact softmax function similar dropout wherein subset network sampled trained training batch evaluating full network participates. importance sampling applied nnlms large output layers previous works however either uniform distribution unigram distribution used sampling aforementioned works exploit maximum likelihood learning model parameter contrast blackout uses discriminative training power-raised unigram distribution sampling; changes important mitigate limitations is-based approaches. is-based approach uniform proposal distribution stable training suffers large bias apparent divergence uniform distribution true data distribution hand unigram-based estimate make learning unstable high variance using power-raised unigram distribution entails better trade-off bias variance thus strikes better balance extremes. addition experiments discriminative training blackout speeds rate convergence traditional maximum likelihood learning. basic idea transform density estimation problem problem learning comparison e.g. estimating parameters binary classiﬁer distinguishes samples data distribution samples generated known noise distribution language modeling setting data distribution distribution interest noise distribution often chosen ones easy sample possibly close true data distribution mnih uses context-independent noise distribution blackout formulated framework considering context-dependent noise distribution estimated samples drawn exactly weighted softmax function deﬁned note noise distribution proposed expensive denominator canceled mnih partition function either treated free parameter learned approximated constant. mnih recommended training. however experiments setting often leads sub-optimal solutions different settings sometimes incur numerical instability since log-sum-exp trick used shift scores output layer range amenable exponential function. blackout hyper-parameter tune log-sum-exp trick still works weighted softmax function discriminative training blackout share objective function shall emphasize according theory samples sampled noise distribution order calculate need samples drawn beforehand. approximation samples drawn samples expression evaluate noise density value required approximation accurate since esk∼q) proved appendix empirically performs much better using unigram noise distribution mnih hierarchical softmax uses hierarchical binary tree representation output layer words leaves. allows exponentially faster computation word probabilities gradients predictive performance resulting model heavily dependent tree used often constructed heuristically. moreover relaxing constraint binary structure introduces structured output layer arbitrary tree structure constructed word clustering. methods speed model training evaluation considerably. sampling-based approximations select random heuristically small subset output layer estimate gradient samples. importance sampling bengio sen´ecal jean mnih fall category recent locality sensitive hashing techniques select subset good samples. blackout close connections importance sampling also falls category. approaches speed model training model evaluation still remains computationally challenging. self normalization extends cross-entropy loss function explicitly encouraging partition function softmax close possible. initially approach speeds model evaluation recently it’s extended facilitate training well theoretical guarantees exact gradient limited loss functions introduces algorithmic approach efﬁciently compute exact loss gradient update output weights training example instead unfortunately applies limited family loss functions includes squared error spherical softmax standard softmax isn’t included. discussed introduction blackout also shares similarity dropout dropout often applied input and/or hidden layers deep neural networks avoid feature co-adaptation overﬁtting uniform sampling blackout applies softmax output layer uses weighted sampling employs discriminative training loss. chose name blackout light similarities method dropout complementary offer train deep neural networks. implemented blackout standard machine dual-socket -core intel rxeon haswell cpu. achieve high throughput train rnnlm back-propagation time mini-batches rmsprop learning rate scheduling gradient clipping avoid gradient explosion issue recurrent networks. latest intel library sgemm calls improved support tall-skinny matrix-matrix multiplications consume run-time rnnlms. expensive access update large models billions parameters. fortunately -of-v encoding input layer blackout sampling output layer model update wout sparse i.e. model parameters corresponding input/output words samples updated training batch. however subnet updates done carefully dependency within rmsprop updating procedure. therefore propose approximated rmsprop enables efﬁcient subnet update thus speeds algorithm even further. details found appendix experiments ﬁrst compare blackout exact softmax using small dataset. evaluate performance blackout recently released billion word language modeling benchmark vocabulary size million. compare performance blackout standard machine versus stateof-the-arts reported literature achieved gpus clusters nodes. implementation scripts open sourced https//github.com/intellabs/rnnlm. corpus models trained evaluated different corpora small dataset provided rnnlm toolkit recently released billion word language modeling benchmark perhaps largest public dataset language modeling. small dataset training sentences words total unique words; test perplexity evaluated test sentences. billion word benchmark constructed monolingual/english corpora; necessary preprocessing including de-duplication normalization tokenization sentences randomly selected training sentences randomly selected test remaining sentences reserved future development used holdout set. evaluate blackout exact softmax small dataset described above. small dataset used train standard rnnlm algorithm exact softmax within reasonable time frame hence provide baseline expected perplexity. many techniques involved training rmsprop learning rate scheduling subnet update mini-batch splicing etc. affect perplexity signiﬁcantly. fair comparison tricks settings algorithms evaluate impact different approximations softmax output layer. moreover hyper-parameters strong impact predictive performance including proposal distribution blackout additionally partition function equal amount effort tune hyper-parameters blackout validation number samples increases. figure shows perplexity reduction function number samples different vocabulary settings full vocabulary words using frequent words vocabulary. latter common approach used practice accelerate rnnlm computation using rnnlm predict frequent words handling rest using n-gram model similar vocabulary settings evaluate blackout large scale billion word benchmark. seen size samples increases general blackout improve prediction accuracy vocabulary settings even samples algorithms still converge reasonable solutions. blackout utilize samples much effectively manifested signiﬁcantly lower perplexities achieved blackout especially number samples small; given samples blackout reach similar perplexities exact softmax expensive train requires evaluate words vocabularies. vocabulary size blackout achieves even better perplexity exact softmax. possible since blackout stochastic sampling training example uses full softmax output layer prediction; similar dropout routinely used input layer and/or hidden layers deep neural networks dropout blackout beneﬁt regularization avoids feature co-adaption possibly less prone overﬁtting. verify hypothesis evaluate perplexities achieved training different algorithms provide results figure appendix seen exact softmax indeed overﬁts training reaches lower training perplexities blackout. figure test perplexity evolution function number samples full vocabulary words frequent words vocabulary. experiments executed rnnlms hidden units. next compare convergence rates blackout training rnnlms hidden units full vocabulary words. figures plot learning curves blackout samples samples used training respectively. ﬁgure shows blackout enjoys much faster convergence rate especially number samples small advantage gets smaller number samples increases also observed similar behavior evaluated blackout large scale billion word benchmark. follow experiments williams compare performance blackout state-of-the-art results provided them. evaluated blackdual-socket -core intel rxeon rhaswell machine williams implemented rnnlm approximation nvidia titan gpus executed array recurrent networks including deep lstm without approximation cluster. besides time-to-solution comparison published results enable cross-check predictive performance blackout another implementation competitive network architectures. frequent words kept vocabulary rest rare words mapped special <unk> token. ﬁrst study importance proposal distribution discriminative training proposed blackout. discussed sec. proposal distribution degenerates uniform distribution words vocabulary recover unigram distribution. thus evaluate impact range figure shows evolution test perplexity function rnnlms hidden units. seen signiﬁcant impact prediction accuracy. commonly used uniform distribution unigram distribution often yield sub-optimal solutions. dataset experiment considered gives best perplexity therefore experiments follow. number samples used vocabulary size. figure demonstrates impact discriminative training maximum likelihood training rnnlms hidden units using different α’s. general observe points perplexity reduction discriminative training traditional maximum likelihood training. finally evaluate scalability blackout number hidden units increases. dataset large observed performance rnnlm depends size hidden layer perform better size hidden layer gets larger. truncated word vocabulary used interpolate rnnlm scores full size -gram rare word probabilities report interpolated perplexities blackout achieved compare results williams table seen blackout reaches lower perplexities reported williams within comparable time frames achieved perplexity hidden layer size best knowledge lowest perplexity reported benchmark. ﬁnal experiments evaluate performance blackout large vocabulary words results provided table largest vocabulary used benchmark could existing literature. consider rnnlm hidden units hidden units compare test perplexities results samples vocabulary size blackout training comparing experiments word vocabulary much smaller used since sampling rate much lower used vocabulary size smaller strikes better balance sample coverage training example convergence rate. contrast setting converges slowly couldn’t reach competitive perplexity within time frame considered results reported here. standard rnn/lstm algorithms used cluster machines used train models hours. blackout enables train large model using single machine hours. since different model architectures used experiments direct comparison test perplexity isn’t meaningful. however experiment demonstrates even though largest model times larger models evaluated blackout along optimization techniques make large scale learning problem still feasible single machine without using gpus clusters. last collect state results aware benchmark summarize table since models interpolated ones interpolate best model table -gram model achieve perplexity score again different papers provide best models trained different architectures vocabulary settings. hence absolutely fair comparison isn’t possible. regardless discrepancies models within different groups vocabulary settings competitive terms prediction accuracy model size. proposed blackout sampling-based approximation train rnnlms large vocabularies established connections importance sampling noise contrastive estimation demonstrated stability sample efﬁciency rate convergence recently released billion word language modeling benchmark. achieved lowest reported perplexity benchmark without using gpus clusters. future extensions plans include exploring proposal distributions theoretical properties generalization property sample complexity bounds blackout. also investigate multi-machine distributed implementation. references andreas jacob klein dan. log-linear models self-normalizing? proceedings annual meeting north american chapter association computational linguistics bengio yoshua sen´ecal jean-s´ebastien. adaptive importance sampling accelerate training neural probabilistic language model. ieee transactions neural networks volume chelba ciprian mikolov tomas schuster mike brants thorsten koehn phillipp robinson tony. billion word benchmark measuring progress statistical language modeling. interspeech chen wang yongqiang xunying gales mark woodland philip efﬁcient gpu-based training recurrent neural network language models using spliced sentence bunch. interspeech devlin jacob zbib rabih huang zhongqiang lamar thomas schwartz richard makhoul john. fast robust neural network joint models statistical machine translation. mikolov tomas deoras anoop povey burget lukar cernocky honza. strategies training large scale neural network language models. ieee automatic speech recognition understanding workshop mikolov tomas sutskever ilya chen corrado greg dean jeffrey. distributed representations words phrases compositionality. burges chris bottou leon welling ghahramani zoubin weinberger kilian advances neural information processing systems morin frederic bengio yoshua. hierarchical probabilistic neural network language model. proceedings international workshop artiﬁcial intelligence statistics citeseer rumelhart david hinton geoffrey williams ronald neurocomputing foundations research. chapter learning representations back-propagating errors press cambridge schwenk holger gauvain jean-luc. training neural network language models large corpora. proceedings human language technology conference conference empirical methods natural language processing srivastava nitish hinton geoffrey krizhevsky alex sutskever ilya salakhutdinov ruslan. dropout simple prevent neural networks overﬁtting. jmlr figure training perplexity evolution function number samples full vocabulary words frequent words vocabulary. experiments executed rnnlms hidden units. rmsprop adaptive learning rate method found much success practice. instead using single learning rate model parameters rmsprop dedicates learning rate model parameter normalizes gradient exponential moving average magnitude gradient learning rate damping factor e.g. rmsprop effective learning rate scheduling techniques requires large amount memory store per-parameter addition model parameter gradients. expensive access update large models billions parameters. fortunately -of-v encoding input layer blackout sampling output layer model update wout sparse e.g. model parameters corresponding input/output words samples updated. however even model parameter involved current training value still needs updated βvt− since ignoring update detrimental effect predictive performance; experiments observed point perplexity loss ignore update completely. resort approximation βvt−. given probability word selected update number time steps elapsed successfully selected follows geometric distribution success rate whose mean value /pu. assume input/output word selected according unigram distribution puni samples drawn approximated mini-batch size bptt block size. update model parameters typically tiny fraction really involved current training thus speed rnnlm training further.", "year": 2015}