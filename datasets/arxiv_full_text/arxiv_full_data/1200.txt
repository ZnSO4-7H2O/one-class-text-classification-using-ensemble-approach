{"title": "Compressing Convolutional Neural Networks", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Convolutional neural networks (CNN) are increasingly used in many areas of computer vision. They are particularly attractive because of their ability to \"absorb\" great quantities of labeled data through millions of parameters. However, as model sizes increase, so do the storage and memory requirements of the classifiers. We present a novel network architecture, Frequency-Sensitive Hashed Nets (FreshNets), which exploits inherent redundancy in both convolutional layers and fully-connected layers of a deep learning model, leading to dramatic savings in memory and storage consumption. Based on the key observation that the weights of learned convolutional filters are typically smooth and low-frequency, we first convert filter weights to the frequency domain with a discrete cosine transform (DCT) and use a low-cost hash function to randomly group frequency parameters into hash buckets. All parameters assigned the same hash bucket share a single value learned with standard back-propagation. To further reduce model size we allocate fewer hash buckets to high-frequency components, which are generally less important. We evaluate FreshNets on eight data sets, and show that it leads to drastically better compressed performance than several relevant baselines.", "text": "convolutional neural networks increasingly used many areas computer vision. particularly attractive ability absorb great quantities labeled data millions parameters. however model sizes increase storage memory requirements classiﬁers. present novel network architecture frequency-sensitive hashed nets exploits inherent redundancy convolutional layers fully-connected layers deep learning model leading dramatic savings memory storage consumption. based observation weights learned convolutional ﬁlters typically smooth low-frequency ﬁrst convert ﬁlter weights frequency domain discrete cosine transform low-cost hash function randomly group frequency parameters hash buckets. parameters assigned hash bucket share single value learned standard back-propagation. reduce model size allocate fewer hash buckets high-frequency components generally less important. evaluate freshnets eight data sets show leads drastically better compressed performance several relevant baselines. recent years convolutional neural networks lead impressive results object recognition face veriﬁcation audio classiﬁcation problems seemed impossibly hard years solved better human accuracy although cnns known quarter century recently superb generalization abilities accepted widely across machine learning computer vision communities. broad acceptance coincides release large collections labeled data deep networks cnns particularly well suited learn large quantities data part arbitrarily many parameters. data sets grow model sizes. ﬁrst winner imagenet competition used already parameters recent winning model required independently another parallel shift computing servers workstations mobile platforms. january already searches smart phones computers. today speech recognition primarily used cell phones intelligent assistants apple’s siri google microsoft’s cortana. trend continues expecting machine learning applications also shift increasingly towards mobile devices. however disjunction deep learning ever increasing model sizes mobile computing reveals inherent dilemma. mobile devices tight memory storage limitations. example even recent iphone features must used operating system application itself. addition developers must make apps compatible limited phone still circulation often restricting models megabytes parameters. response recent interest reducing model sizes deep networks. denil low-rank decomposition weight matrices reduce effective number parameters network. bucilu show complex models compressed -layer neural networks. independently model size neural networks reduced effectively reduced precision paper propose novel approach neural network compression targeted especially cnns. build recent work chen show weights fully connected networks effectively compressed hashing trick nature local pixel correlation images ﬁlters cnns tend smooth. transform ﬁlters frequency domain discrete cosine transform frequency space ﬁlters naturally dominated frequency components. compression takes smoothness property account randomly hashes frequency components ﬁlters given layer common hash buckets. components inside hash bucket share value. lower frequency components pronounced higher frequencies allow collisions similar frequencies allocate fewer hash buckets high frequencies approach several compelling properties number parameters independent number convolutional ﬁlters; testing need low-cost hash function inverse transformation existing code ﬁlter reconstruction; training hashed weights learned simple back-propagation —the gradient hash bucket value gradients hashed frequency components bucket. evaluate compression scheme eight deep learning image benchmark data sets compare four competitive baselines. although compression schemes lead lower test accuracy compression increases freshnets method effective compression method yields lowest generalization error rates almost classiﬁcation tasks. maps input vector much smaller feature space mapping mapping composite approximately uniform auxiliary hash functions element k-dimensional hashed input deﬁned shown property feature hashing preservation inner product operations inner products hashing produce correct pre-hash inner product expectation property holds bias correcting sign factor feature hashing models directly learned much smaller space speeds training evaluation also signiﬁcantly conserves memory. example linear classiﬁer original space could occupy memory model parameters learned hashed space requires parameters. information loss induced hash collision much less severe sparse feature vectors counteracted multiple hashing larger hash tables discrete cosine transform methods built widely used compressing images movies including forming standard technique jpeg expresses function weighted combination sinusoids different phases/frequencies weight sinusoid reﬂects magnitude corresponding frequency input. employed sufﬁcient numerical precision without quantization compression operations inverse lossless. compression made possible images local smoothness pixels well represented regionally fewer non-zero frequency components. though highly related discrete fourier transformation often preferable compression tasks spectral compaction property weights images tend concentrated low-frequency components further transformation yields real-valued representation unlike whose representation imaginary components. given input cosine basis function otherwise. shorthand fdct denote operation i.e. fdct. inverse converts frequency domain back spatial domain reconstructing without loss present freshnets method using weight sharing reduce model size convolutional neural networks. similar work chen achieve smaller models randomly forcing weights throughout network share identical values. unlike previous work implement weight sharing gradient updates convolutional ﬁlters frequency domain. sharing constraints made prior training learn frequency weights sharing assignments. since assignments made hash function incur additional storage. filters spatial frequency domain. matensor rm×n×d×d number input planes output planes respectively resulting total parameters. convolutional ﬁlters represented equivalently either spatial frequency domain mapping inverse. denote ﬁlter frequency figure schematic illustration freshnets. spatial ﬁlters reconstructed frequency weights vector frequency weights accessed hash functions transformed spatial domain. vector partitioned subvectors shared entries similar frequency colors indicate hash bucket accessed. randomly assign value ﬁlter frequency weight na¨ıve implementation random weight sharing would introduce auxiliary matrix track weight assignments using signiﬁcant additional memory. address problem chen advocate hashing trick randomly assign shared parameters. using hashing trick ﬁlter weight j)∈{··· j)∈{±} sign factor computed second hash function preserve inner-products expectation described section mapping implement shared parameter assignments additional storage cost. gradients shared frequency weights. typical convolutional neural networks learn ﬁlters spatial domain. shared weights stored frequency domain derive gradient respect ﬁlter parameters frequency space. following express gradient parameters spatial domain w.r.t. counterparts frequency domain loss function adopted training. using standard back-propagation derive gradient w.r.t. ﬁlter parameters spatial domain chain rule express gradient frequency domain denotes entry matrix frequency sensitive hashing. figure shows ﬁlter spatial frequency domains. spatial domain ﬁlters smooth local pixel smoothness natural images. frequency domain corresponds components large magnitudes frequencies depicted upper left half figure correspondingly high frequencies bottom right half magnitudes near zero. components different frequency groups tend different magnitudes want avoid collisions high frequency components. therefore assign separate hash spaces different frequency groups. particular partition values sub-vectors sizes partitioning allows parameters frequency corresponding index hashed corresponding dedicated hash space rewrite frequency sensitive shared weight assignments maps input natural number {··· deﬁne compression rate frequency region assign rjnj. smaller induces collisions hashing leading increased weight sharing. since lower frequency components tend higher importance making collisions hurtful commonly assign larger low-frequency regions. intuitively given size budget whole convolutional layer want squeeze hash space high frequency region save space frequency regions. compression rates either assigned hand determined programmatically cross-validation demonstrated section several recent studies conﬁrmed signiﬁcant redundancy parameters learned deep neural networks. recent work denil learns parameters fully-connected layers decomposition low-rank matrices i.e. rm×n rm×k rk×n. original parameters could stored storage min. several works apply related approaches speed evaluation time convolutional neural networks. works propose approximate convolutional ﬁlters weighted linear combination basis ﬁlters setting convolution operation needs performed small basis ﬁlters. desired output feature maps computed matrix multiplication weighted basis convolutions. speedup achieved learning rank-one basis ﬁlters convolution operations cheap compute based idea denton advocate decomposing four-dimensional tensor ﬁlter weights different rank-one four-dimensional tensors. addition adopt bi-clustering group ﬁlters subgroup better approximated rank-one tensors. works evaluation time main focus resulting storage reduction achieved merely side effect. works focus entirely compressing fully-connected layers cnns however trend toward architectures fewer fully connected layers additional convolutional layers compression ﬁlters increased importance. another technique speeding convolutional neural network evaluation computing convolutions fourier frequency domain convolution spatial domain equivalent element-wise multiplication frequency domain unlike freshnets ﬁlter size image size mathieu convert ﬁlter frequency domain size oversampling frequencies necessary element-wise multiplication larger image also increases memory overhead test time. training fourier frequency domain advantageous similar reasons particularly convolutions performed large volumes relevant work hashednets compresses fully connected layers deep neural networks. method uses hashing trick efﬁciently implement parameter sharing prior learning achieving notable compression less loss accuracy competing baselines relied low-rank decomposition learning randomly sparse architectures. datasets. experiment eight benchmark datasets cifar cifar svhn challenging variants mnist. cifar dataset contains images pixels three color channels. images selected classes class consisting unique instances. cifar dataset also contains images challenging since images selected classes cifar datasets images designated training remaining images testing. improve accuracy cifar augment horizontal reﬂection cropping resulting training images. svhn dataset large collection digits cropped realworld scenes consisting training images testing images less difﬁcult table network architecture. convolution. relu. max-pooling. dropout. fully-connected. number parameters fully-connected layer speciﬁc input images varies number classes either depending dataset. table test error rates compression factors convolutional layers compressed indicated methods convolutional layer compression applied cnn. fully connected layer compressed hashnets methods including cnn. images additional training. experiments available training images total training samples. mnist variants variation either reduces training size amends original digits rotation background superimposition combination thereof preprocess datasets whitening baselines. compare proposed freshnets four baseline methods hashednets low-rank decomposition ﬁlter dropping frequency dropping hashednets ﬁrst proposed compress fully-connected layers deep neural networks hashing trick. baseline apply hashing trick directly convolutional layer hashing ﬁlter weights spatial domain. induces random weight sharing across ﬁlters single convolutional layer. additionally compare low-rank decomposition convolutional ﬁlters following method unfold four-dimensional ﬁlter tensor form dimensional matrix apply low-rank decomposition. parameters decomposition ﬁne-tuned back-propagation. dropfreq learns parameters frequency domain sets high frequency components meet compression requirement. dropfilt compresses simply reducing number ﬁlters convolutional layer. comprehensive evaluation. adopt network network architecture shown table datasets. architecture deep convolutional neural network consisting convolutional layers fully-connected layer. convolution input feature maps zero-padded output maps remain size input maps convolution. max-pooling performed convolutions layers ﬁlter size stride reducing input dimensions half. rectiﬁed linear units adopted activation function throughout. output network softmax function labels. architecture convolutional layers hold majority parameters training optimize parameters using mini-batch gradient descent batch size momentum percent training validation early stopping. freshnets frequency-sensitive compression scheme increases weight sharing among higher frequency components. baselines apply hashednets fully connected layer corresponding level compression. error results reported test set. table show comprehensive evaluation methods compression ratios respectively. exclude dropfilt dropfreq table neither supports compression architecture layers. methods fully connected layer compressed hashednets corresponding compression rate. ﬁnal size entire network respects speciﬁed compression ratio. reference also show error rate standard convolutional neural network fully-connected layer compressed hashednets compression convolutional layers. excluding reference highlight method best test error dataset bold. discern several general table observe performance dropfilt dropfreq compression. compression rate dropfilt corresponds network ﬁlters layer layers respectively. architecture yields particularly poor test accuracy including essentially random predictions three datasets. dropfreq compression parameterizes ﬁlter original network low-frequency values frequency space performs similarly poor accuracy. rank decomposition hashednets yield similar performance compression. neither explicitly considers smoothness inherent learned convolutional ﬁlters instead compressing ﬁlters spatial domain. method freshnets consistently outperforms baselines particularly higher compression rate shown table using model table figure shows complete curves test errors multiple compression factors cifar datasets. figure results different frequency sensitive compression schemes adopting different beta distribution compression rate frequency. inner ﬁgure shows normalized test error scheme cifar beta distribution hyper-parameters. outer ﬁgure depicts beta distributions varying compression frequency. mentioned section allow higher collision rate high frequency components frequency components ﬁlter. demonstrate utility scheme evaluate several hash compression schemes. systematically compression rate frequency band parameterized function i.e. figure visualization ﬁlters learning mnist uncompressed compressed freshnets compressed hashednets freshnets preserves smoothness ﬁlters whereas hashednets not. experiment beta distribution zxα−β− real number ﬁlter size normalizing factor rjnj adjust control compression rate frequency region. shown figure multiple pairs results different compression scheme. example compression rate monotonically decreases function component frequency meaning parameter sharing among high frequency components quickly evaluate performance scheme simple four-layer freshnets ﬁrst layers dct-hashed convolutional layers containing feature maps respectively last layers fully connected layers. test freshnets cifar compression schemes shown figure each weight sharing limited within groups similar frequencies described section however number unique weights shared within group varied. denote compression scheme frequency-oblivious scheme since produces uniform compression independent frequency. inset plot figure report test error normalized test error frequency-oblivious scheme averaged compression rates proposed scheme fewer shared weights allocated high frequency components outperforms compression schemes. inverse scheme high frequency regions lowest collision rate performs worst. empirical results assumption frequency components ﬁlter important high frequency components. filter visualization. investigate smoothness learned convolutional ﬁlters figure visualizing ﬁlter weights standard uncompressed freshnets hashednets experiment apply four layer network convolutional layers adopt larger ﬁlters better visualization. three networks trained mnist freshnets hashednets compression ﬁrst convolutional layer. plotting scale values ﬁlter matrix range hence white black pixels stand large positive negative weights respectively. observe that although blurry compression ﬁlter weights freshnets still smooth weights hashednets appear chaotic. paper present freshnets method learning convolutional neural networks dramatically compressed model storage. harnessing hashing trick parameter-free random weight sharing leveraging smoothness inherent convolutional ﬁlters freshnets compresses parameters frequency-sensitive fashion signiﬁcant model parameters better preserved. such freshnets preserves prediction accuracy signiﬁcantly better competing baselines high compression rates. references caruana. deep nets really need deep? nips pages bishop. neural networks pattern recognition. oxford university press inc. brosch tam. efﬁcient training convolutional deep belief networks frequency domain deng dong socher l.-j. fei-fei. imagenet large-scale hierarchical image database. computer vision pattern recognition cvpr ieee conference pages ieee denil shakibi dinh freitas predicting parameters deep learning. nips denton zaremba bruna lecun fergus. exploiting linear structure within convolutional networks efﬁcient evaluation. advances neural information processing systems pages pham largman unsupervised feature learning audio classiﬁcation using convolutional deep belief networks. advances neural information processing systems pages", "year": 2015}