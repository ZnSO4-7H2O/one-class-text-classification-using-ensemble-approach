{"title": "Bayesian CP Factorization of Incomplete Tensors with Automatic Rank  Determination", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "CANDECOMP/PARAFAC (CP) tensor factorization of incomplete data is a powerful technique for tensor completion through explicitly capturing the multilinear latent factors. The existing CP algorithms require the tensor rank to be manually specified, however, the determination of tensor rank remains a challenging problem especially for CP rank. In addition, existing approaches do not take into account uncertainty information of latent factors, as well as missing entries. To address these issues, we formulate CP factorization using a hierarchical probabilistic model and employ a fully Bayesian treatment by incorporating a sparsity-inducing prior over multiple latent factors and the appropriate hyperpriors over all hyperparameters, resulting in automatic rank determination. To learn the model, we develop an efficient deterministic Bayesian inference algorithm, which scales linearly with data size. Our method is characterized as a tuning parameter-free approach, which can effectively infer underlying multilinear factors with a low-rank constraint, while also providing predictive distributions over missing entries. Extensive simulations on synthetic data illustrate the intrinsic capability of our method to recover the ground-truth of CP rank and prevent the overfitting problem, even when a large amount of entries are missing. Moreover, the results from real-world applications, including image inpainting and facial image synthesis, demonstrate that our method outperforms state-of-the-art approaches for both tensor factorization and tensor completion in terms of predictive performance.", "text": "abstract—candecomp/parafac tensor factorization incomplete data powerful technique tensor completion explicitly capturing multilinear latent factors. existing algorithms require tensor rank manually speciﬁed however determination tensor rank remains challenging problem especially rank. addition existing approaches take account uncertainty information latent factors well missing entries. address issues formulate factorization using hierarchical probabilistic model employ fully bayesian treatment incorporating sparsity-inducing prior multiple latent factors appropriate hyperpriors hyperparameters resulting automatic rank determination. learn model develop efﬁcient deterministic bayesian inference algorithm scales linearly data size. method characterized tuning parameter-free approach effectively infer underlying multilinear factors low-rank constraint also providing predictive distributions missing entries. extensive simulations synthetic data illustrate intrinsic capability method recover ground-truth rank prevent overﬁtting problem even large amount entries missing. moreover results real-world applications including image inpainting facial image synthesis demonstrate method outperforms state-of-the-art approaches tensor factorization tensor completion terms predictive performance. tive faithful representation structural properties data particular multidimensional data data ensemble affected multiple factors involved. instance video sequence represented third-order tensor dimensionality height width time; image ensemble measured multiple conditions represented higher order tensor dimensionality pixel person pose illumination. tensor factorization enables explicitly take account structure information effectively capturing multilinear interactions among multiple latent factors. therefore theory algorithms active area study past decade successfully applied various application ﬁelds face recognition social network analysis image video completion brain signal processing. popular tensor factorization frameworks tucker candecomp/parafac also known canonical polyadic decomposition zhao laboratory advanced brain signal processing riken brain science institute japan department computer science engineering shanghai jiao tong university china. zhang moe-microsoft laboratory intelligent computing intelligent systems department computer science engineering shanghai jiao tong university china. cichocki laboratory advanced brain signal processing riken brain science institute japan systems research institute polish academy science warsaw poland. tensor complete whereas problem missing data arise variety real-world applications. issue attracted great deal research interest tensor completion recent years. objective tensor factorization incomplete data capture underlying multilinear factors partially observed entries turn predict missing entries. factorization missing data formulated weighted least squares problem termed weighted optimization structured using nonlinear least squares proposed geometric nonlinear conjugate gradient based riemannian optimization manifold tensors presented. however number missing entries increases tensor factorization schemes tend overﬁt model incorrectly speciﬁed tensor rank resulting severe deterioration predictive performance. contrast another popular technique exploit low-rank assumption recovering missing entries; rank minimization formulated convex optimization problem nuclear norm. technique extended higher order tensors deﬁning nuclear norm tensor yielding tensor completion variants also proposed framework based convex optimization spectral regularization uses inexact splitting method fast composite splitting algorithms improve efﬁciency douglas-rachford splitting technique nonlinear gauss-seidal method also investigated. recently nuclear norm based optimization also applied supervised tensor dimensionality reduction method alternative number components factor matrices constrained minimum. model parameters including noise precision considered latent variables corresponding priors placed. complex interactions among multiple factors fully bayesian treatment learning model analytically intractable. thus resort variational bayesian inference derive deterministic solution approximate posteriors model parameters hyperparameters. method characterized tuning parameter-free approach effectively avoid parameter selections. extensive experiments comparisons synthetic data illustrate advantages approach terms rank determination predictive capability robustness overﬁtting. moreover several real-word applications including image completion restoration synthesis demonstrate method outperforms state-of-the-art approaches including tensor factorization tensor completion terms predictive performance. rest paper organized follows. section preliminary multilinear operations notations presented. section introduce probabilistic model speciﬁcation model learning bayesian inference. variant method using mixture priors proposed section section present comprehensive experimental results synthetic data real-world applications followed conclusion section order tensor number dimensions also known ways modes. vectors denoted boldface lowercase letters e.g. matrices denoted boldface capital letters e.g. higher-order tensors denoted boldface calligraphic letters e.g. given nth-order tensor ri×i×···×in entry denoted xii...in indices typically range capital version e.g. in∀n inner product tensors deﬁned ii...in aii...inbii...in squared frobenius norm extension variables generalized inner product vectors matrices tensors deﬁned element-wise products. example given {a|n deﬁne method tensor completion employ adaptive sampling schemes however nuclear norm tensor deﬁned straightforwardly weighted nuclear norm mode-n matricizations related multilinear rank rather rank. addition completion-based methods cannot explicitly capture underlying factors. hence simultaneous tensor decomposition completion method introduced rank minimization technique combined tucker decomposition improve completion accuracy auxiliary information also exploited strongly depends speciﬁc application. also noteworthy rank minimization based convex optimization nuclear norm affected tuning parameters tend overunder-estimate true tensor rank. important emphasize knowledge properties rank deﬁned minimum number rank-one terms decomposition surprisingly limited. straightforward algorithm compute rank even given speciﬁc tensor problem shown np-complete lower upper bound tensor rank studied ill-posedness best low-rank approximation tensor investigated fact determining even bounding rank arbitrary tensor quite difﬁcult contrast matrix rank difﬁculty would signiﬁcantly exacerbated presence missing data. probabilistic models matrix/tensor factorization attracted much interest collaborative ﬁltering matrix/tensor completion. probabilistic matrix factorization proposed fully bayesian treatment using markow chain monte carlo inference shown using variational bayesian inference extensions nonparametric robust variants presented probabilistic frameworks tensor factorization presented variants include extensions exponential family model nonparametric bayesian model however tensor rank model complexity often given tuning parameter selected either maximum likelihood cross-validations computationally expensive inaccurate. another important issue inference factor matrices performed either point estimation prone overﬁtting mcmc inference tends converge slowly. address issues propose fully bayesian probabilistic tensor factorization model according factorization framework. objective infer underlying multilinear factors noisy incomplete tensor predictive distribution missing entries rank true latent tensor determined automatically implicitly. achieve this specify sparsity-inducing hierarchical prior multiple factor matrices individual hyperparameters associated latent dimension indicates yi···in generated multiple rlatent vector contributes observations i.e. subtensor whose mode-n index essential difference matrix tensor factorization inner product vectors allows model multilinear interaction structure however leads many difﬁculties model learning. general effective dimensionality latent space tuning parameter whose selection quite challenging computational costly. therefore seek elegant automatic model selection infer rank latent tensor also effectively avoid overﬁtting. achieve this continuous hyperparameters employed control variance related dimensionality latent space respectively. since minimum desired sense rank approximation sparsity-inducing prior speciﬁed hyperparameters resulting possible achieve automatic rank determination part baybesian inference process. technique related automatic relevance determination sparse bayesian learning however unlike traditional methods place prior either latent variables weight parameters bayesian principle component analysis method considers model parameters latent variables sparsityinducing prior placed shared hyperparameters. speciﬁcally place prior distribution latent factors governed hyperparameters controls component denoted kronecker product matrices ri×j rk×l matrix size ik×jl denoted a⊗b. khatri-rao product matrices ri×k rj×k matrix size deﬁned columnwise kronecker product denoted particular khatri-rao product matrices reverse bayesian tensor factorization probabilistic model priors incomplete nth-order tensor size missing entries. element yii...in observed denotes indices. simplicity also deﬁne binary tensor size indicator observed entries. assume noisy observation true latent tensor noise term assumed i.i.d. gaussian distribution i...in latent tensor i.e. exactly represented model given denotes outer product vectors shorthand notation also termed kruskal operator. factorization interpreted rank-one tensors smallest integer deﬁned rank {a}n factor matrices mode-n factor matrix rin×r denoted row-wise column-wise vectors exact bayesian inference would integrate latent variables well hyperparameters obviously analytically intractable. section describe development deterministic approximate inference variational bayesian framework learn probabilistic factorization model. represents model evidence since model evidence constant maximum lower bound occurs divergence vanishes implies initial derivation assumed variational distribution factorized w.r.t. variable therefore written noted assumption distribution particular functional forms individual factors explicitly derived turn. optimised form factor based maximization given tributions variables except since distributions parameters drawn exponential family conjugate w.r.t. distributions parents derive closed-form posterior update rules parameter using since sparsity enforced latent dimensions initialization point dimensionality latent space usually maximum possible value effective dimensionality inferred automatically bayesian inference framework. noted since priors shared across latent matrices framework learn sparsity pattern them yielding minimum number rank-one terms. therefore model effectively infer rank tensor performing tensor factorization treated bayesian low-rank tensor factorization. simplicity notation unknowns including latent variables hyperparameters collected denoted together probabilistic graph model illustrated fig. easily write joint distribution model i...in oi...in denotes total number observations. without loss generality perform maximum posteriori estimation maximizing extent equivalent optimizing squared error function regularizations imposed factor matrices additional constraints imposed regularization parameters. o···in··· denotes subtensor ﬁxing model-n index noted khatri-rao product computed mode factors except mode performed according indices observations implying factors interact taken account. another complicated part also simpliﬁed multilinear operations i.e. intuitive interpretation given follows. posterior covariance updated combining prior information posterior information factor matrices computed tradeoff terms controlled related quality model ﬁtting. words better ﬁtness current model leads information factors prior information. posterior mean updated ﬁrstly linear combination factors expressed coefﬁcients observed values. implies larger observation leads similarity corresponding latent factors. then rotated obtain property sparsity scaled according model ﬁtness posterior distribution hyperparameters noted that instead point estimation optimizations learning posterior crucial automatic rank determination. seen fig. inference performed receiving messages factor matrices incorporating messages posterior distribution factor matrices seen graphical model shown fig. inference mode-n factor matrix performed receiving messages observed data co-parents including factors hyperparameter expressed likelihood term incorporating messages parents expressed prior term applying shown posteriors factorized independent distributions rows also gaussian given size column computed varying modek index symbol denotes subset columns sampled according subtensor hence denotes posterior covariance matrix khatri-rao product latent factors modes except nth-mode computed columns corresponding observed entries whose mode-n index order evaluate posterior covariance matrix ﬁrst need introduce following results. left term denotes expectation squared inner product vectors right term denotes inner product matrices matrix size denotes expectation outer product vector respectively. intuitive interpretation straightforward. related number observations related residual model ﬁtting measured squared frobenius norm observed entries. lower bound model evidence inference framework presented previous section essentially maximize lower bound model evidence deﬁned since lower bound decrease iteration used test convergence. lower bound logmarginal likelihood computed intuitive interpretation updated squared l-norm component expressed factor matrices. therefore leads larger updated smaller priors factor matrices turn enforces strongly component zero. posterior distribution hyperparameter inference noise precision performed receiving messages observed data coparents including factor matrices incorporating messages hyperprior. applying variational posterior gamma distribution given prior subsequent iteration turn affects hence posterior mean becomes large components {a}∀n forced zero prior information tensor rank obtained simply counting number non-zero components factor matrices. implementation algorithm keep size unchanged iterations; alternative method eliminate zero-components iteration. predictive distribution predictive distributions missing entries given observed entries approximated using variational posterior distribution computational complexity computation cost factor matrices order tensor denotes number observations i.e. input data size. number latent components intuitive interpretation follows. ﬁrst term related model residual; second term related weighted squared l-norm component factor matrices uncertainty information also considered; rest terms related negative divergence posterior prior distributions hyperparameters. initialization model parameters variational bayesian inference guaranteed converge local minimum. avoid getting stuck poor local solutions important choose initialization point. model level hyperparameters including resulting noninformative prior. thus factor matrices {e]}n initialized different strategies randomly drawn denotes left singular vectors denotes diagonal singular values matrix obtained mode-n matricization tensor covariance matrix simply tensor rank usually initialized weak upper bound maximum practice also manually deﬁne initialization value computational efﬁciency. interpretaion automatic rank determination entire procedure model inference summarized algorithm noted tensor rank determined automatically implicitly. speciﬁcally updating iteration results prior then updated using coefﬁcients model learning easily verify posterior distribution also mixture distribution. simplicity thus posterior mean factor matrix updated ﬁrstly applying weq] posterior covariance keep unchanged. furthermore inference variables need changes. conducted extensive experiments using synthetic data real-world applications compared fully bayesian factorization seven state-of-the-art methods. tensor factorization based scheme includes cpwopt cpnls completion based scheme includes halrtc falrtc fcsa hard-completion geomcg stdc objective using synthetic data validate method several aspects capability rank determination; reconstruction performance given complete tensor; iii) predictive performance missing entries given incomplete tensor. real-world applications including image inpainting facial image synthesis used demonstration. experiments performed .ghz memory). synthetic tensor data generated following procedure. factor matrices {a}n drawn standard normal distribution i.e. ∀n∀in then true tensor constructed observed tensor denotes i.i.d. additive noise. missing entries chosen uniformly marked indicator tensor illustrate model provide demo videos supplemental materials. true latent tensor size rank noise parameter entries missing. then applied method initial rank shown fig. three factor matrices inferred components effectively pruned resulting correct estimation tensor rank. lower bound model evidence increases monotonically indicates effectiveness convergence algorithm. finally posterior noise precision implies method’s capability denoising estimation i.e. model complexity tensor rank generally much smaller data size i.e. hence linear complexity w.r.t. data size polynomial complexity w.r.t. model complexity. noted that automatic model selection excessive latent components pruned ﬁrst iterations reduces rapidly practice. computation cost hyperparameter dominated model complexity computation cost noise precision therefore overall complexity algorithm scales linearly data size polynomially model complexity. discussion advantages advantages method discussed follows automatic determination rank enables obtain optimal low-rank tensor approximation even highly noisy incomplete tensor. method characterized tuning parameterfree approach model parameters inferred observed data avoids computational expensive parameter selection procedure. contrast existing tensor factorization methods require predeﬁned rank tensor completion methods based nuclear norm require several tuning parameters. uncertainty information latent factors predictions missing entries inferred method existing tensor factorization completion methods provide point estimations. mixture factor priors low-rank assumption powerful general cases however tensor data satisfy intrinsic low-rank structure large amount entries missing yield oversimpliﬁed model. section present variant bayesian factorization model take account local similarity addition low-rank assumption. wink indicates inth vector similar vectors probability wink. based assumption adjacent rows highly correlated deﬁne mixture coefﬁcients ziexp used ensure mixture fig. determination tensor rank varying experimental conditions. vertical shows mean standard deviation estimations repetitions accuracy detections shown corresponding bar. blue horizontal dash dotted lines indicate true tensor rank. presented model achieve accuracy condition snr= missing ratio. noted that data size larger model achieve accuracy even snr= missing ratio true rank larger model correctly recover rank complete tensor fails missing ratio larger conclude results determination tensor rank depends primarily number observed entries true tensor rank. general observations necessary tensor rank larger; however high-level noise occurs excessive number observations helpful rank determination. predictive performance experiment considered incomplete tensors size generated true rank snr= varying missing ratios. initial rank relative standard error denotes estimation true tensor used evaluate performance. ensure statistically consistent results performance evaluated repetitions condition. shown fig. method signiﬁcantly outperforms algorithms missing ratios. factorization-based methods including cpwopt cpnls show better performance completion-based methods missing ratio relatively small perform fig. example illustrating fbcp applied incomplete tensor. shows hinton diagram factor matrices color size square represent sign magnitude value respectively. bottom shows posterior lower bound model evidence posterior left right. evaluate automatic determination tensor rank extensive simulations performed varying experimental conditions related tensor size tensor rank noise level missing ratio initialization method factor matrices result evaluated repetitions corresponding different tensors generated criterion. four groups experiments. given complete tensors size evaluations performed varying noise levels different initializations given incomplete tensors size snr= evaluations performed different missing ratios different initializations given incomplete tensors snr= evaluations performed varying missing ratios different tensor sizes given incomplete tensors size snr= evaluations performed varying missing ratios different true ranks fig. observe initialization slightly better random initialization terms determination tensor rank. tensor complete model detect true tensor rank accuracy snr≥ although accuracy decreased high noise level error deviation hand tensor incomplete almost free noise detection rate missing ratio error deviation even high missing ratio missing data high noise level structural image uniformly random missing pixels. building facade image missing pixels noise conditions i.e. noise free snr=db considered observations. natural image uniformly random missing pixels. lena image size missing pixels noise conditions i.e. noise free snr=db considered. non-random missing pixels. conducted experiments image restoration corrupted image lenna image corrupted superimposed text used observed image. practice location text pixels difﬁcult detect exactly; simply indicate missing entries value larger ensure text pixels completely missing. scrabbled lenna image used observed image pixels values larger marked missing. object removal. given image mask covering object area goal complete image without object. algorithm settings compared methods described follows. factorization-based methods initial rank cases high missing ratios cases completionbased methods tuning parameters chosen multiple runs performance evaluated groundtruth missing pixels. visual effects image inpainting shown fig. predictive performances shown table case available lack ground-truth. case observe fbcp outperforms methods structural image extremely high missing ratio superiority signiﬁcant additive noise involved. case observe stdc obtains best performance followed fbcp better methods. however stdc severely degraded noise involved obtains performance fbcp visual quality still much better others. indicate additional smooth constraints stdc suitable natural image. case fbcp superior methods followed worse completion methods missing ratio large e.g. falrtc fcsa hardc. achieve similar performances based nuclear norm optimization. geomcg achieves performance comparable cwopt cpnls data complete fails missing ratio becomes high. geomcg requires large number observations precisely deﬁned rank. noted stdc outperforms algorithms except fbcp missing ratio becomes extremely high. results demonstrate fbcp tensor factorization method also effective tensor completion even extremely sparse tensor presented. also conducted additional experiments. reconstruction complete tensor tensor completion noise level high i.e. results experiments presented appendix section applications image inpainting based several benchmark images shown fig. used evaluate compare performance different methods. colorful image represented third-order tensor size conducted various experiments four groups conditions. fig. visual effects image inpainting. seven examples shown bottom facade image missing; facade image missing additive noise; lena image missing; lena image missing additive noise; lena image superimposed text; scribbled lena image; image ocean object. stdc. completion-based methods obtain relatively smoother effects factorization-based methods global color image recovered well resulting poor predictive performance. case fbcp obtains clean image removing object completely ghost effects appear methods. halrtc falrtc fcsa outperform cpwopt cpnls stdc. outperforms completion-based methods image completion. factorization-based methods ability however fbcp signiﬁcantly improves factorization-based automatic model selection robustness overﬁtting resulting potential applications various image inpainting problems. necessary number observed entries mainly depends rank true image. instance structural image intrinsic low-rank need fewer observations natural image. however observed pixels lena image sufﬁcient recover whole image caused low-rank assumption. property common algorithms except stdc stdc employs auxiliary information additional constraints. advantage stdc shown lena image disadvantages auxiliary information must well designed speciﬁc application make difﬁcult applied types data. addition stdc degrades presence non-random missing pixels noise. moreover performance halrtc stdc sensitive tuning parameters must carefully selected speciﬁc condition. therefore crucial drawback completion-based scheme lies tuning parameters whose selection quite challenging ground-truth missing data unknown. next perform image completion extensively eight images fig. randomly missing pixels. since images natural images low-rank approximation cannot recover missing pixels well apply fully bayesian mixture priors comparison fbcp related methods. fbcp fbcp-mp initialization applied cpwopt performed using optimal ranks obtained fbcp fbcp-mp show best performance. parameter selection methods previous experiments. size images table shows quantitative results terms recovery performance runtime. observe fbcp-mp improves performance fbcp signiﬁcantly achieves best recovery performance especially case high missing rate. time costs fbcp fbcp-mp comparable completionbased methods signiﬁcantly lower tensor factorization method. stdc obtains comparable performance fbcp-mp however parameters must manually tuned speciﬁc condition. detailed results image shown visually quantitatively supplemental materials. results demonstrate effectiveness mixture priors advantages local similarity taken account addition low-rank assumption. recognition face images captured surveillance videos ideal solution create robust classiﬁer invariant factors pose illumination. hence arises question whether generate novel facial images multiple conditions given images conditions. tensors highly suitable modeling multifactor image ensemble therefore introduce novel application used dataset basel face model contains ensemble facial images people rendered different poses different illuminations. facial images decimated cropped pixels represented fourth-order tensor size shown fig. images fully missing. since methods either computationally intractable applicable order tensor algorithms ﬁnally applied dataset different missing ratios. initial rank factorization based methods parameters completion based methods well tuned based groundtruth missing images. shown fig. visual effects image synthesis produced fbcp signiﬁcantly superior produced methods. although cpwopt halrtc produce images smooth blurred halrtc obtains much better visual quality cpwopt. detailed performances compared table w.r.t. observed entries reﬂects performance model ﬁtting w.r.t. missing entries particularly reﬂects predictive ability. note =n/a implies halrtc falrtc donot model observed entries. inferred rank fbcp within range close initialization. observe completion based methods including halrtc falrtc hardc. achieve better performance cpwopt. however fbcp demonstrates possibility factorization-based scheme signiﬁcantly outperform completion-based methods especially terms performance missing images. conclusion paper proposed fully bayesian factorization naturally handle incomplete noisy tensor data. employing hierarchical priors unknown parameters derived deterministic model inference fully bayesian treatment. signiﬁcant advantage automatic determination rank. moreover tuning parameter-free approach method avoids parameter selection problem also effectively prevent overﬁtting. addition proposed variant method using mixture priors shows advantages natural images large amount missing pixels. empirical results validate effectiveness terms discovering ground-truth tensor rank imputing missing values extremely sparse tensor. several real-world applications image completion image synthesis demonstrated superiority method state-of-the-art techniques. several interesting properties method would attractive many potential applications. sørensen lathauwer comon icart deneire canonical polyadic decomposition orthogonality constraints siam journal matrix analysis applications accepted sorber barel lathauwer optimizationbased algorithms tensor decompositions canonical polyadic decomposition decomposition rank- terms generalization siam journal optimization vol. signoretto dinh lathauwer suykens learning tensors framework based convex optimization spectral regularization machine learning huang zhang metaxas composite splitting algorithms convex optimization computer vision image understanding vol. paysan knothe amberg romdhani vetter face model pose illumination invariant face recognition advanced video signal based surveillance sixth ieee international conference qibin zhao received ph.d. degree engineering department computer science engineering shanghai jiao tong university shanghai china currently research scientist laboratory advanced brain signal processing riken brain science institute japan. research interests include machine learning tensor factorization computer vision brain computer interface. liqing zhang received ph.d. degree zhongshan university guangzhou china professor department computer science engineering shanghai jiao tong university shanghai china. current research interests cover computational theory cortical networks brain-computer interface perception cognition computing model statistical learning inference. published papers international journals conferences. andrzej cichocki received ph.d. dr.sc. degrees electrical engineering warsaw university technology currently senior team leader laboratory advanced brain signal processing riken brain science institute co-author technical papers monographs associate editor journal neuroscience methods ieee transaction signal processing. silva l.-h. tensor rank ill-posedness best low-rank approximation problem siam journal matrix analysis applications vol. allman jarvis rhodes sumner tensor rank invariants inequalities applications siam journal matrix analysis applications vol. salakhutdinov mnih probabilistic matrix factorization advances neural information processing systems vol. babacan luessi molina katsaggelos sparse bayesian methods low-rank matrix estimation ieee transactions signal processing vol. variational bayesian approach movie rating prediction proceedings workshop lawrence urtasun non-linear matrix factorization gaussian processes proceedings annual international conference machine learning. lakshminarayanan bouchard archambeau robust bayesian matrix factorisation proc. intl. conf. artiﬁcial intelligence statistics denoyer gallinari probabilistic latent tensor factorization model link pattern prediction multirelational networks journal china universities posts telecommunications vol. xiong chen t.-k. huang schneider carbonell temporal collaborative ﬁltering bayesian probabilistic tensor factorization proceedings siam data mining vol. hayashi takenouchi shibata kamiya kato kunieda yamada ikeda exponential family tensor factorization missing-values prediction anomaly detection data mining ieee international conference ieee", "year": 2014}