{"title": "Learning Deep Disentangled Embeddings with the F-Statistic Loss", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Deep-embedding methods aim to discover representations of a domain that make explicit the domain's class structure. Disentangling methods aim to make explicit compositional or factorial structure. We combine these two active but independent lines of research and propose a new paradigm for discovering disentangled representations of class structure; these representations reveal the underlying factors that jointly determine class. We propose and evaluate a novel loss function based on the $F$ statistic, which describes the separation of two or more distributions. By ensuring that distinct classes are well separated on a subset of embedding dimensions, we obtain embeddings that are useful for few-shot learning. By not requiring separation on all dimensions, we encourage the discovery of disentangled representations. Our embedding procedure matches or beats state-of-the-art procedures on deep embeddings, as evaluated by performance on recall@$k$ and few-shot learning tasks. To evaluate alternative approaches on disentangling, we formalize two key properties of a disentangled representation: modularity and explicitness. By these criteria, our procedure yields disentangled representations, whereas traditional procedures fail. The goal of our work is to obtain more interpretable, manipulable, and generalizable deep representations of concepts and categories.", "text": "closer another embedding space animals different species. deep embedding methods trained labeled data labels used solely determine instances different class. paradigm handle arbitrary number classes complete classes speciﬁed advance—as would ordinary classiﬁer—deep embeddings useful few-shot learning given small examples novel class examples projected embedding space unknown instance classiﬁed proximity embeddings labeled examples. typically deep embedding methods evaluated nearestneighbor-like rule classes contained training set. similar deep embeddings literature disentangling attempts discover representations class instances rather making explicit single property instances goal make explicit multiple independent properties refer factors. example disentangled representation animals might include features indicating size length ears whether feet ﬁns. later rigorous deﬁning disentangled representation operate informal notion factors form compositional distributed representation relatively factors relatively values factor factor values recombined span classes. recent research disentangling performed either using fully unsupervised procedure using supervised procedure subset factor values provided training surprisingly little effort made connect research deep embeddings disentangling despite overlapping related goals. work aims merge independent lines research proposing evaluating method obtains class-preserving deep disentangled representations. deep embedding literature goal discover embedding procedure support few-shot learning novel classes. beyond goal teasing apart classes also wish disentangle underlying factors whose conjunctions deﬁne class resulting deep-embedding methods discover representations domain make explicit domain’s class structure. disentangling methods make explicit compositional factorial structure. combine active independent lines research propose paradigm discovering disentangled representations class structure; representations reveal underlying factors jointly determine class. propose evaluate novel loss function based statistic describes separation distributions. ensuring distinct classes well separated subset embedding dimensions obtain embeddings useful few-shot learning. requiring separation dimensions encourage discovery disentangled representations. embedding procedure matches beats state-of-the-art procedures deep embeddings evaluated performance recallk few-shot learning tasks. evaluate alternative approaches disentangling formalize properties disentangled representation modularity explicitness. criteria procedure yields disentangled representations whereas traditional procedures fail. goal work obtain interpretable manipulable generalizable deep representations concepts categories. literature deep embeddings addresses problem discovering representations domain make explicit particular property domain instances. refer property class category identity. example animal images might embedded animals species blending lines research leads paradigm training disentangled representations. previously proposed disentangling procedures extremes supervision either entirely unsupervised requiring factor-aware oracles e.g. oracles name particular factor sets instances either differ factors except factor ordered factor-speciﬁc similarity unsupervised procedures suffer underconstrained; oracle-based procedures require strong supervision. contrast deep-embedding algorithms utilize oracle provides intermediate degree supervision—an oracle speciﬁes whether instances class. algorithms differ number instances among class-equality constraints speciﬁed pairs triplets quadruplets class-equality constraints natural training signal class viewed conjunction factors contrast isolated named factors used disentangling algorithms. although class-equality constraints name individual factors facilitate discovery class-deﬁnitional factors. summarize goal develop deep-embedding algorithm leverages class-equality constraints disentangle factors jointly deﬁne class. evaluate algorithm using deep-embedding objective obtaining representation makes explicit class disentangling objective decomposing class independently varying factors. begin describing challenge constructing embeddings motivates algorithm. describing algorithm showing yields state-of-the-art results recallk task ordinarily used evaluate embeddings turn analyzing well algorithm disentangles factors contribute class identity. perform rigorous evaluation forth formal quantiﬁable criteria disentanglement show algorithm outperforms state-of-the-art embedding procedures achieving criteria. embedding distributed encoding captures class structure metric properties space. illustrate middle panel figure shows projection instances varying classes space. projection separates inputs class therefore facilitates categorization unlabeled instances proximity clusters. embedding also allows classes figure alternative two-dimensional embeddings domain. circles represent instances domain color identiﬁes class. leftmost frame circles superimposed. learned labeled examples projecting examples embedding space. literature somewhat splintered researchers focusing deep embeddings evaluated few-shot learning researchers focusing few-shot learning found deep embeddings useful method figure illustrates fundamental trade formulating embedding. left right frames intra-class variability increases inter-class structure becomes conspicuous. leftmost panel clusters well separated classes equally apart. rightmost panel clusters highly overlapping blue orange cluster centers closer another green. separating clusters desirable capturing inter-class similarity. similarity suppressed instances novel class mapped sensible manner—a manner sensitive input features underlying factors correspondence. middle panel reﬂects compromise discarding variation among instances class preserving relationships among classes. compromise embeddings used model hierarchical class structure facilitate decomposing instances according underlying factors e.g. separating content style trade figure points challenge constructing embeddings. existing methods perfectly separate categories training appropriate labeling errors noisy data. methods require margin parameter determine well separated categories order prevent overﬁtting propose method automatically balances trade using currency probability statistical hypothesis testing. also aligns dimensions embedding space underlying generative factors— categorical semantic features—and thereby facilitates disentangling representations. whereas every deep-embedding method aware uses training criteria based individual instances. example triplet loss attempts ensure speciﬁc triplets closer objectives based speciﬁc instances susceptible noise data prone overﬁtting. example previous section assumed onedimensional embeddings. explored extensions approach many-dimensional embeddings. first assume euclidean distances embedded points gamma distributed—which turns good empirical approximation stage training—then represent numerator denominator statistic sums gamma random variables variant unidimensional separation measure used assess separation based euclidean distances. second apply unidimensional separation measure multiple dimensions many-dimensional embedding space. adopt latter approach because—as explain shortly—it facilitates disentangling. dimension embedding space. select dimensions largest i.e. dimensions best separated already. although important separate classes needn’t separated dimensions pair semantic similarity equivalence along dimensions. pair separated distinguished reliably subset dimensions. training mini-batch multiple instances classes embedding objective maximize joint probability separation class pairs relevant dimensions dαβ. framed loss minimize probability -statistic loss four desirable properties. first gradient rapidly drops zero classes become reliably separated least dimensions leading natural stopping criterion; degree separation obtained related number samples class. property illustrated synthetic two-dimensional data quiver plot figure shows gradients -statistic triplet losses. triplet loss gradients averaged possible triplets batch. represents different batch data. column between-class separation horizontal axis using statistic separate classes expository purposes consider classes instances mapped onedimensional embedding. embedding coordinate instance class denoted zij. goal embedding procedure separate coordinates classes. approach quantify separation probability true class means underlying environment different another. training goal thus formulated minimizing denotes summary statistics labeled embedding points. posterior intractable instead operate likelihood proxy. borrow particular statistic analysis variance hypothesis testing equality means. statistic ratio between-class variability within-class variability expectations null hypothesis additional normality assumption statistic draw fisher-snedecor distribution degrees freedom f˜n. large indicate embeddings different classes well separated relative embeddings class unlikely f˜n. thus distribution offers measure separation classes several comments approach. first although assumes classes equal variance likelihood equation fairly robust inequality variances long second statistic computed arbitrary number classes; generalization likelihood equation conditioned class instances drawn distribution. likelihood weak indicator class separation restrict statistic class pairs. third approach based entirely statistics training classes -statistic equivalent square t-statistic. address potential issue unequal variances explored replacing statistic welch correction statistic found improvement model performance prefer formulating loss terms statistic greater generality. figure quiver plot gradients dimensional space -statistic triplet losses. circle corresponds instance color indicating class. arrows represent gradient direction magnitude computed loss function. separation left high separation right. number samples class. classes reliably separated large between-class distance -statistic gradients drop zero triplet loss continue pushing instances apart. second contrast losses f-statistic loss invariant rotations embedding space; focus separating along speciﬁc dimensions tends yield disentangled features class structure factorial compositional. third embeddings obtained relatively insensitive free parameter fourth loss expressed currency probability readily combined additional losses expressed similarly following sections demonstrate advantages -statistic loss classiﬁcation disentangling attributes related class identity. section demonstrate performance statistic loss compared state-of-the-art deep-embedding losses identity classiﬁcation. ﬁrst task involves matching person wide-angle full-body photograph taken various angles poses. task evaluate using datasets—cuhk market —following methodology ustinova lempitsky second task involves matching bird wide angle photograph; evaluate performance cub-- birds dataset five-fold cross validation performed every case. ﬁrst split used tune model hyper-parameters report accuracy ﬁnal four splits. procedure used evaluate -statistic loss four competitors. training details. cuhk market- deep metric learning architecture following ustinova lempitsky cub- inception network pretrained imagenet extract -dimensional features ﬁnal pooling layer. treat features constants optimize fully connected hidden relu units. every dataset dimensional embedding. nets trained using adam optimizer learning rate losses except f-statistic loss found beneﬁtted slightly higher learning rate split validation withheld training used early stopping. construct mini-batch training randomly select identities samples identity ustinova lempitsky addition -statistic loss evaluated histogram triplet binomial deviance lifted structured similarity softmax losses. triplet loss triplets minibatch. histogram loss binomial deviance losses pairs. -statistic loss class pairs. triplet loss trained evaluated using distances. -statistic loss evaluated using distances. ustinova lempitsky embeddings obtained discovered histogram binomial-deviance losses constrained unit hypersphere; cosine distance used training evaluation. -statistic loss determined best value number dimensions separate using validation ﬁrst split. performance relatively insensitive cuhk chose market- cub- triplet loss found margin worked well datasets. binomial deviance lsss losses table modularity compactness explicitness sixteen codes figure code depicted icon captures class structure embedding space. check mark indicates code satisﬁes given disentangling criterion. depict icon shows code factors modular. individual points represent code particular instance. color denotes value binary factor symbol denotes value second binary factor. horizontal vertical dimensions code ﬁrst second factors respectively. contrast figures present codes factors non-modular. compact representation given factor associated code dimensions. figure shows code single factor compact code. factor four distinct values denoted symbols distinguished along horizontal dimension. contrast figures present codes code dimensions convey information single factor. explicit representation value given factor precisely determined code. eight lower panels figure show noisy versions codes eight upper panels. scattering points code permit recover value every factor every observation. thus code fail criterion explicitness code-conditional entropy factor nonzero. however mutual information code factors aspect explicitness. compare codes figures factor whose values distinguished symbols recover factor values code using linear separator; however linear separator sufﬁcient recover factor values code although mutual information factor code factor explicit representation code less computation required recover factor values. amount used best settings dataset determined ustinova lempitsky results. embedding procedures typically evaluated either recallk few-shot learning paradigm. evaluations similar using held-out classes instances class projected embedding space performance judged proximity query instance references embedding space. evaluate recall -nearest neighbor judges query instance correctly classiﬁed closest reference class. equivalent q-shot learning evaluation; data sets ranged table reports recall accuracy. overall -statistic loss achieves accuracy comparable best competitors obtains best result cuhk ties market- tier best cub-. demonstrated -statistic loss state-of-the-art terms producing domain embeddings cluster instances class remainder paper argue -statistic loss also leads disentangled embeddings. disentangling based premise underlying factors responsible generating observed instances. observation vectors typically high dimensional redundant noisy vector element depends value multiple factors. goal disentangling procedure transform observation vector recover causal factors code vector. term code synonymous embedding prefer ‘code’ section emphasize focus disentangling. notion constitutes ideal code somewhat debate authors preferring avoid explicit deﬁnitions others conﬂicting notions explicit comprehensive deﬁnition disentangling based three criteria. elaborate criteria referring terminology modularity compactness explicitness. section illustrate criteria using sample codes order compare criteria one’s intuitions disentangled representations. modular representation dimension code conveys information factor. consequently code dimensions partitioned factor associated single partition code partition invariant factors. figure figure recall results -statistic loss four competitors across three data sets. shown percentage correct classiﬁcation standard error mean. best algorithm given data highlighted. whether satisfy three criteria. explicitness require linear separability. examples criteria pretty much satisﬁed course specify measures quantify degree criterion satisﬁed. examples factors categorical values—either values. modularity compactness criteria apply directly continuous-valued factors explicitness criterion requires specify function recovers continuous factor value e.g. linear function researchers previously attempted quantify disentangling considered different subsets modularity compactness explicitness criteria. eastwood williams three included; mnih modularity compactness included explicitness; higgins modularity included compactness explicitness. argue modularity explicitness considered deﬁning features disentangled representations compactness. although compactness facilitates interpretation representations signiﬁcant drawbacks. first forcing compactness affect representation’s utility. consider factor determines orientation object image. encoding orientation dimensions captures natural similarity structure orientations compact relative using code. second forcing neural network discover minimal code lead local optima training solution space highly constrained; allowing redundancy code enables many equivalent solutions. section quantify modularity explicitness order evaluate disentangling performance deepembedding procedure. modularity start estimating mutual information code dimension computation required course depends computational primitives disentangling literature appears implicit hierarchy simplicity axis-aligned linear discriminant simpler operator linear discriminant function code dimensions turn simpler conjunction linear discriminant functions etc. present another example might argue explicit codes figures classiﬁer. evaluate explicitness code using distribution values factor value. finally show -statistic loss discovers disentangled representations class utilizing weak supervisory signal common deep-embedding algorithms oracle speciﬁes whether instances different class. explore variants supervision conjunction factors determines similarity call conjunctive-factors training. corresponds closely supervision used work handle case factors discrete values codes continuous values. discretize code histogramming code values equal width bins compute discrete mutual information factor-values code histogram. deep-embedding algorithms. also explore variation similarity determined factor time call uni-factor training. training related existing supervised disentangling methods though weaker factor used determine similarity revealed algorithm. strength supervision training therefore falls supervision used deep-embedding algorithms supervised disentangling. show method training lead representations modular achieve explicitness comparable representations learned state-of-the-art deep-embedding algorithms. explore datasets instance tagged values several statistically independent factors. factors treated class-related noise. first train data video game sprites— pixel color images game characters viewed various angles variety poses identity game characters composed factors—body arms hair gender armor greaves weapon—each distinct values leading total unique identities instantiated various viewing angles poses. also explore small norb dataset dataset composed pixel grayscale images toys various poses lighting conditions. high-level categories types total types toys. imaged camera elevations azimuths lighting conditions. experiments deﬁne identity terms type pose treat lighting condition noise variable. simplicity evaluation partition values elevation azimuth create binary factors grouping elevation high buckets azimuth values right left- facing buckets leading total unique identities. training details. sprites dataset used encoder architecture reed well embedding dimensionality small norb convolutional network convolutional layers ﬁnal fully connected layer embedding dimensionality convolutional layers ﬁlter sizes ﬁlter counts stride relu activation. -statistic loss number training dimensions again nets trained using adam optimizer learning rates used classiﬁcation datasets. construct minibatches section uni-factor case select instances similarity determined single factor construct minibatch. epoch iterate factors trained every instance respect every factor. minibatch composed identities case conjunctive-factors factor-values case uni-factor training. cases train instances class except -statistic loss train instances class avoid underﬁtting. datasets evaluated ﬁve-fold cross validation splitting identity including non-identity variations. split validation used determine stop training based recall performance conjunctive-factors mean factor explicitness uni-factor training. ﬁrst split used tune hyperparameters test sets remaining four splits used report results. experiments compare -statistic loss triplet histogram losses; losses using norm cosine distances yield similar results. results. figure shows distributions modularity explicitness scores representations learned sprites small norb datasets using triplet histogram -statistic losses uni-factor conjunctive-factors training settings explicitness report one-versus-rest classiﬁcation factor-value four cross validation splits. sprites dataset factors total factor-values distributions figure contain total aucs. small norb types binary pose factors total factor-values split total aucs. report modularity score embedding dimension cross validation split total modularity scores sprites small norb. dataset models good modularity scores. case dimensions high mutual information factor mutual information factors. models good modularity overall poor explicitness pose factors. combinatorics factors. types values elevation azimuth. random pair identities selected training chance differing type chance differing azimuth elevation. conjunctivefactors case type naturally over-emphasized training. uni-factor case balance training factor problem arise. -statistic loss combines ideas deepembedding disentangling literatures discover disentangled representations class. representations factors jointly deﬁne class structure modular explicit makes representations particularly well suited interpretation manipulation used generalization—to tasks classes. loss achieves state-of-the-art performance recall task used evaluate deep-embedding few-shot learning methods surpasses methods obtaining disentangled representations. contributions include -statistic loss itself analyses specify desiderata deep embeddings disentangling. current research focuses methods adaptively estimating hyper-parameter governing number training dimensions training. presently determines loss behavior pairs classes must references chen duan houthooft rein schulman john sutskever ilya abbeel pieter. infogan interpretable representation learning information maximizing generative adversarial nets. advances neural information processing systems chopra hadsell lecun. learning similiarty metric discriminatively application face veriﬁcation. proceedings ieee conference computer vision pattern recognition higgins irina matthey loic arka burgess christopher glorot xavier botvinick matthew mohamed shakir lerchner alexander. beta-vae learning basic visual concepts constrained variational framework. iclr kingma diederik mohamed shakir rezende danilo jimenez welling max. semi-supervised learning deep generative models. advances neural information processing systems kulkarni tejas whitney william kohli pushmeet tenenbaum josh. deep convolutional inverse graphics network. advances neural information processing systems lecun yann huang bottou leon. learning methods generic object recognition invariance pose lighting. computer vision pattern recognition cvpr proceedings ieee computer society conference volume ii–. ieee zhao xiao tong wang xiaogang. deepreid deep ﬁlter pairing neural network person reidentiﬁcation. proceedings ieee conference computer vision pattern recognition reed scott sohn kihyuk zhang yuting honglak. learning disentangle factors variation manifold interaction. proceedings international conference machine learning schroff florian kalenichenko dmitry philbin james. facenet uniﬁed embedding face recognition clustering. proceedings ieee conference computer vision pattern recognition snell jake swersky kevin zemel richard. prototypical networks few-shot learning. luxburg guyon bengio wallach fergus vishwanathan garnett advances neural information processing systems xxx–xxx. curran associates inc. song hyun jegelka stefanie savarese silvio. deep metric learning lifted structured feature embedding query retrieval. proceedings ieee conference computer vision pattern recognition szegedy christian vanhoucke vincent ioffe sergey shlens wojna zbigniew. rethinking inception architecture computer vision. proceedings ieee conference computer vision pattern recognition triantaﬁllou zemel urtasan few-shot learning information retrieval lens. luxburg guyon bengio wallach fergus vishwanathan garnett advances neural information processing systems xxx– xxx. curran associates inc. veit andreas belongie serge karaletsos theofanis. disentangling nonlinear perceptual embeddings multi-query triplet networks. arxiv preprint http//arxiv.org/abs/.. zheng liang shen liyue tian wang shengjin wang jingdong tian scalable person reidentiﬁcation benchmark. proceedings ieee international conference computer vision", "year": 2018}