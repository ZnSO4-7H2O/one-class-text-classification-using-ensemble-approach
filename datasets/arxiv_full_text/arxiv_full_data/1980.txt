{"title": "Sparse Factorization Layers for Neural Networks with Limited Supervision", "tag": ["cs.CV", "cs.AI", "stat.ML"], "abstract": "Whereas CNNs have demonstrated immense progress in many vision problems, they suffer from a dependence on monumental amounts of labeled training data. On the other hand, dictionary learning does not scale to the size of problems that CNNs can handle, despite being very effective at low-level vision tasks such as denoising and inpainting. Recently, interest has grown in adapting dictionary learning methods for supervised tasks such as classification and inverse problems. We propose two new network layers that are based on dictionary learning: a sparse factorization layer and a convolutional sparse factorization layer, analogous to fully-connected and convolutional layers, respectively. Using our derivations, these layers can be dropped in to existing CNNs, trained together in an end-to-end fashion with back-propagation, and leverage semisupervision in ways classical CNNs cannot. We experimentally compare networks with these two new layers against a baseline CNN. Our results demonstrate that networks with either of the sparse factorization layers are able to outperform classical CNNs when supervised data are few. They also show performance improvements in certain tasks when compared to the CNN with no sparse factorization layers with the same exact number of parameters.", "text": "whereas cnns demonstrated immense progress many vision problems suffer dependence monumental amounts labeled training data. hand dictionary learning scale size problems cnns handle despite effective low-level vision tasks denoising inpainting. recently interest grown adapting dictionary learning methods supervised tasks classiﬁcation inverse problems. propose network layers based dictionary learning sparse factorization layer convolutional sparse factorization layer analogous fully-connected convolutional layers respectively. using derivations layers dropped existing cnns trained together end-to-end fashion back-propagation leverage semisupervision ways classical cnns cannot. experimentally compare networks layers baseline cnn. results demonstrate networks either sparse factorization layers able outperform classical cnns supervised data few. also show performance improvements certain tasks compared sparse factorization layers exact number parameters. artiﬁcial neural networks especially convolutional neural networks proven powerful ﬂexible models variety complex tasks particularly computer vision composing sequences simple operations complex structures subsequently estimating network parameters gradient backpropagation able learn representations complex phenomena arguably better contemporary alternatives. make seem like black-art work first indeed complex structures hence resist theoretical analysis known results little explain actually learned improve certain network. second complex networks millions parameters hence require large amount supervised data learn coco imagenet youtube-m take many person-years build. need limits artiﬁcial neural network extensibility situations without large-scale data. although efforts toward self-supervision potential mitigate issue begun abundant supervision remains core limitation. contrary sparse representation models like dictionary learning elastic problem afford strong theoretical analysis indeed often rigorous theoretical analysis resulting analytical conﬁdence used. potentially better matched computer vision many inherently sparse problems. furthermore structure tend need signiﬁcantly less training data artiﬁcial neural networks. although seen success vision generative discriminative forms high-levels able keep pace cnns largescale high-dimensional problems strong contrast artiﬁcial networks sparse representations foundation work. seek bridge modeling approaches yielding representation remains general better scalability classical sparse representation learning time requires less labeled data modern artiﬁcial neural networks. indeed forging bridge considered others already. example greedy deep dictionary learning sequentially encodes data series sparse factorizations. interesting approach neither endto-end afford supervised loss function limiting potential many vision problems. convolutional sparse coding derives convolution form sparse representations known embed artiﬁcial neural networks. hand rectiﬁed linear units induce sparsity activation improved performance. however provide means tuning level sparsity. summarily although work forging bridge approach aware successfully uniﬁes sparse representation modeling artiﬁcial neural networks controllable level sparsity capability move supervised unsupervised loss paper take step toward bridging disparate ﬁelds. core novelty work network layers sparse representation learning dictionary learning directly within network. call sparse factorization layers indeed factorize input sparse reconstruction coefﬁcients passed onward later layers network—sparse activation. propose sparse factorization layer learns dictionary input terms analogous fully connected layer convolutional sparse factorization layer slides dictionary input analogous convolutional layer cnn. show compute back-propagation gradients parameters layers leveraging theoretical results local linearity dictionaries hence layers incorporated trained directly within modern networks. furthermore propose semisupervised loss leverages generative nature sparse factorization layers. main contribution paper sparse factorization layers plugged modern convolutional neural networks trained end-to-end manner facilitate tunable activation sparsity. secondary contribution paper semisupervised loss function combines generative nature sparse factorization layers discriminative convolutional network. demonstrate empirical utility layers replacements traditional layers. beyond scope paper evaluate stated goals bridging disparate ﬁelds. focus sparse factorization layers limited data comparison classical convolutional neural network. digit classiﬁcation problem mnist comparable networks layers replacing analogs original perform cnn. however reduce amount training data original size performance models higher sparse factorization convolutional sparse factorization layers respectively. expected relative performance improvement sparse factorization layers decreases respect performance amount training data increases. also show marked improvement performance mnist variations dataset. section full results. ﬁrst focus discussion methods seeking bridge artiﬁcial neural networks sparse representation modeling. greedy deep dictionary learning involves layered generative architecture seeks encode inputs series sparse factorizations trained individually reconstruct input faithfully. however mechanism end-to-end training unsupervised limiting potential various tasks like classiﬁcation given modern performance statistics. sparse autoencoders using linear operations sparsifying transforms place sparse factorization. although models trained end-to-end manner aware work incorporates supervised semisupervised tasks. contrast layer propose subsumes capabilities trained end-to-end embedded supervised semisupervised unsupervised networks. recent work truly convolutional sparse coding strives produce sparse representation image encoding image spatially whole instead isolated patches standard incorporating sparse representations image analysis. method similar proposed convolutional sparse factorization layer although embed sparse coding operation convolution convolutional spirit choosing sparse factorization approach every patch image. although work promise could potentially even used extend ideas paper aware result indicating incorporating artiﬁcial neural networks like methods. lastly hierarchical multi-layered sparse coding images performed greedy fashion end-to-end fashion. however typically involve heuristics lack principled calculus embed within artiﬁcial neural network framework. hence lack compatibility traditional components. contrast layers dropped-in various artiﬁcial neural networks trained architecture back-propagation. sparsity cnns relate work past methods directly induce sparsity artiﬁcial neural networks. various methods indirectly induced sparsity parameters activations incorporating sparsityinducing penalties network loss functions. techniques supervised network loss function compares ﬁnal activation supervision information input measure example classiﬁcation correctness measure output quality. chain rule employed calculate gradient loss respect intermediate activation calculated gradient descent network parameters used shrink overall loss. denotes supervised loss; later incorporate unsupervised loss formulation denote noted networks limited single chain operations restrict discussion simplicity without loss generality. additionally notation purely symbolic parameters scalar vector matrix collection thereof—or missing altogether. sparse representation models like dictionary learning decompose factorize signal linear combination elements overcomplete basis commonly called dictionary. contrast common layers neural networks linear transform fullyconnected layer compose output parameter matrix input. concretely consider linear transform layer like dropout dropconnect artiﬁcially simulate activation sparsity inference prevent redundancy improve performance. glorot argue rectiﬁed linear unit sparsifying activation function improve performance cnns. works primarily limiting sensitivity activations small changes input thereby forming robust error-tolerant network. however methods either lack theoretical basis offer means controlling activations’ sparsity and/or require nonzero activations positive unlike work. rest paper organized follows. section notational modeling foundation paper. section introduces sparse factorization sparse factorization network layers. section describes experimental evaluation work ﬁnally section summarizes work discusses limitations future directions. background notation made effort maintain consistent notation throughout paper sometimes expense deviating common notation existing literature especially dictionary learning. bold lower-case letters represent tensors typically vectors—these e.g. inputs outputs neural network layers sparse representation coefﬁcients. bold upper-case symbols denote tensors operators weight matrices neural networks dictionaries sparse representations greek letters used scalars. convolutional neural networks artiﬁcial neural networks systems constructed composition potentially many simple differentiable operations. denote input network generally tensor order image. ﬁrst operation layer denoted takes input parameters produce output activation serves input second layer. process repeats reach activation layer network readability term activation refer output layer network layers commonly called activation functions like sigmoid tanh layers. despite enormous ﬂexibility models small variety layers commonly used; notably linear transform layers vector parameter matrix convolution layers tensor kernel bank offset parameter-free transforms like local pooling rectiﬁcation sigmoid activation. cnns networks convolution layers. lenet included figure also describe semi-supervised loss leverages generative nature sparse representations regularize classiﬁcation loss section describe results experiments using novel sparse factorization layers demonstrate large gain performance limited training data. sparse factorization layer directly implements sparse representation idea layer plugged artiﬁcial neural network. parameter-forparameter layer replace linear transform fully-connected layer current networks forward computation straightforward explain backward computation leads estimation parameter matrix within gradient descent optimization regime. concretely layer parameterized dictionary rm×k assume known forward pass. dictionary parameter matrix represents overcomplete basis factorize input output activation sparse correctly chosen values -regularizer. again instance elastic-net problem solved least-angle regression completing forward pass layer. backpropagation need compute gradient loss respect previous layer’s activation parameter matrix using transferred gradient ∇ail layer possible forward computation procedure differentiable; mairal prove differentiability optimization mild conditions compute gradients albeit problem task-speciﬁc dictionary learning. present results practical interest layer computation details section figure visual comparison linear transform operation sparse factorization operation layers propose paper able induce sparse activation leveraging ideas sparse representation modeling dictionary learning. denotes matrix multiplication. -norm sparsity-inducing regularizer penalty numerical stability optimization algorithm terms scalar weights them. formulation referred elastic-net problem variant many sparse representation formulations; consideration general. careful inspection indicates linear transform takes dense combination columns parameter matrix weighted input outputs results. however sparse representation instead computes best input respect sparse combination columns parameter matrix returns weights sparse combination. essence factorizes function leading sparse activations; hence name methods. figure provides illustration comparison. sparse activations fundamentally different common sparsity inducing regularization used current neural network training difference leads neural network layers based sparse representation modeling yielding sparse activations strong theoretical guarantees promise training less labeled data. indeed work builds large body work sparse representation modeling dictionary learning prevalent computer vision next show create novel artiﬁcial neural network layers based core idea sparse factorization well strong theoretical foundation based examples layers comparison baseline network based figure illustrative comparison role sparse factorization layers provide classical network architecture suitable digit classiﬁcation problem like mnist. three networks show data layer bottom ﬁnal activation top; computations drawn black activation tensors drawn blue. sparse factorization layer replaces fully connected layer subsequent rectiﬁed linear unit layer network layers. convolutional sparse factorization network replaces ﬁrst convolution layer layer. things equal comparable. gradient ∇ai−ls backpropagated lower layers network gradient parameters ∇pils used update dictionary sparse factorization layer. note every gradient step must normalized element -norm greater without this could scale large constant reducing regularization penalties arbitrarily without improving quality sparse representation. sparse representations success patch-based models likewise convolution layer artiﬁcial neural networks achieved great success translation invariance induced approaches particularly well-suited image-based problems. considered cases linear bilinear though derivation holds differentiable critical work become clear below. compared supervision function task deﬁned minimization thereof. prove differentiable input probability distribution compact support expected satisﬁed operating conditions. given proof compute gradients required update using descent algorithm; similarly relied proof derive gradients eqs. although originally developed compressed sensing take view onto framework shallow artiﬁcial neural network. detail forward pass shallow network below using established convention relating shallow network layers larger networks last process remainder network inserted layers noting proof holds differentiable drop linear layer replace whatever inserted layers without impacting gradient layers way. also separated derivation backpropagation gradient splitting operations three steps resulting trivially fundamental difference sparse factorization layers traditional neural networks counterparts generative nature sparse factorization. consider extended optimization include parameters addition sparse factorization underset-subscript indicate sample index avoid confusion layer indices earlier notation regularization dictionary. immediately notice goal dictionary facilitates hence generalize layer operate convolutionally image patches. layer convolutional sparse factorization layer performs factorization locally rectangular input regions much like convolutional layers perform localized linear transforms. consider layer takes images input performs sparse factorization overlapping patches size input image layer rhqwqc×k layer dictionary. denote patch image layer deﬁned patch computed forward inference. sparse factorization activation vectors analogous localized patch response bank kernels traditional cnns. arranged output tensor–exactly size traditional convolution layer. however note call convolutional layer convolutional spirit patch location solving sparse factorization kernel uses parameter matrix everywhere. deﬁned layer drop-in replacement convolutional layers. backpropagation given output gradients ails computed patch allows compute patch gradients local dictionary current layer activation respectively adapting eqs. linearity convolution simply gradients patches yield layer gradients used update parameter passed lower layer. theoretical basis incorporate sparse factorization convolutional sparse factorization layers within end-to-end backpropagation-based training respective forward operations need differentiable need compute analytical derivatives. review pertinent results mairal serve theoretical basis layers. prove differentiability sparse factorization operation used outline optimization framework allows sparse coding used variety tasks. framework contains three stages vector input ﬁrst multiplied linear transform matrix rp×m; given dictionary rp×k sparse code resulting vector computed elastic function applied sparse output parameters mairal purely supervised training step-down schedule similar proposed mairal except piecewise constant-inverse learning rate schedule favoring instead stochastic gradient descent momentum evaluate layers classic vision problem allowing thoroughly inspect various aspects layers comparison well-studied baseline network. although expect layers positively impact vision problems especially limited training data restrict scope study classiﬁcation moderate scale. recall figure visualizes three core networks compare. baseline model curtailed version lenet- comprising convolution layers linear layers denoted lenet. variants follows replaces ﬁrst convolution layer layer; replaces ﬁrst linear layer layer removes rectiﬁed linear unit layer; csf+sf makes modiﬁcations. things networks equivalent including total number parameters. implementation networks implemented matconvnet trained using stochastic gradient descent momentum. forward computation layers performed spams sparse modeling library please contact authors access source code. digit classiﬁcation performance ﬁrst compare layers traditional methods task digit classiﬁcation. table shows networks’ classiﬁcation accuracy mnist handwritten digit classiﬁcation dataset; mnist-rot modiﬁed mnist digit rotated randomly; mnist-rand contains random noise background every image; mnist-img superimposes written digits randomly selected image patches. original mnist training images testing variants training testing. tasks layer. intuitive especially mnist-rand mnist-img datasets—on outperformed baseline most—given rich history natural-image denoising patch reconstruction sparse representation models performance layer volatile. could unpredictable structure network’s intermediate activations readily admit sparse representation jeopardizing stability layer’s output. could high ﬁdelity reconstruction training data. goal unsupervised contrast supervised goals commonly used train convolutional neural networks. note relation auto-encoders also generative; however layer deﬁnitions follow traditional linear transforms already discussed. course directly solve non-convex optimization solve instead backpropagation using earlier derivations. however incorporate goal seeking dictionary enables high ﬁdelity reconstructions data similar multi-task regularization deﬁne unsupervised loss used regularize layer parameter indicates activation forward pass layer solving respectively. loss summed samples batch. gradient associated task well known dictionary learning literature though sophisticated optimization methods typically preferred learning purely unsupervised dictionaries. expression given note sample activation used calculate unsupervised loss need sample activation used supervised loss need corresponding supervision allowing weakly annotated samples. finally layering sparse factorization layers larger neural network structures need propagate gradient unsupervised loss past dictionary learning end-to-end regime. gradient explore original hypothesis cnns sparse factorization layers perform comparatively better traditional cnns face limited supervised data also evaluate layers’ performance small training data sets. varying number training samples observe network deals severely limited available supervision. data point figure mean several trials method trial trained unique subset data. networks containing sparse factorization layers outperform baseline limited training data accuracy widening data becomes scarcer suggesting resistant overﬁtting. supervision scarce networks outperform lenet network respectively. surprisingly layer outperforms baseline network despite underperforming trained full dataset shown table artifact na¨ıve network design indication structure sparse factorization problem inherently amenable learning low-supervision settings. paper presented novel layers based sparse representational modeling contrast to–and tandem with–more traditional compositional layers. layers sparse factorization convolutional sparse factorization layers analogous fully-connected convolutional layers traditional cnns similarly able dropped existing networks. trained end-to-end back-propagation well also trained semisupervised loss given origins unsupervised learning. experiments clearly demonstrate potential networks layers train strong networks limited labeled data. figure classiﬁcation accuracy baseline lenet networks limited amounts training images averaged several trials. error bars indicate standard deviation. limitations future work much remains explored layers. solving optimization problem every forward pass expensive compared traditional units implementations typically exhibited training speeds times slower pure postulate offset pretraining; since factorization layers capable producing propagating unsupervised loss trained wholly unsupervised fashion initialize network parameters. factorization layers also present unique challenges designing network hope address future work choosing placement width sparsity hyperparameters layers observed non-negligible effect.", "year": 2016}