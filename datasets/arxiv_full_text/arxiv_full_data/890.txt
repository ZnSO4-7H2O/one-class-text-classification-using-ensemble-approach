{"title": "The power of deeper networks for expressing natural functions", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "It is well-known that neural networks are universal approximators, but that deeper networks tend to be much more efficient than shallow ones. We shed light on this by proving that the total number of neurons $m$ required to approximate natural classes of multivariate polynomials of $n$ variables grows only linearly with $n$ for deep neural networks, but grows exponentially when merely a single hidden layer is allowed. We also provide evidence that when the number of hidden layers is increased from $1$ to $k$, the neuron requirement grows exponentially not with $n$ but with $n^{1/k}$, suggesting that the minimum number of layers required for computational tractability grows only logarithmically with $n$.", "text": "well-known neural networks universal approximators deeper networks tend much eﬃcient shallow ones. shed light proving total number neurons required approximate natural classes multivariate polynomials variables grows linearly deep neural networks grows exponentially merely single hidden layer allowed. also provide evidence number hidden layers increased neuron requirement grows exponentially suggesting minimum number layers required computational tractability grows logarithmically deep learning lately shown powerful tool wide range problems image segmentation machine translation. despite success many techniques developed practitioners artiﬁcial neural networks heuristics without theoretical guarantees. perhaps notably power feedforward networks many layers fully explained. goal paper shed light question suggest heuristics deep deep enough. well-known nonlinear neural networks single hidden layer approximate function reasonable assumptions possible networks required extremely large. recent authors shown functions approximated deeper networks much eﬃciently shallower ones. however many functions question complicated arise existence proofs without explicit constructions results often apply types network rarely used practice. deeper networks shown greater representational power respect various notions complexity including piecewise linear decision boundaries topological invariants recently poole raghu showed trajectories input variables attain exponentially greater length curvature greater network depth. work including shows exist functions require exponential width approximated shallow network. mhaskar liao poggio considering compositional functions property inquire whether explicit examples must pathologically complicated. network types standard feedforward model. problem also posed sum-product networks restricted boltzmann machines cohen sharir shashua showed using tools tensor decomposition shallow arithmetic circuits express measure-zero functions expressible deep circuits. weak generalization result convolutional neural networks shown summary recent years seen wide variety theoretical demonstrations power deep neural networks. important timely extend work make concrete actionable deriving resource requirements approximating natural classes functions using today’s common neural network architectures. tegmark rolnick recently proved exponentially eﬃcient deep network shallow network approximating product input variables. present paper greatly extend results include broad natural classes multivariate polynomials tackle question resource depends precise number layers. results apply standard feedforward anns general nonlinearities borne empirical tests. consider complexity approximating multivariate polynomial using feedforward neural network input show general sparse exponentially many neurons required network shallow linearly many deep network. monomials calculate exactly minimum number neurons required shallow network. theorems apply nonlinear activation functions nonzero taylor coeﬃcients; slightly weaker result holds even broader class following theorem generalizes result tegmark rolnick arbitrary monomials. setting below recover result product numbers requires neurons shallow network done linearly many neurons deep network. hidden layers. input dimension show products approximated number neurons exponential justify theoretical predictions empirical results. still exponential shows problems unsolvable shallow networks tractable even modest size. finally ward neural networks prior work complexity boolean circuits. conclude problems independent therefore established hard problems boolean complexity provide obstacle analogous results standard deep neural networks. nonlinear function positive integer multivariate polynomial degree deﬁne minimum number neurons required approximate neural hidden layers nonlinearity error approximation degree least input variables. thus particular minimal integer that note approximation degree allows approximate polynomial high precision long input variables small enough. particular homogeneous polynomials degree adjust order approximate degree-n polynomial eﬀectively single hidden layer must naturally assume nonzero taylor coeﬃcient. however wish make assumptions taylor coeﬃcients still prove following weaker result proof outline. ﬁrst statement follows proof theorem ii.. second consider equation left-hand side written linear combination basis functions form letting vary multisets ﬁxed size right-hand side attains every degree-s monomial divides number monomials coeﬃcient term polynomial since monomials linearly independent conclude number basis functions form must least least picking maximize gives desired result. natural consider cost approximating general polynomials. however without constraint relatively uninstructive polynomials degree variables live within space diproof outline. proof theorem relied upon fact nonzero partial derivatives monomial linearly independent. fact true general polynomials however exactly similar argument shows least number linearly independent partial derivatives taken respect multisets input variables. multivariate polynomials depth oﬀer exponential savings approximating univariate polynomials. show shallow network approximate degree-d univariate polynomial number neurons linear monomial requires neurons shallow network approximated logarithmically many neurons deep network. thus depth allows reduce networks linear logarithmic size multivariate polynomials exponential linear. diﬀerence arises dimensionality space univariate degree-d polynomials linear dimensionality space multivariate degree-d polynomials exponential invertible rows must linearly independent polynomials restricted terms degree must linearly independent. since space degree-d univariate polynomials dimensional linearly independent polynomials various results known symmetric rank tensors complex numbers notably alexander hirschowitz showed highly non-trivial proof symmetric rank generic symmetric tensor dimension order equals following construction square gate squares output preceding square gate yielding inductively result form depth layer. writing binary product gate k−-place; product gate multiplies output preceding product gate output preceding square gate. k−place identity gate instead product gate. thus layer computes computation k−-place process stops product gate outputs conclude section noting interesting connections value established hard problem tensor decomposition. speciﬁcally show minimum number neurons required approximate polynomial equals symmetric tensor rank tensor constructed coeﬃcients consider scales interpolating exponential linear practice networks modest eﬀective representing natural functions. explain theoretically showing cost approximating product polynomial drops rapidly increases. repeated application shallow network construction tegmark rolnick obtain following upper bound conjecture essentially tight. approach reminiscent tree-like network architectures discussed e.g. groups input variables recursively processed successive layers. proof. construct network groups inputs recursively multiplied. inputs ﬁrst divided groups size group multiplied ﬁrst hidden layer using neurons thus ﬁrst hidden layer includes total bn/b neurons. gives values multiply turn divided groups size group multiplied second hidden layer using neurons. thus second hidden layer includes total neurons. experiments used feedforward networks dense connections successive layers nonlinearities instantiated hyperbolic tangent function. similar results also obtained rectiﬁed linear units nonlinearity despite fact function satisfy hypothesis everydiﬀerentiable. number layers varied number neurons within single layer. networks trained using adadelta optimizer minimize absolute value diﬀerence predicted actual values. input variables drawn uniformly random interval expected value output would manageable size. interesting consider results inapproximability simple polynomials polynomial-size neural networks compare results boolean circuits. recall deﬁned problems solved boolean circuit constant depth polynomial size circuit allowed majority gates arbitrary fan-in. open problem whether equals class problems solvable circuits depth logarithmic size input. section consider feasibility strong general no-ﬂattening results. would interesting could show general polynomials variables require superpolynomial number neurons approximate constant number hidden layers. result might seem address questions whether equal. however boolean circuits compute using values neurons artiﬁcial neural networks take arbitrary real values. preserve values neurons restrict inputs values take nonlinear activation heaviside step function follows work artiﬁcial neural nets constant depth polynomial size exactly power circuits. weighted threshold gates simulate simulated constant-depth polynomial-size circuits majority gates. following simple constructions four types gates using weighted gate evaluates true inputs gate evaluates true inputs gate evaluates true input false vice versa majority gate evaluates true least half inputs thus hope easily prove general noﬂattening results boolean functions case polynomials real-valued variables tractable. simply approximating real value boolean circuit requires arbitrarily many bits. therefore performing direct computations real values clearly intractable circuits. moreover related work already proven gaps expressivity real-valued neural networks diﬀerent depths analogous results remain unknown boolean circuits. shown power deeper anns quantiﬁed even simple polynomials. proven exponential width shallow deep networks required approximating given sparse polynomial. variables shallow network requires size exponential deep network requires linearly many neurons. networks constant number hidden layers appear interpolate extremes following curve exponential n/k. suggests rough heuristic number layers required approximating simple functions neural networks. example want layers neurons minimum number layers required grows convolutional neural nets. neuron layer assumed connected small subset neurons previous layer rather entirety fact showed exist natural functions computed linear number neurons neuron connected neurons preceding layer nonetheless cannot computed fewer exponentially many neurons single layer matter connections used. construction also easily framed reference properties mentioned sharing pooling paper focused exclusively resources required compute given function. important complementary challenge quantify resources required learn computation i.e. converge appropriate weights using training data possibly ﬁxed amount thereof suggested simple functions computed polynomial resources require exponential resources learn quite possible architectures considered increase feasibility learning. example residual networks unitary nets powerful representational ability conventional networks size less susceptible vanishing/exploding gradient problem easier optimize practice. look forward future work help understand power neural networks learn. work supported foundational questions institute http//fqxi.org/ rothberg family fund cognitive science grant thank scott aaronson surya ganguli david budden henry helpful discussions suggestions. delalleau bengio advances neural information processing systems martens chattopadhya pitassi zemel advances neural information processing systems", "year": 2017}