{"title": "An Information-Theoretic Framework for Fast and Robust Unsupervised  Learning via Neural Population Infomax", "tag": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "q-bio.NC", "stat.ML"], "abstract": "A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.", "text": "framework presented unsupervised learning representations based infomax principle large-scale neural populations. asymptotic approximation shannon’s mutual information large neural population demonstrate good initial approximation global information-theoretic optimum obtained hierarchical infomax method. starting initial solution efﬁcient algorithm based gradient descent ﬁnal objective function proposed learn representations input datasets method works complete overcomplete undercomplete bases. conﬁrmed numerical experiments method robust highly efﬁcient extracting salient features input datasets. compared main existing methods algorithm distinct advantage training speed robustness unsupervised representation learning. furthermore proposed method easily extended supervised unsupervised model training deep structure networks. discover unknown structures data task machine learning. learning good representations observed data important clearer description help reveal underlying structures. representation learning drawn considerable attention recent years category algorithms unsupervised learning representations based probabilistic models maximum likelihood estimation maximum posteriori probability estimation related methods. another category algorithms based reconstruction error generative criterion objective functions usually involve squared errors additional constraints. sometimes reconstruction error generative criterion also probabilistic interpretation shannon’s information theory powerful tool description stochastic systems could utilized provide characterization good representations however computational difﬁculties associated shannon’s mutual information hindered wider applications. monte carlo sampling convergent method estimating arbitrary accuracy computational inefﬁciency makes unsuitable difﬁcult optimization problems especially cases high-dimensional input stimuli large population networks. bell sejnowski directly applied infomax approach independent component analysis data independent non-gaussian components assuming additive noise method requires number outputs equal number inputs. extensions overcomplete undercomplete bases incur increased algorithm complexity difﬁculty learning parameters since shannon closely related algorithms representation learning based probabilistic models amenable information-theoretic treatment. representation learning based reconstruction error could accommodated also information theory inverse fisher information cram´er-rao lower bound mean square decoding error unbiased decoder hence minimizing reconstruction error potentially maximizes lower bound related problems arise also neuroscience. long suggested real nervous systems might approach information-theoretic optimum neural coding computation however cerebral cortex number neurons huge neurons square millimeter cortical surface often computationally intractable precisely characterize information coding processing large neural populations. address issues present framework unsupervised learning representations large-scale nonlinear feedforward model based infomax principle realistic biological constraints neuron models poisson spikes. first adopt objective function based asymptotic formula large population limit stimuli neural population responses since objective function usually nonconvex choosing good initial value important optimization. starting initial value hierarchical infomax approach quickly tentative global optimal solution layer analytic methods. finally fast convergence learning rule used optimizing ﬁnal objective function based tentative optimal solution. algorithm robust learn complete overcomplete undercomplete basis vectors quickly different datasets. experimental results showed convergence rate method signiﬁcantly faster existing methods often order magnitude. importantly number output units processed method large much larger number inputs. know existing model easily deal situation. approximation mutual information neural populations suppose input k-dimensional vector outputs neurons denoted vector assume large generally denote random variables upper case letters e.g. random variables contrast vector values deﬁned denotes expectation respect probability density function goal maxmize ﬁnding optimal constraint conditions assuming characterized noise model activation functions parameters n-th neuron words optimize solving optimal parameters unfortunately intractable cases solve optimal parameters maximizes however twice continuously differentiable almost every large asymptotic formula approximate true value high accuracy population density paramk= since cerebral cortex usually forms functional column structures column composed neurons properties positive integer regarded number distinct classes neural population. therefore given activation function goal becomes optimal population distribution density parameter vector stimulus response maximized. optimization problem stated follows since convex function {αk} readily optimal solution small efﬁcient numerical methods. large however ﬁnding optimal solution numerical methods becomes intractable. following propose alternative approach problem. instead directly solving density distribution {αk} optimize parameters {αk} {θk} simultaneously hierarchical infomax framework. clarity consider neuron model poisson spikes although method easily applicable noise models. activation function generally nonlinear function sigmoid rectiﬁed linear unit assume nonlinear function n-th neuron following form k-dimensional weights vector nonlinear function general difﬁcult optimal parameters following reasons. first number output neurons large usually second activation function nonlinear function usually leads nonconvex optimization problem. nonconvex optimization problems selection initial values often great inﬂuence ﬁnal optimization results. approach meets challenges making better large number neurons ﬁnding good initial values hierarchical infomax method. divide nonlinear transformation stages mapping ﬁrst regarded membrane potential n-th neuron ﬁring rate. real neurons assume membrane potential corrupted noise deﬁne vectors notice also divided classes assume i.e. assuming additive gaussian noise random variables form markov chain denoted following proposition major advantage incorporating membrane noise facilitates ﬁnding optimal solution using infomax principle. moreover optimal solution obtained robust; discourages overﬁtting strong ability resist distortion. vanishing noise eqs. hold case large optimize probability distribution random variable needs determined i.e. maximizing constraints yield optimal distribution maxp maxp channel capacity neural population coding always suitable linear transformation compatible distribution reasonable choice maximize noise-corrupted version implies minimum information loss ﬁrst transformation step. however exist many transformations maximize ideally transformation maximizes simultaneously reaches maximum value maxp discussion maximizing divided steps namely maximizing maximizing optimal solutions provide good initial approximation tend close optimal solution random variable similar random variable similar similar above. show optimal solution approximated solutions generally consider highly nonlinear feedforward neural network maps input output linear nonlinear function optimal parameter maximizing usually difﬁcult solve optimization problem many local extrema however function easy optimize hierarchical infomax method described good initial approximation global optimization solution ﬁnal optimal solution. information-theoretic consideration neural population coding point view help explain deep structure networks unsupervised pre-training powerful ability learning representations. rk×k identity matrix integer diagonal matrix rk×k matrix rk×k given given matrices obtained optimal weight parameter means input variable must ﬁrst undergo whiteninglike transformation goes transformation matrix optimized below. note weight matrix satisﬁes rank rank matrix dimensionality helps reduce overﬁtting training objective function reduced simpler form gradient also easy compute however computationally expensive update applying gradient directly since requires matrix inversion every another objective function approximation gradient easier compute function usually optimizing objective orthogonality constraint unnecessary. however orthogonality constraint accelerate convergence rate employ initial iteration update applied methods natural images olshausen’s image dataset images handwritten digits mnist dataset using matlab computer intel cores gray level image normalized range image patches size training randomly sampled images. used poisson neuron model modiﬁed sigmoidal tuning function +exp obtained initial values experiments iteration epoch tmax firstly tested case randomly sampled image patches size olshausen’s natural images assuming neurons divided classes input patches preprocessed whitening ﬁlters test algorithms chose batch size equal number training samples although could also choose smaller batch size. updated matrix random start parameters tmax experiments. case optimal solution looked similar optimal solution iica also compared fast algorithm faster iica. also tested restricted boltzmann machine unsupervised learning representations found could easily learn gabor-like ﬁlters olshausen’s image dataset trained contrastive divergence. however improved method adding sparsity constraint output units e.g. sparse sparse autoencoder could attain gabor-like ﬁlters dataset. similar results gabor-like ﬁlters also reproduced denoising autoencoders method requires careful choice parameters noise level learning rate batch size. order compare methods i.e. algorithm algorithm methods i.e. iica fica srbm implemented algorithms using initial weights training data good result iica must carefully select parameters; batch size initial learning rate ﬁnal learning rate exponential decay epoch iterations. iica tends faster convergence rate bigger batch size become harder escape local minima. fica chose nonlinearity function cosh contrast function srbm sparseness control constant number epoches iterations algorithms. figure shows ﬁlters learned methods methods. ﬁlter figure corresponds column vector matrix vector display normalized ˇck/ results figures look similar another slightly different results figure gabor-like ﬁlters figure corresponds srbm figure shows coefﬁcient entropy conditional entropy varied training time. calculated sampling every epoches total epoches. results show algorithms fast convergence rate towards stable solutions values similar algorithm iica converged much slowly. values small figure comparison quantization effects convergence rate coefﬁcient entropy conditional entropy corresponding training results shown figure coefﬁcient entropy conditional entropy shown function training time logarithmic scale. experiments machine using matlab. sampled every epoches total epoches. epoch number alg. alg. start time second. possible good representation learned data set. epoch number algorithms start time second. explains step seen figure alg. alg. since parameter updated epoch number fica convergence rate close algorithms reﬂected quality ﬁlter results figure convergence rate srbm close iica srbm much bigger iica implies information greater loss passing system optimized srbm iica methods. figure decreases increase value sparseness control constant note smaller means sparser outputs. hence sense increasing sparsity result sacriﬁcing information. hand weak sparsity constraint lead failure learning gaborlike ﬁlters increasing sparsity advantage reducing impact noise many practical cases. similar situation also occurs sparse coding provides class algorithms learning overcomplete dictionary representations input signals. however training time consuming expensive computational cost although many training algorithms emerged appendix additional experimental results. paper presented framework unsupervised learning representations information maximization neural populations. information theory powerful tool machine learning also provides benchmark optimization principle neural information processing nervous systems. framework based asymptotic approximation large-scale neural population. optimize infomax objective ﬁrst hierarchical infomax obtain good approximation global optimal solution. analytical solutions hierarchical infomax improved fast convergence algorithm based gradient descent. method allows optimize highly nonlinear neural networks hierarchical optimization using infomax principle. viewpoint information theory unsupervised pre-training deep learning reinterpreted process hierarchical infomax might help explain unsupervised pre-training helps deep learning framework pre-whitening step emerge naturally hierarchical infomax might also explain pre-whitening step useful training many learning algorithms model naturally incorporates considerable degree biological realism. allows optimization large-scale neural population noisy spiking neurons taking account multiple biological constraints membrane noise limited energy bounded connection weights. employ technique attain low-rank weight matrix optimization reduce inﬂuence noise discourage overﬁtting training. model many parameters optimized including population density parameters ﬁlter weight vectors parameters nonlinear tuning functions. optimizing model parameters could easily done many methods. experimental results suggest method unsupervised learning representations obvious advantages training speed robustness main existing methods. model nonlinear feedforward structure convenient fast learning inference. simple ﬂexible framework unsupervised learning presentations readily extended training deep structure networks. future work would interesting method train deep structure networks either unsupervised supervised learning. goodfellow pouget-abadie mirza warde-farley ozair courville bengio generative adversarial nets. advances neural information processing systems karklin simoncelli efﬁcient coding natural images population noisy linear-nonlinear neurons. advances neural information processing systems volume konstantinides statistical analysis effective singular values matrix rank determination. acoustics speech signal processing ieee transactions vincent larochelle lajoie bengio manzagol p.-a. stacked denoising autoencoders learning useful representations deep network local denoising criterion. journal machine learning research denotes standard deviation noise. sometimes know speciﬁc form know samples independent identically distributed samples drawn distribution empirical average approximate integral central limit theorem distribution random variable closer normal distribution distribution original random variable hand models assume multivariate gaussian data whereas models assume multivariate non-gaussian data. hence pca-like whitening transformation approximation laplace’s method asymptotic expansion requires peak close mean random variable needs exactly gaussian. without constraints gaussian channel neural populations especially peak ﬁring rates capacity channel grow indeﬁnitely common constraint neural populations energy power constraint also regarded signal-to-noise ratio constraint. output n-th neuron given unitary orthogonal matrix parameter represents size reduced dimension value determined below. optimal parameters clustered classes obey uniform discrete distribution optimal solution whitening-like ﬁlter. optimal matrix principal component analysis whitening ﬁlters. symmetrical case optimal matrix becomes zero component analysis whitening ﬁlter. case leads overcomplete solution whereas undercomplete solution arises. since achieves minimum however practice factors prevent reaching minimum. example consider average squared weights denotes frobenius norm. value extremely large becomes vanishingly small. real neurons weights connection allowed large. hence impose limitation weights positive constant. yields another constraint objective function positive constant another advantage low-rank matrix signiﬁcantly reduce overﬁtting learning neural population parameters. practice constraint equivalent weight-decay regularization term used many optimization problems reduce overﬁtting training data. prevent neural networks overﬁtting srivastava presented technique randomly drop units neural network during training fact regarded attempt reduce rank weight matrix dropout result sparser weights means update concerned keeping important components similar ﬁrst performing denoising process rank approximation. stage goal maximize optimal parameters input output also clustered classes. responses neurons k-th subpopulation obey poisson distribution mean unit vector k-th element preceding sections obtained initial optimal solutions maximizing section discuss ﬁnal optimal parameters maximizing initial optimal solutions. learning rate parameter changes iteration count tmax. empirical average approximate integral also apply stochastic gradient descent method online updating orthogonality constraint accelerate convergence rate. practice orthogonal constraint objective function strictly necessary case. completely discard constraint condition consider therefore update rule similar learning fact method also extended case using objective function learning rate parameter updated adaptively follows. first calculate calculate value continue next iteration; otherwise vt/κt recalculate update employ gram–schmidt orthonormalization process matrix orthonormalization process accelerate convergence. however discard gram– schmidt orthonormalization process iterative epochs accurate optimization solution case objective function given also optimize parameter gradient descent. objective function without constraint objective function infomax consequence optimal solution hence sense iica regarded special case method. method wider range applications handle generic situations. model derived neural populations huge number neurons restricted additive noise model. moreover method faster convergence rate training iica case computationally expensive update using gradient since needs compute inverse matrix every provide alternative method learning optimal first consider following inequalities. unitary orthogonal matrix unitary orthogonal matrix rectangular diagonal matrix positive real numbers diagonal. matrix hadamard’s inequality cauchy–schwarz inequality deﬁned corresponding optimal ﬁlter. estimate probability density coefﬁcients training samples apply kernel density estimation normal kernel adaptive optimal window width. deﬁne quantized discrete step size. methods iica srbm well methods feedforward structures information transferred directly nonlinear function e.g. sigmoid function. amount transmitted information measure results learned methods. consider neural population neurons stochastic system nonlinear transfer functions. chose sigmoidal transfer function gaussian noise standard deviation system noise. case approximate equivalent case poisson neuron model. follows figure comparison basis vectors obtained method methods. panel correspond panel figure basis vectors given basis vectors panel learned mbdl given compared algorithm up-to-date sparse coding algorithm mini-batch dictionary learning given integrated python library i.e. scikitlearn. input data above i.e. nature image patches preprocessed whitening ﬁlters. denotes optimal dictionary learned mbdl rk×k column represents basis vector. basis vectors shown figure correspond ﬁlters figure figure illustrates optimal dictionary learned mbdl regularization parameter batch size total number iterations perform took hours training. figure basis vectors obtained algorithms local gabor-like shapes except srbm. rank matrix ˇb−t regarded ﬁlter matrix like matrix however column vector matrix ˇb−t cannot local gabor-like ﬁlter resembles ﬁlters shown figure algorithm less computational cost much faster convergence rate sparse coding algorithm. moreover sparse coding method involves dynamic generative model requires relaxation therefore unsuitable fast inference whereas feedforward framework model easy inference requires evaluating nonlinear tuning functions. trained model olshausen’s nature image patches highly overcomplete setup optimizing objective alg. gabor-like ﬁlters. results typical ﬁlters chosen output ﬁlters displayed figure corresponding base shown figure parameters tmax rank compared ica-like results figure average size gabor-like ﬁlters figure bigger indicating small noise-like local structures images ﬁltered out. also trained model images handwritten digits mnist dataset resultant typical optimal ﬁlters bases shown figure figure respectively. parameters figure figure tmax rank ﬁgures salient features input images reﬂected ﬁlters bases. could also similar overcomplete ﬁlters bases srbm mbdl. however results depended sensitively choice parameters training took long time. figure filters bases obtained olshausen’s image dataset mnist dataset algorithm typical ﬁlters corresponding bases obtained olshausen’s image dataset typical ﬁlters corresponding bases obtained mnist dataset figure shows function training time alg. figure corresponds figure learning nature image patches figure corresponds figure learning mnist dataset. parameters tmax experiments varied parameter experiment results indicate fast convergence rate training different datasets. generally convergence insensitive change parameter also performed additional tests image datasets similar results conﬁrming speed robustness learning method. compared methods e.g. iica fica mbdl srbm sparse autoencoders etc. method appeared efﬁcient robust unsupervised learning representations. also found complete overovercomplete ﬁlters bases learned methods local gabor-like shapes results srbm mbdl property. similar sparse coding method applied image denoising method also applied image denoising shown example figure ﬁlters bases learned using image patches sampled left half image subsequently used reconstruct right half image distorted gaussian noise. common practice evaluating results image denoising looking difference reconstruction original image. reconstruction perfect difference look like gaussian noise. figure dictionary learned mbdl orthogonal matching pursuit used estimate sparse solution. method ﬁrst optimal ﬁlters parameter rank matrix distorted image patches ﬁlter outputs reconstruction seen figure method worked better dictionary learning although used bases compared bases used dictionary learning. method also efﬁcient. better optimal bases generative model using infomax approach figure image denoising. right half original image distorted gaussian noise norm difference distorted image original image image denoising method bases used. image denoising using dictionary learning bases used.", "year": 2016}