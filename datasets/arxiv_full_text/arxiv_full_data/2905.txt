{"title": "Control of Memory, Active Perception, and Action in Minecraft", "tag": ["cs.AI", "cs.CV", "cs.LG"], "abstract": "In this paper, we introduce a new set of reinforcement learning (RL) tasks in Minecraft (a flexible 3D world). We then use these tasks to systematically compare and contrast existing deep reinforcement learning (DRL) architectures with our new memory-based DRL architectures. These tasks are designed to emphasize, in a controllable manner, issues that pose challenges for RL methods including partial observability (due to first-person visual observations), delayed rewards, high-dimensional visual observations, and the need to use active perception in a correct manner so as to perform well in the tasks. While these tasks are conceptually simple to describe, by virtue of having all of these challenges simultaneously they are difficult for current DRL architectures. Additionally, we evaluate the generalization performance of the architectures on environments not used during training. The experimental results show that our new architectures generalize to unseen environments better than existing DRL architectures.", "text": "figure example task minecraft. task agent visit block indicator yellow. otherwise indicator green visit blue block. shows agent’s ﬁrst-person observation. bottom visualizes agent’s location; available agent. agent observes yellow indicator. agent looks left sees blue block decides keep going straight previously seen yellow indicator. finally visits block receives positive reward. recently researchers explored problems require faculties associated higher-level cognition advances however restricted supervised learning setting provides clear error signals. paper interested extending success similarly cognition-inspired tasks. speciﬁcally paper introduces tasks minecraft ﬂexible world agent collect resources build structures survive attacks enemies. tasks usual challenges partial observability high-dimensional perception delayed reward also require agent develop movement policies learning active perception observe useful information collect reward. addition tasks require agent learn memory possesses including interaction active perception feeds observations paper introduce reinforcement learning tasks minecraft tasks systematically compare contrast existing deep reinforcement learning architectures memory-based architectures. tasks designed emphasize controllable manner issues pose challenges methods including partial observability delayed rewards high-dimensional visual observations need active perception correct manner perform well tasks. tasks conceptually simple describe virtue challenges simultaneously difﬁcult current architectures. additionally evaluate generalization performance architectures environments used training. experimental results show architectures generalize unseen environments better existing architectures. introduction deep learning approaches made advances many lowlevel perceptual supervised learning problems success extended reinforcement learning problems involve visual perception. example deep q-network architecture shown successfully learn play many atari games arcade learning environment benchmark learning visual features useful control directly pixels using q-learning memory. note simplicity hereafter refer cognition-inspired tasks cognitive tasks acknowledge form best limited exploration range cognitive faculties humans. work systematically evaluate performance different neural network architectures tasks also examine well architectures generalize unseen larger topologies empirical results show existing architectures perform worse unseen larger maps compared training sets maps even though perform reasonably well training maps. motivated lack generalization existing architectures tasks also propose memory-based architectures. proposed architectures store recent observations memory retrieve relevant memory based temporal context whereas memory retrieval existing architectures used problems conditioned context. summary show architectures outperform existing ones tasks well generalize better unseen maps exploiting memory mechanisms. related work neural networks external memory. graves introduced neural turing machine differentiable external memory architecture showed learn algorithms copy reverse. zaremba sutskever proposed rl-ntm nondifferentiable memory scale addressing mechanism applied policy gradient train architecture. joulin mikolov implemented stack using neural networks demonstrated infer several algorithmic patterns. sukhbaatar proposed memory network language modeling tasks stores inputs retrieves relevant memory blocks depending question. deep reinforcement learning. neural networks used learn features tasks decades recently mnih proposed deep q-network training deep convolutional neural networks q-learning end-to-end fashion; achieved state-of-the-art performance atari games. used slow monte-carlo tree search generate relatively small amount data train fast-playing convolutional networks atari games. schulman levine lillicrap successfully trained deep neural networks directly learn policies applied architectures robotics problems. addition deep approaches tasks atari learning algorithms text-based games also attempts learn state-transition models using deep learning improve exploration recently mnih proposed asynchronous showed learn explore environment similar minecraft. unlike work focus systematic evaluation ability deal partial observability active perception external memory different neural network architectures well generalization across size maps. model-free deep pomdps. building modelfree agent partially observable markov decision processes challenging problem agent needs learn summarize history actionselection. deal challenge bakker used long short-term memory network ofﬂine policy learning framework show robot controlled lstm network solve t-mazes robot correct destination depending trafﬁc signal beginning maze. wierstra proposed recurrent policy gradient method showed lstm network trained using method outperforms methods several tasks including tmazes. recently zhang introduced continuous memory states augment state action space showed memorize salient information guided policy search hausknecht stone proposed deep recurrent qnetwork consists lstm based framework demonstrated improved handling partial observability atari games. departure related work. architectures introduce memory mechanisms similar memnn architectures layer constructs query memory retrieval based temporal context. architectures also similar recurrent controller interacts external memory simpler writing addressing mechanism makes easier train. importantly architectures used setting must learn delayed reward signal whereas previous work exploring architectures memory supervised learning setting much direct undelayed error signals. describe details architectures section tasks introduce inspired t-maze experiments well mazebase natural language descriptions mazes available agent. unlike previous tasks mazes high-dimensional visual observations deep partial observability nature worlds. addition agent learn best control active perception system collect useful information right time tasks; necessary previous work. background deep q-learning denote state immediate reward action time respectively. framework every transition stored replay memory. iteration deep neural network trained approximate action-value function transitions minimizing loss functions follows ∇θli esa∼πθ )∇θq es∼πθ target q-value estimated target q-network practice expectation terms approximated sampling mini-batch transitions replay memory. parameter target q-network synchronized learned network ﬁxed number iterations. architectures importance retrieving prior observation memory depends current context. example maze figure color indicator block determines desired target color indicator information important agent seeing potential target decide whether approach different target. motivated lack context-dependent memory retrieval existing architectures present three memory-based architectures section. proposed architectures consist convolutional networks extracting high-level features images memory retains recent history observations context vector used memory retrieval action-value estimation depending context vector constructed obtain three architectures memory q-network recurrent memory q-network feedback recurrent memory q-network encode observation. memory memory operations proposed architectures similar proposed memnn. write. encoded features last observations linearly transformed stored memory value memory blocks illustrated figure formally types memory blocks deﬁned follows illustrated figure given context vector memory module draws soft attention memory locations computing innerproduct context memory blocks follows context retrieve useful information memory context vector capture relevant spatio-temporal information observations. present three different architectures constructing context vector context vector memory cell lstm respectively denotes concatenation vectors input lstm. feedforward architecture constructs context based current observation similar memnn except current input used memory retrieval temporal context problem. rmqn recurrent architecture captures spatio-temporal information history observations using lstm. architecture allows retaining temporal information lstm well external memory. finally frmqn feedback connection retrieved memory context vector illustrated figure allows frmqn architecture reﬁne context based previously retrieved memory complex reasoning time goes note feedback connections analogous idea multiple hops memnn sense architecture retrieves memory blocks multiple times based previously retrieved memory. however frmqn retrieves memory blocks time memnn not. figure examples maps. i-structured topology location indicator goals spawn locations ﬁxed across episodes. goals rooms color patterns. consists randomly generated walls goals. agent spawned anywhere except goal locations. similar except indicator ﬁxed location ﬁxed spawn location. experiments experiments baselines tasks designed investigate useful context-dependent memory retrieval generalizing unseen maps memory feedback connections frmqn helpful. game play videos found supplementary material following website https//sites.google.com/a/umich.edu/ junhyuk-oh/icml-minecraft. next describe aspects common tasks training methodology. environment. tasks episodes terminate either agent ﬁnishes task steps. agent receives reward every time step. agent’s initial looking direction randomly selected among four directions north south east west. tasks randomness randomly sampled instance every episode. actions. following actions available look left/right look up/down move forward/backward. moving actions move agent block forward backward direction facing. pitch limited baselines. compare three architectures baselines drqn architecture takes ﬁxed number frames input. drqn recurrent architecture lstm layer cnn. note cannot take table performance i-maze. entry shows average success rate standard error measured runs. measured average success rate best-performing parameters based performance unseen maps. success rate deﬁned number episodes agent reaches correct goal within steps divided total number episodes. ‘size’ represents number blocks vertical corridor. indicates sizes i-mazes belong training maps. number frames used training ﬁrst convolution layer takes ﬁxed number observations. however drqn architectures take arbitrary number input frames using recurrent layers. additionally architectures arbitrarily large size memory evaluation well. training details. input frames minecraft captured images. architectures -layer architecture described supplementary material. drqn architectures last convolutional layer followed fullyconnected layer hidden units. architectures last convolution layer given encoded feature memory blocks. addition lstm units used drqn rmqn frmqn. details including hyperparameters deep q-learning described supplementary material. implementation based torch public implementation minecraft forge mod. i-maze description results task. i-maze task inspired t-mazes used animal cognition experiments maps task indicator equal chance yellow green. indicator yellow block gives reward blue block gives reward; indicator green block gives blue block gives reward. thus agent memorize color indicator beginning view visit correct goal depending indicator-color. varied length vertical corridor training. last frames given input architectures size memory architectures performance training set. observed stages behavior learning architectures early training discount factor time penalty agent take chance visiting goal later training agent goes correct goal learning correlation indicator goal. seen learning curves figure architectures converge quickly drqn correct behavior. particular observed drqn takes many epochs reach second stage ﬁrst stage reached. possibly long time interval seeing indicator goals. besides indicator block important agent bottom vertical corridor needs decide words indicator information affect agent’s decision making along corridor. makes even difﬁcult drqn retain indicator information long time. hand architectures handle problems storing history observations memory retrieving information important based context. generalization generalization architectures maps vertical corridor lengths present training maps. speciﬁcally testing sizes maps rest sizes maps evaluate interpolation extrapolation performance respectively since unseen maps larger training maps used last frames input evaluation unseen maps architectures except take frames discussed experimental setup. size memory architectures performance unseen maps visualized figure although generalization performances architectures highly variable even training performance converges seen frmqn consistently outperforms architectures terms average reward. investigate performance different lengths vertical corridor measured performance size table turns architectures perform well sizes maps indicates interpolate within training maps. however architectures extrapolate larger maps signiﬁcantly better baselines. analysis memory retrieval. figure visualizes frmqn’s memory retrieval large i-maze frmqn sharply retrieves indicator information reaches corridor makes decision goal block visit. reasonable strategy indicator information important figure learning curves different tasks i-maze pattern matching random mazes x-axis y-axis correspond number training epochs average reward. ‘unseen’ represents unseen maps different sizes different patterns respectively. ‘unseen’ ‘unseen-l’ indicate unseen topologies sizes larger sizes maps respectively. performance measured runs random mazes runs i-maze pattern matching. random mazes show results sequential goals indicator space constraints. plots provided supplementary material. table performance pattern matching. entries represent probability visiting correct goal block maps standard error. performance reported averages runs best-performing parameters run. vertical corridor. qualitative result implies frmqn learned general strategy looks indicator goes corridor retrieves indicator information decides goal block visit. observed similar policies learned rmqn memory attention indicator sharp frmqn’s attention visit wrong goals larger i-mazes often. results i-maze shown suggest solving task maps guarantee solving task similar unseen maps generalization performance highly depends feature representation learned deep neural networks. extrapolation result shows context-dependent memory retrieval architectures important learning general strategy importance observational-event depends highly temporal context. pattern matching description results task. illustrated figure consists rooms. visual patterns rooms either identical different equal probability. rooms exact color patterns agent visit blue block. rooms different color patterns agent visit block. agent receives reward visits correct block reward visits wrong block. pattern matching task requires complex reasoning i-maze task above. generated training unseen maps little overlap sets visual patterns. details generation process described supplementary material. last frames given input architectures size memory performance training set. results plotted figure table show frmqn successfully learned correct goal block runs training maps. observed drqn always learned sub-optimal policy goes goal regardless visual patterns rooms. anobservation training performances rmqn unstable; often learned suboptimal policy whereas frmqn consistently learned correct goal across different runs. hypothesize trivial neural network compare visual patterns observed different time-steps unless network model high-order interactions between speciﬁc observations visual matching might reason drqn fail often. context-dependent memory retrieval mechanism architectures alleviate problem retrieving visual patterns corresponding observations rooms decision making. figure visualization frmqn’s memory retrieval. ﬁgure shows trajectory frmqn following rows visualize attention weights time. agent looks indicator goes corridor retrieves indicator frame visiting goal block. agent looks rooms beginning gradually switches attention weights room another room approaches goal blocks. agent pays attention indicator ﬁrst goal block generalization performance. table figure show frmqn achieves highest success rate unseen maps. interestingly fails generalize unseen visual patterns. observed pays attention visual patterns choosing goals memory retrieval. however since retrieved memory convex combination visual patterns hard compare similarity them. thus believe simply overﬁts training maps memorizing weighted pairs visual patterns training maps. hand frmqn utilize retrieved memory well recurrent connections compare visual patterns time. analysis memory retrieval. example frmqn’s memory retrieval visualized figure frmqn pays attention rooms gradually moving weight time progresses means context vector repeatedly reﬁned based encoded features room retrieved feedback connections. given visualization good generalization performance hypothesize frmqn utilizes feedback connection compare visual features time rather comparing single time-step. result supports view feedback connections play important role tasks complex reasoning required retrieved memories. random mazes description results task. random maze task consists randomly generated walls goal locations shown figure present classes tasks using random mazes. figure precision distance. x-axis represents distance indicator goal single goal indicator task. yaxis represents number correct goal visits divided total number goal visits. task visit blue block ﬁrst block. indicator green task visit block ﬁrst blue block. visiting blocks correct order results ﬁrst block reward second block. visiting blocks reverse order results reward respectively. randomly generated maps used training types unseen evaluation sets maps maps sizes present training maps larger maps. last frames given input architectures size memory performance training set. task agent needs remember important information traversing maps also search goals different maps different obstacle goal locations. table shows rmqn frmqn achieve higher asymptotic performances architectures training maps. generalization performance. larger-sized unseen maps terminated episodes steps rather steps used time penalty considering size. evaluation used frames input drqn frames rmqn table performance random maze. ‘size’ column lists size maps. entries ‘reward’ ‘success’ ‘fail’ columns average rewards success rates failure rates measured runs. picked best parameters based performance unseen maps evaluated episodes. ‘success’ represents number correctly completed episodes divided total number episodes ‘fail’ represents number incorrectly completed episodes divided total number episodes standard errors lower average rewards success rates failure rates respectively. results table show that expected performance architectures worsen unseen maps. learning curves observed generalization performance unseen maps improve epochs even though training performance improving. implies improving policies ﬁxed maps necessarily guarantee better performance environments. however rmqn frmqn generalize better architectures tasks. particular compared architectures drqn’s performance signiﬁcantly degraded unseen maps. addition shows good generalization performance single goal task primarily requires search tasks tends goal regardless important information seen higher failure rate indicator tasks table investigate well architectures handle partial observability measured precision versus distance goal indicator single goal indicator task visualized figure notably architectures architectures becomes larger distance increases. result implies architectures better handling partial observability architectures large distance indicator goal likely introduce deeper partial observability crucial component handling random topologies. addition frmqn rmqn achieve similar performances implies feedback connection always helpful tasks. note given retrieved memory reasoning required tasks simpler reasoning required pattern matching task. analysis memory retrieval. example memory retrieval frmqn visualized figure retrieves memory contains important information visits goal block. memory retrieval strategy reasonable evidence proposed architectures make easier generalize large-scale environments better handling partial observability. discussion paper introduced three classes cognitioninspired tasks minecraft compared performance existing architectures three architectures proposed here. emphasize unlike evaluations algorithms trained evaluated architectures disjoint sets maps speciﬁcally consider applicability learned value functions unseen maps. summary main empirical result contextdependent memory retrieval particularly feedback connection retrieved memory effectively solve tasks require control active perception external physical movement actions. architectures particularly frqmn also show superior ability relative baseline architectures learning value functions whose behavior generalizes better training unseen environments. future work intend take advantage ﬂexibility minecraft domain construct even challenging cognitive tasks further evaluate architectures. references bahdanau dzmitry kyunghyun bengio yoshua. neural machine translation jointly learning international conference align translate. learning representations bakker bram zhumatiy viktor gruener gabriel schmidhuber j¨urgen. robot reinforcement-learns identify memorize important previous observations. intelligent robots systems collobert ronan kavukcuoglu koray farabet cl´ement. torch matlab-like environment machine learning. biglearn advances neural information processing system workshop girshick ross donahue jeff darrell trevor malik jitendra. rich feature hierarchies accurate object deproceedings tection semantic segmentation. ieee conference computer vision pattern recognition xiaoxiao singh satinder honglak lewis richard wang xiaoshi. deep learning real-time atari game play using ofﬂine monte-carlo tree search planning. advances neural information processing system hausknecht matthew stone peter. deep recurrent q-learning partially observable mdps. aaai fall symposium sequential decision making intelligent agents krizhevsky alex sutskever ilya hinton geoffrey imagenet classiﬁcation deep convolutional neural networks. advances neural information processing system lillicrap timothy hunt jonathan pritzel alexander heess nicolas erez tassa yuval silver david wierstra daan. continuous control deep reinternational conference inforcement learning. learning representations mnih volodymyr kavukcuoglu koray silver david rusu andrei veness joel bellemare marc graves alex riedmiller martin fidjeland andreas ostrovski georg petersen stig beattie charles sadik amir antonoglou ioannis king helen kumaran dharshan wierstra daan legg shane hassabis demis. human-level control deep reinforcement learning. nature mnih volodymyr badia adria puigdomenech mirza mehdi graves alex lillicrap timothy harley silver david kavukcuoglu koray. asynchronous proceedmethods deep reinforcement learning. ings international conference machine learning nair vinod hinton geoffrey rectiﬁed linear units improve restricted boltzmann machines. proceedings international conference machine learning narasimhan karthik kulkarni tejas barzilay regina. language understanding text-based games conference using deep reinforcement learning. empirical methods natural language processing zaremba wojciech mikolov tomas joulin armand fergus rob. learning simple algorithms examples. proceedings international conference machine learning zhang marvin levine sergey mccarthy finn chelsea abbeel pieter. policy learning continuous memory states partially observed robotic coninternational conference robotics autrol. tomation junhyuk xiaoxiao honglak lewis richard singh satinder. action-conditional video prediction using deep networks atari games. advances neural information processing system schaul horgan daniel gregor karol silver david. universal value function approximators. proceedings international conference machine learning schulman john levine sergey moritz philipp jordan michael abbeel pieter. trust region policy optimization. proceedings international conference machine learning simonyan karen zisserman andrew. deep convolutional networks large-scale image recognition. international conference learning representations architectures ﬁrst convolution layer consists ﬁlters stride padding second convolution layer consists ﬁlters stride padding deep q-learning batch size discount factor used. used replay memory size random mazes i-maze pattern matching tasks. linearly interpolated initial steps \u0001-greedy policy. chose best learning rate lead value function explosion depending tasks architectures. chosen learning rates shown table parameter updated every steps. rmsprop used momentum momentum squared gradients gradients clipped lnorm prevent divergence. used soft target qnetwork updates momentum suggested generation pattern matching total possible visual patterns room blocks colors. randomly picked patterns generated maps pattern contains pattern rooms another different randomly generated pattern rooms randomly selected. produces maps identical rooms different rooms used training. evaluating generalization picked another exclusive visual patterns generated maps following procedure. figure learning curves. x-axis y-axis correspond number training epochs average reward respectively. i-maze ‘unseen’ represents unseen maps different sizes. pattern matching ‘unseen’ represents maps different visual patterns. rest plots ‘unseen’ ‘unseen-l’ indicate unseen topologies sizes larger sizes maps respectively. performance measured runs random mazes runs i-maze pattern matching. figure frmqn’s play pattern matching task. agent starts looking room turns twice look room. upon observing rooms agent uses backward actions repeatedly move along vertical corridor. finally corridor decides turn move forward blue block visual patterns rooms identical. note agent’s performance near optimal. figure frmqn’s play unseen random maze single goal task. case tasks agent starts looking quickly important stimuli clearly. agent looks around it’s vicinity explores corridors. soon agent sees blue block goes block successfully complete task. figure frmqn’s play unseen larger random maze single goal task. agent explores top-left side top-right side successfully ﬁnds visits blue block. figure frmqn’s play unseen larger random maze single goal task. even though agent explores entire reasonable fails blue block within steps. occur occasionally especially quite large intrinsically complex figure frmqn’s play random maze sequential goals task. case tasks agent begins looking down. looks around block upon ﬁnding block visits agents looks blue block successfully ﬁnds hence complete sequence task notably agent keep searching block visiting block. memory crucial. figure frmqn’s play unseen larger random maze sequential goals task. context visual observations containing blue block agent avoids blue block keeps searching block based memory visited block. ﬁnding visiting block directly goes blue block completing sequence correct order hence task. figure frmqn’s play unseen larger random maze sequential goals task. agent ﬁnds blue blocks. however visiting block following corridor found would lead visiting blue block result negative reward. thus agent attempts search another route reach goal agent fails another route within time limit figure frmqn’s play random maze single goal indicator task. upon seeing indicator green color agent proceeds explore map. search comes across corridor block memory helps. agent avoids block infer agent utilizes memory appropriately. later successfully completes task ﬁnding visiting blue block figure frmqn’s play unseen larger random maze single goal indicator task. correct goal indicator agent able memorize color indicator observed beginning visits correct goal figure frmqn’s play random maze sequential goals indicator task. agent observes indicator yellow agent block near since task ﬁrst visit blue block block indicator yellow avoids moving towards block agent proceeds turning exploring corridors. finally gets glimpse blue block using visual observation along retrieving memory visual observations indicator present agent correctly goes blue block proceeds block hence completing task figure frmqn’s play unseen larger random maze sequential goals indicator task. agent visits block ﬁrst given green indicator agent looks blue block apart block agent ﬁnds blue block ﬁnishes task completing sequence figure frmqn’s play random maze sequential goals indicator task. given indicator green agent visit block ﬁrst blue block later. reason agent tries avoid blue block search another route block however since path block agent keeps searching episode terminates.", "year": 2016}