{"title": "How to Discount Deep Reinforcement Learning: Towards New Dynamic  Strategies", "tag": ["cs.LG", "cs.AI"], "abstract": "Using deep neural nets as function approximator for reinforcement learning tasks have recently been shown to be very powerful for solving problems approaching real-world complexity. Using these results as a benchmark, we discuss the role that the discount factor may play in the quality of the learning process of a deep Q-network (DQN). When the discount factor progressively increases up to its final value, we empirically show that it is possible to significantly reduce the number of learning steps. When used in conjunction with a varying learning rate, we empirically show that it outperforms original DQN on several experiments. We relate this phenomenon with the instabilities of neural networks when they are used in an approximate Dynamic Programming setting. We also describe the possibility to fall within a local optimum during the learning process, thus connecting our discussion with the exploration/exploitation dilemma.", "text": "using deep neural nets function approximator reinforcement learning tasks recently shown powerful solving problems approaching real-world complexity using results benchmark discuss role discount factor play quality learning process deep q-network discount factor progressively increases ﬁnal value empirically show possible signiﬁcantly reduce number learning steps. used conjunction varying learning rate empirically show outperforms original several experiments. relate phenomenon instabilities neural networks used approximate dynamic programming setting. also describe possibility fall within local optimum learning process thus connecting discussion exploration/exploitation dilemma. reinforcement learning learning paradigm aiming learning optimal behaviors interacting within environment main challenge designing reinforcement learning algorithms fact state space large continuous potentially leading fact state value functions represented comprehensively recently empirically demonstrated using deep neural networks function approximator powerful situations high-dimensional sensory inputs atari games benchmark however main drawback using neural networks function approximator potentially become unstable combined q-learning-type recursion paper propose discuss role discount factor play stability convergence deep reinforcement learning algorithms. empirically show increasing discount factor potential improve quality learning process. also discuss link discount factor inﬂuence instabilities neural networks vanilla exploration/exploitation dilemma. motivation work comes empirical studies attentional cognitive mechanisms delay gratiﬁcation. well known experiment domain series studies child offered choice small reward provided immediately small rewards waited short period capacity wait longer preferred rewards seems develop markedly ages years children able demonstrate better self-control gaining control immediate desires. according theory seems plausible following immediate desires young better develop abilities delaying strategies advantageous afterwards pursuing longer term goals. similarly reinforcement learning also advantage starting learn maximizing rewards short-term horizon progressively giving weights delayed rewards. remaining paper organized following ﬁrst recall main equations used deep reinforcement learning problem formulation originally introduced discuss factors inﬂuence stability neural networks. light analysis suggest possibility modify discount factor along convergence ﬁnal value order speed learning process. illustrate approach benchmark proposed context discuss role learning rate well level exploration. maximum rewards discounted time-step achievable behaviour stochastic policy denotes probability action chosen policy state paper task described time-invariant stochastic discrete-time system whose dynamics described following equation element state space action element action space random disturbance element disturbance space generated timeinvariant conditional probability distribution experiments dynamics given atari emulator state space based observed pixels screen action space legal game actions solution bellman equation q-value function given problems approaching real-world complexity need learn parameterized value function general function-approximation system neural networks well suited deal high-dimensional sensory inputs. addition work readily online i.e. make additional samples obtained learning happens using supervised learning problem. case neural network used convergence towards parameters updated stochastic gradient descent updating current maxa∈a value towards target value refers parameters previous q-network. q-learning update using squared-loss amounts updating weights q-learning rule equation directly implemented online using neural network however generalization extrapolation abilities neural nets build unpredictable changes different places state-action space. known errors propagated even become unstable. additional care must therefore taken since guaranteed current estimation accumulated figure sketch algorithm. initialized close everywhere domain; nreplay size replay memory; target q-network parameters updated every iterations q-network parameters held ﬁxed updates; variable corresponds mini-batch tuples taken randomly replay memory variable corresponding target value tuple. costs always underestimates optimal cost therefore convergence assured convergence problems veriﬁed experimentally reported convergence slow even unreliable online update rule offer analysis based complexity policy class parallel machine learning. high complexity machine learning techniques ability represent training well risk overﬁtting. contrast models lower complexity tend overﬁt fail capture important features. reinforcement learning setting equivalence model complexity learning method seen policy class complexity. speciﬁcally context online neural ﬁtted reinforcement learning discount factor well neural network architecture control number possible policies. similarly bias-variance tradeoff observed supervised learning reinforcement learning also faces trade-off large policy class complexity policy class complexity. already known optimal solution actually found policy planning horizon smaller evaluation horizon γeval speciﬁed problem formulation case inaccurate information actual dynamics available priori also well known longer planning horizon greater computational expense computing optimal policy case neural ﬁtted value learning difﬁculty target high policy class complexity severe targeting high discount factor leads propagation errors instabilities. practical ways prevent instabilities make replay memory clipping error term separate target q-network equation convolutional network architecture using double q-learning algorithm also help reducing overestimations q-value function caused generalization exageration regression method paper investigate tradeoff performance targeted policy versus stability speed learning process. investigate possibility reduce instabilities learning working adaptive discount factor soften errors learning. goal target high policy class complexity reducing error propagations deep learning iterations. deep q-learning algorithm described used benchmark. hyperparameters kept identical stated differently. main modiﬁcation comes discount factor increased every epoch following formula learned policies evaluated steps \u0001-greedy policy identical original benchmark \u0001test reported scores highest average episode score simulation evaluation episodes truncated min. game simulated times conﬁguration varying seeds results reported figure observed simply using increasing discount factor learning faster four tested games similar remaining game. conclude starting discount factor obtain faster policy improvement thanks less instability. addition also provides robustness respect neural network weights initialisation. figure summary results increasing discount factor. reported scores relative improvements steps increasing discount factor constant discount factor ﬁnal value details given appendix table discuss stability dqn. experimental rule equation either increase keep constant attains illustrated figure shown increasing without additional care degrades severely score obtained beyond looking average value seen overestimation particularly severe causes poor policies. since instabilities severe discount factor high study possibility aggressive learning rate neural network working discount factor potential errors would less impact stage. learning rate reduced along increasing discount factor stable neural q-learning function. follows equation kept constant attains manage improve score obtained games tested. results reported figure illustration given different games figure noted value function decreases hold ﬁxed learning rate lowered sign decrease overestimations q-value function. figure summary results decreasing learning rate. reported scores improvement steps using dynamic discount factor dynamic learning rate. details given appendix table errors policy also positive impacts since increases exploration using lower discount factor actually decreases exploration opens risk falling local optimum value iteration learning. observed agent gets repeatedly score lower optimal unable discover parts state space. case actor-critic-type algorithm increases level exploration allow overcome problem. believe actor critic agent also manages adaptively level exploration important improve deep reinforcement learning algorithms. order illustrate this simple rule applied case game seaquest seen figure rule adapts exploration training process \u0001-greedy action selection agent able local optimum. figure illustration game seaquest case exploration rate fails local optimum left. right illustration simple rule increases exploration allows local optimum. paper introduced approach speed-up convergence improve quality learned q-function deep reinforcement learning algorithms. works adapting discount factor learning rate along convergence. used deep q-learning algorithms atari computer games benchmark approach showed improved performances tested games. results motivate experiments particular would interesting develop automatic adapt online discount factor along learning rate possibly level exploration. would also interest combine approach recent advances deep reinforcement learning massively parallel architecture \"gorilla\" references volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature richard sutton andrew barto. introduction reinforcement learning. press marc bellemare yavar naddaf joel veness michael bowling. arcade learning environment evaluation platform general agents. arxiv preprint arxiv. walter mischel ebbe ebbesen antonette raskoff zeiss. cognitive attentional mechanisms delay gratiﬁcation. journal personality social psychology martin riedmiller. neural ﬁtted iteration–ﬁrst experiences data efﬁcient neural reinforcement learning method. machine learning ecml pages springer jiang alex kulesza satinder singh richard lewis. dependence effective planning horizon model accuracy. proceedings international conference autonomous agents multiagent systems pages international foundation autonomous agents multiagent systems arun nair praveen srinivasan blackwell cagdas alcicek rory fearon alessandro maria vedavyas panneershelvam mustafa suleyman charles beattie stig petersen massively parallel methods deep reinforcement learning. arxiv preprint arxiv.", "year": 2015}