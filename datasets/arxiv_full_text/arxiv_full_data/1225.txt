{"title": "On non-iterative training of a neural classifier", "tag": ["cs.CV", "cs.LG", "cs.NE", "62M45"], "abstract": "Recently an algorithm, was discovered, which separates points in n-dimension by planes in such a manner that no two points are left un-separated by at least one plane{[}1-3{]}. By using this new algorithm we show that there are two ways of classification by a neural network, for a large dimension feature space, both of which are non-iterative and deterministic. To demonstrate the power of both these methods we apply them exhaustively to the classical pattern recognition problem: The Fisher-Anderson's, IRIS flower data set and present the results.  It is expected these methods will now be widely used for the training of neural networks for Deep Learning not only because of their non-iterative and deterministic nature but also because of their efficiency and speed and will supersede other classification methods which are iterative in nature and rely on error minimization.", "text": "recently algorithm discovered separates points n-dimension planes manner points left un-separated least plane. using algorithm show ways classiﬁcation neural network large dimension feature space non-iterative deterministic. demonstrate power methods apply exhaustively classical pattern recognition problem fisher-anderson’s iris ﬂower data present results. expected methods widely used training neural networks deep learning non-iterative deterministic nature also efﬁciency speed supersede classiﬁcation methods iterative nature rely error minimization. additional words phrases non-iterative training neural networks pattern recognition algorithms reference format k.eswaran k.damodhar non-iterative training neural classiﬁer. setn conf. article pages. introduction many researchers statisticians neural network scientists long trying separate clusters planes discriminant functions. ever since time fisher mahalanobis mccullough pitts kolmogorov werbos rumelhart hinton williams others also deep learning people. great difﬁculties large n-dimensional space found easy. phrases like curse dimensionality hard complexity become part folk lore. however beginning never easy separate clusters planes. mostly because cluster well deﬁned every cluster shape n-dimensions could long thin ﬁlaments kinds snake like dragon like shapes constitute clusters. though statisticians approximate shape cluster ellipsoid even simple sphere clusters general would require parameters mathematically correctly deﬁne shapes number coefﬁcients required deﬁne planes supposed separate them. along perhaps nave tried separate clusters entities mathematically well deﬁned. better separate individual points enormous space degrees freedom available ndimension space separate point rather separate clusters never well deﬁned. arguments genesis idea gave impetus kind research work presently reported paper. author’s addresses k.eswaran k.damodhar department computer science sreenidhi institute science technology yamnampet ghatkesar hyderabad india. permission make digital hard copies part work personal classroom granted without provided copies made distributed proﬁt commercial advantage copies bear notice full citation ﬁrst page. copyrights components work owned others must honored. abstracting credit permitted. copy otherwise republish post servers redistribute lists requires prior speciﬁc permission and/or fee. request permissions permissionsacm.org. acm. -//-art direction greatly encouraged recent discovery algorithm enables separate number given points large dimension space manner every point separated every least plane. best part algorithm non-iterative ﬁnds coefﬁcients equations planes separate given points; computational complexity approx. subject paper pattern recognition purpose paper show means newly discovered algorithm build noniterative classiﬁer assign sample points data appropriate class. actually stated abstract methods used classiﬁcation test-sample ﬁrst directly uses information obtained planes separate trainingsample points assumed class label tries classify test-sample. second method based ﬁrst uses another cluster discovery algorithm clusters belong class training-sample feature space proceeds perform classiﬁcation trying cluster test-point would probably belong second method like traditional neural method non-iterative deterministic. ﬁrst method would seemingly like nearest neighbour algorithm method determines exact quadrant neighbouring point belongs distance. clear n−dimension space nquadrants surrounding neighbouring point present method since separated point planes manner taken quadrant information account whereas nearest neighbour method loses quadrant directional information reducing everything single number distance. aspect makes present method superior actual application reveals. data consists points n-dimensional sample space divided sets pointssay number considered traning another points considered test set. assumed point training belongs class known. points test assumed belonging unknown class discovered classiﬁer. number planesq required would empirical estimate gets better number dimensions gets bigger. ancase found planes planes separated points increase empirical formula since planes.)see details. points separated planes performed separation known method process classiﬁcation points easily devised shall see. information obtained planes separate training-sample points assumed class label classify testsample. brieﬂy describe algorithm applied data retrieval. suppose points dimension x-space algorithm used separate points found running algorithm planes ﬁnally necessary. show information used data retrieval device. data stored manner retrieval done great efﬁciency. assumed every plane speciﬁc normal direction point lies positive side normal said positive side plane hand point lies side said negative side.this direction easily found equation plane known example +αnxn equation particular ndimensional plane point whose coordinates plane said positive side plane +αnpn negative side +αnpn another fact adds prospect success direction that large dimension space number planes needed separating points less number points itself fact purposes illustration assume point represents dimensional sample xspace. sample data point numbers relate medical data person alternatively could image involving pixels represent photograph person.now wish store many samples data points manner classiﬁcation retrieval becomes easy. idea simple algorithm separates data points number planes exact information data points reside x-space respect separation planes orientation vector point stored information store classify data manner possible retrieve easily. mean storage done given fresh data person whose data stored storage receptacle repository possible data retrieve stored data words given sample point retrieve another examplar closest present sample point. example sample point represent medical data person retrieve another data point person closest present sample point show sample point x-space close sample point stored receptacle discover fact calculating orientation vector taking product orientation vector points stored data base. comparison point product ov.ov maximum. oviously point closest sample point hence probability points belong class. associate class sample class point latter known. steps represented diagram nothing neural engine picture below deﬁne point side plane side plane goes planes ..eq. output array nothing hamming vector orientation vector therefore storage plan point hamming vector orientation vector point label label like pointer information stored receptacle space next label easy retrieval. sub-section describe classiﬁcation-retrieval system applied typical data. chosen example iris data ﬁrst introduced ronald fisher famous paper started ﬁeld multivariate analysis statistical classiﬁcation. reason prompted choose particular data fold data elementary complicated drowned details forget essence method wish introduce familiar almost every researcher across disciplines hence feel example appropriate choice interest people techique. iris data data ﬁrst collected edgar anderson data regarding three species ﬂowers iris setosa iris virginica iris versicolor data. four features measured sample length width sepals petals centimetres thus data dimensional. based data task classify sample ﬂower three classes given step means coefﬁcients planes classiﬁcation system shown ﬁgure built system basically ﬁnds ov’s incoming dimensional point case processing elements ..... ﬁrst layer planes. testing process testing process points presented point found ov’s point compared ov’s found that points test data point train data points test data ov’s differing closest match train data component points test data ov’s differing closest match train data components points test data ov’s differing closest match train data three components point wrongly classiﬁed class class conclusion method point test points others correctly classiﬁed according nearest match course since using planes separate training points train points obviosly exactly classiﬁed. method information ov’s application previous algorithm form pure clusters training data manner cluster within points belonging class. done another cluster discovery algorithm complexity easily possible determine architecture three layer neural network processing elements write weights processing elements withado. recalled weights processing elements nothing coefﬁcients planes separate clusters since known weights known. thus obtained non-iterative deterministic method building neural classiﬁer. cluster discovery algorithm simple algorithm obtaining pure bubbles data. mean pure bubble spherical region containing points belonging class given input collection points algorithm ﬁnds bubbles separate points belonging class points belonging different class. step another nearest point center also belongs class include find radius bubble distance center furtherest point contained bubble. step latest point class else step points separated planes. algorithm discover small pure clusters bubbles separated anexisting planes. however times required additional planes bubbles separated least plane. additional planes shown ﬁgure below. simple algorithm used separate bubbles drawing plane perpendicular line joining centers position plane suitably chosen outside bubbles. point used discover particular bubbles need separated additional planes. disabled cluster. cluster common points cluster disabling planes pass cluster. procedure adopted quickly every bubbles separated another requirement atleast plane separates clusters. discovered cluster discovery algorithm. using methods described determine neural architecture classify clusters. importantly case coefﬁcients planes already known hence weights processing elements already determined. hence method iterative like method point side plane side plane goes planese e... examine puts u... obtained orientation vector .ov) choose maximum positive value supose maximum cluster considered attractor input point whose coordinates considered belong class beprocedure results initially found bubbles discovered regrouping bubbles belong class cluster groups. cluster computed disabling planes pass merged bubbles. cluster unique though clusters belong class permitted ovs. stated process creating bubbles merging easily programmed using information points training data set. bubbles belonging different classes separate bubbles drawing necessary planes.in particular iris case needed draw planes. times point belonging class comes within central region bubble belonging different class need separate points creating bubbles. happen bubbles point. cluster discovery algorithm automatically. tested data neural network shown ﬁgure ﬁrst applying training points. points correctly classiﬁed. also tested network passing test points input points point wrongly classiﬁed class points correctly classiﬁed. conclusion paper used algorithm separates points ndimension space planes develop noniterative classiﬁcation techniques solving pattern recognition problems using neural networks. methods developed paper application neural research deep learning noniterative deterministic nature also efﬁciency speed. strongly felt potential supersede classiﬁcation methods iterative nature rely error minimization. acknowledgements authors thank management sreenidhi narsimaha reddy mahi sustained support; aruna varanasi colleagues creating happy environment. authors k.e. k.d.r thank spouses suhasini amani resp. unwavering faith encouragement. appendix particulars iris data plane coefficients iris data taken http archive.ics.uci.edu mlmachinelearning-databases iris. points coordinates removed point order points before points point renumbered results method results method list bubbles following list contains points within bubble. sufﬁxes indicate bubble number bubble. example bubble number contains points. total number bubbles list clusters bubbles merged/ regrouped clusters. cluster unique though clusters belong class permitted ovs. particular case required introduce additional planes separate bubbles belong different classes. distribution bubbles clusters given below numbers calculated algorithms given appendix completely determined architecture iris problem shown last section paper results described therein.", "year": 2015}