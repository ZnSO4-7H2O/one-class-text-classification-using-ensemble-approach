{"title": "Moments in Time Dataset: one million videos for event understanding", "tag": ["cs.CV", "cs.AI"], "abstract": "We present the Moments in Time Dataset, a large-scale human-annotated collection of one million short videos corresponding to dynamic events unfolding within three seconds. Modeling the spatial-audio-temporal dynamics even for actions occurring in 3 second videos poses many challenges: meaningful events do not include only people, but also objects, animals, and natural phenomena; visual and auditory events can be symmetrical or not in time (\"opening\" means \"closing\" in reverse order), and transient or sustained. We describe the annotation process of our dataset (each video is tagged with one action or activity label among 339 different classes), analyze its scale and diversity in comparison to other large-scale video datasets for action recognition, and report results of several baseline models addressing separately and jointly three modalities: spatial, temporal and auditory. The Moments in Time dataset designed to have a large coverage and diversity of events in both visual and auditory modalities, can serve as a new challenge to develop models that scale to the level of complexity and abstract reasoning that a human processes on a daily basis.", "text": "abstract—we present moments time dataset large-scale human-annotated collection million short videos corresponding dynamic events unfolding within three seconds. modeling spatial-audio-temporal dynamics even actions occurring second videos poses many challenges meaningful events include people also objects animals natural phenomena; visual auditory events symmetrical time transient sustained. describe annotation process dataset analyze scale diversity comparison large-scale video datasets action recognition report results several baseline models addressing separately jointly three modalities spatial temporal auditory. moments time dataset designed large coverage diversity events visual auditory modalities serve challenge develop models scale level complexity abstract reasoning human processes daily basis. best things life things moments raining walking splashing resting laughing crying jumping etc. moments happening world unfold time scales second minutes occur different places involve people animals objects well natural phenomena like rain wind silence. particular interest moments seconds represent ecosystem changes surroundings convey enough temporal information interpret auditory visual dynamic world. here introduce moments time dataset collection million short videos label each corresponding actions events unfolding within seconds. crucially temporal events length correspond average duration human working memory working memory short-term memory-in-action buffer specialized representing information changing time. three seconds temporal envelope holds meaningful actions people objects phenomena actors bundling three seconds actions together allows creation \"compound\" activities occurring longer time scale. example picking object carrying away running could interpreted compound action \"stealing\" \"saving\" \"delivering\" depending social context ownership type place activity occurs monfort zhou andonian ramakrishnan vondrick oliva massachusetts institute technology massachusetts cambridge usa. brown gutfreund international business machines binney cambridge usa. bargal boston university cummington mall boston usa. hypothetically describing \"stealing\" event details movement joint limb persons involved. however naturally describe compound events. instead verbs \"picking\" \"carrying\" \"running\". actions typically occur time window seconds. ability automatically recognize short actions core step automatic video comprehension. increasing availability large datasets enabling rapid progress challenging computer vision problems event activity detection common-sense interpretation prediction future events. modeling spatial-temporal dynamics even events occurring second videos poses daunting challenge. instance inspecting videos dataset labeled action \"opening\" people opening doors gates drawers curtains presents animals humans opening eyes mouths arms even ﬂower opening petals. furthermore cases frames reverse order actually depict different action temporal aspect case crucial recognition. humans recognize mentioned scenarios belong category \"opening\" even though visually look different other. common transformation occurs space time involving certain agents and/or objects allows humans associate semantic meaning action \"opening\". challenge develop models recognize transformations allow discriminate different actions generalize agents settings within action. fig. sample videos. day-to-day events happen many types actors different environments different scales. moments time dataset signiﬁcant intra-class variation among categories. illustrate frame video samples actions. example engines open books open tulips open. understand actions dynamics videos. best knowledge collection largest humanannotated video datasets capturing visual and/or audible short events produced humans animals objects nature. classes chosen include commonly used verbs english language covering wide diverse semantic space. work presents ﬁrst version moments time dataset includes action label video different action classes. clearly could action taking place even video three seconds long. hinder performance action recognition models predict action correctly penalized ground truth include action. therefore believe accuracy measure commonly used computer vision models report classiﬁcation performances meaningful version dataset. main purpose paper introduce moments time dataset itself section report experimental results several known models trained tested dataset addressing separately jointly three modalities spatial temporal auditory. likely unfeasible teach exhaustive list possible human-object-element interactions activities strategy provide deep learning algorithms large coverage ecosystem visual auditory moments. diversity moments time dataset enable models learn discriminant information necessarily taught fully supervised manner allowing models robust unexpected events generalize novel situations tasks. related work video datasets large scale image datasets imagenet places allowed great progress made visual recognition static images. years size video datasets video understanding grown steadily. weizmann early datasets human action understanding. hollywood used feature length movie ﬁlms labelme video used consumer video create video datasets action recognition future prediction. dataset thumos datasets built videos become important benchmarks video classiﬁcation. jhmdb human activity categories joints annotated. kinetics youtube-m introduced large number event categories leveraging public videos youtube. micro-videos dataset uses social media videos study open-world vocabulary video understanding. activitynet explores recognizing activities video explores recognizing ﬁne-grained actions localization. something something dataset crowdsourced workers collect compositional video dataset charades uses crowdsourced workers perform activities collect video data. vlog dataset uses daily human activities collect data natural spatio-temporal context. described below features moments time dataset diversity scale. particular focus brief moments video classiﬁcation availability video datasets enabled signiﬁcant progress video understanding classiﬁcation. early work laptev lindeberg developed space-time interest point descriptors klaser designed histogram features video. pioneering work wang developed dense action trajectories seperating foreground motion camera motion. sadanand corso designed actionbank high-level representation video action classiﬁcation pirsiavash ramanan leverage grammar models temporally segmenting actions video. advances deep convolutional networks enabled large-scale video classiﬁcation models various approaches fusing frames temporal dimension explored sportm dataset stream cnns stream static images stream optical ﬂows proposed fuse information object appearance short-term motions convolutional networks convolution kernels extract features sequence dense frames. temporal segment networks sample frames optical different time segments extract information activity recognition cnn+lstm model uses extract frame features lstm integrate features time also used recognize activities videos recently networks stream cnns inﬂated convolutions dense optical sequences achieve state performance kinetics dataset sound classiﬁcation environmental ambient sound recognition rapidly growing area research. stowell collected early dataset assembled challenge sound classiﬁcation piczak collected dataset ﬁfty sound categories enough train deep convolutional models salamon released dataset urban sounds gemmeke videos sound dataset collection. recent work developing models sound classiﬁcation deep neural networks. example piczack pioneered early work convolutional networks sound classiﬁcation aytar transfer visual models sound auditory analysis hershey develop large-scale convolutional models sound classiﬁcation arandjelovi´c zisserman train sound vision representations jointly. moments time dataset many videos visual auditory signals enabling multi-modal video recognition. moments time dataset goal project design high-coverage highdensity balanced dataset hundreds verbs depicting moments seconds. high-quality datasets broad coverage data space high diversity density samples ability scale time article written ﬁrst version moments time dataset consists -second videos corresponding different verbs depicting action activity. verb associated videos resulting large balanced dataset learning basis dynamical events videos. whereas several verbs actions often needed describe richness seconds event ﬁrst release comes ground truth verb video. importantly dataset designed have grow towards large diversity inter-class intra-class variation represent dynamical event different levels abstraction building large-scale dataset important appropriate class vocabulary contains large coverage diversity classes. order ensure captured criteria began building vocabulary using commonly used verbs verbnet clustered verbs according conceptual structure meaning using features verb propbank framenet ontonotes clusters sorted according combined frequency verb member cluster according coca. example found cluster associated \"grooming\" contained following verbs order common least common \"washing showering bathing soaping grooming shampooing manicuring moisturizing ﬂossing\". verbs belong multiple clusters different frames use. instance \"washing\" also belongs group associated cleaning mopping scrubbing etc. given clusters iteratively selected common verb common cluster added vocabulary. verb removed member clusters repeated process remaining verbs set. method creates list verbs ordered according frequency verb also frequency semantic meaning. sorted list verbs hand picked common verbs could recognized -second video. form vocabulary dataset crawl internet download videos related verb variety different sources includes parsing video metadata crawling search engines build list candidate videos verb vocabulary. randomly -second section video grouped corresponding verb. verb-video tuples sent amazon mechanical turk annotation. contains different second videos related single verb ground truth videos used control. ﬁrst questions used train workers task allow continue without selecting correct answer. results hits earn control videos included dataset. chose binary-classiﬁcation setup large number verb categories makes class selection difﬁcult task workers. video training annotation least times require human consensus least considered positive label. validation test increase minimum number rounds annotation human consensus least threshold allow videos actions slightly difﬁcult recognize dataset. figure shows example annotation task presented workers. motivation project gather large balanced diverse dataset training models video understanding. since pull videos different sources able include large breadth diversity would challenging using single source. total collected labelled videos moment classes. graph left figure shows full distribution across classes average number labeled videos class median building diverse dataset restrict active agent videos humans. many events \"walking\" \"swimming\" \"jumping\" \"carrying\" speciﬁc human agents. addition classes contain videos human agents true video understanding models able recognize event across agent classes. mind decided build dataset general across agents present challenge ﬁeld video understanding. middle graph figure shows distribution videos according agent type class. fig. dataset statistics. left distribution number videos belonging category. middle class distribution videos humans animals objects agents completing actions. right class distribution videos require audio recognize class category videos categorized visual information. fig. comparison datasets. dataset provide different comparisons. left total number action labels training set. middle average number videos class .right coverage objects scenes recognized networks trained places imagenet. another feature moments time dataset include sound-dependant classes. restrict videos events seen moment heard video still include presents another challenge purely visual models sufﬁcient completely solve dataset. right graph figure shows distribution videos according whether event video seen. order highlight points dataset compare scale object-scene coverage objectscene-action correlations found moments time large-scale video datasets action recognition. include ucf- activitynet kinetics somethingsomething charades figure compares total number action labels used training average number videos belong class training increase scale action recognition beneﬁcial training large generalizable systems machine learning. additionally compared coverage objects scenes recognized within videos. type comparison helps showcase visual diversity dataset. accomplish this extract frames video evenly spaced video duration layer resnet trained imagenet layer resnet trained places frame average prediction results video. compare total number objects scenes recognized networks figure graph shows scene categories places object categories imagenet recognized dataset. closest dataset comparison kinetics recognized coverage scene categories places object categories imagenet. note comparing recognized categories prediction network. annotated scene locations objects video dataset. however comparison visual features recognized network still serve informative comparison visual diversity. model chance resnet-scratch resnet-places resnet-imagenet tsn-spatial bninception-flow resnet-dyimg tsn-flow soundnet tsn-stream trn-multiscale ensemble ensemble experimental setup data. training testing models video classiﬁcation dataset generate training videos videos class different moment classes. evaluate performance validation videos consists videos classes. additionally withhold test videos consisting videos class used evaluate submissions future action recognition challenge. preprocessing. extract frames videos fps. given videos variable resolution resize frames standard pixels. interest performance pre-compute optical consecutive frames using off-the-shelf implementation optical algorithm opencv toolbox formulation allows discontinuities optical ﬁeld thus robust noise. fast computation discretize values optical ﬁelds integers clip displacement maximum absolute value scale range displacements ﬁelds every optical frame stored grayscale images reduced storage consumption. correct camera motion subtract mean vector displacement ﬁeld stack. video frames random cropping data augmentation subtract imagenet mean images. evaluation metric. top- accuracy top- classiﬁcation accuracy scoring metrics. top- accuracy indicates percentage testing videos conﬁdent predicted label correct. top- accuracy indicates percentage testing videos groundtruth label among ranked predicted labels. top- accuracy appropriate video classiﬁcation videos contain multiple actions within baselines video classiﬁcation here present several baselines video classiﬁcation moments time dataset. show results three modalities well recent video classiﬁcation models temporal segment networks temporal relation networks explore combining models improve recognition spatial modality. experiment layer resnet network classiﬁcation given frames videos. training input network randomly selected frames video. testing average prediction equi-distant frames. train networks weights trained scratch resnet-scrach initialized places resnet-places initialized imagenet resnet-imagenet. auditory modality. many actions recognized visually sound contains complementary even mandatory information recognition particular categories cheering talking seen figure waveforms input modality follow network architecture soundnet output layer changed predict moment categories. ﬁnetune model pre-trained unlabeled videos flickr soundnet. temporal modality. report results temporal modality models. first following compute optical adjacent frames encoded cartesian coordinates displacements. optic images stacking together consecutive frames form channel image bninception base model modifying ﬁrst convolutional layer accept input channels instead bninception-flow. second compute dynamic images means spatiotemporal encoding videos. dynamic image summarizes gist video clip single image. dynamic images represent video ranking function frames using ranksvm ranksvm uses implicit video label frame ordering. residual network layers architecture training dynamic images resnet-dyimg. also train recent action recognition models temporal segment networks temporal relation networks temporal segment networks efﬁciently capture long-range temporal structure videos using sparse frame-sampling strategy. tsn’s spatial stream tsn-spatial fused optical stream tsn-flow average consensus form stream tsn-stream. base model stream bninception model three time segments. temporal relation networks designed explicitly learn temporal dependencies video segments best characterize particular action. plug-and-play\" module model several short-range long-range temporal dependencies simultaneously classify actions unfold multiple time scales. paper multi-scale relations trn-multiscale trained frames using inceptionv base model. number multi-scale relations used note classify trn-multiscale spatiotemporal modality training utilizes temporal dependency different frames. ensemble. combine different modalities action prediction conduct model ensemble performing model modality ensemble strategies ﬁrst average ensemble fig. examples missed detections show examples videos prediction top-. common failures often background clutter poor generalization across agents simply average predicted class probability models; second ensemble concatenate predicted class probabilities stream multi-class one-versus-all linear predict moment categories ensemble enables learn weighted average modalities dependent category. clear appearances lower intra-class variation example bowling surﬁng frequently happen speciﬁc scene categories. difﬁcult categories covering slipping plugging tend wide spatiotemporal support happen scenes objects. recognizing actions uncorrelated scenes objects seems pose challenge video understanding. baseline results table shows top- top- accuracy baseline models validation set. best single model trn-multiscale top- accuracy top- accuracy ensemble model gets top- accuracy figure illustrates high scoring predictions baseline models. qualitative result suggests models recognize moments well action well-framed close however model frequently misﬁres category ﬁne-grained background clutter. figure shows examples ground truth category detected top- predictions either signiﬁcant background clutter difﬁculty recognizing actions across agents. visualize prediction given model generating heatmaps video samples using class activation mapping figure highlights informative image regions relevant prediction. top- prediction resnetimagenet model individual frame given video. understand challenges figure breaks performance category different models modalities. categories perform best tend figure also shows roles different modalities play category performance. auditory models qualtiatively different performance category versus visual models suggesting sound provides complementary signal vision recognizing actions videos. however full ensemble model category performance fairly correlated single image spatial model. given relatively performance moments time suggests still room capitalize temporal dynamics better recognize action categories. figure shows common confusions categories. generally common failures errors ﬁne-grained recognition confusing submerging versus swimming lack temporal reasoning confusing opening versus closing. confusions single frame model full model qualitatively similar suggesting temporal reasoning remains critical challenge visual models. auditory confusions however qualitatively different showing sound important complementary signal video understanding. expect that advance performance dataset models need rich understanding dynamics ﬁnegrained recognition audio-visual reasoning. fig. predictions attention show predictions resnet-imagenet spatial model held-out video data heatmaps highlight informative regions frames. example recognizing action chewing network focuses moving mouth. present moments time dataset large-scale collection video understanding covering wide class dynamic events involving different agents unfolding three seconds. report results several baseline models addressing separately jointly three modalities spatial temporal auditory. dataset presents difﬁcult task ﬁeld computer vision labels correspond different levels abstraction thus serve challenge develop models appropriately scale level complexity abstract reasoning human processes daily basis. future versions dataset include multi-labels action description focus growing diversity agents adding temporal transitions actions agents performed. also plan organize challenges based various releases dataset general action recognition cognitivelevel tasks modeling transformations transfer learning across different agents settings. example consider challenge training actions performed solely humans testing actions performed animals. humans expert analogies ability seemingly transfer knowledge events partial similarity. core common sense reasoning creativity analogies occur across modalities different agents different levels abstraction project aims produce datasets variety levels abstraction agents serve step-stone towards development learning algorithms able build analogies things imagine synthesis novel events interpret compositional scenarios. sami abu-el-haija nisarg kothari joonseok paul natsev george toderici balakrishnan varadarajan sudheendra vijayanarasimhan. youtube-m large-scale video classiﬁcation benchmark. arxiv preprint arxiv. yusuf aytar carl vondrick antonio torralba. soundnet learning sound representations unlabeled video. sugiyama luxburg guyon garnett editors advances neural information processing systems pages curran associates inc. collin baker charles fillmore john lowe. berkeley framenet project. proceedings international conference computational linguistics volume coling stroudsburg association computational linguistics. hakan bilen basura fernando efstratios gavves andrea vedaldi stephen gould. dynamic image networks action recogieee conference computer vision pattern nition. recognition june moshe blank lena gorelick shechtman michal irani ronen basri. actions space-time shapes. computer vision iccv tenth ieee international conference volume pages ieee fabian caba heilbron victor escorcia bernard ghanem juan carlos niebles. activitynet large-scale video benchmark human activity understanding. proceedings ieee conference computer vision pattern recognition pages fig. accuracy distribution categories show left best ensemble model compared soundnet accuracy distribution. middle shows best ensemble model compared resnet-imagenet best spatial model. right shows performances soundnet auditory modality. visualization subset categories labeled. fig. confused categories show commonly confused categories three models. left shows confusions ensemble model combines spatial temporal auditory modalities. middle shows confusions best spatial model right shows confusions sound model. ﬁrst column table shows frequency confusion. jeffrey donahue lisa anne hendricks sergio guadarrama marcus rohrbach subhashini venugopalan kate saenko trevor darrell. long-term recurrent convolutional networks visual recognition description. proceedings ieee conference computer vision pattern recognition pages jort gemmeke daniel ellis dylan freedman jansen wade lawrence channing moore manoj plakal marvin ritter. audio ontology human-labeled dartaset audio events. ieee icassp joanna materzy ´nska susanne westphal heuna valentin haenel ingo fruend peter yianilos moritz mueller-freitag the\" something something\" video database learning evaluating visual common sense. arxiv preprint arxiv. chunhui chen sudheendra vijayanarasimhan caroline pantofaru david ross george toderici yeqing susanna ricco rahul sukthankar cordelia schmid video dataset spatio-temporally localized atomic visual actions. arxiv preprint arxiv. eduard hovy mitchell marcus martha palmer lance ramshaw ralph weischedel. ontonotes proceedings human language technology conference naacl companion volume short papers naacl-short pages stroudsburg association computational linguistics. sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. international conference machine learning pages andrej karpathy george toderici sanketh shetty thomas leung rahul sukthankar fei-fei. large-scale video classiﬁcation proceedings ieee convolutional neural networks. conference computer vision pattern recognition pages joao carreira karen simonyan brian zhang chloe hillier sudheendra vijayanarasimhan fabio viola green trevor back paul natsev kinetics human action video dataset. arxiv preprint arxiv. alexander klaser marcin marszałek cordelia schmid. spatio-temporal descriptor based d-gradients. bmvc british machine vision conference pages british machine vision association imagenet classiﬁcation deep convolutional neural networks. pereira burges bottou weinberger editors advances neural information processing systems pages curran associates inc. bolei zhou aditya khosla agata lapedriza aude oliva antonio torralba. learning deep features discriminative proceedings ieee conference computer localization. vision pattern recognition pages bolei zhou agata lapedriza aditya khosla aude oliva antonio torralba. places million image database scene ieee transactions pattern analysis machine recognition. intelligence bolei zhou agata lapedriza jianxiong xiao antonio torralba aude oliva. learning deep features scene recognition using places database. advances neural information processing systems ivan laptev tony lindeberg. space-time interest points. international conference computer vision nice france pages ieee conference proceedings ivan laptev marcin marszalek cordelia schmid benjamin rozenfeld. learning realistic human actions movies. computer vision pattern recognition cvpr ieee conference pages ieee karol piczak. environmental sound classiﬁcation convolutional neural networks. machine learning signal processing ieee international workshop pages ieee hamed pirsiavash deva ramanan. detecting activities daily living ﬁrst-person camera views. computer vision pattern recognition ieee conference pages ieee hamed pirsiavash deva ramanan. parsing videos actions segmental grammars. proceedings ieee conference computer vision pattern recognition pages olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya imagenet large scale visual khosla michael bernstein international journal computer vision recognition challenge. sreemanananth sadanand jason corso. action bank high-level representation activity video. computer vision pattern recognition ieee conference pages ieee justin salamon christopher jacoby juan pablo bello. dataset taxonomy urban sound research. proceedings international conference multimedia pages christian schuldt ivan laptev barbara caputo. recognizing pattern recognition human actions local approach. icpr proceedings international conference volume pages ieee gunnar sigurdsson varol xiaolong wang farhadi ivan laptev abhinav gupta. hollywood homes crowdsourcing data collection activity understanding. corr abs/. karen simonyan andrew zisserman. two-stream convolutional networks action recognition videos. advances neural information processing systems pages stowell dimitrios giannoulis emmanouil benetos mathieu lagrange mark plumbley. detection classiﬁcation ieee transactions multimedia acoustic scenes events. christian szegedy vincent vanhoucke sergey ioffe shlens zbigniew wojna. rethinking inception architecture computer vision. proceedings ieee conference computer vision pattern recognition pages tran lubomir bourdev fergus lorenzo torresani manohar paluri. learning spatiotemporal features convolutional networks. proceedings ieee international conference computer vision pages heng wang alexander kläser cordelia schmid cheng-lin liu. action recognition dense trajectories. computer vision pattern recognition ieee conference pages ieee limin wang yuanjun xiong wang qiao dahua xiaoou tang gool. temporal segment networks towards good practices deep action recognition. proc. eccv jenny yuen bryan russell antonio torralba. labelme video building video database human annotations. computer vision ieee international conference pages ieee", "year": 2018}