{"title": "Evolving Culture vs Local Minima", "tag": ["cs.LG", "cs.AI", "I.2.6"], "abstract": "We propose a theory that relates difficulty of learning in deep architectures to culture and language. It is articulated around the following hypotheses: (1) learning in an individual human brain is hampered by the presence of effective local minima; (2) this optimization difficulty is particularly important when it comes to learning higher-level abstractions, i.e., concepts that cover a vast and highly-nonlinear span of sensory configurations; (3) such high-level abstractions are best represented in brains by the composition of many levels of representation, i.e., by deep architectures; (4) a human brain can learn such high-level abstractions if guided by the signals produced by other humans, which act as hints or indirect supervision for these high-level abstractions; and (5), language and the recombination and optimization of mental concepts provide an efficient evolutionary recombination operator, and this gives rise to rapid search in the space of communicable ideas that help humans build up better high-level internal representations of their world. These hypotheses put together imply that human culture and the evolution of ideas have been crucial to counter an optimization difficulty: this optimization difficulty would otherwise make it very difficult for human brains to capture high-level knowledge of the world. The theory is grounded in experimental observations of the difficulties of training deep artificial neural networks. Plausible consequences of this theory for the efficiency of cultural evolutions are sketched.", "text": "propose theory relates difﬁculty learning deep architectures culture language. articulated around following hypotheses learning individual human brain hampered presence effective local minima; optimization difﬁculty particularly important comes learning higher-level abstractions i.e. concepts cover vast highly-nonlinear span sensory conﬁgurations; high-level abstractions best represented brains composition many levels representation i.e. deep architectures; human brain learn high-level abstractions guided signals produced humans hints indirect supervision high-level abstractions; language recombination optimization mental concepts provide efﬁcient evolutionary recombination operator gives rise rapid search space communicable ideas help humans build better high-level internal representations world. hypotheses together imply human culture evolution ideas crucial counter optimization difﬁculty optimization difﬁculty would otherwise make difﬁcult human brains capture high-level knowledge world. theory grounded experimental observations difﬁculties training deep artiﬁcial neural networks. plausible consequences theory efﬁciency cultural evolution sketched. interesting connections sometimes made interface artiﬁcial intelligence research sciences understand human brains cognition language society. paper propose elaborate theory interface inspired observations rooted machine learning research so-called deep learning. deep learning techniques training models many levels representation hierarchy features concepts implemented artiﬁcial neural networks many layers. deep architecture typically trained levels representation fact consider deep learning algorithm discover appropriate number levels representation based training data. visual cortex believed levels. theoretical arguments also made suggest deep architectures necessary efﬁciently represent kind high-level concepts required artiﬁcial intelligence paper starts experimental observations difﬁculties training deep architectures builds theory role cultural evolution reduce difﬁculty learning high-level abstractions. gist theory training deep architectures found brain difﬁcult optimization difﬁculty cultural evolution ideas serve whole population humans many generations efﬁciently discover better solutions optimization problem. neural networks artiﬁcial neural networks computational architectures learning algorithms inspired computations believed take place biological neural networks brain dominant successful approaches training artiﬁcial neural networks based idea learning proceed gradually optimizing criterion neural network typically free parameters synaptic strengths associated connections neurons. learning algorithms formalize computational mechanism changing parameters take account evidence provided observed examples. different learning algorithms neural networks differ speciﬁcs criterion optimize often approximately analytic exact solution possible. on-line learning plausible biological organisms involves changes parameters either example seen small batch examples seen training criterion case biological organisms could imagine ultimate criterion involves expected future rewards however intermediate criteria typically involve modeling observations senses i.e. improving prediction could made part observed sensory input given part improving prediction future observations given past observations. mathematically often captured statistical criterion maximizing likelihood i.e. maximizing probability model implicitly explicitly assigns observations. learning learners exploit observations order construct functions capture statistical relationships observed variables. example learners build predictors future events given past observations associate observed different modalities sensors. used learner predict unobserved variable given observed ones. learning problem formalized follows. vector parameters free change learning represent example i.e. measurement variables environment relevant learning agent. agent seen past history realistic cases also depends actions agent. measurement error loss minimized whose future expected value criterion minimized. simple case ignore effect current actions future rewards consider value particular solution learning problem long term objective learner minimize criterion expected future error unknown probability distribution world generates examples learner. realistic setting reinforcement learning objective learner often formalized maximization expected value weighted future rewards weights decay future note training criterion deﬁne called generalization error expected error examples error measured past training examples stationary i.i.d. hypothesis expected future reward estimated ongoing online error average rewards obtained agent. case although training criterion cannot computed exactly unknown learner) criterion approximately minimized stochastic gradient descent learner needs estimate gradient example-wise error respect parameters i.e. estimate effect change parameters immediate error. estimator example could based single example day’s worth examples. small constant called learning rate gain. note examples continuously sampled unknown distribution instantaneous online gradient unbiased estimator generalization error gradient i.e. online learner directly optimizing generalization error. note refer single learning agent exclude effect interactions learning agents like occur communication humans human society. later advocate fact takes account learning stationary i.i.d case examples independently come stationary distribution many machine learning algorithms minimizes training error plus regularization penalty prenote criterion mind specialized single task often case applications machine learning. instead biological learning agent must make good predictions contexts encounters especially relevant survival. type context agent must take decision corresponds task. agent needs solve many tasks i.e. perform multi-task learning transfer learning self-taught learning tasks faced learner share underlying world surrounds agent brains probably take advantage commonalities. explain brains sometime learn task handful even example something seems almost impossible standard single-task learning algorithms. note also biological agents probably need address multiple objectives together. however practice since brain must take decisions affect criteria cannot decoupled lumped single criterion appropriate weightings example likely biological learners must cater predictive type criterion reward type criterion former explains curiosity ability make sense observations learn even derive immediate foreseeable beneﬁt loss. latter clearly crucial survival biological brains need focus modeling efforts matters survival. unsupervised learning learning agent prepare possible task future extracting much information possible observes i.e. ﬁguring unknown explanations observes. issue objective deﬁned terms animal survival reproduction well deﬁned depends behaviors animals whole ecology niche occupied animal interest. change improvements made animals species evolution learning individual animal species’s objective survival also changes. feedback loop means isn’t really static objective complicated dynamical system discussions regarding complications brings beyond scope paper. however interesting note component animal’s objective much stable unsupervised learning objective understanding world ticks. local minima stochastic gradient descent many optimization techniques perform local descent starting particular conﬁguration parameters makes small gradual adjustments average tend improve expected error training criterion. theory proposed relies following hypothesis main argument favor hypothesis would based assumption although state mind changes quickly synaptic strengths neuronal connectivity change gradually. learning algorithm form stochastic gradient descent approximates gradient chosen small enough gradually decrease high probability gradually decreased appropriate rate learner converge towards local minimum proofs usually unbiased case small bias necessarily hurtful shown contrastive divergence especially magnitude bias also decreases gradient decreases note paper talking local minima generalization error i.e. respect expected future rewards. machine learning terms optimization local minimum usually employed respect training criterion formed error training examples seen past learner could possibly overﬁt figure illustration learning proceeds local descent stuck near local minimum horizontal axis represents space synaptic conﬁgurations vertical axis represents training criterion ball represents learner’s current state tends downwards note space synaptic conﬁgurations huge represented schematically single dimension horizontal axis. effective local minima illustrated figure local minimum conﬁguration parameters small change yield improvement training criterion. consequence local descent hypothesis true therefore biological brains would likely stop improving point sufﬁciently approached local minimum. practice learner relies stochastic gradient estimator continue change stochastic nature gradient estimator hovering stochastically around minimum. also quite possible biological learners enough lifetime really close actual local minimum plausible point progress slow practice trains artiﬁcial neural network learning algorithm based stochastic gradient descent often observes training saturates i.e. observable progress seen spite additional examples shown continuously. learner appears stuck near local minimum. difﬁcult verify learner really near local minimum call effective local minima. call effective limitations optimization procedure shape training criterion function parameters. learner equiped optimization procedure stuck effective local minima looks like stuck actual local minima happen training criterion complicated function parameters stochastic gradient descent sometimes practically stuck place possible improve directions powerful descent methods could escape many learning algorithms involve latent variables understood associated particulars factors contribute explain explain current recent observations. latent variables encoded activation hidden units think particular conﬁguration latent hidden variables corresponding state mind. boltzmann machine input presented many conﬁgurations latent variables possible inference mechanism normally takes place order explore possible conﬁgurations latent variables well observed input. inference often iterative although approximated initialized single bottom-up pass perception state-ofmind. inference stochastic deterministic whereas learning brains occurs scale minutes hours days inference occurs scale fraction second seconds. whereas learning probably gradual stochastic inference quickly jump thought pattern another less second. models boltzmann machine learning requires inference inner loop patterns latent variables well observed data reinforced changes synaptic weights follow. confuse local minima synaptic weights local minima inference. randomness association stimuli change state inference past inputs give impression stuck anymore escaped local minimum regards inference process necessarily learning process deep architectures parametrized families functions used model data using multiple levels representation. deep neural networks level associated group neurons sensory perception animal brains information travels quickly lower levels higher levels also many feedback connections well lateral connections neuron group neurons thought capturing concept feature aspect activated concept feature aspect present sensory input model generating internal conﬁguration includes concept feature aspect. note features actually come consciousness inner workings brains directly accessible consciousness. note also particular linguistic concept represented many neurons groups neurons activating particular pattern different levels ideas introduced central connectionist approaches cognitive science artiﬁcial neural networks concept distributed representation would symbolic systems represented single on/off associated brain large number neurons groups neurons activated together particular pattern. concepts close semantically i.e. share attributes overlap brain representation i.e. corresponding patterns activation bits many places. efﬁciency representation deeper architectures much efﬁcient terms representation functions shallow ones shown theoretical results speciﬁc families functions shallow architecture require exponentially resources necessary basic intuition true deep architecture re-use parameters sharing sub-functions build functions. write computer programs single main program instead write many subroutines call subroutines nested re-use provides ﬂexibility also great expressive power. however greater expressive power come price making learning task difﬁcult optimization problem. lower-level features used many ways deﬁne higher-level features interactions parameters levels makes optimization landscape much complicated. extreme many shallow methods associated convex optimization problem i.e. single minimum training criterion. figure example simple manifold space images associated rather lowlevel concrete concept corresponding rotations shrinking speciﬁc instance image drawn digit point manifold corresponds image obtained rotating translating scaling another image manifold. points manifold deﬁnes concrete concept associated drawing particular shape irrespective position angle scale. even learning simple manifolds difﬁcult learning much convoluted higher-dimensional manifolds abstract concepts much harder. high-level abstractions call high-level abstraction kind concept feature could computed efﬁciently deep structure brain edge detector image seen computed single layer neurons pixels using gabor-like ﬁlters. low-level abstraction. combining several detectors detect corners straight line segments curved line segments local simple shapes done layers neurons combined locally insensitive small changes position angle. consider hierarchy gradually complex features constructing detectors abstract concepts activated whenever stimulus within large possible input stimuli presented. higher-level abstraction stimuli represents highly-convoluted points highly curved manifold. picture manifold restrict concrete concept like image speciﬁc object uniform background. factors vary object constancy; correspond changes imaging geometry lighting mathematics help make sense manifolds. think images elicit thought abstract concept human even abstract contexts elicit thought concept riemann integral. contexts images associated high-level concept different other many complicated ways scientists know construct associated manifolds. concepts clearly higher-level others often higher-level concepts deﬁned terms lower-level ones hence forming hierarchy reminiscent kind hierarchy current deep learning algorithms discover discussion brings formulation hypothesis high-level abstractions representation brains. number results machine learning literature suggest training deeper architecture often difﬁcult training shallow following sense. trying train layers together respect joint criterion likelihood inputs conditional likelihood target classes given inputs results worse training shallow model generally suspect current training procedures deep networks underuse representation potential parameters available correspond form underﬁtting inability learning high-level abstractions. unsupervised layer-wise pre-training ﬁrst results nature appear bengio ranzato architecture gives different results depending initialization network weights either purely randomly based unsupervised layer-wise pre-training. idea layer-wise pre-training scheme train layer unsupervised training criterion learns representation taking input representation previous layer. layer thus trained sequence other. although probably biologically plausible such would plausible mechanism providing unsupervised signal layer makes learn better capture statistical dependencies inputs. layer-local signal could still combined global training criterion might help train deep networks optimization difﬁculty coordinating training layers simultaneously. another indication layer-local signal help train deep networks came work weston unsupervised layer-local signal combined supervised global signal propagated whole network. observation advantage brought layer-local signals also made context purely unsupervised learning deep stochastic network deep boltzmann machine pre-training layer restricted boltzmann machine optimizing deep boltzmann machine comprises levels authors able train whereas directly training random initialization problematic. summarize several still happen poor training lower levels corresponding poor representation learning. which ignores interaction levels except receiving input level below. results deep learning literature following observation training deep architectures easier hints provided function intermediate levels compute connected even obvious observation work artiﬁcial neural networks much easier teach network supervised learning expect unsupervised learning discover concept figure effect depth generalization error without layer-wise unsupervised pre-training training problem becomes difﬁcult deeper nets using layer-local initialize level helps push difﬁculty farther improve error rates. difﬁcult deeper architectures abstract concepts another clue training difﬁculty came later studies showing directly training layers together would make difﬁcult exploit extra modeling power deeper architecture would actually worse results number layers increased illustrated figure call observation erhan went attempt understand training difﬁculty studied trajectory deep neural networks training function space. trajectories illustrated figure point trajectory corresponds particular neural network parameter conﬁguration visualized two-dimensional point follows. first approximate function computed neural network non-parametrically i.e. outputs function large test consider neural networks behave similarly provide similar answers test examples. cannot directly network parameters compare neural networks function represented many different ways therefore associate network long vector containing elements concatenation network outputs test examples. vector point high-dimensional space compute points networks experiment. learn mapping points -dimensional approximations preserve local structure much possible using non-linear dimensionality reduction methods t-sne isomap figure allows draw number interesting conclusions observation training trick changes initial conditions descent procedure allows reach much better local minima better local minima appear reachable chance alone figure two-dimensional non-linear projection space functions visited artiﬁcial neural networks training. cross diamond circle represents neural network stage training color indicating starting blue moving towards red. networks computing similar function nearby graph. ﬁgure uses t-sne dimensionality reduction bottom ﬁgure uses isomap vertical crosses circles networks trained random initialization diamonds rotated crosses networks unsupervised pre-training initialization. used phrase single human learner later paper hypothesize collection human learners associated evolution culture help would otherwise effective local minima. finally presumed ability deeper architectures represent higher-level abstractions easily shallow ones discussion section leads human analogue deeper harder hypothesis reﬁnes local minima hypothesis note prevent high-level abstractions represented brain innate programming captured genes phrase single human learner excludes effects culture guidance humans subject next section. hypotheses true wonder humans still manage learn highlevel abstractions. seen much better solutions found learner initialized area gradient descent leads good solution genetic material might provide enough good starting point architectural constraints help learning abstractions. example could plausible explanation visual abstractions visual invariances could chance discovered evolution recent work learning algorithms computer vision also suggest architectural constraints greatly help performance deep neural network point even random parameters lower layers sufﬁce obtain reasonably good performance simple object recognition tasks. labeled examples hints however many abstractions master today recently appeared human cultures could genetically evolved must discovered least human point past propagated improved passed generation generation. return later greater question evolution ideas abstractions cultures ﬁrst focus mechanics communicating good synaptic conﬁgurations brain another. huge number synapses values make sense context values many others difﬁcult imagine recipe deﬁning individual abstractions could communicated individual another direct furthermore need hypothesized mechanism could help escape effective local minima faced single learner. main insight answering question come observation observation training single hidden layer neural network much easier training deeper provide hint function deeper layers capture training would much easier. extreme specifying particular neurons respond speciﬁc instances akin supervised learning. based premises answer propose relies learning agents exchanging bits information presence shared percept. communicating presence concept sensory percept something humans beneﬁt since youngest age. situation illustrated figure figure illustration communication brains typically language give hints higher levels brain concepts represented higher levels another brain. learners shared input produces utterance strongly associated high-level state mind representation also sees utterance input tries predict internal representations turns change speaking listening sense explanation forming respective state mind. language supervised training simple schema would help communicate concept brain another many encounters pairs learners. them learners faced similar percept exchange bits information bits example indicators presence highlevel concepts scene. indicators reﬂect neural activation associated high-level concepts. humans bits information could encoded linguistic convention helps receiver message interpret terms concepts already knows about. primitive cases communication scenario could occur animal human non-verbal communication. example adult animal sees prey could dangerous emits danger signal young animal could supervised training signal associate prey danger. imitation common form learning teaching prevalent among primates learner associates contexts corresponding appropriate behavior. richer form communication would already useful would require simply naming objects scene. humans innate understanding pointing gesture help identify object scene named. learner could develop repertoire object categories could become handy form theories world would help learner survive better. richer linguistic constructs involve combination concepts allow agents describe relations objects actions events sequences events causal links etc. even useful help learner form powerful model environment. guided learning hypothesis. human brain much easily learn high-level abstractions guided signals produced humans hints indirect supervision high-level abstractions. hypothesis related much previous work cognitive science example cognitive imitation observed monkeys learner imitates vocalization behavior something abstract corresponds cognitive rule. learning predicting linguistic output agents human guide another? encouraging learner predict labels teacher verbally associates given input conﬁguration schema figure necessary emitter directly provide supervision high-level layers receiver effect similar supervised learning achieved indirectly simply making sure receiver’s brain include training criterion objective predicting observes includes also linguistic output emitter context shared input percept. fact attentional emotional mechanisms increase importance given correctly predicting humans would approach even classical supervised learning setting. since already assumed training criterion human brains involves term prediction maximum likelihood could happen naturally enhanced innate reinforcement hence top-level hidden units receiver would receive training signal would encourage become good features sense predictive probability distribution utterances received would naturally achieved model deep boltzmann machine long higher-level units strong connection language units associating speech heard speech produced state affairs consistent global wiring structure human brains. process could work verbal non-verbal communication using different groups neurons model associated observations. terms existing learning algorithms could example imagine case deep boltzmann machine linguistic units ’clamped’ external linguistic signal received learner time lower-level sensory input units ’clamped’ external sensory signal conditions likelihood gradient received hidden units encouraging model joint distribution linguistic units sensory units. could imagine many sophisticated communication schemes beyond scenario. example could two-way exchange information. could agents potentially learn something presence shared percept. humans typically possess different views world parties communication event could beneﬁt two-way exchange. sense language provides humans summarize knowledge collected humans replacing real examples indirect ones thus increasing range events human brain could model. context would appropriate simply copy clone neural representations brain another learner must somehow reconcile indirect examples provided teacher world knowledge already represented learner’s brain. could pre-assigned role teacher student depending conﬁdence demonstrated agent particular percept pays less attention communicated output other. could aspects shared percept well mastered agent other vice-versa. humans capability know aspect situation surprising rationally welcome explanations provided others. make diffusion useful knowledge efﬁcient communicating agents keep track estimated degree authority credibility agents. would imagine parents older individuals human group would default credit products human social systems different individuals acquire less authority credibility. example scientists strive maximize credibility rigorous communication practices scientiﬁc method insists verifying hypotheses experiments designed test them. language evoke training examples even interesting scenarios derive linguistic abilities involve ability evoke input scene. need front danger teach describe dangerous situation mention dangerous diffusion fact verbal non-verbal communication animals humans happens noisy bandwidth-limited channel important keep mind. bits information exchanged useful elements communicated. objective maximize collective learning seems point communicating something receiver already knows. however reasons communicate smoothing social interactions acquiring status trust coordinating collective efforts etc. note necessary semantics language deﬁned priori process described work. since learning agent trying predict utterances others learning dynamics converge towards languages become attractors learning agents frequent linguistic representation given percept among population tend gradually dominate population. encounters uniformly random could multiple attractors simultaneously present population i.e. corresponding multiple spatially localized languages. figure general strategy reduce impact local minima followed continuation methods simulated annealing. idea consider sequence optimization problems start easier easy global optimum sequence problems ending problem interest time starting solution previously found easier problem tracking local minima along way. hypothesized demonstrated artiﬁcial neural networks following curriculum could help learners thus better solutions learning problem interest. connection curriculum learning idea learning improved guiding properly choosing sequence examples seen learner already explored past. ﬁrst proposed practical train animals shaping ease simulated learning complex tasks building easier tasks. interesting hypothesis introduced bengio proper choice training examples used approximate complex training criterion fraught local minima smoother gradually introducing subtle examples building already understood concepts typically done pedagogy. bengio propose learner goes sequence gradually difﬁcult learning tasks corresponds optimization literature continuation method annealing method allowing approximately discover global minima illustrated figure interestingly recently observed experimentally humans form curriculum learning strategy asked teach concept robot khan also propose statistical explanation curriculum learning strategy successful based uncertainty learner relevant factors explaining variations seen data. theories correct individual learner helped showing examples abstractions mastered learner also showing well-chosen examples appropriate sequence. sequence corresponds curriculum helps learner build higher-level abstractions lower-level ones thus defeating difﬁculty believed exist training learner capture higher-level abstractions. previous section proposed general mechanism knowledge transmitted brains without actually copy synaptic strengths instead taking advantage learning abilities brains transfer concepts examples. hypothesized mechanisms could help individual learner escape effective local minimum thus construct better model reality learner guided hints provided agents relevant abstractions. knowledge come another agent. knowledge arise ﬁrst place? discuss here. memes evolution noisy copies ﬁrst step back better brains could arise. plausible explanation better brains arise form search optimization huge space brain conﬁgurations genetic evolution form parallel search occurs rather slow time-scale. cultural evolution humans also form search space ideas memes meme unit selection cultural evolution. something copied mind another. like genes copy imperfect. memes analogous genes context cultural evolution genes memes co-evolved although appears cultural evolution occurs much faster scale genetic evolution. culture allows brains modify basic program propose culture also allows brains beyond single individual achieve simply observing nature. culture allows brains take advantage knowledge acquired brains elsewhere previous generations. together knowledge acquired individual brain combines four levels adaptation genetic evolution cultural evolution individual learning discovery inference four cases form adaptation play hypothesize associated form approximate optimization sense stated optimization hypothesis. also consider union four adaptation processes global form evolution adaptation learning guide evolution style baldwinian evolution). whereas genetic evolution form parallel search hypothesized individual learning local search performing approximate descent cultural evolution? cultural evolution based individual learning learners trying predict behavior speech output individuals stated guided learning hypothesis. even though individual learning relies local descent gradually improve single brain considering graph interactions humans evolving population must conclude cultural evolution like genetic evolution form parallel search illustrated figure figure illustration parallel search space synaptic conﬁgurations population learners. learners start conﬁgurations happen lead better solution descending training criterion. basic working principle evolution noisy copy also work cultural evolution meme noisily copied brain another meme sometimes slightly modiﬁed process. meme exists human’s brain aspect dynamics brain’s neural network typically allowing association words language high-level abstractions learned brain meme activated neural conﬁgurations associated arise different memes also connected sense high probability associated together echoing thoughts reasoning planning. remember meme copied process teaching example highly stochastic randomness encounters small number examples meme. creates highly variable randomly distorted version meme learner’s brain. comparison competing less successful meme. happen simply useful meme allows bearer survive longer communicate individuals better ideas promoted genetic evolution necessary copy whole genome individual bearing successful. instead cultural evolution humans mechanisms evaluate individual meme selectively promote good ideas likely subject discussion public communication e.g. public media even better scientiﬁc publications. science involves powerful mechanisms separate worth scientist worth ideas explain pace evolution ideas rapidly increased since mechanisms scientiﬁc discovery scientiﬁc dissemination memes place. fact good idea stand selected value means selective pressure much efﬁcient less hampered noisy evaluation results ﬁtness assigned whole individual integrates many memes genes. context premium assigned novelty cultures particular scientiﬁc research makes sense favors novel memes farther away existing ones. increasing degree exploration mechanism might expect would yield diversity solutions explored thus efﬁcient search achieved appropriate amounts premium novelty. fast-forward divide-and-conquer recombination evolution relied noisy copy principle could speed-up search best linearly respect number individuals population. instead trying random conﬁgurations individuals picking best selective pressure population individuals would discover good selection times faster average. useful hypothesize would enough make real dent optimization difﬁculty huge number poor local minima space synaptic conﬁgurations. fact evolution discovered evolutionary mechanism yield much larger speed-ups based sexual reproduction case genetic evolution. sexual reproduction interaction parent individuals genes genes order create combinations near-neighbors either parent. different simple parallel search explore conﬁgurations beyond local variations around randomly initialized starting stock. importantly recombination operator combine good previously found sub-solutions. maybe father exceptionally good genes eyes mother exceptionally good genes ears probability could both confer advantage before. kind transformation population candidate conﬁgurations called cross-over operator genetic algorithms literature cross-over recombination operator create candidate solutions combining parts previous candidate solutions. cross-over operators combine existing parts solutions form candidate solutions potential much greater speedsimple parallelized search based individual local descent operators potentially exploit form divide-and-conquer which well done could yield exponential speed-up. divide-and-conquer aspect recombination strategy work best sub-solutions contribute good parts good solutions receive high ﬁtness score. well known computer science divide-and-conquer strategies potential achieve exponential speedup compared strategies require blindly searching potential candidate solutions exponential speedup would achieved optimization combined parts done independently others. practice going case memes like genes take value context presence memes individual population. success rate recombination also important i.e. fraction recombination offsprings viable? encoding information genes great inﬂuence success rate well ﬁtness assigned good sub-solutions. hypothesize memes particularly good units selection respects deﬁnition units cultural information meaningfully recombined form knowledge. ideas summarized following hypothesis. memes divide-and-conquer hypothesis. language individual learning recombination memes constitute efﬁcient evolutionary recombination operator gives rise rapid search space memes helps humans build better high-level internal representations world. ideas come from? completely ideas emerge? according views stated here emerge intertwined effects. hand brain easily combine memes different memes inherited humans typically linguistic communication imitation. hand recombination well creations memes must arise optimization process taking place single learning brain tries reconcile sources evidence received kind unifying theory. search local parameter space involve stochastic search space neuronal ﬁring patterns example boltzmann machine neurons randomly probability depends activations connected neurons explore reach plausible interpretations current past observations given current synaptic strenghts. stochastic exploration conﬁgurations neuronal activation randomly arise better explaining data synaptic strengths change slightly make conﬁgurations likely future. already artiﬁcial neural networks learn discover concepts explain input. concepts edges parts face faces emerge deep bolzmann machine sees images faces means recombination operator memes much recombination sense cutting pasting parts together. also possible combinations optimized individual brains better empirical evidence learner access related ideas hinton nowlan global search combined local search effect smoothing ﬁtness function seen global search allowing half-baked ideas tuned working ones i.e. replacing needle haystack glowing needle haystack much easier ﬁnd. summarize motivated theoretical empirical work deep learning developed theory starting hypothesis high-level abstractions difﬁcult learn need represented highly non-linear computation associated enough levels representation difﬁculty corresponds learner getting stuck around effective local minima. proposed argued learning agents provide examples learner effectively change learner’s training criterion difﬁcult local minima minima anymore. happens communications agents provide kind indirect supervision higher levels brain makes task discovering explanatory factors variation much easier. furthermore brain-to-brain communication mechanism allows brains recombine nuggets knowledge called memes. individual learning corresponds searching recombinations variations memes good explaining data observed learners. memes created disseminated population learning agents value them creating cultural evolution. like genetic evolution cultural evolution efﬁciently searches thanks parallelism noisy copying memes creative recombination memes. hypothesize phenomenon provides divide-and-conquer advantage yields much greater speedup optimization performed compared linear speedup obtained simply parallelization search across population. needs done connect hypotheses wealth data ideas arising biological social sciences. certainly reﬁned expanded precise statements. central importance future work following paper could test hypotheses. although many hypotheses agree common sense would worthwhile verifying empirically extent possible. also quite plausible many supporting experimental results neuroscience cognitive science anthropology primatology already exist support hypotheses future work cleanly make appropriate links. test optimization hypothesis would seem require estimating criterion verifying learning improves average. proxy criterion might measurable brain itself example measuring variation presence reward-related molecules activity neurons associated reward. effect learning could tested varying number training trials respect rewarding task. optimization hypothesis considered true testing additional assumptions local descent hypothesis less obvious difﬁcult measure change synaptic strengths many places. however form stability synaptic strengths sufﬁcient condition guarantee optimization proceed small changes. already evidence deep abstraction hypothesis visual auditory cortex sense neurons belong areas away sensory neurons seem perform higher-level function. another type evidence comes time required solve different cognitive tasks since hypothesis would predict tasks requiring local minima hypothesis abstractions harder hypothesis ethically difﬁcult test directly almost corollaries previous hypotheses. indirect source evidence come raising primate without contact primates form guidance humans measure effect operational intelligence different ages. problem experiment would factors might also explain poor performance experiment would require human provides warmth caring guidance whatsoever even indirectly imitation. choosing solitary species orangutan would make sense question whether tested primate could learn survive well wild primates species. guided learning hypothesis could already supported empirical evidence effect education intelligence possibly observations feral children. important point intelligence tests chosen reproducing academic knowledge acquired education decisions integrated knowledge learned high-level abstractions could useful properly interpret situation take correspondingly appropriate decisions. using computational simulations artiﬁcial neural networks machine learning also test validity mechanisms escaping local minima thanks hints another agent. memes divide-and-conquer hypothesis could probably best tested computational models simulate learning population agents share discoveries communicating high-level abstractions corresponding observe question whether could linguistic communication mechanism would help population learners converge faster good solutions compared group isolated learning individuals previous computational work evolution language also relevant course. algorithms would work could also useful advance research machine learning artiﬁcial intelligence take advantage kind massive loose parallelism available type work related research algorithms inspired evolution ideas culture many hypotheses true also draw conclusions regarding efﬁciency cultural evolution different social structures inﬂuence efﬁciency i.e. yield greater group intelligence long run. main factors would seem inﬂuence efﬁciency efﬁciency exploration memes society rate spread good memes. efﬁciency exploration meme-space would boosted greater investment scientiﬁc research especially high-risk high potential impact areas. would also boosted encouraging diversity forms would mean individual humans explore less charted region meme-space. example diversity would boosted non-homogeneous education system general bias favoring openness ideas multiple schools thought generally marginal beliefs individual differences. second factor rate spread good memes would boosted communication tools internet particular open free access education information general scientiﬁc results particular. investment education would probably strongest contributors factor interesting contributors would social structures making easy every individual disseminate useful memes e.g. publish operation non-centralised systems rating published helping interesting ideas bubble spread faster contributing diversity memes efﬁcient dissemination useful memes. good rating systems could help humans detect selfish memes look good self-propagate easily wrong reasons attempts objectivity replicability scientists using help there. author would like thank caglar gulcehre aaron courville myriam cˆot´e olivier delalleau useful feedback well nserc canada research chairs funding.", "year": 2012}