{"title": "SparsityBoost: A New Scoring Function for Learning Bayesian Network  Structure", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We give a new consistent scoring function for structure learning of Bayesian networks. In contrast to traditional approaches to scorebased structure learning, such as BDeu or MDL, the complexity penalty that we propose is data-dependent and is given by the probability that a conditional independence test correctly shows that an edge cannot exist. What really distinguishes this new scoring function from earlier work is that it has the property of becoming computationally easier to maximize as the amount of data increases. We prove a polynomial sample complexity result, showing that maximizing this score is guaranteed to correctly learn a structure with no false edges and a distribution close to the generating distribution, whenever there exists a Bayesian network which is a perfect map for the data generating distribution. Although the new score can be used with any search algorithm, we give empirical results showing that it is particularly effective when used together with a linear programming relaxation approach to Bayesian network structure learning.", "text": "give consistent scoring function structure learning bayesian networks. contrast traditional approaches scorebased structure learning bdeu complexity penalty propose data-dependent given probability conditional independence test correctly shows edge cannot exist. really distinguishes scoring function earlier work property becoming computationally easier maximize amount data increases. prove polynomial sample complexity result showing maximizing score guaranteed correctly learn structure false edges distribution close generating distribution whenever exists bayesian network perfect data generating distribution. although score used search algorithm give empirical results showing particularly eﬀective used together linear programming relaxation approach bayesian network structure learning. consider fundamental problem statistics machine learning automatically extract structure data? mathematically problem formalized learning structure bayesian network discrete variables. bayesian networks refer compact factorization multivariate probability distribution one-to-one ∗current aﬃliation search algorithms shutteracyclic graph structure conditional probability distribution random variable depends values parent variables. application bayesian network structure learning discovery protein regulatory networks gene expression cytometry data existing approaches structure learning follow basic methodologies either search structures maximize likelihood observed data test conditional independencies constrain space possible structures. former approach leads extremely diﬃcult combinatorial optimization problems space possible bayesian networks exponentially large eﬃcient algorithms known maximizing scores. latter approach gives fast algorithms often leads poor structure recovery outcomes independence tests inconsistent sample size problems violations assumptions. formulate objective function structure learning complete data obtains best worlds score-based method based predominantly likelihood also makes conditional independence information. particular objective sparsity boost corresponding log-probability conditional independence test correctly shows edge cannot exist. show empirically objective substantially outperforms previous state-of-the-art methods structure learning. particular synthetic distributions learns true network structure less half data tenth computation. contributions paper introduction scoring function proof consistency carefully designed importance sampling algorithm efﬁciently computing conﬁdence scores used objective. proof sample complexity importance sampling algorithm develop several results information theory constructing precise mappings parametrization distributions variables mutual information characterizing rate convergence various quantities relating mutual information. expect many techniques developed broadly useful beyond bayesian network structure learning. paper considers problem learning bayesian network structure complete data collection random variables. reasons explain next section results restricted case variables binary i.e. formally bayesian network speciﬁed pair every variable conditionally independent nondescendants given parents. joint distribution shown factorize xpa) denotes parent variable refers assignment parents. bayesian network called independence distribution independence relationships implied present going step further called perfect independence conditional independence relationships implied ones present denote sequence observations random variables generated i.i.d. unknown words case structure learning problem strictly equivalent case hypothesis testing well-studied classic problem statistics speciﬁcally testing hypothesis whether independent. case three variables equivalence longer holds strict sense. constraint-based approaches results conditional independence tests infer model structure. methods solve structure learning problem sequentially ﬁrst learning undirected skeleton graph skel orienting edges obtain dag. assuming perfect conditionally independent conclude neither either parents parents separating proving conditional independence thus make assumption variable ﬁxed number parents yield polynomial time algorithm structure learning however approach number drawbacks diﬃculty setting thresholds propagation errors inconsistencies. measure conditional independence conditioned given inﬁnite data variables independent mutual information zero. however ﬁnite data mutual information biased away zero result diﬃcult distinguish between independence dependence. here log-likelihood data given number parameters weighting function property score called theoretically justiﬁed terms bayesian probability. intuitively explain bic/mdl score log-likelihood regularized complexity penalty keep fully connected models always winning. finding structure maximizes score known nphard heuristic algorithms proposed running time solving integer linear programs dramatically increases amount data used learning increases counter-intuitive data make learning problem easier harder. core problem amount data increases likelihood term grows magnitude whereas complexity penalty shrinks. necessary prove scoring functions consistent i.e. limit inﬁnite data structure maximizes score fact true structure. consequence however score becomes near optimum large number local maxima making optimization problem extremely diﬃcult solve. design scoring function structure learning consistent easy solve regardless amount data available learning. property want scoring function amount data increases optimization becomes easier harder. little data available reduce existing scoring functions. deﬁne testing procedure input null hypothesis alternative hypothesis type error deﬁned probability test rejecting true type error deﬁned probability test falsely accepting theory neyman-pearson hypothesis testing composite hypotheses tells construct hypothesis test maximal power setting test corresponds computing deciding test statistic exintuitive explanation type error probability obtaining test statistic extreme test statistic hand high probability power test threshold approaches approaches exponentially fast hand high probability power approaches approaches largest improvement score. algorithms separate constraintscore-based approaches distinct steps contrast approach directly incorporates conditional independence tests term score itself. work aware studied incorporation reliability independence tests score-based structure search campos objective function diﬀerent ours comparing empirical mutual information expected value assuming independence contrast campos’s score sparsityboost score consistent provably able recover true structure. importance using type error. knowledge previous approaches bayesian network structure learning type error assessing reliability independence test asymptotically given distribution. relatively high threshold needs speciﬁed order prevent false rejection independence correct multiple hypothesis testing. contributions show type error. minimizing type error essential want side caution large sparsity boost sure corresponding edge exist. type errors hand corrected part objective corresponding score. instead used type error probability within score would corresponded dependence boost rather independence would fooled failed good separating section prove consistency sparsityboost score. order state main results need deﬁne certain additional parameters. first positive integer bounds in-degree vertices family order hold must every joint assignment val. reason taking minimum suﬃces rule maximum sab. sparsity boost remains explain compute implementation score sηψψ. according deﬁnition data-dependent makes impractical compute quickly enough algorithm. make approximation ﬁxing single reference distribution uniform marginals satisfying case distributions. namely denote uniform distribution related work. score similar hybrid algorithms conditional independence tests score-based search structure learning notably fast greedy relaxation algorithm tsamardinos max-min hillclimbing algorithm. mmhc algorithm stages ﬁrst using independence tests construct skeleton bayesian network performing greedy search orientations edges using bdeu score. relax algorithm starts performing conditional independence tests learn constraints followed edge orientation produce initial model. ﬁrst model identiﬁed relax uses local greedy search possible relaxations constraints step choosing single constraint which relaxed leads order explain signiﬁcance result helpful relate three representative sample complexity results literature h¨oﬀgen friedman yakhini result diﬀers result gives conditions score beat individual competplexity polynomial card friedman yakhini puts restriction in-degree competing networks obtains complexity exponential result diﬀer h¨oﬀgen friedman yakhini provide guarantees learning correct structure distribution ζ-close reason paper need deﬁne minimal edge strength parameter whereas h¨oﬀgen friedman yakhini main parameter error tolerance call sab) then min∈g sab). next need notion error tolerance turn follows notion ζ-far true network deﬁne probability distribution distribution factors according theorem suppose bayesian fect assume minimal edge deﬁned deﬁned error probability tolerance denote score sηψψ sequence observations sampled i.i.d. techniques derived chernoﬀ bound yield version theorem exponent denominator term improve exponent need strengthened result linear growth sparsity boost false edge speed order method derived sanov’s theorem instead chernoﬀ’s bound. knowledge sanov’s theorem study concentration mutual information novel contribution information theory. following reason preferring parameter means sanov’s theorem pinsker’s inequality obtain general result bounds times squared l∞distance distribution speciﬁcally deﬁning complement feature proposition obtaining concentration results independent bernoulli parameter concentration result show high probability positive larger ζ-far suﬃciently large concentration result show sparsity boost true edge bounded constant suﬃciently large negative contribution bounded. bounds suﬃce prove theorem parameterization sample parameters independently according gaussian distributions. selection marginals identical gaussians centered becoming concentrated since cannot possibly tabulate every empirical sequence might arise tabulate strategically chosen grid values learning phase interpolate extrapolate tabulated values. interpolate/extrapolate linearly statistics hpη). sanov’s theorem gives computing conﬁdence measure. figure present several empirical results help justify methods calculating measure reliability independence test. first show using method summing types calculate running time whereas monte carlo method explained section interpolation procedure obtaining linear pre-computed tables receives heuristic support sanov’s theorem; receives empirical support figure shows dependence inputs ﬁxed roughly linear. sample complexity. section study accuracy learning algorithm function amount data provide compare algorithm baselines max-min hillclimbing. equivalent score without number observations sampled sequence length consider length- vectors nonnegative integers summing every corresponds one-to-one distribution denote number sequences corresponding type diﬃcult monte carlo computation. place exact calculation estimate means monte carlo integration using importance sampling domain reduce variance. order implement this ﬁrst observe essentially riemann deﬁnite integral replace summation integral. second integrand initially obtain manner numerous discontinutegrand largest hpη) small strongly concentrated around unproven conjecture supported numerical evidence less approximately importance sampling algorithm sparsity boost terms. mmhc state-of-the-art terms speed quality recovery shown outperform constraintbased approaches discussed earlier mmhc also hybrid algorithm using conditional independence tests scorebased search. implementation mmhc provided authors part causal explorer default parameters. integer linear program exactly solve bayesian network maximizes sparsityboost scores solve cussens’ gobnilp software together scip conveniently since sparsity boost terms objective subsumed parent scores oﬀ-the-shelf bayesian network solvers without modiﬁcation. data learning sampled synthetic distributions based alarm network structure alarm network variables edges maximum in-degree synthetic distributions every variable states conditional probability distribution given logistic function enumerating separating sets size two. larger separating sets less useful lead smaller less data computation create objective. alarm network every results shown figure measure quality structure recovery using structural hamming distance partially directed acyclic graphs representing equivalence classes true learned networks deﬁned number timing experiments reported section performed single core intel core processor memory. mmhc’s average running time less seconds sample sizes. mmhc signiﬁcantly faster quickly prunes edges cannot exist second step uses greedy optimization algorithm score-based search. approach maintains advantages scorebased approaches structure learning ability k-best bayesian networks ease introducing additional constraints order optimize score virtually optimization procedure used. since easy solve suggests greedy structure search also able easily best-scoring bayesian network sparsityboost score. subject future investigations generalize sharpen results various ways. using similar construction believe possible extend score proof consistency nonbinary variables. also believe possible eliminate dependence parts theorem parameter leaving dependence part cases much smaller another issue explored future line investigation choice measure reliability overall motivation capture probability type error statistical test independent distributions null hypothesis distributions alternative hypothesis choice uniform marginals represents expedient choice providing objective function manageable implement compute still reasonable theoretical empirical sample complexity. better results might obtained setting marginals approximate generally contemplate incorporating various statistically derived probabilities objective function. leads broader point objective functions optimization discrete spaces structures ubiquitous throughout computer science statistics. work suggests paradigm incorporating information classical hypothesis tests objective functions used machine learning. paradigm provides computational statistical eﬃciency. figure total running time learn bayesian network data sparsityboost mmhc. maximize sparsityboost scores solving integer linear program optimality. edge additions deletions reversals make pdags match. plots sparsityboost nearly identical sparsityboost consistently learns better structures mmhc often perfectly recovers networks samples. sparsityboost obtains smaller average error samples representing reduction number samples needed learning. also found sparsityboost results substantially less variance either mmhc. theoretical results guarantee exact recovery synthetic distributions computed edges true structure minimum these ranged fact smaller largest value considered experiments despite this obtained excellent empirical results sparsityrunning time. show running time objective compared figure ﬁgure shows total time includes time compute score parent sets time solve optimality. results conﬁrm hypothesis score would substantially easier optimize. found linear programming relaxation sparsityboost tight nearly instances branch-and-bound need performed. sparsityboost objective computed solved within jaakkola tommi sontag david globerson amir meila marina. learning bayesian network structure using relaxations. pages proceedings thirteenth international conference artiﬁcial intelligence statistics vol. jmlr w&cp. sachs karen perez omar pe’er dana lauﬀenburger douglas nolan garry causal protein-signaling networks derived multiparameter single-cell data. science", "year": 2013}