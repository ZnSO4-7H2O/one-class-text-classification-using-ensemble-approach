{"title": "Using Rule-Based Labels for Weak Supervised Learning: A ChemNet for  Transferable Chemical Property Prediction", "tag": ["stat.ML", "cs.AI", "cs.CV", "cs.LG"], "abstract": "With access to large datasets, deep neural networks (DNN) have achieved human-level accuracy in image and speech recognition tasks. However, in chemistry, data is inherently small and fragmented. In this work, we develop an approach of using rule-based knowledge for training ChemNet, a transferable and generalizable deep neural network for chemical property prediction that learns in a weak-supervised manner from large unlabeled chemical databases. When coupled with transfer learning approaches to predict other smaller datasets for chemical properties that it was not originally trained on, we show that ChemNet's accuracy outperforms contemporary DNN models that were trained using conventional supervised learning. Furthermore, we demonstrate that the ChemNet pre-training approach is equally effective on both CNN (Chemception) and RNN (SMILES2vec) models, indicating that this approach is network architecture agnostic and is effective across multiple data modalities. Our results indicate a pre-trained ChemNet that incorporates chemistry domain knowledge, enables the development of generalizable neural networks for more accurate prediction of novel chemical properties.", "text": "introduction chemical sciences designing chemicals desired characteristics drug interacts specically intended target material specied physical performance ratings despite decades research still largely driven serendipity chemical intuition. decades various machine learning algorithms developed predict activity property chemicals using engineered features developed using domain knowledge. recent work also started using deep neural networks average typically accurate traditional models compared modern deep learning research models chemistry relies heavily engineered features. approach advantageous utilizes existing knowledge using engineered features limit search space potentially developable representations. exacerbated situations engineered features appropriate inadequate lack well-developed domain knowledge. growth chemical data desirable fully leverage representation learning enable predict novel chemical properties lile feature engineering research performed. computer vision research achieved using data. example unaltered images used input various models chemistry models leverage representation learning data starting emerge. example minimal feature engineering molecular graphs used train models approaches images train convolutional neural network models smiles strings train recurrent neural network models factor complicates representation learning limited amount usable labeled data chemistry signicantly smaller available modern deep learning research. example labeled datapoints considered signicant accomplishment chemistry. contrast computer vision research datasets like imagenet includes million images typically starting point. sizable chemical databases like pubchem chembl exist labels skewed towards biomedical data databases abstract access large datasets deep neural networks achieved human-level accuracy image speech recognition tasks. however chemistry data inherently small fragmented. work develop approach using rule-based knowledge training chemnet transferable generalizable deep neural network chemical property prediction learns weak-supervised manner large unlabeled chemical databases. coupled transfer learning approaches predict smaller datasets chemical properties originally trained show chemnet’s accuracy outperforms contemporary models trained using conventional supervised learning. furthermore demonstrate chemnet pre-training approach equally eective models indicating approach network architecture agnostic eective across multiple data modalities. results indicate pretrained chemnet incorporates chemistry domain knowledge enables development generalizable neural networks accurate prediction novel chemical properties. concepts computing methodologies→ transfer learning; neural networks; natural language processing; computer vision; applied computing chemistry; computational biology; eory computation semi-supervised learning; reference format garre charles siegel nathan hodas abhinav vishnu. using rule-based labels weak supervised learning. proceedings sigkdd conference london pages. permission make digital hard copies part work personal classroom granted without provided copies made distributed prot commercial advantage copies bear notice full citation page. copyrights third-party components work must honored. uses contact owner/author. london copyright held owner/author. ---//.... sparsely labelled labeled data available small subset entire database. erefore current state labeled chemical data small fragmented reduces eectiveness representation learning using conventional supervised training approaches. contributions work addresses small fragmented data landscape chemistry. achieved leveraging rule-based knowledge obtained prior feature engineering research chemistry perform weak supervised learning combining transfer learning methods used modern deep learning research specically develop chemnet deep neural network pre-trained chemistry-relevant representations making analogous counterpart resnet googlenet chemical sciences. contributions follows. demonstrate chemnet learns chemistry-relevant internal representations coupled transfer learning approach used predict novel chemical properties originally trained demonstrate generalizability chemnet predicting broad range chemical properties relevant multiple chemical-aiated industries including pharmaceuticals biotechnology materials consumer goods. demonstrate chemnet-based models outperform otherwise identical models trained using conventional supervised learning chemnet also matches exceeds current state-of-the-art models chemistry literature. organization rest paper follows. section outline motivations developing chemistry-relevant rule-based weak supervised learning approach design principles behind chemnet. section examine datasets broad applicability chemical-aiated industries well training protocols used pre-training chemnet evaluating performance unseen chemical tasks. lastly section explore dierent chemnet models various factors aect model accuracy generalization. best chemnet model evaluated models trained using conventional supervised learning approaches. related work transfer learning established technique deep learning research approach trains neural network larger database ne-tuning smaller dataset. example using resnet pre-trained imagenet classify various common objects used transfer learning techniques classify specic clothing type. addition long sucient overlap image space network trained seemingly unrelated outcomes achieved. example model pre-trained imagenet also ne-tuned classify medical images medical applications seemingly unrelated conventional image recognition tasks sets data natural photographs thus lower-level basic representations utilized. chemistry literature molecular diagrams substantially dierent natural photographs chemistryspecic information encoded image channels existing pre-trained models computer vision literature images example would fully applicable. addition limited examples using weak supervised learning utilize large chemical databases traing neural networks. challenge convert sparsely labeled chemical database usable form used transfer learning approach existing chemistry models non-trivial task. work molecular descriptors generate consistent inexpensive rule-based labels combined weak supervised learning approach train chemnet develop chemically-relevant internal representations approach conceptually unique relative existing methods. chemnet design section provide brief introduction molecular descriptors role alternate labels weak supervised learning chemnet. document design principles behind chemnet. much success modern computer vision research comes availability large labeled datasets like imagenet however chemical sciences generating labels resource-intensive time-intensive endeavors goal chemistry research predict chemicals desired characteristics futhermore deep neural networks trained recognize paerns cannot easily programmed specic rules chemistry. erefore order develop generalizable chemistry-expert model incorporates existing domain knowledge need train neural network manner learn basic universal chemical representations. analagous hypothetical scenario explains data challenges chemistry traditional computer vision research context would follows. ctitious example research objective classify object’s names imagenet obtaining names large dataset unfeasible. however concepts associated images easily obtained. example classication images assume alternative labels describe texture whether man-made cial] easily computed. erefore model trained alternate labels weak supervised learning approach representations developed classify texture whether object man-made leveraged second transfer learning phase model ne-tuned smaller labeled dataset labels object’s names. example developing internal representations identify furry surface images assist identifying cats trees cars distinguishing characteristc figure schematic illustration chemnet pre-training chembl database using rule-based molecular descriptors followed ne-tuning smaller labeled datasets unseen chemical tasks. moving back chemical sciences approach therefore molecular descriptors alternate labels perform weak supervised learning transfer learning methods applied ne-tune pre-trained model directly smaller labeled datasets unseen chemical properties interests. molecular descriptors engineered features developed historical research stretches back late molecular descriptors developed rule-based chemistry knowledge molecular descriptors typically computable properties rule-based descriptions chemical’s structure. molecular descriptors hydrogen bond donor count correspond intuitive chemical knowledge chemists conceptualize understand complex chemical phenomena. hand descriptors balaban’s index intuitive nevertheless abstract topological description chemical useful various modeling studies. absence copious amount data representation learning ability deep neural networks learn optimal features. solution illustrated figure uses molecular descriptors generate consistent inexpensive rule-based labels large chemical databases typically sparsely inconsistently labeled. rule-based labels used train chemnet supervised manner using multi-task learning conguration network aempts predict molecular descriptors simultaneously. process neural network develops chemistry-relevant representations hypothesize serve beer initialization network’s weights ne-tuning smaller unrelated chemical tasks predicted. hypothesis behind chemnet work evaluated chemnet approach chemception cnn-based model strong relationship chemical’s structure property hypothesize images molecular drawings cnn-based model therefore help facilitate learning chemistry-relevant structural representations. evaulated chemnet approach smilesvec rnn-based model directly uses smiles strings predicting chemical properties. smiles chemical language encodes structural information compact text representation. oneto-one mapping specic characters string specic structural elements chemical hypothesize chemnet also facilitate learning chemistry-relevant structural representations even using text representation chemical. lastly using multi-task learning anticipate shared representation learned generalizable used building blocks develop sophisticated task-specic representations ne-tuning smaller datasets. methods section provide details datasets used data spliing data preparation steps. document training transfer learning protocols well evaluation metrics used work. dataset pre-training chemnet trained chembl database manually curated database bioactive molecules drug-like properties. work curation approximately compounds used. initial pre-training stage compute molecular descriptors serve inexpensive consistent rule-based labels. specically used rdkit compute list descriptors includes basic computable properties connectivity constitutional topological descriptors. dataset performance evaluation chemnet pre-trained ne-tune evaluate chemnet’s performance smaller datasets. ensure results comparable contemporary models reported literature earlier work chemception models smilesvec models used freesolv dataset moleculenet benchmark predicting toxicity activity solvation free energy respectively. datasets used comprises large small datasets physical non-physical properties regression classication problems. none above-mentioned chemical tasks related molecular descriptors used train chemnet thus also serve measure chemnet’s ability generalize predict unseen chemical properties. industrial application based datasets tested elaborate applications chemical industry. first toxicity prediction high relevance notably chemicals require approval includes drugs therapeutics well cosmetics activity prediction proxy well-suited chemical drug therefore relevance pharmaceuticals biotechnology industries. solvation free energy values computable physics-based simulations methods currently employed pharmaceuticals consumer goods materials industries. erefore using neural networks predict computable properties similar accuracy potentially lead several orders magnitude speed compared traditional computational chemistry simulations typically take order minutes hours calculation. respective molecular structures using rdkit coordinates molecule used onto discretized image pixels corresponds resolution pixel. initially used greyscale color-coding scheme reported earlier chemception paper however subsequent experiments utilized sophisticated -channel color-coding scheme atom bond pixel assigned color based local atomic/bond properties atomic number partial charge valence hybridization. specifically used engd augmented image representation colorcoding details data preparation protocol obtained published work preparation chemical text data identical reported earlier work smiles string canonicalized using rdkit unique characters string mapped one-hot vectors. zero padding also applied right string construct uniform entries characters long. data splitting dataset spliing steps identical reported previously used -fold cross validation protocol training evaluated performance early stopping criterion model using validation set. also included performance separate test indicator generalizability. specically chembl database dataset dataset separated form test freesolv dataset dataset used form test set. remaining dataset used random -fold cross validation approach training chemnet. classication tasks also over-sampled minority class address class imbalance observed dataset. achieved computing imbalance ratio appending additional data minority class ratio. oversampling step performed stratication ensure molecule repeated across training/validation/test sets. training neural network chemnet trained using tensorow backend acceleration using nvidia cudnn libraries. network created executed using keras functional interface rmsprop algorithm train epochs chembl epochs freesolv using standard seings recommended used batch size also included early stopping protocol reduce overing. done monitoring loss validation improvement validation loss epochs last best model saved model. addition images training chemnet performed additional realtime data augmentation image using imagedatagenerator function keras image randomly rotated degrees. unless specied otherwise used weights chemnet model trained molecular descriptors initial weights initialize subsequent individual models predicting toxicity activity solvation energy. also explored dierent ne-tuning protocols segments chemnet weights xed. loss functions performance metrics classication tasks used binary crossentropy loss function regression tasks used mean-squared-error loss function. initial chemnet pre-training chembl database performed min-max normalization molecular descriptors normalized labels used training neural network. ensures molecular descriptor given equal emphasis training. classication tasks evaluation metric reported paper determines model’s performance area roc-curve freesolv dataset evaluation metric rmse. reported results paper mean value evaluation metric obtained runs -fold cross validation. experiments section conduct several experiments determine factors aect performance generalizability chemnet models. next demonstrated chemnet approach data modalities training chemnet model. establishing best chemnet model compare performance earlier chemception/smilesvec models contemporary models literature. chemnet model exploration absence data network architecture driver increasing model accuracy erefore examine network architecture hyperparameters followed evaluation image representation used. full list chemnet models explored summarized table evaluated eect chemception architecture performance chemical tasks toxicity activity free energy solvation earlier work optimizing chemception architecture evaluated baseline optimized architectures nomenclature used refers general depth network refers number lters convolutional layers. addition also tested wider deeper chemception architecture. figure chemnet architecture generally consistently better performance validation auc/rmse toxicity activity solvation energy predictions. higher better. freesolv lower rmse better. architecture wider layers accommodate representations simultaneous prediction molecular descriptors aained lowest normalized validation loss chemnet pre-training. slightly lower still order magnitude compared architectures. illustrated figure terms validation metrics unseen chemical tasks architecture generally consistently beer performance even though approximately half number parameters. implies suffering undering. time architecture also beer performance implies adding parameters retaining similar network architecture depth help improve generalizability model. erefore ndings indicate amongst network architectures tested work deep narrow architecture provides best performance generalizing unseen chemical tasks. evaluated results using standard images reported earlier work however subsequent improvements shown augmenting image channels basic atom and/or bond-specic chemical information improves overall performance note addition localized chemical information image channels complementary beginning chemnet model weights frozen incrementally unfreeze ne-tuned) network starting segment. used segment-based approach instead conventional layer-based approach network architecture designed segments base unit mind. resulting model performance across chemical tasks recorded function number segments ne-tuned. figure using augmented images results consistently better performance validation auc/rmse toxicity activity solvation energy predictions. higher better. freesolv lower rmse better. chemnet’s transfer learning approach work training neural network reproduce global chemical properties entire molecule such combination chemnet augmented images lead additional performance improvement. results summarized figure indicates training chemnet augmented images consistently improved performance relative standard images independent network architecture chemical task. erefore ndings indicate using augmented images molecular drawings synergistic approach chemnet transfer learning methods. investigated various factors impact chemnet performance come following conclusions using chemception architecture provided best consistent performance training augmented images consistently improved performance. erefore remainder work explore results using best model identied eng. transferability learned representations presented results preceding sections used weights chemnet initialization scheme individually trained networks smaller freesolv datasets. anticipate hierarchical nature deep neural networks learn hierarchical chemical representations basic representations need re-trained. order determine layers chemnet needs netuned systematically explore freezing weights various segments chemnet model. architecture constructed segments segment comprises several convolutional layers grouped together based similarities function. specically chemnet starts stem segment single convolutional layer used dene basic spatial region network. following stem segment alternating series inception-resnet segments reduction segments. inception-resnet segment group convolutional layers collectively perform inception-style operation residual links reduction segment group convolutional layers downsamples image. details network architecture refer readers earlier work figure fine-tuning beyond segment chemnet architecture yield diminishing returns performance improvement toxicity activity solvation energy predictions. illustrated figure less segments netuned also network weights frozen model performance relatively poor. expected behavior chemnet trained separate molecular descriptors unrelated toxicity activity solvation energy predictions. number segments ne-tuned increases model’s performance. limit segments ne-tuned recover results reported previous section. observed reaches point diminishing performance improvement around segment additional ne-tuning segments consistently improve results. indicates almost half network need re-trained suggest half chemnet developed basic chemical representations eminently transferable chemical tasks second half chemnet develops complex representations needs ne-tuned specic property predicted. erefore ndings suggest chemnet particularly lower layers learned universal chemical representations generalizable prediction chemical properties. rnn-based model uses smiles strings input. network architecture based original smilesvec work results summarized figure show similar performance gain acheived models using chemical text representation. specically chemnet achieves validation toxicity activity predictions respectively validation rmse kcal/mol solvation free energy. erefore ndings indicate chemnet pretraining approach provides consistent performance improvement independent network’s architecture data modality. identied best chemnet model ne-tuning protocol evaluate performance chemnet earlier chemception models utilize transfer learning. figure summarize performance across various generations chemception-based models chemception refers original model uses standard images augchemception refers modication using augmented images chemnet refers results work. figure chemnet provides consistently better performance validation auc/rmse toxicity activity solvation energy predictions compared earlier chemception models utilize transfer learning. across chemical tasks observed chemnet achieves best performance. specically chemnet achieves validation toxicity activity predictions respectively validation rmse kcal/mol solvation free energy. furthermore emphasize dierence augchemception chemnet terms network architecture images used superivsed learning step means performance improvement solely originating transfer learning techniques applied. erefore ndings indicate transfer learning techniques used chemnet provide non-trivial improvement model performance even factors held constant. figure chemnet provides consistently better performance validation auc/rmse toxicity activity solvation energy predictions compared earlier smilesvec models utilize transfer learning. chemnet state-of-the-art models established chemnet provides consistently beer performance counterpart chemception smilesvec models perform benchmarks relative contemporary deep learning models literature. specically compare model trained molecular ngerprints addition also include convgraph algorithm novel graph-based method representing chemical data current state-of-the-art many chemical tasks figure chemnet consistently outperforms models trained engineered features ngerprints) matches performance convgraph validation auc/rmse toxicity activity solvation energy predictions. observe chemnet chemnet consistently outperforms across chemical tasks. relative convgraph algorithm best chemnet model matches performance activity toxicity predictions chemnet signicantly outperforms solvation free energy prediction. rule-based weak supervised learning approach developed train chemnet unique chemistry several design principles generalized domains. specically following factors critical enabling rule-based weak supervised learning work availability large datasets inability generate ground-truth labels largescale prior research feature engineering rule-based models used generate relatively inexpensive labels. erefore likely scientic engineering nancial modeling applications substantial research rule-based models historically invested benet approach. furthermore emphasized approach process pre-training network arguably important accuracy initial chemnet model predicting various rule-based labels technically rule-based labels chemnet pre-trained related subsequent chemical properties chemnet ne-tuned however hierarchical representations deep neural networks form good parallel hierarchical nature scientic concepts built another. suggests process using rule-based transfer learning could potentially simulate conventional learning process domain expert without need explicitly introduce domain-specic rules. plausible lower layers network learn representations analogous simpler concepts technical domain representations give ability adapt dierent and/or unseen data. ne-tuning experiments work indicates almost half chemnet developed universal representations re-used predicting novel chemical properties suggests least example chemistry approach possible. conclusions conclusion developed approach integrating rulebased knowledge deep neural networks weak supervised learning. using chemistry domain example demonstrate rule-based knowledge adapted transfer learning techniques train models large unlabeled chemical databases chemicals. resulting model chemnet ne-tuned much smaller datasets chemicals predict unrelated novel chemical properties relevance many chemistry-aiated industries. addition chemnet pretraining approach works eectively across network architectures data modalities models using chemical images models using chemical text consistently beer performance. models show combination using augmented chemical images chemception architecture ne-tuning half network provides best generalizable performance models also produce comparable results. chemnet consistently outperforms earlier versions chemception smilesvec toxicity activity solvation energy predictions achieving validation validation rmse kcal/mol respectively. addition chemnet consistently outperforms contemporary deep learning models trained engineered features like molecular ngerprints outperforms current state-of-the-art convgraph algorithm certain tasks. furthermore ne-tuning experiments suggest lower layers chemnet learned universal chemical representations inspired rule-based knowledge improves generalizability prediction unseen chemical properties. lastly anticipate design principles behind rule-based weak supervised learning approach adaptable scientic engineering domains existing rule-based models used generate data training domain-expert neural networks specic application. acknowledgments authors would like thank nathan baker helpful discussions. work supported following pnnl ldrd programs pauling postdoctoral fellowship deep learning scientic discovery agile investment. references mart´ın abadi paul barham jianmin chen zhifeng chen andy davis jerey dean mahieu devin sanjay ghemawat georey irving michael isard tensorflow system large-scale machine learning.. osdi vol. esben jannik bjerrum. smiles enumeration data augmentation neural network modeling molecules. arxiv preprint arxiv. peter buchwald nicholas bodor. computer-aided drug design role quantitative structure–property structure–activity structure–metabolism relationships drugs future artem cherkasov eugene muratov denis fourches alexandre varnek igor baskin mark cronin john dearden paola gramatica yvonne martin roberto todeschini qsar modeling been? going journal medicinal chemistry sharan chetlur woolley philippe vandermersch jonathan cohen john tran bryan catanzaro evan shelhamer. cudnn ecient primitives deep learning. arxiv preprint arxiv. john chodera david mobley michael shirts richard dixon branson vijay pande. alchemical free energy methods drug discovery progress challenges. current opinion structural biology david duvenaud dougal maclaurin jorge iparraguirre rafael bombarell timothy hirzel al´an aspuru-guzik ryan adams. convolutional networks graphs learning molecular ngerprints. advances neural information processing systems. anna gaulton louisa bellis patricia bento chambers mark davies anne hersey yvonne light shaun mcglinchey david michalovich bissan al-lazikani chembl large-scale bioactivity database drug discovery. nucleic acids research d–d. garre nathan hodas charles siegel abhinav vishnu. smilesvec interpretable general-purpose deep neural network predicting chemical properties. arxiv preprint arxiv. garre charles siegel abhinav vishnu nathan hodas nathan baker. chemception deep neural network minimal chemistry knowledge matches performance expert-developed qsar/qspr models. arxiv preprint arxiv. garre charles siegel abhinav vishnu nathan hodas nathan baker. much chemistry deep neural network need know make accurate predictions? arxiv preprint arxiv. kaiming xiangyu zhang shaoqing jian sun. delving deep rectiers surpassing human-level performance imagenet classication. proceedings ieee international conference computer vision. hinton srivastava swersky. rmsprop divide gradient running average recent magnitude. neural networks machine learning coursera lecture steven kearnes kevin mccloskey marc berndl vijay pande patrick riley. molecular graph convolutions moving beyond ngerprints. journal computer-aided molecular design sunghwan paul iessen evan bolton chen gang asta gindulyte lianyi jane siqian benjamin shoemaker pubchem substance compound databases. nucleic acids research d–d. naomi kruhlak joseph contrera daniel benz edwin mahews. progress qsar toxicity screening pharmaceutical impurities regulated products. advanced drug delivery reviews landrum. rdkit open-source cheminformatics soware. andreas mayr g¨unter klambauer omas unterthiner sepp hochreiter. deeptox toxicity prediction using deep learning. frontiers environmental science maxime oquab leon boou ivan laptev josef sivic. learning transferring mid-level image representations using convolutional neural networks. proceedings ieee conference computer vision paern recognition. john pla. inuence neighbor bonds additive bond properties parans. journal chemical physics bharath ramsundar steven kearnes patrick riley dale webster david konerding vijay pande. massively multitask networks drug discovery. arxiv preprint arxiv. olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein imagenet large scale visual recognition challenge. international journal computer vision chuen-kai shie chung-hisang chuang chun-nan chou meng-hsi edward chang. transfer representation learning medical image analysis. engineering medicine biology society annual international conference ieee. ieee christian szegedy yangqing pierre sermanet reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. proceedings ieee conference computer vision paern recognition. tors. vol. john wiley sons. izhar wallach michael dzamba abraham heifets. atomnet deep convolutional neural network bioactivity prediction structure-based drug discovery. arxiv preprint arxiv. david weininger. smiles chemical language information system. introduction methodology encoding rules. journal chemical information computer sciences zhenqin bharath ramsundar evan feinberg joseph gomes caleb geniesse aneesh pappu karl leswing vijay pande. moleculenet benchmark molecular machine learning. arxiv preprint arxiv.", "year": 2017}