{"title": "Optimal Nudging: Solving Average-Reward Semi-Markov Decision Processes  as a Minimal Sequence of Cumulative Tasks", "tag": ["cs.LG", "cs.AI"], "abstract": "This paper describes a novel method to solve average-reward semi-Markov decision processes, by reducing them to a minimal sequence of cumulative reward problems. The usual solution methods for this type of problems update the gain (optimal average reward) immediately after observing the result of taking an action. The alternative introduced, optimal nudging, relies instead on setting the gain to some fixed value, which transitorily makes the problem a cumulative-reward task, solving it by any standard reinforcement learning method, and only then updating the gain in a way that minimizes uncertainty in a minmax sense. The rule for optimal gain update is derived by exploiting the geometric features of the w-l space, a simple mapping of the space of policies. The total number of cumulative reward tasks that need to be solved is shown to be small. Some experiments are presented to explore the features of the algorithm and to compare its performance with other approaches.", "text": "paper describes novel method solve average-reward semi-markov decision processes reducing minimal sequence cumulative reward problems. usual solution methods type problems update gain immediately observing result taking action. alternative introduced optimal nudging relies instead setting gain ﬁxed value transitorily makes problem cumulative-reward task solving standard reinforcement learning method updating gain minimizes uncertainty minmax sense. rule optimal gain update derived exploiting geometric features depends stochastically preceding move made history positions moves that game modelled markov decision process. further since matches terminate process episodic. learning game solving decision process equivalent ﬁnding best playing strategy determining moves make position order maximize probability winning expected payout. type problem commonly solved cumulative-reward reinforcement learning methods. assume that given nature game often case winning probability optimized cautious policy whose gameplay favours avoiding risks hence results relatively long matches. example assume game policy known certainty moves. hand typically policies trade performance speed assume anknown policy obviously sub-optimal sense expected payout winning probability takes moves average terminate. going play single episode doubtlessly ﬁrst strategy best available since following winning guaranteed. however sequence games second policy outperform ‘optimal’ important sense move costs whereas policy always wins receives average ./move strategy earns twice much. thus hour lifetime playing ostensibly sub-optimal game strategy double earnings apparent optimizer. consequence fact second policy higher average reward receiving larger payout action taken. finding policies optimal sense problem solved average-reward reinforcement learning. general case move associated diﬀerent cost time would take move token board number steps dictated problem would average-reward semi-markov goal would change ﬁnding policy possibly diﬀerent either discussed above maximizes expected amount payout received unit action cost. average-reward semi-markov tasks arise naturally areas repeated episodic tasks example discussed queuing theory autonomous robotics quality service communications among many others. paper presents algorithm solve average-reward semi-markov decision processes. traditional solutions kind problems require large number samples sample usually observation eﬀect taking action state cost action reward received resulting next state. sample algorithms basically update gain task gain-adjusted value good idea taking action state. methods literature follow solution template r-learning algorithms singh smart algorithm gosavi table section introduces comprehensive taxonomy solution methods average-reward semi-markov problems. method introduced paper optimal nudging operates diﬀerently. instead rushing update gain sample temporarily ﬁxed value resulting cumulative-reward task solved then based solution found gain updated minimizes uncertainty range known contain optimum. main contribution paper introduction novel algorithm solve semi-markov decision processes reducing minimal sequence cumulative-reward tasks solved fast robust existing methods kind problems. hence refer method used solve tasks ‘black box’. central step optimal nudging algorithm rule updating gain calls ‘black-box’ solver solving resulting cumulativereward task worst case associated uncertainty around value optimal gain smallest possible. addition this derivation optimal nudging update rule yields early termination condition related sign changes value reference state successive iterations policy optimal. condition unique optimal nudging equivalent possible preceding algorithms literature. complexity optimal nudging understood number calls blackbox routine shown worst logarithmic desired ﬁnal uncertainty upper bound optimal gain. number samples required call principle inherited black also depends strongly whether example transfer learning used state values reset iterations. finally experimental results presented show performance optimal nudging even without tuning similar better best usual algorithms. experiments also illuminate particular features algorithm particularly great advantage early termination condition. rest paper structured follows. section formalizes problem deﬁning diﬀerent types markov decision processes uniﬁed notation introducing important unichain condition describing important assume holds. section presents summary solution methods three types processes emphasizing distinctions dynamic programming model-based modelfree reinforcement learning algorithms. section also introduces taxonomy average-reward algorithms literature allows propose generic algorithm encompasses them. special attention given section family stochastic shortest path methods concept bertsekas split extracted. finally motivating example task introduced compare performance traditional algorithms optimal nudging. early termination condition zero crossing presented special case reduction enclosing triangles exploration optimal reduction leads main theorem ﬁnal proposition algorithm. section markov decision processes described three diﬀerent reward maximization problems introduced expected cumulative reinforcement average-reward average reward semi-markov models. average-reward problems subset semimarkov average reward problems. paper introduces method solve kinds average-reward problems minimal sequence cumulative-reward episodic processes. relevant common assumptions average reward models unichain condition holds recurrent state exists described discussed section. cases agent environment observes current state take actions that following static distribution lead state result real-valued reinforcement/reward. assumed markov property holds next state depends current state action taken history previous states actions. deﬁned minimally four-tuple states environment. actions equal subset actions available take assume ﬁnite. stationary function deﬁnes transition probabilities system. taking action likewise state resulting state probability denotes real-valued reward observed taking action transitioning state task episodic must terminating state deﬁned transitioning probability reward without loss generality multiple terminating states treated single one. state action take concerned deterministic policies state associated single action take probability one. restrictive since puterman shown that optimal policy exists optimal deterministic policy exists well. moreover policies assumed herein stationary. value policy given state expected cumulative reward observed starting following remark discount factor ensures convergence inﬁnite policy values equation used make value bounded rewards bounded even problems policies episodes inﬁnite duration. ostensibly introducing makes rewards received sooner desirable received later would make useful goal optimize measure immediate reward. however purposes paper relevant policies discussed resulting mdps assumed terminate eventually probability states inﬁnite converge even without discount. furthermore perceived advantages discount less sturdy initially apparent discounting guaranteed lead gain optimality thus discount used paper average reward model inﬁnite-horizon mdps maximize reward received step without discount non-zero-valued policies would signed inﬁnite value goal must change obtaining largest positive smallest negative rewards frequently possible. case gain policy state deﬁned average reward received action taken following policy state ﬁner typology optimal policies average-reward problems discriminates biasoptimal policies which besides gain-optimal also maximize transient reward discussion diﬀerences book puterman paper focus problem ﬁnding gainoptimal policies. average-reward model state transitions weigh equally. equivalently actions states considered same—unity—duration cost. semi-markov decision processes goal remains maximizing average reward received action actions required weight. positive real numbers). also since reward possibly lump decision epochs expectation marginalized expected transition times well states. consequently gain policy state becomes propose alternative interpretation smdp framework taking actions yield constant-length transitions consuming varying amounts resources results agent observing real-valued action cost necessarily related reward received taking action state above assumption cost depends initial ﬁnal states action taken expectation form general costs supposed positive purposes paper relaxed requiring policies positive expected cost states. likewise without loss generality assumed action costs either zero expected magnitude greater equal observe deﬁnitions analytically equivalent. role gain. although deﬁnition interpretations varies— expected time transition versus expected action cost—both give origin identical problems gain naturally action costs transition times equal semi-markov model reduces average rewards scale problems identical costs/times unityvalued. notational simplicity refer gain problems simply following sections discuss technical assumptions commonly used average-reward semi-markov decision process literature simplify analysis guaranteeing optimal stationary policies exist. transition probabilities ﬁxed deterministic policy deﬁne stochastic matrix transition matrix homogeneous markov chain embedded chain state called transient visit non-zero probability never returning state recurrent transient. recurrent state visited ﬁnite time probability one. recurrent class recurrent states outside states reached states inside set. multichain single optimality expression suﬃce describe gain optimal policy stationary optimal policies exist theory algorithms complex. hand unichain clearly given states gain simpliﬁes analysis suﬃcient condition existence stationary gain-optimal policies. ghavamzadeh mahadevan references thereon). nevertheless problem deciding whether given unichain trivial. fact kallenberg posed problem whether polynomial algorithm exists determine unichain answered negatively tsitsiklis proved np-hard. term recurrent also used confusingly describe state decision process belongs recurrent class every policy. expression recurrent state used sense paper. multiunichain processes recurrent states. however feinberg yang proved recurrent state instead actually using methods would require full knowledge transition probabilities assume emphasize central role recurrent states exist induced simplifying analysis. section summarizes relevant solution methods cumulative-reward mdps average-reward smdps special emphasis stochastic shortest path algorithms. proposed solution average-reward smdps call bertsekas split algorithms convert problem minimal sequence mdps solved existing cumulative-reward methods. section simple experimental task sutton barto presented examine performance discussed methods motivate subsequent derivation. cumulative-reward mdps widely studied. survey kaelbling books bertsekas tsitsiklis sutton barto szepesv´ari include comprehensive reviews approaches algorithms solve mdps present brief summary propose taxonomy methods suit approach accessing black reinforcement learning solver. general instead trying policies maximize state value equation directly solution methods seek policies optimize state-action pair value function dynamic programming methods assume complete knowledge transitions rewards seek solve equation directly. iteration type algorithm ﬁnds approximates value current policy subsequently sets current policy greedy respect values found puterman provides comprehensive summary dynamic programming methods including linear programming solve kind problems. earliest studied model-based methods pac-mdp algorithms minimize high probability number future steps agent receive near-optimal reinforcements. sparse sampling mbie notable examples family algorithms. kakade’s strehl’s dissertations paper strehl provide extensive theoretical discussions broad range pac-mdp algorithms. another learning framework model-based methods exists kwik decision epoch agent must return approximation transition probability corresponding observed state action next state. approximation must arbitrarily precise high algorithms variations literature derive sarsa q-learning belong class temporal diﬀerence methods. sarsa on-policy algorithm improves approximating value current policy using update rule action selected state st+. q-learning oﬀ-policy algorithm that following samples obtained acting current values approximates value optimal policy updating rule assumption states visited actions taken inﬁnitely often proven converge asymptotically optimal value probability furthermore discounted settings convergence bounds exist case every state–action pair keeps independent learning rate form {+|visits q-updates case parallel sampler every call returns transition/reward observations every state–action pair available additional pac-mdp model-free version q-learning interest delayed qlearning although main derivation discounted settings usual kind algorithms building work kakade variation brieﬂy discussed discount rather hard horizon assumption next action-choices agent contribute value function. method solving smdps assume access learning method ﬁnite-horizon undiscounted mdps. requirements solution provide discussed analysis algorithm. mentioned above average-reward problems subset average-reward smdps transitions take time unit actions equal unity cost. thus would suﬃcient consider larger set. however average-reward problems subject research resulting algorithms easily extended semi-markov framework multiplying gain cost relevant optimality equations presented jointly here. section introduce novel taxonomy diﬀering parameters update rules main published solution methods average-reward–including semi-markov– tasks. allows propose discuss generic algorithm covers existing solutions yields compact summary them presented table below. puterman mahadevan present comprehensive discussions dynamic programming methods solve average-reward problems. solution principle similar used cumulative reward tasks value evaluation followed policy iteration. however approximation average rewards policy evaluated must either computed approximated successive iterates. parametric variation average-reward value iteration bertsekas central method discussed depth below. smdps discuss speciﬁc synchronous asynchronous versions relative value iteration algorithm white among model-based methods listed cumulative reward problems originally deﬁnitions average reward models including pac-mdp bounds polynomial terms parameter called optimal \u0001-mixing time deﬁned smallest time observed average reward optimal policy actually becomes policy. observe ﬁnite state must reachable other problem must communicating rigid assumption unichain condition similarly regal algorithm bartlett tewari vector. case underlying process required weakly communicating subsets recurrent transient states equation also rigid assumption unichain condition. regarding pac-mdp methods model free algorithms similar delayed-q known present average reward problems. mahadevan discusses without complexity analysis model-based approach jalali ferguson reﬁned tadepalli h-learning algorithm relative value iteration applied transition probability matrices gain approximated observed samples. close analysis literature reveals h-learning related model-based algorithms well methods based update equation described using generic algorithm step corresponds observation tuple following current version degree exploration commonly introduced stage; instead taking best-known action argmaxˆa∈as suboptimal action chosen. instance traditional ε-greedy action selection method probability random action chosen uniformly. sutton barto discuss number exploration strategies. exploration/exploitation trade-oﬀ explore learn knowledge reward maximization active research ﬁeld reinforcement learning. pac-mdp related algorithms listed based optimism face uncertainty scheme initializing upper bound value states address less explicitly problem optimal exploration. h-learning learning stage algorithm includes updating approximate transition probabilities state action observed estimating state value using version equation updated probabilities. model-free methods summarized table learning stage usually -step update equation update also commonly done step number diﬀerent update rules. algorithms literature vary dimensions. ﬁrst update. compute updated approximation every action others last cases learning rate. addition perform updates algorithms literature also vary model used learning rates simplest models take parameters constant equal case q-learning convergence proved sequences conditions equations hold. call decaying learning rates. simple decaying learning rate form easily shown rate gives raise ratio updates equation methods require keeping individual learning rate state-action pair. type update equations —and associated convergence guarantees—hold practical advantages search-then-converge procedure darken called authors. update would example table describes updates learning rates model-free average reward algorithms found literature together applicable modelbased hierarchical algorithms focus interest additional model-free average-reward algorithm abounadi suggested dynamic programming method bertsekas connects average-reward problem parametrized family stochastic shortest path problems. fundamental observation that problem unichain average reward stationary policy must equal ratio expected total reward expected total cost visits reference state. thus idea separate visits start episodic task. achieved splitting recurrent state initial terminal state. assuming task unichain recurrent state refer bertsekas split resulting problem zero epochs decaying learning rate. derivation algorithm includes proof that equals optimal gain corrected value initial state zero. expected since subtracted equals expected average reward expectation vanishes. observe that case stops changing iterations. provide alternative derivation fact perspective fractional programming. remark methods described belong generic family described algorithm moreover action value update sspq identical version average corrected q-updates equation making gain update considerably slower value update. necessary q-update provide suﬃcient approximation value current policy improvement. short term q-update sees constant simple average-reward example discussion schwartz’s r-learning sutton barto study behaviour algorithms described compare method proposd paper. respectively. decision epoch customer head queue either assigned free server pay-oﬀ equal customer’s priority; rejected zero pay-oﬀ. decision epochs servers free independently probability naturally goal maximize expected average reward. states correspond combination priority customer head queue number free servers actions simply accept reject. simplicity single state corresponding free servers priority reject action available reward zero. thus make accept action available states priority number free servers resulting task optimal policies original problem state servers occupied become recurrent. indeed state free servers policy nonzero probability next customers priority servers free current next decision epochs. since available action customers priority accept them servers would then state policy nonzero probability reaching state free servers making recurrent. moreover since recurrent state reached state must single recurrent class policy containing all-occupied state states reached policy unichain condition also holds resulting task. measure algorithm performance. sutton barto show result applying rlearning task using ε-greedy action selection parameters call set-up r-learning however instead single samples results states many free servers grossly undersampled order study convergence action values states reset process uniformly-selected random state every steps experiments. algorithms tested r-learning smaller rate updates smart parameters update algorithm gosavi individual decaying learning rates equal inverse number past occurrences statefigure performance average-reward reinforcement learning algorithms queuing task. left gain r-learning right gain set-ups. bottom left convergence value optimal policy states. bottom right number states current policy diﬀers optimal. relatively quickly doesn’t guarantee equally fast approximation optimal policy value function. value used r-learning sutton barto causes fast approximation followed oscillation approximations smaller approaches stable. overall r-learning set-ups corresponding solid black grey lines plots figure achieve policy closer optimal better approximation value considerably faster algorithms. hand almost immediately smart gosavi’s algorithm sspq reach policies diﬀer optimal states remain error range whereas longer transient r-learning variants policies less non-optimal actions much better value approximations. remarkably r-learning set-up value approximation diﬀerences optimal policy increase start actually converging faster closer optimal methods. algorithm set-up unique optimal policy visited sometimes. suggests that slowest updating diﬀerent part policy space visited early stages training. moreover appears beneﬁcial eﬀect particular task speed quality ﬁnal convergence. optimal nudging method introduced below goes even direction freezing value whole runs reinforcement learning method updating gain value current best-known policy closely approximated. figure shows result applying vanilla version optimal nudging access control queuing task compared best results above corresponding r-learning -step–required compute parameter–which takes samples optimal nudging steps proper series blocks transition observations ﬁxed taking action followed q-learning update values r-learning experiments. edges plateaus black curve left plot signal points gain updated value following optimal nudging update rule. ﬁgure shows algorithms similar performance reaching comparably good approximations value optimal policy roughly number iterations. moreover optimal nudging ﬁnds similarly good approximation optimal policy diﬀering state. however optimal nudging number advantages parameter less adjust learning rate consequently performs update less iteration/action taking. dramatic eﬀect cases obtaining sample transitions quick updates cannot done parallel transitions states. additionally tuning implementation underlying q-learning method; whereas r-learning adjusted sutton barto yield best possible results setting r-learning implementation optimal nudging simply inherits relevant parameters without adjustments guarantees best performance. even setting number samples changes parameter improved. plateaus left plot suggest early stages learning good value approximations could found much faster that possibly adaptive rule termination call reinforcement learning algorithm might lead accelerated learning free later iterations ﬁner approximation optimal value. section present main algorithm optimal nudging. belonging realm generic algorithm philosophy optimal nudging disentangle gain updates value learning turning average-reward semi-markov task sequence cumulative-reward tasks. dual advantage letting treat black reinforcement learning method used allowing eﬃcient update scheme gain. method favours intuitive understanding term much approximation optimal average reward rather punishment taking actions must compensated rewards obtained afterwards. exploits bertsekas split focus value cost successive visits possible update gain ensure convergence solution. finally show updates performed optimally requires small number calls black-box reinforcement learning method. summary assumptions. algorithm derived requires average-reward semi-markov decision process solved ﬁnite state action sets contain least recurrent state unichain. further assumed expected cost every policy positive larger magnitude non-zero action costs also larger one. avoid duplication cases considered mathematical derivation would insight also assumed least policy positive average reward. case naturally optimal policy positive gain. following restatement known fractional programming result linking notation optimization gain ratio parametric family linear problems policy value cost. order compute propose separating problem parts ﬁnding reinforcement learning optimal policy value ﬁxed gain independently gain-update. thus value-learning becomes method-free robust methods listed section used stage. original problem turned sequence mdps series temporarily ﬁxed hence remaining within bounds generic algorithm propose hurry update every step q-update. additionally consequence lemma method comes gain-optimal suggests nudged version learning algorithm algorithm term nudged comes understanding measure punishment given agent action order promote receiving largest rewards soon possible. remaining problem describe suitable ρ-update rule. observe a—possibly loose—bound however become tight case task gain-optimal policy termination occurs step reward magnitude importantly assumptions deﬁnition policies ﬁnite real expected value ﬁnite positive cost mapping. ready propose simple mapping policy value cost -dimensional space using transformation equations direct consequence proposition whole policy space level line slope d−ˆv d+ˆv intercept origin. thus stated proposition policies value axes further policies expected value line. shows cost level sets space. interesting question whether origin actually belongs space. since would correspond policies inﬁnite expected cost would contradict either unichain assumption assumption recurrent state origin excluded space. following result derives fact that even though value cost expressions equations convex functions value cost level sets lines divides triangle polygons stance policy maximum value value level line splits space triangle points corresponding higher value lower value level set. mapping interior point convex hull policy cloud points edge convex hull consequently least which discussed above fact would correspond policy value unity cost policy would receive highest possible reinforcement recurrent state would return step probability one. which again line space whose slope further depends common nudged value thus instance level corresponding policies zero nudged value interest termination condition fractional programming lemma algorithm unity slope. proof result proved straightforward algebra. consider ﬁxed line level sets corresponding nudged values ˆhρi ˜hρi. making right hand sides expressions form equation equal solving component intersection yields section introduce notion enclosing triangle. method introduced reduce gain uncertainty exploits properties reducing enclosing triangles optimally reduce uncertainty iterations. figure illustration deﬁnition enclosing triangle. segment joining vertices belongs line slope one. vertex anywhere light grey triangle including edges. slopeﬁfth sixth conditions conﬁne possible location triangle vertices intersection lines cross slope zero slope minus one. triangle pictured thick dashed lines figure remark observe that deﬁnition enclosing triangle degenerate indeterminate cases possible. first concurrent must concurrent them—so triangle fact point—and terms slope conditions deﬁnition become indeterminate. alternately condition must collinear. admit degenerate cases valid enclosing triangles since discussed below correspond instances solution underlying average-reward task found. since assume positive-gain policies thus positive-gain optimal policies exist direct application deﬁnition enclosing triangle leads following proposition order understand reduction uncertainty solving reinforcement learning task ﬁxed gain consider geometry setting value within uncertainty range initial enclosing triangle example value initial state resulting optimal policy solving reinforcement learning problem rewards semi-markov task would solved since termination condition would met. observe would imply require knowledge value cost conversely coordinates optimal policy. however complete solution task would known optimal gain would optimal policy gain-adjusted value would respectively policy value found. space knowledge required conclude coordinates optimal policy problems would somewhere inside space line slope figure geometry solution ﬁxed initial enclosing triangle. nudged optimal policy maps somewhere segment gain-optimal policy must somewhere area shaded points regions discarded. first policies task points triangle since would higher ρ-nudged value contradicting optimality policy found. second policies coordinates region labelled would lower gain policies segment known contain least policy nudged optimizer. thus direct consequence able discard regions uncertainty halves range equation diﬀerence -projections vertices triangle line values considered example total uncertainty reduces approximately ﬁvefold thus running reinforcement learning algorithm solve nudged problem within bounds initial enclosing triangle allows ﬁrst determination smaller enclosing triangle second corresponding reduction gain uncertainty. figure possible outcomes solving nudged task within bounds arbitrary enclosing triangle. cases either uncertainty range vanishes point line segment problem solved; smaller enclosing triangle results observations valid general case illustrated figure consider arbitrary enclosing triangle vertices within limits determined triangle resulting nudged problem solved yielding possible degenerate cases reduction point level crosses points situation problem solved geometrically uncertainty area reduces enclosing triangle line segment. three instances method stops gain uncertainty reduces right triangle cannot correspond policies would solved nudged problem instead gain optimal policy must inside triangle vertices second case level intercept becomes intercept slope- projection again points light grey triangle vertices cannot contain mapped policies would contradict nudged optimality furthermore points trapeze left enclosing triangle cannot contain gain-optimal larger gain. policy since policies includes -projection smaller than also -projection larger resulting proof preceding discussion figure show build triangle case must contain mapping optimal policy. appendix show resulting triangle indeed enclosing holds conditions deﬁnition strictly smaller original enclosing triangle. another important geometrical feature arises case policy nudged-optimal diﬀerent nudges optimal nudged values diﬀerent signs. pictured figure assume gain value nudged task solved resulting nudged-optimal policy positive value geometry shown left plot. case lemma figure region shaded light grey cannot contain mapping policies without contradicting optimality dark grey area represents next enclosing triangle. next gain policy geometry shown right plot policies mapping light gray area remarkably also conclude optimizer cases also gain optimal policy global task. indeed bottom plot figure shows light grey union areas cannot contain policy mappings. principle would reduce uncertainty region remark similar observation made bertsekas observed optimal value reference state ssps concave monotonically decreasing piecewise linear function gain. however consequence value-drift learning unlike methods literature optimal nudging rely termination ﬁrst zero-crossing. ﬁnal step remaining formulation start iteration algorithm deciding gain geometrically corresponds choosing location vertex next pencil lines. next section shows optimally. already discussed implications setting current gain/nudging value solving resulting cumulative reward task. problem remains ﬁnding good update turns updates done optimally minmax sense. start arbitrary enclosing triangle vertices deﬁnition enclosing triangle slope unity. refer slope segment notational simplicity refer projection line direction slope point coordinates projection kind projection simple general form assume value inside uncertainty region. goal best location solving ρx-nudged problem cumulative task disregarding cases leads immediate termination resulting geometry problem would similar shown middle right bottom middle plots figure three cases resulting reduction uncertainty form proof lemma presented appendix maximum left uncertainty enclosing triangle described equation conic section rearrangement terms also represented using homogeneous form right uncertainty. derivation optimization right uncertainty function considerably intricate left uncertainty. following result summarizes features. proof since maximum left right uncertainty respectively monotonically increasing decreasing functions minimum value zero maximum minimized equal. thus principle problem choosing order minimize possible uncertainty next iteration reduces making right hand sides equations equal solving although ﬁnding analytical expression solution seems intractable clearly algorithm root ﬁnding readily used here however even necessary. since maximum minimum left uncertainty conic sections homogeneous form known intersection straightforward time following process described detail perwass section interested ﬁnding bounds number calls black reinforcement learning solver inside loop algorithm easy therein lies bulk computation since steps involve geometric algebraic computations done constant negligible time. moreover type reinforcement learning performed speciﬁc algorithm used complexity bounds convergence guarantees principle transparent optimal nudging. order study number calls reinforcement learning inside algorithm start introducing closely related variant showing immediately provides bound optimal nudging. consider α-nudged algorithm gain-update step nudging fraction interval current bounds ﬁxed throughout. easy upper bound reduction uncertainty α-nudging step assume largest among whole interval whole possible therefore setting ensures uncertainty range reduce least half iterations. obviously ﬁrst bound complexity optimal nudging since algorithm practical purposes adaptively adjusting iterations designed minimize uncertainty algorithm never outperform remaining question whether logarithmic bound tight enclosing triangle gain update midpoint uncertainty range best possible reduction uncertainty half. turns almost case. consider enclosing triangle vertices small value further suppose gain-update step optimal nudging algorithm ﬁnds also expressed adaptive form following values expressions left right uncertainty readily found thus iteration corresponding enclosing triangle tends zero optimal nudging fact approaches α-nudging however maximum possible resulting uncertainty reduction case slightly strictly smaller half number qualiﬁcations bound required however. observe that point optimal nudging setting gain ﬁxed value solving resulting cumulative-reward task termination conditions indicating global smdp solved. bound considers uncertainty range worst case worst possible enclosing triangle. thus much faster operation suggested reduction-of-uncertainty-inhalf bound expected. illustrate this sampled million valid enclosing triangles space studied much optimizing eﬀectively reduces uncertainty range. sampling procedure obtain enclosing triangle ﬁrst generated uniformly figure shows approximation level sets distribution relative uncertainty reduction marginal expectation length initial uncertainty. bound computed above expect triangles small initial uncertainty namely close origin uncertainty reduce half well triangles reductions line. indeed case sample expected reduction much larger average less original uncertainty. thus although obviously actual number calls reinforcement learning algorithm inside loop optimal nudging task-dependent gain-uncertainty expected reduce considerably half call. moreover nothing bars transfer learning iterations. optimal policy found iteration estimated value used starting point learning stage setting optimal policies gains close expect learning second iteration converge much faster started scratch. consequence this calls black learning algorithm required also iterations progress calls yield solutions faster. although transfer reinforcement learning active research ﬁeld knowledge problem transfer tasks whose diﬀerence lies reward function explicitly explored. suggest kind transfer convenient practical implementations optimal nudging leave theoretical study open question. section present experimental results applying optimal nudging sample tasks. goal experiments compare performance methods introduced paper algorithms literature well study certain features optimal nudging algorithm itself complexity recall motivating example task section figure shows simple implementation optimal nudging approximately good best ﬁnely tuned version r-learning task while discussed less parameters updates step well room speed improvement fronts. section return task explore eﬀect parameter. deﬁnition remember bound unsigned unnudged reward thus possibly loose bound gain. since rewards task non-negative optimal nudging results presented section could approximated setting gain zero ﬁnding policy maximum expected reward returning recurrent state servers occupied. ﬁrst half million samples used purpose. exact value found task method tightest possible value would maxπ∈π deﬁnition experiment value zero-gain cumulative-optimal policy recurrent state found exactly using dynamic programming figure shows solid black lines performance optimal nudging expected small number iterations–around four–the current approximation gain becomes nearly optimal nudged value recurrent state approaches zero termination condition. solid grey lines plots show eﬀect overestimating hundred times since number calls black-box reinforcement learning algorithm inside optimal nudging loop grows logarithm number iterations required achieve performance increases three tenfold increase hand dashed grey lines plots show eﬀect underestimating since case smallest valid value setting parameter cause loss theoretical guarantees method. however values method still works resulting slightly faster convergence optimal gain termination condition. case long mapped policy cloud remains policies visited algorithm negative implementation method fails negative arguments root right uncertainty equation thus although overestimating naturally causes algorithm converge larger number iterations increase buﬀered logarithmic complexity optimal nudging respect parameter. conversely underestimating tightest bound accelerate learning somewhat cost loss theoretical guarantees eventual failure method. second experiments consider number tasks paper bertsekas introduces state-space splitting process optimal nudging. paper presents versions dynamic-programming stochastic-shortest-path updates ssp-jacobi ssp-gauss-seidel. consider ﬁrst kinds randomly generated tasks called simply problems table paper; respectively hereon. types task states reward taking action randomly selected range according uniform distribution pair states transition probability non-zero selected according rule non-zero transition probabilities drawn uniform distribution normalized. cases ensure compliance unichiain condition n-th state recurrent bertsekas split normalization small transition probabilities results listed count number sweeps methods. sweep simply pass updating value states. methods perform continuous sweeps termination optimal nudging iteration comprised number sweeps added determine total number algorithm. context original paper taking actions causes positive cost goal minimize average cost. consistency derivation without altering algorithmic performance instead maximize positive reward. worth noting sample update value state methods also update bounds gain gain itself optimal nudging doesn’t perform additional updates means nudged iterations outset considerably faster cases. tasks action available state obviously policy task problem policy evaluation rather improvement. source paper consider cases transition probability matrix sparse unstructured. transition non-zero probability implementations optimal nudging tasks mirror jacobi gaussseidel updates original paper reinforcement learning step include cases learning gain update values states reset zero change-of-sign termination condition ignored learning transfer latest values gain updates termination zero crossing. preliminary experiments showed last conﬁguration signiﬁcant reduction number sweeps comes termination condition. crossing striking. whereas version optimal nudging take average times many sweeps altogether reach similar results including changeof-sign condition never yialds ratio higher considering inside sweeps nudged update faster means cases task optimal nudging comparable performance methods. moreover many cases remarkably sparse–most diﬃcult–ones smallest optimal nudging requires average less sweeps algorithms. table summarizes ratio number sweeps required optimal nudging required algorithms. case even version optimal nudging never takes times many sweeps notably ratio reduces consistently size problem grows. zero-crossing condition included optimal nudging versions become much faster requiring less sweeps methods largest tasks. ﬁnal experiment compares performance optimal nudging r-learning problem complex dynamics larger action space. task discretization tracking experiment discussed paper hasselt wiering figure environment tracking task. target moves anticlockwise light grey cell another. agent move four cells direction without crossing black obstacle exiting grid. goal agent follow target closely possible moving little possible. centred move four cells direction. suﬃcient manoeuvre around obstacle agent able keep target. case collision obstacle edges grid agent moves valid cell closest exit collision point. described task isn’t unichain doesn’t recurrent state. indeed policy staying place recurrent state task unichain condition doesn’t hold straightforward design policies bring keep agent diﬀerent cells recurrent sets states common. overcome this arbitrarily recurrent state target agent cell right bottom-left corner grid. state target position agent moves recurrent state probability action. observe making transition probability recurrent state smaller would increase value would change optimal policies gain/value. average runs algorithms solve task optimal nudging diﬀerent r-learning set-ups. cases ε-greedy action selection used order sample state space uniformly every moves state reset randomly selected one. optimal nudging q-learning learning rate compute gain every samples. initialized zero state-action pairs. transfer made nudging iterations. although upper bound readily available still ﬁrst batch samples approximate additionally although keep track zero-crossing condition terminate computation holds allow algorithm observe million samples. exactly machine precision. something r-learning couldn’t accomplish experimental runs contrary optimal nudging always found gain-optimal policy half cases r-learning found policy diﬀered optimal least state. additionally grey vertical line indicates point zero-crossing termination condition optimal nudging runs fact particular experiment optimal nudging requires thirds samples approximation optimal gain third error r-learning. presented novel semi-markov average-reward reinforcement learning method that belonging generic class algorithms described algorithm diﬀers crucial aspect frequency mode gain updates. methods described preceding literature update gain sample update gain-adjusted value state dynamic programming methods taking action model-based model-free methods optimal nudging algorithm consists series cumulative tasks ﬁxed gains updated current task considered solved. delaying gain updates feasible because nudged value ﬁxed gain known possible relatively simple ensure update maximum possible gain uncertainty minimized. introduced straightforward disentangling value gain updates advantage allowing cumulative-reward reinforcement learning method either exploiting robustness speed approximation inheriting theoretical convergence properties guarantees. regarding complexity optimal nudging itself shown number calls learning algorithm logarithmic desired precision parameter upper bound gain. also proved additional condition early termination optimal nudging iterations policy optimizes nudged value optimal value reference state switches sign. condition unique optimal nudging since method continuously updates gain many sign changes expected reference state throughout none conclusively guarantee termination. moreover experiments random tasks bertsekas show condition eﬀective reducing number samples required learn practical cases. additionally compared traditional algorithms maintaining competitive performance sometimes outperforming traditional methods tasks increasing complexity optimal nudging advantage requiring least parameter less learning rate gain updates well three updates less sample. last improvement represent signiﬁcant reduction computation time cases samples already stored observed quickly finally number lines future work open study improvement optimal nudging algorithm. first figure suggests call balck-box reinforcement learning method termination condition could probably adaptively depend nudged-optimal value recurrent state since early learning iterations nudged-optimal policies likely require less precision terminate without aﬀecting performance. likewise mentioned above transfer learning case keeping state values updating gain lead faster termination reinforcement learning algorithm specially towards diﬀerences nudged-optimal policies tend reduce. although observation bertsekas testbed transfer doesn’t eﬀect zero-crossing stopping condition would suggest study alters learning speed either diﬀerent tasks diﬀerent algorithms theoretical perspective. transformation could directly applied solve average-reward semi-markov tasks continuous state/action spaces. kind problem received knowledge little attention literature preliminary experiments suggest extension results arena relatively straightforward. ﬁnal avenue future work suggested extension complexity results section case black-box algorithm invoked inside optimal nudging loop pac-mdp. preliminary analysis indicates number calls algorithm would dependent term smallest sampling suggests positive also small equal approximately however since analytic expression thus neither proving indeed case means trivial question exact type dependence number calls inverse remains open anyway. triangle. setting wb−lb wc−lc solving resulting task rewards in-line discussion figure show depending sign optimal value recurrent state ﬁxed gain remains show resulting triangle strictly smaller well enclosing order study whether conditions listed hold simple aﬃne transformation space auxiliary space horizontal-component value original likewise value equal +βbxy point segment express mapping hand must found analytically intersection point axycxy segments. observe slope axycxy segment positive. indeed since condition holds assumption triangle degenerate factors summands nonnegative. furthermore since rule possibility ﬁrst term indeed positive thus inequality holds. direct consequence ordering extreme positions also strictly positive intermediate points. arbitrary ﬁxed then location interest maximum right uncertainty. since point lies segment line whose expression known solve problem appropriate sign maximizer. notice that enclosing triangle expression inside radical always non-negative. since positive non-negative. also |mβ| opposite signs expressions inside radicals always positive. non-degenerate enclosing positive |mγ| triangle recall factors ﬁrst root positive negative exactly negative factors second root. conversely negative optimal nudging update stage additional step required order determine precisely four points corresponds intersection current left right uncertainty segments conics. easily done ﬁnding experience veriﬁcation suﬃcient updated gain multiple intercepts inside segment. however several competing candidates appear additional veriﬁcation step required determine simultaneous unique solution equations", "year": 2015}