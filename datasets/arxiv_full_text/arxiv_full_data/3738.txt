{"title": "Learning Complex Swarm Behaviors by Exploiting Local Communication  Protocols with Deep Reinforcement Learning", "tag": ["cs.MA", "cs.AI", "cs.LG", "cs.SY", "stat.ML"], "abstract": "Swarm systems constitute a challenging problem for reinforcement learning (RL) as the algorithm needs to learn decentralized control policies that can cope with limited local sensing and communication abilities of the agents. Although there have been recent advances of deep RL algorithms applied to multi-agent systems, learning communication protocols while simultaneously learning the behavior of the agents is still beyond the reach of deep RL algorithms. However, while it is often difficult to directly define the behavior of the agents, simple communication protocols can be defined more easily using prior knowledge about the given task. In this paper, we propose a number of simple communication protocols that can be exploited by deep reinforcement learning to find decentralized control policies in a multi-robot swarm environment. The protocols are based on histograms that encode the local neighborhood relations of the agents and can also transmit task-specific information, such as the shortest distance and direction to a desired target. In our framework, we use an adaptation of Trust Region Policy Optimization to learn complex collaborative tasks, such as formation building, building a communication link, and pushing an intruder. We evaluate our findings in a simulated 2D-physics environment, and compare the implications of different communication protocols.", "text": "natural counterparts. kube example investigate cooperative prey retrieval ants infer rules swarm robots fulﬁll task cooperative box-pushing. similar work found e.g. however extracting rules tedious complexity tasks solve explicit programming limited. paper want learn complex swarm behavior using deep reinforcement learning based locally sensed information agents desired behavior deﬁned reward function instead hand-tuning simple controllers agents. swarm systems constitute challenging problem reinforcement learning algorithm needs learn decentralized control policies cope limited local sensing communication abilities agents. collective tasks require form active cooperation agents. efﬁcient cooperation agents need implement basic communication protocols transmit local sensory information neighbored agents. learning communication protocols simultaneously learning behavior agents seems reach current reinforcement learning algorithms explains limited success deep reinforcement learning swarm systems. however using prior knowledge given task simple communication protocols deﬁned much easily directly deﬁning behavior. paper propose evaluate several simple communication protocols exploited deep reinforcement learning decentralized control policies multi robot swarm environment. communication protocols based local histograms encode neighborhood relation agent agents also transmit task-speciﬁc information shortest distance direction desired target. histograms deal varying number neighbors sensed single agent depending current neighborhood conﬁguration. protocols used generate high dimensional observations individual agents turn exploited deep reinforcement learning efﬁciently learn complex swarm behavior. base algorithm trust region policy optimization state-of-the-art deep reinforcement learning algorithm. resulting algorithm provides integrated learning framework speciﬁcally tailored swarm setting. approach able learn decentralized control policies end-to-end fashion mappings local sensory input actions without need complex abstract— swarm systems constitute challenging problem reinforcement learning algorithm needs learn decentralized control policies cope limited local sensing communication abilities agents. although recent advances deep algorithms applied multi-agent systems learning communication protocols simultaneously learning behavior agents still beyond reach deep algorithms. however often difﬁcult directly deﬁne behavior agents simple communication protocols deﬁned easily using prior knowledge given task. paper propose number simple communication protocols exploited deep reinforcement learning decentralized control policies multi-robot swarm environment. protocols based histograms encode local neighborhood relations agents also transmit task-speciﬁc information shortest distance direction desired target. framework adaptation trust region policy optimization learn complex collaborative tasks formation building building communication link pushing intruder. evaluate ﬁndings simulated d-physics environment compare implications different communication protocols. nature provides many examples performance collective limited beings exceeds capabilities individual. ants transport prey size single could carry termites build nests nine meters height bees able regulate temperature hive. common phenomena fact individual basic local sensing environment limited communication capabilities neighbors. inspired biological processes swarm robotics tries emulate complex behavior collective rather simple entities. typically robots limited movement communication capabilities sense local neighborhood environment distances bearings neighbored agents. moreover agents limited memory systems agents access short horizon perception. consequence design control policies capable solving complex cooperative tasks becomes non-trivial problem. common approach program systems extracting rules observed behavior demonstrate three cooperative learning tasks simulated swarm environment. environment inspired colias robot modular platform wheel motor-driven movement various sensing systems. paper outline section review concepts trpo describe problem domain. section show detail tackle challenges modeling observations policy partially observable swarm context adapt trpo setup. section present model parameters agents introduce three tasks evaluate proposed observation models policies. trust region policy optimization trust region policy optimization algorithm optimize control policies single-agent reinforcement learning problems problems formulated markov decision processes compactly written tuple agent chooses action policy based current state progresses state according transition function step agent assigned reward provided reward function judges quality decision. goal agent policy maximizes cumulative reward achieved certain period time. policy parametrized parameter vector containing weights biases neural network. following denote parameterized policy reinforcement learning objective expressed ﬁnding policy maximizes expected advantage function current policy i.e. estimate advantage function current policy πold deﬁned qπold old. herein state-action value function qπold typically estimated single trajectory rollout value function πold rather simple baselines used ﬁtted monte-carlo returns. objective maximized subject ﬁxed constraint kullback-leibler divergence policy parameter update ensures updates policy’s parameters bounded order avoid divergence learning process. overall optimization problem summarized problem domain building upon theory single-agent reinforcement learning formulate problem domain swarm environments. instead considering single agent consider multiple agents type interact environment. limited sensory input agent obtain local observation vicinity environment hence partially informed global system state global system state case comprised local states agents additional attributes environment. order cope limitation agents need make observation histories successfully solve global collaborative task considering homogeneous system assume agents execute distributed policy deﬁned mapping histories past actions observations within ﬁnite horizon global task agents encoded reward function write denote joint action vector whole swarm. related work currently majority deep reinforcement learning literature focuses single-agent scenario approaches tackling multi-agent problem. approaches found authors variation deep deterministic policy gradient algorithm learn centralized q-function policy. accounts reasoning agents behavior linear increase dimensionality joint observation action spaces makes scaling algorithm many agents hard. another algorithm tackling credit assignment problem found here baseline agents’ behavior subtracted centralized critic reason quality single agent’s behavior. however approach possible scenarios discrete action spaces since requires marginalization agents’ action space. finally different line work concerning learning communication models agents found section brieﬂy discuss used model state representation single agent. subsequently introduce different communication protocols based neighborhood histograms used combination solve complex swarm behaviors. algorithm relies deep neural network policies special architecture exploit structure high-dimensional observation histories. present network model subsequently discuss small adaptations make trpo algorithm order apply cooperative multi-agent setting. orientation i.e. xmax ymax robot control speed wheels. therefore apply force left right side agent similarly wheels real robot. model single agent inspired colias robot underlying principles straightforwardly applied swarm settings limited observations. neighborhood histograms individual agents observe distance bearing neighbored agents communicate agent. assume agents constantly sending signal neighbored agents localize sources. arising neighborhood conﬁguration important source information used observations individual agents. arising difﬁculties case handle changing number neighbors would result variable length observation vector. policy representations neural networks expect ﬁxed input dimension. possible solution problem allocate ﬁxed number neighbor relations agent. agent experiences less neighborhood relations standard values could used high distance bearing. however approach comes several drawbacks. first size resulting representation scales linearly number agents system number parameters learned. second execution learned policy limited scenarios exact number agents present training. third ﬁxed allocation neighbor relation inevitably destroys homogeneity swarm since agents longer treated interchangeably. particular using ﬁxed allocation rule requires agents must able discriminate neighbors might even possible ﬁrst place. solve problems propose histograms observed neighborhood relations e.g. distances bearing angles. representation inherently respects agent homogeneity naturally comes ﬁxed dimensionality. hence canonical choice swarm setting. experiments consider different types representations concatenated one-dimensional histograms distance bearing multidimensional histograms. types illustrated figure one-dimensional representation advantage scalability grows linearly number features. downside shortest path partitions many applications important transmit location point interest neighbored agents currently observe point limited sensing ability. assume agent observe bearing distance point interest within communication radius. agent transmits observed distance agents. agents point interest might case observe message another agent containing distance point interest. distance sending agent added received distance obtain distance point interest would sending agent point. agent might compute several distances transmits minimum distance computed indicate length shortest path seen. location neighbored agents including distance shortest path information important knowledge policy e.g. navigating point interest. hence adapt histogram representation. partition contains minimum received shortest path distance agent located position. weight sharing policy networks policy maps sequences past actions observations action. histories ﬁxed length input policy feed-forward deep neural network architecture. architecture chosen initial experiments recurrent neural networks poor results small history lengths often sufﬁcient swarm behaviors communication protocols provide enough useful information. single observation constituted multiple histograms partitions observation space already quite high dimensional. using history observations adds dimensionality input network. cope high input dimensionality propose weight sharing approach. action-observation pair agent’s history ﬁrst processed independently network using weights. initial reduction dimensionality hidden states concatenated subsequent layer ﬁnally mapped output. independent layers formally described {wi} {bi} weight matrices bias vectors activation function number independent hidden layers. input ﬁrst layer deﬁned denotes concatenation shared vectors. note whole length history reducing number parameters necessary process history. fig. diagram shows model proposed policy numbers inside boxes denote dimensionalities hidden layers. plus sign denotes concatenation vectors. number combined hidden layers. output last layer used action vector agent. homogeneity agents achieved using parameters policies. diagram architecture shown figure adaptations trpo order apply trpo multi-agent setup small changes original algorithm made similar formulation first since assume homogeneous agents parameters policy shared agents. since agents cannot rely global state advantage function redeﬁned order estimate function agent assigned global reward time step transitions treated executed single agent. colias colias robot consists base module housing motion controller wheel-driven movement three shortrange bump sensors several extension modules e.g. long-range module sensors camera module bluetooth communication. furthermore ambient light sensor. work rely solely data short long range sensors ambient light sensor. agents able move maximum speed approximately cm/s. case active object detection short range sensors range long fig. illustration three cooperative tasks used paper. green dots represent agents green ring segments located next agents indicate short range front sensors. outer green circles illustrate maximum range distances bearings agents observed depending used observation model. edge task rings show penalty zones agents punished outer green rings indicate zones legal edges formed. link task dots correspond points need connected agents. push task represents intruder tries reach point marked black agents push away possible. history individually hidden layers size output second layer concatenated form input third hidden layer eventually maps actions left right motor. example case observation representation using histogram dimensionality previous timesteps instead process input vector size dimensions single timestep dimensional space concatenate representation dimensional representation. task building graph ﬁrst task goal agents maintain certain distance other. kind behavior required example surveillance tasks group autonomous agents needs maximize coverage target area maintaining communication links. formulate task graph problem agents maximize number active edges graph. herein edge considered active whenever distance corresponding agent lies certain range. setting visualized figure range sensors range signals transmitted agents received reliably processed distance platform still development thus able deploy learned policies yet. generally observation model comprised sensor readings short long range sensors furthermore augment observation representation communication protocols developed section model also accounts noise estimation since precise positions provided agents. consider following dimensionality observations discretizing leads resolution distance angle histogram representation. note either histograms used time. additional features added depending task explained experiments section. simulation running realistic physics engine allowing correct physical interaction bodies agents. fig. learning curves edge task. curves show mean values plus minus standard deviation computed eight learning trials. legend two-dimensional histogram distances bearings independent histograms distances bearing distance histogram bearing histogram sensor histogram. second task adds another layer difﬁculty. maintaining network agents locate connect randomly placed points state space. link established successfully communicating agents connecting points. figure shows example active link spanned three agents points. task resembles problem establishing connection nodes wireless network experiments distance points chosen larger requiring least three agents bridge between. reward determined length shortest distance points dopt length shortest active link spanned agents task pushing intruder third task group colias robots shall prevent another colias robot reaching speciﬁed target position. task similar box-pushing problem challenging intruder adds additional dynamics problem. target position indicated light source allows intruder determine euclidean distance target ambient light sensor. defenders however informed distance intruder light position within communication range light position part agents’ observation. policy intruder modeled phototaxis behavior part learning process. reward computed based current distance intruder target position. distance given light given point intruder’s position reward function addition standard sensor readings give agents ability distinguish intruder agents. additional information encoded extra dimensions range bearing observation representation. task tested different observation protocols i.e. neighborhood histograms well shortest path partitions intruder’s position used point interest. task shortest path partitions communication protocol. agent communicates shortest path knows points interests resulting partitions used observation input single time step. evaluate task standardized environment size initialize agents randomly scene. special amount information provided agents affects overall system fig. learning curves link task push task obtained different observation models ﬁxed history length curves show mean values plus minus standard deviation based eight learning trials. legend dimensional histogram shortest paths two-dimensional histogram distances bearings independent histograms distances bearing sensor histogram. performance. herein keep mind general information-complexity trade-off i.e. high-dimensional local observations generally provide information global system state time result complex learning task. recall information content mostly inﬂuenced factors edge task first evaluate history length affects system performance. figure shows evaluation weight sharing policy using two-dimensional histogram distances bearings. interestingly observe longer observation histories show increase performance. either increase information could counter effect increased learning complexity history length already sufﬁcient solve task. ﬁndings history length remainder experiments. next analyze impact observation model. figure shows results learning process different observation modalities. ﬁrst observation that irrespective used mode agents able establish certain number edges. naturally complete information distances bearing yields best performance. however independent histogram representation yields comparable results dimensional histogram. again aforementioned complexity trade-off higher amount information makes learning process difﬁcult. advanced shortest path histograms distance bearing. based ﬁndings edge task keep history length figure shows results learning process observation model tested averaged trials. since least three agents necessary establish link points models without shortest path information struggle reliably establish connection. chance spread wide possible thus cover area points. again interesting independent histograms counts seem favorable histogram. however versions surpassed histogram shortest paths yields information current state whole network agents currently connected points. push task push task turned hardest challenge. given information current distance bearing neighboring agents defenders unable strategy prevent intruder reaching light position. agents access shortest path intruder able push away goal. however would like investigate improve agents’ behavior successfully execute task. paper demonstrated histograms simple local features effective processing information robot swarms. central aspect model ability handle arbitrary system sizes without discriminating agents makes perfectly suitable swarm setting agents identical number agents neighborhood varies time. protocols adaptation trpo witkowski habbal herbrechtsmeier tanoto penders alboul gazi. ad-hoc network communication infrastructure multi-robot systems disaster scenarios. proceedings iarp/euron workshop robotics risky interventions environmental surveillance swarm setup learn cooperative decentralized control policies number challenging cooperative task. evaluation approach showed histogram-based model leads agents reliably fulﬁll tasks. alonso-mora montijano schwager rus. distributed multi-robot formation control among obstacles geometric proceedings ieee optimization approach consensus. international conference robotics automation pages chen gauci groß. strategy transporting tall objects swarm miniature mobile robots. proceedings ieee international conference robotics automation pages foerster assael freitas whiteson. learning communicate deep multi-agent reinforcement learning. advances neural information processing systems pages goldberg mataric. robust behavior-based control distributed multi-robot collection tasks. technical report university southern california angeles united states lillicrap ghahramani turner levine. q-prop sample-efﬁcient policy gradient off-policy critic. proceedings international conference learning representations hoff sagoff wood nagpal. foraging algorithms robot swarms using local communication. proceedings ieee international conference robotics biomimetics pages martinoli easton agassounon. modeling swarm robotic systems case study collaborative distributed manipulation. international journal robotics research mnih kavukcuoglu silver rusu veness bellemare graves riedmiller fidjeland ostrovski human-level control learning. nature", "year": 2017}