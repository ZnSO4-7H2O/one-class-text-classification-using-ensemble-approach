{"title": "On better training the infinite restricted Boltzmann machines", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "The infinite restricted Boltzmann machine (iRBM) is an extension of the classic RBM. It enjoys a good property of automatically deciding the size of the hidden layer according to specific training data. With sufficient training, the iRBM can achieve a competitive performance with that of the classic RBM. However, the convergence of learning the iRBM is slow, due to the fact that the iRBM is sensitive to the ordering of its hidden units, the learned filters change slowly from the left-most hidden unit to right. To break this dependency between neighboring hidden units and speed up the convergence of training, a novel training strategy is proposed. The key idea of the proposed training strategy is randomly regrouping the hidden units before each gradient descent step. Potentially, a mixing of infinite many iRBMs with different permutations of the hidden units can be achieved by this learning method, which has a similar effect of preventing the model from over-fitting as the dropout. The original iRBM is also modified to be capable of carrying out discriminative training. To evaluate the impact of our method on convergence speed of learning and the model's generalization ability, several experiments have been performed on the binarized MNIST and CalTech101 Silhouettes datasets. Experimental results indicate that the proposed training strategy can greatly accelerate learning and enhance generalization ability of iRBMs.", "text": "abstract infinite restricted boltzmann machine extension classic rbm. enjoys good property automatically deciding size hidden layer according specific training data. sufficient training irbm achieve competitive performance classic rbm. however convergence learning irbm slow fact irbm sensitive ordering hidden units learned filters change slowly left-most hidden unit right. break dependency neighboring hidden units speed convergence training novel training strategy proposed. idea proposed training strategy randomly regrouping hidden units gradient descent step. potentially mixing infinite many irbms different permutations hidden units achieved learning method similar effect preventing model over-fitting dropout. original irbm also modified capable carrying discriminative training. evaluate impact method convergence speed learning model's generalization ability several experiments performed binarized mnist caltech silhouettes datasets. experimental results indicate proposed training strategy greatly accelerate learning enhance generalization ability irbms. symmetrically-coupled binary stochastic units proposed find statistical regularities data. popular subset boltzmann machines restricted boltzmann machine various extensions enjoyed much popularity pattern analysis generation. generality flexibility rbms enable used wide range applications e.g. image audio classification generation collaborative filtering motion modeling etc. specifically bipartite graphical model uses layer hidden binary variables units model probability distribution layer visible variables. enough hidden units able represent binary probability distribution fitting training data well possible. general adding hidden units improve representational power model however number hidden units increases many learned features become strongly correlated increases risk fitting. choosing proper number hidden units requires procedure model selection time-consuming. deal issue welling propose boosting algorithm feature space iteration feature added greedily learned. nair，et conceptually weights infinite number binary hidden units connect sigmoid units noisy rectified linear units better feature learning. recently côté larochelle proposed non-parametric model called irbm. making effective number hidden units participating energy function change freely training irbm automatically adjust effective number hidden units according data. implicitly infinite capability irbm also makes capable lifelong learning. despite that major drawback irbm slow convergence learning offsets advantage brings certain degree. reason drawback that hidden units correlated given visible variables. learned filters feature detectors change slowly left-most hidden unit right also called \"ordering effect\" fixed order hidden units reason strong dependency filters learned neighboring hidden units. newly added hidden unit always begins learns features jointly previous hidden units. fact hidden units constrained fixed order possible permutations considered evaluated. achieve this random permutation hidden units sampled certain distribution gradient descent step. thus neighbors hidden unit continuously changing training progresses encourages hidden units learn features depending themselves. this different irbm trained gradient descent step. infinite many hidden units irbm mixture infinite many irbms different permutation hidden units shared parameters achieved theoretically. point view proposed training strategy provides effective preventing model over-fitting averaging different models nearly always generalization performance similar effect achieved dropout randomly drops units neural network training equivalent combining exponentially many different networks also serves regularization adaptive weight decay sparse representation fact generalized irbm defined treating permutation hidden units model parameter. besides training strategy another contribution work extending irbm capable performing supervised learning related work needs mentioned that ping proposed alternative definition infinite accordingly frank-wolfe algorithm learn name model fw-irbm avoid confusing model studied paper. definition fw-irbm motivated marginal log-likelihood classic rbms. weight matrix treated samples weight vector marginal log-likelihood thus sampling approximation marginal log-likelihood so-called \"fractional rbm\". adding hidden unit equivalent draw sample fractional rbm. training objective fractional tries learning distribution proposes greedy training procedure adds hidden unit iteration updates weight newly added hidden unit. advantage fw-irbm generalized model extended uncountable continuous distribution. however even number hidden units long though order hidden units invariant model's marginal log-likelihood training procedure fw-irbm indicates order hidden units still effect final performance. reason step parameter newly added hidden unit updated based previous added hidden units. greedy optimization algorithm likely result sub-optimal solution reason performance monotonically improving model size gets larger. irbm encounter problem simultaneously updates non-zero parameters automatically decides whether adding hidden unit not. remainder paper organized following order sect. gives brief review irbm model discriminative irbm introduced cause ordering effect briefly analyzed. sect. proposed training strategy formally presented condition model invariant order hidden units proposed proof appendix. sect. several experiments performed empirically evaluate training strategy. finally conclude work sect. section original irbm first introduced briefly. that modifications energy function irbm made leads discriminative irbms. finally briefly analyze cause ordering effect irbms. irbm extension classic mixes infinitely many rbms different number hidden units rbms choose hidden units sequence starting first one. defines probability dimensional visible vector representing observable element infinite-dimensional hidden vector data. random variable understood total number hidden units penalty selected selected participate energy function. weight matrix connecting visible hidden nearly types computing partition function intractable therefore gradients cannot exactly computed. also case irbms infinite many hidden units. côté larochelle suggest using contrastive divergence persistent contrastive divergence algorithms approximately calculate gradients i.e. gibbs sampling used sample second expectation estimated samples. order apply irbm discrimination tasks possible modify energy function make target dependent case classification call model discriminative infinite restricted boltzmann machine energy function dis-irbm defined below reason slow convergence learning irbms newly added hidden units correlated previous hidden units takes long time filters become diverse other. involves summing possible states hidden units conditional probability multiply terms sigmoid function identical classic reflects influence hidden units. hidden units co-adapted represent features. co-adaption sometimes harmful generalization. proposed training strategy formally presented consists parts dynamic training objective approximated gradient descent algorithm optimizing objective. stochastic gradient descent mini-batch gradient descent method used minimize objective function however mentioned above convergence learning slow irbms. hidden unit independent hidden units. first hidden units independent indicates that like classic order first hidden units influence performance model. assume order hidden units changeable jointly train irbms possible orders bias might alleviated learned features become closer inspires propose alternative training objective follows. permutations where numbers. paper uniform distribution gives equal chance permutation. distributions also used. note maximal number activated hidden units gradient descent step stabilize growing selected hidden units part regrouped word \"dynamic\" used emphasize that training objective changes accordingly number regrouping hidden units changes. proposed training procedure briefly illustrated fig. irbms trained gradient descent step irbms share hidden units different permutations. number hidden units grows training number mixed irbms grows non-zero weights accordingly. theoretically allowed take arbitrary positive integer value. thus proposed training objective potentially allow mixture infinite many irbms. convenience name training strategy \"random permutation large model grow explosively. small boosting effect minor. prefer many hidden units regrouped possible meanwhile model would growing rapidly. strategy choosing proper specifically discussed sec. identical learning irbm generative part briefly introduced sect.. directly used compute gradients. approximated gradient generative part given below however approximated gradient causes high variance learning practice. alleviate problem alternative derivation discriminative part proposed follows exactly computed shown appendix. however infinite many parameters. avoid issue compute gradients parameters first hidden units leave remaining parameters operation equivalent using compute gradients non-zero parameters remaining parameters. experimental results shown training successfully make condition proposition approximately satisfied ordering gives nearly identical result thus small enough give good estimate section evaluate training strategy empirically according convergence speed final generalization performance. datasets used evaluation binarized mnist caltech silhouettes .the mnist dataset composed images size pixels representing handwritten digits among images used training images testing. pixel image stochastically binarized according intensity caltech silhouettes dataset composed images size binary pixels representing object silhouettes classes. dataset divided three parts examples training validation testing.we reshape image datasets -dimensional vector concatenating adjacent rows one. designed several experiments different purposes. subsect. principle choosing proper regrouping rate experimentally investigated. subsect. evaluated generalization performance irbm trained according log-likelihood test sets binarized mnist caltech silhouettes. subsect. evaluated generalization performance dis-irbms trained classification tasks. experiments mini-batch size used compute gradients. max-norm regularization also used suppress large weights bounds respectively. côté larochelle claims results learning robust value hidden unit penalty tried enables model grow proper several different size faster beginning learning. however ordering effect takes long time hidden units learn filters diverse other. training also prefers small allow many hidden units mixed possible. small likely cause model grow explosively. based arguments convenience comparing used models paper identical also used regularization regularization regularize models. code reproduce results paper available github. architectures etc. however paper propose alternative training strategy faster convergence better generalization original irbms general. combining techniques benefit training strategies focus comparing basic settings parameters. order preliminary understanding different regrouping rates influence growing model tried several different settings model trained epochs binarized mnist epochs caltech silhouettes setting training repeated times. mean results -time trials illustrated fig. results illustrate growing irbms without training baselines comparison. shown fig. growing model smaller growing even remarkably influenced long stable using caltech silhouettes. similar numbers hidden units activated even though inputs quite different. number activated hidden units ranges binarized mnist caltech silhouettes. fig. illustration training examples epochs training binarized mnist epochs training caltech silhouettes results without training bottom training. case easily checked training examples arbitrary permutation computing epochs training binarized mnist epochs caltech silhouettes average value training sets respectively fairly small. condition proposition approximately satisfied. plots subsection aims evaluating generalization performance irbm trained density model. firstly investigated property training different learning rates regularizations. also trained irbm without training comparison method. irbms trained binarized mnist. different learning rates decaying learning rate adaptive learning rate method adagrad used. global learning rate adagrad gives best results weights regularizations model trained epochs performed annealed importance sampling estimate likelihood test every epochs．the results shown fig. ordinarily trained irbms show results using adagrad decaying learning rate leads fairly poor convergence. epochs training effective activated number hidden units ordinary trained irbms using larger trained irbms convergence much slower later. \"ordering effect\" many filters former inadequately trained results \"redundant\" hidden units later. also noticeable regularization gives better results regularization. indicate sparsity lead better generalization irbms. fig. log-likelihood test binarized mnist irbms different training strategies. \"lr\" \"lr\" stand regularization\" regularization\" respectively \"ada\" stands \"adagrad\" \"dcl\" stands \"decaying learning rate\". fig. illustrates filters learned hidden units irbms epochs training left-most filters illustrated. shown fig. filters learned hidden units training diverse learned without training. former contains various kinds local features strokes specific character parts. ordering effect later obvious left filters look like mixtures different characters right fig. comparison learned filters irbms different learning strategies epochs training without left-most filters shown starting top-left corner incrementing across rows first. based experimental results used steps gibbs sampling steps gibbs sampling train models binarized mnist caltech silhouettes respectively. regularization used models. model trained epochs. also performed grid search hyper parameters .the best results different models illustrated table best model trained binarized mnist effective hidden units. global learning rate regularization weight average log-likelihood test similar to-. size model also smaller hidden units non-zero parameters. best model trained caltech silhouettes effective hidden units. global learning rate regularization weight average log-likelihood test better results fw-irbm also listed fig. random samples drawn best models randomly initializing visible units running gibbs steps examples test also illustrated binarized mnist; caltech silhouettes. classification tasks subsection evaluating generalization performance dis-irbms trained binarized mnist caltech silhouettes. firstly compared performance ordinary training validate boosting learning speed guidance selecting hyper-parameters. discriminative training objective together adagrad used. learning rate trained model without training strategy epochs. every epoch miss-classification rate test computed. repeated training procedure parameter setting times. results shown fig. shown fig. accelerates learning learning rates. adagrad coupled training achieves best performance experiment. best result training epoch high learning rate encourages model quickly explore different regions weight space also likely cause oscillating unless proper learning rate decay used. adagrad gives smaller learning rates parameters close convergence thus stable fixed learning rate. another fact observed fig. that training makes learning stable variance learning much smaller. additional label information makes filters prefer classes instead others \"ordering effect\" significant. according results experiment used train dis-irbm binarized mnist caltech silhouettes evaluated generalization performance test miss-classification errors. adagrad used experiments. validation sets used search hyper parameters log-scale best results different models method illustrated table achieves test error mnist better achieved normally trained dis-irbm. global learning rate regularization weight dis-irbm performs slightly worse classrbm mnist difference best results smaller commonly regarded statistical insignificant mnist. best result caltech silhouettes also achieved dis-irbm trained using hybrid training objective training. test error better achieved normally trained dis-irbm. global learning rate best model regularization weight interesting fact model size irbms trained different training objective size smaller merely discriminative training objective used. makes sense less features often needed model needs discriminate objects other instead modeling examples well. fw-irbm also used classification taking hidden units' activation vectors using input softmax regression fw-irbm cannot perform discriminative training directly training objective learning fw-irbm learning separate procedures. i.e. training fw-irbm iterations parameters using hidden units' activation vectors train results fw-irbm also listed table. \"trick\" make learning faster using momentum different momentum values parameters different hidden units according time trained. hidden unit added training starting momentum value gradually increases paper proposed novel training strategy infinite rbms aims achieving better convergence generalization. core concept training dynamic training objective allows different model optimized gradient descent step. specifically irbm random grouping hidden units sampled gradient descent. implicit mixture infinite many irbms different permutations hidden units achieved training. experiments binarized mnist caltech silhouettes shown that train hidden units efficiently thus results smaller hidden layer size better generalization performance. compared fw-irbm irbm trains hidden units jointly unit greedily update step thus former likely reach sub-optimal solution. future datasets especially real-valued datasets used give evaluation performance training strategy. meanwhile exploring multi-layer extension irbm idea training also applied architecture combined greedy layer-wise pre-training.", "year": 2017}