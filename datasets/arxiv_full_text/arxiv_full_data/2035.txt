{"title": "Variance-Based Rewards for Approximate Bayesian Reinforcement Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "The explore{exploit dilemma is one of the central challenges in Reinforcement Learning (RL). Bayesian RL solves the dilemma by providing the agent with information in the form of a prior distribution over environments; however, full Bayesian planning is intractable. Planning with the mean MDP is a common myopic approximation of Bayesian planning. We derive a novel reward bonus that is a function of the posterior distribution over environments, which, when added to the reward in planning with the mean MDP, results in an agent which explores efficiently and effectively. Although our method is similar to existing methods when given an uninformative or unstructured prior, unlike existing methods, our method can exploit structured priors. We prove that our method results in a polynomial sample complexity and empirically demonstrate its advantages in a structured exploration task.", "text": "explore–exploit dilemma central challenges reinforcement learning bayesian solves dilemma providing agent information form prior distribution environments; however full bayesian planning intractable. planning mean common myopic approximation bayesian planning. derive novel reward bonus function posterior distribution environments which added reward planning mean results agent explores eﬃciently eﬀectively. although method similar existing methods given uninformative unstructured prior unlike existing methods method exploit structured priors. prove method results polynomial sample complexity empirically demonstrate advantages structured exploration task. central challenges reinforcement learning explore–exploit dilemma. agent must maximize rewards simultaneously sacriﬁcing immediate gains learn ways exploit future approach addressing dilemma bayesian reinforcement learning agent endowed explicit representation distribution environments could acts receives observations updates belief environment distribution accordingly. bayes-optimal agent solves explore– exploit dilemma explicitly including information belief state representation incorporating information changes plans however bayes-optimal planning intractable general. number recent methods attempted approximate bayesian planning remains challenging problem. another approach addressing explore–exploit dilemma explicit modiﬁcation objective reward function—adding reward bonus exploration. refer modiﬁed reward function internal reward function. agent always exploits internal reward function accomplishes exploration exploitation thus solving dilemma. approach exempliﬁed many methods framework bound complexity learning explicitly motivating agent sample state-action pairs enough times ensure explored suﬃciently. internal-reward methods advantage bayesian approach—modifying reward function greatly inﬂuence behavior often without greatly aﬀecting computational cost. contrast important advantage bayesian approach exploration guided prior knowledge environment. environments require exploration others; areas state space uncertain others; interestingly information gained area state space aﬀect knowledge areas. bayesian approach expresses speciﬁcation agent’s prior belief. work contribute internal-reward method eﬃcient exploration takes advantage prior knowledge form bayesian prior. thus method retains computational advantage internal reward methods obtaining guidance bayesian prior. speciﬁcally method provides reward bonus proportion square root variance agent’s posterior distribution envifull bayesian planning expensive many priors possible belief states large inﬁnite. furthermore agent constantly learning updating belief agent rarely never revisit belief states. reasons agents must approximate bayes optimality general. planning mean respect belief simple myopic approximation bayesian planning removes state-space explosion belief states preserving physical state dynamics. resulting bellman equations identical equations bayesian planning except belief state updated right-hand side denote value’s dependence current belief subscript emphasize belief’s invariance planning. time step mean agent acts greedily respect observing result action updates belief using bayes’ rule computes thus agent plans mean update belief receives experience planning. reward bonus help compensate information accounted mean planning approximation full bayesian planning. mean plus reward bonus agent identical mean agent described above except uses internal reward function deﬁned mean reward function plus added reward bonus term ˆrb. formally mmdp+rb agent belief always acts greedily respect action-value function deﬁned ronments. proposed method similar existing methods using uninformative prior independent dirichlet distribution transition dynamics. structured priors however show method capable achieving theoretically lower sample complexity bound existing methods. demonstrate method compares favorably existing approximate bayesian methods structured unstructured priors environments including hunt wumpus environment prior knowledge critical over-exploration lead permanent death. method tuple state space action expected reward function state–action pairs pθ|s probability transitioning state given action taken state discount factor. often directly refer mdp’s model parameters subscripts mdp’s functions. sometimes clear context also refer cardinality corresponding sets. unlike standard setting bayesian agent provided a-priori distribution environments could face called prior initial belief belief denote probability particular agent begins acting know speciﬁc environment instead updates belief based experience. planning bayes-optimal agent considers eﬀects future changes belief addition changes physical state. refer joint system information-state mdp. full information state pair state belief state. observing state transition reward agent updates belief bayesian posterior belief using bayes’ rule. therefore state transition information-state possible belief states. deﬁne mean reward function given belief rθb. similarly deﬁne mean pθ|s a)b. number times state–action pair sampled. showed mmdp+rb agent reward bonus β/nsa approximates bayesian planning polynomial time high probability special case independent dirichlet prior transition dynamics state–action pair known reward function. algorithm baseline experiments. mbie-eb algorithm form mmdp+rb though derived bayesian setting. algorithm agent plans maximum-likelihood estimate mdp; similar mean dirichlet prior. mbie-eb features reward bonus nsa. described detail later algorithm also connects closely method; another baseline experiments. baselines share property reward bonuses decrease independently state–action pair sampled. intuitively measure uncertainty agent state–action pair. however neither accounts information contained prior distribution next subsection deﬁne variance-based reward bonus capable measuring uncertainty arbitrary bayesian priors environments. importantly variance world dynamics instead variance model parameters respect agent’s belief. agent gathers experience world—as agent becomes certain truth—these values tend decrease regardless stochastic world notice made state–action independence requirements prior. therefore depending belief experience gained state aﬀect variance term state. section show reward bonus used bound error mean respect random true high probability. using fact show exist constants agent acts greedily respect acts optimally respect random true polynomially bounded number time steps high probability. insight behind variance measure agent’s uncertainty bound deviation mean model true using chebyshev’s inequality states deviation random variable mean multiple variance high probability random variable mean standard deviation bounds deviation mean. lemma belief state–action pair exists value bounds max-norm error mean transition model probability least norm inequality satisﬁed similar analysis reward function though require union bound. lemma belief state–action pair belief distribution rewards ﬁnite variance exists value bounds error mean reward function probability least deﬁnition deﬁne sample complexity function minimum number transitions observed starting belief additive reward term using updated belief less i.e. ˆrbd probability least refer state–action pair known sampled times. make important assumption experience gained sampling state–action pair increase sample complexity another state–action pair. assumption trivially true prior independent state–action pair. furthermore expect reasonable assumption many correlated priors. fact believe method’s performance correlated priors strengths. rather hurting convergence experience gained area state space general reduce number samples required another present evidence consistent below. next present central theoretical result bounds sample complexity respect true mdp. distinct sample complexity bound kolter deﬁned respect information state mdp. theorem sample complexity state action internal reward deﬁned lemma random true model parameters distributed according prior belief algorithm follow \u0001-optimal policy current state respect theorem applied many prior distributions. remainder section apply simple special cases. first provide concrete bound case independent dirichlet prior known reward function. special case connect result related work. lemma number times state–action pair sampled. known reward function independent dirichlet prior next-state transition dynamics state–action pair internal reward feature decreases rate prior-dependent term indicating number times state–action pair must sampled becomes known. algorithm updates value function estimate time state–action pair becomes known. allows bound number times agent plans. section bound sample complexity algorithm following abstract framework strehl central aspect framework principle optimism face uncertainty. ensuring agent remains optimistic ensure agent ignore potentially lucrative opportunities. model error bounds allow provide reward bonus ensures optimism. unless stated otherwise proofs section deferred appendix. convergence rate algorithm depends convergence rate posterior distribution. here abstractly deﬁne rate allow depend state action. later provide examples function speciﬁc classes priors. note deﬁned reward bonus upper bound error mean deﬁning sample complexity respect reward bonus term bounds number samples accurate model. lemma present without proof uses result lemma provide sample complexity bound algorithm dirichlet prior. lemma sample complexity function independent dirichlet prior transition dynamics known reward function stated before mean dirichlet prior analogous estimate mbie-eb. notice also reward bonus derived similar reward bonus mbie-eb. words eﬀectively re-derived mbie-eb using bayesian methods; could replace variproofs produce similar result. thus method similar existing methods using dirichlet distribution unstructured uninformative prior. said variance term provides slightly advance algorithm prior methods lies ability take advantage structured informative priors. initial simple example advantage present sample complexity function stochastic prior unknown deterministic mdps. utilizing knowledge require full probabilistic bayesian reasoning—the eﬀects unknown actions appear stochastic. however sampling state–action pair once agent know eﬀect. state formally lemma without proof. lemma prior deterministic worlds. sample complexity function furthermore distribution independent state–action pairs sampling state–action pair cause variance associated state– action pairs also fact variance feature unobserved state–action pairs sometimes increase experience; however sample complexity result always holds. uses variance calculation guide exploration. properly demonstrate variance reward bonus deserves credit method’s success comparison methods given prior belief properly update posterior belief given prior. simplest baseline method compete mean agent reward bonus. fairly compare mbie-eb approximate bayesian method kolter test corresponding mmdp+rb agents reward bonuses respectively. boss algorithm internal-reward approach direct competitor method another sense. algorithm aware currently provides sample complexity guarantees function bayesian prior. time plans samples mdps posterior distribution. plans combined state space state available actions. essentially combined allows agent choose independently state sampled mdp’s dynamics would like follow. planning method optimistic enough samples many approximate methods bayesian planning. using standard benchmark task below able compare published results method. beetle algorithm directly approximates full bayesian planning compressing information state space. approximate bayesian planning. given unstructured prior dirichlet distribution performance variance reward bonus similar performance reward bonuses discussed above. chain environment actions action advances agent along chain action resets agent ﬁrst node. taken last node action leaves agent gives reward —otherwise results reward. action gives reward states. however probability agent slips outcomes switched. optimal behavior always chooses action environment designed require smart exploration optimal policy produces distant reward many sub-optimal policies yield immediate reward. past works consider performance agents diﬀerent priors environment. full prior agent uses independent dirichlet prior distribution state–action pair. tied prior agent knows underlying transition dynamics except value single slip probability shared state–action pairs. semi-tied prior allows diﬀerent slip probability action shared across states. tied semi prior distribution slip probability represented beta distribution. keeping published results problem table reports cumulative return ﬁrst steps averaged runs. standard error order optimal policy true scores present performance method along side comparison methods table comparison algorithm given indicated prior maintains correct posterior distribution; however diﬀers method used approximate full bayesian planning. reward-based methods parameterized coeﬃcient reward bonus. table coeﬃcient optimized separately result. predicted reward bonus methods perform similarly case full prior. although tied semi priors structured essentially make problem easy—other naive mean approach fail diﬀerentiate methods. next demonstrate advantages method task non-trivial correlated belief reasoning requirements poor early decisions lead agent’s death. hunt wumpus environment adapted russell norvig pictured figure discrete world based computer game requires intelligent exploration. world consists cave. agent always starts top-left corner. navigate turning left turning right moving forward location. lurking cave uniformly random location wumpus beast eats anyone enters location agent cannot wumpus location cardinally adjacent wumpus smell vile stench. location also prior probability containing deep trap wandering adventurer ending episode. agent next feel breeze though cannot sense direction originating agent carries arrow shoot action. executed ﬁres arrow entire length cave direction facing. hits wumpus wumpus dies agent receives reward. otherwise episode ends reward. time steps agent receives small penalty reward. summary agent senses tuple location orientation stench breeze. represent conditions terminal states. agent receives reward killing wumpus. other agent receives dying. provide agent true prior environments. evaluate agents setting chance kill wumpus—each episode independently drawn prior. words wumpus locations resampled episodes. world’s dynamics expressed function agent’s observable location change episode model episode bayesian distribution mdps. however environment unforgiving exploration benchmark; agent over-explores fall eaten. opportunity learn dying agent lives episode. order avoid likely death taking enough risks kill wumpus agent need properly utilize prior. spite fact dynamics deterministic probabilistic bayesian reasoning necessary stochastic nature prior. often case that given agent’s experience adjacent locations likely contain pits others. information gathered location increase decrease probabilities existence pits wumpus locations. times agent must take calculated risks—stepping locations nonzero probability containing pit—in order gain information needs locate wumpus. present empirical results table method present mean objective reward obtained episode averaged episodes. episode capped maximum length steps. before reported method given prior properly updates belief experience; algorithms diﬀer approximate bayesian planning given beliefs. several algorithms free parameter optimized. internal reward methods searched reward bonus scalars range increments increments variance method present only reward function known. boss method tested sample sizes variance internal reward method achieves performance follows eﬀective controlled exploration strategy. optimal however; because enjoys exploring occasionally spend time identifying location pits already located wumpus. boss agent performs poorly. mentioned above boss ensures optimism building assumption agent knows fact control agent control policy immediately turns ﬁres arrow time step chooses imagined wumpus current line sight. mean baseline agent performs better boss though policy merely slightly better heuristic. walks straight line encounters breeze point ﬁres direction unexplored locations unless experiences rare situation disambiguates wumpus’s location case ﬁres correct direction. reward bonus methods’ policies small deviation mean mdp’s policy— notice small magnitude reward coeﬃcient per-step penalty objective reward. figure illustrates eﬀect choice reward scaling parameter performance variance reward methods note choice results mean agent. seen variance reward results good performance many choices performs extremely poorly large reward values—the agent spends majority time following safe actions turning circles provide information. although full bayesian planning produces optimal behavior intractable. work contributed novel internal-reward algorithm sample complexity bound derives reward bonus bayesian prior distribution. method similar existing approaches given unstructured priors factored dirichlet distribution; however unlike previous reward bonuses approach capable exploiting structure prior. addition providing theoretical results supporting claims demonstrated method exploits structured prior knowledge hunt wumpus environment. acknowledgements work supported force oﬃce scientiﬁc research grant fa--- well grant opinions ﬁndings conclusions recommendations expressed authors necessarily reﬂect views sponsors.", "year": 2012}