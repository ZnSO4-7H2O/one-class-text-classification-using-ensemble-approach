{"title": "FRULER: Fuzzy Rule Learning through Evolution for Regression", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In regression problems, the use of TSK fuzzy systems is widely extended due to the precision of the obtained models. Moreover, the use of simple linear TSK models is a good choice in many real problems due to the easy understanding of the relationship between the output and input variables. In this paper we present FRULER, a new genetic fuzzy system for automatically learning accurate and simple linguistic TSK fuzzy rule bases for regression problems. In order to reduce the complexity of the learned models while keeping a high accuracy, the algorithm consists of three stages: instance selection, multi-granularity fuzzy discretization of the input variables, and the evolutionary learning of the rule base that uses the Elastic Net regularization to obtain the consequents of the rules. Each stage was validated using 28 real-world datasets and FRULER was compared with three state of the art enetic fuzzy systems. Experimental results show that FRULER achieves the most accurate and simple models compared even with approximative approaches.", "text": "regression problems fuzzy systems widely extended precision obtained models. moreover simple linear models good choice many real problems easy understanding relationship output input variables. paper present fruler genetic fuzzy system automatically learning accurate simple linguistic fuzzy rule bases regression problems. order reduce complexity learned models keeping high accuracy algorithm consists three stages instance selection multi-granularity fuzzy discretization input variables evolutionary learning rule base uses elastic regularization obtain consequents rules. stage validated using real-world datasets fruler compared three state genetic fuzzy systems. experimental results show fruler achieves accurate simple models compared even approximative approaches. predictive modelling aims build models values input variables predict expected output. models usually complementary requirements accuracy interpretability hand accuracy indicates ability model predict values close real ones. hand interpretability refers capability model understood human building models means fuzzy rule-based systems combines interpretability expressiveness rules ability fuzzy logic representing uncertainty. interpretability fuzzy systems involves main issues readability related simplicity fuzzy system structure i.e. number variables linguistic terms variable fuzzy rules premises rule etc. represents quantitative objective part interpretability model. important aspect fuzzy system terms interpretability deﬁnition fuzzy partition variable also called data base deﬁnition. diﬀerent approaches used deﬁne data base linguistic rules share fuzzy partition variable; approximative uses diﬀerent deﬁnition fuzzy labels rule rule base. former implies interpretability higher simplicity comprehensibility latter usually obtains accurate solutions. however approximative approaches lead complex partitions input space make diﬃcult understand input associated response. moreover linguistic partitions summation degree fulﬁllment fuzzy sets equal value inside domain recognized interpretable fuzzy partitions satisfy semantic constraints case regression problems diﬀerent alternatives fuzzy modelling used literature depending main objective pursued was. mamdani fuzzy systems antecedent consequent represented fuzzy sets primarily used obtain interpretable models. furthermore precise fuzzy modelling mainly developed using takagi-sugeno-kang fuzzy knowledge systems antecedents still represented fuzzy sets consequent weighted combination input variables. although mamdani systems well-known semantic interpretability linear model rules also good choice since straightforward understand relationship output input variables. particular interest many ﬁelds robotics medical imaging industrial estimation optimization processes widely used learning algorithms automatic building fuzzy rule bases genetic fuzzy systems i.e. combination evolutionary algorithms fuzzy logic. evolutionary algorithms appropriate learning fuzzy rules ﬂexibility —that allows codify part fuzzy rule base system— capability manage balance accuracy simplicity model eﬀective way. particular recent developments using multi-objective evolutionary fuzzy systems found mamdani systems proposed solve large-scale regression problems. moreover adaptive fuzzy inference system proposed cope high-dimensional problems. simplicity models obtained gfss regression mostly achieved literature control number rules and/or number labels used rule base multi-objective approach recently instance selection techniques received attention classiﬁcation regression problems. approach faces problems once decreases complexity largescale problems reduces overﬁtting rules generated part training data error rule base estimated another part training set. moreover expert knowledge available determine fuzzy labels diﬀerent approaches applied uniform discretization combined lateral displacements non-uniform discretization recently shown application non-uniform discretization techniques classiﬁcation problems. fuzzy rule bases implies another complexity dimension polynomial consequent usually degree widely used approach learning coeﬃcients polynomial least squares method. however choice often leads models overﬁt training data misbehave test. problem solved shrinking setting coeﬃcients zero obtaining simpler models. moreover combination regularizations called elastic used. paper present fruler algorithm obtaining accurate simple linguistic tsk- fuzzy rule base models solve regression problems. simplicity fuzzy system aims improve readability model —and therefore interpretability— obtaining linguistic fuzzy partitions labels number rules regularization consequents —which reduces number input variables contribute output. main contributions work instance selection method regression novel multi-granularity fuzzy discretization input variables order obtain non-uniform fuzzy partitions diﬀerent degrees granularity iii) evolutionary algorithm uses fast scalable method elastic regularization generate accurate simple tsk- fuzzy rules. work structured follows. section deﬁnes model used work. section describes diﬀerent stages instance selection method discretization approach evolutionary algorithm. sec. shows results approach regression problems comparison proposals statistical tests. finally conclusions presented sec. takagi sugeno kang proposed fuzzy rule model antecedents comprised linguistic variables case mamdani consequent represented polynomial function input variables. type rules called fuzzy rules. common function consequent rule linear combination input variables structure follows linguistic fuzzy term j-th input variable k-th rule t-norm conjunctive operator usually minimum function. ﬁnal output fuzzy rule base system composed fuzzy rules computed average individual rule outputs weighted matching degree linguistic fuzzy rule systems represent good trade-oﬀ accuracy interpretability linguistic terms antecedent rules provides full description input space thus even fuzzy rule systems less comprehensible natural language terms mamdani approach system provide useful understandable information preferable choice domains. article focus developing simple accurate fuzzy rule models based linguistic representation antecedents. section presents three main components fruler two-stage preprocessing —formed instance selection multi-granularity fuzzy discretization modules— genetic algorithm contains ad-hoc -order rule generation module preprocessing techniques executed improve simplicity fuzzy rule bases obtained evolutionary algorithm. hand instance selection reduces variance models focusing generated rules representative examples. hand multigranularity fuzzy discretization decreases complexity fuzzy partitions therefore necessary establish upper bound number rules evolutionary stage. evolutionary learning process obtains deﬁnition data base knowledge system. novel ad-hoc -order rule generation module calculates antecedents consequents possible rule using representative examples. finally knowledge base generated evolutionary algorithm evaluated using full training dataset. figure fruler architecture showing three separated stages. dashed lines indicate data sets dotted lines multigranularity information solid lines represent process ﬂow. instance selection method regression improvement ccisr algorithm adaptation regression instance selection method classiﬁcation ccis main diﬀerences fruler instance selection process ccisr output variable discretized order simplify generation diﬀerent graphs needed process. however discretization imply ccis process used without modiﬁcation selected instances must assure good behavior regression problems. error measure based nearest neighbor approach regression thus reducing complexity calculations compared ccisr uses ad-hoc fuzzy system evaluate instances. stopping criteria ﬂexible allowing iterations without improvement termination instance selection process based relation called class conditional nearest neighbor deﬁned pairs points labeled training follows given class ccnn associates instance nearest neighbor computed among instances class thus relation describes proximity information conditioned class label. regression problems outputs real values instead labels therefore must discretized order ccnn relation. traditionally unsupervised discretization process needs deﬁnition either number intervals shape fruler shape intervals guided output density i.e. intervals selected represent dense clusters. words split points intervals selected zones density output locally minimum. kernel density estimation gaussian kernel order estimate probability density function output variable non-parametric way. order select appropriate kernel bandwidth scott’s rule applied. probability density function obtained local minimum determines split points therefore labels/classes used ccnn relation. thus instance associated labels obtained process instance selection method follow ccis procedure. graphs used deﬁne instance scoring function means directed information-theoretic measure applied in-degree distributions graphs. scoring function named score deﬁned number examples. method proposed uses leave-one-out mean squared error order estimate information loss. thus although scoring function graphs based labels obtained error measure based original regression problem. number classes obtained error using examples choice motivated least example class error second part interpreted miss-classiﬁcation probability divided range output min. thus second part indicates least miss-classiﬁed examples must selected order correctly classiﬁed. this instance selection method iteratively selects instances adds choosing ﬁrst place highest score. process terminates examples itwi —the number consecutive iterations empirical error increases— greater threshold allows iterations without improvement beginning selection process error sensitive stops earlier number selected instances high. order improve number selected instances ccis uses thin-out post-processing algorithm selects points close decision boundary rule. achieved selecting instances positive in-degree between-class graph storing then iterative process done follows points positive in-degree added isolated previous iteration in-degree zero iterative process terminates empirical error increases deﬁnition fuzzy partition input variable critical step design fuzzy rule bases. knowledge available fuzzy labels variable automatically obtained fuzzy discretization. moreover number labels unknown multi-granularity approach used i.e. deﬁnition diﬀerent fuzzy partition regarded granularity. speciﬁcally granularity divides variable fuzzy labels i.e. generation fuzzy linguistic labels divided stages. first variable must discretized obtain split points granularity then given split points fuzzy labels deﬁned granularity. top-down approach split points searched iteratively i.e. split point added step obtaining intervals. therefore approach proposed work aims preserve interpretability contiguous granularities adding label previous granularity modifying ﬂanks adjacent labels regression problems discretization process must search split point minimizes error linear model applied resulting intervals. order select maximum number split points variable used well-known bayesian information criterion measure separated parts error applying model data complexity. case error obtained summation mean squared error least squares ﬁtted model interval discretization. hand complexity model determined number parameters case number inner splits parameters ﬁtted regression applied interval. pseudocode discretization method variable shown fig. first split points granularity initialized using domain limits measure ﬁrst granularity calculated using function gets examples learns linear regression model using least squares ﬁnally calculates mean squared error model. case number parameters corresponding coeﬃcients linear model. that iterative process executed step split points granularity deﬁned adding split point previous granularity order obtain split point granularity ﬁrst best split point interval split points previous granularity obtained best split point deﬁned point obtains global minimum function linearerror interval linearerror gets examples split point calculates total squared error calculated corresponding linear regression models side split point. split points obtain intervals size least taken account assure obtained linear regressions statistically valid. selected split point added granularity split points measure calculated number parameters used measure interval. number intervals calculated |cg| subtracted disregard split points domain variable finally number consecutive iterations without improvement value greater /min algorithm stops criterion ensures beginning discretization process —the granularity low— worsen iterations larger granularities algorithm becomes stricter stopping criterion. number data points divided order obtain maximum number intervals. obtaining discretization variable granularity method proposed applied —set split points granularity order multi-granularity fuzzy partitions. method uses fuzziness parameter indicates fuzzy linguistic labels. fuzziness indicates crisp intervals fuzziness indicates selection fuzzy smallest kernel —set points membership equal then multi-granularity fuzzy discretization process obtains fuzzy partitions input variable. finally evolutionary algorithm searches best data base conﬁguration using obtained fuzzy partifigure evolutionary learning process used fruler. dashed lines indicate data sets dotted lines multigranularity information solid lines represent process ﬂow. chromosome codiﬁcation represents parameters needed create data base rule base. individual codify single fuzzy partition input variable fuzzy partitions obtained multigranularity fuzzy discretization moreover individuals also -tuple representation labels approach applies displacement linguistic term within interval expresses movement label adjacent labels. case diﬀerent displacement going applied split points. represents granularity selected input variable granularity variable equal used antecedent part. however variable still used consequent since could relevant calculating output. represents lateral displacement split point variable lateral displacement vary interval represents half distance split point example lateral displacement seen figure fuzzy partitions always strong —the degree fulﬁllment point domain always equal therefore interpretability maintained. initial pool individuals generated combination initialization procedures. half individuals generated random granularity variable half created diﬀerent random granularity variable. lateral displacements initialized cases. that product granularities indicated greater number input variables times highest maximum granularity variables variable randomly selected removed antecedent part —its granularity previous condition satisﬁed. done order avoid complex solutions initialization stage —during evolutionary learning upper bound number rules apply. figure lateral displacement example. dashed lines indicate original fuzzy partition solid lines indicate obtained partition displacement applied. ad-hoc method used construct rule base data base codiﬁed chromosome i.e. fuzzy partitions indicated applying displacement wang mendel algorithm used create antecedent part rule base individual. method quick simple obtains representative rule base given deﬁnition data base examples. consequent part rules learned using elastic method order obtain coeﬃcients degree polynomial rule. elastic linearly combines outputs vector inputs matrix size —rows represent examples columns input variables— regularization parameter represents trade-oﬀ penalization. order elastic learning consequents coeﬃcients rule cannot calculated separately aggregation function used obtain output system therefore coeﬃcients must optimized time taking account degree fulﬁllment rule input vector thus matrix modiﬁed follows order solve minimization problem elastic stochastic gradient descent optimization technique used gradient descent method characterized updating coeﬃcient separately using example time. particularly suited sparse datasets common case constructed using pseudocode method shown figure needs three parameters solve elastic approach regularization trade-oﬀ lasso ridge initial learning rate hand usually takes value order behave like shrinkage features coeﬃcient equal hand obtained using grid search —testing possible values predeﬁned interval— using small subset examples since convergence properties maintained algorithm composed three diﬀerent loops lines represent iteration whole dataset lines iterate example iii) lines iterate coeﬃcients. note that case number coeﬃcients number columns i.e. number input variables problem times number rules first examples shuﬄed time whole dataset used. then example incremented learning rate shrinkage portion updated that coeﬃcient line applies ridge regularization lines apply lasso approach lasso approach uses thresholds order decide variable going selected updates threshold input variable next iterations finally coeﬃcient determination calculated compared best obtained far. better estimated coeﬃcients updated number iterations without improvement incremented itwi exceeds threshold deﬁned line algorithm stops. threshold directly proportional number examples decreases number iterations. examples used obtain rule base codiﬁed chromosome. manner examples representative considered rule generation. thus method avoids creation speciﬁc rules reduces time needed create rule base. etra full training dataset output obtained knowledge base input using examples evaluation seen validation process rule base constructed subset crossover operations deﬁned one-point crossover exchanging parts parts equal parent-centric used crossover part. order prevent crossover similar individuals incest prevention implemented. euclidean distance lateral displacements less particular threshold individuals crossed. mutation applies possible operations equal probability randomly selected gene part decreasing granularity increasing granularity speciﬁc granularity —all granularities chance. order calculate lateral displacements corresponding part displacements previous granularity taken account. displacement associated particular split point calculated adding displacements nearest split points previous granularity weighted distance split points. replacement individuals used local search process. stage generates parts equal less granularity —with equal probability— variable. then part generated randomly uniform distribution interval. chromosomes decoded evaluated solution obtains better ﬁtness replaces original individual. restart mechanism uses incest prevention threshold trigger. first initialized maximum length part i.e. product number input variables times largest maximum granularity variables divided implies incest prevention allows crossovers individuals distance higher quarter maximum euclidean distance. then iteration decreased diﬀerent ways order accelerate convergence finally reaches population restarted reinitialized. best individual kept local search process executed best individual order generate individuals population complete. restart criterion fulﬁlled twice algorithm stops i.e. single restart executed. moreover number evaluations reaches threshold algorithm also stopped. evolutionary algorithm stops best rule base consequents optimized applying algorithm using training examples. order analyze performance fruler used real-world regression problems keel project repository table shows characteristics datasets number instances ranging examples number input variables complex problems —large scale— number examples variables ones last rows electrical length plastic strength quake electrical maintenance frie friedman auto delail delta ailerons daily electricity energy delelv delta elevators analcat auto abalone concrete compressive strength stock prices weather ankara weather izmir forest fires mortgage treasury baseball california housing artiﬁcial domain house-h elevators computer activity pole pole telecommunications pumadyn ailerons fruler designed keep number parameters possible. instance selection technique parameters needed. multi-granularity fuzzy discretization fuzziness parameter used generation fuzzy intervals split points i.e. highest fuzziness value. evolutionary algorithm values parameters were population size maximum number evaluations pcross pmut generation fuzzy rule bases weight tradeoﬀ regularizations elastic regularization parameter obtained grid search interval obtained halving initial value result worsens. -fold cross validation used experiments. moreover trials fruler executed -fold cross validation. thus total runs obtained dataset. results shown next section mean values runs. time measures done using single thread intel xeon processor table shows average values reduction error increase data set. percentage reduction achieved general datasets. another four datasets reduction range dataset reduction reduction rate depend neither size dataset number variables complexity data. hand increase error greater eight datasets time needed execution instance selection process generally large scale problems consume minutes. maximum granularity among variables dataset. represents upper bound fuzzy partitions obtained dataset. expected smaller value simpler models obtained fruler. table summarizes results dataset. average maximum granularity cases except delail dataset. moreover maximum granularity always cases granularity even datasets high granularities maximum number fuzzy sets generate huge search space evolutionary algorithm. finally number variables without discretization cases. terms computational time discretization module almost cost expensive discretization process less seconds. e+tune multi-objective evolutionary algorithm learns mamdani fuzzy rule bases. algorithm learns granularities uniform multi-granularity fuzzy partitions lateral displacement labels. includes post-processing algorithm tuning parameters membership functions rule selection. a-metsk-hde multi-objective evolutionary algorithm learns approximative tsk- fuzzy rule bases. algorithm starts solution obtained ﬁrst stage applies tuning membership functions rule selection kalman-based calculation consequents rules. table shows average results fruler three algorithms selected comparison. diﬀerent results shown algorithm dataset number rules obtained rule base test error measured using equation test data. indicators allow compare simplicity accuracy learned models. values best accuracy —lowest error— best number rules table marked bold. seen number rules fruler lowest majority datasets. noted number rules large scale problems also despite high number e+tune mamdani proposal produces lowest number rules. examples. problems fsmogfs case accuracy problems fruler achieves best results. datasets best e+tune a-metsk-hde results results fsmogfs inﬂuence performance fruler neither training dataset size dimensionality problem. order analyze statistical signiﬁcance results stac platform used apply statistical tests. friedman test used number rules test error order ranking algorithms check whether diﬀerences statistically signiﬁcant. table shows ranking test error p-value test. proposal —generates linguistic tsk- rules— gets ranking i.e. best results accuracy among algorithms. then next algorithm ranking approximative approach tuning rules followed linguistic approaches. order compare whether diﬀerence fruler second ranked algorithm signiﬁcant wilcoxon test performed p-value indicates diﬀerence statistically signiﬁcant using signiﬁcance level thus even linguistic rules fruler obtains great accuracy compared approximative approaches getting simpler models. analyze complexity models obtained algorithm friedman test performed number rules table again fruler lowest ranking. next algorithm e+tune mamdani approach followed metsk-hde approaches diﬀerence ranking fsmogfs ranking. order assess whether diﬀerence complexity among accurate proposals signiﬁcant wilcoxon test also applied diﬀerence statistically signiﬁcant number rules. shows fruler obtains accurate solutions simpler models. table shows average time consumed fruler dataset. also display number evaluations stopping condition met. although stages fruler increases computational complexity contribute focus search simplest models. method obtains solutions range minutes datasets solutions range hours datasets moreover number evaluations limit except largest problem computational time fruler order magnitude a-metsk-hd worse datasets order demonstrate simplicity models generated fruler figure shows example rule bases generated dataset. figure shows columns rule fuzzy sets used figure example fuzzy rule base dataset. system uses variables antecedent part diﬀerent rules. sake simplicity understandability consequents represented absolute value scaled maximum weight equal test error obtained example antecedent weight variables consequent. sake simplicity understandability consequents represented absolute value scaled maximum weight equal antecedent uses variables granularity respectively thus rules needed cover combinations. hand consequent column shows importance input variable rule providing qualitative understanding model. case ﬁrst three variables greatest importance consequent. note that even though simplest models obtained fruler test error paper novel genetic fuzzy system called fruler presented. fruler learns simple linguistic tsk- knowledge bases regression problems. approach general-purpose preprocessing stages regression problems instance selection regression novel non-uniform multi-granularity fuzzy discretization. evolutionary learning algorithm incorporates automatic generation fuzzy rule bases fuzzy partitions uses elastic order obtain consequents overﬁtting. fruler compared three state algorithms learn diﬀerent types fuzzy rules linguistic mamdani linguistic tsk- approximative tsk-. results analyzed using statistical tests show fruler obtains high accuracy lower number rules linguistic data base. particular interest problems high accuracy interpretability demanded order provide qualitative understanding model users. work supported spanish ministry economy competitiveness projects tin--c- tin--c--r galician ministry education projects cn/. rodriguez-fdez supported spanish ministry education national plan", "year": 2015}