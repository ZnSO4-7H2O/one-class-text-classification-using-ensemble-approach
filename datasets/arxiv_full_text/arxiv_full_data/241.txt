{"title": "Path-SGD: Path-Normalized Optimization in Deep Neural Networks", "tag": ["cs.LG", "cs.CV", "cs.NE", "stat.ML"], "abstract": "We revisit the choice of SGD for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights. We argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest Path-SGD, which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization. Path-SGD is easy and efficient to implement and leads to empirical gains over SGD and AdaGrad.", "text": "revisit choice training deep neural networks reconsidering appropriate geometry optimize weights. argue geometry invariant rescaling weights affect output network suggest path-sgd approximate steepest descent method respect path-wise regularizer related max-norm regularization. path-sgd easy efﬁcient implement leads empirical gains adagrad. training deep networks challenging problem various heuristics optimization algorithms suggested order improve efﬁciency training however training deep architectures still considerably slow problem remained open. many current training methods rely good initialization performing stochastic gradient descent sometimes together adaptive stepsize momentum term revisiting choice gradient descent recall optimization inherently tied choice geometry measure distance norm divergence. gradient descent example tied norm steepest descent respect norm parameter space coordinate descent corresponds steepest descent respect norm exp-gradient updates tied entropic divergence. moreover least objective function convex convergence behavior tied corresponding norms potentials. example gradient descent convergence speeds depend norm optimum. norm divergence viewed regularizer updates. therefore also strong link regularization optimization regularization learning optimization provide implicit regularization terms corresponding geometry ideal optimization performance optimization geometry aligned inductive bias driving learning geometry weights appropriate geometry space deep networks? suggest geometry desirable properties would enable faster optimization perhaps also better implicit regularization? suggested above question also linked choice appropriate regularizer deep networks. focusing networks relu activations observe scaling incoming edges hidden unit scaling outgoing edges factor yields equivalent network computing function. since predictions invariant rescalings natural seek geometry corresponding optimization method similarly invariant. consider geometry inspired max-norm regularization seems provide better inductive bias compared norm achieve rescaling invariance max-norm itself rather minimum max-norm rescalings weights. discuss measure expressed path regularizer computed efﬁciently. therefore suggest novel optimization method path-sgd approximate steepest descent method respect path regularization. path-sgd rescaling-invariant demonstrate pathsgd outperforms gradient descent adagrad classiﬁcations tasks several benchmark datasets. notations feedforward neural network computes function represented directed acyclic graph input nodes output nodes vout vout weights activation function applied internal nodes denote function computed network fgwσ. paper focus relu activation function σrelu max{ refer depth network length longest directed path deﬁne deﬁned similarly paths vertices longest path length input unit output units. layered networks special properties relu activation function non-negative homogeneity. scalar σrelu σrelu. interesting property allows network rescaled without changing function computed network. deﬁne rescaling function given weights network constant node rescaling function multiplies incoming edges divides outgoing edges maps weights rescaled network easy rescaled network computes function i.e. fgwσrelu fgρcvσrelu. networks weights rescaling equivalent denoted transformed another applying sequence rescaling functions ρcv. given training goal minimize following objective weights step optimization. consider update step following form example gradient descent −η∇l) stepsize. stochastic setting mini-batch gradient descent calculate gradient small subset training set. since rescaling equivalent networks compute function desirable update rule affected rescaling. call optimization method rescaling invariant updates rescaling equivalent networks rescaling equivalent. start either rescaling equivalent weight vectors applying update steps separately remain rescaling equivalent figure evolution cross-entropy error function training feed-forward network mnist hidden layers containing hidden units. unbalanced initialization generated applying sequence rescaling functions balanced initializations updates simple case input thresholds zero stepsize gradient respect output updated network case input thresholds zero stepsize gradient respect output unfortunately gradient descent rescaling invariant. main problem gradient updates scaling weights edge also scale gradient which later exactly opposite expected rescaling invariant update. furthermore gradient descent performs poorly unbalanced networks. network balanced norm incoming weights different units roughly within small range. example figure shows huge performance initialized randomly generated balanced network training mnist compared network initialized unbalanced weights generated applying sequence random rescaling functions ˜w). unbalanced network gradient descent updates could blow smaller weights keeping larger weights almost unchanged. illustrated figure issue could scale weights update. however unbalanced network relative changes weights also different compared balanced network. example figure shows rescaling equivalent networks could computing different function single update. simple cases group-norm correspond overall regularization weight decay respectively. another form regularization shown effective relu networks max-norm regularization maximum units norm incoming edge unit max-norm correspond per-unit regularization equation written following form weight decay probably commonly used regularizer. hand per-unit regularization might seem ideal extreme sense value regularizer corresponds highest value among nodes. however situation different networks relu activations cases per-unit regularization shown effective main reason could relu networks rebalanced hidden units norm. hence per-unit regularization crude measure anymore. since rescaling invariant values scale measure different rescaling equivalent networks desirable look minimum value regularizer among rescaling equivalent networks. surprisingly feed-forward network minimum per-unit regularizer among rescaling equivalent networks efﬁciently computed single forward step. this consider vector path vector number coordinates equal total number paths input output units coordinate equal product weights along path input nodes output node. p-path regularizer deﬁned norm this deﬁnition max-norm different used context matrix factorization later similar minimum upper bound norm outgoing edges input units incoming edges output units layer feed-forward network. deﬁnition p-path regularizer involves exponential number terms. computed efﬁciently dynamic programming single forward step using following equivalent form nested sums motivated empirical performance max-norm regularization fact path-regularizer invariant rescaling interested deriving steepest descent direction respect path regularizer know path-sgd approximate steepest descent respect path-regularizer whether makes path-sgd rescaling invariant optimization method. next theorem proves path-sgd indeed rescaling invariant. theorem path-sgd rescaling invariant. proof. sufﬁcient prove using update rule ρcv) ρcv). edge network neither incoming outgoing edge node since gradient also edge however incoming edge moreover since outgoing edges divided γpe) therefore efﬁcient implementation path-sgd update rule written needs consider paths exponential depth network. however calculated time forward-backward step single data point. mini-batch setting batch size backpropagation mini-batch done time running time path-sgd mini-batch roughly moderate runtime increase typical mini-batch sizes hundreds thousands points. algorithm shows efﬁcient implementation path-sgd update rule. section compare -path-sgd commonly used optimization methods deep learning adagrad. conduct experiments four common benchmark datasets standard mnist dataset handwritten digits cifar- cifar- datasets tiny images natural scenes street view house numbers dataset containing color images house numbers collected google street view details datasets shown table experiments trained feed-forward networks hidden layers containing hidden units. used mini-batches size step-size integer choose dataset considered validation errors validation figure learning curves using different optimization methods datasets without dropout. left panel displays cross-entropy objective function; middle right panels show corresponding values training test errors values reported different epochs course optimization. best viewed color. picked reaches minimum error faster. trained network entire training set. networks trained without dropout. training dropout update step retained figure learning curves using different optimization methods datasets dropout. left panel displays cross-entropy objective function; middle right panels show corresponding values training test errors. best viewed color. unbalanced setting ﬁrst initialized weights balanced weights. picked hidden units randomly replacement. unit multiplied incoming edge divided outgoing edge chosen randomly log-normal distribution. optimization results without dropout shown figure four datasets plots objective function training error test error shown left right plot values reported different epochs optimization. although proved path-sgd updates balanced unbalanced initializations verify despite numerical issues indeed identical trained path-sgd balanced unbalanced initializations. since curves exactly show single curve. adagrad path-sgd performs essentially same. another interesting observation even balanced settings path-sgd often value objective function training test error faster also ﬁnal generalization error path-sgd sometimes considerably lower adagrad plots test errors could also imply implicit regularization steepest descent respect path-regularizer leads solution generalizes better. view similar observations role implicit regularization deep learning. results training dropout shown figure suppressed results unbalanced initializations. observe except mnist path-sgd convergences much faster adagrad. also generalizes better test shows effectiveness path-normalized updates. results suggest path-sgd outperforms adagrad different ways. first achieve accuracy much faster second implicit regularization path-sgd leads local minima generalize better even training error zero. better analyzed looking plots number epochs provided figures also point path-sgd easily combined adagrad take advantage adaptive stepsize used together momentum term. could potentially perform even better compare path-sgd revisited choice euclidean geometry weights relu networks suggested alternative optimization method approximately corresponding different geometry showed using alternative geometry beneﬁcial. work show proof-of-concept success expect path-sgd beneﬁcial also large-scale training deep convolutional networks. combining path-sgd adagrad momentum optimization heuristics might enhance results. although believe path-sgd good optimization method easy plug-in hope work also inspire others consider geometries regularizers perhaps better update rules. particular property path-sgd rescaling invariance argue appropriate relu networks. path-sgd certainly rescaling invariant update possible invariant geometries might even better. finally choose steepest descent simplicity implementation. better choice might mirror descent respect appropriate potential function construction seems particularly challenging considering non-convexity neural networks.", "year": 2015}