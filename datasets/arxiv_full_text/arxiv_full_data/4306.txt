{"title": "Sparsity Based Poisson Denoising with Dictionary Learning", "tag": ["cs.CV", "stat.ML"], "abstract": "The problem of Poisson denoising appears in various imaging applications, such as low-light photography, medical imaging and microscopy. In cases of high SNR, several transformations exist so as to convert the Poisson noise into an additive i.i.d. Gaussian noise, for which many effective algorithms are available. However, in a low SNR regime, these transformations are significantly less accurate, and a strategy that relies directly on the true noise statistics is required. A recent work by Salmon et al. took this route, proposing a patch-based exponential image representation model based on GMM (Gaussian mixture model), leading to state-of-the-art results. In this paper, we propose to harness sparse-representation modeling to the image patches, adopting the same exponential idea. Our scheme uses a greedy pursuit with boot-strapping based stopping condition and dictionary learning within the denoising process. The reconstruction performance of the proposed scheme is competitive with leading methods in high SNR, and achieving state-of-the-art results in cases of low SNR.", "text": "problem approximations fact hold true measured pixels high intensity i.e. high photon count measured detectors. thumb rule transformations accurate peak value larger case noise looks similar gaussian one. peak value smaller structure noisy image quite different many zero pixels others small values. example peak equals almost binary image containing mainly either zeros ones. shows noisy versions peppers different peak values. seen indeed peak value increases noise looks like gaussian. work denoising poisson noisy images peak≤ anscombe fitz transformations less effective. anscombe transform non-linear element-wise transabstract—the problem poisson denoising appears various imaging applications low-light photography medical imaging microscopy. cases high several transformations exist convert poisson noise additive i.i.d. gaussian noise many effective algorithms available. however regime transformations signiﬁcantly less accurate strategy relies directly true noise statistics required. recent work salmon took route proposing patch-based exponential image representation model based leading state-of-the-art results. paper propose harness sparse-representation modeling image patches adopting exponential idea. scheme uses greedy pursuit boot-strapping based stopping condition dictionary learning within denoising process. reconstruction performance proposed scheme competitive leading methods high achieving state-of-the-art results cases snr. poisson noise appears many applications night vision computed tomography ﬂuorescence microscopy astrophysics spectral imaging. given poisson noisy kronecker delta function i-th component respectively. notice poisson noise additive strength dependent image intensity. lower intensity image yields stronger noise many schemes recovering exist popular strategy relies transformations anscombe fisz convert poisson denoising problem gaussian plenty methods exist noise becomes approximately white gaussian unit variance. mention light poisson denoising applications dictionary already known. example fluorescence microscopy measured image might sparse itself. hence cases introduction recovery method important. introduce novel stopping criteria iterative algorithm incorporation pursuit leads much improved results. best knowledge boot strapping based stopping criterion ﬁrst appears paper. interplay dictionary learning based models signiﬁcance seen treatment simpler gaussian image denoising problem. migration dictionary learning poses series difﬁculties paper describes solves. note paper utilize learning strategy organization paper follows. section describes poisson denoising problem details presents previous contributions. section introduces proposed denoising algorithm starting pursuit task moving clustering employ achieving non-locality denoising process discussing role learning dictionary concluding overall scheme. section presents various tests comparisons demonstrate denoising performance superiority proposed scheme. section discusses future work directions conclusions. image patches represented sparsely given dictionary rd×n assumption patch represented k-sparse i.e. non-zero entries image dimension treat overlapping patches image leads state-of-the-art results gaussian denoising order sparsity-inspired model poisson noise case options convert poisson noise gaussian done adapt gaussian denoising tools poisson statistics. explained above later important cases anscombe non-effective approach indeed practiced maximizing log-likelihood poisson distribution provides following minimization problem recovering i-th patch corresponding poisson noisy patch therefore applying denoising technique stablized data results estimate fanscombe rather thus need apply inverse transform order estimate note using algebraic inverse results biased estimator non-linearity transform. problem addressed providing exact unbiased inverse anscombe transform eventually leads better recovery performance. however said above even exact inverse recovery error dramatically increases peak< order deal deﬁciency strategy relies directly poisson statistics required. direction taken providing gaussian mixture model based approach relies directly poisson noise properties. dividing image overlapping patches dividing large clusters performing projection onto largest components pca-like basis group state-of-the-art results reported small peak values. approach versions. ﬁrst non-local projection computed minimizing poissonian bregman divergence using newton steps second non-local sparse spiral method adds regularization term minimized objective used resulting better recovery performance. work take similar path image patches modeling using recovery process. however take different route propose sparse representation modeling dictionary learning based denoising strategy instead gmm. employ greedy omp-like method sparse coding patches smart boot-strapped stopping criterion. demonstrate superiority proposed scheme various experiments. presented preliminary version proposed omp-like technique achieves relatively poor recovery performance local conference paper algorithmic experimental parts remarkably improved. main contributions paper introduce greedy technique poisson sparse model. pursuit methods gaussian noise commonly used extensive work devoted past decades construction analysis. thus proposal greedy strategy poisson model importance open door many variants theoretical study similar exists gaussian regime. example greedy method extended poisson deconvolution problem done greedy techniques serve basis poisson inpainting vector composed ones operation applied element-wise. note minimization problem allows zero entries corresponding entries zeros well. minimizer noisy patch itself thus using enough prior needed. using standard sparsity prior patches practiced elsewhere following minimization problem semi-norm counts number non-zeros vector. besides fact combinatorial problem also imposes non-negativity constraint recovered patch complicates numerical task hand. order resolve latter issue follow applied element-wise still k-sparse vector. leads following minimization problem non-negativity constraint removed still need approximation algorithm solving likely np-hard. option relaxation leads spiral method another simpler option reduce dictionary columns thus minimized standard optimization toolbox convex optimization. approach taken nlpca technique patches clustered small number disjoint groups group narrow dictionary. denoting qjnj} αjnj} rd×k group number noisy patches representations patches dictionary minimization problem nlpca aims solving j-th group nlpca poisson denoising based ideas related model developed gaussian denoising case course method used poisson noise applying anscombe transform. however approach shown inferior poisson model based strategy low-photon count case. like work anscombe rely poisson based model. however opposed global dictionary patches propose greedy algorithm ﬁnding representation patch. approach similar advocated treat similar patches jointly forcing share atoms representations. introduces non-local force sharing information different regions image. dictionary learning process utilized scheme well order improve initial dictionary used data initial dictionary embark global images. moving next section mention technique proposed enhance noisy images. method effective especially scenarios peak smaller instead denoising given image directly downsample image applying lowpass ﬁlter followed down-sampling. provides smaller image higher snr. example low-pass ﬁlter kernel size containing ones sample every third every third column nine times smaller image nine times larger peak value. low-res noisy image apply poisson denoising algorithm perform upscaling interpolation recovered small image order return original dimensions. method referred binning related multi-scale programs note technique especially effective anscombe based techniques peak value processed image larger initial value. since minimizing respect dictionary representations time hard problem minimize function alternately. pursuit performed using greedy technique returns k-sparse representation patch unlike notice calculated also part minimization process group dictionary optimized. typical sparsity number clusters used nlpca respectively. hard minimize respect dictionary representations alternating minimization process used applying newton steps updating dictionary representations alternately. recovered patches return corresponding location recovered image average pixels mapped place authors suggest repeat whole process using output algorithm input clustering process applying algorithm division. improved version nlpca nlspca proposed replacing newton step algorithm poisson greedy algorithm input rd×n{q q˜l} poisson distributed vector mean variance approximated maximal cardinality representations assumed support. optional parameter estimates true image patches p˜l}. begin algorithm -initialize support grouping algorithm described details algorithm creates disjoint groups size sequential adding elements one. group gets destination size algorithm continues elements whose distance ﬁrst element group away distance last added element reason selected strategy clustering ability divide patches groups similar size. reason target similar sizes want guarantee enough atoms group selecting correct support group sparse coding step hand need many groups possible since otherwise enough information updating dictionary select support patches group. could selected group atom separately would result overlapping groups chosen computational reasons. group size qgng}. begin algorithm -convolve image kernel take size -extract overlapping patches size corresponding patches ˜qn} size -set ﬁrst group pivot index argmin≤i≤n k˜qik -initialize iprev global newton step spiral guaranteed sparse output. learning dictionary technique updates dictionary together representations supports kept ﬁxed order boost performance algorithm exploit fact similar patches image domain support sparse representation patches clustered large number small disjoint groups similar patches. turn describe details step algorithm. ideally poisson noisy images almost binary images good criterion measuring similarity patches would earth mover’s distance approximate measure setting distance patches euclidean distance image passes gaussian ﬁlter. clear euclidean distance option best. nevertheless reason selected distance gives bigger weight entries noisy pixels large values. usually rare reﬂect locations high intensity original image. expecting patches high intensity similar original image concentrations high photons counts locations. element gradually iteration. given support previous iteration next added atom support atoms dictionary calculating following minimization problem index potential atom addition {qi} patches decode {αi} representations size t−∪{j} restricted support restricted entries supported vector notice patches cluster enforced sparsity pattern perform minimization patches together select atom all. notice problem ﬁxed given support. support ﬁxed convex problem appears also nlpca technique solved newton method convex optimization toolbox. bootstrapping based stopping criterion used provided. serve oracle patches {pi} estimate patches true image therefore atoms representation till distance patches starts increasing. case output sparse coding decoded representation previous iteration. argue used poissonian bregman divergence calculating error error. reason criterion standard measure checking quality image reconstruction. note distance measure denoised image noisy image. ﬁrst time apply sparse coding algorithm recovery process cannot bootstrapping based stopping criterion estimate original image yet. therefore cardinality groups. subsequent iterations patches recovered image previous stage used input algorithm bootstrapping stopping criterion. also exactly reason chosen current clustering method off-the-shelf methods greedy nature seems make faster. note well aware fact choice grouping method suboptimal results section indicate sufﬁciently good-performing. calculating representations joint sparsity assumption group patches greedy algorithm ﬁnds representations group together. algorithm iterative iteration adds atom reduces cost function representations belong group. explain matter discuss equation important aspect pursuit algorithm decide many atoms associate patch i.e. stopping criterion algorithm. employ options. ﬁrst algorithm constant number iterations. however choice leads suboptimal denoising effect different patches contain different contentcomplexity thus require different sparsity levels. bootstrapping based stopping criterion another option different cardinality group. order need evaluate error iteration respect true image stop iterations error starts increasing. option estimating error using poission unbiased risk estimator variant stein’s unbiased risk estimator poisson noise done nl-means algorithm. however context computation pure demanding requires re-applying denoising method image changing pixel time. done efﬁciently simple structure denoising method used there. case cannot done. thus boot-strapping rely fact scheme iterative step representation decoding dictionary update estimate original image. patches reconstructed image previous iteration proxy patches true image compute error respect them. might think patch previous iteration used determine number iterations patch current iteration make change stuck outcome. however averaging process applied middle together dictionary update step expected back patch rather different denoising result. note since update dictionary iterations average patches recovered image stuck patch using stopping criteria. practice condition improves recovery performance signiﬁcantly group patches represented cardinality suites content better. algorithm sparse poisson denoising algorithm input rd×n poisson noise data mean variance patches approximated cardinality used representations ﬁrst sparse coding step group sizes kernel ﬁlter used clustering algorithm number advanced dictionary learning rounds number inner learning iterations ﬁrst round number rest rounds. begin algorithm -extract overlapping patches -apply patch grouping algorithm -for group apply poisson greedy algorithm -put ﬁrst estimate re-projection recovered patches averaging. -set -put estimate re-projection recovered patches averaging. -extract overlapping patches -for group apply poisson greedy algorithm bootstrapping stopping criterion relies patches extracted given decoded representations proceed update dictionary. could used simple classical learning mechanism update dictionary atoms done using newton step similar instead utilize advanced learning strategy keeping supports representations ﬁxed update dictionary representations without changing support. second notice similarity concatenation different small dictionaries view generalization groups share atoms. sense look advanced learning strategy generalization gmm. fact small groups apply sparse coding advanced learning step gives freedom versatility scheme. connection solve using technique solving applying alternating newton steps dictionary representations. since inner iterations reinforce relation dictionary atoms related representation number iterations depends conﬁdence selected supports. select different number iterations ﬁrst learning round rest supports ﬁrst round outcome sparse coding ﬁxed cardinality size. initial dictionary selection initial dictionary dictionary trained off-line patches clean piecewise constant image shown fig. training process initial dictionary needed since unlike standard sparsity model many good dictionaries present exponential sparsity model dictionary intuitively known. notice also representation model sensitive scale image unlike standard scale invariant. fact given constant necessarily thus train different initialization dictionaries different peak values. training done applying denoising algorithm clean image scaled required peak value. mention training done off-line. iterating pursuit dictionary learning stages estimate image patches. recovering whole image reproject patch corresponding location averaging. point possible re-cluster patches according recovered image repeat process. proposed sparse poisson denoising algorithm summarized fig. algorithm main difference proposed algorithm nlpca nlspca reminiscent difference k-svd image denoising algorithm alternative approach developed gaussian denoising problem. furthermore comes iteration adding element representation sparse coding passes atoms dictionary solves checking potential error them. calculating error constant number newton steps. before complexity minimizing since start square dictionary target non-zero patch elements worst-case complexity therefore overall complexity algorithm performance repeat denoising experiment performed test recovery error various images different peak values ranging tested images appear fig. methods compare nlspca exact unbiased inverse anscombe best-performing methods date. code techniques available clustering patches nlpca nlspca small number large clusters obtained using k-means like algorithm. scheme notion joint sparsity used implies large number disjoint small groups divided using simple algorithm. nlpca nlspca different basis vectors used cluster dictionary patches together subspaces created different choices small groups atoms. unlike nlpca nlspca dictionary change size learning steps cardinalities allocated different patches dynamic. last point mention nlpca uses newton step nlspca uses spiral algorithm representation decoding propose greedy pursuit task guarantees destination cardinality reconstruction error. compare algorithm’s complexity nlpca. interested order computational cost focus bottlenecks algorithm. nlpca major computational part solving done applying constant number newton steps patches local dictionaries. complexity newton steps order hessian inversion. using special properties hessian possible perform inversion complexity nlspca complexity nlpca dependent peak value. peak≤ dictionary trained squares image peak <peak≤ train peak= peak> train peak= note since poisson noise mean equal original signal rough estimation peak easy task. note algorithm sensitive initial dictionary selection peak value inaccurately estimated wrong dictionary selected recovery affected signiﬁcantly. indeed could trained different initial dictionary peak value. however chose order demonstrate insensitivity scheme initialization. remark possible reasonable initializations update process absolute values transform. experience much difference reconstruction result recovery error range .db. parameter selection summarized table comparison recovery image peak= presented observed image recovered accurately methods many artifacts. samples dictionary atoms learned spda presented fig. observed dictionary learning process captures shape stars lines ﬂag. figures present recovery ridges saturn house peak= peak= respectively. seen peak value binning methods capture structure image better provide lower error. however peak higher binning provides degraded performance compared reconstruction using original image. images spda recovers images’ details better. recovery error terms psnr images different peak values appears table looking overall performance proposed strategy provides better performance average tested peakvalues. note even case behave better difference insigniﬁcant online parameter settings appears code. binning follow ones kernel increases peak value times higher bilinear interpolation upscaling low-resolution recovered image. spda following parameter setting size patch pixels. start sparse coding ﬁxed sparsity apply rounds sparse coding bootstrapping based stopping criterion together advanced dictionary learning mechanism contains joint update dictionary representations ﬁrst round apply inner iterations dictionary update rest inner iterations. reason different number inner iterations ﬁrst round round supports representations less reliable selected ﬁxed support size. ﬁrst round bootstrapping based stopping criterion employed group decoded different cardinality leading better support selection. re-cluster using outcome process repeat again. remind reader selecting clusters size trade-off number groups size cluster. group selects support groups provides information dictionary update process. hand larger cluster probable select right support. course could used overlapping groups method nlspca nlspcabin bmdbin spda spdabin nlspca nlspcabin bmdbin spda spdabin nlspca nlspcabin bmdbin spda spdabin nlspca nlspcabin bmdbin spda spdabin nlspca nlspcabin bmdbin spda spdabin nlspca nlspcabin bmdbin spda spdabin peak values spdabin behaves better larger ones spda preferred. addition binning algorithms improves performance signiﬁcantly lower peak values. peak raises effectiveness binning reduces. note efﬁciency reduces slower bmd. might explained fact relies anscombe becomes much effective peak increases. remark algorithm beneﬁts structured images methods. advantage enabling exploitation self-similarities images important real life applications used extensively literature many images likely patterns repeating again especially comes small patches. note peak intensities equal nlspca shows better psnr results spda situation changes binning used. reason peak values counts working patch-size used higher peaks leads weaker results. lead dictionary learning process learn isolated noise points dictionary elements happens spda fig. thus believe larger patch sizes could better results peak values. however using large patch sizes feasible computationally. using binning compensates computational barrier working resolution version image patch size. regular size nl-pca overcomes resolution problem fact uses large cluster sizes. case cannot large cluster sizes need many clusters possible dictionary update step. conclusion spda seems good denoising quality poisson noisy images achieving state-ofthe-art recovery performance. interesting explore contribution stage algorithm quality recovered image. therefore evaluate performance spda different setups applying spda sparse coding sparse coding followed sparse coding bootstrapping based stopping criterion; applying spda simple dictionary learning steps without joint representation dictionary update stage. binning performed peak≤ learning iterations. otherwise dictionary learning iterations ﬁrst stage re-cluster apply additional iterations; applying spda advanced dictionary learning steps reclustering; using setup table advanced dictionary learning steps followed another steps reclustering. setup calculate different peak values average psnr eight images fig. result presented table iii. first note recovery result simple sparse coding advanced dictionary learning reclustering average signiﬁcant. looking contribution stage algorithm observe effect bootstrapping based stopping criterion negligible case binning improves recovery result case nobinning. believe reason number atoms used recovery determines resolution recovered patches. binning coarser resolution image processed therefore reconstruction result less sensitive number patches used recovery no-binning patches ﬁner resolution used therefore number atoms used decoding critical recovery performance. thus noticeable main improvement recovery dictionary learning. using dictionary learning steps improvement average. re-clustering additional dictionary learning steps improvement another .db. though difference using spda advanced dictionary learning steps simple dictionary learning steps signiﬁcant average advantages peaks usage binning preferable improvement average spdabin peaks better binning spda. convergence algorithm faster advance learning steps advanced learning steps versus simple learning steps. since bottleneck method sparse coding step learning step followed using spda advanced dictionary learning steps runs four times faster simple steps finally compare algorithm gaussian version anscombe. version aims minimizing standard error objective standard sparsity constraints. dictionary learning steps algorithm advanced update step adopt exactly setup spda. present average denoising error table different peak values. note algorithm perform well used binning. reason binning scenario less patches updating dictionary. therefore beneﬁt learning process binning. interestingly case work poisson objective. shows advantage working directly poisson data. without binning performance gaussian version beneﬁts dictionary update larger number patches learning. however case well better spda. note spda outperform gaussian version peak values except peak= better spda binning anyway. work proposes scheme poisson denoising sparse poisson denoising algorithm relies poisson statistical model presented uses dictionary learning strategy sparse coding algorithm employs boot-strapping based stopping criterion. recovery performance spda state-of-the-art scenarios outperform existing algorithms however still room improvement current scheme leave future work harmany marcia willett this spiral-tap sparse poisson intensity reconstruction algorithms theory practice ieee trans. image processing vol. march giryes elad sparsity based poisson denoising ieee convention electrical electronics engineers israel burger harmeling improving denoising algorithms multi-scale meta-procedure pattern recognition ser. lecture notes computer science mester felsberg eds. springer berlin heidelberg vol. engan aase hakon husoy method optimal directions frame design ieee international conference acoustics speech signal processing vol. engan skretting husy family iterative ls-based dictionary learning algorithms ils-dla sparse signal representation digital signal processing vol. applications type images observed known beforehand e.g. space images ﬂuorescence microscopy cell images. off-line training dictionary dedicated speciﬁc task improve current results. setting suitable number dictionary learning iterations important quality reconstruction. automated tuning technique considered purpose setting number images gives mid-level results basically almost images lost output quality ﬁxed stopping point. mention applications issue tuning done manually user based qualitative results visual feedback. giryes thanks azrieli foundation azrieli fellowship. research supported european community’s fperc program grant agreement addition authors thank reviewers manuscript suggestions greatly improved paper. gil-rodrigo portilla miraut suarez-mesa efﬁcient joint poisson-gauss restoration using multi-frame l-relaxed-l analysisbased sparsity ieee international conference image processing sept. boulanger kervrann bouthemy elbau j.-b. sibarita salamero patch-based nonlocal functional denoising ﬂuorescence microscopy image sequences ieee trans. med. imag. vol. feb. sapiro mallat solving inverse problems piecewise linear estimators gaussian mixture models structured sparsity ieee trans. image processing vol.", "year": 2013}