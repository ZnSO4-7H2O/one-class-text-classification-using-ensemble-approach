{"title": "Automatic Tuning of Interactive Perception Applications", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Interactive applications incorporating high-data rate sensing and computer vision are becoming possible due to novel runtime systems and the use of parallel computation resources. To allow interactive use, such applications require careful tuning of multiple application parameters to meet required fidelity and latency bounds. This is a nontrivial task, often requiring expert knowledge, which becomes intractable as resources and application load characteristics change. This paper describes a method for automatic performance tuning that learns application characteristics and effects of tunable parameters online, and constructs models that are used to maximize fidelity for a given latency constraint. The paper shows that accurate latency models can be learned online, knowledge of application structure can be used to reduce the complexity of the learning task, and operating points can be found that achieve 90% of the optimal fidelity by exploring the parameter space only 3% of the time.", "text": "interactive applications incorporating high-data rate sensing computer vision becoming possible novel runtime systems parallel computation resources. allow interactive applications require careful tuning multiple application parameters meet required ﬁdelity latency bounds. nontrivial task often requiring expert knowledge becomes intractable resources application load characteristics change. paper describes method automatic performance tuning learns application characteristics effects tunable parameters online constructs models used maximize ﬁdelity given latency constraint. paper shows accurate latency models learned online knowledge application structure used reduce complexity learning task operating points found achieve optimal ﬁdelity exploring parameter space time. digital acquisition video become commonplace availability low-cost digital cameras recording hardware. recently applications making video data largely limited recording compression streaming playback human consumption. computer applications directly make video streams example medium sensing environment detecting activities mode input human users becoming areas active research development. particular class interactive perception applications uses video high-data rate sensing interactive gaming natural gesture-based interfaces visually controlled robotic actuation becoming increasingly important. great challenge applications dealing tremendous computational costs involved even rudimentary video analytics computer vision algorithms latencies needed effective interaction often limited current processors fast enough deal applications. broad approaches used achieve required speeds. ﬁrst attempts parallelize execution applications clusters machines transforming applications pipeline data graph connected processing stages effectiveness approach depends greatly extent particular steps parallelized. second approach trades result quality ﬁdelity computation cost algorithms used interactive perception applications typically many parameters signiﬁcant effect ﬁdelity latency. example feature extraction stage tunable threshold detection maximum iterations parameter even switch select alternative algorithms. success approach depends tuning parameters available application. dynamically adjustable degree parallelism algorithmic parameters provide opportunity control latency. given application computing resources parameter settings yield best combination latency ﬁdelity? application performance depends potentially large number parameters nonlinear effects complex interactions. addition also depends content input data environmental conditions furthermore dynamic parameter adjustments require time take effect long settling times. information needed model application performance unlikely known priori difﬁcult reason analytically static. performance. first demonstrate predict latency workloads streams incoming data using state-of-the-art techniques online convex programming investigate structured unstructured approaches problem compare accuracy computational costs. second build system learns predict latency workloads also optimizes ﬁdelity. problem formulated online constrained optimization problem unknown latency constraints. ε-greedy strategies explore space latency constraints show appropriate mixture exploration exploitation leads practical system solving problem. ﬁrst application online learning constraints structured real-world problem whose cost function unknown. consider parallel interactive perception applications structured data graphs. vertices graph coarse-grained sequential processing steps called stages edges connectors reﬂect data dependencies stages. stages interact connectors share state otherwise. source stages provide input data application example stream video camera. data ﬂows transformed multiple processing stages which example implement computer vision algorithm detect user performs particular gesture. finally data model concurrency explicit stages within application execute parallel constrained data dependencies available processors. application-independent runtime system distribute execute applications parallel compute cluster. system provides mechanisms export dynamically tunable parameters including algorithmic parameters controls degree parallelism system also monitors application performance provides interfaces extracting latency data stage level. study applications paper. ﬁrst implementation algorithm object instance recognition pose registration used robotics shown data figure image ﬁrst passes proportional down-scaler. sift features extracted image matched previously constructed models objects interest. features object clustered position separate distinct instances. random sample consensus algorithm non-linear optimization used recognize instance estimate pose. figure shows poses recognized objects. implementation includes tuning parameters degree image scaling threshold number features produced degree data parallelism used feature extraction model matching clustering. details parameters presented table listed default values parameters maximize application ﬁdelity without regard latency. reduce application latency parameters maximize application ﬁdelity. application latency reduced invoking instances features extraction face detection stages. also scale frame size used branch decrease latency. however using higher degree scaling reducing quality face detection degrade application ﬁdelity. application latency order needed achieve responsive user interface. note applications processing time vision algorithms primary contributers latency dominate sources network transfer overheads. application ﬁdelity varies function parameter settings. however different parameters affect ﬁdelity different ways. example degree parallelism data parallel operation generally affect ﬁdelity data items independent. contrast parameter changes content input data reducing resolution image signiﬁcantly affect ﬁdelity. fidelity often difﬁcult quantify especially relates vague notions perceptual quality. even speciﬁc measures like accuracy detection algorithm true effect cannot quantiﬁed without ground truth particular input. absence model based ground truth ﬁdelity approximated relative best parameter settings usually known. since stages encompass arbitrary code practical assume application performance characteristics effects tuning parameters performance known priori particularly varying hardware platforms. practical expect application programmers supply even subset information. space tuning parameters large settings non-linear effects interactions. finally application performance data-dependent reason change time. therefore application stages must treated black performance models learned online. higher degree data parallelism three stages. extracting fewer features scaling image also accelerate application. however leads degradation application ﬁdelity. application intended visual servoing robot requires tight end-to-end latencies; goal latency bound. second application provides interface control television gestures camera positioned near television observes viewer shown figure video frame sent separate tasks face detection motion extraction shown figure latter accumulates frame pairs extracts siftlike features encode optical addition appearance. features ﬁltered positions detected faces aggregated window frames using previously-generated codebook create histogram occurrence frequencies. histogram treated input vector support vector machines trained control gestures. application exports tuning parameters degree scaling branch quality face detection degree data parallelism feature extraction face detection. describe details parameters table above default valdegree image scaling left branch degree image scaling right branch quality face detection degree data parallelism feature extraction degree data parallelism face detection tency predicted using stage models computing critical path data graph. solver used search operating points maximize ﬁdelity given latency constraint. changes parameter settings applied running application. problem formulation application deﬁned tuple data graph space dynamically tunable parameters latency bound. speciﬁcally directed graph consisting computation stages connectors {eij| stage requires data stage nodes graph weighted computation latency. weight node latency single execution stage latency application length ii∈c simplicity omit inter-stage communication latency formulation incorporated adding edge weights represent communication costs. values tunable parameters time data gathered time corresponding ﬁdelity latency respectively. problem maximizing ﬁdelity subject latency constraints formulated attempt directly learn model end-to-end application latency. however potentially large space tuning parameters need fast efﬁcient learning important reduce size learning task. application structure given data graph provides natural partition problem. single stage affected subset tuning parameters. example tuning parameters tend inﬂuence localized sections application graph affect stages upstream parallel branches. addition stages contribute little total latency vary little modeled simply end-to-end latency obtained combining predictions stage models according critical path data graph. example figure end-to-end latency latencies source copy classify sink stages maximum latencies face detection motion extraction subgraphs computed similarly. total complexity learning individual stage models parameters signiﬁcantly lower learning single model based parameters. reduce automatic tuning problem follows. ﬁrst observations stage latencies identify critical stages based contribution end-toend latency. dependency analysis performed identify parameters affect critical stage. specifically parameter associated critical stage correlation value parameter stage latency exceeds threshold then additional periodic observations explore parameter space learn predictor function relevant tuning parameters critical stage. non-critical stages modeled moving average. end-to-end laonline learning regressor formulated online convex programming problem. online convex programming involves convex feasible sequence convex functions time step decision maker chooses action based past functions actions ft−. goal minimize regret online learning non-linear regressors done ways. first directly solve kernelized version problem main problem approach maintain compact representation kernel matrix. second option expand original feature space non-linear features learn linear regressor space. technique suitable quadratic cubic kernels instance. experimental section cubic kernels adopt latter approach. unfortunately cubic expansion feature space costly. make approach applicable larger problems take advantage critical path data graph decomposes based structure graph instance note cost regressor problem figure rewritten correspond left right subgraphs deﬁned subspaces solve problem structured manner learn regressor subspaces combine deterministic function parallel stages sequential structures. rest paper assume reward function known focus learning cost function learning step alternated solving equation using ε-greedy policy result practical solution problem. choose discussed section simplicity assume cost function change time. represented tabular form learn ε-close approximation probability polynomial time unfortunately learning representation practical even possible. instance discrete exponential note problems figures tunable parameters parameters continuous would suitable task. hand note errors related though lipschitz factors particular smooth minimization average error leads minimizing max-norm error. trends shown section number objects scene indicates whether object recognized translation error rotation error. weights respectively. motion sift f-measure classiﬁcation performance ﬁrst experiment study quality latency predictors depends complexity. predictors learned online using linear quadratic cubic kernels. particular time step randomly sample action update predictors described section figure shows errors predictors tend decrease time. increase pose detection dataset frame corresponds change scene notebook appeared. increased number sift features scene consequently computational requirements process single frame. general cubic predictors yield smallest errors predictors almost good ofﬂine counterparts. addition note max-norm errors latency predictors also decrease time. observation consistent expectation minimization average error leads minimizing max-norm error cost function regressor smooth figure shows expected errors unstructured structured latency predictors almost identical. main advantage using structured predictors cubic feature space expansion smaller. instance figure average rewards costs action conﬁgurations action space convex hull represents payoffs feasible playing randomized strategy action conﬁguration. section evaluate approach three ways. first compare accuracy latency predictors learned online learned ofﬂine. second study effect using application structure comparing accuracy single predictor application predictors learned individual stages combined described section finally examine tradeoffs exploration exploitation determine high-ﬁdelity operating points given latency bound. conducted experiments using system applications described section application used input video sequence annotated ground truth. pose detection application video consists series objects different positions orientations ground truth object label measured translation rotation frame. gesture-based control application referred henceforth motion sift low-level feature uses represent motion video consists single user performing control gestures ground truth label gesture occurring frame any. greater experimental control repeatability results experiments done execution traces. collected cluster servers connected gbps ethernet switch. server intel xeon processors memory runs ubuntu linux application created conﬁgurations selecting random valid values tunable parameters figures static conﬁgurations sequence frames collected performance logs runtime extracted latency measures frame. conﬁgurations point-based approximation total space traces predeﬁned alternative fufigure comparison linear quadratic cubic latency predictors. predictors learned online compared cumulative average expected max-norm errors frame. errors corresponding ofﬂine predictors shown dashed lines. motion sift dataset takes features describe structured unstructured spaces respectively. thus updating structured predictor twice fast practice. problems involved hundreds variables speedup would likely much signiﬁcant. finally note max-norm errors structured latency predictors signiﬁcantly smaller errors unstructured predictors. explaining results expected max-norm errors coupled tightly subspaces problems. turn minimization expected error subspaces results smaller total max-norm error. last experiment latency predictors build control policy maximizes ﬁdelity subject latency constraints. resulting controller simply ε-greedy policy deﬁnes amount exploration. optimal exploration rate vary measure corresponding average ﬁdelity constraint violation sults compared payoff randomized strategies action space intuitively want achieve close-to-zero constraint violation maximize ﬁdelity operating point. figure illustrates performance policies various exploration rates latency bounds varies policies usually follow u-shaped curve. particular small estimate latency function uncertain results policies signiﬁcantly violate latency bound hand close figure comparison unstructured structured latency predictors. predictors learned online compared cumulative average expected max-norm errors frame. ideal exploration rate guarantees explore enough learn cost function exploit enough optimize main objective. reasonable choices yields setting regret sublinear proporincreases polynomially thus performance system improve time. diamonds figure mark operating points -greedy policies. experiments policies yield high rewards constraint violations. particular note rewards always within percent optimum. moreover average constraint violation experiments second never exceeds second. measured relatively latency bound average worst-case constraint violations percent respectively. machine learning used several contexts predict tune interactive parallel applications. domain mobile devices narayanan satyanarayanan modeled latency battery consumption interactive mobile applications function tunable ﬁdelity settings. initially random sampling used ofﬂine train linear predictors user-supplied terms followed online reﬁnement model coefﬁcients. contrast approach automatically learns performance ﬁdelity relationships online. figure average rewards constraint violations polices various exploration rates policies compared respect payoff randomized strategies action space operating points scientiﬁc computing workﬂows nimo system employs active learning estimate completion times applications particular datasets heterogeneous processing storage resources. employs multivariate linear regression design experiments approach exploring tradeoffs interactions attributes since experiment take hours days complete. system tunable parameters changed dynamically execution facilitating rapid online exploration although must balanced need avoid large perturbations system. also domain artiﬁcial neural networks used construct performance models partitioned parallel worksets shown effective polynomial regression approaches modeling scientiﬁc computing applications less domain-speciﬁc knowledge required model formulation. active learning used reduce ann-based model error selecting samples generated highest model ensemble finally reinforcement learning used select representation format sparse matrices based characteristics data minimize execution time ﬁxed multiplications applications model server performance black-box function parameters employ smart hill climbing algorithm weighted latin hypercube sampling high performance conﬁgurations ofﬂine. method requires advance planning careful tracking prior samples. reinforcement learning used online tuning conﬁguration paramcontrast these work focuses tuning interactive parallel applications focus ﬁdelity latency. online learning algorithm strong theoretical guarantees learn latency function take advantage application structure reduce complexity learning task. paper shown practical application online learning estimate performance dynamically adjust tunable parameters real-world interactive perception applications. approach outlined readily model latency characteristics complex application. uses application structure reduce learning task complexity improve expected max-norm prediction error. finally trades exploration exploitation high-ﬁdelity operating points given latency bound. work enables critical capability emerging class interactive perception applications ability automatically adapt particulars distributed parallel computing resources dynamically changing workload characteristics. work seen general template practical application machine learning techniques important real-world problems. among areas future work plan incorporate models network latency study exploration strategies take account cost changing parameter settings expected improvement ﬁdelity.", "year": 2012}