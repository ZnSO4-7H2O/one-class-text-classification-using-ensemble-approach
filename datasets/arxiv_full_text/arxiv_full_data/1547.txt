{"title": "Navigational Instruction Generation as Inverse Reinforcement Learning  with Neural Machine Translation", "tag": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "abstract": "Modern robotics applications that involve human-robot interaction require robots to be able to communicate with humans seamlessly and effectively. Natural language provides a flexible and efficient medium through which robots can exchange information with their human partners. Significant advancements have been made in developing robots capable of interpreting free-form instructions, but less attention has been devoted to endowing robots with the ability to generate natural language. We propose a navigational guide model that enables robots to generate natural language instructions that allow humans to navigate a priori unknown environments. We first decide which information to share with the user according to their preferences, using a policy trained from human demonstrations via inverse reinforcement learning. We then \"translate\" this information into a natural language instruction using a neural sequence-to-sequence model that learns to generate free-form instructions from natural language corpora. We evaluate our method on a benchmark route instruction dataset and achieve a BLEU score of 72.18% when compared to human-generated reference instructions. We additionally conduct navigation experiments with human participants that demonstrate that our method generates instructions that people follow as accurately and easily as those produced by humans.", "text": "paper speciﬁcally consider surrogate problem synthesizing natural language route instructions describe method generates free-form directions people accurately efﬁciently follow environments unknown priori speciﬁc problem previously considered robotics community important human-robot collaborative tasks search-and-rescue exploration surveillance robotic assistants serve guides museums ofﬁces public spaces. generally problem relevant beyond human-robot interaction broader domain indoor navigation unavailable existing solutions rely upon abstract—modern robotics applications involve humanrobot interaction require robots able communicate humans seamlessly effectively. natural language provides ﬂexible efﬁcient medium robots exchange information human partners. signiﬁcant advancements made developing robots capable interpreting free-form instructions less attention devoted endowing robots ability generate natural language. propose navigational guide model enables robots generate natural language instructions allow humans navigate priori unknown environments. ﬁrst decide information share user according preferences using policy trained human demonstrations inverse reinforcement learning. translate information natural language instruction using neural sequence-tosequence model learns generate free-form instructions natural language corpora. evaluate method benchmark route instruction dataset achieve bleu score compared human-generated reference instructions. additionally conduct navigation experiments human participants demonstrate method generates instructions people follow accurately easily produced humans. robots increasingly used partners working alongside people whether serving assistants homes transporting cargo warehouses helping students language learning classroom acting guides public spaces order humans robots work together effectively robots must able communicate human partners order establish shared understanding collaborative task coordinate efforts natural language provides efﬁcient ﬂexible medium humans robots exchange information. consider example search-and-rescue operation carried human-robot team. human ﬁrst issue spoken commands direct robots navigate throughout building searching occupants process robot engage user dialogue resolve ambiguity task user’s ability trust robotic partners also integral effective collaboration robot’s ability generate natural language explanations template-based instructions referring distances street names suitable. primary challenges generating effective natural language route instructions characteristic general problem free-form generation. problem deciding much information convey user part directions. general detailed instruction less ambiguous however verbose instructions unnatural hard followers remember thus ineffective. consequently important balance value including particular information part route instruction cost comes increasing level detail. further information equally informative. existing commercial navigational solutions typically rely hand-crafted rules consider street names metric distances valid candidates latter requires follower’s keep track progress. contrast studies shown people prefer route instructions reference physical salient landmarks environment however standard exists regards landmarks selected depend nature environment demographics follower propose method models content selection problem markov decision process learned policy decides much include formal language speciﬁcation task learn policy inverse reinforcement learning demonstrations route instructions provided humans. avoids need hand-crafted selection rules allows method automatically adapt preferences communication style target populations simultaneously choose convey information minimizes ambiguity instruction avoiding verbosity. second challenge surface realization task synthesizing natural language sentence refers content selected ﬁrst step. existing solutions rely sentence templates generating sentences populating manually deﬁned ﬁelds direction) serializing sentences turn-by-turn fashion. expected templates reduces coherence across sentences limits ability adapt different domains additionally output technically correct resulting sentences tend rigid unnatural. studies show language generated robot effective emulates communication style people address surface realization problem neural sequence-to-sequence model translates formal language speciﬁcation selected command natural language sentence. model takes form encoderaligner-decoder architecture ﬁrst encodes formal path speciﬁcation recurrent neural network using long shortterm memory model decodes resulting abstraction input natural language sentence using alignment mechanism reﬁne selected information associate output words corresponding elements input formal speciﬁcation. lstms hidden units enables model capture long-term dependencies exist among selected information among words resulting instruction. train surface realization model instruction corpora enables method generate natural language directions emulate style human instructions without need templates specialized features linguistic resources. evaluate method benchmark sail dataset human-generated route instructions instructions generated method achieve sentence-level bleu score indicating similarity reference human-provided instructions. perform series ablations visualizations better understand contributions primary components model. additionally conduct human evaluation experiments demonstrate learning-based method generates instructions people able follow efﬁciently accurately generated humans. qualitative assessment reveals participants rate quality information conveyed instructions ease interpretable participants’ conﬁdence following instructions equivalent even better human-generated directions. existing research related generation route instructions spans ﬁelds robotics natural language processing cognitive science psychology. early work area focuses understanding humans generate natural language route instructions properties make good instructions easier people follow studies shown people prefer give directions sequence turn-by-turn instructions favor physical objects locations intuitive landmarks based studies much existing research generating route instructions involves handcrafted rules designed emulate manner people compose navigation instructions look compose route instructions using templates application rules engineered based upon corpus humangenerated route instructions. look improves upon work incorporating human cognitive spatial models generate high-level route overviews augment turn-by-turn directions. similarly dale analyze dataset route instructions composed people derive handdesigned rules mimic content style human directions. goeddel olson describe particle ﬁlterbased method employs generative model direction following produce templated instructions maximize likelihood reaching desired destination. challenge instruction generation systems rely upon hand-crafted rules difﬁcult design policy generalizes wide variety scenarios followers whose preferences vary depending factors cultural background gender cuay´ahuitl seek improve upon using reinforcement learning hand-crafted reward functions model length instructions likelihood confuse follower. learn policy reasons best route corresponding navigational instructions. however approach still requires domain experts deﬁne reward functions specify model parameters. contrast oswald model problem deciding include instruction markov decision process learn policy human-written navigation corpus using maximum entropy inverse reinforcement learning. given content identiﬁed policy framework perform surface realization instead generates instructions matching selected content nearest match database human-generated instructions. method also uses inverse reinforcement learning content selection unlike system method also learns perform surface realization directly corpora thus generating newly-composed natural language instructions. relatedly much attention paid recently inverse problem learning follow natural language route instructions. statistical methods primarily formulate problem converting instructions actions either semantic parsing task symbol grounding problem alternatively learn translate natural language instructions action sequences end-to-end fashion using encoderaligner-decoder architecture. meanwhile selective generation considers general problem converting rich database natural language utterance existing methods generally focusing individual problems content selection surface realization. barzilay perform content selection collections unannotated documents sake text summarization. barzilay lapata formulate content selection collective classiﬁcation problem simultaneously optimizing local label assignments pairwise relations. liang consider related problem aligning elements database textual description clauses. propose generative semi-markov model simultaneously segments text utterances aligns utterance corresponding entry database. meanwhile walker perform surface realization sentence planners trained generate sentences dialogue context planning. wong mooney effectively invert semantic parser generate natural language sentences formal meaning representations using synchronous context-free grammars. rather consider individual sub-problems recent work focuses solving selective generation single framework angeli model content selection surface realization local decision problems log-linear models employ templates generation. formulate selective generation end-toend learning problem propose recurrent neural network encoder-aligner-decoder model jointly learns perform content selection surface realization database-text pairs. consider problem generating natural language instructions allow humans navigate environments unknown priori. broader class language generation problems task requires deciding information convey user correct overly verbose unambiguous. task requires conveying information language sentence syntactically correct semantics consistent intended message natural free-form. formally given environment desired path task produce natural language instruction guides user along path. takes form hybrid metric-topologic-semantic representation encodes position connectivity dense locations environment position type objects environment features path sequences poses corresponds minimum distance route given initial pose desired goal pose. split path according changes direction representing path sequence intermediate segments training data comes form tuples drawn human demonstrations environment human-generated natural language route instruction path different human took following instructions. test time consider path pair known hold human-generated instruction evaluation. dataset training validation testing comes benchmark sail corpus given path framework performs content selection decide information share human follower subsequently performs surface realization generate natural language instruction according selected content. method learns perform content selection surface realization human demonstrations produce instructions similar generated humans. order bridge low-level nature input paths natural language output encode paths using intermediate logic-based formal language. speciﬁcally compound action speciﬁcation representation provides formal abstraction navigation commands hybrid metric-topologic-semantic maps ours. language consists actions associated number attributes together deﬁne speciﬁc commands distinguish structures instructions attributes left empty thereby deﬁning class instructions commands correspond instantiated instructions attributes particular values english instruction dataset generate corresponding command using marco architecture .for complete description language macmahon many ways compose speciﬁcation desired path terms type information conveyed well speciﬁc references humans exhibit common preferences terms type information shared speciﬁc nature information depends upon environment followers’ demographics goal learn preferences dataset instructions generated humans. inverse reinforcement learning similar fashion oswald formulate content selection problem markov decision process goal identifying information selection policy maximizes long-term cumulative reward consistent human preferences however reward function unknown priori generally difﬁcult deﬁne. assume humans optimize common reward function composing instructions employ inverse reinforcement learning learn policy mimics preferences humans exhibit based upon human demonstrations. deﬁned tuple states actions reward received executing action state transitioning state probability transitioning state state executing action discount factor. policy corresponds distribution actions given current state. case route instruction domain state deﬁnes user’s pose path context environment. represent state terms context features express characteristics changes orientation position relative location objects nearby environment features encode state -dimensional binary vector indicates context features active state. state space spanned possible instantiations context features. meanwhile action space corresponds space different structures used deﬁne path. seek policy maximizes expected cumulative reward. however reward function deﬁnes value particular characteristics instruction unknown difﬁcult deﬁne. reason frame task inverse reinforcement learning problem using humanprovided route instructions demonstrations optimal policy. speciﬁcally learn policy using maximum entropy formulation models user actions distribution paths parameterized log-linear model e−θξ feature vector deﬁned actions. consider instruction features include features expressing number landmarks included instruction frame reference used complexity command. feature vector takes form -dimensional binary vector. appendix presents full context property features used parameterize state action respectively. maximum entropy solves distribution following optimization policy deﬁnes distribution structure compositions terms feature encoding. perform inference policy identify maximum posteriori property vector maxξ invert feature mapping match vector database structures formed training set. rather choosing nearest match result inconsistent structure retrieve nearest neighbors database using weighted distance terms mutual information expresses importance different features based upon context. several valid employ spectral clustering using similarity strings identify candidate sequence-to-sequence model formulate problem generating natural language route instructions inference probabilistic model sequence words instruction sequence tokens command. sequence includes token action tokens form attribute.value couple example turn represented sequence generating instruction sequence corresponds inference model encoder hidden state token nonlinear functions deﬁne later. aligner computes context vector encodes language instruction time decodes context vector arrive desired likelihood encoder encoder takes input sequence tokens command transform token ke−dimensional binary vector using word embedding representation feed sequence encoder employs lstms recurrent unit result ability learn long-term dependencies among instruction sequences without prone vanishing exploding gradients. lstm-rnn encoder summarizes relationship elements command yields sequence hidden states encodes words including practice reverse input sequence feeding structures sentence planning given candidate structures method next chooses attributes values ﬁnal commands valid ambiguous. compute likelihood command valid instruction path deﬁned number attributes deﬁned number attributes deﬁned also valid respect inputs candidate structure generate multiple commands iterating possible attributes values. evaluate correctness ambiguity conﬁguration according equation command deemed valid likelihood greater threshold since number possible conﬁgurations structure increases exponentially respect number attributes assign attributes using greedy search. iteration algorithm constrained objects properties environment visible follower. result valid commands. identiﬁed commands suitable given path method proceeds generate corresponding natural language route instruction. formulate problem translating instruction speciﬁcation formal language natural language equivalent. perform translation using encoder-aligner-decoder model enables framework generate natural language instructions learning examples humangenerated instructions without need specialized features resources templates. training train encoder-aligner-decoder model predict natural language instruction given input sequence using training human-generated reference instructions. negative log-likelihood reference instructions time step loss function. inference given command represented sequence tokens generate route instruction sequence maximum posteriori words learned model beam search perform approximate inference empirically found greedy search often perform better. reason generate candidates using greedy beam search. language model inference procedure results multiple candidate instructions given segment additional candidates exist multiple speciﬁcations. rank candidate instructions using language model trained large amounts english data. formulate lstm-rnn assigns perplexity score corresponding instructions. train evaluate system using publicly available sail route instruction dataset collected macmahon original data without correcting typos wrong instructions dataset consists demonstrations arranged paragraphs produced instructors different paths throughout virtual environments demonstration provides map-path-command tuple partitioned dataset separate training validation test sets. command-instruction pairs training validation test sets respectively training hyper-parameter tuning testing encoder-aligner-decoder model. path-command pairs training pairs validation tune hyper-parameters content selection model. finally path-instruction pairs test evaluate performance afﬁne transformation logistic sigmoid restricts input input output forget gates lstm respectively memory cell activation vector. memory cell summarizes lstm’s previous memory current input modulated forget input gates respectively. aligner encoded input command sequence hidden annotations decoder seeks generate natural language instruction sequence words. employ alignment mechanism permits model match focus particular elements sequence salient current word output instruction. compute context vector alignment term expresses degree element position around match output time term represents decoder hidden state previous time step. alignment modeled one-layer neural perceptron decoder model employs lstm decoder takes input context vector decoder hidden state previous time step outputs conditional probability distribution next token deep output layer data augmentation sail dataset signiﬁcantly smaller typically used train neural sequenceto-sequence models. order overcome scarcity augmented original dataset using particular command-instruction pair original dataset generate number demonstrations iterating possible values attribute command updating relative instruction accordingly. example given original pair turn left) augment dataset pairs namely turn right) turn back). augmented dataset consists demonstrations training validation respectively. implemented tested proposed model using following values system parameters encoder-alignerdecoder consisted layers encoder decoder lstm units layer. language model similarly included -layer recurrent neural network lstm units layer. size natural language vocabularies respectively based upon sail dataset. parameters chosen based performance validation set. train model using adam optimization. test time perform approximate inference using beam width two. method requires average generate instructions path consisting movements laptop ram. neural models performance would improve signiﬁcantly using gpu. best knowledge ﬁrst sail dataset purposes generating route instructions. consequently evaluate method comparing generated instructions reference human-generated commands sail dataset using bleu score purpose command-instruction pair validation ﬁrst feed command model obtain generated instruction secondly respectively reference hypothesis computing -gram bleu score. consider average bleu scores individual sentence level well full-corpus level quality instructions effort evaluate accuracy usability method conducted human evaluation experiments asked novice participants amazon mechanical turk follow natural language route instructions randomly chosen equal-sized sets instructions generated method humans distinct paths various lengths. paths corresponding human-generated instructions randomly sampled sail test set. given route instruction human participants asked navigate best ability using keyboard within ﬁrst-person three-dimensional virtual world representative three environments sail corpus. fig. provides example participants’ ﬁeld view following route instructions. attempting follow instruction participant given survey composed eight questions three requesting demographic information requesting feedback experience quality instructions followed. collected data total experiments system randomly assigned experiments discourage participants learning environments becoming familiar style particular instructor. participants experienced scenario human annotated machine generated instructions. appendix provides details regarding experimental procedure. evaluate performance architecture scoring generated instructions using -gram bleu score commonly used automatic evaluation mechanism machine translation. comparing human-generated instructions method achieves sentencecorpus-level bleu scores respectively validation set. test method achieves sentencecorpus level bleu scores respectively. fig. model employs aligner order learn focus particular tokens salient words output instruction. evaluate contribution aligner implementing training alternative model last encoder hidden state decoder. table compares performance models original validation set. inclusion aligner results slight increase bleu score generated instructions relative human-provided references also useful means visualizing inner workings model additionally empirically aligner improves model’s ability learn association elements words output thereby yielding better instructions. method employs language model rank instructions generated different candidate commands across different settings beam width. practice language model trained large amounts english data helps remove grammatically incorrect sentences produced sequence-to-sequence model trained smaller pairwise dataset. table presents instruction candidates generated encoder-aligner-decoder model different commands. language model successfully assigns high perplexity scores incorrect instructions chosen instruction grammatically correct. generated route instruction different scenarios drawn sail validation set. visualizations demonstrate method learns align elements formal command corresponding words generated instruction. example network learns association honeycomb textured ﬂoor color bench refers sofa objects phrase have indicates veriﬁcation action evaluate accuracy human participants followed natural language instructions terms manhattan distance desired destination participant’s location s/he ﬁnished scenario. figure compares accuracy participants’ paths following human-generated instructions corresponding instructions method produced. report fraction times participants ﬁnished within different distances goal. results demonstrate participants reached desired position often following instructions generated using method compared human instruction baseline. didn’t reach destination participants reached location within vertex away often given instructions. meanwhile method yields failure rate lower. note scenarios participants reached destination total time required interpret follow method’s instructions less humangenerated instructions though difference statistically signiﬁcant. figure presents participants’ responses survey questions query experience following instructions. using learn content selection policy constructing structures method generates instructions convey enough information follow command rated providing little information less frequently human-generated baseline meanwhile participants felt instructions easier follow human-generated baselines participants conﬁdent ability follow method’s instructions felt backtrack less often meanwhile types instructions confused equally often machinegenerated however participants less sure generated instructions relative human baseline. figure compares paths participants took following instructions took given reference human-generated directions. case left none participants reached correct destination with back wall turn left. walk along ﬂowers hatrack. turn left. walk along brick alleys past lamp. turn left. move along wooden ﬂoor chair. next block hatrack fig. examples paths sail corpus participants followed according instructions generated humans method. paths traversed according human-generated instructions paths green executed according instructions. circles denote start goal locations respectively. following human-generated instruction. participant reached location three participants stopped location participant went wrong direction outset. contrast participants reached goal directly following instruction. scenario depicted right participants failed reach destination provided human-generated instruction. participants went directly location participants navigated location participant went location backtracking taking right location attribute failures ambiguity human-generated instruction references walled areas could correspond hallways portion hand participants followed intended path reached goal following instruction generated using method. note second-best candidate framework considered mentions olive hallway unique reference person take left. presented model natural language generation context providing indoor route instructions exploits structured approach produce unambiguous easy remember grammatically correct human-like route instructions. currently model generates natural language route instructions shortest path goal. nevertheless situations longer path afford instructions straightforward increase likelihood reaching destination another interesting direction future work would involve integration model instruction followers architecture effort learn generate instructions easier follow. approach would permit training model reinforcement learning setting directly optimizing task performance. description change orientation change position change orientation position change position orientation ﬁnal place contains object pass object walking ﬁnal place dead-end ﬁnal pose goal pose ﬁrst action take ﬁnal pose faces ﬂoor color object visible ﬁnal pose object turn location ﬁnal pose faces wall color place turn dead-end contexts features paths instruction properties features structures. demonstration path represented single binary vector elements instruction represented integer-valued vector elements. lists contexts instruction properties model shown table respectively. description number information remember low-level command groundtruth command maximum depth number deﬁned attributes number ﬂoor colors mentioned number wall colors mentioned whether head towards object number landmarks mentioned turn reference frame evaluated accuracy quality generated instructions experiments human participants asked navigate three-dimensional virtual environment according route instruction provided. participants recruited using amazon mechanical turk crowd-sourcing platform. recruiting message said objective experiment understand people follow route instructions. offered completed scenario. fifty-four people participated completed total experiments. omitted experiments participant answered disclose question native english speaker? since included requirement participating. procedure resulted total participants total experiments. omit experiments based participants’ performance answers gave questions regarding demographic information. paid participants contribution regardless whether native english speakers. prior taking part participant spent least seconds navigating within held-out environment order familiarize interface. experimented lasted average seconds. route instructions randomly sampled generated using method provided humans part sail corpus. following outlines procedure participant followed participant presented survey consisting eight questions three requesting demographic information requesting feedback experience quality instructions followed. andrist spannan mutlu. rhetorical robots making robots effective speakers using linguistic cues proc. acm/ieee int’l. conf. human-robot expertise. interaction pages tokyo japan march simple domainproc. independent probabilistic approach generation. conf. empirical methods natural language processing pages barzilay lapata. collective content selection proc. human language techconcept-to-text generation. nology conf. conf. empirical methods natural language processing pages chen mooney. learning interpret natural proc. language navigation instructions observations. nat’l conf. artiﬁcial intelligence pages francisco august chung propp walter howard. performance hierarchical distributed correspondence graphs efﬁcient symbol grounding robot instructions. proc. ieee/rsj int’l conf. intelligent robots systems hamburg germany october correa walter fletcher glass teller davis. multimodal interaction autonomous forklift. proc. acm/ieee int’l. conf. human-robot interaction pages osaka japan march cuay´ahuitl dethlefs frommberger k.-f. richter bateman. generating adaptive route instructions using proc. int’l conf. hierarchical reinforcement learning. spatial cognition pages curry gkatzia rieser. generating evaluating landmark-based navigation instructions virtual environments. proc. europ. workshop natural language generation pages brighton september graves abdel-rahman hinton. speech recognition deep recurrent neural networks. proc. ieee int’l conf. acoustics speech signal processing pages hayaski sakamoto kanda shiomi koizumi ishiguru ogasawara hagita. humanoid robots passive-social medium ﬁeld experiment train station. proc. acm/ieee int’l. conf. human-robot interaction pages arlington march information-theoretic dialog improve spatial-semantic representations. proc. ieee/rsj int’l conf. intelligent robots systems hamburg germany october interactive robots social partners peer tutors children ﬁeld trial. human-computer interaction june mooney. generative alignment semantic parsing learning ambiguous supervision. proc. int’l conf. computational linguistics pages konstas lapata. unsupervised concept-to-text generation hypergraphs. proc. conf. north american chapter assoc. computational linguistics pages landsiedel nijs khnlenz wollherr buss. route description interpretation automatically proc. ieee int’l conf. robotics labeled robot maps. automation look kottahachchi laddaga shrobe. location representation generating descriptive walking directions. proc. int’l conf. intelligent user interfaces pages lovelace hegarty montello. elements good route directions familiar unfamiliar environments. int’l conf. spatial information theory pages macmahon stankiewicz kuipers. walk matuszek koscher. following directions proc. acm/ieee using statistical machine translation. int’l. conf. human-robot interaction pages osaka japan march bansal walter. talk how? selective generation using lstms coarse-to-ﬁne proc. conf. north american chapter alignment. assoc. computational linguistics raman lignos finucane marcus kress-gazit. sorry dave afraid can’t that explaining unachievable robot tasks using natural language. proc. robotics science systems berlin germany june ./rss..ix.. clair matari´c. robot verbal feedback improve team performance human-robot task collaborations. proc. acm/ieee int’l. conf. human-robot interaction pages portland march striegnitz denis gargett garouﬁ koller theune. report second challenge generating instructions virtual environments proc. europ. workshop natural language generation pages september tellex kollar dickerson walter banerjee teller roy. understanding natural language commands robotic navigation mobile manipulation. proc. nat’l conf. artiﬁcial intelligence pages francisco august tellex thaker deits simeonov kollar roy. toward information theoretic human-robot dialog. proc. robotics science systems sydney australia july torrey fussell kiesler. robot give advice. proc. acm/ieee int’l. conf. human-robot interaction pages tokyo japan march walker rambow rogati. spot trainable north american sentence planner. chapter assoc. computational linguistics pittsburgh june walters dautenhahn woods koay. robotic etiquette results user studies involving fetch proc. acm/ieee int’l. conf. humancarry task. robot interaction pages arlington march wang pynadath hill. trust calibration within human-robot team comparing automatically generated explanations. proc. acm/ieee int’l. conf. human-robot interaction christchurch zealand march ward newcombe overton. turn left church three miles north study direction giving differences. environment behavior wong mooney. generation inverting semantic parser uses statistical machine translation. proc. conf. north american chapter assoc. computational linguistics human language technologies pages", "year": 2016}