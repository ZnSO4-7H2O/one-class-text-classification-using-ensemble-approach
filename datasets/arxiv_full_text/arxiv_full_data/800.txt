{"title": "Learning by Stimulation Avoidance: A Principle to Control Spiking Neural  Networks Dynamics", "tag": ["cs.NE", "cs.AI", "cs.LG"], "abstract": "Learning based on networks of real neurons, and by extension biologically inspired models of neural networks, has yet to find general learning rules leading to widespread applications. In this paper, we argue for the existence of a principle allowing to steer the dynamics of a biologically inspired neural network. Using carefully timed external stimulation, the network can be driven towards a desired dynamical state. We term this principle \"Learning by Stimulation Avoidance\" (LSA). We demonstrate through simulation that the minimal sufficient conditions leading to LSA in artificial networks are also sufficient to reproduce learning results similar to those obtained in biological neurons by Shahaf and Marom [1]. We examine the mechanism's basic dynamics in a reduced network, and demonstrate how it scales up to a network of 100 neurons. We show that LSA has a higher explanatory power than existing hypotheses about the response of biological neural networks to external simulation, and can be used as a learning rule for an embodied application: learning of wall avoidance by a simulated robot. The surge in popularity of artificial neural networks is mostly directed to disembodied models of neurons with biologically irrelevant dynamics: to the authors' knowledge, this is the first work demonstrating sensory-motor learning with random spiking networks through pure Hebbian learning.", "text": "designed research produced analysed data wrote paper. ‡designed research contributed analytic tools revised paper. ¶contributed simulation ideas. ¤current address university tokyo graduate school arts sciences department general systems studies tokyo japan lanasacral.c.u-tokyo.ac.jp learning based networks real neurons extension biologically inspired models neural networks find general learning rules leading widespread applications. paper argue existence principle allowing steer dynamics biologically inspired neural network. using carefully timed external stimulation network driven towards desired dynamical state. term principle learning stimulation avoidance demonstrate simulation minimal sufficient conditions leading artificial networks also sufficient reproduce learning results similar obtained biological neurons shahaf marom examine mechanism’s basic dynamics reduced network demonstrate scales network neurons. show higher explanatory power existing hypotheses response biological neural networks external simulation used learning rule embodied application learning wall avoidance simulated robot. surge popularity artificial neural networks mostly directed disembodied models neurons biologically irrelevant dynamics authors’ knowledge first work demonstrating sensory-motor learning random spiking networks pure hebbian learning. networks spiking neurons currently model closely reproduces real neuron’s dynamics theoretical approaches shown spiking models tremendous potential learning processing information efforts still ongoing potential mainstream applications. finding general learning rules governing dynamics spiking networks closer goal. paper propose biologically plausible rule explain influences dynamics network demonstrate practice obtain desired dynamics initially random networks finally show used obtained desired behaviour simulated robot. constrained simulated models also offers interpretation unexplained experimental results obtained vitro networks. papers published shahaf marom conduct experiments training method drives rats’ cortical neurons cultivated vitro learn given tasks show stimulating network focal current removing stimulation desired behaviour executed sufficient strengthen said behaviour. training behaviour obtained reliably quickly response stimulation. specifically networks learn increase firing rate group neurons inside time window response external electric stimulation applied part network result powerful first generality network initially random input output zones’ size position chosen experimenter well output’s time window desired output pattern. second attractive feature experiment simplicity training method. obtain learning network shahaf marom repeat following steps apply focal electrical stimulation network. desired behavior appears remove stimulation. first desired output seldom appears required time window several training cycles output reliably obtained. marom explains results invoking stimulus regulation principle level neural network postulates stimulation drives network different topologies modifying neuronal connections removing stimulus simply freezes network last configuration explicitly postulates strengthening neural connections occurs result stimulus removal. generality results obtained shahaf marom suggests form learning must crucial basic property biological neural networks. entirely explain experimental results. several training cycles necessary stability guarantees configuration network preserved stopping stimulation? modifiability conflict idea learning cannot prevent good topology modified stimulation training cycle? propose different explanatory mechanism principle learning stimulation avoidance emergent property spiking networks coupled hebbian rule external stimulation. states networks reliably learn exhibit firing patterns lead removal external stimulation avoid exhibiting firing patterns lead application external stimulation opposition postulate stimulus intensity major drive changes network rather timing stimulation relative network activity crucial. relies entirely time dependent strengthening weakening neural connections. addition proposes explanatory mechanism synaptic pruning covered srp. shahaf postulates might work real brains. indeed found take place brain fundamental rule emerges spike-timing dependant plasticity found vivo vitro networks. stdp hebbian learning rule fundamental consistently found brains wide range species insects humans stdp causes changes synaptic weight firing neurons depending timing activity presynaptic neuron fires within postsynaptic neuron synaptic weight increases; presynaptic neuron fires within aftter postsynaptic neuron synaptic weight decreases. although stdp occurs neuronal level direct consequences sensory-motor coupling animals environment. vitro vivo experiments based stdp reliably enhance sensory coupling decrease bidirectional changes even combined create receptive fields sensory neurons studies reaction cortical neurons stimuli gradually enhanced made fire within stimulation inhibited made fire within start stimulation. paper show conditions required emergence positive negative prepost-synaptic neurons illustrated fig. therefore although stdp rule operates scale neuron expected emerge network level real brains well emerges artificial networks. network level requires additional condition burst suppression. paper tested mechanisms. simply adding white noise neurons short term plasticity rule figure using external stimulation obtain neurons synaptic changes mediated stdp. stopping stimulation output neuron fires leads necessary stdp condition increasing weight input last fired output. interesting note without firing latency effect stdp would possible. starting stimulation output fires leads necessary condition decreasing weight output last fired input. structure paper follows model presented section show conditions necessary obtain sufficient reproduce biological results section study dynamics minimal network neurons section show supports hypotheses explain biological mechanisms covered implement simple embodied application using sole learning mechanism section section explore effect parameter changes learning performance network. model spiking neuron devised izhikevich simulate excitatory neurons inhibitory neurons simulation time step equations neural model resulting dynamics shown fig. experiments section section simulate fully connected networks neurons excitatory inhibitory neurons. ratio inhibitory neurons standard simulations close real biological values initial weights maximum weight fixed delays signal transmission neurons. neurons receive three kinds input gaussian noise standard deviation representing noisy thalamic input. external stimulation value frequency timing stimulation depends experiment. stimulation neurons neuron spikes weight added input neuron inputs added neuron iteration figure equations dynamics regular spiking fast spiking neurons simulated izhikevich model. equations dynamics regular spiking fast spiking neurons simulated izhikevich model. spiking figures reproduced permission www.izhikevich.com. experiments reduced network simulate neurons excitatory. initial weights fixed except noted otherwise. experiments section burst suppression obtained -neuron network reducing number connections obtain sparsely connected network neuron random connections neurons high maximum weight high external input high noise synaptic plasticity networks form stdp proposed stdp applied excitatory neurons; connections keep initial weight simulation. additive stdp fig. shows variation weight synapse going neuron neuron shown figure negative fires first positive fires first. total weight varies maximum value weight wmax reset wmax. experiments -neurons networks also apply decay function weights network. decay function applied iteration experiments section burst suppression obtained adding phenomenological model short term plasticity network suppress bursts keeping network fully connected. reversible plasticity rule decreases intensity neuronal spikes close time preventing network enter state global synchronized activity front direction robot. sensors range activated robot less wall direction supported sensor’s orientation. input zones network receive input sensors follows sensitivity sensors fixed constant value duration experiment. simplicity robot’s steering non-differential. controlled spikes output zones network spike left output zone robots steers radian showed simulated random spiking network built combined stdp could driven learn desired output patterns using training method similar shahaf shahaf shows training protocol reduce response time network. response time defined delay application stimulation observation desired output network. first series experiments desired output defined fulfilment condition condition electrical activity must increase chosen output zone experiment reproduced demonstrating learning behaviour direct effect stdp captured principle firing patterns leading removal external stimulation strengthened firing patterns lead application external stimulation avoided. section show methods sufficient obtain results similar second series experiments performed shahaf desired output simultaneous fulfilment condition defined condition different output zone must exhibit enhanced electrical activity. conditions fulfilled result called selective learning output zone must learn increase activity inside time window output zone must change activity. reproduce experiment follows. network neurons group excitatory neurons stimulated frequency different groups neurons monitored define desired output pattern neurons firing output zone less neurons firing output zone conditions must fulfilled simultaneously simultaneously means millisecond. stop external stimulation soon desired output observed. desired output observed stimulation stimulation also stopped. random delay stimulation starts again. addition neuron stimulated noisy input weights subject slow decay indicated model section. paper provides theoretical explanation shahaf’s vitro experiment simulating spiking neurons. important differences stimulation frequency intensity time window output also fully connected network biological network grown vitro likely sparsely connected figure evolution reaction times successful neural networks selective learning task. networks simulated successfully learned task lines represent training cycles ended without network producing expected correct output. learning curve clearly visible. task learned reaction time network reaches value inferior keeps limit. success rate percentage networks successfully learned task less final reaction time calculated successful networks learning. standard error indicated. despite differences obtain results comparable shahaf reaction time initially random becomes shorter training also perform experiment stimulation find success rate statistics selective learning experiment summarized table shown results network exhibits selective learning defined shahaf. also find despite success rate exhibiting desired firing pattern firing rates output zone output zone increase equivalent proportions output zones fire rate desynchronized although data firing rates specifically discussed paper shahaf reports experiment half in-vitro networks succeeded selective learning succeeded simple learning task. hypothesis bursts detrimental learning explain difficulty obtaining selective learning. hypothesis true burst suppression improve learning. discussing burst suppression next section quickly survey dynamics minimal network neurons dynamics affect learning global bursts. discuss -neurons network global bursting suppression. showed minimal network neurons consistently follows principle lsa; also showed single neuron able prune synapse enhance another synapse simultaneously depending stimulation received presynaptic neurons. experiment examine figure dynamics weight changes induced small networks neurons. positive spiking stops stimulation direct weight grows faster weights even starting lower value. artificially fixing direct weight longer pathway connections established. negative spiking starts external stimulation result pruned. weights dynamics chain excitatory neurons connected other simplification happening fully connected network neuron used input output separated hidden neuron. neurons labeled shows results experiments different learning conditions different initial states. results summarised follows positive direct connections input output privileged indirect connections. connections updated time step therefore fastest path always cause neuron fire longer path completely activated. hand direct connection exists weights longer paths correctly increased. negative prunes weights direct connections input output sufficient stop stimulation output neuron. neurons strongly stimulated default behaviour individual weights increase except submitted influence negative lsa. neurons fire constantly bias neurons fire them mechanically increasing output weights. raises concerns stability larger fully connected networks; weights could simply increase maximum value. introducing inhibitory neurons network improve network stability experiments -neuron networks inhibitory neurons fixed input figure raster plots regular network network parameters tuned reduce bursting enhance desynchronized spiking. desynchronizing effect sparsely connecting network increasing noise clearly visible. weights output weights. addition make hypothesis global bursts network impair neurons fire together make impossible tease apart individual neuron’s contributions postsynaptic neuron’s excitation. global bursts also considered pathologic behaviour vitro networks occur healthy vivo networks seen simple model reproduce results shahaf’s experiments potentially explain behaviour. also explains behaviour discussed synapse pruning. predicts networks evolve much possible towards dynamical states cause less external stimulation. experiment change network’s parameters sparsely connected network strong noise networks less prone global bursts exhibit strong desynchronized activity shown fig. monitor output zones stimulation conditions input zone stimulated. neuron fires output zone external stimulation input zone stopped. input zone stimulated. neuron fires output zone whole network stimulated goal obtain true selective learning increasing firing rate output zone compared output zone maximum weight value spiking presynaptic neurons necessary make postsynaptic neuron fire. consequence condition must able prune many input synapses output zone possible. importance suppressing global bursts becomes obvious global bursts cause output zone fire time whole network making impossible update relevant weights without also updating unrelated weights. result expect network move state output zones fire rate state output zone fires high rates output zone fires lower rates. prediction realized fig. trajectory firing rates goes space external stimulation. fig. show comparison trajectory networks condition applied average firing rates output zones equivalent individual networks trajectories ending indiscriminately left bottom right space. results could potentially reproduced network vitro izhikevich model spiking network found exhibit dynamics real neurons experiments stdp reproduce results biological experiments; therefore probability results predicted still holds biological networks suppressed bursts especially since shown gives promising results biological networks embodied simple robots following section). show used practical embodied application wall avoidance learning. simulate simple robot moving inside closed arena. robot distance sensors front allowing detect walls -neuron network takes sensors’ values respective input input zones activity output zones network allows robot turn right left. conditions steering encountering wall stop stimulation received input zones distance sensors direction robot points away walls. behaviour enhanced therefore wall avoidance. call experiment closed loop experiment steering away walls automatically stops external stimulation right timing. network burst suppression mediated compare results experiment control experiment constant stimulation applied network’s input zones independently distance orientation robot relative walls conditions tested times averaged. fig. shows important effects constant stimulation leads higher activity network provoking random steering robot leads level wall avoidance closed loop feedback necessary obtain actual wall avoidance learning. indeed closed loop experiment robot spends time less pixels wall open loop experiment. importance feedback simply high stimulation demonstrated fact open loop robot receiving overall greater amount stimulation closed loop robot. learning curve closed loop robot starts sloping down robot received average stimulation millisecond. open loop robot time reached best performance received mv/ms. addition robot learns avoid walls less stimulation receives average. despite reach final state robot spends time walls. different experiment give open loop robots amount average stimulation received closed loop robots; experiment open loop robots still spend time close arena walls. study effect stimulation strength feedback learning performance varying sensitivity distance sensors. average state last seconds task reported fig. open loop result previous experiment included reference. fig. indicates learnability task improved sensitive sensors limit sensors sensitivity improve performance robot. result direct contradiction srp’s leading hypothesis postulates intensity stimulation driving force behind network modification. case sensitive sensors always lead better learnability. contrast emphasises timing strength simulation. hypothesise limit least part figure trajectory network two-dimensional space firing rates output zone output zone using steer network space; contrast selective learning experiment maintains network balanced relatively firing rates. statistical results networks. figure learning curves wall avoidance task. robot considered close wall less pixels wall corresponds range distance sensors. results show random steering high random activity network leads spending time close walls learning leads time spent close walls. statistical results networks standard error indicated. figure learnability wall avoidance based sensor sensitivity. task cannot learned; learnability improves leading maximum performance robot. open loop closed loop results sensitivity reported fig. statistical results networks standard error. unreliable feedback steering away wall robot directly contact another wall stuck corner action lead start removal stimulation depending context. finally perform parameter search explore working conditions simple learning task. previous experiments network fully connected initial connection weights followed uniform distribution excitatory neurons inhibitory neurons. section vary number connections network variance weights. neuron output connection chosen random weight initialised perform experiments length seconds. average difference firing rate output zone first seconds last seconds reported heat fig. figure shows ideal region parameter space obtain good learning results connections neuron. values hypothesise lowest number neurons connecting input neurons output neurons becomes weakening correlation input output. figure performance learning depending network connectivity initial weights variance. ideal region parameter space obtain good learning results connections neuron. comparison variance less influence. statistical results networks parameter set. values increased connectivity network might cause many bursts affecting learning results. comparison variance initial weights influence final learning results. paper introduce principle explaining dynamics spiking neural networks influence external stimulation. model presented paper simplified compared biological neurons uses types neurons type stdp homeostatic mechanism etc. nevertheless model able reproduce features experiments conducted biological neurons explain results obtained vitro neurons submitted external stimulation. also offers explanation biological mechanism ignored theory namely pruning synapses. direct practical applications engineering causal relationships neural dynamics external stimulations induce learning change dynamics neurons outside. relies mechanism stdp demonstrated conditions obtain external stimulation minimal threshold; causal coupling neural network’s behaviour environmental stimulation; burst suppression. obtain burst suppression increasing input noise model using stp. assume healthy biological neurons neuronal noise introduced spontaneous neuronal activity. shown support theory stimulus regulation principle. could closer principle free energy minimisation introduced friston free energy principle states networks strive avoid surprising inputs learning predict external stimulation. expected behaviour networks obeying free energy principle obeying fall dark room paradox avoiding incoming input cutting sources external simulation. difference free energy principle network predict incoming input. importantly automatically stimuli environment terminate right timing network self-organize using environmental information.", "year": 2016}