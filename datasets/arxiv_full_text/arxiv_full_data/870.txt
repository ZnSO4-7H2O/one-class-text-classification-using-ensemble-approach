{"title": "Model Accuracy and Runtime Tradeoff in Distributed Deep Learning:A  Systematic Study", "tag": ["stat.ML", "cs.DC", "cs.LG", "cs.NE"], "abstract": "This paper presents Rudra, a parameter server based distributed computing framework tuned for training large-scale deep neural networks. Using variants of the asynchronous stochastic gradient descent algorithm we study the impact of synchronization protocol, stale gradient updates, minibatch size, learning rates, and number of learners on runtime performance and model accuracy. We introduce a new learning rate modulation strategy to counter the effect of stale gradients and propose a new synchronization protocol that can effectively bound the staleness in gradients, improve runtime performance and achieve good model accuracy. Our empirical investigation reveals a principled approach for distributed training of neural networks: the mini-batch size per learner should be reduced as more learners are added to the system to preserve the model accuracy. We validate this approach using commonly-used image classification benchmarks: CIFAR10 and ImageNet.", "text": "selection hyper-parameter tuning improve neural network’s performance working constraints computational resources available single computing node perspective prior work addressed extent problem accelerating neural network training mapping asynchronous version mini-batch stochastic gradient descent algorithm onto multiple computing nodes. contrary popular belief asynchrony necessarily improves model accuracy adopting approach scaledeep learning using asynchronous-sgd gives rise complex interdependencies training algorithm’s hyperparameters distributed implementation’s design choices ultimately impacting neural network’s accuracy overall system’s runtime performance. paper present rudra parameter server based deep learning framework study interdependencies undertake empirical evaluation public image classiﬁcation benchmarks. contributions investigation impact interdependence training algorithm’s hyperparameters distributed implementation’s parameters neural network’s classiﬁcation accuracy training time. observation maintain given level model accuracy necessary reduce mini-batch size number learners increased. suggests hard limit amount parallelism exploited training given model. neural network computes parametric non-linear transformation represents adjustable parameters supervised learning context input image corresponds label assigned image. deep neural abstract—deep learning large number parameters requires distributed training model accuracy runtime important factors considered. however systematic study tradeoff factors model training process. paper presents rudra parameter server based distributed computing framework tuned training large-scale deep neural networks. using variants asynchronous stochastic gradient descent algorithm study impact synchronization protocol stale gradient updates minibatch size learning rates number learners runtime performance model accuracy. introduce learning rate modulation strategy counter effect stale gradients propose synchronization protocol effectively bound staleness gradients improve runtime performance achieve good model accuracy. empirical investigation reveals principled approach distributed training neural networks mini-batch size learner reduced learners added system preserve model accuracy. validate approach using commonly-used image classiﬁcation benchmarks cifar imagenet. deep neural network based models achieved unparalleled accuracy cognitive tasks speech recognition object detection natural language processing certain image classiﬁcation benchmarks deep neural networks touted even surpass human-level performance accomplishments made possible ability perform fast supervised training complex neural network architectures using large quantities labeled data. training deep neural network translates solving non-convex optimization problem high dimensional space absence solid theoretical framework solve problems practitioners forced rely trial-and-error empirical observations design heuristics help obtain well-trained model. naturally fast training deep neural network models enable rapid evaluation different network architectures facilitate thorough hyper-parameter optimization models. recent years seen resurgence interest deploying large-scale computing infrastructure designed speciﬁcally training deep neural networks. notable efforts direction include distributed computing infrastructure using thousands cores high-end graphics processors combination cpus gpus large-scale deep learning problem hence viewed conﬂuence elements machine learning high-performance computing much work community focused non-convex optimization model unique mini-batch. model parallelism approach augments framework splitting neural network model across multiple learners. model parallelism learner trains portion network; edges cross learner boundaries must synchronized gradients computed entire model. several different synchronization strategies possible. commonly used asynchronous protocol learners work completely independently parameter server. section discuss three different synchronization strategies associated unique tradeoff model accuracy runtime. throughout paper following deﬁnitions parameter server server holds model weights. describes typical parameter server using distributed key-value store synchronize state processes. parameter server collects gradients learners updates weights accordingly. updates mini-batch size. learning rate. number learners. epoch pass entire training dataset. timestamp scalar clock represent weights timestamp starting weight update increments timestamp timestamp gradient timestamp weight used compute gradient. staleness gradient. gradient timestamp pushed parameter server current weight timestamp tsi. deﬁne staleness gradient average staleness gradients. timestamps advancement weights timestamp tsi− form vector clock tsin max{i average staleness gradients deﬁned hardsync protocol advance weights timestamp tsi+ learner calculates exactly minibatch sends gradient parameter server. parameter server averages gradients updates weights according equation broadcasts weights learners. staleness hardsync protocol always zero. network organizes parameters multiple layers consists linear transformation followed nonlinear function sigmoid tanh etc. feed-forward deep neural network layers arranged hierarchically output layer feeds input layer terminal layer generates network’s output corresponding input neural network training algorithm seeks parameters minimizes discrepancy ground truth usually accomplished deﬁning differentiable cost function iteratively updating model parameters using variant gradient descent algorithm represents parameter iteration step size batch size. batch gradient descent algorithm sets equal total number training examples large amount training data deep neural networks typically trained using stochastic gradient descent parameters updated randomly selected training example performance improved computing gradients using minibatch containing training examples. deep neural networks generally considered hard train trained model’s generalization error depends strongly hyperparameters initializations learning rates mini-batch size network architecture etc. addition neural networks prone overﬁt data. regularization methods applied training shown combat overﬁtting reduce generalization error. scale-out deep learning typical implementation distributed training deep neural networks consists master orchestrates work among slaves learner followings gradients along gradients’ timestamp parameter server. size pull push messages model size plus size scalar timestamp equal one. data server hosted gpfs global system. learner thread prefetches minibatch random sampling prior training. prefetching completely overlapped computation. parameter server multithreaded process accumulates gradients learner applies update rules according equations study implemented hardsync protocol n-softsync protocol. learning rate conﬁgured differently either protocol. hardsync protocol learning λµ/b batch size reference model. n-softsync protocol learning rate multiplied reciprocal staleness. demonstrate section treatment learning rate n-softsync signiﬁcantly improve model accuracy. parameter server records vector clock weight update keep track average staleness. speciﬁed number epochs trained parameter server shuts learner. statistics server multithreaded process receives training error learner receives model parameter server epoch tests model. monitors model training quality. architecture non-blocking everywhere except pushing gradients pushing weights blocking calls parameter server handles incoming message precisely control learner’s gradients received handled parameter server. purpose rudra-base control noise system study interplay scalefactor hyperparameter selection. moderatelysized dataset like cifar- rudra-base shows good scale-out factor achieve high classiﬁcation accuracy required model size quite large many cases achieve best possible model accuracy mini-batch size must small demonstrate section v-b. order meet requirements acceptable performance implemented rudra-adv rudra-adv∗. gradient staleness hard control asynchrony system. describe downpour implementation async protocol commodity scale-out system staleness large hundreds. n-softsync protocol learner pulls weights parameter server calculates gradients pushes gradients parameter server. parameter server updates weights collecting least gradients. splitting parameter vary n-softsync weight update rule given section show homogeneous cluster learner proceeds roughly speed staleness model empirically bounded note equal weight update rule parameter server exactly async protocol. figure illustrates parameter server design study interplay hyperparameter tuning system scale-out factor. system implements hardsync nsoftsync protocols. arrows entity represent message except communication learner data server achieved global system. describe entity’s role implementation below. learner single-process multithreaded solver. training mini-batch learner pulls weights corresponding timestamp parameter server. learner reduces pullweights trafﬁc ﬁrst inquiring timestamp parameter server timestamp local weights’ learner pull weights. training mini-batch learner sends rudra-adv system architecture. rudra-base clearly scalable solution model gets large. ideal circumstances single learner pushing model would take transfer data. tasks sending receiver link contention would take second messages delivered. alleviate network trafﬁc parameter server build parameter server group forms tree. colocate tree leaf node node learners responsible. node parameter server group responsible averaging gradients sent learners relaying averaged gradient parent. root node parameter server group responsible applying weight update broadcasting updated weights. non-leaf node pulls weights parent responds children’s weight pulling requests. rudra-adv signiﬁcantly improve performance compared rudrabase manage scale large model small maintaining control gradients’ staleness. figure illustrates system architecture rudra-adv. boxes represent parameter server group gradients pushed aggregated upwards. green boxes represent learners learner pushes gradient parameter server parent receives weights parameter server parent. difference rudra-adv sharded parameter server design adam weights maintained rudra-adv timestamps whereas shared parameter servers maintain weights different timestamps. consistent weights makes analysis hyperparameter/scale-out interplay much tractable. rudra-adv∗ system architecture. built rudra-adv∗ improve runtime performance ways broadcast weights within learners. reduce trafﬁc parameter server group form tree within learners broadcast weights tree. network links to/from learners also utilized. asynchronous pushgradient pullweights. ideally would non-blocking send calls asynchronously send gradients weights. however depending implementation difﬁcult guarantee non-blocking send calls make progress background therefore open additional communication threads blocking send calls threads. learner process runs additional communication threads pullweights thread pushgradient thread. manner computing continue without waiting communication. note since need control must guarantee learner pushes calculated gradient server. alternatively could locally accrue gradients send however effectively increase reason pushgradient thread cannot start sending current gradient previous delivered. demonstrated table long optimize network links constraint bearing runtime performance even extremely small. contrast pullweights thread constraint maintain computation buffer communication buffer pullweights thread communication always happens background. newly received weights requires pointer swap. figure illustrates system architecture rudra-adv∗. different rudra-adv learner continuously receives weights weights downcast tree consists level parameter server node learners. measure communication overlap calculating ratio computation time computation communication time. table records communication overlap rudra-base rudra-adv rudra-adv∗ adversarial scenario. rudra-adv∗ almost completely overlap computation communication. rudra-adv∗ scale large model size work smallest possible size mini-batch. section demonstrate rudra-adv∗’s effectiveness improving runtime performance achieving good model accuracy. deploy rudra distributed deep learning framework supercomputer. node system contains four eight-core power processors optical connect controller chip memory. single node theoretical ﬂoating point peak performance gﬂop/s memory bandwidth gb/s bi-directional interconnect bandwidth gb/s. evaluate rudra’s scale-out performance employ commonly used image classiﬁcation benchmark datasets cifar imagenet cifar dataset comprises total images size pixels partitioned training test image belongs classes images class. dataset construct deep convolutional neural network convolutional layers followed subsampling/pooling layer. output pooling layer connects fully-connected layer -way softmax output layer generates probability distribution output classes. neural network architecture closely mimics cifar model available part opensource caffe deep learning package total number trainable parameters network resulting model size using -bit ﬂoating point data representation. neural network trained using momentum-accelerated mini-batch batch size momentum data preprocessing step per-pixel mean computed entire training dataset subtracted input neural network. training performed epochs results model achieves misclassiﬁcation error rate test dataset. base learning rate reduced factor epoch. learning rate schedule proves quite essential obtaining test error collection natural images used part edition imagenet large scale visual recognition challenge training subset hand-labeled imagenet database contains million images. validation dataset images. image maps non-overlapping object categories. images converted ﬁxed resolution used input deep convolution neural network. dataset consider neural network architecture introduced consisting convolutional layers fully-connected layers. last layer outputs probability distribution object categories. neural network million trainable parameters total model size network trained using momentum-accelerated batch size momentum similar cifar benchmark per-pixel mean computed entire training dataset subtracted input image feeding neural network. prevent overﬁtting weight regularization penalty applied layers network dropout applied fully-connected layers. initial learning rate equal reduced factor epoch. training epochs results top- error top- error validation set. fig. average staleness gradients function weight update step parameter server using -softsync -softsync λ-softsync protocol. inset shows distribution gradient staleness values λ-softsync protocol. number learners space exploration cifar dataset rudrabase system architecture. subsequently extend ﬁndings imagenet dataset using rudra-adv rudra-adv∗ system architectures. hardsync protocol introduced section iii-a transition involves aggregating gradients calculated result gradients carries staleness equal however departing hardsync protocol towards either n-softsync async protocol inevitably adds staleness gradients subset learners contribute gradients calculated using weights timestamp earlier current timestamp weights parameter server. measure effect gradient staleness using n-softsync protocol cifar dataset train neural network described section iv-b using learners. -softsync protocol parameter server updates current weights received total gradients learners. similarly -softsync protocol forces parameter server accumulate gradient contributions learners updating weights. shown figure average staleness -softsync -softsync protocols remains close respectively. -softsync protocol staleness gradients computed learner takes values whereas -softsync protocol. figure shows gradient staleness n-softsync protocol case parameter server updates weights receiving gradient learners. large fraction gradients staleness close probability exceed measurements show that general implementation n-softsync protocol. modifying learning rate stale gradients experiments n-softsync protocol found beneﬁcial times necessary modulate learning rate take account staleness gradients. n-softsync protocol learning rate effect learning rate modulation strategy dividing learning fig. rate average staleness aids better convergence achieves lower test error using n-softsync protocol. number learners mini-batch size representative results illustrating beneﬁts adopting learning rate modulation strategy. show evolution test error cifar dataset function training epoch different conﬁgurations nsoftsync protocol number learners conﬁgurations setting learning rate accordance equation results lower test error compared cases learning rate surprisingly conﬁguration -softsync fails converge shows constant high error rate reducing learning rate factor makes model much lower test error. hyperparameter optimization plays central role obtaining good accuracy neural network models training algorithm includes search neural network’s training parameters learning rates weight regularization depth network mini-batch size etc. order improve quality trained neural network model additionally distributing training problem across multiple learners parameters number learners synchronization protocol enforced amongst learners impact runtime algorithm also quality trained model. although explored part work certainly possible implement ﬁner-grained learning rate modulation strategy depends staleness gradients computed learners instead average staleness. strategy apply smaller learning rates staler gradients fig. tradeoff curves λ-softsync protocol -softsync protocol. shaded region shows region bounded contours hardsync protocol. note n-softsync protocol degenerates hardsync protocol small model used cifar dataset clearly intractable models datasets scale imagenet. develop better understanding interdependence among various tunable parameters distributed deep learning problem introduce notion tradeoff curves. tradeoff curve plots error validation total time train model different conﬁgurations average gradient staleness mini-batch size learner number learners conﬁgurations achieve virtuous combination test error small training time preferred form ideal candidates hyperparameter optimization. figure shows curves hardsync protocol i.e. baseline conﬁguration learner mini-batch size achieves test error exception modifying learning rate neural network’s hyperparameters kept unchanged baseline conﬁguration generating data points different values note possible achieve test error lower baseline reducing mini-batch size however conﬁguration increases training time compared baseline. primarily fact dominant computation performed learners involves multiple calls matrix multiplication compute samples mini-batch form columns matrix reducing mini-batch size cause proportionate decrease gemm throughput slower processing mini-batch learner. appear nearly identical observed λ-softsync protocol. case average staleness irrespective number learners. since parameter server waits gradients arrive updating weights reduction pullweights trafﬁc parameter server result -softsync protocol avoids degradation runtime observed λ-softsync protocol conﬁguration distinction terms runtime performance protocols becomes obvious comparing speed-ups obtained different minibatch sizes -softsync λ-softsync protocol demonstrate similar speed-ups baseline conﬁguration upto learners. case communication learners parameter server sporadic enough prevent learners waiting parameter server updated weights. however number learners increased beyond bottlenecks parameter server expected diminish speed-up obtainable using λ-softsync protocol. effect frequent pushgradient pullweights requests smaller parameter manifest clearly mini-batch size reduced case λ-softsync protocol shows subdued speed-up compared -softsync protocol. either scenario hardsync protocol fares worst terms runtime performance improvement scaling large number learners. upside adopting hardsync protocol however achieves substantially lower test error even large mini-batch sizes. hardsync protocol given conﬁguration learners mini-batch size learner parameter server averages number gradients reported learners. using equations last step equation valid since training example drawn independently training also hardsync protocol ensures learners compute gradients identical weights i.e. according equation conﬁgurations equivalent perspective stochastic gradient descent optimization. general hardsync conﬁgurations product expected give nearly test error. small differences ﬁnal test error achieved arise inherent nondeterminism random sampling stochastic gradient descent random initialization weights. training time reduces monotonically albeit expense increase test error. traversing along contour conﬁguration helps restore much degradation test error partially sacriﬁcing speed-up obtained virtue scaling learners. fig. speed-up training time mini-batch size different protocols hardsync λ-softsync -softsync. speednumbers calculated relative respectively. figure shows tradeoff curves λsoftsync protocol. protocol parameter server updates weights soon receives gradient learners. therefore shown section average gradient staleness σmax high probability. learning rate accordance equation hyperparameters left unchanged baseline conﬁguration. qualitatively tradeoff curves λ-softsync look similar observed hardsync protocol. case however degradation test error relative baseline conﬁguration much pronounced. observed previously increase test error largely mitigated reducing size mini-batch processed learner note however sharp increase training time conﬁguration compared smaller mini-batch size reduces computational throughput learner also increases frequency pushgradient pullweights requests parameter server. addition small mini-batch size increases frequency weight updates parameter server. since rudra-base architecture learner proceed computation next mini-batch till received updated gradients trafﬁc parameter server frequent weight updates delay servicing learner’s pullweights request potentially stalling gradient computation learner. interestingly conﬁgurations along contour show similar better test error baseline. conﬁgurations average staleness varies empirical observation infer small mini-batch size learner confers upon training algorithm fairly high degree immunity stale gradients. even n-softsync protocol conﬁgurations maintain constant achieve comparable test errors. time test error turns rather independent staleness gradients given product. instance table shows gradient staleness allowed vary test error stays ∼-%. although result seem counterintuitive plausible explanation emerges considering measurements shown earlier figure implementation n-softsync protocol achieves average gradient staleness bounding maximum staleness consequently stage gradient descent algorithm weights used different learners differ signiﬁcantly considered approximately same. quality approximation improves update creates small displacement weight space. controlled suitably tuning learning rate qualitatively learning rate decrease staleness system increases order reduce divergence across weights seen learners. learning rate modulation equation achieves precisely effect. results help deﬁne principled approach distributed training neural networks mini-batch size learner reduced learners added system keeps product constant. addition learning rate modulated account stale gradients. table -softsync protocol invariably shows smallest training time expected since -softsync protocol minimizes trafﬁc parameter server. table also shows test error increases monotonically product. results reveal scalability limits constraints preserving model accuracy. since smallest possible mini-batch size maximum number learners bounded. upper bound maximum number learners relaxed inferior model acceptable. alternatively possible reduce test error higher running number epochs. scenario adding learners system give diminishing improvements overall runtime. machine learning perspective points interesting research direction designing optimization algorithm learning strategies perform well large mini-batch sizes. table summarizes results obtained cifar dataset using rudra-base system architecture. reference comparison baseline conﬁguration achieves test error takes seconds ﬁnish training epochs. large model size neural network used imagenet benchmark associated computational cost training model prohibits exhaustive state space exploration. baseline conﬁguration takes hours/epoch. guided results section ﬁrst consider conﬁguration employ rudra-base architecture hardsync protocol conﬁguration performs training speed minutes/epoch achieves top- error matching accuracy baseline conﬁguration synchronization overheads associated hardsync protocol deteriorate runtime performance training speed improved switching softsync protocol. training using -softsync protocol mini-batch size learners takes minutes/epoch reaching top- accuracy epochs particular benchmark training setup -softsync protocol differs hardsync protocol certain subtle important ways. first adaptive learning rate method improve stability training using -softsync protocol. second improve convergence adopt strategy warmstarting training procedure initializing network’s weights model trained hardsync epoch. improvement runtime performance obtained adding learners system. however observed previous section increase number learners needs accompanied reduction mini-batch size prevent degradation accuracy trained model. combination large number learners small mini-batch size represents scenario rudra-base architecture suffer bottleneck parameter server frequent pushgradient pullweights requests. effects expected pronounced large models imagenet. rudra-adv architecture alleviates bottlenecks extent implementing parameter server group organized tree structure. learners processing mini-batch size trains minutes/epoch using rudraadv architecture -softsync protocol case rudra-base average staleness gradients close conﬁguration also achieves top- error rudra-adv∗ architecture improves runtime preventing computation learner stalling parameter server. however improvement performance comes cost increasing average staleness gradients deteriorate quality trained model. previous conﬁguration runs minutes/epoch suffers increase top- validation error using rudra-adv∗ architecture table summarizes results obtained conﬁgurations discussed above. worth mentioning conﬁguration performs signiﬁcantly worse producing model gives top- error trains speed minutes/epoch. supports observation scaling large number learners must accompanied reducing mini-batch size learner quality trained model preserved. figure compares evolution top- validation error training different conﬁguration summarized table training speed follows order adv∗-softsync adv-softsync base-softsync base-hardsync. result adv∗-softsync ﬁrst conﬁguration validation error mark. conﬁgurations base-hardsync show marginally higher validation error compared baseline. mentioned earlier experiments -softsync protocol adagrad achieve stable convergence. welldocumented adagrad sensitive initial setting learning rates. speculate tuning initial learning rate help recover slight loss accuracy -softsync runs. accelerate training deep neural networks handle large dataset model size many researchers adopted gpu-based solutions either single-gpu multigpu gpus provide enormous computing power particularly suited matrix multiplications core many deep learning implementations. however relatively limited memory available gpus restrict applicability large model sizes. distbelief pioneered scale-out deep learning cpus. distbelief built tens thousands commodity employs model parallelism dividing single model learners data parallelism model replication. reduce trafﬁc parameter server distbelief shards parameters parameter server group. learners asynchronously push gradients pull weights parameter server. frequency communication tuned npush nfetch parameters. recently adam employs similar system architecture distbelief improving distbelief respects better system tuning e.g. customized concurrent memory allocator better linear algebra library implementation passing activation error gradient vector instead weights update; leveraging recent improvement duchi hazan singer. adaptive subgradient methods online learning stochastic optimization. journal machine learning research imagenet large scale visual recognition chal krizhevsky sutskever hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems pages lamport. time clocks ordering events parameter server based deep learning system staleness negatively impact model accuracy. orthogonal system design many researchers proposed solutions counter staleness system bounding staleness system changing optimization objective function elastic averaging paper empirically study staleness affects model accuracy discover heuristics smaller mini-batch size effectively counter system staleness. experiments derive heuristics small problem size heuristic applicable even much larger problem size ﬁnding coincides recent theoretical paper authors prove order achieve linear speedup using asynchronous protocol maintaining good model accuracy needs increase number weight updates conducted parameter server. system theoretical ﬁnding equivalent keeping constant number training epochs reducing mini-batch size. best knowledge work ﬁrst systematic study tradeoff model accuracy runtime performance distributed deep learning. paper empirically studied interplay hyperparameter tuning scale-out three protocols communicating model weights asynchronous stochastic gradient descent. divide learning rate average staleness gradients resulting faster convergence lower test error. experiments show -softsync protocol minimizes gradient staleness achieves lowest runtime given test error. found maintain model accuracy necessary reduce mini-batch size number learners increased. suggests upper limit level parallelism exploited given model consequently need algorithms permit training larger batch sizes.", "year": 2015}