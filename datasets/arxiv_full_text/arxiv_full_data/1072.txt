{"title": "Generalization and Equilibrium in Generative Adversarial Nets (GANs)", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "We show that training of generative adversarial network (GAN) may not have good generalization properties; e.g., training may appear successful but the trained distribution may be far from target distribution in standard metrics. However, generalization does occur for a weaker metric called neural net distance. It is also shown that an approximate pure equilibrium exists in the discriminator/generator game for a special class of generators with natural training objectives when generator capacity and training set sizes are moderate.  This existence of equilibrium inspires MIX+GAN protocol, which can be combined with any existing GAN training, and empirically shown to improve some of them.", "text": "show training generative adversarial network good generalization properties; e.g. training appear successful trained distribution target distribution standard metrics. however generalization occur weaker metric called neural distance. also shown approximate pure equilibrium exists discriminator/generator game special class generators natural training objectives generator capacity training sizes moderate. generative adversarial networks become dominant methods ﬁtting generative models complicated real-life data even found unusual uses designing good cryptographic primitives survey goodfellow various novel architectures training objectives introduced address perceived shortcomings original idea leading stable training realistic generative models practice huang radford tolstikhin salimans jiwoong durugkar reference therein). sample distribution close target distribution dreal dreal together generator also trains discriminator deep trying maximise ability distinguish samples dreal long discriminator ∗princeton university computer science department email aroracs.princeton.edu †duke university computer science department email ronggecs.duke.edu ‡princeton university computer science department email yingyulcs.princeton.edu §princeton university computer science department email tengyucs.princeton.edu ¶princeton university computer science department email yzcs.princeton.edu this updated version icml’ paper title. main diﬀerence icml’ version pure equilibrium result proved wasserstein gan. current version result applies reasonable training objectives. particular theorem applies original wasserstein gan. deciding whether particular sample came dreal. basic iterative conclude generator wins game close dreal metric? seems distribution dreal could complicated many peaks valleys; figure another open theoretical issue whether equilibrium always exists game generator discriminator. zero gradient necessary condition standard optimization halt corresponding necessary condition two-player game equilibrium. conceivably instability often observed training gans could arise lack equilibrium. suggest using wasserstein objective practice reduces instability still lack proof existence equilibrium.) standard game theory help need so-called pure equilibrium simple counter-examples rock/paper/scissors show doesn’t exist general. however guarantee weaker notion generalization introducing metric distributions neural distance. show generalization happen moderate number training examples thus unsurprisingly inﬁnite mixture game. prove rigorously even ﬁnite mixture fairly reasonable size closely approximate performance inﬁnite mixture insight also allows construct architecture generator network exists approximate equilibrium pure. existence proof approximate equilibrium unfortunately involves quadratic blowup size generator improving left future theoretical work. propose heuristic approximation mixture idea introduce framework training call mix+gan. added existing training procedure including divergence objectives. experiments section show several previous techniques mix+gan stabilizes training cases improves performance. generators discriminators. denote class generators function often neural network practice indexed denotes parameters generators. denotes possible ranges parameters without loss generality assume subset unit ball. generator deﬁnes distribution follows generate -dimensional spherical gaussian distribution apply generate sample distribution dgu. drop subscript it’s clear context. denote class discriminators function sampled distribution dreal value sampled synthetic distribution dgu. training discriminator consists trying make synthetic distribution similarto dreal sense discriminator’s output tends similar assume l-lipschitz respect parameters. intuitively says discriminator give high values real samples values generated examples. function suggested interpretation likelihood also nice information-theoretic interpretation described below. however practice cause problems since objective still makes intuitive sense replace monotone function yields samples dreal estimate value ex∼dreal)]. ﬁnite training examples dreal uses i=)] estimate quantity ex∼dreal)]. call distribution gives probability xi’s empirical version real distribution. similarly empirical version estimate ex∼dgu standard interpretation distance distributions. towards analyzing gans researchers assumed access inﬁnite number examples discriminator chosen optimally within large class functions contain possible neural nets. often allows computing analytically optimal discriminator therefore removing maximum operation objective leads interpretation sense using original objective function optimal choice among possible functions preal+pg shown goodfellow preal density real distribution density distribution generated generator using discriminator though it’s computationally infeasible obtain show minimization problem generator correspond minimizing measuring functions choice discriminator class leads diﬀerent distance function distribution divergence. notably arjovsky shows discriminator chosen among -lipschitz functions maxing interpretation gans terms minimizing distance real distribution generated distribution relies crucial assumptions expressive class discriminators bounded discriminator -lipschitz discriminators large number examples compute/estimate objective neither assumption holds practice show next greatly aﬀects generalization ability notion introduce section similarly examples generated distribution training gans implicitly uses approximate quantity ex∼dreal)]. inspired observation training objective gans variants minimize distance dreal using ﬁnite samples deﬁnition given ˆdreal empirical version true distribution samples generated distribution generalizes divergence distance distributions generalization error following holds high probability words generalization gans means population distance true generated distribution close empirical distance empirical distributions. target make former distance small whereas latter access minimize practice. deﬁnition allows polynomial number samples generated distribution training algorithm polynomial time. consequences lemma first consider situation dreal long polynomial second consider case dreal ˆdreal memorizes training examples ˆdreal. case since discrete distribution ﬁnite supports enough examples eﬀectively also therefore whereas words polynomial number examples it’s possible overﬁt training examples using wasserstein distance. argument also applies divergence. appendix formal proof. notice result contradict experiments arjovsky since actually wasserstein distance surrogate distance generalize show next. analyze generalization performance? towards answering questions full generality consider following general distance measure uniﬁes divergence wasserstein distance neural distance deﬁne later section. distance function slightly abuse notation write simply supd∈f |ex∼µ] ex∼ν]| example {all functions divergence. {all -lipschitz functions wasserstein distance. example suppose neural networks original objective function equivalent ming cally arjovsky equivalent ming gans training uses class neural nets bound number parameters. informally refer neural distance. next theorem establishes generalization sense equation hold assume measuring function takes values lφ-lipschitz. further appendix proof. intuition aren’t many distinct discriminators thus given enough samples expectation empirical distribution converges expectation true distribution discriminators. theorem shows neural network divergence much better generalization properties jensen-shannon divergence wasserstein distance. successfully minimized neural network divergence empirical distrisince ﬁnal goal gans training learn distribution worth understanding though weak generalization sense section guaranteed comes cost. divergence wasserstein distance distance distributions small safe conclude distributions almost same. however neural distance small even close. simple corollary lemma obtain neural network distance nets parameters cannot distinguish distribution distribution support fact proof still works disriminator allowed take many samples reason don’t help capacity limited explanation starts thought experiment. imagine allowing much powerful generator namely inﬁnite mixture deep nets size long deep class capable generating simple gaussians mixtures quite powerful since classical result distribution closely approximate arbitrary dreal respect natural metrics like show can. informal theorem discriminator deep parameters mixture o/\u0001) generator nets produce distribution discriminator unable distinguish dreal probability notation hides nuisance informal theorem also component result existence approximate pure equilibrium. ﬁrst show ﬁnite mixture generators discriminators discuss mixed generator realized single generator network -layer deeper. example therefore mixed generator linear mixture generators. mixture discriminators complicated objective function need linear discriminator. however case interest generator wins even mixture discriminators cannot eﬀectively distinguish generated real distribution. therefore consider mixture discriminators here. course equilibrium involving inﬁnite mixture makes little sense practice. show approximate min-max solution mixture ﬁnitely many generators discriminators. precisely deﬁne \u0001-approximate equilibrium strategies susv pure strategies pair called \u0001-approximate pure equilibrium. suppose lφ-lipschitz bounded generator discriminators llipschitz respect parameters l-lipschitz respect inputs setting theorem settings above generator approximate point mass universal constant exists loglφ·p/\u0001) generators uniform distribution discriminator outputs \u0001-approximate equilibrium. proof uses standard probabilistic argument epsilon argument show sample generators discriminators inﬁnite mixture form approximate equilibrium high probability. second part fact generator approximate point mass inﬁnite mixture generators approximate real distribution dreal win. therefore indeed mixture generators achieve \u0001-approximate equilibrium. give construction augment network structure achieve approximate pure equilibrium game generator nets size interpreted deep nets size capable generating point mass game generator neural network size approximate equilibrium generator wins. mixture generators construct single neural network approximately generates mixture distribution using gaussian input has. that pass input generators show implement multi-way selector selector involves simple -layer remark practice gans highly structured deep nets convolutional nets. current proof existence pure equilibrium requires introducing less structured elements namely multiway selectors implement mixture within single net. left future work whether pure equilibria exist original structured architectures. meantime practice recommend using even w-gan mixture structured nets training seems help experiments reported below. theorem theorem show using mixture generators discriminators guarantees existence approximate equilibrium. suggests using mixture lead stable training. experiments correspond older version paper done using mixture generator discriminators. course impractical large mixtures propose mixture components large allowed size memory namely train mixture generators {gui discriminators {dvi share network architecture trainable parameters. maintaining mixture means course maintaining weight generator corresponds probability selecting output gui. weights also updated backpropagation. heuristic combined existing methods like dcgan w-gan etc. giving training methods mix+dcgan mix+w-gan etc. exponentiated gradient store log-probabilities {αui note algorithm maintaining weights diﬀerent generators discriminators. diﬀerent idea boosting weights maintained samples. adagan uses ideas similar boosting maintains weights training examples. payoﬀ function equation measuring functions experiments alternatively update generators’ discriminators’ parameters well corresponding log-probabilities using adam learning rate empirically observed components mixture tend collapse weights diminish training. encourage full mixture capacity training objective entropy regularizer term discourages weights away uniform table inception scores cifar-. mixture dcgans achieves higher score single-component dcgan does. models except wassersteingan variants trained labels. dcgan architecture uses deep convolutional nets generators discriminators. trained mix+dcgan mnist celeba using authors’ code black compared visual qualities generated images dcgan. results mnist shown figure experiment baseline dcgan consists pair generator discriminator -layer deconvoluitonal neural networks conditioned image labels. mix+dcgan model consists mixture dcgans generators discriminators. observe method produces somewhat cleaner digits baseline results celeba dataset also figure using architecture mnist except models conditioned image labels anymore. again method generates faithful diverse samples baseline. note need zoom fully perceive diﬀerence since datasets rather easy dcgan. turn quantitative measurement using inception score method applied dcgan wassersteingan arjovsky throughout mixtures generators discriminators used. ﬁrst sight comparison dcgan v.s. mix+dcgan seems unfair latter uses times capacity former corresponding penalty running time epoch. address this also compare method larger versions dcgan roughly number parameters found former consistently better later detailed below. construct mix+dcgan build dcgan trained losses proposed huang best variant without improved training techniques. hyper-parameters used fair comparison. huang details. similarly mix+wassersteingan base identical proposed arjovsky using hyper-parameter scheme. table shows results cifar- dataset. that simply applying method baseline models mix+ models achieve v.s. dcgan v.s. wassersteingan. conﬁrm superiority mix+ models solely parameters also tested dcgan model times many parameters tuned using grid search sets hyper-parameters gets lower mix+dcgan. unclear apply mix+ s-gans. tried mixtures upper bottom generators separately resulting worse scores somehow. leave future exploration. figure shows inception scores mix+dcgan v.s. dcgan evolve training. mix+dcgan outperforms dcgan throughout entire training process showing makes eﬀective additional capacity. arjovsky shows wasserstein loss neural network divergence deﬁnition meaningful correlates well visual quality generated samples. figure shows training dynamics neural network divergence mix+wassersteingan v.s. wassersteingan strongly indicates mix+wassersteingan capable achieving much lower divergence well improving visual quality generated samples. notion generalization gans clariﬁed introducing notion distance distributions neural distance. assuming visual cortex also deep generalization respect metric principle suﬃcient make ﬁnal samples look realistic humans even doesn’t actually learn true distribution. issue raised analysis current gans objectives cannot even enforce synthetic distribution high diversity empirically veriﬁed followsuspecting pure equilibrium exist objectives recommend practice mix+gan protocol using small mixture discriminators generators. experiments show improves quality several existing training methods. paper done part authors hosted simons institute. thank moritz hardt kunal talwar luca trevisan eric price referees useful comments. research supported oﬃce naval research simons foundation.", "year": 2017}