{"title": "Character-Aware Neural Language Models", "tag": ["cs.CL", "cs.NE", "stat.ML"], "abstract": "We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60% fewer parameters. On languages with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model outperforms word-level/morpheme-level LSTM baselines, again with fewer parameters. The results suggest that on many languages, character inputs are sufficient for language modeling. Analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information.", "text": "nlms shown outperform count-based n-gram language models blind subword information example know priori eventful eventfully uneventful uneventfully structurally related embeddings vector space. embeddings rare words thus poorly estimated leading high perplexities rare words especially problematic morphologically rich languages long-tailed frequency distributions domains dynamic vocabularies work propose language model leverages subword information character-level convolutional neural network whose output used input recurrent neural network language model unlike previous works utilize subword information morphemes model require morphological tagging pre-processing step. unlike recent line work combines input word embeddings features character-level model model utilize word embeddings input layer. given parameters nlms word embeddings proposed model signiﬁcantly fewer parameters previous nlms making attractive applications model size issue describe simple neural language model relies character-level inputs. predictions still made word-level. model employs convolutional neural network highway network characters whose output given long short-term memory recurrent neural network language model english penn treebank model existing state-of-the-art despite fewer parameters. languages rich morphology model outperforms word-level/morpheme-level lstm baselines fewer parameters. results suggest many languages character inputs sufﬁcient language modeling. analysis word representations obtained character composition part model reveals model able encode characters only semantic orthographic information. language modeling fundamental task artiﬁcial intelligence natural language processing applications speech recognition text generation machine translation. language model formalized probability distribution sequence strings traditional methods usually involve making n-th order markov assumption estimating n-gram probabilities counting subsequent smoothing count-based models simple train probabilities rare n-grams poorly estimated data sparsity neural language models address n-gram data sparsity issue parameterization words vectors using inputs neural network parameters learned part training process. word embeddings obtained nlms exhibit property whereby semantically close words likewise close induced vector space copyright association advancement artiﬁcial intelligence rights reserved. architecture model shown figure straightforward. whereas conventional takes word embeddings inputs model instead takes output single-layer character-level convolutional neural network max-over-time pooling. notation denote vectors bold lower-case matrices bold upper-case scalars italic lower-case sets cursive uppercase letters. notational convenience assume words characters already converted indices. recurrent neural network recurrent neural network type neural network architecture particularly suited modeling sequential phenomena. time step takes input vector hidden state vector produces next hidden state applying following recursive operation rm×n rm×m parameters afﬁne transformation element-wise nonlinearity. theory summarize historical information time hidden state practice however learning long-range dependencies vanilla difﬁcult vanishing/exploding gradients occurs result jacobian’s multiplicativity respect time. addresses problem learning long range dependencies augmenting memory cell vector time step. concretely step lstm takes input produces following intermediate calculations tanh element-wise sigmoid hyperbolic tangent functions element-wise multiplication operator referred input forget output gates. initialized zero vectors. parameters lstm memory cells lstm additive respect time alleviating gradient vanishing problem. gradient exploding still issue though practice simple optimization strategies work well. lstms shown outperform vanilla rnns many tasks including language modeling easy extend rnn/lstm layers another network whose figure architecture language model applied example sentence. best viewed color. model takes absurdity current input combines history predict next word first layer performs lookup character embeddings stacks form matrix convolution operations applied multiple ﬁlter matrices. note example twelve ﬁlters—three ﬁlters width four ﬁlters width three ﬁlters width four max-over-time pooling operation applied obtain ﬁxed-dimensional representation word given highway network. highway network’s output used input multi-layer lstm. finally afﬁne transformation followed softmax applied hidden representation lstm obtain distribution next word. cross entropy loss distribution next word actual next word minimized. element-wise addition multiplication sigmoid operators depicted circles afﬁne transformations represented solid arrows. recurrent neural network language model ﬁxed size vocabulary words. language model speciﬁes distribution given historical sequence recurrent neural network language model j-th column rm×|v| bias term. similarly conventional rnn-lm usually takes words inputs input rnn-lm input embedding k-th column embedding matrix rn×|v|. model simply replaces input embeddings output character-level convolutional neural network described below. denote sequence words training corpus training involves minimizing negative log-likelihood sequence character-level convolutional neural network model input time output character-level convolutional neural network describe section. cnns achieved state-of-the-art results computer vision also shown effective various tasks architectures employed applications differ typically involve temporal rather spatial convolutions. vocabulary characters dimensionality character embeddings rd×|c| matrix character embeddings. suppose word made sequence characters length word character-level representation given matrix rd×l j-th column corresponds character embedding apply narrow convolution ﬁlter rd×w width bias apply nonlinearity obtain feature rl−w+. speciﬁcally i-th element given work predictions word-level hence given usually small authors work onehot representations characters. however found using lower dimensional representations characters performed slightly better. technical details warrant mention here append start-of-word end-of-word characters word better represent preﬁxes sufﬁxes hence actually columns; batch processing zero-pad number columns constant words feature corresponding ﬁlter idea capture important feature— highest value—for given ﬁlter. ﬁlter essentially picking character n-gram size n-gram corresponds ﬁlter width. described process feature obtained ﬁlter matrix. charcnn uses multiple ﬁlters varying widths obtain feature vector total ﬁlters input representation many highway network could simply replace rnn-lm show later simple model performs well could also multilayer perceptron model interactions character n-grams picked ﬁlters found resulted worse performance. instead obtained improvements running highway network recently proposed srivastava whereas layer applies afﬁne transformation followed nonlinearity obtain features nonlinearity called transform gate called carry gate. similar memory cells lstm networks highway layers allow training deep networks adaptively carrying dimensions input directly output. construction dimensions match hence square matrices. table corpus statistics. word vocabulary size; character vocabulary size; number tokens training set. small english data penn treebank arabic data news-commentary corpus. rest workshop machine translation. large special characters. standard training validation test splits along pre-processing mikolov approximately tokens version extensively used language modeling community publicly available. optimal hyperparameters tuned apply model various morphologically rich languages czech german french spanish russian arabic. nonarabic data comes workshop machine translation train/validation/test splits botha blunsom data publicly available obtained preprocessed versions authors whose morphological serves baseline work. train small datasets tokens language large datasets including large english data much bigger ptb. arabic data comes news-commentary corpus perform preprocessing train/validation/test splits. datasets singleton words replaced <unk> hence effectively full vocabulary. worth noting character model utilize surface forms tokens stick preprocessed versions exact comparison prior work. optimization models trained truncated backpropagation time backpropagate time steps using stochastic gradient descent learning rate initially halved perplexity decrease validation epoch. data-s batch size data-l batch size size total ﬁlters); nonlinearity functions; number layers; number hidden units. greater efﬁciency). gradients averaged batch. train epochs non-arabic epochs arabic data picking best performing model validation set. parameters model randomly initialized uniform distribution support regularization dropout probability lstm input-to-hidden layers hidden-to-output softmax layer. constrain norm gradients norm gradient exceeds renormalize updating. gradient norm constraint crucial training model. choices largely guided previous work zaremba wordlevel language modeling lstms. finally order speed training data-l employ hierarchical softmax common strategy training language models large |v|—instead usual softmax. pick number exclusive collectively exhaustive subsets equal size. becomes while brown clustering/frequency-based clustering commonly used literature brown clusering) used random clusters implementation enjoys best speed-up number words cluster approximately equal. found random clustering work surprisingly well. table performance model versus neural language models english penn treebank test set. refers perplexity size refers approximate number parameters model. kneser-ney -gram language model serves non-neural baseline. †for models authors explicitly state number parameters hence sizes shown estimates based understanding papers private correspondence respective authors. english penn treebank train versions model assess trade-off performance size. architecture small large models summarized table another baseline also train comparable lstm models word embeddings lstm-word-small uses hidden units lstm-wordlarge uses hidden units. word embedding sizes also respectively. chosen keep number parameters similar corresponding character-level model. seen table large model existing state-of-the-art despite approximately fewer parameters. small model signiﬁcantly outperforms nlms similar size even though penalized fact dataset already words replaced <unk> lower perplexities reported model ensembles include comparable current work. table test perplexities data-s. first rows botha last paper. kneserney -gram language model mlbl best performing morphological logbilinear model botha small/large refer model size word/morph/char models words/morphemes/characters inputs respectively. compare results morphological logbilinear model botha blunsom whose model also takes account subword information morpheme embeddings summed input output layers. comparison mlbl models confounded lstms—widely known outperform feed-forward/log-bilinear cousins—we also train lstm version morphological input representation word given lstm summation word’s morpheme embeddings. concretely suppose morphemes language rn×|m| matrix morpheme embeddings j-th column given input word feed following representation lstm word embedding morphemes word morphemes obtained running unsupervised morphological tagger preprocessing step. emphasize word embedding added morpheme embeddings done botha blunsom morpheme embeddings size small/large models respectively. train wordlevel lstm models another baseline. table test perplexities data-l. first rows botha last three rows small lstm models described paper. kneser-ney -gram language model mlbl best performing morphological logbilinear model botha word/morph/char models words/morphemes/characters inputs respectively. spite again smaller. character models also outperform morphological counterparts although improvements morphological lstms measured. note morpheme models strictly parameters word models word embeddings used part input. memory constraints train small models data-l interestingly observe signiﬁcant differences going word morpheme lstms spanish french english. character models outperform word/morpheme models. also observe signiﬁcant perplexity reductions even english large. conclude section noting used architecture languages perform language-speciﬁc tuning hyperparameters. discussion learned word representations explore word representations learned models ptb. table nearest neighbors word representations learned word-level characterlevel models. character models compare representations obtained highway layers. highway layers representations seem solely rely surface forms—for example nearest neighbors your young four youth close terms edit distance. highway layers however seem enable encoding semantic features discernable orthography alone. highway layers nearest neighbor orthographically distinct you. another example though— words apart edit distance-wise composition model able place near other. model difference parameters greater non-ptb corpora size word model scales faster |v|. example arabic small/large word models parameters corresponding character models parameters respectively. figure plot character n-gram representations english. colors correspond preﬁxes sufﬁxes hyphenated others preﬁxes refer character n-grams start start-of-word character. sufﬁxes likewise refer character n-grams end-of-word character. learned representations words positioned near words part-of-speech. model also able correct incorrect/non-standard spelling indicating potential applications text normalization noisy domains. learned character n-gram representations discussed previously ﬁlter charcnn essentially learning detect particular character n-grams. initial expectation ﬁlter would learn activate different morphemes build semantic representations words identiﬁed morphemes. however upon reviewing character n-grams picked ﬁlters found correspond valid morphemes. better intuition character composition model learning plot learned representations character n-grams principal components analysis feed character n-gram charcnn charcnn’s output ﬁxed dimensional representation corresponding character n-gram. apparent figure model learns differentiate preﬁxes sufﬁxes others also representations particularly sensitive character n-grams containing hyphens presumably strong signal word’s part-of-speech. highway layers quantitatively investigate effect highway network layers ablation studies train model withhighway layers performance decreases signiﬁcantly. difference performance could decrease model size also train model feeds table nearest neighbor words word representations large word-level character-level models trained ptb. last three words words therefore representations word-level model. hypothesize highway networks especially well-suited work cnns adaptively combining local features detected individual ﬁlters. cnns already proven successful many tasks posit gains could achieved employing highway layers existing architectures. also anecdotally note highway layers important highway layers generally resulted similar performance convolutional layers max-pooling help highway layers improve models used word embeddings inputs. effect corpus/vocab sizes next study effect training corpus/vocabulary sizes relative performance different models. take german dataset data-l vary training corpus/vocabulary sizes calculating perplextable perplexity reductions going small word-level character-level models based different corpus/vocabulary sizes german vocabulary size number tokens training set. full vocabulary dataset less hence scenario unavailable. reductions result going small word-level model small character-level model. vary vocabulary size take frequent words replace rest <unk>. previous experiments character model utilize surface forms <unk> simply treats another token. although table suggests perplexity reductions become less pronounced corpus size increases nonetheless character-level model outperforms word-level model scenarios. report experiments observations combining word embeddings charcnn’s output form combined representation word resulted slightly worse performance surprising improvements reported partof-speech tagging named entity recognition concatenating word embeddings output character-level cnn. could model requires additional convolution operations characters thus slower comparable word-level model perform simple lookup input layer found difference manageable optimized implementations—for example large character-level model trained tokens/sec compared word-level model trained tokens/sec. scoring model running time pure word-level model charcnn’s outputs pre-computed words would however expense increased model size thus trade-off made run-time speed memory neural language models encompass rich family neural network architectures language modeling. example architectures include feed-forward recurrent sum-product log-bilinear convolutional networks. order address rare word problem alexandrescu kirchhoff —building analogous work count-based n-gram language models bilmes kirchhoff —represent word shared factor embeddings. factored neural language model incorporate morphemes word shape information annotation represent words. speciﬁc class fnlms leverages morphemic information viewing word function morpheme embeddings example luong socher manning apply recursive neural network morpheme embeddings obtain embedding single word. models proved useful require morphological tagging preprocessing step. another direction work involved purely characterlevel nlms wherein input output characters character-level models obviate need morphological tagging manual feature engineering attractive property able generate novel words. however generally outperformed word-level models experimented concatenation tensor products averaging adaptive weighting schemes whereby model learns convex combination word embeddings charcnn outputs. guimaraes representing word concatenation word embedding output characterlevel using combined representation features conditional random field zhang zhao lecun away word embeddings completely show text classiﬁcation deep characters performs well. ballesteros dyer smith characters train transitionbased parser obtaining improvements many morphologically rich languages. finally ling apply bi-directional lstm characters inputs language modeling part-of-speech tagging. show improvements various languages remains open character composition model performs better. introduced neural language model utilizes character-level inputs. predictions still made word-level. despite fewer parameters model outperforms baseline models utilize word/morpheme embeddings input layer. work questions necessity word embeddings neural language modeling. analysis word representations obtained character composition part model indicates model able encode characters only rich semantic orthographic features. using charcnn highway layers representation learning remains avenue future work. insofar sequential processing words inputs ubiquitous natural language processing would interesting architecture introduced paper viable tasks—for example encoder/decoder neural machine translation", "year": 2015}