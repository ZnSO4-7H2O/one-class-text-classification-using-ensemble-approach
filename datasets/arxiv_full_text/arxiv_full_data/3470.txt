{"title": "Learning Transferable Architectures for Scalable Image Recognition", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Developing neural network image classification models often requires significant architecture engineering. In this paper, we attempt to automate this engineering process by learning the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. Our key contribution is the design of a new search space which enables transferability. In our experiments, we search for the best convolutional layer (or \"cell\") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters. Although the cell is not searched for directly on ImageNet, an architecture constructed from the best cell achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS -- a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of our models exceed those of the state-of-the-art human-designed models. For instance, a smaller network constructed from the best cell also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. On CIFAR-10, an architecture constructed from the best cell achieves 2.4% error rate, which is also state-of-the-art. Finally, the image features learned from image classification can also be transferred to other computer vision problems. On the task of object detection, the learned features used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset.", "text": "cation represents important breakthroughs deep learning. successive advancements benchmark based convolutional neural networks achieved impressive results signiﬁcant architecture engineering paper consider learning convolutional architectures directly data application imagenet classiﬁcation. addition difﬁcult important benchmark computer vision features derived imagenet classiﬁers great importance many computer vision tasks. example features networks perform well imagenet classiﬁcation provide stateof-the-art performance transferred computer vision tasks labeled data limited approach inspired recently proposed neural architecture search framework uses policy gradient algorithm optimize architecture conﬁgurations. even though attractive method search good convolutional network architectures applying directly imagenet dataset computationally expensive given size dataset. therefore propose search good architecture smaller cifar- dataset automatically transfer learned architecture imagenet. achieve transferrability designing search space complexity architecture independent depth network size input images. concretely convolutional networks search space composed convolutional layers identical structure different weights. searching best convolutional architectures therefore reduced searching best cell structure. searching best cell structure main beneﬁts much faster searching entire network architecture cell likely generalize problems. experiments approach signiﬁcantly accelerates search best architectures using cifar- factor learns architectures successfully transfer imagenet. developing neural network image classiﬁcation models often requires signiﬁcant architecture engineering. paper attempt automate engineering process learning model architectures directly dataset interest. approach expensive dataset large propose search architectural building block small dataset transfer block larger dataset. contribution design search space enables transferability. experiments search best convolutional layer cifar- dataset apply cell imagenet dataset stacking together copies cell parameters. although cell searched directly imagenet architecture constructed best cell achieves among published works state-of-the-art accuracy top- top- imagenet. model better top- accuracy best human-invented architectures having billion fewer flops reduction computational demand previous state-of-the-art model. evaluated different levels computational cost accuracies models exceed state-of-theart human-designed models. instance smaller network constructed best cell also achieves accuracy better equivalently-sized state-of-the-art models mobile platforms. cifar architecture constructed best cell achieves error rate also state-of-the-art. finally image features learned image classiﬁcation also transferred computer vision problems. task object detection learned features used faster-rcnn framework surpass state-of-the-art achieving coco dataset. tion. imagenet architecture constructed best cell achieves among published works state-ofthe-art accuracy top- top-. result amounts improvement top- accuracy best human-invented architectures billion fewer flops. cifar- itself architecture achieves error rate also state-of-the-art. additionally simply varying number convolutional cells number ﬁlters convolutional cells create convolutional architectures different computational demands. thanks property cells generate family models achieve accuracies superior human-invented models equivalent smaller computational budgets notably smallest version learned model achieves accuracy imagenet better previously engineered architectures targeted towards mobile embedded vision tasks finally show image features learned image classiﬁcation generically useful transfer computer vision problems. experiments features learned imagenet classiﬁcation combined faster-rcnn framework achieve state-of-the-art coco object detection task largest well mobile-optimized models. largest model achieves better previous state-of-the-art. work makes search methods good convolutional architectures dataset interest. main search method work neural architecture search framework proposed controller recurrent neural network samples child networks different architectures. child networks trained convergence obtain accuracy held-out validation set. resulting accuracies used update controller controller generate better architectures time. controller weights updated policy gradient main contribution work design novel search space best architecture found cifar- dataset would scale larger higherresolution image datasets across range computational settings. inspiration search space recognition architecture engineering cnns often identiﬁes repeated motifs consisting combinations convolutional ﬁlter banks nonlinearities prudent selection connections achieve state-of-the-art results observations suggest possible controller predict generic convolutional cell expressed terms motifs. cell figure overview neural architecture search controller predicts architecture search space probability child network architecture trained convergence achieving accuracy scale gradients update controller. approach overall architectures convolutional nets manually predetermined. composed convolutional cells repeated many times convolutional cell architecture different weights. easily build scalable architectures images size need types convolutional cells serve main functions taking feature input convolutional cells return feature dimension convolutional cells return feature feature height width reduced factor two. name ﬁrst type second type convolutional cells normal cell reduction cell respectively. reduction cell make initial operation applied cell’s inputs stride reduce height width. operations consider building convolutional cells option striding. figure shows placement normal reduction cells cifar- imagenet. note imagenet reduction cells since incoming image size compared cifar. reduction normal cell could architecture empirically found beneﬁcial learn separate architectures. common heuristic double number ﬁlters output whenever spatial activation size reduced order maintain roughly constant hidden state dimension importantly much like inception resnet models consider number motif repetitions number initial convolutional ﬁlters free parameters tailor scale image classiﬁcation problem. step controller selects method combine hidden states either element-wise addition between hidden states concatenation hidden states along ﬁlter dimension. finally unused hidden states generated convolutional cell concatenated together depth provide ﬁnal cell output. allow controller predict normal cell reduction cell simply make controller predictions total ﬁrst predictions normal cell second predictions reduction cell. finally work makes reinforcement learning proposal intensively; however also possible random search search models search space. random search instead sampling decisions softmax classiﬁers controller sample decisions uniform distribution. experiments random search worse reinforcement learning cifar- dataset. although value using reinforcement learning smaller found original work result suggests search space well designed random search perform reasonably well. compare reinforcement learning random search section section describe experiments method described learn convolutional cells. summary architecture searches performed using cifar- classiﬁcation task controller trained using proximal policy optimization employing global workqueue system generating pool child networks controlled rnn. experiments pool workers workqueue consisted gpus. please appendix complete details architecture learning algorithm controller system. figure scalable architectures image classiﬁcation consist repeated motifs termed normal cell reduction cell. diagram highlights model architecture cifar- imagenet. choice number times normal cells gets stacked reduction cells vary experiments. within search space deﬁned follows. search space cell receives input initial hidden states outputs cells previous lower layers input image. controller recursively predicts rest structure convolutional cell given initial hidden states predictions controller cell grouped blocks block prediction steps made distinct softmax classiﬁers corresponding discrete choices elements block algorithm appends newly-created hidden state existing hidden states potential input subsequent blocks. controller repeats prediction steps times corresponding blocks convolutional cell. experiments selecting figure controller model architecture recursively constructing block convolutional cell. block requires selecting discrete parameters corresponds output softmax layer. example constructed block shown right. convolutional cell contains blocks hence controller contains softmax layers predicting architecture convolutional cell. experiments number blocks result search process days yields several candidate convolutional cells. note search procedure almost faster previous approaches took days. additionally demonstrate resulting architecture superior accuracy. figure shows diagram performing normal cell reduction cell. note prevalence separable convolutions number branches compared competing architectures subsequent experiments focus convolutional cell architecture although examine efﬁcacy other top-ranked convolutional cells imagenet experiments report results well. call three networks constructed best three searches nasneta nasnet-b nasnet-c. demonstrate utility convolutional cells employing learned architecture cifar- family imagenet classiﬁcation tasks. latter family tasks explored across orders magnitude computational budget. learned convolutional cells several hyper-parameters explored build ﬁnal network given task number cell repeats number ﬁlters initial convolutional cell. selecting number initial ﬁlters common heuristic double number ﬁlters whenever stride finally deﬁne simple notation e.g. indicate parameters networks indicate number cell repeats number ﬁlters penultimate layer network respectively. particular note previous architecture search used gpus days resulting gpu-hours. method paper uses gpus across days resulting gpu-hours. former effort used nvidia gpus whereas current efforts used faster nvidia discounting fact faster hardware estimate current procedure roughly efﬁcient. task image classiﬁcation cifar- test accuracies best architectures reported table along state-of-the-art models. seen table large nasnet-a model cutout data augmentation achieves state-of-the-art error rate slightly better previous best record best single model achieves error rate. performed several sets experiments imagenet best convolutional cells learned cifar-. emphasize merely transfer architectures cifar- train imagenet models weights scratch. results summarized table figure ﬁrst experiments train several image classiﬁcation systems operating resolution images different experiments scaled computational demand create models roughly computational cost inception-v inception-v polynet show family models achieve state-of-the-art performance fewer ﬂoating point operations parameters comparable architectures. second demonstrate adjusting scale model achieve state-of-the-art performance smaller computational budgets exceeding streamlined cnns hand-designed operating regime note residual connections convolutional cells models learn skip connections own. empirically found manually inserting residual connections cells help performance. training setup imagenet similar please appendix details. table shows figure architecture best convolutional cells blocks identiﬁed cifar- input hidden state previous activations output result concatenation operation across resulting branches. convolutional cell result blocks. single block corresponds primitive operations combination operation note colors correspond operations figure lems. particular model based convolutional cells exceeds predictive performance corresponding hand-designed model. importantly largest model achieves state-of-the-art performance imagenet based single non-ensembled predictions surpassing previous best published result among unpublished works model best reported result significantly fewer ﬂoating point operations. figure shows complete summary results comparison finally test well best convolutional cells perform resource-constrained setting e.g. mobile devices settings number ﬂoating point operations severely constrained predictive performance must weighed latency requirements device limited computational resources. mobilenet shufﬂenet provide state-of-the-art refigure accuracy versus computational demand number parameters across performing published architectures imagenet ilsvrc challenge prediction task. computational demand measured number ﬂoating-point multiplyadd operations process single image. black circles indicate previously published results squares highlight proposed models. table performance architecture search published state-of-the-art models imagenet classiﬁcation. mult-adds indicate number composite multiply-accumulate operations single image. note composite multiple-accumulate operations calculated image size reported table. model size calculated open-source implementation. table performance imagenet classiﬁcation subset models operating constrained computational setting i.e. multiply-accumulate operations image. models images. sults obtaining accuracy respectively images using multliply-add operations. architecture constructed best convolutional cells achieves superior predictive performance surpassing previous models comparable computational demand. summary learned convolutional cells ﬂexible across model scales achieving state-of-the-art performance across almost orimage classiﬁcation networks provide generic image features transferred computer vision problems important problems spatial localization objects within image. validate performance family nasnet-a networks test whether object detection systems derived nasnet-a lead improvements object detection address question plug family nasnet-a networks pretrained imagenet faster-rcnn object detection pipeline using opensource software platform retrain resulting object detection pipeline combined coco training plus validation dataset excluding mini-validation images. perform single model evaluation using proposals image. words pass single image single network. evaluate model coco mini-val test-dev dataset report mean average precision computed standard coco metric library perform simple search learning rate schedules identify best possible model. finally examine behavior object detection systems employing best performing nasnetimage featurization well image featurization geared towards mobile platforms mobile-optimized network resulting system achieves exceeding previous mobileoptimized networks employ faster-rcnn best nasnet network resulting network operating images spatial resolution achieves exceeding equivalent object detection systems based lesser performing image featurization finally increasing spatial resolution input image results best reported single model result object detection surpassing best previous best results provide evidence nasnet provides superior generic image features transferred across computer vision tasks. figure figure appendix show four examples object detection results produced nasnet-a faster-rcnn framework. primary advance best reported object detection system introduction novel loss pairing loss nasnet-a image featurization lead even performance gains. additionally performance gains achievable ensembling multiple inferences across multiple model instances image crops figure measuring efﬁciency random search reinforcement learning learning neural architectures. x-axis measures total number model architectures sampled y-axis validation performance epochs proxy cifar- training task. emphasize absolute performance proxy task important relative gain initial state. pair curves measures mean accuracy across ranking models identiﬁed algorithm. open question proposed method training efﬁciency architecture search algorithm. section demonstrate effectiveness reinforcement learning architecture search cifar- image classiﬁcation problem compare brute-force random search given equivalent amount computational resources. deﬁne effectiveness architecture search algorithm increase model performance initial architecture identiﬁed search method. importantly emphasize absolute value model performance proxy task less important artiﬁcially reﬂects irrelevant factors employed architecture search process e.g. number training epochs speciﬁc model construction. thus employ increase model performance proxy judging convergence architecture search algorithm. figure shows performance reinforcement learning random search model architectures sampled. note best model identiﬁed signiﬁcantly better best model found measured proxy classiﬁcation task cifar-. additionally ﬁnds entire range models superior quality random search. observe mean performance top- top- table object detection performance coco mini-val test-dev datasets across variety image featurizations. results faster-rcnn object detection framework single crop image. rows highlight mobile-optimized image featurizations bottom rows indicate computationally heavy image featurizations geared towards achieving best results. mini-val results employ subset validation images models identiﬁed versus take results indicate although provide viable search strategy signiﬁcantly improve ability learn neural architectures. proposed method related previous work hyperparameter optimization especially recent approaches designing architectures neural fabrics diffrnn metaqnn deeparchitect ﬂexible class methods designing architecture evolutionary algorithms much success large scale. yuille also transferred learned architectures cifar- imagenet performance models notably previous state-of-the-art concept neural network interact second neural network learning process learning learn meta-learning attracted much attention recent years approaches scaled large problems like imagenet. exception recent work focused learning optimizer imagenet classiﬁcation achieved notable improvements design search space took much inspiration lstms neural architecture search cell modular structure convolutional cell also related previous methods imagenet inception resnet/resnext xception/mobilenet classiﬁcation tasks. learned architecture quite ﬂexible scaled terms computational cost parameters easily address variety problems. cases accuracy resulting model exceeds human-designed models ranging models designed mobile applications computationally-heavy models designed achieve accurate results. insight approach design search space decouples complexity architecture depth network. resulting search space permits identifying good architectures small dataset transferring learned architecture image classiﬁcations across range data computational scales. resulting architectures approach exceed stateof-the-art performance cifar- imagenet datasets less computational demand humandesigned architectures imagenet results particularly important many state-of-theart computer vision problems face detection image localization derive image features architectures imagenet classiﬁcation models. instance image features obtained imagenet used combination fasterrcnn framework achieves state-of-the-art object detection results. finally demonstrate resulting learned architecture perform imagenet classiﬁcation reduced computational budgets outperform streamlined architectures targeted mobile embedded platforms", "year": 2017}