{"title": "Order-Embeddings of Images and Language", "tag": ["cs.LG", "cs.CL", "cs.CV"], "abstract": "Hypernymy, textual entailment, and image captioning can be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. In this paper we advocate for explicitly modeling the partial order structure of this hierarchy. Towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.", "text": "ivan vendrov ryan kiros sanja fidler raquel urtasun department computer science university toronto {vendrovrkirosfidlerurtasun}cs.toronto.edu hypernymy textual entailment image captioning seen special cases single visual-semantic hierarchy words sentences images. paper advocate explicitly modeling partial order structure hierarchy. towards goal introduce general method learning ordered representations show applied variety tasks involving images language. show resulting representations improve performance current approaches hypernym prediction image-caption retrieval. computer vision natural language processing becoming increasingly intertwined. recent work vision moved beyond discriminating ﬁxed object classes automatically generating open-ended lingual descriptions images recent methods natural language processing young learn semantics language grounding visual world. looking future autonomous artiﬁcial agents need jointly model vision language order parse visual world communicate people. what precisely relationship images words captions describe them? akin hypernym relation words textual entailment among phrases captions simply abstractions images. fact three relations seen special cases partial order images language illustrated figure refer visualsemantic hierarchy. partial order relation transitive woman walking woman walking person walking person entity valid abstractions rightmost image. goal work learn representations respect partial order structure. recent approaches modeling hypernym entailment image-caption relations involve learning distributed representations embeddings. powerful general approach maps objects interest—words phrases images— points high-dimensional vector space. line work exempliﬁed chopra ﬁrst applied caption-image relationship socher requires mapping distance-preserving semantically similar objects mapped points nearby embedding space. symmetric distance measure euclidean cosine distance typically used. since visual-semantic hierarchy antisymmetric relation expect approach introduce systematic model error. approaches explicit constraints learning more-or-less general binary relation objects interest e.g. bordes socher notably existing approach directly imposes transitivity antisymmetry partial order leaving model induce properties data. contrast propose exploit partial order structure visual-semantic hierarchy learning mapping distance-preserving order-preserving visualsemantic hierarchy partial order embedding space. call embeddings learned order-embeddings. idea integrated existing relational learning methods simply replacing comparison operation ours. modifying existing methods order-embeddings provide marked improvement state-of-art hypernymy prediction caption-image retrieval near state-of-the-art performance natural language inference. paper structured follows. begin section giving uniﬁed mathematical treatment tasks describing general approach learning order-embeddings. next three sections describe detail tasks tackle apply order-embeddings idea them results obtain. tasks hypernym prediction captionimage retrieval textual entailment unify treatment various tasks introduce problem partial order completion. partial order completion given positive examples ordered pairs drawn partially ordered negative examples know unordered. goal predict whether unseen pair ordered. note hypernym prediction caption-image retrieval textual entailment special cases task since involve classifying pairs concepts visual-semantic hierarchy. tackle problem learning mapping partially ordered embedding space idea predict ordering unseen pair based ordering embedding space. possible mapping satisﬁes following crucial property deﬁnition function order-embedding deﬁnition implies combination embedding space order orderembedding determines unique completion data partial order following ﬁrst consider choice discuss appropriate reversed product order choice somewhat application-dependent. purpose modeling semantic hierarchy choices narrowed following considerations. much expressive power human language comes abstraction composition. concepts name concept abstraction mammal well concept composes chasing cat. order represent visual-semantic hierarchy need choose order rich enough embed relations. also restrict orders element every element order. visual-semantic hierarchy element represents general possible concept; practically provides anchor embedding. finally choose embedding space continuous order allow optimization gradient-based methods. natural choice satisﬁes three properties reversed product order conjunction total orders coordinate vectors nonnegative coordinates. note reversal direction smaller coordinates imply higher position partial order. origin element order representing general concept. instead viewing embeddings single points also view sets meaning word union concepts hypernym meaning sentence union sentences entail visual-semantic hierarchy seen special case subset relation connection also used young ﬁxed embedding space order consider problem ﬁnding orderembedding space. practice order embedding condition restrictive impose hard constraint. instead approximate order-embedding mapping violates order-embedding condition imposed soft constraint little possible. crucially according reversed product order; order satisﬁed positive. effectively imposes strong prior space relations encouraging learned relation satisfy partial order properties transitivity antisymmetry. penalty method. throughout remainder paper previous work used symmetric distances learned comparison operators. recall positive negative examples respectively. then learn approximate order-embedding could max-margin loss encourages positive examples zero penalty negative examples penalty greater margin practice often given negative examples case loss admits trivial solution mapping objects point. best dealing problem depends application describe task-speciﬁc variations loss next several sections. test ability model learn partial orders incomplete data ﬁrst task predict withheld hypernym pairs wordnet hypernym pair pair concepts ﬁrst concept specialization instance second e.g. setup differs signiﬁcantly previous work wordnet hierarchy training data. similar evaluation baroni external linguistic data form distributional semantic vectors. bordes socher also evaluate wordnet hierarchy relations wordnet training data additionally latter consider direct hypernyms rather full transitive hypernymy relation. predicting transitive hypernym relation better-deﬁned problem individual hypernym edges wordnet vary dramatically degree abstraction require. instance direct hypernym pair takes eight hypernym edges organism. apply order-embeddings hypernymy follow setup socher learning n-dimensional vector concept wordnet replace neural tensor network order-violation penalty deﬁned like them corrupt hypernym pair replacing concepts randomly chosen concept corrupted pairs negative examples training evaluation. max-margin loss encourages order-violation penalty zero positive examples greater margin transitive closure wordnet hierarchy gives edges concepts wordnet. like bordes randomly select edges test split another development set. note majority test edges inferred simply applying transitivity giving strong baseline. learn -dimensional nonnegative vector concept wordnet using max-margin objective margin sampling true false hypernym pairs batch. train epochs using adam optimizer learning rate early stopping validation set. evaluation optimal classiﬁcation threshold validation apply test set. since setup novel published numbers compare therefore compare three variants model baselines results shown table transitive closure baseline involves learning; simply classiﬁes hypernyms pairs positive transitive closure union edges training validation sets. wordgauss baseline evaluates approach vilnis mccallum represent words gaussian densities rather points embedding space. allows natural representation hierarchies using divergence. used -dimensional diagonal gaussian embeddings trained epochs max-margin objective margin chosen grid search. order-embeddings full model using symmetric cosine distance instead asymmetric penalty. order-embeddings replaces penalty bilinear model used socher order-embeddings full model. full model better transitive baseline showing value exploiting partial order structure contrast using symmetric similarity learning general binary relation previous work bilinear baseline figure -dim order-embedding small subset wordnet hypernym relation. true hypernym pairs correctly embedded spurious pairs introduced. direct hypernyms shown. caption-image retrieval task become standard evaluation joint models vision language task involves ranking large dataset images relevance query caption ranking captions relevance query image given aligned image-caption pairs training data goal learn caption-image compatibility score used test time. many modern approaches model caption-image relationship symmetrically either embedding common visual-semantic space inner-product similarity using canonical correlations analysis distributed representations images captions karpathy plummer model ﬁner-grained alignment regions image segments caption similarity still symmetric. alternative learn unconstrained binary relation either neural language model conditioned image using multimodal facilitate comparison pairwise ranking loss socher kiros karpathy used task—simply replacing symmetric similarity measure asymmetric order-violation penalty. loss function encourages ground truth caption-image pairs greater pairs margin learned matrix dimensionality embedding space. image feature used klein rescale images smallest side pixels take crops corners center horizontal reﬂections crops -layer network simonyan zisserman average features. table results caption-image retrieval evaluation coco. recallk median rank. metrics models test images averages -image splits -image test best results overall bold; best results using -crop features underlined. evaluate microsoft coco dataset images least human-annotated captions image. largest dataset commonly used caption-image retrieval. data splits karpathy training validation test train model standard pairwise ranking objective sample minibatches random image-caption pairs draw contrastive terms minibatch giving contrastive images caption captions image. train epochs using adam optimizer learning rate early stopping validation set. dimension embedding space hidden state dimension learned word embeddings margin hyperparameters well learning rate batchsize selected using validation set. consistency kiros mitigate overﬁtting constrain caption image embeddings unit norm. constraint implies points exactly ordered zero order-violation penalty since ranking loss relative size penalties matters. given query caption image sort images captions test order increasing penalty. standard ranking metrics evaluation. measure recallk percent queries term ﬁrst retrieved; median mean rank statistics position term retrieval order. facilitate comparison evaluate contributions various components model evaluate four variations order-embeddings order-embeddings full model described above. order-embeddings reverses order captions image embeddings orderviolation penalty—placing images captions partial order learned model. seemingly slight variation performs atrociously conﬁrming prior captions much abstract images placed higher semantic hierarchy. order-embeddings computes image feature using center crop instead averaging crops. order-embeddings replaces asymmetric penalty symmetric cosine distance allows embedding coordinates negative—essentially replicating mnlm better image features. different margin works best. four models previous work whose results incommensurable dvsa since uses less discriminative krizhevsky region features instead single whole-image feature. aside limitation single models considered order-embeddings signiﬁcantly outperform state-of-art approaches image retrieval even control image features. intuitively symmetric similarity fail image captions different levels detail captions dissimilar impossible embeddings close image embedding. order-embeddings don’t problem less detailed caption embedded away image remaining partial order. evaluate intuition caption length proxy level detail select among pairs co-referring captions validation pairs biggest length difference. image retrieval target images mean rank captions orderembeddings cosine similarity much bigger difference entire dataset. particularly dramatic examples shown figure moreover shorter caption query retrieve captions order increasing error mean rank longer caption order-embeddings cosine similarity showing order-embeddings able capture relatedness co-referring captions different lengths. also explains order-embeddings provide much smaller improvement caption retrieval image retrieval caption retrieval metrics based position ﬁrst ground truth caption retrieval order embeddings need learn retrieve image’s captions well symmetric similarity well suited for. natural language inference seen generalization hypernymy words sentences. example woman walking park infer woman walking park woman black dog. given pair sentences task predict whether infer second sentence ﬁrst evaluate order-embeddings natural language inference task recently proposed snli corpus contains pairs sentences labeled entailment inference valid contradiction sentences contradict neutral inference invalid contradiction. method allows discriminate entailment non-entailment merge contradiction neutral classes together serve negative examples. caption-image ranking dimensions embedding space hidden state dimension word embeddings constrain embeddings unit norm. train epochs batches sentence pairs. adam optimizer learning rate early stopping validation set. evaluation optimal classiﬁcation threshold validation threshold classify test set. bridge facilitate comparison challenging baseline evaluated -class -class problems. baseline referred skip-thoughts involves feedforward neural network skip-thought vectors state-of-the-art semantic representation sentences. given pairs sentence vectors input network concatenation absolute difference tuned number layers layer dimensionality dropout rates optimize performance development using adam optimizer. batch normalization prelu units used. best network used hidden layers units each dropout rate across input hidden layers. backpropagate skip-thought encoder. also evaluate classiﬁer -class baseline introduced version model order-violation penalty replaced symmetric cosine distance order-embeddings results models shown table order-embeddings outperform skipthought baseline despite using external text corpora. method almost certainly worse state-of-the-art method rockt¨aschel uses word-by-word attention mechanism also much simpler. introduced simple method encode order learned distributed representations allows explicitly model partial order structure visual-semantic hierarchy. method easily integrated existing relational learning methods demonstrated three challenging tasks involving computer vision natural language processing. tasks hypernym prediction caption-image retrieval methods outperform previous work. promising direction future work learn better classiﬁers imagenet image classes arranged wordnet hierarchy. previous approaches including frome norouzi embedded words images shared semantic space symmetric similarity—which experiments suggest poor partial order structure wordnet. expect signiﬁcant progress imagenet classiﬁcation related problems one-shot zero-shot learning possible using order-embeddings. going further order-embeddings enable learning entire semantic hierarchy single model jointly reasons hypernymy entailment relationship perception language unifying almost independent lines work. kyunghyun merri¨enboer bart gulcehre caglar bahdanau dzmitry bougares fethi schwenk holger bengio yoshua. learning phrase representations using encoderdecoder statistical machine translation. emnlp dahua fidler sanja kong chen urtasun raquel. visual semantic search retrieving videos complex textual queries. proceedings ieee conference computer vision pattern recognition tsung-yi maire michael belongie serge hays james perona pietro ramanan deva doll´ar piotr zitnick lawrence. microsoft coco common objects context. eccv norouzi mohammad mikolov tomas bengio samy singer yoram shlens jonathon frome andrea corrado greg dean jeffrey. zero-shot learning convex combination semantic embeddings. iclr plummer bryan wang liwei cervantes chris caicedo juan hockenmaier julia lazebnik svetlana. flickrk entities collecting region-to-phrase correspondences richer image-tosentence models. arxiv preprint arxiv. rockt¨aschel grefenstette edward hermann karl moritz koˇcisk`y tom´aˇs blunsom phil. reasoning entailment neural attention. arxiv preprint arxiv. young peter alice hodosh micah hockenmaier julia. image descriptions visual denotations similarity metrics semantic inference event descriptions. tacl mikolov showed word representations learned using wordvec exhibit semantic regularities king woman queen. kiros showed similar regularities hold joint image-language models. order-embeddings exhibit novel form regularity shown figure elementwise operations embedding space roughly correspond composition abstraction respectively. figure multimodal regularities found embeddings learned caption-image retrieval task. note images slightly cropped easier viewing relevant objects removed.", "year": 2015}