{"title": "Hyperparameter Optimization: A Spectral Approach", "tag": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "abstract": "We give a simple, fast algorithm for hyperparameter optimization inspired by techniques from the analysis of Boolean functions. We focus on the high-dimensional regime where the canonical example is training a neural network with a large number of hyperparameters. The algorithm --- an iterative application of compressed sensing techniques for orthogonal polynomials --- requires only uniform sampling of the hyperparameters and is thus easily parallelizable.  Experiments for training deep neural networks on Cifar-10 show that compared to state-of-the-art tools (e.g., Hyperband and Spearmint), our algorithm finds significantly improved solutions, in some cases better than what is attainable by hand-tuning. In terms of overall running time (i.e., time required to sample various settings of hyperparameters plus additional computation time), we are at least an order of magnitude faster than Hyperband and Bayesian Optimization. We also outperform Random Search 8x.  Additionally, our method comes with provable guarantees and yields the first improvements on the sample complexity of learning decision trees in over two decades. In particular, we obtain the first quasi-polynomial time algorithm for learning noisy decision trees with polynomial sample complexity.", "text": "give simple fast algorithm hyperparameter optimization inspired techniques analysis boolean functions. focus high-dimensional regime canonical example training neural network large number hyperparameters. algorithm iterative application compressed sensing techniques orthogonal polynomials requires uniform sampling hyperparameters thus easily parallelizable. experiments training deep neural networks cifar- show compared state-of-the-art tools algorithm ﬁnds signiﬁcantly improved solutions cases better attainable hand-tuning. terms overall running time least order magnitude faster method inspired provably-eﬃcient algorithms learning decision trees using discrete fourier transform. obtain improved sample-complexty bounds learning decision trees matching state-of-the-art bounds running time large scale machine learning optimization systems usually involve large number free parameters user according application. timely example training deep neural networks signal processing application specialist needs decide architecture depth network choice connectivity layer choice optimization algorithm recursively choice parameters inside optimization library given hyperparameters potential assignments naive practice search entire grid parameter assignments pick performed best a.k.a. grid search. number hyperparameters increases number possible assignments increases exponentially grid search becomes quickly infeasible. thus crucial method automatic tuning parameters. auto-tuning ﬁnding good setting parameters referred hyperparameter optimization simply automatic machine learning continuous hyperparameters gradient descent usually method choice discrete parameters however choice architecture number layers connectivity forth signiﬁcantly challenging. formally function mapping hyperparameter choices test error model. dimension corresponds certain hyperparameter simplicity illustration encode choices parameter binary numbers third point important since clearly information-theoretically hard evaluations function necessary worst case. diﬀerent works considered exploiting properties above. approach bayesian optimization addresses structure assumes useful prior distribution structure known advance. multi-armed bandit algorithms random search exploit computational parallelism well exploit particular structure approaches surveyed detail later. paper introduce spectral approach hyperparameter optimization. main idea make assumptions structure fourier domain. speciﬁcally assume approximated sparse degree polynomial fourier basis. means intuitively approximated well decision tree. implication assumption obtain rigorous theoretical guarantee approximate minimization boolean hypercube function evaluations linear sparsity carried parallel. give improved heuristics basic construction show experiments showing assumptions validated practice applied deep learning image datasets. variables uniform distribution. observe classical sample complexity bound linial quadratic size tree matching best known quasipolynomial bound running time. demonstrate signiﬁcant improvements accuracy sample complexity running time deep neural training experiments. compare state-of-the-art solvers bayesian optimization multi-armed bandit techniques random search. projecting even higher numbers hyperparameters perform simulations show several orders-of-magnitude speedup versus bayesian optimization techniques. literature discrete-domain roughly divided probabilistic approaches decision-theoretic methods. critical applications researchers usually grid search parameter space becomes quickly prohibitive number hyperparameter grows. gradient-based methods applicable continuous hyperparameters consider. probabilistic methods bayesian optimization. bayesian optimization algorithms tune hyperparameters assuming prior distribution loss function keep updating prior distribution based observations. observation selected according acquisition function balances exploration exploitation observation gives better result helps gain information loss function. approach inherently serial diﬃcult parallelize theoretical guarantees thus limited statistical consistency decision-theoretic methods. perhaps simplest approach random sampling diﬀerent choices parameters picking best amongst chosen evaluations naturally easy implement parallelize. upon simple technique researchers tried allocate diﬀerent budgets diﬀerent evaluations depending early performance. using adaptive resource allocation techniques found multi-armed bandit literature successive halving algorithm introduced hyperband improves automatically tuning hyperparameters learning decision trees. prior work learning decision trees used celebrated low-degree algorithm linial mansour nisan algorithm uses random sampling estimate low-degree fourier coeﬃcient high accuracy. make approach stobbe krause showed learn low-degree sparse boolean functions using tools compressed sensing negahban shah observe approach extended learn functions approximately sparse approximately low-degree implies ﬁrst decision tree learning algorithm polynomial sample complexity handles adversarial noise. addition obtain optimal dependence error parameter problem learning exactly k-sparse boolean functions variables haviv regev recently shown uniformly random samples suﬃce. result algorithmic provide upper bound information-theoretic problem many samples required learn. best algorithm terms running time learning k-sparse boolean functions requires time based blum kalai wasserman algorithm learning parities noise techniques. methods heavily based known results analysis boolean functions well compressed sensing. relevant material literature given next section. context hyperparameter optimization function evaluation expensive although parallelizable corresponds training deep neural net. contrast computation involve function evaluation considered less expensive computations require time somewhat large subexponential paper work exclusively parity basis. results apply generally however orthogonal family polynomials example wished work continuous hyperparameters could work families hermite orthogonal polynomials respect multivariate spherical gaussian distributions. compressed sensing sparse recovery problem sparse recovery learner attempts recover sparse vector sparse i.e. observation vector assumed equal according random orthonormal family. concretely non-zero entries. k-bounded random orthonormal family independent draws corresponding distribution deﬁne matrix rauhut gives following result recovering sparse vectors main component spectral algorithm hyperparameter optimization given algorithm essentially extension sparse recovery orthogonal basis polynomials addition optimization step. figure illustration. prove harmonica’s theoretical guarantee show gives rise theoretical results learning uniform distribution. theorem {ψs} k-bounded orthonormal polynomial basis distribution -bounded function deﬁnition respect basis algorithm time sample complexity returns theorem indeed theoretical results paper follow main recovery properties procedure main technical lemma follows outline compressed sensing result stobbe krause generalization functions approximately sparse low-degree lemma {ψs} k-bounded orthonormal polynomial basis distribution -bounded deﬁnition respect basis procedure ﬁnds function time sample complexity proof lemma ease notation assume -bounded equivalently write degree polynomial includes coeﬃcients magnitude least constant term polynomial expansion ˆfs| fact sparse. function thus remaining terms included draw random labeled examples enumerate basis functions ψn}. form matrix consider problem recovering sparse given vector coeﬃcients entry equals prove constant probability choice random examples observing σs/ε+ recover parseval’s identity. note however rescale con) obtain error incur additional constant magnitude ε/s. fact parseval’s identity thus therefore triangle inequality remains bound note since examples chosen independently entries independent random variables. since linear combination orthonormal monomials ez∼d] remark note proof also holds adversarial agnostic noise setting. adversary could noise vector labels received learner. recover polynomial squared-error re-scaling constant factor noisy recovery lemma basis enhanced algorithm next section well learning-theoretic result learning decision trees detailed next subsection imply recovery global optimum. reason noisy recovery guarantees output hypothesis close underlying function even single noisy point completely change optimum. proof theorem polynomials enumeration polynomials draw labeled examples independently construct matrix since written sparse linear combination exists s-sparse vector entry hence apply theorem recover exactly. non-zero coeﬃcients expansion terms {ψs}. since recovered corollary class decision trees size variables. learnable respect uniform distribution time sample complexity further labels corrupted arbitrary noise vector known structural facts decision trees tree size )-concentrated norm bounded fact function norm bounded exists sparse function noise tolerance property follows immediately comparison low-degree algorithm. prior work learning decision trees used celebrated low-degree algorithm linial mansour nisan algorithm uses random sampling estimate low-degree fourier coeﬃcient high accuracy. contrast compressed-sensing approach stobbe krause takes advantage incoherence design matrix gives results seem unattainable lowdegree algorithm. rather applying algorithm directly found performance greatly enhanced iteratively using procedure estimate inﬂuential hyperparameters optimal values. rest section describe iterative heuristic essentially runs algorithm multiple stages. concretely continue invoke subroutine search space becomes small enough base hyperparameter optimizer space minimizing assignments multivariate polynomial highly non-convex contain many distinct points. such take average several best minimizers stage. scalability. hidden function s-sparse harmonica sparse function using samples. every stage harmonica target function approximated sparse function need samples number stages. real world applications deep neural network hyperparameter tuning seems reasonable assume hidden function indeed sparse every stage optimization time. harmonica runs lasso algorithm stage solve well studied convex optimization problem fast implementations. hyperband also eﬃcient terms running time function number function evaluations require sorting simple computations. running time bayesian optimization cubic number function evaluations limits applicability large number evaluations high dimensionality shall section parallelizability. harmonica similar hyperband random search straightforward parallel implementations. every stage algorithms could simply evaluate objective functions randomly chosen points parallel. compare harmonica spearmint hyperband random search. spearmint hyperband state-of-the-art algorithms observed random search competitive benchmark beats many algorithms. ﬁrst experiment training residual network cifar- dataset. included binary hyperparameters including initialization optimization method learning rate schedule momentum rate etc. table details hyperparameters considered. also include dummy variables make task challenging. notice hyperband random search agnostic dummy variables sense value dummy variables randomly therefore select essentially conﬁgurations without dummy variables. harmonica spearmint sensitive dummy variables learn high dimensional function space. make fair comparison spearmint without dummy variables. hyperparameters consistent eﬀect network becomes deeper common hand-tuning strategy tune small network apply knowledge network harmonica also exploit strategy selects important features stage-by-stage. speciﬁcally feature selection stages harmonica tuning layer neural network training epochs. stage take samples extract important features restriction size that important features random search base algorithm layer neural network training whole epochs. clarify stage means stages hyperparameter algorithms epoch means epochs training neural network. tried three versions harmonica experiment harmonica stage stages stages base algorithm. test error results running times diﬀerent algorithms depicted figure based algorithms return fewer results. runs variants harmonica resulting test error figure python implementation harmonica found https//github.com/callowbird/harmonica https//github.com/hips/spearmint.git implemented parallel version hyperband lua. e.g. https//github.com/facebook/fb.resnet.torch other algorithms like spearmint hyperband etc. used base algorithms well. test error scalability harmonica- uses less time hyperband time compared random search gets better results competing algorithms. beats random search benchmark harmonica- uses slightly time able better results comparable spearmint running time. improving upon human-tuned parameters harmonica- obtains better test error compared best hand-tuning rate reported harmonica- uses days less half environment gpus running parallel. notice cherry pick results harmonica-. section show running harmonica- longer time obtain solutions better hand tuning. computed average test error among random samples layer network epochs stage. figure selecting features stage average test error drops indicates features important. proceed stage improvement test error becomes less signiﬁcant selected features stage mild contributions. clear harmonica hyperparameters needs including number stages regularizer lasso number features selected stage base algorithm small network conﬁguration number samples stage. note however reduced search space general hyperparameter optimization hyperparameters. empirically algorithm robust diﬀerent settings parameters even attempt tune base algorithm stages. tried diﬀerent versions harmonica including harmonica stage stages stages using base algorithm stage stages using random search base algorithm stages stages running base longer time seen figure variants produce better results less running time. moreover harmonica longer time obtain stable solutions less variance test error. lasso parameters stable. table stable range regularization term number samples. stable range means long parameters range features signs weights procedure change. words feature selection outcome aﬀected. parameters outside stable ranges usually features still unchanged miss features. second experiment considers synthetic hierarchically bounded function harmonica samples features selected stage stages using degree features. figure optimization time comparison. plot optimization time spearmint takes samples. harmonica several magnitudes faster spearmint. figure show harmonica able estimate hidden function error proportional noise level. synthetic function deﬁned follows. three stages i-th stage sparse vectors binary. therefore contains binaries represents integer denoted cij. sc∗+cc noise uniformly sampled words every stage sparse vector sij. based pick next sparse function proceed next stage. thanks vitaly feldman pointing work stobbe krause thank sanjeev arora helpful discussions encouragement. elad hazan supported grant project supported microsoft azure research award amazon research award.", "year": 2017}