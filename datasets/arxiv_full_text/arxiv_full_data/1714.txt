{"title": "Taking into Account the Differences between Actively and Passively  Acquired Data: The Case of Active Learning with Support Vector Machines for  Imbalanced Datasets", "tag": ["cs.LG", "cs.CL", "stat.ML", "I.2.6; I.2.7; I.5.1; I.5.4"], "abstract": "Actively sampled data can have very different characteristics than passively sampled data. Therefore, it's promising to investigate using different inference procedures during AL than are used during passive learning (PL). This general idea is explored in detail for the focused case of AL with cost-weighted SVMs for imbalanced data, a situation that arises for many HLT tasks. The key idea behind the proposed InitPA method for addressing imbalance is to base cost models during AL on an estimate of overall corpus imbalance computed via a small unbiased sample rather than the imbalance in the labeled training data, which is the leading method used during PL.", "text": "actively sampled data different characteristics passively sampled data. therefore it’s promising investigate using different inference procedures used passive learning general idea explored detail focused case cost-weighted svms imbalanced data situation arises many tasks. idea behind proposed initpa method addressing imbalance base cost models estimate overall corpus imbalance computed small unbiased sample rather imbalance labeled training data leading method used recently considerable interest using active learning reduce annotation burdens. actively sampled data different characteristics passively sampled data therefore paper proposes modifying algorithms used infer models since research assumes learning algorithms used passive learning paper opens thread research accounts differences passively actively sampled data. datasets. collectively factors interest widespread class imbalance many tasks interest using svms research showing performance improved substantially addressing imbalance indicate importance case svms imbalanced data. extensive research shown learning algorithms’ performance degrades imbalanced datasets techniques developed prevent degradation. however date relatively little work addressed imbalance contrast previous work paper advocates scenario brings need modify approaches dealing imbalance. particular method developed cost-weighted svms estimates cost model based overall corpus imbalance rather imbalance labeled training data. section discusses related work section discusses experimental setup section presents method called initpa section evaluates initpa section concludes. problem imbalanced data class boundary learned svms close positive examples recall suffers. many approaches presented overcoming problem setting. many require substantially longer training times exthis paper focuses fundamental case binary classiﬁcation class imbalance arises positive examples rarer negative examples situation naturally arises many tasks. kernel highest-performing systems aimed date perform -fold cross validation. reuters- modapte split. since document belong category category treated separate binary classiﬁcation problem largest categories imbalances ranging propose approaches sets based level imbalance labeled training data aims based estimate overall corpus imbalance drastically differ level imbalance actively sampled training data. ﬁrst method called currentpa depicted figure note step loop based distribution positive negative examples current labeled data. however observe ratio labeled examples labeled examples current labeled data gets skewed ratio entire training data tune parameters thus ideal cost-weighted svms hand promising approach impose extra training overhead. cwsvms introduce unequal cost factors optimization problem solved becomes important part paper cost factors ratio quantiﬁes importance reducing slack error train examples relative reducing slack error negative train examples. value ratio crucial balancing precision recall tradeoff well. showed setting training examples training examples effective heuristic. section explores using heuristic during explains modiﬁed heuristic could work better propose using balancing training data occurs result al-svm handle imbalance measures address imbalance. used resampling address imbalance based amount resampling analog cost model amount imbalance current labeled train data approaches contrast initpa approach section bases cost models overall corpus imbalance rather amount imbalance current labeled data. relation extraction text classiﬁcation datasets svmlight training svms. aimed previously used train protein interaction extraction systems previous work cast binary classiﬁcation task estimate ratio high conﬁdence. estimate used setting throughout process. call method setting based small initial labeled data initpa method. like currentpa except move step executed time loop value iteration loop. guide size make initial labeled data determine sample size required estimate proportion positives ﬁnite population within sampling error desired level conﬁdence using standard statistical techniques found many college-level statistics references example carrying computations aimed dataset shows size enables conﬁdent proportion estimate within true proportion. experiments used initial labeled size addition initpa currentpa also implemented methods implemented oversampling duplicating points bootos avoid cluttering graphs show highest-performing oversampling variant duplicating points. learning curves presented figures therefore next algorithm aims based ratio instances entire corpus. however since don’t labels entire corpus don’t know ratio. using small initial sample labeled data we’ve made case importance al-svm imbalanced datasets showed scenario calls modiﬁcations approaches addressing imbalance. al-svm idea behind initpa base cost models estimate overall corpus imbalance rather class imbalance labeled data. practical utility initpa method demonstrated empirically; situations initpa won’t help much made clear; analysis showed sources initpa’s gains better sampling better predictive models. initpa instantiation general idea using inference algorithms instead modifying inference algorithms suit esoteric characteristics actively sampled data. idea seen relatively little exploration ripe investigation.", "year": 2014}