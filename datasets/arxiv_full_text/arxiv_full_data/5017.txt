{"title": "Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces", "tag": ["cs.AI", "cs.LG"], "abstract": "While recent advances in deep reinforcement learning have allowed autonomous learning agents to succeed at a variety of complex tasks, existing algorithms generally require a lot of training data. One way to increase the speed at which agents are able to learn to perform tasks is by leveraging the input of human trainers. Although such input can take many forms, real-time, scalar-valued feedback is especially useful in situations where it proves difficult or impossible for humans to provide expert demonstrations. Previous approaches have shown the usefulness of human input provided in this fashion (e.g., the TAMER framework), but they have thus far not considered high-dimensional state spaces or employed the use of deep learning. In this paper, we do both: we propose Deep TAMER, an extension of the TAMER framework that leverages the representational power of deep neural networks in order to learn complex tasks in just a short amount of time with a human trainer. We demonstrate Deep TAMER's success by using it and just 15 minutes of human-provided feedback to train an agent that performs better than humans on the Atari game of Bowling - a task that has proven difficult for even state-of-the-art reinforcement learning methods.", "text": "figure deep tamer framework proposed paper. human observes autonomous agent trying perform task high-dimensional environment provides scalar-valued feedback means shape agent behavior. interaction human agent learns parameters deep neural network used predict human’s feedback. prediction drives agent’s behavior policy. paper speciﬁcally consider atari game bowling uses pixel-level state space. prohibitively difﬁcult. instead good policies often found automatically machine learning methods reinforcement learning techniques compute optimal policies without explicit human direction using data available agent interacts environment. state-of-the-art reinforcement learning approaches ﬁnding optimal decision-making policies enjoy reasonable success also usually require large amount time and/or data. requirement primarily fact interesting tasks often exhibit high-dimensional state spaces arise agent’s state information includes image data. absence priori information performing reinforcement learning state spaces requires large number parameters estimated scratch result even state-of-theart techniques typically require extremely large amounts training data often translates large amounts time agents able learn good policies. example recent methods deep reinforcement learning must acquire tens millions video frames worth experience simulator learn good strategies playing atari games recent advances deep reinforcement learning allowed autonomous learning agents succeed variety complex tasks existing algorithms generally require training data. increase speed agents able learn perform tasks leveraging input human trainers. although input take many forms real-time scalar-valued feedback especially useful situations proves difﬁcult impossible humans provide expert demonstrations. previous approaches shown usefulness human input provided fashion thus considered high-dimensional state spaces employed deep learning. paper both propose deep tamer extension tamer framework leverages representational power deep neural networks order learn complex tasks short amount time human trainer. demonstrate deep tamer’s success using minutes human-provided feedback train agent performs better humans atari game bowling task proven difﬁcult even state-of-the-art reinforcement learning methods. many tasks would like autonomous agents able accomplish thought agent making series decisions time. example autonomous robot tasked navigate speciﬁc goal location physical space making series decisions regarding particular movement actions execute every instant. solutions types sequential decision making problems speciﬁed policies i.e. mappings agent’s state actions agent might take armed particular policy agent decide actions take ﬁrst estimating state acting according output policy. simple cases human experts able completely specify advance policies allow agents accomplish certain tasks. many complex tasks however specifying policies manner copyright association advancement artiﬁcial intelligence rights reserved. propose achieve rapid agent learning adapting typical reinforcement learning paradigm able exploit availability human trainer develop novel method human trainers useful typically possess good understanding task would like agent perform outset understanding thought critical priori information might allow agent drastically reduce training time. observation recently made exploited series papers outlining tamer framework shown agents learning directly non-expert human trainers improve agent-only learning terms sample complexity overall performance. follow-on work examined quality human feedback function factors relating user engagement framework shown work low-dimensional state spaces. tamer framework utilized higherdimensional state spaces. propose incorporating recent function approximation techniques deep learning enabled number recent successes reinforcement learning high-dimensional spaces. particular contributions work twofold order evaluate deep tamer focus specifically atari game bowling. training agents successful game proven difﬁcult even state-of-the-art deep reinforcement learning methods. example even training times order days several recently-proposed frameworks deep reinforcement learning able train agents obtain scores ranging maximum contrast show paper that using proposed technique human trainers train better agents within minutes interactive training. moreover show human trainers often able train agent achieve better score bowling trainers themselves. related work large body literature techniques allow autonomous agents learn interaction human trainers none addresses problem domain uses methodology propose study here. developed address problem typically take input observations human performing task demonstrations order compute policies allow autonomous agent also accomplish demonstrated task. learning demonstration techniques often exhibit impressive performance terms training time necessity demonstration data sometimes prove infeasible expert human demonstrator always available difﬁcult tasks even even exist. also problematic related area inverse reinforcement learning autonomous agents attempt learn reward function that useful policy using explicit demonstrations task. moreover since goal setting often mimic human performance underlying task typically capped exhibited demonstrator makes difﬁcult task training autonomous agents capable super-human performance. another area research aims explicit human interaction increase speed autonomous agents learn perform tasks reward shaping reward shaping techniques require demonstrations rather humans modify directly low-level reward function used specify desired task. assuming human good understanding task intricacies appropriately modify underlying reward function approaches also quite effective. however learning demonstration often case human either level understanding proﬁciency machine learning able interact agent way. closely related work presented here several methods recently proposed allow agent learn interaction non-expert human work consider tamer technique training autonomous agents particular. tamer allows autonomous agent learn non-expert human feedback real time series critiques human trainer observes agent’s behavior provides scalar feedback indicating good assess current behavior agent response tries estimate hidden function approximates human’s feedback adjusts behavior align estimate. tamer framework proven successful several limited tasks existing work considered highdimensional state spaces often encountered many complex tasks interest. separately recent work yielded major breakreinforcement learning utilizing function approximation techniques deep learning advancement spurred huge leap forward ability reinforcement learning techniques learn high-dimensional state spaces encountered even simple atari games deep learning also applied several areas described above including learning demonstration inverse reinforcement learning however effect using deep learning techniques learn human interaction remains largely understudied could enable highly efﬁcient human-agent learning many tasks interest. notable place deep learning learning human interaction studied recent work there deep learning applied paradigm learner actively queries humans compare behavior examples learning process. work indeed similar ours highlight differences. first method discussed requires learner access background simulator human interaction always possible realworld situations might arise robotics. method describe require access simulator interaction. second technique proposed requires order million learning time steps executed simulator interaction human possible access extremely powerful hardware. method propose learns human real-time without simulator requires thousand time steps interaction learn good policy environment considered here. enables success standard computing hardware paper concerned answering following speciﬁc question impact using deep neural networks effectiveness learning real-time scalar-valued human feedback high-dimensional state spaces? section formulate problem precisely. adopt formulation similar proposed main ways differs choice function class agent uses represent feedback coming human speciﬁc optimization techniques used train agent. consider agent learning problem context classical sequential decision making speciﬁcally denote states autonomous agent itself denote actions agent execute. agent selects executes actions result state trajectory assume human trainer observes state trajectory periodically provides scalar-valued feedback signals convey assessment agent’s behavior. here assume larger correspond positive assessment human. critically also assume human provides feedback according hidden function human implicitly understands. agent learning problem consider computing estimate function denote good match given current estimate assume agent behaves myopically according ﬁxed action-selection policy selects next action maximizes predicted human feedback i.e. maxa given ﬁxed behavior policy problem described exactly supervised machine learning agent observes inputs form experience observations related output form human’s feedback interval time agent spent state taking action time human’s feedback observed. observations agent tasked computing estimate importantly assume one-to-one mapping inputs outputs. expect observed corresponds several recently-observed moreover corresponding feedback information all. encode assumption learning process using following loss function judge quality scalar-valued weighting function larger pairs hypothesize human intended apply exact form discussed next section importantly property zero-valued less becomes negligible sufﬁciently greater events correspond human feedback provided state-action pair occurred long respectively. using loss function amounts judging quality according predictive power context prior belief trainer intended evaluate feedback. therefore learning problem becomes simply computing estimate minimizes available observations. since would like agent learn throughout interaction human after deﬁne problem online supervised learning treat observations realizations random variables seek minimize loss statistical sense. here accomplish goal seeking minimizes expected value loss i.e. expectation taken respect pairs generated real-time interaction human agent. programs form amenable online solution techniques shall describe next section. figure speciﬁc network structure state encoder ﬁxed front half work. parameters learned optimal found autoencoder pre-training phase states obtained agent executes random policy simulation. shall adopt common technique approaching problems formulated observations sestochastic gradient descent quentially available broadly speaking computes incremental estimates descent procedure uses instantaneous approximations gradient i.e. above iteration index denotes gradient taken respect ﬁrst argument given pair sampled uniformly random stream experience observations feedback observations importance weights selecting loss function opted weighted squared loss weights given weights importance weights i.e. bias solution toward pairs hypothesize human intended feedback information apply agent experience since select pair uniformly random stream experience feedback observations simply using loss would amount assuming human feedback applied experience. assumption clearly odds intent human trainers providing real-time feedback watching agent behave instead assume human trainers intend feedback apply recent agent behavior. speciﬁcally compute importance weights probability feedback provided time applies state-action pair occurred time interval according assumed probability distribution fdelay. discussion) shall continuous uniform distribution interval thus observed feedback nonzero observed seconds feedback occurred. learning efﬁciency performing sample pairs corresponding zero since pairs result update deep reward model order efﬁciently learn human reward function dimension state space large propose model function using deep neural network. speciﬁcally since particularly interested state spaces comprised images assume takes form deep convolutional neural network followed neural network comprised sevfunction. means quickly learn parameters limited amount real-time human interaction agent receives perform updates ﬁxed rate typically exceeds rate humans typically able provide feedback agent. able storing observed human feedback relevant agent experience feedback replay buffer sampling repeatedly buffer replacement. speciﬁcally contains running history human feedback information along with individual feedback signal experience points yield nonzero importance weights. shorthand quantity speciﬁed states occur within ﬁxed window time corresponding observed easily obtained stored immediately upon observation work consider limiting size simply store observed human feedback information provided interaction though continually perform updates ﬁxed rate sampling also wish ensure human feedback immediate effect agent’s behavior. therefore perform updates ﬁxed rate parameter algorithm. experiments select parameter results buffer updates every time steps observed human feedback typically provided rate approximately signal every time steps practice perform mini-batch updates using average gradient computed several samples instead one. particular mini-batches formed ﬁrst sampling several individual adding pairs corresponding nonzero mini-batch. deep tamer already discussed deep encoder portion discussed remainder network. two-layer fully-connected neural network hidden units layer output node available action similar input-output structure value networks used recent deep reinforcement learning literature exact structure shown figure overpredicted human reward value given state-action pair found using input fully-connected network selecting output node corresponding action training errors back single relevant output node. eral fully-connected layers i.e. order efﬁciently learn parameters network under constraints limited training time limited human input adopt strategies pre-train portion using autoencoder feedback replay buffer means increase rate learning. deep autoencoder means immediately reduce number parameters agent must estimate real-time interaction human pre-train convolutional layers network using autoencoder. autoencoder comprised functions encoder decoder parameterized respectively. encoder deep accepts states dimension outputs encoded states dimension atari bowling environment represent most-recent game images i.e. encoder structure produces -dimensional output. exact structure given figure decoder pre-training mirror image deep deconvolutional neural network accepts encoded states outputs decoded states dimension parameters encoder decoder jointly found trying minimize reconstruction error training states. optimal values given acquire training states environment offline simulation using random policy. training complete resulting encoder ﬁxed front feedback replay buffer even parameters ﬁxed still large number parameters must learned namely fullyconnected deep network comprises second half figure average score episode wall-clock time atari bowling game. blue orange lines show deep tamer tamer learning curves averaged human trainers. yellow line shows double performance. purple line shows average performance parallel workers. dashed line shows average game score achieved human trainers. black dashed line shows average game score achieved professional human play tester blue dashed line shows deep q-learning human demonstration method previous best method atari bowling. atari bowling screenshot atari game bowling depicted figure time step image game screen displayed player select four actions no-action down bowl. convert game image grayscale game state tensor corresponding most-recent game screens. bowling single ball begins player moving character avatar vertically desired position release ball. then player must execute bowl action start ball rolling toward pins. finally player chance single point ball’s journey toward lane cause ball start spinning direction executing actions respectively. game proceeds scored like common ten-pin bowling though extra balls allowed tenth frame resulting maximum score training procedure evaluate system human trainers train agent play bowling using deep tamer. training performed using experimental computer trainers supervised experimenters. means provide familiarization game characterterm using deep reward model training method described including pre-training autoencoder using importance-weighted stochastic optimization technique feedback replay buffer deep tamer propose autonomous agents learn real-time human interaction environments high-dimensional state spaces. complete procedure summarized algorithm beyond deep neural network function approximation scheme deep tamer differs tamer several important ways. first speciﬁc loss function used different used tamer. using deep tamer seeks minimize weighted difference between human reward predicted value state-action pair individually. contrast loss function used tamer framework deﬁned entire window samples i.e. hypothesize faithfully reﬂects intuition human’s reward signal applies individual state-action pairs. another major difference deep tamer tamer frequency learning. tamer learns state-action pair whereas deep tamer learn multiple times feedback replay buffer. moreover unlike replay buffers used recent deep literature deep tamer’s feedback replay buffer used explicitly address sparse feedback opposed overcoming instability learning. experimentally evaluated deep tamer context atari game bowling environment proven difﬁcult several recent state-of-the deep reinforcement learning algorithms. experiments used implementation provided aracade learning environment included part openai suite section shall describe environment detail procedure humans train agent present ways evaluated deep tamer discuss results evaluation. short found that using deep tamer human trainers able train successful bowling agents minutes. moreover found agents trained using deep tamer outperformed agents trained using stateof-the deep reinforcement learning techniques well agents trained using original tamer method proposed cases deep tamer agents even performed better human trainers themselves. results demonstrate proposed enhancements tamer namely deep reward model allow framework successful environments high-dimensional state spaces. additionally favorable comparisons deep reinforcement learning techniques human players indicates continued utility paradigm autonomous agent learning real-time human interaction. figure individual human trainer performance. panel shows deep tamer learning curve game score training time human trainers experiment dashed line shows average game score achieved trainers playing complete bowling games. deep tamer agents able meet exceed human trainers minutes training. individual human performance trainer ﬁrst played ten-frame games recorded game score. then trainer allowed -minute training session recorded means practice giving feedback agent. practice session trainers allowed opportunity experimenter questions might have. finally practice session experimenter reset agent trainer used deep tamer train agent minutes interaction information data relating agent recorded. evaluation compared performance deep tamer atari bowling double-dqn tamer performance human trainers themselves. used implementations d-dqn made available openai tamer implemented algorithm using credit assignment scheme difference that similar separate linear parameter vector learned discrete action. figure shows average game score function training time deep tamer tamer ddqn deep tamer tamer results computed averaging results across human-trained policies each. comparison scores human trainers tested experiment well expert human game tester reported shown horizontal dashed lines. additionally deep tamer compared previously best method atari bowling used human demonstration data train deep network expected double-dqn agents fail learn useful policy minutes allotted human trainers using deep tamer. further even given full amount training data required methods algorithms still fail learn successful policy bowling probably sparse reward signal environment. seen original tamer algorithm also failed learn useful policy allotted time. likely fact tamer uses linear model human reward function insufﬁcient pixel-based state input here. deep tamer also performs better learning demonstration method indicating advantage reward shaping methods demonstration methods task difﬁcult human perform difﬁcult human critique. finally seen figure deep tamer able exceed performance human trainers minutes training surpasses performance expert human minutes. last point especially interesting implies tamer useful allows nonexperts train agents perform well complex tasks also promising methodology easily creating agents capable super-human performance. figure demonstrates point detail deep tamer performance human trainers along individual play performance shown. performance increase noisy likely result stochastic optimizapaper proposed extension tamer framework learning real-time human interaction. technique called deep tamer enables tamer paradigm successful even environments highdimensional state spaces. success deep neural network function model approximate human trainer’s reward function also modiﬁed supervised learning procedure proposed parameters model. evaluated deep tamer challenging atari game bowling pixel-level state features found agents trained humans using deep tamer signiﬁcantly outperformed agents trained humans using tamer. additionally results reafﬁrm attractiveness overall tamer paradigm. minutes real-time interaction human deep tamer agents able achieve higher scores agents trained using state-ofthe-art deep reinforcement learning techniques orders magnitude training data. moreover human trainers actually able produce agents actually exceeded performance. portion work taken place learning agents research group austin. larg research supported part intel raytheon lockheed martin. peter stone serves board directors cogitai inc. terms arrangement reviewed approved university texas austin accordance policy objectivity research.", "year": 2017}