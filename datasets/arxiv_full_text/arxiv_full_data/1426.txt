{"title": "Improving image generative models with human interactions", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "GANs provide a framework for training generative models which mimic a data distribution. However, in many cases we wish to train these generative models to optimize some auxiliary objective function within the data it generates, such as making more aesthetically pleasing images. In some cases, these objective functions are difficult to evaluate, e.g. they may require human interaction. Here, we develop a system for efficiently improving a GAN to target an objective involving human interaction, specifically generating images that increase rates of positive user interactions. To improve the generative model, we build a model of human behavior in the targeted domain from a relatively small set of interactions, and then use this behavioral model as an auxiliary loss function to improve the generative model. We show that this system is successful at improving positive interaction rates, at least on simulated data, and characterize some of the factors that affect its performance.", "text": "gans provide framework training generative models mimic data distribution. however many cases wish train generative models optimize auxiliary objective function within data generates making aesthetically pleasing images. cases objective functions difﬁcult evaluate e.g. require human interaction. here develop system efﬁciently improving target objective involving human interaction speciﬁcally generating images increase rates positive user interactions. improve generative model build model human behavior targeted domain relatively small interactions behavioral model auxiliary loss function improve generative model. show system successful improving positive interaction rates least simulated data characterize factors affect performance. generative image models improved rapidly past years part success generative adversarial networks gans gans attempt train generator create images mimic real images training fool adversarial discriminator attempts discern whether images real fake. solution difﬁcult problem learning don’t know write objective function image quality take empirical distribution good images match often want impose additional constraints goal distribution besides simply matching empirical data. write objective reﬂects goals often simply incorporate loss function achieve goals. example trying generate would like network creative innovative rather imitating previous styles including penalty loss producing recognized styles appears make gans creative conditioning image content class training discriminator classify image content well making real/fake judgements including loss term fooling discriminator class allows targeted image generation improves overall performance however sometimes easy write explicit objective reﬂects goals. often effective evaluate machine learning systems complex tasks asking humans determine quality results actually trying real world. incorporate kind feedback efﬁciently guide generative model toward producing better results? without prohibitively expensive slow amount data collection? paper tackle speciﬁc problem kind generating images cause positive user interactions. imagine measured generic positive interaction rate could come wide variety sources. example users might asked rate aesthetically pleasing image stars. could computed weighted frequently different ratings chosen. alternatively images could used background pages. assess user interactions webpage variety ways summarize interactions pir. tasks don’t know exactly features affect certainly don’t know explicitly compute image. however empirically determine quality image actually showing users paper show small amount data efﬁciently tune generative model produce images increase pir. work focus simulated values proof concept future work investigate values obtained real user interactions. straight-forward improve image might evaluate images model produces real users training step. however process slow. instead want able collect batch data batch images improve generative model many gradient steps; want despite fact images generator producing evolve different original images collected data order this batch image data train estimator model predicts pirs images. estimated pirs training step produce loss. approach inspired work christiano colleagues integrated human preference ratings action sequences training reinforcement learning model. however problem approach differ several ways. first optimizing generative image model rather model. difﬁcult ways since output space much higher-dimensional typical problems means low-dimensional feedback harder system learn from. difﬁculty partially offset fact assume reward information image evaluate instead getting preference underlying reward function. perhaps importantly estimation model fully-differentiable loss function training instead using estimated rewards. allows effectively exploit knowledge objective function fig. diagram general operation system. generative model produces images served users. using interaction data users train estimator model predicts pirs given background image incorporate estimated loss generative model tune produce higher quality images. below discuss components detail. begin pre-trained produce images target distribution dsource source estimated discriminator generator noise input generator sampled multivariate standard normal distribution real images shown discriminator. deﬁne note difference losses standard formulation given maximize lfake image fools rather minimizing seems result generation slightly better images practice. parameterized generator deep neural network begins fully-connected mapping latent space dimensional image successively upsampled padded applied convolution leaky relu nonlinearity repeatedly. repeated process times stepping image depth follows ﬁnally output image. means ﬁnal output images fully connected layer single output real/fake. used leaky relu nonlinearity layer except ﬁnal layer used tanh. trained dataset consisting landscape images mountains coastlines generated google image searches topics. generator trained adam optimizer discriminator rmsprop. learning rates adam used latent size units. model trained gradient steps generated images appeared stop improving. worth noting generative model photorealistic expressive capacity limited clear output modes limited intra-mode variability. however purposes matter. indeed ways interesting tweak model optimize many objective functions since limited expressive capacity make difﬁcult estimate pursue real objective limited images effectively give fewer points estimate function from reduce space model easily produce images thus reducing possibility gettign optimal images model. example model produces images birds produce data points provide good estimates based much image looks like even could able produce images more car-like. able succeed improving pirs generative model likely better generative model would yield even better results. show images users variety ways depending target domain. purposes paper however simulated interaction data. section information produced simulated data. course since showing images users expensive prospect wanted limit size datasets used train model. typical datasets used train vision models order millions images completely infeasible collect user data number images. estimated could show images times generate datasets. used dataset sizes number impressions experiments discussed here added noise pirs binomially distributed according number times image shown true objective. ﬁnal component system estimator model learns predict background image. denote model image parameterize model deep neural network. speciﬁcally take inception architecture remove output layer replace fully-connected layer estimates. instead predicting scalar directly predict classifying bins softmax performs better empirically. choice motivated noting scalar version trouble ﬁtting highly multi-modal distributions appear data. initialize inception parameters version model trained internal version dataset trained estimator adam optimizer evaluating using model improving froze weights estimator. also reduced output softmax’s temperature behaving almost like empirically improved results. intuitively softmax temperature training allows system rapidly gradients many different output bins adjust distribution appropriately whereas actually using system want conservative estimates biased probability bins modal estimate. make estimated simply another auxiliary output discriminator like class acgan estimator needs held constant order provide accurate training objective. estimates produced discriminator discriminator changed accurately discriminate evolving generator images estimates would tend drift without ground-truth train separating discriminator estimator allows freeze estimator still letting discriminator adapt. trained estimator model improve gan. follows. denote estimator model above. deﬁne lpir expectation estimated produced images sampled generator made magnitude loss loss terms roughly comparable. otherwise used parameters training above except reduced learning rate allow system adapt smoothly multiple objectives trained steps. paper simulated interaction data proof concept. raises issue functions simulate user interactions? human behavior complex already knew precisely guided user interactions would need actually collect human behavioral data all. since don’t know features guide human behavior next best thing ensure system able alter image generation model broad variety ways ranging level features altering complex semantic features also want avoid hand-engineering tasks greatest extent possible. ﬁrst approach took evaluating system’s ability train different features activity hidden layers computer vision model speciﬁcally trained imagenet particular took norm activity ﬁlter within layer normalized norm total layer’s activity i.e. letting vgglf vector unit activations ﬁlter layer image computed image given layer ﬁlter noted above also added binomially distributed noise pirs. approach simulating pirs several beneﬁts. first gives wide variety complex objectives nevertheless easily computed simulate data. second models like exhibit hierarchical organization lower levels generally respond lower-level features edges colors higher levels respond higher-level semantic features faces means modulate activity randomly chosen ﬁlters layer compare performance across different layers order evaluate types objectives system capable optimizing effectively. furthermore since features various levels related corresponding levels human macaque visual cortex might hope system capable optimizing features also capable optimizing human preferences. caveats approach however. first although higher layers cnns somewhat selective abstract object categories also fooled adversarial images humans would directly optimizing inputs high level features actually produce semantically meaningful images thus even system succeeds increasing activity targeted layer semantically selective likely adversarially exploiting particulars parameterization classiﬁcation problem necessarily failure system exploits simple features objective given increase pirs indeed seen success. however success task necessarily guarantee success changing semantic level content interacting actual humans. indeed easier estimator model learn objectives come another general possible objectives. fact adversarial examples sometimes transfer networks different architectures suggests computations performed networks somewhat architecture invariant. thus objectives easier estimator learn real world objectives. tried minimize problems greatest extent possible using different network architectures trained different datasets imagenet respectively) estimator objective. however cannot certain network cheating tasks results must considered qualiﬁcation mind. despite qualiﬁcations think evaluating system’s ability optimize objectives generated various layers show ability optimize variety complex objectives thus serve useful indicator potential improve pirs real users. using system tasks above noted performance quite poor layers compared tasks could suggest system unable capture complex features represented higher levels vgg. however also noticed feature representations layers tended quite sparse many simulated pirs generated actually zero within width estimator. respectively layers non-zero pirs around half ﬁlters producing zero pirs. cases layers generated pirs zero. clearly makes learning infeasible indeed noted strong relationship number non-zero simulated pirs training dataset ability system improve somewhat troubling since probably images real world produce truly zero. thus order evaluate whether poor performance system higher layers attributable number zeros complexity features created less sparse features layers simply targeting ﬁlters sampled without replacement layer rather single ﬁlter. taking norm across target ﬁlters equivalently summing squared norms ﬁlters taking square root normalizing activity layer before. formally letting ﬁlter indices sampled without replacement number ﬁlters layer} computed image single ﬁlter cases thought special case this complement these also tried above also added binomially distributed noise pirs. also thought perhaps realistic simulation human behavior sense highly unlikely single feature inﬂuences human pirs. rather probably many related features inﬂuence various ways. thus important evaluate system’s ability target types features well. finally also considered simpler objectives based targeting speciﬁc colors output images. analogously features computed pirs vector norm given image targeted color normalized total image value. considered several objectives type objectives provide useful complement objectives discussed section although single color objectives relevant classiﬁcation task performs split color tasks less likely relevant classiﬁcation. note important split images along width instead height dimension well semantically relevant features corresponding color divisions along height dimension e.g. blue upper half green lower half likely correlates outdoor images would provide useful class information. contrast harder imagine circumstances different colors left right halves image semantically predictive especially since ﬂipping left right usually included data augmentation computer vision systems. thus success optimizing objectives would increase conﬁdence generality system. present results terms change mean images produced tuning images produced tuning terms effect size change assess whether changes signiﬁcant performing welch’s t-test prepost-tuning image sets. overall system quite successful improving pirs across range simulated objective functions below discuss results detail. figure change produced system different objectives. values greater zero indicate improvement. panel represents tasks color/column represents subset point represents objective optimized generative model. points change mean signiﬁcant welch’s t-test threshold partially transparent. reduced number points pool pool layers ﬁlters task layers ﬁlters respectively. system largely succeed increasing pirs variety objectives however several interesting patterns note. first system particularly successful targeting single ﬁlters pool layers. however believe fact ﬁlters layers produce relatively sparse activation section indeed performance system seems much consistent optimizing sets ﬁlters single ﬁlters. even using ﬁlters however noticeable decline effect size improvement system able make higher layers suggests objectives grow complex system ﬁnding less accurate approximations them. however system’s continuing success higher layers suggests model capable least partially capturing complex objective functions. overall system performed quite well optimizing color objectives particularly single two-color results difﬁculty optimizing three-color results indeed produced small improvements usual tuning steps generative model steps able produce signiﬁcant improvements three objectives color objectives easiest assess visually included results variety objectives fig. single color objectives improvement quite clear example images fig. appear much blue pre-training ones. color objectives appears system found trick reducing third color example red-green split images fig. appear much less blue pre-training images. even three-color images system struggled visible signs improvement example green-blue-red task system started producing number images blue streak middle. expected value looking tuning generative model tell well system thinks doing i.e. much estimates improved pir. comparison reveals interesting pattern system overly pessimistic performance. fact tends underestimate performance factor however fairly consistently. effect appears driven system consistently underestimating pirs probably caused change softmax temperature training estimator tuning generative model contrast possible priori expectation model would systematically overestimate performance overﬁtting imperfectly estimated objective function. although decreasing softmax temperature training using obscures effect evidence this; complex objectives seem lower estimated changes mean true changes even lower estimated ones thus although system somewhat aware reduced effectiveness objectives reducing estimates sufﬁciently account true difﬁculty objectives however system generally still able obtain positive results objectives general trend variability initial dataset strongly positively related change system able produce fact initial standard deviation explains variance change mean pir. perhaps surprising variability means generative model capacity produce higher images without much tweaking estimator model gets wider range values learn from. still attempting system practice important keep mind starting sufﬁciently expressive generative model likely produce better results starting limited model. given model improves pirs obvious question whether iterate process. increased pirs train estimator samples generative model increase pirs again? could iterate many times might able create much larger improvements single step. hand possible single step optimization effectively saturated easily achievable improvement model steps result much improvement. evaluate this took subset models trained single ﬁlters layers pool used images generated post-tuning model along original images tune estimators another steps tuned generative model using updated estimator another steps. evaluated before fig. results. second iteration results mixed pool models improved ﬁrst step second none improved much ﬁrst step many models actually performed worse second step. however possible hyperparameter tuning could improve results certainly case running multiple steps iteration selecting best model experimentation could yield better results worth investigating further. overall system appears relatively successful. optimize generative model produce images targeting functions ranging low-level visual features colors early features features computed layers vgg. success across wide variety objective functions allows somewhat conﬁdent system able achieve success optimizing real human interactions. furthermore system require inordinate amount training data. fact able successfully estimate many different objective functions images several orders magnitude fewer typically used train cnns vision tasks. furthermore images came biased narrow distribution reﬂective neither images used pre-train inception model estimator images model trained success small amount data suggests system able optimize real human interactions able feasible number training points. results exciting model able approximate apparently complex objective functions small amount data even though data comes biased distribution unrelated objectives question. really learned? case color images it’s clear model something close correct. however objectives derived really assess whether model making images better adversarial. instance optimizing logit magpie it’s almost certainly case result optimization look like magpie human even rate images magpie-like. hand necessarily failure system accurately capturing objective function given. remains seen whether capture background images inﬂuence human behavior well capture vagaries deep vision architectures. believe many domains system similar could useful. mentioned producing better webpage backgrounds making aesthetic images above many potential applications improving gans limited amount human feedback. example model could trained produce better music system could also offer different approach problem tackled elgammal colleagues asking artists evaluate original art. tuning worth noting decrease loss usually accompanied increase generator loss often partial collapse generator output especially surprising weighted loss highly model rewarded trading image diversity image optimality. depending desired application weight loss could adjusted necessary trade producing images close data distribution optimizing pir. extreme could down-weight generator loss entirely train model produces single optimal image. however generator likely provides regularization constraining images somewhat close real images reduce overﬁtting imperfect estimate function. furthermore many settings want generate variety images reasons chose keep generator loss tuning gan. number future directions suggested work. first number techniques could explored improve system. mentioned above iterating multiple steps click collection generative model tuning worth exploring further. also form data scaling might allow system perform better tasks variance. brieﬂy tried normalizing data objective mean standard deviation achieve particularly good results this possibly many outliers getting clipped still many possibilities scaling data could potentially result improvement performance. also alternative approach training produce high-pir images would estimator objective plug play generative networks framework instead using tune gan. could interesting direction explore success would probably depend expressiveness initial generative model. mediocre model started with it’s probably better actually tune model itself allow explore parts image space previously. another fascinating direction would system objectives layers understand features layers attending analyzing distribution images produced. sense system thought offering approach multifaceted feature visualization system attempts optimize distribution images objective encourages diversity distribution produced rather optimizing single image. finally also believe results objectives interesting perspective distillation imitation approaches attempt train network emulate another well perspective understanding computations vision architectures perform. aware results ﬁrst show deep vision model tuned rapidly relatively little data produce outputs accurately emulate behavior hidden layers another deep vision architecture trained different dataset. suggests inductive biases shared among architectures causing similar solutions networks ﬁnal layers represent computations earlier hidden layers somewhat accessible. described system efﬁciently tuning generative image model according slow-toevaluate objective function. demonstrated success system targeting variety objective functions simulated different layers deep vision model well low-level visual features images shown small amount data. quantiﬁed features affect performance including variability training data number zeros contains. system’s success wide variety objectives suggests able improve real user interactions objectives", "year": 2017}