{"title": "Scene Grammars, Factor Graphs, and Belief Propagation", "tag": ["cs.CV", "cs.AI"], "abstract": "We consider a class of probabilistic grammars for generating scenes with multiple objects. Probabilistic scene grammars capture relationships between objects using compositional rules that provide important contextual cues for inference with ambiguous data. We show how to represent the distribution defined by a probabilistic scene grammar using a factor graph. We also show how to efficiently perform message passing in this factor graph. This leads to an efficient approach for inference with a grammar model using belief propagation as the underlying computational engine. Inference with belief propagation naturally combines bottom-up and top-down contextual information and leads to a robust algorithm for aggregating evidence. We show experiments on two different applications to demonstrate the generality of the framework. The first application involves detecting curves in noisy images, and we address this problem using a grammar that generates a collection of curves using a first-order Markov process. The second application involves localizing faces and parts of faces in images. In this case, we use a grammar that captures spatial relationships between the parts of a face. In both applications the same framework leads to robust inference algorithms that can effectively combine weak local information to reason about a scene.", "text": "consider class probabilistic grammars generating scenes multiple objects. probabilistic scene grammars capture relationships objects using compositional rules provide important contextual cues inference ambiguous data. show represent distribution deﬁned probabilistic scene grammar using factor graph. also show efﬁciently perform message passing factor graph. leads efﬁcient approach inference grammar model using belief propagation underlying computational engine. inference belief propagation naturally combines bottom-up top-down contextual information leads robust algorithm aggregating evidence. show experiments different applications demonstrate generality framework. ﬁrst application involves detecting curves noisy images address problem using grammar generates collection curves using ﬁrst-order markov process. second application involves localizing faces parts faces images. case grammar captures spatial relationships parts face. applications framework leads robust inference algorithms effectively combine weak local information reason scene. primary motivation work objects scenes represented using hierarchical structures deﬁned compositional rules. instance faces composed eyes nose mouth. similarly geometric objects curves deﬁned terms shorter curves recursively described. hierarchical structure deﬁned compositional rules deﬁnes rich description scene captures presence different objects relationships among them. moreover compositional rules provide contextual cues inference ambiguous data. example presence parts face scene provides contextual cues presence parts. models consider every object type ﬁnite alphabet pose ﬁnite large pose space. classical language models generate sentences using single derivation grammars consider generate scenes using multiple derivations. derivations unrelated share sub-derivations. allows general descriptions scenes. show represent distributions deﬁned probabilistic scene grammars using factor graphs loopy belief propagation approximate inference. inference simultaneously combines bottom-up top-down contextual information. example faces deﬁned using composition eyes nose mouth evidence face parts provides contextual inﬂuence whole composition. inference message passing naturally captures chains contextual evidence. also naturally combines multiple contextual cues. example presence provide contextual evidence face different demonstrate practical feasibility approach different applications curve detection face localization. figure shows samples different grammars experimental results. contributions work include uniﬁed framework contextual modeling used variety applications; construction maps probabilistic scene grammar factor graph together efﬁcient message passing scheme; experimental results showing effectiveness approach. probabilistic grammars compositional models widely used parsing sentences natural language processing recursive descriptions objects using grammars production rules also widely used computer graphics generate geometric objects biological forms landscapes variety compositional models used computer vision. models consider closely related markov backbone model related approaches include previous methods relied mcmc heuristic methods inference dynamic programming scenes single objects. models consider generalize part-based models object detection pictorial structures constellations features particular grammars consider deﬁne objects composed parts allow modeling objects variable structure. models consider also explicitly capture scenes multiple objects. point departure probabilistic scene grammar deﬁnes distribution scenes. approach based markov backbone scenes deﬁned using library building blocks bricks type pose. bricks generated spontaneously expansions bricks. leads hierarchical organization elements scene. deﬁnition probabilistic scene grammar consists ﬁnite symbols types ﬁnite pose space symbol ﬁnite production rules rule form anr} denote rules symbol left-hand-side denote i-th symbol right-hand-side rule bricks present rule brick pose directed graph capturing bricks generate bricks production. rule include grammar acyclic acyclic. topological ordering ordering bricks appears whenever generate acyclic compute topological ordering topological sorting vertices note topological ordering bricks brick included considered expansion. particular brick expanded exactly once. leads derivation trees rooted brick scene. expansion different bricks generate brick leads collision derivations. derivations collide share sub-derivation rooted point collision. derivations terminate using rules form early termination branch probability although assumed acyclic grammar derivation distribution equation factor graph construction also applied arbitrary grammars. makes possible deﬁne probability distributions scenes using cyclic grammars without relying generative process formulation. perform approximate inference factor graph representation loopy belief propagation describe compute messages efﬁciently factor graphs represent scene grammars. factors model kinds factor deﬁned equation captures noisy-or distribution factors deﬁned equations capture categorical distributions outcome probabilities depend state switching random variable. figure shows local graphical representation types factors. computation messages variables factors follows standard equations. describe efﬁciently compute messages factors variables. computational complexity message updates kinds factors linear degree factor. derivations assume messages non-zero value. consider factor represents noisy-or relationship binary inputs ...yn binary output suppose leak noisy-or probability independent failure parameter grammar ﬁxed structure learn production rule probabilities self-rooting parameters approach involves iterative updates. iteration compute conditional marginal probabilities training examples current model parameters update model parameters according sufﬁcient statistics derived output lbp. marginal probability brick expanded using rule training example factor graph representation corresponds marginal probability random variable takes particular value quantity approximated output lbp. update demonstrate generality approach conducted experiments different applications curve detection face localization. previous approaches problems typically fairly distinct methods. here demonstrate handle problems within framework. particular used single implementation general computational engine applications. computational engine perform inference learning using arbitrary scene grammars. report speed inference performed laptop intel .ghz ram. framework implemented matlab/c using single thread. study curve detection used berkeley segmentation dataset following experimental setup described dataset contains natural images object boundaries manually marked human annotators. experiments used standard split dataset training images test images. image boundaries marked single human annotator deﬁne ground-truth binary contour maps model binary contour maps ﬁrst-order markov process generates curves different orientations varying lengths. grammar deﬁned symbols consider curves possible orientations. image size pose space grid pose space grid. express rules grammar denotes rotation consider generating horizontal curve orientation starting pixel process starts brick expansion brick generate brick denote pixel part curve scene. expansion ﬁrst rule ends curve expansion rules continues curve three pixels right values right rules indicate probabilities. learn rule probabilities self-rooting parameters used approach outlined section show random contour maps generated grammar figure model generates multiple curves single image self-rooting parameters. figure curve detection results bsds test set. ground-truth contour maps. middle noisy observations bottom estimated probability curve goes pixel dark values high-probability pixels. curve |i). involves running factor graph representing curve grammar. inference observed image model equation factor graph means variable connected receives ﬁxed-message inference test image took hours. quantitative evaluation compute score corresponding area precision-recall curve obtained thresholding |i). also evaluate baseline no-context model probability pixel belongs curve computed using observation pixel. grammar model obtained score no-context baseline achieved score comparison score reported single-scale field-of-patterns model. contextual information deﬁned curve grammar described signiﬁcantly improves curve detection performance. although method performed well detecting curves extremely noisy images model trouble ﬁnding curves high curvature. believe primarily grammar used notion curvature. possible deﬁne detailed models curves improve performance. however note simple ﬁrst-order model curves curvature information sufﬁcient compete well approaches study face localization performed experiments faces wild dataset dataset contains faces unconstrained environments. goal task localize face image well face parts eyes nose mouth. randomly select images training images testing. although dataset comes annotated identity persons image come part annotations. manually annotate training test images bounding information parts face left right nose mouth. examples manual annotation shown figure face grammar symbols face left right nose mouth symbol associated poses form represent position scale image. refer collection symbols parts face. grammar single rule form express geometric relationship face parts scale-dependent figure localization results. left annotated ground-truth bounding boxes. middle results grammar model. right results baseline model using ﬁlters alone. parts face left right nose mouth offset region uncertainty pose space. offset captures mean location part relative face region uncertainty captures variability relative locations. concretely suppose face pose then part face would expand part somewhere uniform region centered sbz. part-dependent base offset allows express information mouth typically near bottom face nose typically near middle face. dependence offset scale face allows place parts correct position independent face size. model relationship scales face part similar way. modeling relation scales allows represent concepts large faces tending large parts. learn geometric parameters part offsets collecting statistics training data. figure shows samples scenes face generated grammar model estimated training images face dataset. note location scale objects varies signiﬁcantly different scenes relative positions objects fairly constrained. samples scenes multiple faces included supplemental material. data model based ﬁlters train ﬁlters using publicly-available code train separate ﬁlters symbol grammar using annotated images deﬁne positive examples. negative examples taken pascal dataset images containing class people removed. score ﬁlter real-valued. convert score probability using platt’s method involves ﬁtting sigmoid. allows estimate |score) symbol observation model require quantity interpreted proportionality constant. note |score)/p approximate using self-rooting probability connect data model grammar normalized scores ﬁlter used deﬁne messages corresponding bricks factor graph. result inference grammar model leads probability object type pose image. show detection localization results images multiple faces supplementary material. quantify performance model localizing face parts images containing single face take highest probability pose symbol. baseline consider localizing symbol using ﬁlter scores independently without using compositional rule. figure shows localization results. results illustrate context deﬁned compositional rule crucial accurate localization parts. inability baseline model localize part implies local image evidence weak. making contextual information form compositional rule perform accurate localization despite locally weak image evidence. provide quantitative evaluation grammar model baseline model table face localization accuracy models comparable. however attempting localize smaller objects eyes context table mean distance part ground truth location. standard deviations shown brackets. grammarfull denotes grammar model faces ﬁlters symbols. grammar-parts denotes grammar model ﬁlter face symbol. grammar models signiﬁcantly outperform baseline localization accuracy. further localization face symbol grammar-parts good suggesting context alone sufﬁcient localize face. becomes important since local image evidence ambiguous. also experiment grammar model without ﬁlter face. here grammar unchanged data model associated face symbol. seen bottom table localize faces well despite lack face data model suggesting contextual information alone enough accurate face localization. inference using grammar model test image took minutes. probabilistic scene grammars deﬁne priors capture relationships objects scene. using factor graph representation apply belief propagation approximate inference models. leads robust algorithm aggregating local evidence contextual relationships. framework quite general practical feasibility approach illustrated different applications. cases contextual information provided scene grammar proves fundamental good performance.", "year": 2016}