{"title": "Online Edge Grafting for Efficient MRF Structure Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Incremental methods for structure learning of pairwise Markov random fields (MRFs), such as grafting, improve scalability to large systems by avoiding inference over the entire feature space in each optimization step. Instead, inference is performed over an incrementally grown active set of features. In this paper, we address the computational bottlenecks that current techniques still suffer by introducing online edge grafting, an incremental, structured method that activates edges as groups of features in a streaming setting. The framework is based on reservoir sampling of edges that satisfy a necessary activation condition, approximating the search for the optimal edge to activate. Online edge grafting performs an informed edge search set reorganization using search history and structure heuristics. Experiments show a significant computational speedup for structure learning and a controllable trade-off between the speed and the quality of learning.", "text": "incremental methods structure learning pairwise markov random ﬁelds grafting improve scalability large systems avoiding inference entire feature space optimization step. instead inference performed incrementally grown active features. paper address computational bottlenecks current techniques still suffer introducing online edge grafting incremental structured method activates edges groups features streaming setting. framework based reservoir sampling edges satisfy necessary activation condition approximating search optimal edge activate. online edge grafting performs informed edge search reorganization using search history structure heuristics. experiments show signiﬁcant computational speedup structure learning controllable trade-off speed quality learning. powerful family approaches learning structure markov random ﬁelds based minimizing ℓ-regularized scores negative likelihood fully connected mrf. regularization reduces parameters irrelevant edges zero. main challenge using methods that large enough mrfs feature space becomes extremely large causes overwhelmingly high computational cost. active-set methods grafting introduced promote scalability. despite beneﬁts active-set learning grafting retains signiﬁcant computational costs mandatory pre-learning step grafting requires computing sufﬁcient statistics possible variable pairs enable greedy activation tests entire search space iteration grafting requires search entire combinatorial space possible edges. furthermore grafting reason graph structure activating single parameter time. paper introduces online edge grafting active-set method activates edges online fashion groups parameters. derive edge-activation test using structured group-ℓ learning objective. edge search space stored min-heap priority queue edges assigned different search priorities. method performs online edge activation using control sample candidate edges stored limited-memory reservoir. search priorities updated based search history structural information derived partially constructed structure learned far. framework provides high scalability since allows on-demand computation sufﬁcient statistics edges likely activated. strategy eliminates heavy computation grafting’s pre-learning phase enabling online edge grafting start—and even ﬁnish cases—learning well grafting able begin learning. also introduce trade-off parameter balance speed learning quality learned mrfs. experiments show online edge grafting scales better large datasets grafting. synthetic experiments show proposed method performs well recovering true structure real data experiments show obtain high quality learned mrfs large diverse datasets. representing cliques variables feature functions often deﬁned indicator functions. case pairwise markov random ﬁelds clique either refer node edge. pairwise associated undirected graph nodes corresponding variables edges corresponding pairwise cliques. factorization pairwise given data {x}m=...n likelihood expressed pw). high likelihood reﬂects model’s ability represent predict data. numerical stability formulate learning minimization negative likelihood correspond vectors respectively. minimizing often done using gradient-based methods. gradient respect feature model’s expectation feature function gradient respect feature words feature-wise gradient error data expectation model expectation goal learning seen minimizing error. avoid over-ﬁtting promote sparsity learned weights learned markov network classical methods regularization solve minw λ||w||. despite clarity expression computation gradient prohibitively expensive data expectation requires computation sufﬁcient statistics every unary pairwise feature. signiﬁcant computations especially large datasets. classical formulation ignores structure mrfs treats parameters independently. treatment dismisses importance local consistencies intrinsic identity variable edge. finally inference model expectations generally p-complete main focus existing techniques minimizing cost inference subroutine active-set methods allow learning optimization compute inference simpler mrfs. various approximate inference methods efﬁciently compute approximate expectations loopy belief propagation variants proposed work addresses ﬁrst challenges sufﬁcient statistics computation structure coherence expands active activating feature using gradient test. gradient test based karush-kuhn-tucker conditions ℓ-regularized objective. grafting converges activation step feature predeﬁned maximum number features fmax reached. grafting startup bottleneck computational cost compute sufﬁcient statistics feature system variables maximum number states smax. addition grafting performs exhaustive o-complexity search activate feature. bottlenecks translate poor scalability large systems datasets. variant called grafting-light faster extension grafting aims reduce computational cost optimization running orthant-wise gradient step time feature selected. although method produces faster optimization step address cost exhaustive activation tests likely produce high number irrelevant active features yielding slower learning stronger risk over-ﬁtting. although grafting grafting-light fairly suitable learning mrfs consider structural information consider different clique-memberships features. furthermore grafting methods require computing sufﬁcient statistics feature search space even activating ﬁrst feature. operation requires cost prohibitively expensive large-scale datasets. address issues proposed method. propose online edge grafting method grafts edges—as opposed features—in online fashion using variation reservoir sampling method online activates edges without considering entire search space performs edge grafting activates entire edges groups parameters. online edge grafting computes statistics on-demand within learning loop reorganizes search space assigning search priorities edges based search history structure information derived partially constructed graph. goal prioritize testing edges likely relevant. ﬁrst introduce exhaustive variant approach introduce online activation mechanism. ﬁrst extension classical feature-based grafting encodes structural information objective function. refer variation edge grafting exhaustive version proposed online method. edge grafting activates edges instead features. accordingly redeﬁne search active respectively sets inactive active edges. include structural information likelihood function group-ℓ formulation. learned parameters node edge tend homogeneous whole edges tend sparse. deﬁne group-ℓ negative likelihood follows group either refer node edge. consequently refers sub-vector containing weights related features group deﬁne number states clique. elastic-net methods ℓ-norm avoid shortcomings group-ℓ regularization parameter imbalance aggressive group selection. using group-ℓ regularizer maintaining active edges edge grafting operates analogously grafting except activates parameters entire edge potentials once. group-ℓ formulation edges allows sparsity consistent graph structure intuitions variable independence mrfs. summarize edge grafting discussed section algorithm deﬁne edgenum maximum allowed number edges graft initialize possible edges compute sufﬁcient statistics addedeges continue true edgenum addedeges continue improvements edge grafting addresses shortcoming using classical grafting structure learning. however still requires compute sufﬁcient statistics possible edges search possible edges iteration. next introduce full proposed approach addresses bottlenecks. edge grafting results fewer activation optimization iterations grafting since activates groups parameters rather edge features. however still suffers major bottlenecks compute sufﬁcient statistics possible edges perform activation tests edges iteration. operations computational cost. address bottlenecks derive online edge grafting activates edges without exhaustively computing sufﬁcient statistics performing activation tests. instead approach starts activating edges computing small fraction edge sufﬁcient statistics. naive strategy online activation would activate ﬁrst encountered edge satisﬁes however approach introduce large number spurious edges. propose adaptive approach inspired best-choice problems uses control sample edges satisfying stored limited-memory reservoir maintaining reservoir potential edges activate increase probability algorithm activating relevant edges. overall method summarized follows. algorithm ﬁrst initializes min-heap priority queue possible edges. main learning loop extracts edges priority queue places edges pass activation test reservoir retaining edges violate optimality condition reservoir. enough edges seen algorithm selects non-adjacent edges reservoir activate performs inference update model expectations. priority queue updated based newly estimated structure next iteration main loop begins. quadratic-time operations within main loop algorithm quadratictime operation initialization priority queue could replaced randomsampling approach unseen edges. even though elements priority queue dependency quadratic-time operations enabling prioritization management large search space. detail speciﬁc strategies various phases remainder section. online edge grafting uses strategies inﬂuence reservoir potential edges activate order contain edges likely relevant. unknown edges satisfy activation iteration. construct control sample activate edges using conﬁdence interval. ﬁrst activation iteration search stored minheap priority queue equal priorities. then parse priority queue extracting edge time generating prioritized stream edges test. tested edge satisﬁes gets added otherwise gets ignored. maximum number edge tests tmax reached start activating edges. maintain high-quality reservoir edge activation starts capacity ﬁrst edge activation iteration. furthermore reaches capacity tmax reached replace minimumscoring edge newly tested edge whenever higher score. simple strategy activate edges reservoir high activation scores consider edges least above-average score. compute average activation score deﬁne conﬁdence interval choosing edges likely relevant activation scores less maxe∈r deriving edges activated decreasing order respect scores. avoid redundant edges promote scale-free structure activate edges adjacent. note select maximum-scoring edge. reducing increases number edges activated certain step also result adding spurious edges. experiments show impact different values quality learned mrfs convergence speed. figure high-level operational scheme edge activation mechanism. left right ﬁrst structure priority queue initialized possible edges next diamond-shaped represents activation test edge either added reservoir frozen edge container tmax represents maximum number edge tests reached edges activated. gray dashed line right indicates injection minimum scoring edge full. gray dashed line left indicates reﬁlling priority queue emptied frozen edges. search space reorganization performed assigning updating search priority edges priority queue. reorganization increase quality received stream edges increase reservoir quality. leverage search history structural information. search history given activation iteration edge small activation score unlikely satisfy future placed toward tail priority queue. deﬁne edge violation offset activation tests fail edges dropped low-score edges immediately returned priority queue instead placed along violation offsets separate container refer edges frozen edges. search priority queue emptied reﬁll re-injecting frozen edges respective violation offsets priorities. partial structure information active grows underlying graph resulting partial structure contains rich information dependencies variables. rely hypothesis graphs real networks scale-free structure therefore promote structure learned graph encouraging testing edges incident central nodes. start measuring node centrality partially constructed graph detect nodes. degree-based node centrality node fraction possible neighbors connected |ni| centrality threshold identify hubs ˆc}. finally prioritize edges incident nodes decrementing min-heap value total cost updating priority queue represents cost updating priority edge min-heap structure. reorganizing priority queue pushes edges higher chance relevant front priority queue. reorganization encourages higher-quality reservoir activates higher-quality edges activation iteration. deﬁne edgenum maximum allowed number edges graft initialize possible edges fill elements assign equal priorities addedeges fill capacity edgenum addedeges takes algorithm edge tests reservoir pass online edge grafting performs operations compute sufﬁcient statistics necessary reservoir activate ﬁrst edges. start grafting ﬁrst edge. since method provides drastic speed circumvent quadratic term replace linear term. worth noting priority-queue initialization complexity online edge grafting. onetime cost scale number training examples avoided using random sampling edges unseen edges rejecting edges seen prioritized using hash-collision strategy previously unseen edge desired. completeness include pseudocode discussed algorithms appendix. perform experiments synthetic real data linux machine -ghz processor. scalability analysis datasets varying variable dimensions dataset sizes. synthetic setting simulate mrfs generate data using gibbs sampler. real setting three diverse datasets. experiments using feature-based grafting proved extremely slow would well methods edge-grafting family. instead primary baselines exhaustive edge grafting online edge grafting reservoir activates ﬁrst edge passes edge-activation condition. interested measuring recovery true underlying graphical model structure well ability learned model data distribution. structure-recovery metric recall deﬁned fraction true edges recovered. monitor recall time. slope recall time indicator precision high slope indicates method adds relevant edges small slope indicates activation several false-positive edges. also measure learning objective compare well method optimizes. measure well learned mrfs represent true data distribution measure pseudo-likelihood held-out test data. construct random scale-free-structured mrfs variables states each. generate preferential-attachment graphs size parameters respectively. grow graph attaching nodes edges preferentially attached existing nodes high node centrality. algorithm produces edges. node created edge sample corresponding parameters normal distribution mean equal standard deviations setting puts emphasis edges helps avoid overﬁtting model unary potentials. grid search detect best combination limit search range case choose pair produces best recall test nlpl baseline i.e. edge grafting. worth noting notice high sensitivity methods different values whereas smaller values produces spurious edges different methods. ﬁrst investigate behavior different methods executed violating edge remains search space. interested measuring different methods’ speeds convergence validating lead similar solutions. figure shows methods appear reach similar minimum value objective function faster convergence rate online edge grafting. ﬁrst-hit baseline starts fast descent rate activation lower-violation edges eventually causes converge slower edge grafting. using reservoir maintains steep descent convergence. experiments also conﬁrm edge grafting suffers major computational bottleneck computing sufﬁcient statistics causing start optimization later online edge grafting online edge grafting nearly converged. figure plots recall true edges time. maximum number edges activated stopping early algorithm exhausts limit. increasing values better recall different sizes. however comes cost expensive learning optimization. fact smaller values online edge grafting tends activate edges lower quality introduces greater number false-positive edges. also observe that increasing amounts variables learning online edge grafting exhaustive edge grafting increases demonstrates better scalability online algorithm. figure plots learning objective exhibits similar trends. lower values enable faster optimization. however difference quality less contrasted pronounced learning objectives different values hypothesize effect model different structure mimic data distribution. experiments heldtesting data presented figure show positive correlation learning objective test negative pseudo-likelihood conﬁrms learned models over-ﬁt data even small values result suggests that higher values help recover true structure sole goal learning structure produce good generative model lower values used speed learning small loss quality. ﬁrst-hit baseline reaches maximum number added edges faster. however results slower overall convergence figure poor qualitative quantitative behavior seen figures since adds poor-quality edges. smaller graphs edge grafting advantage greedy search worst-violating edge enables large improvements objective. however larger graphs even though edge grafting precomputes sufﬁcient statistics cost greedy search causes objective descend slower rate online variants avoid exhaustive search. evaluate effectiveness structure heuristics figure plots edge recall limited number activated edges without structure-based priority queue reorganization. signiﬁcant recall curves shows produce higher-quality edge stream priority queue resulting higher-quality reservoir relevant edges activate. three datasets usda plants jester yummly recipes. plants dataset comprises plants different locations world. treat plants instances locations binary variables leading parameters. jester dataset joke ratings dataset containing jokes user ratings. jokes variables user ratings instances. ratings interval continuous interval discrete interval leading parameters. sample recipes yummly dataset. treat common ingredients binary variable consider recipe data instance leading parameters. figures show positive correlation minimization learning objective testing negative pseudo-likelihood. results similar synthetic experiments online edge grafting converges faster edge grafting smaller values result faster convergence. different-sized datasets illustrate higher scalability online edge grafting improvement convergence time edge grafting increases larger datasets. presented online edge grafting method based group-ℓ formulation online reservoir sampling. incremental method activates edges instead features uses reservoir approximate greedy activation. theoretical analysis shows method provides higher scalability exhaustive edge grafting avoiding major bottlenecks. experiments synthetic real data demonstrate online edge grafting yields faster convergence multiple datasets producing good structure recovery predictive ability similar costly exhaustive edge grafting method.", "year": 2017}