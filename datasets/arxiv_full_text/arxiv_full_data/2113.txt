{"title": "The Max $K$-Armed Bandit: A PAC Lower Bound and tighter Algorithms", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "We consider the Max $K$-Armed Bandit problem, where a learning agent is faced with several sources (arms) of items (rewards), and interested in finding the best item overall. At each time step the agent chooses an arm, and obtains a random real valued reward. The rewards of each arm are assumed to be i.i.d., with an unknown probability distribution that generally differs among the arms. Under the PAC framework, we provide lower bounds on the sample complexity of any $(\\epsilon,\\delta)$-correct algorithm, and propose algorithms that attain this bound up to logarithmic factors. We compare the performance of this multi-arm algorithms to the variant in which the arms are not distinguishable by the agent and are chosen randomly at each stage. Interestingly, when the maximal rewards of the arms happen to be similar, the latter approach may provide better performance.", "text": "consider k-armed bandit problem learning agent faced several sources items interested ﬁnding best item overall. time step agent chooses obtains random real valued reward. rewards assumed i.i.d. unknown probability distribution generally differs among arms. framework provide lower bounds sample complexity correct algorithm propose algorithms attain bound logarithmic factors. compare performance multi-arm algorithms variant arms distinguishable agent chosen randomly stage. interestingly maximal rewards arms happen similar latter approach provide better performance. classic stochastic multi-armed bandit problem learning agent faces stochastic arms wishes maximize cumulative reward highest mean reward model studied extensively statistical learning literature example comprehensive survey. consider variant problem called k-armed bandit problem variant objective obtain sample highest possible reward precisely considering setting objective return -correct sample namely sample reward value ǫ-close overall best possible reward probability larger addition wish minimize sample complexity namely expected number samples observed learning algorithm terminates. classical problem algorithms best sense presented lower bounds sample complexity presented essential difference respect work objective -correct sample case. scenario considered max-bandit model relevant single best item needs selected among several clustered sets items represented single arm. sets represent parts come different manufacturers produced different processes candidates referred different employment agencies ﬁnding best match certain genetic characteristics different populations choosing best channel among different frequency bands cognitive radio wireless network. max-bandit problem apparently ﬁrst proposed reward distribution functions speciﬁc family algorithm upper bound sample complexity increases provided case discrete rewards another algorithm presented without performance analysis. later similar model objective maximize expected value largest sampled reward given number samples studied work attained best reward compared expected reward obtained oracle samples best time. algorithm suggested shown secure upper bound order difference determined properties distribution functions decreases away speciﬁc functions family. basic assumption present paper known lower bound available tail distributions namely probability reward given close maximum. special case probability densities near maximum larger given value consider general function classes. assumption provide algorithm sample complexity increases ln−ln provides improvement factor result obtained speciﬁc model. compare result observe choice algorithm obtain expected shortfall largest sample respect maximal reward possible order furthermore provide lower bound sample complexity every -correct algorithm holds several arms posses maximal rewards close best arm. lower bound shown coincide logarithmic term upper bound derived proposed algorithm. basic feature max-bandit problem goal quickly focusing best sampling much possible. interest compare obtained results alternative approach ignores distinction arms simply draws sample random round. interpreted mixing items associated sampling; accordingly refer variant uniﬁed-arm problem. problem actually coincides so-called inﬁnitely-many armed bandit model studied speciﬁc case deterministic arms studied conclusion weather apply multi-arm approach uniﬁed-arm approach inconclusive. however rule thumb maximal possible rewards many arms optimal multi-arm approach better performance. paper proceeds follows. next section present model. section provide lower bound sample complexity every -correct algorithm. section present -correct algorithms provide upper bound sample complexity them. ﬁrst algorithm simple bound order lower bound logarithmic term second algorithm complicated believe bound larger double logarithmic term lower bound. section consider comparison uniﬁed-arm case. section close paper concluding remarks. certain proofs differed appendix space limitations. consider ﬁnite arms denoted stage learning agent chooses real valued reward obtained arm. rewards obtained independent identically distributed distribution function denote maximal possible reward µ∈r{µ|fk assumed ﬁnite maximal reward among arms maxk∈k throughout paper shall make following assumption. assumption exist known constants that every holds algorithm max-bandit model samples time step based observed history require algorithm terminate random number samples ﬁnite probability return reward maximal reward observed entire period. algorithm said -correct turning proposed algorithm provide lower bound sample complexity -correct algorithm. bounds holds assumption case complicated analysis still unclear whether lower bound holds case. lower bound interpreted summing minimal number times optimal needs sampled. important observe several optimal arms excluded summation. indeed bound effective several optimal arms denominator summand larger arms. appear surprising ﬁrst sources good rewards available; however single strictly better others quickly singled many arms nearly optimal rewards samples waisted determining best. proof theorem provided appendix proceeds showing algorithm -correct sample complexity lower certain threshold reward distributions algorithm cannot -correct related reward distributions. provide -correct algorithms. ﬁrst algorithm based sampling highest upper conﬁdence bound maximal reward time step second algorithm based arms elimination. algorithm starts sampling certain number times arm. then repeatedly calculates index interpreted certain upper bound maximal reward samples largest index. algorithm terminates number samples largest index certain threshold. idea similar algorithm provided following corollary present ratio lower bound presented theorem upper bound theorem corollary upper bound sample complexity order lower bound theorem logarithmic factor proof. every follows establish theorem ﬁrst bound probability event upper bound best maximal reward. then bound largest number samples algorithm terminates assumption upper bound best maximal reward. algorithm starts sampling certain number times arm. then repeatedly calculates index interpreted certain upper bound maximal reward eliminates arms index maximal sampled reward far. sample retained arms number times doubled sampling phase. idea similar median elimination algorithm provided provide performance analysis algorithm however since number times conﬁdence bounds correct logarithmic number total samples therefore believe upper bound sample complexity algorithm would algorithm multiplied upper bound would order lower bound theorem double logarithmic terms. model arms uniﬁed uniﬁed sample effectively obtained random arm. uniﬁed-arm model agent samples uniﬁed certain chosen uniformly reward sampled arm. denote uniﬁed remainder section provide lower bound sample complexity correct algorithm attains order bound uniﬁed-arm model. then discuss approach better different model parameters provide examples illustrate cases. algorithm certain number rewards sampled algorithm chooses best among them. following theorem provide bound sample complexity achieved algorithm theorem assumption algorithm -correct sample complexity bound k))β hence upper bound sample complexity algorithm smaller lower bound uniﬁed-arm model theorem provide example illustrate case numerically. example sample complexity attained algorithm lower bound uniﬁed-arm model sample complexity attained algorithm multiplicative factor upper bound sample complexity algorithm bound larger lower bound uniﬁed-arm model theorem following example illustrate case numerically. example remain example sample complexity algorithm shown example cases bound sample complexity algorithm larger algorithm comparing upper bounds algorithms believe logarithmic factor bound algorithm required. observed comparing lower upper bounds multi-arm uniﬁed-arm model uniﬁed-arm algorithm provides tighter upper bound therefore beneﬁt obtained multi-arm model small proﬁt obtained applying multi-arm algorithm turns loss. results compared uniﬁed-arm model learning algorithm effectively uniﬁes different arms one. multi-arm algorithm usually performs better cases particular arms optimal uniﬁed algorithm provide better performance. still remains shown whether algorithm provides performance beneﬁts approaches devised.", "year": 2015}