{"title": "Graph Kernels via Functional Embedding", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We propose a representation of graph as a functional object derived from the power iteration of the underlying adjacency matrix. The proposed functional representation is a graph invariant, i.e., the functional remains unchanged under any reordering of the vertices. This property eliminates the difficulty of handling exponentially many isomorphic forms. Bhattacharyya kernel constructed between these functionals significantly outperforms the state-of-the-art graph kernels on 3 out of the 4 standard benchmark graph classification datasets, demonstrating the superiority of our approach. The proposed methodology is simple and runs in time linear in the number of edges, which makes our kernel more efficient and scalable compared to many widely adopted graph kernels with running time cubic in the number of vertices.", "text": "propose representation graph functional object derived power iteration underlying adjacency matrix. proposed functional representation graph invariant i.e. functional remains unchanged reordering vertices. property eliminates diﬃculty handling exponentially many isomorphic forms. bhattacharyya kernel constructed between functionals signiﬁcantly outperforms state-of-the-art graph kernels standard benchmark graph classiﬁcation datasets demonstrating superiority approach. proposed methodology simple runs time linear number edges makes kernel eﬃcient scalable compared many widely adopted graph kernels running time cubic number vertices. graphs becoming ubiquitous modern applications spanning bioinformatics social networks search computer vision natural language processing etc. computing meaningful similarity measure graphs crucial prerequisite variety learning algorithms operating graph data. notion similarity typically varies application. designing similarities graphs desirable measure incorporates rich structural information aﬀected spurious transformations like reordering vertices. report mainly acrhival purppose. essentially paper initially submitted conference dec. seperate paper focuses social networks used similar methodologies note that certain applications graphs come additional label information node edge labels additional annotations always available every domain typically expensive obtain. paper focus basic graph structures without assuming additional information. common approach computing kernels extract explicit feature graph compute kernel values certain standard operations features line techniques typically make graph invariants eigenvalues graph laplacian features. example uses harmonic analysis techniques extract graph invariants. shown simple linear kernel i.e. product graph invariant numbers outperforms many graph kernels. alternatively design kernel function given graphs directly using similarity example random walk kernel based counting common random walks given graphs. another example shortest-path kernel based counting pairs vertices graphs similar shortest distance them. although random walk kernels path based kernels still among widely adopted graph kernels common disadvantage walks paths capture information substructures present graph address problem ﬂurry interest arose kernels based counting common subgraph patterns. counting possible common subgraphs known np-complete development graph kernels focusing counting small subgraphs; example counts common subgraphs nodes also called graphlets. kind technique popular social network classiﬁcation. recently representing graphs think adjacency matrix rn×n matrix operator operating natural characterizing operator transforms given vector idea pioneered case graphs works diﬀusion kernels followed binet-cauchy kernels here adjacency matrix treated dynamical system similarity measure systems used similarity corresponding graphs. observed random walk kernel simply discounted summation similarity summation taken take account covariance structure dynamical system. particular given adjacency matrix think series identifying characteristics series relates kind autocovariance structures crucial time series modeling literature. unfortunately information taken consideration computing similarity expressive kernels arma models like determinant kernel however determinant kernel arma models applicable graphs sensitive reordering rows noted given permutation matrix adjacency matrix leads different dynamical systems graphs represented used histograms size four subgraphs classifying facebook social networks. however simply counting common substructures like walks paths subgraphs etc. ignores crucial relative information substructures. instance information diﬀerent triangles relatively embedded graph structure cannot captured simply counting number triangles. relative information show paper necessary discriminating diﬀerent graph structures. paper follows altogether diﬀerent approach. represent graph expressive functional object. ﬁrst dynamical properties graph adjacency matrix construct informative summary graph. impose probability distribution summary show distribution graph invariant. bhattacharyya kernel obtained distribution call power kernel signiﬁcantly outperforms wellknown graph kernels standard benchmark graph classiﬁcation datasets. addition show that unlike kernels require time compute kernel computed time linear number edges makes proposed methodology signiﬁcantly practical larger graphs. given graph nodes denote adjacency matrix rn×n. paper entries binary i.e. means edge node node interchangeably terms nodes vertices terms graph adjacency matrix graph always assumed unlabeled undirected unweighted default number nodes unless otherwise speciﬁed. vector ones. vector mean column vector i.e. matrix. avoid overloading subscripts follow matlab style notation denoting rows columns matrix. given matrix denote refer column. vector denote component. every permutation associated corresponding permutation matrix important property permutation matrix transpose equal inverse eﬀect left multiplying given matrix shuﬄes rows according i.e. eﬀect right multiplying eﬀect columns instead rows. permutation matrix graphs represented proceed computing isomorphic invariant functional representation given graph captures covariance information dynamical system. describe functional embedding next section. simply power iteration matrix small history power iteration often captures sufﬁcient information underlying matrix representation capitalizes fact. ﬁrst extract summary power iteration shown algorithm standard power iteration start given normalized vector iteration generate vector ||x|| recursively. choice normalization important. refer matrix whose column corresponds permutation invariant general equal. however starting vector reordering nodes permutation matrix shuﬄes rows order. fact stated following theorem. intuitive theorem true disregard normalization imagine time step associate every node graph starting number every iteration algorithm multiplication update number every node numbers neighbors. simple recursive argument tells sequence numbers generated node process going change long neighborhood structure preserved. unit vector starting choice distinguish nodes. fact vector treated representation corresponding node graph. kind updates informative motivation behind many celebrated link analysis algorithms including hyper-text induced topic search light theorem associate vec∈ rn×k graph tors corresponding rows permutation invariant representation. proposal therefore mathematical quantity describes vectors representation graph. choices either think subspace represented vectors think vectors samples probability distribution choice depends size case large compared subspace represented vectors dimension almost always whole dimensional euclidean vector space informative. hand large compared subspace representation informative compared ﬁtting distribution. example decide gaussian distribution vectors covariance matrix informative. power iteration converges quickly geometric rate convergence. therefore need much smaller values compared hence associate probability function rows variety permutation independent functional embeddings diﬀerent choices distribution functions. natural distribution function gaussian major reasons similarity computations usually closed form nicely captures correlation structure since always notational convenience drop subscript designing kernels graph ensuring positive semi-deﬁniteness trivial many previously proposed kernels satisfy property since kernel kernel well studied mathematical representation property free immediate consequence result bhattacharyya kernels positive semideﬁnite. value determines number power iterations algorithm adjacency matrix ...λn eigenvalues ...vn corresponding eigenvectors. iteration vector generate since representation deﬁned gaussian density vectors also interpreted gaussian process representation desired property invariant reordering nodes. although theorem captures graph isomorphism direction representation relationship cannot hope would solved graph isomorphism problem. although complexity graph isomorphism problem still open question graphs practice known easy small summary power iteration almost always enough discriminate non-isomorphic graphs. fact real wold graphs usually possess distinct spectral behavior therefore expect embedding eﬀective representation graphs encountered practice. might seem little uncomfortable call distribution vectors never change nothing stochastic. better think representation graph object functional space distribution analogy gives motivation mathematical object vectors simple intuition theorem true given theorem work based extracting permutation invariant features graph using algebraic approach. representation leads invariants. consequence theorem graph invariants. deﬁne number disjoint paths length given graph node points. computing allow repetition nodes. simple observation component fact proven simple inductive argument base case correlations among number paths length length common endpoint. interpreted variance number paths length common endpoint. hindsight diﬃcult aggregated statistics paths diﬀerent lengths starting given node graph invariants. next section information useful discriminating various graph structures. captures information mean statistic diﬀerent kind paths present graph. captures relative structure nodes graph. correlations various kinds paths relative node indicated relative connectivity graph structure. kind relative correlation information missing random walk kernels path based kernels count common paths walks length given graphs. even kernels trying count common small subgraphs power iteration looses information eigenvalue eigenvector exponential rate matrix uniquely characterλ ized eigenvalues eigenvectors need fully capture information matrix. noted here unlike machine learning applications small eigenvalues corresponds noise case information whole spectrum needed. therefore need small values like larger values cause information larger eigenvalues dominate representation make kernel values biased towards dominant spectrum analyze running time step algorithm simplicity max. step requires running algorithm graphs consists matrix vector multiplications iterations. complexity step thus steps compute mean vectors dimension cost steps compute sample covariance matrix whose complexity each. ﬁnal step requires evaluating computed mean covariance matrices requires operations. overall total time complexity computing kernel scratch recommended value usually small constant even large graphs. treating constant time complexity worst case. fact sparsity adjacency matrix actual time complexity total number edges words total running time linear number edges. current state-of-the-art kernels including skew spectrum graph random walk kernel require computations shortest path kernel even costlier. graphs encounter real-world applications general sparse i.e. moreover number edges order number vertices algorithm actually linear makes proposed power kernels scalable even applications. capture relative structural information suﬃciently. capture aggregate behavior paths relative diﬀerent nodes also treated informative summary given graph. gaussian density function exploiting correlation structure. generate functionals rows example generate expressive functional using kernel density estimation vectors theorem guarantees obtained functional graph invariant. believe invariants provide deeper insight could prove beneﬁcial many applications dealing graphs. behaviors graph invariants raise many interesting theoretical questions could independent interest. instance what behavior invariants graph expansion? follow evaluation procedure chose four benchmark graph classiﬁcation datasets consisting graph structure chemical compounds mutag enzymes used diversity terms size well tasks. dataset data point graph structure associated classiﬁcation label. mutag dataset mutagenic aromatic hetroaromatic nitro compounds labeled according whether mutagenic eﬀect gram-negative bacterium salmonell typhimurium. maximum number nodes dataset mean around maximum number edges mean around enzymes dataset protein tertiary structure used consists enzymes brenda enzymes database multiclass classiﬁcation task enzyme label level class belongs maximum number nodes dataset average around maximum number edges mean around balanced datasets classify compounds based whether active anti-cancer screen maximum number nodes mean around maximum number edges mean around focus remain evaluating basic structure captured functional representation therefore focus comparisons methodologies relying node edge label information. repeat evaluation procedure followed power kernel. evaluations consists running kernel four datasets using diﬀerent kernel. standard evaluation procedure used follows. first split dataset folds identical size. combine folds split parts ﬁrst parts train c-svm part validation best performing value choice train c-svm folds predict fold acting independent evaluation set. procedure repeated times fold acting independent test once. dataset whole procedure repeated times randomizing partitions. mean classiﬁcation accuracy standard errors shown table since results averaged runs different partitions numbers stable. borrowed accuracy values state-of-the-art unlabeled graph kernels random walk kernel shortest path kernel graphlet count kernel reduced skew spectrum graph parameters kernels optimized best performance. noted before value large. though choice tune value different datasets independently keep things simple allow easy replication results report results ﬁxed value four datasets. results mutag dataset power kernel outperforms kernels remaining datasets. larger datasets larger graphs compared mutag beat previous best performing kernel based skew spectrum graph huge margin. datasets power kernel gives classiﬁcation accuracy around best performing baseline achieve around case enzymes dataset shortest path kernel performs best among baseline kernels achieves accuracy achieve around signiﬁcant improvement clearly establishes expressiveness representation capturing structure graphs. mutag dataset accuracy achieved reduced skew spectrum kernel power kernel gives believe fact mutag consists relatively much smaller graphs seems graph invariant features generated reduced skew spectrum suﬃciently capture discriminative information dataset. datasets larger graphs features less expressive functional representation hence power kernel leads much better results. table prediction accuracy percentage power kernel state-of-the-art graph kernels four classiﬁcation benchmark datasets. reported results averaged repetitions -fold cross-validation. standard errors indicated using parentheses. always outperform graphlet count kernel random walk kernel shortest path kernel. shows basic representation much expressive superior. surprising capturing higher order correlation information kernels based counting common paths subgraphs small size miss relative information. dissecting graphs small subgraphs looses information. shown section algorithm runs statistics dataset average edges order vertices running time complexity power kernel case actually around competing methods except graphlet count kernel require least therefore huge gain performance. running time complexity graphlet kernel competitive method accuracy wise method much superior. whole procedure power kernel simple since haven’t tuned anything except numbers easily reproducible. determining graphs uniquely determined spectrum general hard problem graphs encountered practice well behaved uniquely determined dynamics. hence proposed embedding loose much information. power kernels clear theorem graphs isomorphic permutation invariance property worry ordering nodes consider long exist gives required bijection. graphs isomorphic spectrum follow diﬀerent behaviors hence kernel value much less illustrate representation satisﬁes property fact spectrum adjacency matrix usually stable small perturbations here perturbations means operations like adding deleting nodes edges. diﬀerent usual small normed perturbations. moreover kernel relies stable statistics covariance mean undergo major jump small changes assuming size graph large. method thus ensures small graph perturbations lead blow causing relatively changes kernel values. although might diﬃcult quantify sensitivity power kernels respect small perturbations graph empirically verify claim. chose four datasets used experiments. dataset randomly sample graphs evaluations. perturb graph structure ﬂipping random edge i.e. choose nodes randomly edge present graph delete edge otherwise edge graph. perturbation process times other thereby obtaining sequence graphs increasing amount perturbations. perturbation compute kernel value perturbed graph original graph. value", "year": 2014}