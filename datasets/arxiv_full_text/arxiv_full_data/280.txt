{"title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in  Visual Question Answering", "tag": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "abstract": "Problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in models that ignore visual information, leading to an inflated sense of their capability.  We propose to counter these language priors for the task of Visual Question Answering (VQA) and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs. Our complete balanced dataset is publicly available at www.visualqa.org as part of the 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA v2.0).  We further benchmark a number of state-of-art VQA models on our balanced dataset. All models perform significantly worse on our balanced dataset, suggesting that these models have indeed learned to exploit language priors. This finding provides the first concrete empirical evidence for what seems to be a qualitative sense among practitioners.  Finally, our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair, also provides a counter-example based explanation. Specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users.", "text": "language vision problems image captioning visual question answering gained popularity recent years computer vision research community progressing beyond bucketed recognition towards solving multi-modal problems. complex compositional structure language makes problems intersection vision language challenging. recent works pointed language also provides strong prior result good superﬁcial performance without underlying models truly understanding visual content. phenomenon observed image captioning well visual question answering instance dataset common sport answer tennis correct answer questions starting what sport correct answer questions starting many. moreover zhang points particular ‘visual priming bias’ dataset speciﬁcally subjects image asking questions thus people question clock tower picture? images actually containing clock towers. particularly perverse example questions problems intersection vision language signiﬁcant importance challenging research questions rich applications enable. however inherent structure world bias language tend simpler signal learning visual modalities resulting models ignore visual information leading inﬂated sense capability. propose counter language priors task visual question answering make vision matter speciﬁcally balance popular dataset collecting complementary images every question balanced dataset associated single image rather pair similar images result different answers question. dataset construction balanced original dataset approximately twice number image-question pairs. complete balanced dataset publicly available http//visualqa.org/ part iteration visual question answering dataset challenge benchmark number state-of-art models balanced dataset. models perform signiﬁcantly worse balanced dataset suggesting models indeed learned exploit language priors. ﬁnding provides ﬁrst concrete empirical evidence seems qualitative sense among practitioners. finally data collection protocol identifying complementary images enables develop novel interpretable model addition providing answer given pair also provides counterexample based explanation. speciﬁcally identiﬁes image similar original image believes different answer question. help building trust machines among users. language priors give false impression machines making progress towards goal understanding images correctly exploiting language priors achieve high accuracy. hinder progress pushing state computer vision aspects multi-modal work propose counter language biases elevate role image understanding vqa. order accomplish goal collect balanced dataset signiﬁcantly reduced language biases. specifically create balanced dataset following given triplet dataset human subject identify image similar results answer question become examples balanced dataset shown fig. random examples seen fig. project website. hypothesis balanced dataset force models focus visual information. question different answers different images respectively) know right answer looking image. language-only models simply basis differentiating cases construction must wrong. believe construction also prevent language+vision models achieving high accuracy exploiting language priors enabling evaluation protocols accurately reﬂect progress image understanding. balanced dataset also particularly difﬁcult picked complementary image close original image semantic space vggnet features. therefore models need understand subtle differences images predict answers images correctly. note simply ensuring answer distribution uniform across dataset would accomplish goal alleviating language biases discussed above. language models exploit correlation question n-grams answers e.g. questions starting clock answer time questions starting standing answer time. need higher entropy across dataset higher entropy image must play role determining motivates balancing perquestion level. complete balanced dataset contains approximately million pairs almost double size dataset approximately million associated answers images coco believe balanced dataset better dataset benchmark approaches publicly available download project website. finally data collection protocol enables develop counter-example based explanation modality. propose novel model answers questions images also ‘explains’ answer imagequestion pair providing hard negatives i.e. examples images believes similar image hand believes different answers question. explanation modality allow users model establish greater trust model identify oncoming failures. main contributions follows balance existing dataset collecting complementary images almost every question balanced dataset associated single image rather pair similar images result different answers question. result balanced dataset also approximately twice size original dataset. evaluate state-of-art models balanced dataset show models trained existing ‘unbalanced’ dataset perform poorly balanced dataset. ﬁnding conﬁrms hypothesis models exploiting language priors existing dataset achieve higher accuracy. finally data collection protocol identifying complementary scenes enables develop novel interpretable model addition answering questions images also provides counterexample based explanation retrieves images believes similar original image different answers question. explanations help building trust machines among users. visual question answering. number recent works proposed visual question answering datasets models work builds dataset antol widely used datasets. reduce language biases present popular dataset resulting dataset balanced twice size dataset. benchmark ‘baseline’ model attention-based model winning model real open ended challenge balanced dataset compare language-only model. data balancing augmentation. high level work viewed constructing rigorous evaluation protocol collecting ‘hard negatives’. spirit similar work hodosh created binary forced-choice image captioning task machine must choose caption image similar captions. compare hodosh implemented hand-designed rules create similar captions images create novel annotation interface collect similar images questions vqa. similar results answer question become built annotation interface collect complementary images amazon mechanical turk workers shown nearest-neighbor images question answer asked pick image list images makes sense answer capture question makes sense explained workers premise assumed question must hold true image select. instance question what woman doing? assumes woman present seen image. make sense question image without woman visible compute nearest neighbors ﬁrst representing image activations penultimate layer deep convolutional neural network particular vggnet using distances compute neighbors. complementary images collected conduct second round data annotation collect answers images. speciﬁcally show picked image question workers collect ground truth answers common answer among answer two-stage data collection process ﬁnally results pairs complementary images semantically similar different answers respectively question since semantically similar model understand subtle differences provide right answer images. example complementary images shown fig. fig. project website. note sometimes possible pick neighbors complementary image. because either question make sense images question applicable neighboring images answer question still cases data collection interface allowed workers select possible. analyzed data annotated possible selection workers found typically happens object talked question small original image thus nearest neighbor images globally similar necessarily contain object resulting question making sense concept question rare perhaps relevant work zhang study goal balancing fairly restricted setting binary questions abstract scenes made clipart using clipart allows zhang human annotators change clipart scene answer question changes. unfortunately ﬁnegrained editing image content simply possible real images. novelty work zhang proposed complementary image data collection interface application real images extension questions benchmarking state-of-art models balanced dataset ﬁnally novel model counter-example based explanations. models explanation. number recent works proposed mechanisms generating ‘explanations’ predictions made deep learning models typically ‘black-box’ noninterpretable. generates natural language explanation image categories. provide ‘visual explanations’ spatial maps overlaid images highlight regions model focused making predictions. work introduce third explanation modality counter-examples instances model believes close belonging category predicted model. build dataset introduced antol real images dataset contains images coco free-form natural language questions million free-form answers dataset spurred signiﬁcant progress domain discussed earlier strong language biases. idea counter language bias following every triplet dataset goal identify image total possible selections make questions dataset. believe sophisticated interface allowed workers scroll many neighboring images could possibly reduce fraction. likely still task would signiﬁcantly cumbersome workers making data collection signiﬁcantly expensive. collected complementary images corresponding answers train test splits dataset. workers picked possible approximately total questions. total collected approximately complementary images train complementary images complementary images test set. addition augment test additional pairs provide additional means detect anomalous trends test data. hence complete balanced dataset contains train test pairs. following original dataset divide test splits test-dev test-standard test-challenge testreserve. details please refer complete balanced dataset publicly available download. publicly released evaluation script experiments. evaluation metric uses groundtruth answers question compute accuracies. described above collected answers every complementary image corresponding question consistent dataset note unlikely possible majority vote answers match intended answer person picking image either inter-human disagreement worker selecting complementary image simply fig. compares distribution answers questiontype balanced dataset original dataset notice several interesting trends. first binary questions signiﬁcantly balanced distribution answers balanced dataset compared unbalanced dataset. baseball slightly popular tennis what sport importantly overall baseball tennis dominate less answer distribution. several sports like frisbee skiing soccer skateboarding snowboard surﬁng visible answer distribution balanced dataset suggesting contains heavier tails. similar trends seen across board colors animals numbers etc. quantitatively entropy answer distributions averaged across various question types increases balancing conﬁrming heavier tails answer distribution. statistics show balanced dataset perfectly balanced signiﬁcantly balanced original dataset. resultant impact balancing performance state-of-the-art models discussed next section. ﬁrst approach training model emphasizes visual information language-priors-alone re-train existing state-of-art models balanced dataset. hypothesis simply training model answer questions correctly balanced dataset already encourage model focus visual signal since language signal alone impoverished. experiment following models deeper lstm question norm image model introduced together dataset. uses embedding image long-short term memory embedding question combines embeddings point-wise multiplication followed multi-layer perceptron classiﬁer predict probability distribution frequent answers training dataset. hierarchical co-attention recent attention-based model ‘co-attends’ image question predict answer. speciﬁcally models question hierarchical fashion word-level phrase-level entire question-level. levels combined recursively produce distribution frequent answers. multimodal compact bilinear pooling winning entry real images track challenge model uses multimodal compact bilinear pooling mechanism attend image features combine attended image features language features. combined features passed fully-connected layer predict probability distribution frequent answers. noted uses image features powerful architecture resnet previous models image features vggnet baselines accuracies models perspective compare following baselines prior predicting common answer training test questions. common answer unbalanced balanced sets. language-only language-only baseline similar architecture deeper lstm question norm image except accepts question input utilize visual information. comparing models languageablations quantiﬁes extent models succeeded leveraging image answer questions. results shown table fair comparison accuracies original dataset create balanced train similar size original dataset benchmarking also report results using full balanced train set. table performance models trained/tested unbalanced/balanced datasets. stands training unbalanced train testing balanced datasets. bhalfb deﬁned analogously. current state-of-art models trained original dataset perform signiﬁcantly worse evaluated balanced dataset compared evaluating original unbalanced dataset ﬁnding conﬁrms hypothesis existing models learned severe language biases present dataset resulting reduced ability answer questions correctly question different answers different images. models trained balanced dataset performance improves further models trained complete balanced dataset accuracy improves increase accuracy suggests current models data starved would beneﬁt even larger datasets. absolute numbers table suggest signiﬁcant room improvement building visual understanding models extract detailed information images leverage information answer free-form natural language questions images accurately. expected construction balanced dataset question-only approach performs signiﬁcantly worse balanced dataset compared unbalanced dataset conﬁrming language-bias original dataset successful alleviation proposed balanced dataset. note addition lack language bias visual reasoning also challenging balanced dataset since pairs images similar image representations learned cnns different answers question. successful models need understand subtle differences images. paired construction dataset allows analyze performance models unique ways. given prediction model count number questions complementary images received correct answer predictions corresponding question received identical answer predictions received different answer predictions. hiecoatt model trained unbalanced dataset pairs answered correctly pairs identical predictions pairs different predictions. comparison trained balanced dataset model answered pairs correctly increase performance moreover predicts identical answers fewer pairs shows training balanced dataset model learned tell difference otherwise similar images. however signiﬁcant room improvement remains. model still tell difference images noticeable difference difference enough result images different ground truth answers question asked humans. benchmark models dataset also train models train+val report results test-standard table papers reporting results dataset suggested report teststandard accuracies compare methods’ accuracies accuracies reported table table accuracy breakdown answer types achieved hiecoatt models trained/tested unbalanced/balanced datasets. stands training unbalanced train testing balanced datasets. bhalfb deﬁned analogously. results shown table first immediately notice accuracy answer-type yes/no drops signiﬁcantly suggests models really exploiting language biases yes/no type questions leads high accuracy unbalanced unbalanced also contains biases. performance drops signiﬁcantly tested balanced signiﬁcantly reduced biases. second note state-of-art models largest source improvement bhalfb yes/no answer-type number answer-type trend particularly interesting since yes/no number answer-types ones existing approaches shown minimal improvements. instance results announced real open ended challenge accuracy top- approaches mere yes/no answer-type category similarly number answer-type accuracies vary respectively. primary differences between current generation state-of-art approaches seem come other answer-type accuracies vary among top- top- entries. ﬁnding suggests language priors present unbalanced dataset lead similar accuracies state-of-art models rendering vastly different models virtually indistinguishable benchmarking different models balanced dataset ﬁnally allow distinguish ‘good’ models others simply high-capacity models tuning biases dataset. counterexamples. propose model asked question image provides answer also provides example images similar input image model believes different answers input question. would instill trust user model fact ‘understand’ concept asked about. instance question what color ﬁre-hydrant? model perceived trustworthy addition saying also adds unlike this shows example image containing ﬁre-hydrant red. time negative explanation counter-example explanation model functions steps. ﬁrst step similar conventional model takes pair input predicts answer apred. second step uses predicted answer apred along question retrieve image similar different answer apred question ensure similarity model picks nearest neighbor images counter-example. negative explanations? picking counter-example follow classical hard negative mining strategy popular computer vision. speciﬁcally simply pick image lowest compare strong baseline. ensures ensure makes sense thus trying negative explanation woman doing? playing tennis hard negative mining strategy might pick image without woman would make confusing non-meaningful explanation show user goal convince model understood question. could component question relevance identify better counter-examples. instead take advantage balanced data collection mechanism directly train identifying good counter-example. note picked humans good counter-example deﬁnition. relevant different answer similar thus supervised training data counter-example question answer train model learns provide negative counter-example explanations supervised data. summarize test time model things ﬁrst answers question second explains answer counter-example. ﬁrst step given input image question outputs predicted answer apred. second step given input question answer explained model identify counterexample. training time model given image question corresponding ground-truth answer learn answer questions. also given learn explain. model architecture contains heads shared base ‘trunk’ head answering question head providing explanation. speciﬁcally model consists three major components shared base ﬁrst component model learning representations images questions. -channel network takes image embedding input branch question lstm embedding input another branch combines embeddings point-wise multiplication. gives joint embedding similar model second third components answering model explaining model take joint embedding input therefore considered heads ﬁrst shared component. total images original image candidate images passed shared component network. answering head second component learning answer questions. similar consists fullyconnected layer softmax predicts probin practice answer explained would answer predicted ﬁrst step apred. however access negative explanation annotations humans ground-truth answer question. providing explanation module also helps evaluating steps answering explaining separately. head model three negative explanations produced explanation head. explanations sensible reasonable images similar answers different predicted quantitative evaluation compare model number baselines random sorting candidate images randomly. random image picked likely counter-example. distance sorting candidate images increasing order distance original image image similar picked likely counterexample. model using model’s probability predicted answer sort candidate images ascending order image least likely answer picked likely counter-example. note image picked humans good counter-example necessarily unique counter-example. humans simply asked pick image makes sense answer natural criteria convey humans pick best clear best would mean ﬁrst place. provide robustness potential ambiguity counter-example chosen humans manner similar imagenet top- evaluation metric evaluate approach using recall metric. measures often human picked among top- sorted list model produces. table explanation model signiﬁcantly outperforms random baseline well model. interestingly strongest baseline distance. approach outperforms clear identifyability distribution answers given embedding. embedding corresponding original image passed component result crossentropy loss. explaining head third component learning explain answer counter-example image. -channel network linearly transforms joint embedding answer explained common embedding space. computes inner product embeddings resulting scalar number image inner-product values candidate images passed fully connected layer generate scores candidate images sorted according scores least likely good counter-examples negative explanations. component trained pairwise hinge ranking losses encourage i.e. score human picked image encouraged higher candidate images desired margin slack course classical ‘constraint form’ pairwise hinge ranking loss minimize standard expression combined loss function shared component where ﬁrst term cross-entropy loss second term pairwise hinge losses encourage explaining model give high score image trade-off weight parameter losses. note theory could provide apred input training instead matches expected case scenario test time. however alternate setup leads peculiar unnatural explanation training goal speciﬁcally explanation head still learning explain since answer collected negative explanation human annotations. simply unnatural build model answers question apred learn explain different answer note interesting scenario current push towards end-to-end training everything breaks down. image counter-example among nearest neighbors challenging task. again suggests visual understanding models extract meaningful details images still remain elusive. summarize paper address strong language priors task visual question answering elevate role image understanding required successful task. develop novel data-collection interface ‘balance’ popular dataset collecting ‘complementary’ images. every question dataset complementary images look similar different answers question. effort results dataset balanced original dataset construction also twice size. qualitatively quantitatively ‘tails’ answer distribution heavier balanced dataset reduces strong language priors exploited models. complete balanced dataset publicly available http//visualqa.org/ part iteration visual question answering dataset challenge benchmark number state-of-art models balanced dataset testing balanced dataset results signiﬁcant drop performance conﬁrming hypothesis models indeed exploited language biases. finally framework around complementary images enables develop novel explainable model asked question image model returns answer also produces list similar images considers ‘counter-examples’ i.e. answer predicted response. producing explanations enable user build better mental model system considers response mean ultimately build trust. acknowledgements. thank anitha kannan aishwarya agrawal helpful discussions. work funded part career awards award grant n--- sloan fellowship awards allen distinguished investigator award paul allen family foundation ictas junior faculty awards google faculty research awards amazon academic research awards education research grant nvidia donations views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied u.s. government sponsor.", "year": 2016}