{"title": "Mapping distributional to model-theoretic semantic spaces: a baseline", "tag": ["cs.CL", "cs.AI", "stat.ML"], "abstract": "Word embeddings have been shown to be useful across state-of-the-art systems in many natural language processing tasks, ranging from question answering systems to dependency parsing. (Herbelot and Vecchi, 2015) explored word embeddings and their utility for modeling language semantics. In particular, they presented an approach to automatically map a standard distributional semantic space onto a set-theoretic model using partial least squares regression. We show in this paper that a simple baseline achieves a +51% relative improvement compared to their model on one of the two datasets they used, and yields competitive results on the second dataset.", "text": "understanding information word embeddings contain subsequently high interest. investigated method word embeddings formal semantics center interest paper. speciﬁcally given feature word vector concept tried automatically often given concept given feature. example concept always vegetable concept coat time concept plug sometimes prongs concept never wings. concept features feature dataset annotated three different humans. annotation quantiﬁer reﬂects frequently concept feature. five quantiﬁers used word embeddings shown useful across state-of-the-art systems many natural ranging question answering systems dependency parsing. explored word embeddings utility modeling language semantics. particular presented approach automatically standard distributional semantic space onto set-theoretic model using partial least squares regression. show paper simple baseline achieves relative improvement compared model datasets used yields competitive results second dataset. word embeddings main components many state-of-the-art systems natural language processing language modeling text classiﬁcation question answering machine translation well named entity recognition quantiﬁers converted numerical format following mapping value averaged three annotators. using mapping concept model-theoretic vector feature annotated concept element model-theoretic vector corresponding feature value result element model-theoretic vector value correspond feature either annotated three annotators annotated given many features possible annotated concept model-theoretic vector quite sparse. additional coordinates corresponding remaining features would zero. concept word vector dimension dataset. coordinates mean concept another. example feature vegetable appears coordinate position vectors. themcrae norms contains concepts covering living nonliving entities well features. concept annotated features average human annotators. annotated pairs concept-feature dimension model-theoretic vectors therefore model-theoretic vector average elements unannotated features. previous section seen convert concept model-theoretic vector based human annotations. goal analyze whether exists transformation word embedding concept model-theoretic vector gold standard human annotations. word embeddings taken word embeddings pre-trained wordvec googlenews-vectors-negative trained part google news dataset consisting approximately billion words. transformation used based partial least squares regression plsr ﬁtted training inputs word embeddings concept outputs model-theoretic vectors concept. spearman rank-order correlation coefﬁcient computed predictions gold modeltheoretic vectors ignoring features concept annotated. idea features might present given options annotation. method therefore penalized suggesting them. figure illustrates model. figure overview system. word embedding concept transformed modeltheoric vector plsr. quality predicted model-theoric vector assessed spearman rank-order correlation coefﬁcient predictions gold model-theoretic vectors. note elements equal gold model-theoretic vector correspond features annotated concept. features omitted evaluating spearman rank-order correlation coefﬁcient. also dimension model-theoretic vectors could larger smaller dimension word embedding. since word embeddings dimensions model-theoretic vectors smaller word embeddings dataset larger dataset. mode predictor outputs feature common feature value training set. example feature annotated concepts predictor always output feature. ﬁnding common value feature ignore concepts feature annotated. resulting predictor take concept account making prediction. indeed predicted values always same regardless concept. feature value concepts predictor perform reasonably well. also apply retroﬁtting word embeddings order leverage relational information semantic lexicons encouraging linked words similar vector representations. using retroﬁtting tool retroﬁt word embeddings datasets present retroﬁtting tool last row. plsr stands partial least squares regression nearest neighbor ppdb paraphrase database ways compute mode either taking mode means annotations mode annotations potentially different annotations concept-feature pair annotation concept-feature pair result mode true-mode similar results potentially different results qmr. train/test split randomly chosen table table presents results using spearman correlation performance metric. experiment coded python using scikit-learn source well complete result datasets available online. could reproduce results dataset using plsr wordvec embeddings experiments could exactly reproduce results dataset experiments) discrepancy likely results choice training set. experiments’ results averaged runs training/test split randomly chosen constraint number training samples dataset worst achieved best achieved emphasizes lack robustness results respect train/test split. variability much lower figure stacked bars showing distribution quantiﬁers among features dataset features tend clearly dominant quantiﬁer. example feature almost always annotated quantiﬁer dataset expected since signiﬁcantly larger furthermore mode baseline yields results good dataset plsr wordvec implementation) signiﬁcantly better models dataset i.e. improvement). intuition mode baseline works well figures show features tend clearly dominant quantiﬁer dataset. similar trend found dataset. dataset features annotated concepts. dataset features annotated concepts average. result much difﬁcult plsr learn mapping word embeddings model-theoretic vectors dataset dataset. explains mode baseline outperforms plsr random vector baseline plsr performs mediocrely dataset poorly dataset. nearest neighbor baseline yields competitive results dataset lower results dataset. lastly using retroﬁtting increases performances datasets. expected applying retroﬁtting word embeddings leverages relational information semantic lexicons encouraging linked words similar vector representations. paper presented several baselines mapping distributional model-theoretic semantic spaces. mode baseline signiﬁcantly outperforms model dataset yields competitive results dataset. indicates state-of-the-art models efﬁciently word embeddings model-theoretic vectors datasets. figure heatmap showing distribution quantiﬁers among features dataset features tend clearly dominant quantiﬁer. values heatmap given following quantiﬁer-scalar mapping", "year": 2016}