{"title": "Clustering by Deep Nearest Neighbor Descent (D-NND): A Density-based  Parameter-Insensitive Clustering Method", "tag": ["stat.ML", "cs.CV", "cs.LG", "stat.CO", "stat.ME"], "abstract": "Most density-based clustering methods largely rely on how well the underlying density is estimated. However, density estimation itself is also a challenging problem, especially the determination of the kernel bandwidth. A large bandwidth could lead to the over-smoothed density estimation in which the number of density peaks could be less than the true clusters, while a small bandwidth could lead to the under-smoothed density estimation in which spurious density peaks, or called the \"ripple noise\", would be generated in the estimated density. In this paper, we propose a density-based hierarchical clustering method, called the Deep Nearest Neighbor Descent (D-NND), which could learn the underlying density structure layer by layer and capture the cluster structure at the same time. The over-smoothed density estimation could be largely avoided and the negative effect of the under-estimated cases could be also largely reduced. Overall, D-NND presents not only the strong capability of discovering the underlying cluster structure but also the remarkable reliability due to its insensitivity to parameters.", "text": "abstract density-based clustering methods largely rely well underlying density estimated. however density estimation also challenging problem especially determination kernel bandwidth. large bandwidth could lead over-smoothed density estimation number density peaks could less true clusters small bandwidth could lead under-smoothed density estimation spurious density peaks called ripple noise would generated estimated density. paper propose density-based hierarchical clustering method called deep nearest neighbor descent could learn underlying density structure layer layer capture cluster structure time. over-smoothed density estimation could largely avoided negative effect under-estimated cases could also largely reduced. overall d-nnd presents strong capability discovering underlying cluster structure also remarkable reliability insensitivity parameters. introduction cluster analysis popular powerful data analysis tool diverse fields science engineering business based pair-wise similarities used divide groups massive dataset e.g. gene expression profiles images texts internet usersâ€™ consuming data. clustering data could effectively organized underlying group relationships among data points could discovered. data could thus become useful informative among various ways clustering idea density-based clustering never loses appeal like k-means popular clustering method density-based clustering algorithms simple effective relatively time complexity thus eligible processing large datasets besides unlike k-means density-based clustering algorithms able detect arbitrarily shaped clusters robust noise outliers insensitive initialization need specify cluster number advance. besides density-based clustering quite intuitive since instance input data points usually associated mountain-like density function multiple density peaks fig. shows. density function reflects distributwo questions however easy simultaneously solved appear. following introduce several density-based clustering methods directly indirectly consciously unconsciously involved solving questions. note methods introduced chronological order. denclue representative intuitive density-based clustering method. question underlying density function simply approximated certain kernel functions centered point tion whereas making hard question opposite model-based clustering methods typically t-distribution mixture model instance assumes data points sampled probabilistic density function combination multivariate gaussian functions. solving parameters clustering assignment easily solved bayesian criterion. nevertheless well-known hard solve analytically iterative solution usually used alternative solve parameters whereas still involves open problems i.e. sensitive initialization hard specify component advance. denclue obviously clustering assignment cannot simply solved bayesian criterion like gmm. instead denclue uses classical gradient ascent also known steepest ascent hill-climbing method identify density peaks. specifically point shifts direction gradient ascent following iteration shifting step current next locations respectively. iteration designed stop taken so-called density-attractor density-attractor together attracted points assigned cluster. besides threshold also suggested judge whether attractor significant useful distinguishing outliers since usually densities. overall denclue provides simple intuitive effective solve question using whereas opinion also brings least following nontrivial problems fixed step inherent problem small step length could make iteration slow converge large could make iteration oscillate around certain density peak. limited applications. applied numerical vector data many datasets practice contain categorical vectors directly pair-wise dissimilarity rather vectors. sensitive kernel bandwidth compared fig. small could make estimated density function under-smoothed spurious density peaks would lead over-partitioning clustering results. instance points fig. would become different density-attractors cluster centers cluster belong fact fig. exist least three clusters risk over-partitioned. contrast large would make estimated density over-smoothed lead under-partitioning clustering result. nevertheless denclue also good properties instance insensitive noise. hard understand since noise hardly make significant effect overall density function. besides denclue close relationship dbscan another popular density-based clustering method k-means popular partition-based clustering method. dbscan turns special case denclue denclue uses uniform spherical kernel globally optimal clustering results k-means clustering results denclue provided cluster numbers identical. iteration expression next location simply computed weighted mean around current location fixed step parameter avoided here. fact step length denoted adaptive local distribution. denser area smaller would method so-called mean-shift first proposed fukunaga hostetler developed brought notice cheng become popular since comaniciu meer mean-shift proved defined follows equivalent density function normal kernel expression. mean-shift guaranteed converge using certain kernels however mean-shift still iterative nature searching modes thus still theoretically time-consuming. reason many efforts made accelerate besides also easy find that like denclue mean-shift suitable numerical data sensitive kernel parameter despite fact density function needed computed. graph-based gradient ascent besides mean-shift fact another variant unlike mean-shift method proposed implemented based graph theory instead numerical analysis thus call method graph-based gradient ascent graph-ga proved also equivalent terms kerf graph-ga approximates gradient point referring function neighbors follows denotes density difference distance points neighbors point within radius however value estimated gradient used. instead based expression gradient researchers care neighbor corresponds maximum output. particular neighbor denoted also called parent node point thus defined treat point node graph link node parent node directed edge would result directed usually unconnected graph several separated sub-graphs. sub-graph directed tree containing root node. root nodes approximately correspond density peaks. based graph next process identifying attractor node could simply follow directions edges graph. words next computation involves serial identification parent node parent node node etc. computation time almost negligible compared mean-shift step approaching mode involves computation first iteration. reason graph-ga efficient mean-shift. besides graph-ga constrained attributes data directly take dissimilarity input. however like mean-shift graph-ga also sensitive kernel-like hard whether guaranteed in-tree. left question. rameter hard understand since graph-ga graph-based implementation thus essence same. besides graph-ga involves nontrivial process avoiding cycle graph somewhat sensitive order data points processed which perspective largely utilized avoid cycle. denclue uses original form brings four non-trivial problems mean-shift changes iteration consequently avoids fixed step problem whereas still suffering p~p; graph-ga solves first three problems last still remains. following introduce progresses issue. first introduce novel method proposed year call decision graph extremely simple effective method successfully solving problems also largely reducing negative effect ripple noise fourth problem. besides whole also introduce novel method previously proposed similar called nearest descent together progresses made nearest neighbor descent hierarchical nearest neighbor descent h-nnd comparable advantage like graph-ga solve first three problems serve methods ones pivotal element method proposed paper. rodriguez laio proposed simple effective method fast determine density peaks scatter plot called decision graph density peaks interactively determined users. decision graph plotted based effective features variables discovered. local density special distance defined minimum distance point points dataset larger density otherwise found density peaks large values features thus would feature space coordinated variables could interactively identified since density point fig. case fig. corresponding clustering result fig. clearly reveals largely reduce problem brought ripples. words valid density peaks selectively shown decision graph. instance points fig. point decision graph taken cluster center. reason that despite high density points distance variable point small distance variable point much larger note fig. select salient points obvious another point outside also saliently popping out. also enclose point would make cluster over-partitioned. nevertheless negative effect ripple noise would still largely reduced since number clusters could over-partitioned could reduced overall success could provide fast non-iterative interactive identify density peaks since graph-ga following method could well. also could resist disturbance invalid density peaks ripples also discussed rodriguez laio paper. latter property contributes fact sensitivity kernel bandwidth largely reduced. thing worth noting interactive operation could general enhance reliability whole process since believed participation users visualized environment could contribute reliable exploration unknown dataâ€™s world. however bandwidth large behind interaction operation would still exist risk generating misleading results besides risk would also exist clearly separated cluster structure dataset since case would hard obtain explicit decision graph saliently popping-out points. also opened physically inspired method while instead using concept gravitational force particles explain movement would make evolution particle system extremely complex method inspired another idea reckons particles curve space turn curved space particles tend move specifically curved space approximately viewed potential profile fig. points directly vertical axis fig. called potential actually terms computation potential negative density although insensitive parameter range values lead good performance relatively narrow. fact complexity degree revealed affinity propagation clustering although based gravitational force http//einstein.stanford.edu/spacetime/spacetime. fig. illustration estimated potentials dataset. bluer areas appear lower potentials are. equivalent representation space potential represented additional axis. data structure constructed colors nodes denote potentials. edges cyan denote redundant edges need removed further. decision graph used determine redundant edges. identified pop-out points correspond start nodes redundant edges. clustering result removing redundant edges. also original physical space approximately represent curved space fig. shows different curvature space approximately revealed uneven potentials space consequently uneven curvature space uneven potential distribution space almost feel evolution tendency particle system particle tends descend descending direction potential. tendency simplified algorithm named nearest descent descent refers particle descends descending direction potential. specially means point descends nearest descending direction potential. accordingly also called parent node point defined defined previous equation otherwise node globally lowest potential dataset. like graph-ga global perspective time independent hopping point parent node lead graph structure fig. shows. however unlike graph-ga guaranteed make data points organized fully connected graph special name called in-tree graph theory also called in-arborescence in-branching directed graph meets node outdegree node outdegree cycle; fully connected. proven structure guaranteed generated actually intermediate result fig. shows something surprising exciting expected beginning. instance structure significant features. clusters already captured except small number redundant edges. reason clustering problem reduced classical edge-removing problem alike minimal-spanning-tree based clustering easy find redundant edges distinguishable edges thus would easy removed however quite unlike mst-based clustering. among close clusters clusters contaminated noise would exist short-linked redundant edges hard determined. contrast determine redundant edges instance simply ranking edge lengths decreasing order choose longest ones. clustering result fig. shows could also largely reduce negative effect ripple noise. fact fig. shows also serve effective method determine redundant edges because according requirement node directed edge started i.e. edge thus length edge together magnitude potential node variables node obtain similar decision graph like fig. fact technical point view rodriguez laioâ€™s happen essence same despite different backgrounds implementations fact view rodriguez laioâ€™s counterpart i.e. nearest ascent ascent refers ascending direction density. words means point ascends nearest point ascending direction density. later proposed method called nearest neighbor descent unlike requires point descends nearest neighbor descending direction potential. words parent node node constrained neighborhood relationship follows denotes local minimum called candidate parent points nodes node neighborhood constraint parent node prevent redundant edges occurring. like graph-ga graph generated usually fully connected. fig. shows generates seven sub-graphs representing cluster perfectly consistent underlying clustering structure. besides sub-graph root node fig. graphs constructed different values parameter left well partitioned result; right over-partitioned result. points inside circles root nodes separate sub-graph although compared complete task withadditional requirement removing redundant edges doesnâ€™t become powerful always perfect looks fig. because easy find quite alike denclue mean-shift graph-ga thus also severely affected ripple noise revealed over-partitioning result fig. nonetheless perspective found another value subsequently proposed another method called hierarchical nearest neighbor descent descending process divided stages. first stage executed non-extreme nodes stage executed extreme nodes expressions written like h-nnd also generates structure redundant edges could become salient thus could make repair process become much easier reliable. cases fig. removing longest edges well clustered results would obtained without over-partitioning problem. motivation idea paper despite h-nnd could largely reduce risk under-smoothed density potential ultimate clustering results risk over-smoothed case clustering result still exists. words methods could still affected kernel bandwidth despite safe range bandwidth become relatively larger. theoretically adjust bandwidth judging whether current bandwidth leads better clustering result nontrivial. besides cases e.g. multiple scales different clusters unbalanced element numbers different clusters fixed kernel bandwidth cannot well reveal underlying density. previously proposed visualization method called it-map help reliable bandwidth different bandwidths different clusters using divide-and-conquer strategy supervision users. besides expect kernel density estimation method could make bandwidth adaptive local distributions data points. nevertheless possible bandwidth problem could solved density-based clustering method rather help it-map adaptive kernel density estimation methods? order fulfill goal first answer question cause bandwidth problem? perspective answer lies fact processes involved i.e. density estimation clustering separate performed serial density-based clustering methods h-nnd consequently clustering process largely relies performance density estimation. therefore paper propose density-based hierarchical clustering method called deep nearest neighbor descent processes interplay. fulfilled making full hierarchical strategy merit nnd. specifically layer hierarchy density certain nodes updated based discovered cluster structure turn updated density estimation used renew cluster structure. density layer estimated based local information whereas layer number increases magnitudes estimated potentials sample nodes higher layers could gradually grasp global density distribution dataset. proposed method could adaptive multi-resolutions different clusters. d-nnd expected largely reduce negative effect made under-smoothed potential estimation property inherited h-nnd also largely reduce risk over-smoothed case. effect proposed method could appear insensitive parameters considerably large ranges values. d-nnd contains stages bottom-up top-down stages. bottom-up stage summarized table d-nnd takes input distance pair beginning points zero potential data points node layer contains steps. first neighbor nodes null dataset steps parent node node identified. root nodes dataset serve input next layer. last layer occurs time root node conclusion given input nodes layer functions update potentials nodes identify root nodes identify parent nodes remaining nodes illustrated fig. connect node parent node view points root nodes effect data points nested layer layer number root nodes reduced layer layer data points organized fully connected data structure root node left data structure correspond nodes data points original input node parent node respectively defines start nodes directed edge distance defines edge length note that directed edge directed edge denoted started node words start node edge serve unique identifier edge. basis decision-graph-cut top-down stage. pleasingly like structures constructed h-nnd generated also shares good property redundant edges distinguishable edges thus hard determined fig. shows redundant edges easily determined different methods e-cut decision-graph-cut. e-cut plot lengths edges decreasing order decision-graph-cut features directed edge associated variables edge length potential identifier right image fig. shows pop-out points could rightly correspond redundant edges fig. shows methods lead almost perfect clustering result. fact besides e-cut decision-graph-cut previously devised effective methods remove redundant edges e.g. it-map it-dendrogram g-ap ss-cut simplicity still e-cut decision-graph-cut following experiments demonstrate power proposed method d-nnd. saliency redundant edges diversity edge removing methods could guarantee good performances clustering results. fig. illustration d-nnd dataset data points. left right layer increases number root nodes successively reduced time whole data points gradually organized fully connected graph i.e. clustering purpose redundant edges require removed however obviously distinguishable edges thus easily determined. different methods determine redundant edges left plot lengths edges. longest edges need removed easily determined according plot setting threshold points large counting number points saliently large values right interactively determined pop-out points correspond start nodes redundant edges. methods lead clustering result almost perfect clustering results demonstrate effectiveness whole process. illustration step kernel-like parameter exponential function used compute dissimilarity step show experiments that proposed method sensitive extremely large range. fact parameter could function like fine-tune knot however good thing technical point view. experiments first tested three datasets respectively using large range values dataset varied dataset also varied dataset varied note that first dataset data dimension normalized like fig. cases points popping decision graphs corresponding clustering results consistent visual perception fig. shows. fact seen clustering performances also achieved using e-cut since edge length variable alone enough distinguish pop-out points order demonstrate this experiments first dataset varies varies case longest edges structures removed results clusters. comparing clustering assignments results benchmark data average error rate clustering assignments almost negligible words clustering results almost perfect vary large ranges values. then tested high-dimensional datasets containing data points whereas dimension varies dataset chose largely different values together extremely different values used e-cut cases. plots edge lengths shown fig.. plot salient determined number largest edge lengths rest ones setting appropriate threshold gap. almost corresponding clustering results perfect. clustering error rate cases cluster numbers consistent underlying number groups except slightly larger cluster numbers either however found extra cluster contains point actually regarded outlier. also tested united state poster service digit number dataset contains grayscale handwrite digits. digit image treated -dimensional vector test. here used decision-graph-cut expecting reach small error assignment small cluster number. also selected five significantly different values extremely different values testing results cases shown fig. conclusions section first summarized general questions density-based clustering accordingly introduced seven density-based clustering methods i.e. denclue mean-shift graph-ga rodriguez laioâ€™ methods proposed time introduced four problems denclue methods partly solve problems; relationship viewed nearest ascent counterpart relationship newly proposed novel methods previous methods could serve them. section explained motivation main idea paper section introduced proposed method d-nnd details. section showed experiments d-nnd strong capability discovering underlying cluster structure also remarkable reliability insensitivity parameters large ranges values. although avoid first three problems proposed hierarchical learning framework last problem also largely solved proposed method d-nnd. endows d-nnd advantages non-iterative unconstrained attributes data insensitive kernel bandwidth words efficiency general meaning reliability. besides d-nnd also shares seven general merits density-based clustering algorithms. like affinity propagation bottom-up stage proposed d-nnd also interpreted process selecting exemplars prospective fig. analyzed like this beginning points viewed exemplars themselves. comparing among exemplars several continue exemplars exemplars next layer. process proceeds higher layers exemplar becomes exemplar remaining exemplars certain layer. thus whole bottom-up stage roughly viewed hierarchical exemplar election system. since layer local information considered could make selection layer local optimum. layer increases exemplars could actually represent large ranges areas points global perspective. words selection gradually become global optimum higher layers. center-biased trend exemplars guarantees saliency generated redundant edges since redundant edge approximately start center cluster center another cluster. stark contrast case redundant edges usually occur marginal areas clusters thus usually hard determined. note that fig. shows case undesired edges generated simultaneously last layer also case redundant edges generated different layers fig. shows. hand since density estimation layer based neighborhood relationship kernel constrained local sphere thus matter large kernel-like parameter makes litter effect global distribution thus over-smoothing density phenomenon largely avoided provided neighborhood relationship degree guaranteed hand local constraint density estimation would make under-smoothed density problem even worse. denclue mean-shift graph-ga would mean severer over-partitioning result. however revealed experiments under-smoothed case troublesome d-nnd either hierarchical procedure strategy redundancy design overall prevention over-smoothed density estimation ability tackle problem brought under-smoothed case make proposed method insensitive parameters large ranges. particularity generality thing hierarchical strategy largely solves over-partitioning problem nnd. another role fully played revealed significant element layer. reasons analyzed follows. suppose layer gradient descent rather first problems also exist here. layer number increases overall problems nontrivial hierarchical system. contrast given estimated potentials nodes brings additional problem matter many layers hierarchy contains. makes possible could alternative task? perspective provides general solution problem locating maximum points density function i.e. traditional optimization problem. contrast grasps particularity. particularity mainly refers fact value independent variable assumed underlying density function particular problem finite discrete known reason graph could constructed first independent hopping point parent node. fact particularity also explain previously could also exist alternatives i.e. mean-shift graph-ga. fact steepest pursuit behind necessary terms computation time task. reason although lets point choose steepest path reach density peak step approaching density peak involves equivalent computation. comparison first step involves certain degree computation computation time remaining steps almost negligible pre-specified paths constructed graph. even compared graph-ga graph-based approximation steepest pursuit also makes considerable advantage computation time. admittedly path root node node graph constructed graph-ga generally nearer steepest path thus contains less directed edges compared nnd. illustrated fig. point path left image obviously contains less edges right image overall number total edges generated graph-ga less nnd. nevertheless brings graph-ga negligible advantage terms computation time graph-based searching compared similar case since computation time kind graph-based search could already negligible general revealed table where instance computation time dataset data points costs note although graph constructed searching root nodes process here. beside steepest pursuit also efficient terms parameter task. reason revealed comparing graph-ga. compared graph-ga less sensitive parameter constraint nearest requirement. partly shown h-nnd insensitivity neighborhood parameter large range also fully revealed stark comparison graph-ga fig. different values neighborhood parameter note that k-nn graph define neighborhood relationship. besides literature graph-ga authors first estimate density based neighborhood relationship. fig. order compare graph-ga assume graph-ga underlying densities already well estimated fig. shows. that compared corresponding result fig. increased fig. significant change whereas problem starts occur graph-ga. four seven undesired edges falsely generated whereas corresponding results graph-ga severer. besides removing largest four seven edges good clustering results obtained whereas graph-ga redundant edges successfully removed would lead severe over-partitioning clustering results. besides note becomes obviously circumstance. perspective although hierarchical strategy makes h-nnd generate distinguishable redundant edges d-nnd robust parameters compared contributing factor d-hhn together h-nnd fault-tolerant design. reasons analyzed follows. unlike denclue mean-shift graph-ga particular nodes i.e. modes density peaks h-nnd d-nnd privilege consequently redundant edges valid modes would produced. however tolerance kind mistakes also endows resistance another kind mistakes thus risk generating over-partitioned clustering results largely reduced them. also guideline fault-tolerant design proposed d-nnd reduce another risk i.e. risk generating under-partitioned clustering results using another novel implementation constructing besides fault-tolerant design here it-based clustering family enriched variety methods similarly opinion fault-tolerant design could also used explain enrichment traditional link-based hierarchical clustering methods different methods proposed construct dendrogram. despite similarity terms fault-tolerant design however compared l-hc advantages it-based clustering twofold fault-tolerant design thing vital mistakes designed generated since distinguishable mistakes easier reliable following repair methods could h-nnd proposed method d-nnd biggest advantage since redundant edges could general distinguishable sharp contrast traditional l-hc besides saliency redundant edges diversity repair methods another advantage it-based clustering methods compared l-hc since effective repair methods used help remove error links particular graph structure also sharp contrast choice l-hc. seems fault-tolerant design could quite efficient reach certain degree robustness lowest cost using simpler system. think globally learning locally hierarchy. like idea famous dimensionality reduction method locally linear embedding also local information estimate density despite global feature. nevertheless special problem also combine local estimation hierarchical strategy that layer increases magnitudes estimated potentials points higher layer gradually reveal global density features. also since layer potential updated based local neighborhood relationship instead single-scaled kernel bandwidth density estimation could effect adaptive multiple scales clusters. fact opinion successes methods segmentation weighted aggregation tree preserving embedding solving similar multi-resolution challenges unsupervised tasks also largely benefited similar idea reflected d-nnd thinking globally learning locally hierarchy. jain data clustering years beyond k-means. pattern recognit. lett. wunsch survey clustering algorithms. ieee trans. neural netw. theodoridis koutroumbas pattern recognition fourth edition koontz narendra fukunaga graph-theoretic approach nonparametric cluster analysis. ieee trans. comput. ester kriegel sander density-based algorithm discovering clusters large spatial databases noise. hinneburg keim efficient approach clustering large multimedia databases noise. rodriguez laio clustering fast search find density peaks. science yang physically inspired clustering algorithm evolve like particles. arxiv preprint arxiv.. clustering descending nearest neighbor delaunay graph space. arxiv preprint arxiv.. h-nnd member in-tree clustering family. arxiv preprint arxiv.. macqueen methods classification analysis multivariate observations. proceedings fifth berkeley symposium mathematical statistics probability neyman mclachlan peel robust cluster analysis mixtures multivariate t-distributions. advances pattern recognition peel mclachlan robust mixture modelling using distribution. statistics computing dempster laird rubin maximum likelihood incomplete data algorithm. journal royal statistical society. series fukunaga hostetler estimation gradient density function applications pattern recognition. ieee trans. inf. theory cheng mean shift mode seeking clustering. pattern analysis machine intelligence ieee transactions comaniciu meer mean shift robust approach toward feature space analysis. pattern analysis machine intelligence ieee transactions elgammal duraiswami davis efficient kernel density estimation using fast gauss transform applications color modeling tracking. pattern analysis machine intelligence ieee transactions carreira-perpiÅ„Ã¡n acceleration strategies gaussian mean-shift image segmentation. computer vision pattern recognition ieee computer society conference georgescu shimshoni meer mean shift based clustering high dimensions texture classification example. computer vision proceedings. ninth ieee international conference yang duraiswami dementhon davis mean-shift analysis icip using quasinewton methods. proceedings. international conference ii-- vol. paris durand topological approach hierarchical segmentation using mean shift. computer vision pattern recognition cvpr'. ieee conference shneiderman picture data visualization. science zahn graph-theoretical methods detecting describing gestalt clusters. ieee trans. comput. it-map effective nonlinear dimensionality preprint reduction method arxiv.. it-dendrogram member in-tree clustering family. arxiv preprint arxiv.. generalized affinity propagation clustering algorithm nonspherical cluster discovery. arxiv preprint arxiv.. frÃ¤nti virmajoki iterative shrinking method clustering problems. pattern recognit. knowl. discovery data franti virmajoki hautamaki fast agglomerative clustering using k-nearest neighbor graph. pattern analysis machine intelligence ieee transactions frey dueck clustering passing messages data points. science sharon galun sharon basri brandt hierarchy adaptivity segmenting visual scenes. nature shieh hashimoto airoldi tree preserving embedding. proc. natl. acad. sci. u.s.a.", "year": 2015}