{"title": "Matching Pursuit LASSO Part II: Applications and Sparse Recovery over  Batch Signals", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Matching Pursuit LASSIn Part I \\cite{TanPMLPart1}, a Matching Pursuit LASSO ({MPL}) algorithm has been presented for solving large-scale sparse recovery (SR) problems. In this paper, we present a subspace search to further improve the performance of MPL, and then continue to address another major challenge of SR -- batch SR with many signals, a consideration which is absent from most of previous $\\ell_1$-norm methods. As a result, a batch-mode {MPL} is developed to vastly speed up sparse recovery of many signals simultaneously. Comprehensive numerical experiments on compressive sensing and face recognition tasks demonstrate the superior performance of MPL and BMPL over other methods considered in this paper, in terms of sparse recovery ability and efficiency. In particular, BMPL is up to 400 times faster than existing $\\ell_1$-norm methods considered to be state-of-the-art.O Part II: Applications and Sparse Recovery over Batch Signals", "text": "abstract—in part matching pursuit lasso algorithm presented solving large-scale sparse recovery problems. paper present subspace search improve performance continue address another major challenge batch many signals consideration absent previous ℓ-norm methods. result batch-mode developed vastly speed sparse recovery many signals simultaneously. comprehensive numerical experiments compressive sensing face recognition tasks demonstrate superior performance bmpl methods considered paper terms sparse recovery ability efﬁciency. particular bmpl times faster existing ℓ-norm methods considered state-of-the-art. least-angle regression gradient projection sparse reconstruction projected gradient fast iterative shrinkage-threshold algorithm coordinate descent methods proximal gradient homotopy method interested readers refer part references therein comprehensive review. existing ℓ-norm methods however suffer high computational complexity large-scale problems. critically problems like batch many signals need sparsely recovered simultaneously computations even expensive. here batch problem carried solve following optimization problem rn×p records measurements signals k·kf denotes -norm matrix. batch problem plays important role many applications face recognition compressive sensing dictionary learning face recognition achieved promising performance recently basic assumption that testing image lies subspace spanned training images person thus sparsely represented training images. here training images formed dictionary rn×m denotes number pixels features face image denotes number training images. core task based face recognition sparse representation testing image however directly solving problem computationally expensive especially large researchers propose reduce computational cost dimension reduction methods random projections however recognition rates affected dimension reduction practice often required recognize many face images simultaneously real-time challenging based methods address this authors suggest directly solving minx denoted authors argue solving ||x|| least square problem minx denoted achieve stable performance. method optimal solution r+q⊤b fast development compressive sensing theory sparse recovery gained increased attention recently signal processing community also become fundamental element many research areas image processing computer vision data mining machine learning formally seeks recover unknown k-sparse signal nonadaptive linear measurement rn×m denotes dictionary represents noise column vector referred atom. recover need solve ℓ-norm minimization problem denotes decomposition denotes pseudo inverse. method optimal solution −a⊤b. therefore fast predictions achieved simple matrix-vector products pre-computing r−q⊤ offline. however since solutions methods sparse recognition performance degraded. rest paper organized follows. section brieﬂy review algorithm propose improved algorithm subspace exploratory matching. section describe batch mode method. numerical experiments real-world applications presented sections respectively. conclusive remarks given section throughout paper denote transpose vector/matrix superscript zero vector diag diagonal matrix diagonal entries equal addition kvkp denote ℓp-norm ℓ-norm vector respectively. function gradient subgradient respectively. sparse vector calligraphic letter support {i|xi support subvector indexed complementary i.e. m}\\t furthermore represent element-wise product matrices lastly denote columns indexed introduce bring support detection vector impose ℓ-norm constraint namely enforce sparsity. here predeﬁned integer satisfying domain propose solve integer programming model lasso details presented algorithm basically iteratively adds active atoms worst-case analysis step conducts master problem optimization steps a⊤αt− index detected atoms iteration worst-case analysis update sparse recovery core element recently developed compressive sensing theory signal acquisition compressive sensing signal allowed captured rate signiﬁcantly lower nyquist rate compressible sparsely decomposed basis rm×m recover original signal need solve sparse recovery problem might expensive. moreover real-world sensing tasks imaging video sensing often necessary sense large number signals simultaneously real-time. therefore critical efﬁciently address large-scale batch problem compressive sensing. dictionary learning aims good dictionary based training signals recently become increasingly important many areas signal processing computer vision machine learning learn good dictionary many training examples usually required sparsely represented time leading intolerable cost dictionary learning. large-scale batch problem therefore core step dictionary learning part paper presented matching pursuit lasso algorithm relation computational issues lasso dictionaries. paper ﬁrst present subspace search improve performance continue address computational bottleneck created batch problem. main contributions paper summarized follows subspace exploratory matching proposed improve performance mpl. matching pursuit scheme takes less seconds recover -sparse signal dictionary million atoms. apply bmpl face recognition tasks wellknown face databases namely extended yaleb databases. comprehensive experiments show bmpl achieves comparable better recognition rates baselines comparable time complexity. importantly bmpl times faster existing ℓ-norm methods considered state-of-the-art. algorithm reduced orthogonal matching pursuit also related stagewise stagewise weak gradient pursuits sense atoms iteration. however swcgp stomp number atoms added iteration changes complex thresholding strategies example stomp knowledge noise required determine number atoms. knowledge however available general problems address this swcgp adopts simpler thresholding strategy independent noise however swcgp iteration conducted master problem optimization. result master problem sufﬁciently optimized many non-support atoms might included accordingly leading degraded performance. contrast takes iterations master problem optimization following stopping condition achieved convergence studied part however performance might affected value explain this ﬁrst present bound regarding progress objective value outer loop. according lemma choosing atoms largest |gi| guarantee best improvement objective values iteration inner loop. however atoms cannot guarantee best objective value improvement inner iterations used. words worst-case analysis step might suboptimal relatively large particular non-support atoms large values |gi| might mistakenly added address this propose ﬁrst include atoms largest |gi| solve master problem selected atoms. finally choose atoms decrease objective value most-active atoms. scheme referred subspace exploratory matching summarized algorithm improve efﬁciency adopt warmstart strategy equation stopping condition master problem optimization. convenience hereafter refer algorithm subspace search smpl. general since atoms chosen smpl achieve better improvement objective value convergence speed sparse recovery performance boosted observed fig. section iv-b. proposed subspace search related atom selection strategies used cosamp ompr example true supports cosamp choose additional atoms respectively active atom set. that pruning step performed atoms kept active atom set. contrast atom replacement deletion w.r.t. outer iterations. consequently smpl guaranteed monotonically decrease objective values lastly subspace search cosamp ompr relies estimation required smpl. however practice choose small order reduce solution bias lasso directly. small stops ||α|| ||e|| possible over-ﬁtting problem happen. prevent over-ﬁtting problem stop early following stopping conditions achieved several implementation techniques adopted improve efﬁciency mpl. note master problem optimization w.r.t. small atoms only. index selected atoms. need calculate small scale matrix-vector products convenience refer partial matrix-vector product correspondingly refer full matrix-vector product firstly since computing pmvp much cheaper fmvp fully exploit advantage store atom atom main memory easily retrieve atoms indexed using pointers. secondly dealing dictionaries cache-tomemory efﬁciency important. example calculations pmvps cache-tomemory efﬁcient since active atoms general away main memory. address this explicitly store main memory. accordingly compute pmvps efﬁciently. thirdly several iterations regarding master problem optimization sufﬁcient signiﬁcantly reduce number pmvps. moreover updating purpose warm-start signiﬁcantly improve efﬁciency master problem optimization. fair comparison employ techniques implement ℓnorm methods whenever intermediate variables sparse denote supports intermediate replace improve efﬁciency considerably. batch problem suppose signals sparsely represented time. existing ℓ-norm methods fista take cost iteration. suppose stop iterations total cost recovering signals contrary suppose stops iterations reduce cost nevertheless complexity smpl still dependent making expensive tackle large-scale problems large essentially computational burden brought calculation cost) worst-case analysis. therefore reduce cost critical improving efﬁciency. according studies discrete fourier transform basis wavelet basis sampled form dictionary computational complexity reduced help fast fourier transform however technique cannot applied general dictionaries. tackle many signals general dictionaries propose batch-mode computational cost greatly reduced. actually a⊤a. pre-compute store main memory calculate according remark apply need compute matrix rm×m cost efﬁcient regarding single signal. however since calculated off-line cost negligible dealing many signals. existing ℓ-norm methods even though intermediate variables sparse easy conduct batch mode optimization since support intermediate variables might change frequently optimization. result frequent retrievals computationally expensive. batch scheme applicable dictionary large number atoms space complexity store nevertheless bmpl applied many large-scale tasks. example efﬁciently deal dictionaries atoms memory machine sufﬁcient many real-world applications face recognition dictionary learning denotes recovered signal. here sparse signal successfully recovered rmse ≤e−. complete comparison record empirical probability successful reconstruction independent experiments compare fista active-set methods recovering -sparse bernoulli sparse signal -sparse gaussian sparse signal gaussian dictionary study effect given basic study study namely .||a⊤b||∞ .||a⊤b||∞. fig. report objective values comparison methods w.r.t. iterations. table table record following metrics number full matrix-vector products number partial matrix-vector products number nonzeros solutions; decoding time signal; speedup fastest method others. based results draw following conclusions. fig. different converge much faster baseline methods. particular smpl times faster others gaussian sparse signal. fista converges well .||a⊤b||∞. particular objective value decreases quickly converges slowly .||a⊤b||∞. fact generally speaking convergence rate fista sub-linear e.g. contrast fista solves sequence subproblems attain linear convergence rate subproblem strongly convex overall performs much better fista. tables general also need much fewer number pmvps others. moreover scale pmvps much smaller fista. example .||a⊤b||∞ sparsity solution much larger mpl. words master problem optimization expensive. four state-of-the-art ℓ-solvers shotgun uses fista parallel coordinate descent uses accelerated proximal gradient method continuation technique uses homotopy method improve convergence speed adopts screening test predict zero entries improve decoding efﬁciency several related greedy methods romp stomp swcgp used comparison. addition four well-known greedy algorithms i.e. orthogonal matching pursuit accelerated iterative hard thresholding subspace pursuit orthogonal matching pursuit replacement also included baseline methods. experiments shotgun conducted parallel intel core -bit linux methods conducted -bit windows operating system computer conﬁguration. fair comparison methods except romp stomp written running single core. written matlab parallel eight-core machine. following .||a⊤b||∞ ℓnorm methods. unless noted otherwise apply de-biasing technique reduce solution bias ℓ-norm methods apply early stopping avoid over-ﬁtting problem stopping condition following study compressive sensing problems gaussian design matrices. study types sparse signals e.g. bernoulli sparse vector gaussian sparse signal observation https//www.select.cs.cmu.edu/projects. https//www.eecs.berkeley.edu/∼yang/software/lbenchmark/index.html. http//www.princeton.edu/∼zxiang/home/index.html. https//www-personal.umich.edu/∼romanv/software/romp.m. https//sparselab.stanford.edu/. https//www.personal.soton.ac.uk/tbm/publications.html. https//sites.google.com/site/igorcarron/cscodes. fig. convergence comparison methods bernoulli sparse vectors gaussian sparse vectors active-set method record iteration. record results within iterations methods. experiment conduct sensitivity study smpl. .ka⊤bk∞ vary note smpl reduced conduct independent experiments record epsr values averaged decoding time fig. fig. respectively. fig. smpl larger tends better recovery performance terms epsr. however improvement becomes less signiﬁcant. reason that large enough atoms largest |gi| already include potential active atoms thus increasing signiﬁcantly improve performance. fig. shows worst decoding efﬁciency. reason that without subspace search non-support atoms might mistakenly included needs iterations converge. compare romp stomp swcgp gaussian sparse signals default parameter settings stomp swcgp. conduct independent experiments record epsr value averaged decoding time fig. fig. respectively. also record sparsity solutions table iii. fig. fig. outperforms baselines terms sparse recovery performance decoding efﬁciency. stomp cannot successfully recover sparse signals table stomp swcgp include atoms indicates many non-support atoms included. problem becomes severe swcgp since master problem sufﬁciently optimized. result cannot recover k-sparse signals shown fig. lastly romp shows much worse sparse recovery performance methods consistent conclusions experiment compare performance baseline methods median-scale problem shotgun work parallel. independent trials. apply early stopping avoid over-ﬁtting problem. ompr necessary calculate x+ηa⊤ learning rate ompr setting crucial performance feasible range provided satisﬁes condition. unfortunately well scaled scale vary setting difﬁcult. address issue propose variant ompr adaptively adjusted applying rule. distinguish variant ompr refer ompra. epsr value recovery time gaussian sparse signals method presented fig. ﬁgure smpl show much better recovery performance methods gaussian sparse signals terms epsr. general smpl shows better recovery performance terms epsr. ompr shows worse recovery performance greedy methods. experiments ompra uses adaptive learning rate improves ompr greatly. however ompra still worse mpl. .ka⊤bk∞ lasso related algorithms maximum iterations fista report rmse recovery time fig. fig. respectively. according reported results following conclusions drawn. fig. shows better rmse methods smpl signiﬁcantly improves terms rmse addition aiht cannot recover k-sparse signal lastly fista show worse recovery performance methods terms rmse coincides results tables interested readers details real-world applications face recognition task million training images many persons based face recognition training images formed dictionary. reasons inefﬁciency fista. firstly require many iterations converge means need compute many times mpl. secondly computing large dictionaries data exchange main memory cache memory inefﬁcient. contrast master problem optimization w.r.t. small active atoms only e.g. apparently data exchange main memory cache memory w.r.t. much efﬁcient. bomp batch mode implementation simulation generate gaussian random matrix generate gaussian sparse signals sparsity vector measurements produced ax+ξ gaussian noise sampled total time spent bmpl bomp decoding signals averaged root-mean-square error reported table table bmpl times faster bomp. moreover bmpl gains better comparable armse bomp apply bmpl many-face recognition tasks solving problem adopt bomp baseline methods. besides method adopted comparison since shown better efﬁciency ℓ-norm methods follow experimental settings comparison. negligible. bmpl bomp experiments. furthermore considering images cannot sparse-represented training images constrain extended yaleb databases used comparison. extended yaleb database consists frontal face images subjects captured various lighting conditions cropped normalized pixels. experiment take images person resulting images total. database consists frontal images individuals image normalized pixels. computing images extended yaleb takes seconds seconds respectively. words time spent negligible. many-face recognition different number experiment down-sample impixels chosen ages yaleb images images. accordingly dimension image vector original image vector. following randomly choose half images person training remaining images testing set. prediction accuracies yaleb images shown table measure difference results wilcoxon test signiﬁcance conducted bmpl winner indicates signiﬁcant difference. database bmpl performs signiﬁcantly better methods bmpl particular shows much stable performance methods. particular database achieves prediction accuracy down-sampling rate caused unstable pseudo inverse ill-conditioned matrix regularized method method shows stable performance however still worse bmpl. report total time spent various methods table state-of-the-art ℓ-solver needs several hours predict testing images database unbearable many real-world applications. contrary bmpl completes prediction seconds only times faster pgh. bmpl also times faster bomp. lastly bmpl achieves comparable efﬁciency remaining question sparsity help improve average sparsity recognition performance? list bmpl bomp table vii. note solutions obtained methods sparse. table bmpl bomp show comparable signiﬁcantly better recognition rates methods yaleb database. addition bmpl outperforms methods database enough pixels. therefore sparsity indeed helps improve recognition rates. face recognition different number training samples ratio number training images total number images. experiment vary change number training images. prediction accuracy prediction time w.r.t. shown tables viii respectively. general training images matrix becomes ill-conditioned. table viii bmpl performs signiﬁcantly better words bmpl achieves stable performance becomes ill-conditioned. finally table bmpl shows comparable efﬁciency methods. developed vastly speed many signals. comprehensive experiments demonstrate superb efﬁciency proposed methods. general tens times faster state-of-the-art ℓ-norm methods. recovery time smpl method dictionary million atoms less seconds. apply bmpl batch face recognition tasks. experimental results show bmpl achieves signiﬁcantly better recognition rates comparable computational cost. notably bmpl times faster batch-mode times faster ℓ-norm methods considered state-of-the-art. authors would like thank anonymous reviewers insightful comments suggestions greatly improved paper. research partially supported australian research council future fellowship australian research council grants elhamifar vidal sparse subspace clustering cvpr adler elad hel-or fast subspace clustering sparse representations department computer science technion israel tech. rep. figueiredo nowak wright gradient projection sparse reconstruction application compressed sensing inverse problems ieee sel. top. sign. proces. special issue convex optimization methods signal processing nesterov gradient methods minimizing composite objective function center operations research econometrics catholic university louvain tech. rep. giryes elad rip-based near-oracle performance guarantees subspace-pursuit cosamp iterative hard-thresholding ieee trans. signal process. vol. pati rezaiifar krishnaprasad orthogonal matching pursuit recursive function approximation applications wavelet decomposition twenty-seventh asilomar conference signals systems computers. tropp greed good algorithmic results sparse approximation ieee trans. info. theory vol. donoho tsaig drori starck sparse solution underdetermined systems linear equations stagewise orthogonal matching pursuit ieee trans. info. theory vol. bradley kyrola bickson guestrin. parallel coordinate descent l-regularized loss minimization icml xiang ramadge. learning sparse representations", "year": 2013}