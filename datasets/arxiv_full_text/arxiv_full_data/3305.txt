{"title": "Multi-criteria Similarity-based Anomaly Detection using Pareto Depth  Analysis", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "We consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection. Similarity-based anomaly detection algorithms detect abnormally large amounts of similarity or dissimilarity, e.g.~as measured by nearest neighbor Euclidean distances between a test sample and the training samples. In many application domains there may not exist a single dissimilarity measure that captures all possible anomalous patterns. In such cases, multiple dissimilarity measures can be defined, including non-metric measures, and one can test for anomalies by scalarizing using a non-negative linear combination of them. If the relative importance of the different dissimilarity measures are not known in advance, as in many anomaly detection applications, the anomaly detection algorithm may need to be executed multiple times with different choices of weights in the linear combination. In this paper, we propose a method for similarity-based anomaly detection using a novel multi-criteria dissimilarity measure, the Pareto depth. The proposed Pareto depth analysis (PDA) anomaly detection algorithm uses the concept of Pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights. The proposed PDA approach is provably better than using linear combinations of the criteria and shows superior performance on experiments with synthetic and real data sets.", "text": "dissimilarities data samples using single dissimilarity criterion euclidean distance. examples include approaches based k-nearest neighbor distances local neighborhood densities local p-value estimates geometric entropy minimization many application domains involving categorical data possible practical represent data samples geometric space order compute euclidean distances. furthermore multiple dissimilarity measures corresponding different criteria required detect certain types anomalies. example consider problem detecting anomalous object trajectories video sequences different lengths. multiple criteria dissimilarities object speeds trajectory shapes used detect greater range anomalies single criterion. order perform anomaly detection using multiple criteria could ﬁrst combine dissimilarities criterion using non-negative linear combination apply anomaly detection algorithm. however many applications importance different criteria known advance. thus difﬁcult determine much weight assign criterion anomaly detection algorithm multiple times using different weights selected grid search similar method. propose novel multi-criteria approach similaritybased anomaly detection using pareto depth analysis uses concept pareto optimality typical method deﬁning optimality multiple conﬂicting criteria comparing items. item said pareto-optimal exist another item better equal criteria. item pareto-optimal optimal usual sense combination criteria. hence able detect anomalies multiple combinations criteria without explicitly forming combinations. approach involves creating dyads corresponding dissimilarities pairs data samples criteria. sets pareto-optimal dyads called pareto fronts computed. ﬁrst pareto front non-dominated dyads. second pareto front obtained removing non-dominated dyads i.e. peeling ﬁrst front recomputing ﬁrst pareto front remaining. process continues dyads remain. dyad assigned pareto front depth abstract—we consider problem identifying patterns data exhibit anomalous behavior often referred anomaly detection. similarity-based anomaly detection algorithms detect abnormally large amounts similarity dissimilarity e.g. measured nearest neighbor euclidean distances test sample training samples. many application domains exist single dissimilarity measure captures possible anomalous patterns. cases multiple dissimilarity measures deﬁned including nonmetric measures test anomalies scalarizing using non-negative linear combination them. relative importance different dissimilarity measures known advance many anomaly detection applications anomaly detection algorithm need executed multiple times different choices weights linear combination. paper propose method similarity-based anomaly detection using novel multi-criteria dissimilarity measure pareto depth. proposed pareto depth analysis anomaly detection algorithm uses concept pareto optimality detect anomalies multiple criteria without algorithm multiple times different choices weights. proposed approach provably better using linear combinations criteria shows superior performance experiments synthetic real data sets. index terms—multi-criteria dissimilarity measure similaritybased learning combining dissimilarities pareto front scalarization partial correlation identifying patterns anomalous behavior data often referred anomaly detection important problem diverse applications including intrusion detection computer networks detection credit card fraud medical informatics similarity-based approaches anomaly detection generated much interest relative simplicity robustness compared model-based cluster-based density-based approaches approaches typically involve calculation similarities work partially supported grant wnf--- wnf---. paper submitted ieee tnnls special issue learning non-metric spaces review october revised july accepted july preliminary version work reported conference publication fig. illustrative example training samples test samples dyads training samples along ﬁrst pareto fronts criteria |∆x| |∆y|. pareto fronts induce partial ordering dyads. dyads associated test sample marked circle concentrate around shallow fronts dyads associated test sample marked triangle concentrate around deep fronts. majority training samples assumed nominal. thus nominal test sample would likely similar many training samples criteria dyads nominal test sample would appear shallow pareto fronts. hand anomalous test sample would likely dissimilar many training samples many criteria dyads anomalous test sample would located deep pareto fronts. thus computing pareto depths dyads corresponding test sample discriminate nominal anomalous samples. assumption multi-criteria dyads modeled realizations k-dimensional density provide mathematical analysis properties ﬁrst pareto front relevant anomaly detection. particular scalarization theorem prove upper lower bounds degree pareto fronts non-convex. algorithm using non-negative linear combinations criteria non-convexities pareto fronts contribute artiﬁcially inﬂated anomaly score resulting increased false positive rate. thus analysis shows precise sense outperform algorithm uses non-negative linear combination criteria. furthermore theoretical prediction experimentally validated comparing several single-criterion similarity-based anomaly detection algorithms experiments involving synthetic real data sets. rest paper organized follows. discuss related work section section provide introduction pareto fronts present theoretical analysis properties ﬁrst pareto front. section relates pareto fronts multi-criteria anomaly detection problem leads anomaly detection algorithm. finally present three experiments section provide experimental support theoretical results evaluate performance anomaly detection. several machine learning methods utilizing pareto optimality previously proposed; overview found methods typically formulate supervised machine learning problems multi-objective optimization problems potentially inﬁnite candidate items ﬁnding even ﬁrst pareto front quite difﬁcult often requiring multi-objective evolutionary algorithms. methods differ pareto optimality consider pareto fronts created ﬁnite items need employ sophisticated algorithms order fronts. rather utilize pareto fronts form statistical criterion anomaly detection. finding pareto front ﬁnite items also referred literature skyline query maximal vector problem research skyline queries focused efﬁciently compute approximate items ﬁrst pareto front efﬁciently store results memory. algorithms skyline queries used proposed approach computing pareto fronts. work differs skyline queries focus utilization multiple pareto fronts purpose multi-criteria anomaly detection efﬁcient computation approximation ﬁrst pareto front. hero fleury introduced method gene ranking using multiple pareto fronts related approach. method ranks genes order interest biologist creating pareto fronts data samples i.e. genes. paper consider pareto fronts dyads correspond dissimilarities pairs data samples multiple criteria rather samples themselves distribution dyads pareto fronts perform multicriteria anomaly detection rather gene ranking. another related area multi-view learning involves learning data represented multiple sets features commonly referred views. case training view assumed help improve learning another view. problem view disagreement samples take different classes different views recently investigated views similar criteria problem setting. however setting different criteria orthogonal could even give contradictory information; hence severe view disagreement. thus training view could actually worsen performance another view problem consider differs multi-view learning. similar area multiple kernel learning typically applied supervised learning problems unlike unsupervised anomaly detection setting consider. many methods anomaly detection previously proposed. hodge austin chandola provide extensive surveys different anomaly detection methods applications. paper focuses similarity-based approach anomaly detection also known instance-based learning. approach typically involves transforming similarities between test sample training samples anomaly score. byers raftery proposed distance sample kth-nearest neighbor anomaly score sample; similarly angiulli pizzuti eskin proposed distances sample nearest neighbors. breunig used anomaly score based local density nearest neighbors sample. hero sricharan hero introduced non-parametric adaptive anomaly detection methods using geometric entropy minimization based random k-point minimal spanning trees bipartite k-nearest neighbor graphs respectively. zhao saligrama proposed anomaly detection algorithm k-lpe using local p-value estimation based k-nn graph. aforementioned anomaly detection methods depend data pairs data points deﬁne edges k-nn graphs. methods designed single criterion unlike anomaly detection algorithm propose paper accommodates dissimilarities corresponding multiple criteria. related approaches anomaly detection include class support vector machines classiﬁer trained given samples single class tree-based methods anomaly score data sample determined depth tree ensemble trees. isolation forest sciforest treebased approaches targeted detecting isolated clustered anomalies respectively using depths samples ensemble trees. tree-based approaches utilize depths form anomaly scores similar pda; however operate feature representations data rather dissimilarity representations. developing multi-criteria extensions non-similarity-based methods beyond scope paper would worthwhile future work. consider following problem given items denoted criteria evaluating item denoted functions select minimizes fd]. settings possible single item simultaneously minimizes many approaches multi-criteria optimization problem reduce combining criteria single criterion process often referred scalarization common approach non-negative linear combination fi’s item minimizes linear combination. different choices weights linear combination yield different minimizers. case would need identify optimal solutions corresponding different weights using example grid search weights. robust powerful approach involves identifying pareto-optimal items. item said strictly dominate another item greater criterion less least criterion. relation written pareto-optimal items called pareto front items strictly dominated another item contains minimizers found using non-negative linear combinations also includes items cannot found linear combinations. denote pareto front call ﬁrst pareto front. second pareto front constructed ﬁnding items strictly dominated remaining items members generally deﬁne pareto front distribution number points ﬁrst pareto front ﬁrst studied barndorff-nielsen sobel problem garnered much attention since. hwang tsai provide good surveys recent results. concerned properties ﬁrst pareto front relevant anomaly detection algorithm considered literature. identically distributed density function denote ﬁrst pareto front general multi-criteria optimization framework points images feasible solutions optimization problem vector objective functions convexity pareto front. several ways pareto front non-convex. first suppose distributed domain continuous density function strictly positive portion boundary unit inward normal conditions guarantee portion ﬁrst pareto front concentrate near suppose contained interior convex hull points portion pareto front near cannot obtained linear scalarization non-convex portion front. non-convexities direct result geometry domain depicted fig. preliminary version work studied expectation number points pareto front within neighborhood result showed unlikely would enough information compute constant paper instead study second type non-convexity pareto front. non-convexities strictly randomness positions points occur even domain convex following assume i.i.d. unit hypercube bounded density function continuous origin strictly positive assumptions turns asymptotics independent hence results applicable wide range problems without need know detailed information density length context multi-criteria anomaly detection point dyad corresponding dissimilarities between data samples multiple criteria number criteria. common approach multi-objective optimization linear scalarization constructs single criterion non-negative linear combination criteria. well-known easy linear scalarization identify pareto-optimal points boundary convex hull although common motivation pareto optimization methods best knowledge results literature regarding many points pareto front missed scalarization. present result section namely scalarization theorem. subset contains pareto-optimal points obtained selection non-negative weights linear scalarization. denote cardinality denote cardinality uniformly distributed unit hypercube barndorff-nielsen sobel showed many recent works studied variance proven central limit theorems works assume uniformly distributed summary works studied general distributions domains smooth non-horizontal boundaries near pareto front multivariate normal distributions non-horizontal condition excludes hypercubes. best knowledge results asymptotics non-uniformly distributed points unit hypercube. great importance impractical multi-criteria optimization assume coordinates points independent. typically coordinates images feasible solution several different criteria general independent. develop results size number items discoverable scalarization compared number items discovered pareto front. larger suboptimal scalarization relative pareto optimization. since boundary convex hull size related deﬁnition number criteria number training samples training sample single test sample dissimilarity training samples using criterion dyad training samples dyads training samples pareto front dyads training samples total number pareto fronts dyads training samples dyad training sample test sample pareto depth dyad training sample test sample number nearest neighbors criterion total number nearest neighbors anomaly score test sample data samples available. given test sample objective anomaly detection declare anomaly signiﬁcantly different samples suppose different evaluation criteria given. criterion associated measure computing dissimilarities. denote dissimilarity computed using dissimilarity measure corresponding criterion note need metric; particular necessary distance function sample space satisfy triangle inequality. deﬁne dyad pair samples vector dk]t ferent dyads training set. convenience denote dyads deﬁnition strict dominance section dyad strictly dominates another dyad di∗j∗ ﬁrst pareto front corresponds dyads strictly dominated dyads second pareto front corresponds dyads strictly dominated dyads deﬁned section iii. recall refer deeper front dyads corresponding connections training samples. dyads located shallow pareto fronts dissimilarities training samples small combination criteria. thus likely nominal sample. basic idea proposed multi-criteria anomaly detection method using pda. construct pareto fronts dyads training total number fronts required number fronts dyad member front. test sample obtained create dyads fig. non-convexities pareto front induced geometry domain non-convexities randomness points. case larger points pareto-optimal large black points cannot obtained scalarization. scalarization theorem shows fraction pareto-optimal points cannot obtained linear scalarization least provide experimental evidence supporting bounds section v-a. corresponding connections training samples illustrated fig. like many similarity-based anomaly detection methods connect test sample nearest neighbors. could different criterion denote choice criterion create dyads corresponding connections union nearest neighbors criterion words create dyad test sample training sample among nearest neighbors criterion front i.e. strictly dominates least single dyad deﬁne pareto depth recall scalarization theorem provides bounds fraction dyads ﬁrst pareto front cannot obtained linear scalarization. speciﬁcally least dyads missed linear scalarization average. dyads associated deeper fronts linear scalarization artiﬁcially inﬂate anomaly score test sample resulting increased false positive rate algorithm utilizes non-negative linear combinations criteria. effect cascades dyads deeper pareto fronts also assigned inﬂated anomaly scores. provide evidence effect real data experiment section v-c. finally lower bound increases monotonically implies approach gains additional advantages linear combinations number criteria increases. responding pairs training samples. computing pairwise dissimilarities criterion requires ﬂoating-point operations denotes number dimensions involved computing dissimilarity. time complexity training phase dominated construction pareto fronts non-dominated sorting. non-dominated sorting used heavily evolutionary computing community; fastest algorithm non-dominated sorting proposed jensen later generalized fortin utilizes comparisons. complexity analyses asymptotic assume ﬁxed. unaware analyses asymptotics another non-dominated sorting algorithm proposed constructs pareto fronts using k-nn based anomaly detection algorithms mentioned section ii-b anomaly score function nearest neighbors test sample. multiple criteria could deﬁne anomaly score scalarization. probabilistic properties pareto fronts discussed section iii-b know pareto optimization methods identify pareto-optimal points linear scalarization methods signiﬁcantly pareto-optimal points single weight scalarization. motivates develop multi-criteria anomaly score using pareto fronts. start observation fig. dyads corresponding nominal test sample typically located near shallower fronts dyads corresponding anomalous test sample. test sample associated dyads dyad depth pareto depth multi-criteria dissimilarity measure indicates dissimilarity test sample training sample multiple combinations criteria. test sample deﬁne anomaly score mean ei’s corresponds average depth dyads associated equivalently average multi-criteria dissimilarities test sample nearest neighbors. thus anomaly score easily computed compared decision threshold using test nearest neighbors multiple criteria multiple copies dyad created. also experimented creating single copy dyad found little difference detection accuracy. theorems require i.i.d. samples dyads independent. however dyads dyad dependent dyads. suggests theorems also hold noni.i.d. dyads well supported experimental results presented section v-a. testing phase involves creating dyads test sample nearest training samples criterion requires ﬂops. dyad need calculate depth involves comparing test dyad training dyads multiple fronts training dyad dominated test dyad. front training dyad part using binary search select front another binary search select training dyads within front compare need make comparisons compute anomaly score computed taking mean ei’s corresponding test sample; score compared threshold determine whether sample anomalous. selection parameters unsupervised learning problems difﬁcult general. criterion construct kl-nn graph using corresponding dissimilarity measure. construct symmetric kl-nn graphs i.e. connect samples nearest neighbors nearest neighbors choose starting point necessary increase kl-nn graph connected. method choosing motivated asymptotic results connectivity k-nn graphs used heuristic unsupervised learning problems spectral clustering heuristic works well practice including real data pedestrian trajectories present section v-c. ﬁrst present experiment involving scalarization dyads compare method single-criterion anomaly detection algorithms simulated data real data set. algorithms comparison follows local density nearest neighbors -svm -class support vector machine methods linear combinations criteria different weights compare performance proposed multi-criteria method. accuracies nearest neighbor-based methods fig. sample means /cnd versus dimension dyads. upper lower bounds established scalarization theorem given dotted lines ﬁgure. ﬁgure fraction pareto optimal points obtainable linear scalarization increases dimension. vary much experiments results report neighbors. -class difﬁcult choose bandwidth gaussian kernel without labeled anomalous samples. linear kernels found perform similarly gaussian kernels dissimilarity representations svms classiﬁcation tasks hence linear kernel scalarized dissimilarities -class svm. independence built assumptions theorems thus scalarization theorem clear dyads independent. dyad represents connection independent training samples given dyad corresponding dyads involving clearly independent dij. however dyads independent dij. dyads dyad independent dyads except size since scalarization theorem asymptotic result observation suggests hold dyads even though i.i.d. subsection present experimental generate synthetic dyads drawing i.i.d. uniform samples constructing dyads corresponding criteria |∆x| |∆y| denote absolute differences coordinates respectively. domain resulting dyads case scalarization theorem suggests grow logarithmically. fig. shows sample means versus number dyads best logarithmic number dyads. vary number dyads increments compute size increment. compute sample means realizations. linear regression versus gives falls range speciﬁed scalarization theorem. next explore dependence dimension here generate dyads before dimensions criteria case correspond absolute differences dimension. fig. plot e/cnd versus dimension show fraction pareto-optimal points cannot obtained scalarization. recall theorem based ﬁgure might conjecture fraction unattainable pareto optimal points converges true would essentially imply linear scalarization useless identifying dyads ﬁrst pareto front large number criteria. before compute sample means realizations experiment. experiment perform multi-criteria anomaly detection simulated data multiple groups categorical attributes. groups could represent different types attributes. data sample consists groups categorical attributes. denote attribute group denote number possible values attribute. randomly select possible values attribute equal probability independent attributes. attribute random variable described categorical distribution parameters qnij categorical distribution sampled dirichlet distribution parameters αnij nominal data sample αnij attribute group simulate anomalous data sample randomly select group probability parameters dirichlet distribution changed αnij attribute group note different anomalous samples differ group selected. pi’s probability test sample anomalous non-uniform distribution pi’s results criteria fig. compared aucs single-criterion methods simulated experiment. single-criterion methods randomly sampled weights linear scalarization weights ordered worst choice weights best choice terms maximizing auc. proposed algorithm multi-criteria algorithm require selecting weights. outperforms single-criterion methods even best choice weights known advance. useful others identifying anomalies. criteria anomaly detection taken dissimilarities data samples groups attributes. group calculate dissimilarity attributes using dissimilarity measure anomaly detection categorical data proposed draw training samples nominal distribution test samples mixture nominal anomalous distributions. single-criterion algorithms baselines comparison linear scalarization multiple choices weights. since grid search scales exponentially number criteria computationally intractable even moderate values instead uniformly sample weights -dimensional simplex. words sample weights uniform distribution convex combinations criteria. detection accuracy different methods evaluated using receiver operating characteristic curve area curve ﬁrst number criteria mean aucs simulation runs shown fig. multiple choices weights fig. normalized computation time function number training samples simulated experiment. best curves form best curve -svm extrapolated beyond samples best curve pda-deb extrapolated beyond samples. fig. normalized computation time function number criteria simulated experiment. pda-deb appears linear predicted. pda-fortin initially appears exponential computation time continue increase exponentially beyond ﬁrst measure computation time uniformly distributed scale since actual computation time depends heavily implementation non-dominated sorts normalize computation times time required train anomaly detector observe scaling normalized times well k-lpe -svm shown fig. best curves form also plotted estimated linear regression. pda-deb time complexity estimated exponent four algorithms worst scaling pda-fortin time complexity estimated exponent conﬁrming scales much better pda-deb applicable large data sets. k-lpe representative k-nn algorithms time complexity estimated exponent difﬁcult determine time complexity -svm iterative nature. estimated exponent -svm suggesting scales worse pda-fortin. next measure computation time varying normalize time required train anomaly detector observe scaling normalized time pda-deb shown fig. along best line form normalized time indeed appear linear fig. ratio compared best median aucs scalarization using number criteria varied simulated experiment. choices weights uniformly sampled -dimensional simplex chosen scalarization. perfoms signiﬁcantly better median weights outperforms best weights scalarization margin increases increases. used linear scalarization single-criterion algorithms; results ordered worst best weight terms maximizing auc. k-lpe perform roughly equally shown ﬁgure. table presents comparison median best aucs choices weights scalarization. mean standard error aucs simulation runs shown. notice outperforms even best weighted combination single-criterion algorithms signiﬁcantly outperforms combination resulting median representative performance expects obtain arbitrarily choosing weights. next investigate performance scalarization number criteria varies performance single-criterion algorithms close show scalarization results lof. ratio aucs best median weights scalarization shown fig. offers signiﬁcant improvement compared median weights scalarization. small values performs roughly equally scalarization best choice weights. increases however clearly outperforms scalarization grows believe partially inadequacy scalarization identifying pareto fronts described scalarization theorem partially difﬁculty selecting optimal weights criteria. grid search able reveal better weights scalarization also computationally intractable large thus conclude clearly superior approach large computation time evaluate computation time scales varying using nondominated sorting procedures fortin discussed section iv-c. time complexity testing phase negligible compared training phase fig. compared aucs single-criterion methods pedestrian trajectories experiment. single-criterion methods linear scalarization uniformly spaced weights; weights ordered worst best terms maximizing auc. outperforms single-criterion methods almost choices weights. second criterion compute dissimilarity shape. calculate shape dissimilarity trajectories apply technique known dynamic time warping ﬁrst non-linearly warps trajectories time match optimal manner. take dissimilarity summed euclidean distance warped trajectories. dissimilarity also satisfy triangle inequality general thus metric. training experiment consists randomly sampled trajectories data small fraction anomalous. test consists trajectories trajectories test labeled nominal anomalous human viewer. labels used ground truth evaluate anomaly detection performance. fig. shows anomalous trajectories nominal trajectories detected using pda. experiment times; different random sample training trajectories. fig. shows performance compared algorithms using uniformly spaced weights convex combinations. notice higher methods almost choices weights criteria. shown table along aucs median fig. anomalous pedestrian trajectories detected pda. trajectories relatively anomaly scores. criteria used walking speed trajectory shape. anomalous trajectories could anomalous speeds shapes anomalous trajectories look anomalous shape alone. grows slowly. normalized time pda-fortin shown fig. along best curve form notice computation time initially increases exponentially increases much slower possibly even linear rate beyond analyses time complexity asymptotic assume ﬁxed; aware analyses time complexity asymptotic experiments suggest pda-fortin computationally tractable non-dominated sorting even large finally note scaling scalarization methods trivial depending simply number choices scalarization weights exponential grid search. denote pedestrian’s position time step pedestrian trajectories different lengths cannot simply treat trajectories vectors calculate euclidean distances them. instead propose calculate dissimilarities trajectories using separate criteria trajectories dissimilar. ﬁrst criterion compute dissimilarity walking speed. compute instantaneous speed time steps along trajectory ﬁnite differencing i.e. time step given trajectory obtained manner. take dissimilarity trajectories kullback-leibler divergence normalized speed histograms trajectories. divergence commonly used measure difference probability distributions. divergence asymmetric; convert dissimilarity symmetrized divergence originally deﬁned fig. curves attainable region choices weights pedestrian trajectories experiment. attainable region denotes possible curves corresponding different choices weights linear scalarization. rocs linear scalarization vary greatly function weights. fig. aucs different choices pedestrian trajectories experiment. parameters chosen using proposed heuristic within maximum obtained parameters scalarization) shown fig. convex fronts denote shallowest deepest convex fronts containing dyads illustrated pareto front. test samples associated dyads near middle pareto fronts would suffer aforementioned score inﬂation would found deeper convex fronts tails. finally note proposed algorithm appear sensitive choices number neighbors shown fig. fact heuristic proposed choosing kl’s section iv-d performs quite well experiment. speciﬁcally obtained using parameters chosen proposed heuristic close maximum choices number neighbors paper proposed method similarity-based anomaly detection using novel multi-criteria dissimilarity measure pareto depth. proposed method utilizes notion pareto optimality detect anomalies multiple criteria examining pareto depths dyads corresponding test sample. dyads corresponding anomalous sample tended located deeper fronts compared dyads corresponding nominal sample. instead choosing speciﬁc weighting performing grid search weights dissimilarity measures corresponding different criteria proposed method efﬁciently detect anomalies manner tractable large number criteria. furthermore proposed pareto depth analysis approach provably better using linear combinations criteria. numerical studies validated theoretical predictions pda’s performance advantages compared using linear combinations simulated real data. interesting avenue future work extend approach extremely large data sets using approximate rather exact pareto fronts. addition skyline algorithms information retrieval community focus approximating ﬁrst pareto front recent work approximating pareto fronts using partial differential fig. comparison pareto front dyads convex fronts obtained linear scalarization. dyads towards middle pareto front found deeper convex fronts towards edges. result would inﬂated anomaly scores samples associated dyads middle pareto fronts using linear scalarization. best choices weights single-criterion methods. mean standard error runs shown. best choice weights single-criterion method highest also lowest worst choice weights. detailed comparison curve attainable region shown fig. note curve vary signiﬁcantly based choice weights. -svm also depends heavily weights. unsupervised setting unlikely would able achieve curve corresponding weight highest expected performance closer median aucs table iii. many pareto fronts dyads non-convex partially explaining superior performance proposed algorithm. non-convexities pareto fronts lead inﬂated anomaly scores linear scalarization. comparison pareto front convex fronts angiulli pizzuti fast outlier detection high dimensional spaces proceedings european conference principles data mining knowledge discovery helsinki finland aug. eskin arnold prerau portnoy stolfo geometric framework unsupervised anomaly detection detecting intrusions unlabeled data applications data mining computer security barbar´a jajodia eds. norwell kluwer sendhoff pareto-based multiobjective machine learning overview case studies ieee transactions systems cybernetics part applications reviews vol. blum mitchell combining labeled unlabeled data co-training proceedings annual conference computational learning theory madison jul. sindhwani niyogi belkin co-regularization approach semi-supervised learning multiple views proceedings icml workshop learning multiple views bonn germany aug. christoudias urtasun darrell multi-view learning presence view disagreement proceedings conference uncertainty artiﬁcial intelligence helsinki finland jul. detecting clustered anomalies using sciforest proceedings european conference machine learning principles practice knowledge discovery databases barcelona spain sep. h.-k. hwang t.-h. tsai multivariate records based dominance electronic journal probability vol. baryshnikov yukich maximal points gaussian ﬁelds preprint. available http//www.math.illinois. edu/∼ymb/ps/by.pdf jeff calder currently morrey assistant professor mathematics university california berkeley berkeley usa. received ph.d. degree applied interdisciplinary mathematics university michigan arbor supervision selim esedoglu alfred hero. primary research interests analysis partial differential equations applied probability mathematical problems computer vision image processing. received m.sc. degree mathematics queens university kingston canada supervision a.-r. mansouri developed sobolev gradient techniques image diffusion sharpening b.sc. degree mathematics engineering queens university specialization control communications. f.-a. fortin grenier parizeau generalizing improved run-time complexity algorithm non-dominated sorting proceedings annual conference genetic evolutionary computation amsterdam netherlands jul. boriah chandola kumar similarity measures categorical data comparative evaluation proceedings siam international conference data mining atlanta apr. majecka statistical models pedestrian behaviour forum master’s thesis school informatics university edinburgh kullback leibler information sufﬁciency alfred hero received b.s. boston university ph.d princeton university electrical engineering. since university michigan arbor jamison betty williams professor engineering co-director michigan institute data science primary appointment department electrical engineering computer science also appointments courtesy department biomedical engineering department statistics. held digiteo chaire d’excellence ecole superieure d’electricite gif-suryvette france. fellow institute electrical electronics engineers several research articles received best paper awards. alfred hero awarded university michigan distinguished faculty achievement award received ieee signal processing society meritorious service award ieee third millenium medal ieee signal processing society technical achievement award alfred hero president ieee signal processing society board directors ieee served director division served ieee nominations appointments committee alfred hero currently member data special interest group ieee signal processing society. since member committee applied theoretical statistics national academies science. alfred hero’s recent research interests data science high dimensional spatio-temporal data statistical signal processing machine learning. particular interest applications networks including social networks multi-modal sensing tracking database indexing retrieval imaging biomedical signal processing biomolecular signal processing. ko-jen hsiao obtained ph.d. electrical engineering computer science university michigan received b.a.sc. degree electrical engineering national taiwan university master degrees electrical engineering systems applied mathematics university michigan kojen hsiao’s main research interests statistical signal processing machine learning applications anomaly detection image retrieval recommendation systems time series analysis. currently data scientist applied researcher machine learning whispertext. kevin received b.a.sc. degree electrical engineering university waterloo m.s.e. ph.d. degrees electrical engineering systems university michigan respectively. recipient natural sciences engineering research council canada postgraduate masters doctorate scholarships. currently assistant professor eecs department university toledo previously held in-dustry research positions technicolor main research interests machine learning statistical signal processing applications network science human dynamics.", "year": 2015}