{"title": "Approximated Structured Prediction for Learning Large Scale Graphical  Models", "tag": ["cs.LG", "cs.AI"], "abstract": "This manuscripts contains the proofs for \"A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction\".", "text": "composed conjugate dual soft-max conjugate dual norm. recall conjugate dual soft-max entropy barrier probability distributions theorem linear shift soft-max argument result linear shift conjugate dual thus ﬁrst part dual function dual norm deriving dual minimizing compact beliefs enables obtain unconstrained dual corresponds approximated structured prediction program. dual function described conjugate dual function every constant cxyv→α zero µxyα→v corresponds norm computed max-function. moreover either and/or zero objective optimal λxyv→α computed arbitrary similarly concludes proof whenever quantitates zero danskin’s theorem theorem states corresponding subgradient described probability distribution maximal assignments. therefore objective function equality holds every similarly whenever objective equality holds every claim block coordinate descent algorithm lemmas monotonically reduces approximated structured prediction objective theorem therefore value objective guaranteed converge. moreover objective guaranteed converge global minimum sequence beliefs guaranteed converge unique solution approximated structured prediction dual. proof approximated structured prediction dual strictly concave dual variables bxyv bxyα subject linear constraints. claim properties direct consequence type programs. claim whenever approximated structured prediction convex i.e. algorithm lemmas guaranteed converge whenever converges reaches stationary point primal dual approximated structured prediction programs. proof approximated structured prediction theorem unconstrained. update rules deﬁned lemmas directly related vanishing points gradient function even non-convex. therefore stationary point algorithm corresponds assignment λxyv→α gradient equals zero equivalently stationary point approximated structured prediction. dual approximated structured prediction constrained optimization stationary points saddle points lagrangian deﬁned theorem respect probability simplex bxyv bxyα ∆yα. note since entropy functions barrier functions nonnegative cone therefore need consider nonnegative constraints beliefs. following show stationary points inferred beliefs lagrangian satisfy marginalization constraints therefore saddle points lagrangian. prove beliefs correspond stationary point show satisfy marginalization constraints. fact direct consequence update rule lemma direct computation verify following deﬁnition bxyv update rule lemma enforces marginalization constraints. implies gradient approximated structured bxyα bxyv gradient vanishes agree. therefore beliefs correspond saddle point lagrangian.", "year": 2010}