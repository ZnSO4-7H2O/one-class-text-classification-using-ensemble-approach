{"title": "Apprentice: Using Knowledge Distillation Techniques To Improve  Low-Precision Network Accuracy", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Deep learning networks have achieved state-of-the-art accuracies on computer vision workloads like image classification and object detection. The performant systems, however, typically involve big models with numerous parameters. Once trained, a challenging aspect for such top performing models is deployment on resource constrained inference systems - the models (often deep networks or wide networks or both) are compute and memory intensive. Low-precision numerics and model compression using knowledge distillation are popular techniques to lower both the compute requirements and memory footprint of these deployed models. In this paper, we study the combination of these two techniques and show that the performance of low-precision networks can be significantly improved by using knowledge distillation techniques. Our approach, Apprentice, achieves state-of-the-art accuracies using ternary precision and 4-bit precision for variants of ResNet architecture on ImageNet dataset. We present three schemes using which one can apply knowledge distillation techniques to various stages of the train-and-deploy pipeline.", "text": "deep learning networks achieved state-of-the-art accuracies computer vision workloads like image classiﬁcation object detection. performant systems however typically involve models numerous parameters. trained challenging aspect performing models deployment resource constrained inference systems models compute memory intensive. low-precision numerics model compression using knowledge distillation popular techniques lower compute requirements memory footprint deployed paper study combination techniques models. show performance low-precision networks signiﬁcantly improved using knowledge distillation techniques. approach apprentice achieves state-of-the-art accuracies using ternary precision -bit precision variants resnet architecture imagenet dataset. present three schemes using apply knowledge distillation techniques various stages train-and-deploy pipeline. background today’s high performing deep neural networks computer vision applications comprise multiple layers involve numerous parameters. networks compute requirements generate models storage further memory compute requirements training inference quite different training performed datasets large batch-sizes memory footprint activations dominates model memory footprint. hand batch-size inference typically small model’s memory footprint dominates runtime memory requirements. complexity compute memory storage requirements training phase networks performed and/or clusters distributed computing environment. trained challenging aspect deployment trained models resource constrained inference systems portable devices sensor networks applications real-time predictions required. performing inference edge-devices comes severe constraints memory compute power. additionally ensemble based methods potentially improved accuracy predictions become prohibitive resource constrained systems. quantization using low-precision numerics model compression emerged popular solutions resource constrained deployment scenarios. quantization low-precision version network model generated deployed device. operating lower precision mode reduces compute well data movement storage requirements. however majority existing works low-precision dnns sacriﬁce accuracy baseline full-precision networks. model compression smaller memory footprint network trained mimic behaviour original complex network. training process called knowledge distillation used transfer knowledge complex network smaller network. work hinton shows knowledge distillation scheme yield networks comparable slightly better accuracy original complex model. however best knowledge prior works using model compression techniques target compression full-precision. proposal paper study combination network quantization model compression show accuracies low-precision networks signiﬁcantly improved using knowledge distillation techniques. previous studies model compression large network teacher network small network student network. small student network learns teacher network using distillation process. network architecture student network typically different teacher network e.g. hinton investigate student network fewer number neurons hidden layers compared teacher network. work student network similar topology teacher network except student network low-precision neurons compared teacher network neurons operating full-precision. call approach apprentice study three schemes produce low-precision networks using knowledge distillation techniques. three schemes produce state-of-the-art ternary precision -bit precision models. ﬁrst scheme low-precision network full-precision network jointly trained scratch using knowledge distillation scheme. later paper describe rationale behind approach. using scheme state-of-the-art accuracy obtained ternary -bit precision resnet- resnet- resnet- imagenet dataset. fact using scheme accuracy full-precision model also slightly improves. scheme serves baseline schemes investigate. second scheme start full-precision trained network transfer knowledge trained network continuously train low-precision network scratch. low-precision network converges faster trained complex network guides training. third scheme start trained full-precision large network apprentice network initialised full-precision weights. apprentice network’s precision lowered ﬁne-tuned using knowledge distillation techniques. low-precision network’s accuracy marginally improves surpasses accuracy obtained ﬁrst scheme. scheme sets state-of-the-art accuracies resnet models ternary -bit precision. overall contributions paper techniques obtain low-precision dnns using knowledge distillation technique. scheme produces low-precision model surpasses accuracy equivalent low-precision model published date. schemes also helps low-precision model converge faster. envision accurate low-precision models simplify inference deployment process resource constrained systems even otherwise cloud-based deployment systems. lowering precision model parameters resource constrained inference systems impose significant restrictions memory compute power budget. regard storage model parameters activation maps occupy memory inference phase dnns. phase memory allocated input output feature maps required single layer dynamic memory allocations reused layers. total memory allocation inference maximum maximum memory required across layers plus weight tensors inference phase dnns performed small batch size memory footprint weights dictionary deﬁnes apprentice person learning trade skilled employer agreed work ﬁxed period wages. work apprentice low-precision network learning knowledge high precision network ﬁxed number epochs. exceeds footprint activation maps. aspect shown figure different networks inception-resnet-v resnet- resnet- running image patches. thus lowering precision weight tensors helps lower memory requirements deployment. beneﬁt low-precision compute low-precision compute simpliﬁes hardware implementation. example compute unit perform convolution operation involves ﬂoating-point multiplier using fullprecision weights activations. ﬂoatingpoint multiplier replaced much simpler circuitry using binary precision weights activations similarly using ternary precision weights full-precision activations multiplier unit replaced sign comparator unit simpler hardware also helps lower inference latency energy budget. thus operating lower precision mode reduces compute well data movement storage requirements. drawback low-precision models however degraded accuracy. discuss later paper network accuracies obtained using methods proposed literature. accuracies serve starting point baselines compare work. low-precision networks low-precision dnns active area research. low-precision networks acknowledge parameterization aspect today’s architectures and/or aspect lowering precision neurons post-training often impact ﬁnal performance. reducing precision weights efﬁcient inference pipeline well studied. works like binary connect ternary-weight networks ﬁne-grained ternary quantization target precision reduction network weights. accuracy almost always affected quantizing weights signiﬁcantly -bits precision. alexnet imagenet loses top- accuracy. schemes like work sung mellempudi ﬁne-tuning quantize network weights. work xnor-net binary neural networks dorefa trained ternary quantization target training pipeline. targets weight quantization works targeting activation quantization show quantizing activations always hurt accuracy. xnor-net approach degrades top- accuracy dorefa quantizing weights activations -bit work gupta advocates low-precision ﬁxed-point numbers training. show -bits sufﬁcient training cifar dataset. work seide quantizes gradients distributed computing system. knowledge distillation methods general technique distillation based methods involves using teacher-student strategy large deep network trained given task teaches shallower student network task. core concepts behind knowledge distillation transfer technique around while. buciluˇa show compress information ensemble single network. caurana extend approach study shallow wide fully connected topologies mimicking deep neural networks. facilitate learning authors introduce concepts learning logits rather probability distribution. hinton propose framework transfer knowledge introducing concept temperature. idea divide logits temperature factor performing softmax function. using higher temperature factor activations incorrect classes boosted. facilitates information ﬂowing model parameters back-propagation operation. fitnets extend work using intermediate hidden layer outputs target values training deeper thinner student model. netnet also uses teacher-student network system function-preserving transformation approach initialize parameters student network. goal netnet approach accelerate training larger student network. zagoruyko komodakis attention mechanism transferring knowledge network another. similar theme propose information metric using teacher transfer distilled knowledge student dnns. learning work ashok propose reinforcement learning based approach compressing teacher network equally capable student network. achieve compression factor resnet- cifar datasets. sparsity hashing popular techniques model compression pruning hashing weight sharing pruning leads removing neurons entirely ﬁnal trained model making model sparse structure. hashing weight sharing schemes hash function used alias several weight parameters hash buckets effectively lowering parameter memory footprint. realize beneﬁts sparsity hashing schemes runtime efﬁcient hardware support required figure shows schematic knowledge distillation setup. given input image teacher maps image predictions class predictions obtained applying softmax function un-normalized probablity values i.e. image student network predicts training cost function given where parameters teacher student network respectively ground truth denotes loss function weighting factors prioritize output certain loss function other. equation lowering ﬁrst term cost function gives better teacher network lowering second term gives better student network. third term knowledge distillation term whereby student network attempts mimic knowledge teacher network. hinton logits teacher network divided temperature factor using higher value produces softer probability distribution taking softmax logits. studies cross-entropy function perform transfer learning process using logits teacher network. experiments study effect varying depth teacher student network precision neurons student network. low-precision dnns target storage compute efﬁciency aspects network. model compression targets efﬁciency parameters point view network architecture. apprentice combine techniques improve network accuracy well runtime efﬁciency dnns. using teacher-student setup described last section investigate three schemes using obtain low-precision model student network. ﬁrst scheme jointly trains networks full-precision teacher low-precision student network. second scheme trains low-precision student network distills knowledge trained full-precision teacher network throughout training process. third scheme starts trained full-precision teacher full-precision student network ﬁne-tunes student network lowering precision. details schemes discuss accuracy numbers obtained using low-precision schemes described literature. accuracy ﬁgures serve baseline comparative analysis. focus -bits precision inference deployments speciﬁcally ternary -bits precision. found scheme achieving state-of-the-art accuracy ternary precision weights full-precision activations. imagenet-k achieves top- error rate resnet- model. implemented scheme resnet- resnet- models trained imagenet-k achieved top- error rates respectively. scheme baseline -bits weight full-precision activations. -bits weight -bits activation work mellempudi achieve best accuracies reported literature. resnet- mellempudi obtain top- error. consider work baseline -bits weight -bits activation models. -bits precision wrpn scheme report highest accuracy. implemented scheme -bits weight -bits activations. resnet- resnet- models trained imagenet-k achieve top- error rates respectively. ﬁrst scheme investigate full-precision teacher network jointly trained lowprecision student network. figure shows overall training framework. resnet topology teacher student network. using certain depth student network pick teacher network either larger depth. buciluˇa hinton student network trains distilling knowledge teacher network. case jointly train rationale teacher network would continuously guide student network ﬁnal trained logits also path teacher takes towards generating ﬁnal higher accuracy logits. implement pre-activation version resnet tensorflow. training process closely follows recipe mentioned torch implementation resnet batch size hyper-parameters changed mentioned recipe. teacher network experiment resnet- resnet- resnet- options. student network experiment low-precision variants resnet- resnet- resnet-. low-precision numerics using ternary precision ternary weight network scheme weight tensors quantized per-layer scaling coefﬁcient computed based mean positive terms weight tensor. wrpn scheme quantize weights activations -bits -bits. lower precision ﬁrst layer ﬁnal layer apprentice network. based observation almost prior works lowering precision layers degrades accuracy dramatically. training ﬁne-tuning gradients still maintained full-precision. table top- validation error rate imagenet-k resnet- student network precision activations weight changes. last three columns show error rate student resnet- paired resnet- resnet- resnet-. results resnet- table shows effect lowering precision accuracy resnet- baseline resnet- resnet- resnet- teachers. table denotes precision activation maps denotes precision weights. baseline top- error full-precision resnet- lowering precision without using help teacher network accuracy drops using ternary -bits precision distillation based technique accuracy low-precision conﬁgurations improves signiﬁcantly. fact accuracy fullprecision resnet- also improves paired larger full-precision resnet model best full-precision accuracy achieved student resnet- resnet- teacher full-precision resnet- best achieved ternary weight resnet- accuracy student resnet- model beat baseline accuracy. hypothesize regularization low-precision reason this. improving accuracy beyond baseline ﬁgure seen resnet-. figure shows difference top- error rate achieved best low-precision student networks versus using help teacher network. ﬁgure difference top- error best low-precision student network calculated baseline full-precision network i.e. want close low-precision student network come full-precision baseline model. low-precision network accuracies signiﬁcantly close full-precision accuracy figure difference top- error rate lowprecision variants resnet- without distillation scheme. difference calculated accuracy resnet- full-precision numerics. higher difference denotes better network conﬁguration. hinton mention improving baseline full-precision accuracy student network paired teacher network. mention improving accuracy small model mnist dataset. show efﬁcacy distillation based techniques much bigger model much larger dataset table top- validation error rate imagenet-k resnet- student network precision activations weight changes. last three columns show error rate student resnet- paired resnet- resnet- resnet-. table top- validation error rate imagenet-k resnet- student network precision activations weight changes. ﬁnal columns show error rate student resnet- paired resnet- resnet-. figure difference top- error rate low-precision variants resnet- resnet- without distillation scheme. difference calculated accuracy baseline network resnet- operating full-precision. higher difference denotes better network conﬁguration. results resnet- resnet- table table show effect lowering precision accuracy resnet- resnet- respectively distillation based technique. student resnet- network resnet- resnet- resnet- teachers. student resnet- network resnet- resnet- teachers. top- error full-precision resnet- best -bits weight -bits activation resnet- within number signiﬁcantly improves upon previously reported error rate -bits weight -bits activation resnet- gives model within full-precision model accuracy figure figure show difference top- error achieved best low-precision resnet- resnet- student networks respectively compares results obtained using methods proposed literature. apprentice scheme signiﬁcantly closes full-precision baseline networks low-precision variants networks. cases scheme better previously reported accuracy numbers .%-%. discussion scheme-a teacher network always large larger number parameters student network. experimented ternary resnet- student network paired full-precision resnet-. ternary model resnet- smaller size compared full-precision resnet- model. ﬁnal trained accuracy resnet- ternary model setup worse obtained pairing ternary resnet- network resnet- teacher network. suggests distillation scheme works teacher network higher accuracy student network further beneﬁt using larger teacher network saturates point. seen picking precision point looking error rates along table concern early stages investigation joint training low-precision small network high precision large network inﬂuence small network’s accuracy accuracy large network. using joint cost function smaller network’s probability scores matched predictions teacher network. joint cost added term total loss function posit larger network’s learning capability affected inherent impairment smaller low-precision network. further since smaller student network learns form larger teacher network vicious cycle might form student network’s accuracy drop teacher network’s learning capability impeded. however practice phenomenon occurring case teacher network jointly trained student network accuracy teacher network always within accuracy teacher network without jointly supervising student network. could choice values. section mentioned temperature softmax function hyper-parameters since train directly logits teacher network experiment appropriate value required training soft targets produced teacher network. although extensive studies experimenting training soft targets opposed logits gives best results training soft targets. hinton mention student network signiﬁcantly smaller teacher network small values effective large values. low-precision conﬁgurations experimented conﬁgurations yielded lower performance model compared original choice parameters. third term equation experimented mean-squared error loss function also loss function logits student teacher network improvement accuracy compared original choice cost function formulation. thorough investigation behavior networks values hyper-parameters different loss functions agenda future work. overall distillation process quite effective getting high accuracy low-precision models. low-precision models surpass previously reported low-precision accuracy ﬁgures. example scheme achieves top- error rate resnet- -bits weight. best resnet- model using scheme-a -bits weight achieves error rate improving model accuracy ttq. similarly scheme mellempudi achieves top- error -bits weight -bits activation. best performing apprentice network precision achieves top- error. scheme-b scheme-c describe next scheme-a serves baseline. scheme start trained teacher network. referring back figure input image passed teacher student network except learning back-propagation happens precision student network trained scratch. scheme used buciluˇa hinton training student networks. scheme ﬁrst term equation zeroes last terms equation contribute toward loss function. teacher network. scheme-b might also help scenario student network attempts learn dark knowledge teacher network already trained private sensitive data scheme-a hypothesis student network would inﬂuenced dark knowledge teacher network also path teacher adopts learn knowledge. scheme-b student network gets similar accuracy numbers teacher network albeit fewer number epochs. scheme training accuracies similar reported table low-precision student networks however learn fewer number epochs. figure plots top- error rates conﬁgurations experiment suite. plots student network scheme-b converges around th-th epoch compared epochs schemea. general student networks scheme-b learn fewer epochs student networks trained using scheme-a. scheme-c similar scheme-b except student network primed full precision training weights start training process. beginning training process weights activations lowered student network sort ﬁne-tuned dataset. similar scheme-b ﬁnal terms equation comprise loss function low-precision student network trained back-propagation algorithm. since network starts good initial point comparatively learning rate used throughout training process. clear recipe learning rates works across conﬁgurations. general found training learning rate epochs followed another epochs followed another epochs give best accuracy. conﬁgurations epochs stabilizing. conﬁgurations found training using scheme-b warm startup equally good. found ﬁnal accuracy models obtained using scheme better obtained using scheme-a scheme-b. table shows error rates conﬁgurations lowprecision student network obtained using scheme-a scheme-c. resnet- student network accuracy ternary weights improved compared obtained using scheme-a. note performance ternary networks obtained using schemealready state-of-the-art. hence resnet- ternary networks top- error rate state-of-the-art. this ternary resnet- within baseline accuracy similarly -bits weight -bits activations resnet- model obtained using scheme-c better obtained scheme-a mentioned earlier low-precision form model compression. many works target network sparsiﬁcation pruning techniques compress model. ternary precision models model size reduces factor compared full-precision models. apprentice show performant model ternary precision. many works targeting network pruning sparsiﬁcation target full-precision model implement scheme. comparable model size ternary networks full-precision model needs sparsiﬁed further effective sparse model needs store every non-zero value denoting position value weight tensor. adds storage overhead sparse model needs sparse at-par memory size -bit model. note ternary precision also inherent sparsity ternary models sparse. work sparsiﬁcation full-precision networks proposed sparsity achieved less further network accuracy using techniques works lead larger degradation accuracy compared ternary models. overall believe ternary precision models state-of-the-art accuracy also considers size model accuracy level achieved low-precision sparse networks. present three schemes based knowledge distillation concept improve accuracy lowprecision networks. three schemes improve accuracy low-precision network conﬁguration compared prior proposals. motivate need smaller model size batch real-time resource constrained inference deployment systems. envision lowprecision models produced schemes simplify inference deployment process resource constrained systems cloud-based deployment systems latency critical requirement. cristian buciluˇa rich caruana alexandru niculescu-mizil. model compression. proceedings sigkdd international conference knowledge discovery data mining york acm. isbn ---. http//doi.acm.org/./.. alfredo canziani adam paszke eugenio culurciello. analysis deep neural network models practical applications. corr abs/. http//arxiv.org/ abs/.. wenlin chen james wilson stephen tyree kilian weinberger yixin chen. compressing neural networks hashing trick. corr abs/. http//arxiv. org/abs/.. matthieu courbariaux yoshua bengio. binarynet training deep neural networks weights activations constrained corr abs/. http//arxiv. org/abs/.. matthieu courbariaux yoshua bengio jean-pierre david. binaryconnect training deep neural networks binary weights propagations. corr abs/. http //arxiv.org/abs/.. misha denil babak shakibi laurent dinh marc’aurelio ranzato nando freitas. predicting parameters deep learning. corr abs/. http//arxiv.org/abs/ suyog gupta ankur agrawal kailash gopalakrishnan pritish narayanan. deep learning limited numerical precision. corr abs/. http//arxiv.org/abs/ song huizi william dally. deep compression compressing deep neural network pruning trained quantization huffman coding. corr abs/. http//arxiv.org/abs/.. song jeff pool john tran william dally. learning weights connections advances neural information processing systems annual efﬁcient neural network. conference neural information processing systems december montreal quebec canada song xingyu huizi jing ardavan pedram mark horowitz william dally. efﬁcient inference engine compressed deep neural network. acm/ieee annual international symposium computer architecture isca seoul south korea june ./isca... https//doi.org/. /isca... alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. pereira burges bottou weinberger advances neural information processing systems curran associates inc. yann lecun john denker sara solla. optimal brain damage. touretzky advances neural information processing systems morgan-kaufmann http//papers.nips.cc/paper/-optimal-brain-damage.pdf. daisuke miyashita edward boris murmann. convolutional neural networks using logarithmic data representation. corr abs/. http//arxiv.org/ abs/.. angshuman parashar minsoo anurag mukkara antonio puglielli rangharajan venkatesan brucek khailany joel emer stephen keckler william dally. scnn accelerator compressed-sparse convolutional neural networks. corr abs/. http//arxiv.org/abs/.. mohammad rastegari vicente ordonez joseph redmon farhadi. xnor-net imagenet classiﬁcation using binary convolutional neural networks. corr abs/. http//arxiv.org/abs/.. adriana romero nicolas ballas samira ebrahimi kahou antoine chassang carlo gatta yoshua bengio. fitnets hints thin deep nets. corr abs/. http //arxiv.org/abs/.. olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein alexander berg fei-fei. imagenet large scale visual recognition challenge. international journal computer vision ./s---y. frank seide jasha droppo gang dong -bit stochastic gradient descent application data-parallel distributed training speech dnns. interspeech september yaman umuroglu nicholas fraser giulio gambardella michaela blott philip heng leong magnus jahre kees vissers. finn framework fast scalable binarized neural network inference. corr abs/. http//arxiv.org/abs/. urban geras ebrahimi kahou aslan wang caruana mohamed philipose richardson. deep convolutional nets really need deep convolutional? arxiv e-prints march ganesh venkatesh eriko nurvitadhi debbie marr. accelerating deep convolutional networks using low-precision sparsity. corr abs/. http//arxiv.org/ abs/.. kilian weinberger anirban dasgupta josh attenberg john langford alexander smola. feature hashing large scale multitask learning. corr abs/. http //arxiv.org/abs/.. junho donggyu jihoon junmo kim. gift knowledge distillation fast optimization network minimization transfer learning. ieee conference computer vision pattern recognition july sergey zagoruyko nikos komodakis. paying attention attention improving performance convolutional neural networks attention transfer. corr abs/. http//arxiv.org/abs/.. aojun zhou anbang yiwen yurong chen. incremental network quantization towards lossless cnns low-precision weights. corr abs/. http//arxiv.org/abs/.. shuchang zhou zekun xinyu zhou yuxin yuheng zou. dorefa-net training bitwidth convolutional neural networks bitwidth gradients. corr abs/. http//arxiv.org/abs/.. addition imagenet dataset also experiment apprentice scheme cifar- dataset. cifar- dataset consists training images testing images classes. various depths resnet topology study. implemention resnet cifar- closely follows conﬁguration network inputs images. ﬁrst layer convolutional layer followed stack layers convolutions feature sizes layers feature size. numbers ﬁlters layers. followed global average pooling -way fully connected layer softmax layer. thus total weight layers. figure shows impact lowering precision depth resnet varies. network becomes larger size impact lowering precision diminished. example resnet- full-precision top- error rate depth ternarizing model also gives similar accuracy comparing resnet- full-precision ternary model overall ternarizing model closely follows accuracy baseline full-precision model. however lowering weights activations almost always leads large accuracy degradation. accuracy -bits weight -bits activation network .%-.% worse full-precision model. using apprentice scheme considerably lowered. figure shows impact lowering precision low-precision network paired full-precision network. analysis scheme-a jointly train teacher student network. resnet depths used study resnet resnet- student network paired deeper resnets i.e. resnet- similarly resnet- student network paired deeper resnet- apprentice scheme improve baseline full-precision accuracy. scheme also helps close improved baseline accuracy accuracy lowering precision weights activations. -bits weight -bits activation network .%-.% worse full-precision model.", "year": 2017}