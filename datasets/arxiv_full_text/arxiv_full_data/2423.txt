{"title": "Large-Sample Learning of Bayesian Networks is NP-Hard", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In this paper, we provide new complexity results for algorithms that learn discrete-variable Bayesian networks from data. Our results apply whenever the learning algorithm uses a scoring criterion that favors the simplest model able to represent the generative distribution exactly. Our results therefore hold whenever the learning algorithm uses a consistent scoring criterion and is applied to a sufficiently large dataset. We show that identifying high-scoring structures is hard, even when we are given an independence oracle, an inference oracle, and/or an information oracle. Our negative results also apply to the learning of discrete-variable Bayesian networks in which each node has at most k parents, for all k > 3.", "text": "paper provide results algorithms variable bayesian results rithm uses scoring simplest tive distributio algorithm fore hold ever learning criterion uses consistent large dataset. plied sufficiently show high-scoring identifying np-hard even given inde­ pendence information also apply learning bayesian networks parents paper version ering reduce known np-complete learning construct work defines bayesian single dataset result discouraging hope that scenarios \"well behaved\" learning number records large discrete-variable tial ranking scoring model represent model cannot models represent bution almost scoring tent including rule models apriori length terion. researchers cepted without networks bayesian quently large amount work community dedicated techniques good models. identify number discouraging plexity results indicate ering shows general used) class bayesian highest-scoring hard even node par­ ents. dasgupta shows hard find polytree score. maximum-likelihood though identify tree struc­ ture using polynomial criterion path structure-is tree node variables) solution constant rithm spirtes tify best network structure independence tests. know value limit worst-case independence tests used algorithm. tively respect tain unobserved variables) ordering best structure using polynomial terion. algorithm deletes unfortunately solutions special-case efficient real-world scenarios. general-without erative observables given total ordering-large-sample hard. demonstrate given independence given given given inference information also show results ruc­ apply pruulem iueatifying tures node parents assume conditional tion full table. separate given every distinct variable necessary thus assuming multinomial parent configurations distinct conditional contain assume number parameter values. also states depend number variables pearl provides separation dence constraint reader familiar l.lg denote assertion imposes constraint-via values probability l.lpyiz denote assertion independent respect distribution model independence dependence d-separation hold perfect exists respect paper concentrate variables finite number bayesian-ne twork model variables pair defines joint probability tion directed short-consisting to-one correspondence directed edges connect parameter ability easy exists gorithm optimal large-sample data learn simply learn best model evaluate number parameters learn np-hard conclude optimal learn np-hard using reduction are­ stricted feed­ problem back set. feedback problem garey johnson follows define decision prove learning tion limit large data consistent criteria generative bution among mod­ include rank according ported defined summing distribution defined variables structure instance contains states theses argue reduction easy specify bound polynomial specify nomial time well. parents probability rameters. sented size instance written either lows specification equation denominator thus using straight­ forward represent bits. thus polynomial. entire instance duction. sider sub-graphs spond nodes \"relevant\" instance particular simplify discussion ponent refer sub-graph {v;aijeijcijdijeijfijgij component explicit results configurations contains remain­ arcs a'ry contains edges shown figure contains first argue acyclic. edge component contains rected instance a'ry; corresponding dbfas a'ry contains path directed fore hypothetical corresponding arcs a'ry impossible solution includes parameters ponent regardless easy verify local distributions parameters. corresponding nodes parameters; nodes contribute parameters. dbfas exactly instance parents. exactly actly parent. node states therefore nine states local distributions thus conclude exactly proof given solution tion follows. edge instance path directed corresponding con­ tains edges shown figure otherwise least directed corresponding increase edge component need first cover edge adding least contained \"covering least parameters increase crease). component transformation exist know fact d-connected ditioning contains fact includes requires least parameters contradicting parameter nent smaller tween directed must also contain would exist conditioning ciijj_f e;jifij tradicting edges least component transformation supposition edge component subgraph corresponding clude includes also includes strict edges nodes) corresponding ri£; bution know xl-restricted path corresponds thus corresponding active path theorem know exists edge additions forms another ditions fl'· lemma lemma covered edge reversal ters supported number parameters evaluating increase additions transformed edge component edges fl'· either resulting parameters resulting supports least edge component total parameter increase nodes component subset variables constant shown compute constant time trick always cut-set constant polytrees num­ nodes; containing number nodes time. define cut-set node note given conditioning active node interior node even i.e. non-endpoint time compute non-asymptotic np-hard gardless present ering hard node restricted parents whereas sample learning tion. leads question large-sample parents; np-hard practice ally requires samples resentation alternatively actual approach large-sample number data points problem instance variables importance enough data relative models considered rithm large-sample limit number records compute score candidate reasonable contains variables-ti ables determined polytree computed polytree. contain node states terms above computed finally tions node parents problem", "year": 2012}