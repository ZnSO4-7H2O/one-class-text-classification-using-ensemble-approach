{"title": "An empirical analysis of dropout in piecewise linear networks", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "The recently introduced dropout training criterion for neural networks has been the subject of much attention due to its simplicity and remarkable effectiveness as a regularizer, as well as its interpretation as a training procedure for an exponentially large ensemble of networks that share parameters. In this work we empirically investigate several questions related to the efficacy of dropout, specifically as it concerns networks employing the popular rectified linear activation function. We investigate the quality of the test time weight-scaling inference procedure by evaluating the geometric average exactly in small models, as well as compare the performance of the geometric mean to the arithmetic mean more commonly employed by ensemble techniques. We explore the effect of tied weights on the ensemble interpretation by training ensembles of masked networks without tied weights. Finally, we investigate an alternative criterion based on a biased estimator of the maximum likelihood ensemble gradient.", "text": "recently introduced dropout training criterion neural networks subject much attention simplicity remarkable effectiveness regularizer well interpretation training procedure exponentially large ensemble networks share parameters. work empirically investigate several questions related efﬁcacy dropout speciﬁcally concerns networks employing popular rectiﬁed linear activation function. investigate quality test time weight-scaling inference procedure evaluating geometric average exactly small models well compare performance geometric mean arithmetic mean commonly employed ensemble techniques. explore effect tied weights ensemble interpretation training ensembles masked networks without tied weights. finally investigate alternative criterion based biased estimator maximum likelihood ensemble gradient. dropout recently garnered much attention novel regularization strategy neural networks involving structured masking noise stochastic gradient-based optimization. dropout training viewed form ensemble learning similar bagging ensemble size exponential number hidden units input features members ensemble share subsets parameters. combining predictions enormous ensemble would ordinarily prohibitively expensive scaling weights admits approximate computation geometric mean ensemble predictions. dropout crucial ingredient winning solution several high-proﬁle competitions notably visual object recognition well merck molecular activity challenge adzuna salary prediction competition. also inspired work activation function design well extensions basic dropout technique similar fast approximate model averaging methods several authors recently investigated mechanism dropout achieves regularization effect linear models well linear sigmoidal hidden units however many recent empirical successes dropout feed forward neural networks generally utilised piecewise linear activation functions work empirically study dropout rectiﬁed linear networks employing recently popular hidden unit activation function max. begin expanding upon previous work investigated quality dropout’s approximate ensemble prediction comparing monte carlo estimates correct geometric average here compare true average networks size small enough exact computation tractable. exhaustive enumeration sub-networks small cases weight scaling approximation remarkably somewhat surprisingly accurate surrogate true geometric mean. next consider importance geometric mean itself. traditionally bagged ensembles produce averaged prediction arithmetic mean weight scaling trick employed dropout provides efﬁcient approximation geometric mean. while noted difference bounded immediately obvious effect source error classiﬁcation performance practice. therefore investigate question empirically conclude geometric mean indeed suitable replacement arithmetic mean context dropout-trained ensemble. questions raised thus pertain primarily approximate model averaging performed test time dropout training also raises important questions. update dropout learning rule follows gradient true bagging training would follow. however case traditional bagging members ensemble would independent parameters. case dropout training models share subsets parameters. unclear much coordination serves regularize eventual ensemble. also clear whether important effect dropout performs model averaging dropout encourages individual unit work well variety contexts. investigate question train independent models resamplings training data traditional bagging. ensemble member trained single randomly sampled dropout mask ﬁxed throughout steps training. combine independently trained networks ensembles varying size compare ensembles’ performance single network identical size trained instead dropout. evidence support claim weight sharing taking place context dropout plays important role regularizing ensemble. finally investigate alternative criterion training exponentially large shared-parameter ensemble invoked dropout. rather performing stochastic gradient descent randomly selected sub-network manner similar bagging consider biased estimator gradient geometrically averaged ensemble likelihood particular estimator bearing resemblance boosting criterion employing masking noise exact distribution employed dropout yields discernible robustness gains networks trained ordinary stochastic gradient descent. dropout ensemble learning prediction technique applied deterministic feedforward architectures predict target given input vector architectures contain series hidden layers dropout trains ensemble models consisting models contain subset variables parameters used parameterize family distributions binary mask vector determining variables include model e.g. given input unit hidden unit zero corresponding element presentation training example train different sub-network following gradient different randomly sampled many parameterizations instantiation different sub-networks obtained element-wise multiplication mask dropout training similar bagging related ensemble methods bagging ensemble learning technique models trained different subsets dataset. test time predictions models averaged training stops ensemble starts overﬁt. guarantee individual models trained convergence. fact typically vast majority sub-networks never trained even gradient step. functional form model becomes important comes time ensemble make prediction averaging together sub-networks’ predictions. softmax predictive distribution deﬁned renormalizing geometric mean simply given softmax. also true sigmoid output units special cases softmax. result holds exactly case single layer softmax model non-linearity applied unit previous work dropout applies scheme deep architectures hidden units nonlinearities rectiﬁed linear units method approximation geometric mean. approximation characterized mathematically linear sigmoid networks seems perform especially well practice nonlinear networks piecewise linear activation functions initial investigations employed rectiﬁer networks hidden layers hidden units layer single logistic sigmoid output unit. applied class networks binary classiﬁcation problems derived popular multi-class benchmarks simpliﬁed fashion order allow much simpler architectures effectively solve task well synthetic task design. speciﬁcally chose four binary sub-tasks mnist handwritten digit database training sets consisted occurrences digit classes within ﬁrst examples mnist training occurrences last examples held back validation set. used corresponding occurrences ofﬁcial mnist test evaluating test error. also chose binary sub-tasks covertype dataset machine learning repository speciﬁcally discriminating classes classes task represents different domain ﬁrst datasets neural network approaches nonetheless seen success ﬁnal task synthetic task dimensions inputs domain divided regions equal area diamond corners union outlying triangles. order keep synthetic task moderately challenging training size restricted points sampled uniformly random. additional points sampled validation another test set. unlike rifai train evaluate records class data split advertised original dataset description. makes task much challenging many methods prone overﬁtting. ensemble computation though dropout typically applied input layer inclusion probabilities higher employed krizhevsky making necessary unevenly weight terms average. chose hyperparameters random search learning rate momentum well mini-batch size. performed early stopping validation terminating lower validation error observed epochs; training dropout ﬁgure merit early stopping validation error using weight-scaled predictions. srivastava goodfellow previously investigated ﬁdelity weight scaling approximation context rectiﬁer networks maxout networks respectively monte carlo approximation true model average. concerning small networks exhaustive enumeration possible able avoid effect additional variance monte-carlo average compute exact geometric mean possible dropout sub-networks. tasks randomly sampled sets hyperparameters trained networks dropout. computed point test task activities network corresponding possible dropout masks. geometrically averaged predictions computed geometric average prediction point test set. finally compared misclassiﬁcation rate using predictions obtained using approximate weight-scaled predictions. figure comparison test error obtained exhaustive computation geometric mean relative difference test error obtained weight-scaling approximation. results shown figure point represents different hyperparameter conﬁguration. overall result approximation yields network performs similarly. order make differences visible plot y-axis relative difference test error true geometric average network weight-scaled approximation different networks achieving different values test error. additionally statistically tested ﬁdelity approximation wilcoxon signed-rank test nonparametric paired sample test similar paired t-test applying bonferroni correction multiple hypotheses. signiﬁcant differences observed seven tasks. though inexpensive computation approximate geometric mean noted little said choice geometric mean. ensemble methods literature often employ arithmetic mean model averaging. thus natural pose question whether choice geometric mean impact generalization capabilities ensemble. using networks trained section combined forward-propagated predictions models using arithmetic mean. figure plot relative difference test error arithmetic mean predictions. across seven tasks geometric mean reasonable proxy arithmetic mean relative error rarely exceeding except synthetic task. absolute terms discrepancy test error achieved geometric mean arithmetic mean never exceeded tasks. turn investigation characteristics inference dropout-trained networks investigation training procedure. remainder experiments trained networks realistic size capacity full multiclass mnist problem. again employed layers rectiﬁed linear units. addition dropout utilised norm constraint regularization incoming weights hidden unit. performed random search hyperparameter values including search initial ranges weights number hidden units layers maximum weight vector norms layer. dropout training viewed performing bagging ensemble size exponential number hidden units member ensemble shares parameters members ensemble. gradient step taken different mini-batch training data sub-network seen trained different resampling training traditional bagging. furthermore step taken respect likelihood single ensemble member effect weight update applied members ensemble simultaneously investigate role complex weight-sharing scheme training ensemble independent networks resamplings training data single dropout mask ﬁxed place throughout training. ﬁrst performed hyperparameter search sampling hyperparameter conﬁgurations choosing network lowest validation error. best networks obtains test error matching results reported srivastava using hyperparameters trained models initialized different random seeds different resamplings training traditional bagging. instead applying dropout training sampled dropout mask model held ﬁxed throughout training test time. resulting networks thus architectures sampled distribution sub-networks trained dropout training network’s parameters independent networks. evaluate test error ensembles networks combining predictions geometric mean approximately done context dropout. results various sizes ensemble shown figure results suggest indeed effect; combining independently trained figure average test error mnist varying sizes untied-weight ensembles. networks trained convergence single randomly sampled dropout mask ﬁxed place throughout. networks pre-softmax activations averaged produce predictions varying sizes ensembles. size disjoint subsets combined fashion test error mean standard deviation ensembles shown here. confounding factor non-architectural hyperparameters selected context performance using dropout used as-is train networks untied weights; although early-stopped independently remains unclear efﬁciently optimize hyperparameters individual members large ensemble facilitate fairer comparison algorithms denoising autoencoders motivated idea models trained noise robust slight transformations inputs. previous work drawn connections noise regularization penalties similar connections case dropout recently noted natural question whether dropout wholly characterized terms learned noise robustness whether model-averaging perspective necessary fruitful. order investigate question propose algorithm injects exactly noise dropout. test effective require algorithm successfully minimize training error obtain acceptable generalization performance. needs perform least well standard maximum likelihood; otherwise done designed pathological algorithm fails train. therefore introduce dropout boosting. objective function pair dropout boosting likelihood data according ensemble; however parameters current sub-network updated example. ordinary dropout performs bagging maximizing likelihood correct target current example current sub-network whereas dropout boosting takes account contributions sub-networks manner reminiscent boosting. boosting learning rule select model update parameters given models. conventional boosting models already trained convergence. dropout boosting models actually share parameters network trained given step initially models trained all. learning rule select sub-network indexed follow ensemble gradient pensemble i.e. rather using boosting-like algorithm could obtain generic monte-carlo procedure maximizing likelihood ensemble averaging together gradient multiple values optionally using different term left term right. empirically obtained best results special case boosting term left uses term right terms gradient apply updates member ensemble even though criterion optimized global. note intractable pensemble still appears learning rule. implement training algorithm efﬁciently approximate ensemble predictions using weight scaling approximation. introduces bias estimator ﬁndings section suggest approximation error small. note dropout boosting employs exactly noise regular dropout uses perform bagging thus perform similarly conventional dropout learned noise robustness important ingredient. instead take view large ensemble complex learners whose likelihood jointly optimized would expect employing criterion similar boosting bagging would perform poorly. boosting maximizes likelihood ensemble would perhaps prone overﬁtting setting ensemble large learners particularly weak. starting models trained section employed hyperparameters train matched networks dropout boosting another plain stochastic gradient descent. figure plot relative performance dropout dropout boosting compared model hyperparameters trained sgd. dropout unsurprisingly shows consistent edge dropout boosting performs average little better stochastic gradient descent. wilcoxon signed-rank test similarly failed signiﬁcant difference dropout boosting several outliers approach good performance dropout boosting average better often slightly worse maximum likelihood training stark contrast dropout’s systematic advantage generalization performance. investigated several questions related efﬁcacy dropout focusing speciﬁc case popular rectiﬁed linear nonlinearity hidden units. showed weight-scaling approximation remarkably accurate proxy usually intractable geometric mean possible sub-networks geometric mean compares favourably traditionally popular arithmetic mean terms classiﬁcation performance. demonstrated weight-sharing members implicit dropout ensemble appears signiﬁcant regularization effect comparing analogously trained ensembles form share parameters. finally demonstrated simply adding noise even noise identical characteristics noise applied dropout training sufﬁcient obtain beneﬁts dropout introducing dropout boosting training procedure utilising masking noise conventional dropout successfully trains networks loses dropout’s beneﬁts instead performing roughly well ordinary stochastic gradient descent. results suggest dropout extremely effective ensemble learning method paired clever approximate inference scheme remarkably accurate case rectiﬁed linear networks. research necessary shed light model averaging interpretation dropout. hinton noted dropout forces hidden unit perform computation useful wide variety contexts. results sizeable ensemble independent bagged models seem lend support view though experiments limited ensembles several hundred networks most tiny comparison weight-sharing ensemble invoked dropout. relative importance astronomically large ensemble versus learned mixability hidden units remains open question. another interesting direction involves methods able efﬁciently approximately average different classes model share parameters manner rather merely averaging members model class. authors would like acknowledge efforts many developers theano pylearn utilised experiments. would also like thank nserc compute canada calcul qu´ebec providing computational resources. goodfellow supported google fellowship deep learning. bastien lamblin pascanu bergstra goodfellow bergeron bouchard bengio theano features speed improvements. deep learning unsupervised feature learning nips workshop. bergstra breuleux bastien lamblin pascanu desjardins turian warde-farley bengio theano math expression compiler. proceedings python scientiﬁc computing conference oral presentation. goodfellow warde-farley lamblin dumoulin mirza pascanu bergstra bastien bengio pylearn machine learning research library. arxiv preprint arxiv.. hinton srivastava krizhevsky sutskever salakhutdinv improving neural jarrett kavukcuoglu ranzato lecun best multi-stage architecture object recognition? proc. international conference computer vision pages ieee. schapire strength weak learnability. machine learning srivastava improving neural networks dropout. master’s thesis toronto. vincent larochelle lajoie bengio manzagol p.-a. stacked denoising autoencoders learning useful representations deep network local denoising criterion. journal machine learning research", "year": 2013}