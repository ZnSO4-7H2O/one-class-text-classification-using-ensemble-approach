{"title": "Adversarial Vulnerability of Neural Networks Increases With Input  Dimension", "tag": ["stat.ML", "cs.CV", "cs.LG", "68T45", "I.2.6"], "abstract": "Over the past four years, neural networks have proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when seen as a function of the inputs. For most current network architectures, we prove that the $\\ell_1$-norm of these gradients grows as the square root of the input-size. These nets therefore become increasingly vulnerable with growing image size. Over the course of our analysis we rediscover and generalize double-backpropagation, a technique that penalizes large gradients in the loss surface to reduce adversarial vulnerability and increase generalization performance. We show that this regularization-scheme is equivalent at first order to training with adversarial noise. Finally, we demonstrate that replacing strided by average-pooling layers decreases adversarial vulnerability. Our proofs rely on the network's weight-distribution at initialization, but extensive experiments confirm their conclusions after training.", "text": "past four years neural networks proven vulnerable adversarial images targeted imperceptible image perturbations lead drastically different predictions. show adversarial vulnerability increases gradients training objective seen function inputs. current network architectures prove -norm gradients grows square root input-size. nets therefore become increasingly vulnerable growing image size. course analysis rediscover generalize doublebackpropagation technique penalizes large gradients loss surface reduce adversarial vulnerability increase generalization performance. show regularization-scheme equivalent ﬁrst order training adversarial noise. finally demonstrate replacing strided average-pooling layers decreases adversarial vulnerability. proofs rely network’s weight-distribution initialization extensive experiments conﬁrm conclusions training. since trendsetting paper goodfellow convolutional neural networks found utterly vulnerable adversarial examples adversary drive performance state-of-the cnns chance-level imperceptible changes inputs. number studies tried address issue stressed that adversarial examples essentially small input changes create large output variations inherently caused large gradients neural network respect inputs. notable exceptions hein andriushchenko cisse offer adversarial robustness guarantees facebook research paris/new york empirical inference department planck institute intelligent systems t¨ubingen germany. correspondence carl-johann simongabriel <cjsimontue.mpg.de>. contributions. spirit present theoretical arguments empirical one-to-one relationship gradient norm training objective adversarial vulnerability. evaluating norms based weight-statistics initialization show that design cnns exhibit increasingly large gradients input dimension leaves vulnerable adversarial noise. vulnerability dampened average-pooling layers strided max-pooling ones. based link large loss-gradients adversarial vulnerability propose regularizer explicitely penalizes norm gradients technique already proposed drucker lecun name double-backpropagation. tikhonov regularization equivalent training random noise show proposed regularization-scheme equivalent frist order training adversarial noise proposed goodfellow conﬁrm theoretical results extensive experiments. overall ﬁndings call line research design neural network architectures inherently smaller gradients pioneered cisse results thereby serve guidelines practitioners network-designers. suppose given classiﬁer classiﬁes image category deﬁnition adversarial image small modiﬁcation barely noticeable human sufﬁces fool classiﬁer predicting class different small perturbation inputs creates large variation outputs. adversarial examples thus seem inherently related large gradients network. connection clarify. adversarial vulnerability variations. practice adversarial image constructed adding perturbation original image number given norm input space. call perturbed input \u0001-sized ·-attack attack successfull motivates coordinates ∂xl. second remark taylor expansion actually dominated higher-order terms. nevertheless ﬁrst-order term shows loss surface large gradients vulnerable adversarial attacks. explains importance lemma ﬁrst order approximation \u0001-sized adversarial attack generated norm increases loss point \u0001|||∂xl||| |||·||| dual norm consequently evaluate adversarial vulnerability trained classiﬁer \u0001-sized ·-attacks ex|||∂xl|||. natural take loss-after-attack training objective. introduced factor reasons become clear moment. incidentally loss reduces regularizationscheme proposed drucker lecun called double-backpropagation. time authors argued slightly decreasing function’s classiﬁer’s sensitivity input perturbations improve generalization. sense exactly motivation defending adversarial examples. thus surprising regularization term. note reasoning shows training speciﬁc norm |||·||| equation helps protect adversarial examples generated priori know happen attacks generated another norm; experiments suggest training norm also protects attacks link adversarially-augmented training. equation designates attack-size threshold regularization-strength. rather notational conﬂict reﬂects intrinsic duality complementary interpretations investigate further. suppose that instead using loss-after-attack augment training \u0001-sized ·-attacks training point perturbation generated locally maximize loss-increase. effectively training classiﬁer classiﬁes test images drawn accuracy adversarial vulnerability exactly average increase induced zero-one-loss adversarial perturbation test inputs. here designates true label shall consider induced losses agree simply call ‘the loss’ write instance equality average loss increase adversarial vulnerability approximate classiﬁer accuracy switches wrong classes ignored switchees wrong true classes compensate switches direction. practice however practitionner interested accuracy-drop adversarial attacks trained accurate classiﬁer exact count class-switches. actually call average relevant adversarial vulnerability remember that rising accuracy increasingly good approximation total adversarial vulnerability. practice usually train classiﬁers zero-one loss. replace smoother loss consider good enough approximation example cross-entropy -loss. agree good enough approximation also ought agree variations accurately reﬂect hence average faithfully approximates relevant adversarial vulnerability. consequently trained classiﬁer robust adversarial examples average small adversarial perturbation creates small variation loss. using ﬁrst order taylor expansion shows denotes gradient respect last equality almost deﬁnition dual norm |||·||| remarks. first dual norm kicks input noise optimally adcoordinates within \u0001-constraint. brandmark adversarial noise different coordinates instead statiscally canceling would random noise. example impose stricly align ∂xl. instead align sign ﬁrst introduced goodfellow name fgsm-augmented training. using ﬁrst order taylor expansion equation ’oldplus-post-attack’ loss equation simply reduces loss-after-attack. proposition ﬁrst-order approximations small enough adversarially-augmented training \u0001-sized ·-attacks amounts penalizing dual norm |||·||| weight particular double-backpropagation corresponds training -attacks fgsm-augmented training corresponds -penalty ∂xl. correspondance training preturbations using regularizer compared tikhonov regularization tikhonov regularization amounts training random noise training adversarial noise amounts penalizing ∂xl. calibrating threshold attack-norm gounit perturbation. ex|||∂xl||| hence intuitively captures discrepancy perception classiﬁer’s perception input-perturbation small size course viewpoint supposes actually found norm faithfully reﬂects human perception project right beyond scope paper. however clear threshold choose depend norm hence input-dimension particular given pixel-wise order magnitude perturbations p-norm perturbation scale like d/p. suggests write threshold used p-attacks denotes dimension-independent constant. appendix show scaling also preserves average signal-to-noise ratio accross norms dimensions could correspond constant human perception-threshold. mind turn section evaluate size ∂xlq standard neural network architectures. analyse fully-connected networks followed general theorem particular encompasses cnns without strided convolutions. ﬁnish showing that contrary strided layers average-poolings efﬁciently decrease norm ∂xl. start analysis showing changing affects size ∂xlq. suppose moment coordinates typical magnitude |∂xl|. ∂xlq scales like d/q|∂xl|. consequently equation carries important messages. first ∂xlq depends dependance seems highest account varying perceptibility threshold adversarial vulnerability scales like d·|∂xl| whatever p-norm use. second equation shows robust type p-attack input-dimension average absolute value coefﬁcients must grow slower catch brings core insight. order preserve activation variance neurons layer layer neural weights usually initialized variance inversely proportional number inputs neuron. imagine moment network consisted output neuron linearly connected input pixels. purpose example assimilate initialize weights variance average absolute value |∂xo| |∂xl| grows like rather required equation adversarial vulnerability \u0001∂xoq \u0001∂xlq therefore increases like d/√d example shows standard initialization scheme preserves variance layer layer causes average coordinate-size |∂xl| grow like instead ∞-attack tweaks \u0001-sized inputperturbations align coordinate-signs coordinates absolute value resulting output-perturbation scales like leaves network increasingly vulnerable growing input-dimension. next theorems generalize previous example wide class feedforward nets relu activation functions. illustrational purposes start fully connected nets proceed broader step statistical properties ∂xfk. paths input neuron output-logit successive neurons path path without input neuron. designate weight finally equal relu node active input otherwise. ﬁrst equality uses decouple expectations weights relus applies lemma appendix uses kill cross-terms take expectation weights inside product. second equality uses fact resulting product active paths. third equality counts number paths conclude noting terms cancel except input layer class includes succession convolutional and/or strided layers. essence proofs iterate insight layer sequence layers. rely following hypotheses relu non-input neuron here weights considered layer end-nodes belong layer. follow common practice initialize nets proposed satisﬁed initialization design usually good approximation training however cannot expect hypotheses hold. necessary enforce adding bathchnorms penalties layerwise weight averages variances absurd believe keep weights uncorrelated implied h-h. statements section understood orders magnitudes well satisﬁed initialization theory practice conﬁrm experimentally training section said differently theorems rely statistics neural nets initialization experiments conﬁrm conclusions training. theorem consider succession fully connected layers relu activations takes inputs dimension satisﬁes assumptions outputs logits ﬁnal cross-entropy-loss layer coordinates ∂xfk grow like convolutions stride plus average-pooling layer. average-pooling introduces deterministic weights size theorem assumes random weights apply anymore. thus wonder replacement help protecting adversarial examples? following theorem says does. theorem consider succession possibly strided convolution dense layers relu activations satisfying assumptions replacing strided convolution selects neuron convolution stride followed average-pooling neurons divides output loss gradients proof appendix previous statements suggest replace strided convolution nonstrided counterpart followed average-pooling layer. furthermore theorem shows systematically reduce number pixels channel combining convolutional average-pooling layers adversarial vulnerability becomes independent input-resolution provided assumptions stay valid training. section ﬁrst empirically verify average -norm adversarial vulnerability grow like input dimension predicted corollary compare loss-gradient regularization methods adversarially-augmented training ﬁnish verifying increased effectiveness average-pooling strided layers layers decrease adversarial vulnerability experiments consider adversarial vulnerability ∞-attacks approximate using fgsmimplementation python foolbox-package test-attacks image datasets globally normalized images perturbation imperceptible. \u0001-threshold confused regularization-strengths appearing varied experiments. theorems corollary predict linear growth average -norm square root input dimension therefore also adversarial vulnerability test predictions created class dataset approximately -sized rgb-images merging similar imagenet-classes resizing smallest image-edge pixels center-cropping result. downsized images pixels edge trained cnns downsized datasets. computed adversarial decrease ordinary classiﬁcation error still valid strategy adversarial examples. reﬂects fact increasing classiﬁcation margin larger gradients classiﬁer’s logits needed push images side classication boundary other. conﬁrmed theorem hein andriushchenko also equation appendix turn general theorem covers much wider class nets implies theorem following symmetry assumption neural connections. given path path-degree multiset encountered in-degrees along path fully connected network unordered sequence layer-sizes preceding last path-node including input-layer. consider multiset {dp}p∈p path-degrees varies among paths input output symmetry assumption input nodes multiset {dp}p∈p intuitively means statistics degrees encountered along paths output input nodes. symmetry assumption exactly satiﬁed fully connected nets almost satisﬁed cnns exactly satisﬁed strided layers layersize multiple stride. theorem consider non-recurrent neural network relu activation functions satisﬁes assumptions outputs logits cross-entropy-loss ∂xfk independent input dimension moreover satiﬁes symmetry assumption coordinates ∂xfk scale like equation still holds ∂xlq proof appendix corollary succession convolution dense layers strided relu activations satisﬁes assumptions outputs logits cross-entropy-loss logitcoordinates scale like satisﬁes equation particular increasingly vulnerable growing input-resolution attacks generated p-norm. common practice cnns average-pooling layers strided convolutions progressively decrease number pixels channel. corollary shows using strided convolutions protect adversarial examples. however replace strided convolutions vulnerability average network held-out test-dataset. figure summarizes results. dashed-line follows median group networks; errorbars show quantiles. predicted theorems adversarial vulnerability grow almost linearly growth \u0001|||∂xl||| means large outside linear regime lemma indeed occurs largest resolution experiments crossentropy loss ranges meanwhile ex∂xl leads \u0001∂xl even bigger .-range therefore beyond linear approximation validity. networks exactly amount parameters similar structure accross various input-resolutions. cnns succession ‘convolution batchnorm relu’ layers output channels followed ﬁnal full-connection logit-outputs. used -max-poolings layers ﬁnal maxpooling layer neuron channel fully-connected layer. ensure convolutionkernels cover similar ranges images accross input-resolutions respectively dilated convolutions factor figure adversarial vulnerability ex∂xl increase linearly square-root imageresolution predicted corollary here adversarial vulnerability gets dampened higher dimension ﬁrst-order approximation made equation becomes less less valid. section check correspondance gradient-regularization adversarially-augmented training compare methods another gradient-regularizer proposed hein andriushchenko cross-lipschitz regularizer. train several cnns architecture classify cifar- images speciﬁc training method speciﬁc regularization value training methods used -penalization adversarial augmentation ∞attacks cross-lipschitz regularizer network ‘convolution batchnorm relu’ layers output-channels each followed average-pooling feeds neuron channel ﬁnal fully-connected linear layer. results summarized figure point represents trained speciﬁc adversarial training method speciﬁc curve groups points training method. illustration proposition upper figure plots ex∂xl adversarial vulnerability accuracy function d/p. excellent match adversarial augmentation curve gradient-regularization dual counterpart illustrates duality threshold adversarially-augmented training regularization constant regularized loss conﬁrmation equation still upper curves reason match plotted threshold relative speciﬁc attack-norm. however equation suggested rescaled thresholds \u0001d/p approximately correspond ‘threshold-unit’ accross p-norms accross dimension. well conﬁrmed upper plots rescaling x-axis curves almost super-imposed. ex∂xl measures adversarial vulnerability. figure ex∂xl independent training regularization method used. strikingly clear conﬁrmation adversarial examples primarily caused large gradients classiﬁer captured induced loss. adversarial regularization improves generalization. adversarial vulnerability steadily decreases growing figure shows that whatever adversarial method used accuracy attack held-out test ﬁrst increases decreasing adversarial vulnerability thus improves generalization recall original motivation drucker lecun introduce doublebackpropagation. accuracy-vs-vulnerability trade-off. concentrate accuracy versus vulnerability trade-offs figure merges figures taking irrelevant parameter following curves right left corresponds implicitely increasing long horizontal plateaus conﬁrm adversarial vulnerability massively driven without losing generalization performance. methods perform equally well small enought \u0001-values long best accuracy-tovulnerability ratios obtained traditional adversarially augmented training using fgsm figure average norm ex∂xl loss-gradients adversarial vulnerability accuracy various networks trained different adversarial regularization methods regularization strengths point represents trained network curve training-method. upper accuracy ﬁrst improves rising average gradient norms adversarial vulnerability steadily decrease. priori regularization parameter different meanings method. relatively good superposition curves upper-row plot illustrates dual-correspondance adversarially-augmented training gradient-loss penalization conﬁrms rescaling proposed equation adversarial vulnerability almost function average loss-gradient norm depend training method used. conﬁrms large gradients loss main cause adversarial vulnerability. ﬁgure concentrates accuracy-vs-vulnerability trade-offs taking figures methods ﬁrst perform similarly small best ratios eventually obtained adversarial-augmentation methods almost perfect linear relation -norms explain striking similarity curves figure suggests protecting given attack-norm also protects others. keeping inside function equation ˜l\u0001p also account higher order taylor-variation original loss might explain methods perform better higher ﬁrst-order counter-parts penalty-norm matter. surprised figures curves alidentical indicates norms used interchangeably suggests protecting agains speciﬁc attack-norm also protects others. equation provide explanation coordinates behave like centered uncorrelated variables equal variance –which follows assumptions -norms simply proportional. plotting ex∂xl ex∂xl figure conﬁrms explanation. slope independent training method. therefore penalizing during training decrease ex∂xl also drive ex∂xl vice-versa. theorems show that contrary strided layers averagepoolings decrease adversarial vulnerability. tested hypothesis cnns trained cifar- blocks ‘convolution batchnorm →relu’ output-channels followed ﬁnal average pooling feeding neuron channel last fully-connected linear layer. additionally every second convolution placed pooling layer stride tested average-pooling strided max-pooling layers trained networks architecture. results shown figure predicted networks average pooling layers much robust adversarial images others. although accuracies close slightly better average pooling striding slightly worse max-pooling. proposed rifai used regularize encoder-features. seeing adversarial training generalization method also mention hochreiter schmidhuber propose enhance generalization searching parameters minimum region loss. leads penalty involving gradient loss taken respect weights rather inputs. vein gradient-regularization loss generative models also appears proposition ollivier stems codelength bound data finally cisse propose network-architectures small gradients design rather special training approach makes sense considering conclusion theorems details references adversarial attacks defenses refer yuan ﬁrst showed adversarial vulnerability increases gradients loss conﬁrmed near-perfect functional relationship gradient norms vulnerability evaluated size ∂xlq showed usual convolutional fully connected architectures vulnerable p-attacks growing input dimension using strided convolutions help using sufﬁciently many average-poolings signiﬁcantly robustify network. results rely statistical weightdistribution initialization experiments conﬁrm conclusions even training. also proposed regularize training penalizing -)norm ∂xl. like tikhonov-regularization approximately equivalent training random noise showed penalization equivalent training adversarial noise theoretically ﬁrst order empirically thereby linked double-backpropagation fgsm-augmented training. however even combining tricks networks remain surprisingly vulnerable adversarial attacks suggests long need design network architectures inherently resolution-invariant.", "year": 2018}