{"title": "Fast, simple and accurate handwritten digit classification by training  shallow neural network classifiers with the 'extreme learning machine'  algorithm", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "Recent advances in training deep (multi-layer) architectures have inspired a renaissance in neural network use. For example, deep convolutional networks are becoming the default option for difficult tasks on large datasets, such as image and speech recognition. However, here we show that error rates below 1% on the MNIST handwritten digit benchmark can be replicated with shallow non-convolutional neural networks. This is achieved by training such networks using the 'Extreme Learning Machine' (ELM) approach, which also enables a very rapid training time (~10 minutes). Adding distortions, as is common practise for MNIST, reduces error rates even further. Our methods are also shown to be capable of achieving less than 5.5% error rates on the NORB image database. To achieve these results, we introduce several enhancements to the standard ELM algorithm, which individually and in combination can significantly improve performance. The main innovation is to ensure each hidden-unit operates only on a randomly sized and positioned patch of each image. This form of random `receptive field' sampling of the input ensures the input weight matrix is sparse, with about 90% of weights equal to zero. Furthermore, combining our methods with a small number of iterations of a single-batch backpropagation method can significantly reduce the number of hidden-units required to achieve a particular performance. Our close to state-of-the-art results for MNIST and NORB suggest that the ease of use and accuracy of the ELM algorithm for designing a single-hidden-layer neural network classifier should cause it to be given greater consideration either as a standalone method for simpler problems, or as the final classification stage in deep neural networks applied to more difficult problems.", "text": "recent advances training deep architectures inspired renaissance neural network use. example deep convolutional networks becoming default option diﬃcult tasks large datasets image speech recognition. however show error rates mnist handwritten digit benchmark replicated shallow nonconvolutional neural networks. achieved training networks using ‘extreme adding distortions common practise mnist reduces error rates even further. methods also shown capable achieving less error rates norb image database. achieve results introduce several enhancements standard algorithm individually combination signiﬁcantly improve performance. main innovation ensure hidden-unit operates randomly sized positioned patch image. form random ‘receptive ﬁeld’ sampling input ensures input weight matrix sparse weights equal zero. furthermore combining methods small number iterations single-batch backpropagation method signiﬁcantly reduce number hiddenunits required achieve particular performance. close state-of-the-art results mnist norb suggest ease accuracy algorithm designing singlehidden-layer neural network classiﬁer cause given greater consideration either standalone method simpler problems ﬁnal classiﬁcation stage deep neural networks applied diﬃcult problems. current renaissance ﬁeld neural networks direct result success various types deep network tackling diﬃcult classiﬁcation regression problems large datasets. said initiated development convolutional neural networks lecun colleagues late given enormous impetus work hinton colleagues deep belief networks last decade would reasonable deep networks considered default option machine learning large datasets. initial excitement methods triggered success mnist handwritten digit recognition problem several years standard benchmark problem hard large dataset machine learning. high accuracy mnist regarded basic requirement credibility classiﬁcation algorithm. methods notable ﬁrst published posting best results respective time mnist problem. images size greyscale pixels standard training images standard test images numerous papers report results algorithms applied test images e.g. report introduce variations extreme learning machine algorithm report performance mnist test set. results equivalent superior original results achieved problem achieved signiﬁcantly lower network training complexity. poses important question whether training algorithm popular choice type problem commonplace algorithm ﬁrst step machine learning. table summarises results shows comparison points results obtained methods past results surpass results using earlier deep networks recent regularisation methods drop connect stochastic pooling dropout so-called ‘deeply supervised networks’ enabled deep convolutional networks state-of-the-art performance mnist case data-augmentation used. nevertheless best result much simpler single-hiddenlayer neural network classiﬁer trained using fast algorithm without using data augmentation extreme learning machine training algorithm relevant single hidden layer feedforward network similar standard neural network. however three departures conventional slfns. hidden layer frequently much larger neural network trained using backpropagation; weights input hidden layer neurons randomly initialised ﬁxed thereafter output neurons linear rather sigmoidal response allowing output weights solved least squares regression. attributes also combined learning systems several times previously standard algorithm provide good results machine learning problems requiring classiﬁcation regression paper demonstrate provides accuracy mnist problem superior prior reported results similarly-sized slfn networks begin introducing three parameters deﬁne dimensions used -category classiﬁer dimension input vectors number hidden layer units number distinct labels training samples. case classifying test vectors convenient deﬁne following matrices projections input vectors inputs hidden-units. bias hidden unit added expanding size input dimension setting additional input element always unity training test data bias values included additional column win. shorthand notation fact element dtest nonlinearly converted term-by-term corresponding element atest. example hidden unit response given logistic sigmoid function many nonlinear activation functions equally eﬀective rectiﬁed linear unit function absolute value function quadratic function. standard artiﬁcial neural networks utility nonlinearity introduces hidden-unit responses represent correlations ‘interactions’ input elements rather simple linear combinations them. describe training algorithm. introduce denote number training vectors available. convenient introduce following matrices relevant training xtrain size numerically represents labels class training vector; convenient deﬁne mathematically column single entries zero. entry column occurs corresponding label class training vector. however number unknown variables wout number equations although exact solution potentially exists usually case system overcomplete. usual approach then seek solution minimises mean square error between ylabel ytrain. standard least squares regression problem exact solution wout ylabela also useful regularise problem reduce overﬁtting ensuring weights wout become large. standard ridge-regression approach produces following closed form solution output weights simple random weights typically produce products distinct rows close zero albeit exactly zero product always much larger zero. addition beneﬁcial normalise length occurs orthogonal case. contrast also weights that rather selected random distribution instead chosen well matched statistics data hope improve generalisation classiﬁer. ideally want learn weights rather form weights simple function data. focus primarily improving performance algorithm biasing selection input layer weights diﬀerent ways several recently introduced literature several novel paper. methods follows select input layer weights random biased using training data samples product weights training data samples likely large. called computed input weights restrict weights hidden layer neuron non-zero small random rectangular patch input visual ﬁeld; call receptive field although believe method approaches inspired machine learning approaches mimic cortical neurons limited visual receptive ﬁelds convolutional neural networks application backpropagation method method highlights performance enhanced adjusting input layer weights simultaneously based training data. output layer weights maintained least-squares optimal state recalculating input layer backpropagation updates. process backpropagation updating input weights followed standard recalculation output weights repeated iteratively convergence. rf-elm combination ciw-elm celm two-layer reported ﬁrst time. demonstrate methods independently improves performance basic algorithm combination produce results equivalent many deep networks mnist problem first however describe method detail. ciw-elm approach motivated considering standard backpropagation algorithm feature weight-learning algorithms operate adding weights proportion training samples linear diﬀerence training samples. words apart possible random initialization weights constrained take ﬁnal values drawn space deﬁned terms linear combinations input training data basis vectors—see fig. argued withreason strength thusly constrained basis constraint input weights bias network towards conventional solution. divide hidden layer neurons blocks output classes; data sets number training data samples class equal block size denote number training samples class training data sets class equal size block size adjusted proportional data size. recently published method constraining input weights difference vectors between-class samples. diﬀerence vectors between-class samples vectors connecting samples class samples diﬀerent class sample space—see fig. addition methodology proposed eliminating vectors potentially overlapping spaces reducing near-parallel vectors order uniformly sample weight space. found data-blind manipulation input weights improves generalization performance. approach added bonus input weight matrix sparse high percentage zero entries could advantageous rf-elm approach inspired neurobiology strongly resembles machine learning approaches biological sensory neurons tend tuned preferred receptive ﬁelds receive input subset overall input space. region responsiveness tends contiguous pertinent dimension space visual touch systems frequency auditory system. interestingly contiguity aspect lost beyond earliest neural layers features combined randomly. order loosely mimic organisation biological sensory systems paper consider image classiﬁcation tasks hidden unit create randomly positioned sized rectangular masks smaller overall image. masks ensure small subset length-l input data vectors inﬂuence given hidden unit—see fig. flatten receptive ﬁeld matrix length vector entry corresponds pixel entry data vectors xtest xtrain. concatenate resulting vectors receptive ﬁeld matrix size additionally found beneﬁcial exclude pixels mask training images identical values regions. mnist database typically means ensuring masks exclude ﬁrst last rows ﬁrst last columns. mnist found reasonable value minimum three approaches described provide weightings pixels hidden layer unit. ciw-elm c-elm weight pixels bias hidden-units towards larger response training data speciﬁc class. sparse weightings provided rf-elm bias hidden-units respond pixels speciﬁc parts image. found enhanced classiﬁcation performance achieved combining shaped weights obtained either ciw-elm c-elm receptive ﬁeld masks provided rf-elm. algorithm either rf-ciw-elm rf-c-elm follows. found results obtained rf-c-elm rf-ciw-elm similar terms error percentage applied mnist benchmark errors follow diﬀerent patterns. such combination methods seemed oﬀer promise. combined methods using multiple-layer consists rf-c-elm network rf-ciw-elm network parallel ﬁrst layers. outputs networks combined using further network thought elmautoencoder albeit twenty input neurons output neurons; input neurons eﬀectively sets labels. structure shown fig. input networks ﬁrst trained completion usual autoencoder layer trained using outputs input networks input correct labels outputs. result second-layer network quick implement signiﬁcantly better input networks note middlemost layer shown fig. consists linear neurons therefore removed combining input output weights connecting weight matrix. however computationally disadvantageous number multiplications increase. variations described report hidden layer weights updated trained according output error output layer weights solved using least squares regression. considerably reduces trainability network number free parameters restricted output layer argued network total number weights output layer less number training points likely enhanced using backpropagation train weights previous layers hence following example deep networks speciﬁc versions backpropagation experimented using backpropagation ﬁne-tune hidden layer weights. re-introduce possibility overﬁtting well-understood problem neural networks usual methods avoiding apply here. simplicity batch mode backpropagation implemented using following algorithm. note eqn. assume logistic activation function hidden layer neurons derivative plus row-normalisation. following normalisation rows input weight matrix unity multiplied values entire input weight matrix factor seven methods scaling found close optimal cases. results shown fig. obtain indication variance resulting randomness inherent input-weight shaping method trained elms using method plotted ensemble mean function hidden-layer size also plotted actual error rates trained network. seen fig. error rate decreases approximately log-linearly small slowing approaches fig. shows error rate actual training data used. since best test results occur error rate training data smaller found signiﬁcant test error improvement larger conclude increasing shown produces ﬁtting. veriﬁed cross-validation training data. also trained networks using methods described previous section plus iterations elm-backpropagation using learning rate inferred fact training still relatively high error rates backpropagation optimal give converged results. shown figure comparison figure iterations ﬁxed learning rate still provide signiﬁcant improvement error rate small hand improvement minimal surprising given error rate training data without using backpropagation value already well likely enhancements error rates optimising backpropagation learning rate increasing number iterations used. moreover several methods accelerating convergence carrying backpropagation described previously used methods here. however best error rate result reported previously methods applied mnist benchmark achieved hidden units best error rate backpropagation method used rf-c-elm-bp rf-ciw-elm-cp match surpass former result despite using least advanced backpropagation method prior work latter ﬁgure easily surpassed methods hidden units. moreover training time reported prior work signiﬁcantly slower time required using shaped input weights outcomes indicate input-weight shaping prior backpropagation outperforms backpropagation alone large margin terms error rate training time. many applications time required train network considered important time required network classify data. however exist applications statistics training data change rapidly meaning retraining required deployment trained classiﬁer required rapidly data gathering. example ﬁnancial sports medical data analysis deployment newly trained classiﬁer required within minutes acquiring data retraining required periodically e.g. hourly. hence emphasize paper rapidity training. speed testing negligible comparison. mean training runtime methods shown figure obtained using matlab running macbook intel core running ram. times plotted figure total times setup training excluding time required load mnist data memory ﬁles. version matlab used default exploits four cores matrix multiplication least squares regression. note diﬀerences time method negligible expected since time-consuming part formation train. time testing included shown data. found predictably scaled linearly seconds comparison data tabulated previously backpropagation shows least minutes order achieve accuracy best result minutes contrast runtimes reported standard algorithm previously comparable illustrates improving error rate shaping input weights done substantial beneﬁts many approaches classifying mnist handwritten digits improve error rates preprocessing and/or expanding size training applying aﬃne elastic distortions standard also experimented distorting training improve error rates reported here. example pixel aﬃne translations able achieve error rates smaller added random rotations scalings shears elastic distortions achieved best repeatable error rate overall best error rate however adding distortions training substantially increases runtime reasons. first training points generally requires larger hidden layer size. example increase size training factor found need achieve error rates smaller signiﬁcantly aﬀects time matrix multiplication required. stage chosen systematically continue improve implement distortions approach state mnist results preliminary results show training capable using methods enhance error rate performance expense signiﬁcant increase runtime expected non-elm methods. brieﬂy present results second wellknown image classiﬁcation benchmark norb-small database database consists stereo greyscale images classes standard stereo images training standard testing. images images. contrast-normalised image subtracting mean dividing standard deviation. figure shows results error rate test application rf-c-elm method number hidden units increases. minimum receptive ﬁeld size ridge regression parameter practice known generally computationally eﬃcient avoid explicit calculation matrix inverses pseudo-inverses solving linear sets equations. principle applies using training algorithm hence preferable avoid explicit calculation inverse eqn. instead treat following linear equations solved unknown variables fast methods solving equations exist decomposition method used here. large memory computational speed bottleneck implementation becomes large matrix multiplication atraina train. however simple methods still enable solution eqn. large multiplication carried calculation. well known software packages matlab automatically exploit multiple cores available modern speed execution algorithm using multithreading. alternative methods like explicitly calculating pseudo-inverse singular value decomposition comparison signiﬁcantly slower. using linear equation solution method main component training runtime large hiddenlayer sizes becomes large matrix multiplication retrain. clearly much potential speeding simple time-consuming operation using gpus hardware acceleration methods. text discusses standard single-batch approach. also online incremental solutions real-time streaming operations large data sets singular value decomposition oﬀers additional insight network structure optimization describe iterative method oﬀers advantages training output weight matrix need calculated once. consider combination several non-iterative learning methods deﬁne projection input space hidden layer. hidden layer output solved simply using least squares regression applied single batch training data weights linear output layer. extremely high accuracy required outputs slfns combined using simple autoencoder stage. maximum accuracy obtained comparable best published results standard mnist problem without augmentation dataset preprocessing warping noising/denoising non-standard modiﬁcation. accuracies achieved basic slfn networks cases equal higher achieved best eﬀorts deep belief networks example. moreover using receptive ﬁeld method shape inputs weights resulting input weight matrix becomes highly sparse using algorithm above close input weights exactly zero. note also implementations part carried standard desktop required little computation comparison deep networks. highlighted found signiﬁcant speed increases training avoiding explicit calculation matrix inverses. moreover shown possible circumvent memory diﬃculties could arise large training sets iteratrain still computing output weights once. method could also used streaming applications matrix train could updated every training sample output weights updated periodically. ways avoid previously identiﬁed potential limitations training algorithm regarding matrix inversion discussed principles implemented training algorithm particular single-batch least squares regression linear output layer following random projection nonlinear hidden-units parallel principled approach modelling neurobiological function known neural engineering framework recently framework utilized large model functioning brain known spaun computational performance advantages demonstrated could potentially boost performance well course many applications neural networks. although deep networks convolutional networks standard hard problems image speech processing merits originally argued almost entirely basis success classiﬁcation problems mnist. argument form networks able achieve accuracy unique hierarchy representation features oﬀered deep networks convolutional processing oﬀered cnns must therefore necessary potential drawback following standard method solving output weights using training data batch large amount memory potentially required. example mnist training images hidden layer size double precision representations requires approximately ram. although typically available modern amount memory required becomes problematic training data enhanced distortions amount hidden units needs increased signiﬁcantly. train train formed training points without need keep atrain matrix memory formed least squares solution method applied. matrix atraina train still requires large amount memory using method number training points greatly expanded incur runtime cost. practice rather form training points eﬃcient form batches subsets training points form sums size batch determined maximum available. shown simple slfns capable achieving accuracy deep belief networks convolutional neural networks canonical benchmark problems deep learning image classiﬁcation. accurate networks achieve accuracies. however exists neural network hierarchical representation features obtain accuracy does argument case conﬁrmation bias. shown results equivalent originally obtained deep networks cnns mnist obtained simple singlelayer feedforward networks layer nonlinear processing; results obtained quick implementations. intuitive elegance deep networks hard deny economy structure multilayer networks single layer networks proven would argue speed training ease elm-type single layer networks makes pragmatic ﬁrst choice many real-world machine learning applications. mark mcdonnell’s contribution supported australian research fellowship australian research council andr´e schaik’s contribution supported australian research council discovery project", "year": 2014}