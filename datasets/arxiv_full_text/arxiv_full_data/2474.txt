{"title": "A Sparse Nonlinear Classifier Design Using AUC Optimization", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "AUC (Area under the ROC curve) is an important performance measure for applications where the data is highly imbalanced. Learning to maximize AUC performance is thus an important research problem. Using a max-margin based surrogate loss function, AUC optimization problem can be approximated as a pairwise rankSVM learning problem. Batch learning methods for solving the kernelized version of this problem suffer from scalability and may not result in sparse classifiers. Recent years have witnessed an increased interest in the development of online or single-pass online learning algorithms that design a classifier by maximizing the AUC performance. The AUC performance of nonlinear classifiers, designed using online methods, is not comparable with that of nonlinear classifiers designed using batch learning algorithms on many real-world datasets. Motivated by these observations, we design a scalable algorithm for maximizing AUC performance by greedily adding the required number of basis functions into the classifier model. The resulting sparse classifiers perform faster inference. Our experimental results show that the level of sparsity achievable can be order of magnitude smaller than the Kernel RankSVM model without affecting the AUC performance much.", "text": "formulations binary classiﬁcation problem assumes misclassiﬁcation costs equal classes. therefore svms suitable data strongly imbalanced. proposed simple extension svms using diﬀerent penalization positive negative examples. approach useful misclassiﬁcation costs known typically case practice. thus necessary diﬀerent measure learning imbalanced data. important performance measure optimization eﬀective especially class distributions heavily skewed. however computing costly operation written pairwise losses examples diﬀerent classes quadratic number training examples. further continuous function training set. makes optimization challenging task. many algorithms designed optimize using surrogate loss functions joachims kotlowski high computational demands variants algorithms either one-pass algorithms online algorithms rely sampling. zhao proposed online algorithm based idea reservoir sampling. idea helps represent received examples examples stored buﬀers ﬁxed size. proposed regression based algorithm one-pass optimization. algorithm maintains ﬁrst second order statistics training data memory thereby resulting storage requirement independent training dataset size. algorithms learn linear classiﬁers directly suitable design complex nonlinear decision boundaries typically possible using kernel classiﬁers. calders proposed polynomial approximations computed scan dataset. approximation used design linear classiﬁer. yang proposed online learning algorithm optimize score learning nonlinear decision function kernel trick. method called online imbalanced important performance measure applications data highly imbalanced. learning maximize performance thus important research problem. using max-margin based surrogate loss function optimization problem approximated pairwise ranksvm learning problem. batch learning methods solving kernelized version problem suﬀer scalability result sparse classiﬁers. recent years witnessed increased interest development online single-pass online learning algorithms design classiﬁer maximizing performance. performance nonlinear classiﬁers designed using online methods comparable nonlinear classiﬁers designed using batch learning algorithms many real-world datasets. motivated observations design scalable algorithm maximizing performance greedily adding required number basis functions classiﬁer model. resulting sparse classiﬁers perform faster inference. experimental results show level sparsity achievable order magnitude smaller kernel ranksvm model without aﬀecting performance much. binary classiﬁcation classiﬁer often trained optimizing performance measure accuracy. data highly imbalanced accuracy good measure optimize. all-positive allnegative classiﬁer achieve good classiﬁcation accuracy. result misclassiﬁcation important rare events typically belong minority class. situations datasets imbalanced uncommon real-world applications cases classiﬁers designed optimizing measures accuracy computer science automation iisc bangalore india. ‡computer science automation iisc bangalore india. §microsoft research bangalore india. ¶iit gandhinagar india. function outi puts otherwise. thus maximizing equivalent minimizing writing using max-margin based surrogate loss function following regularized formulations corresponding loss functions work focus problem continuously diﬀerentiable function devise eﬃcient algorithm solve unlike typical classiﬁcation problems loss function calculated every single training example second term involves losses deﬁned pairs examples diﬀerent classes. makes problem challenging. many online algorithms proposed learn linear classiﬁer maximizing score. algorithms include online maximization adaptive online maximization optimization online learning learning kernels maintains buﬀer store informative support vectors. buﬀer update policies ﬁrst-in-ﬁrst-out reservoir sampling investigated. cost determining score large algorithms avoid exact computation score resort online one-pass approaches making buﬀers store relevant information. although storage requirements reduced methods generalization performance resulting classiﬁers comparable nonlinear classiﬁers designed using batch learning algorithms many real world datasets. relevant work paper largescale kernel ranksvm algorithm proposed algorithm though designed solving ranking problem extended solve optimization problem. however kernel evaluations bottleneck training kernel ranksvm. alleviate problem proposed store full kernel matrix. although reduces repeated kernel evaluations storage full kernel matrix issue dataset sizes large requires storage. further kernel ranksvm result model uses large number support vectors thereby incurring high inference cost. motivated observations propose algorithm learn sparse models maximizing using max-margin based surrogate loss function. experimental results show level sparsity achievable order magnitude smaller kernel ranksvm model without aﬀecting performance much. helps achieve signiﬁcant speed-up prediction. nature algorithm parallelization possible demonstrate signiﬁcant training speed-up achievable using multi-core version algorithm. notation discuss notations used work. vectors column vector vectors denoted superscript norm vector denoted kxk. denotes cardinality denotes kernel matrix. refers submatrix made rows columns indexed refers matrix refers submatrix made rows indexed columns indexed challenging task computation score involves pairwise losses instances opposite classes. tackle challenge online learning uses idea buﬀer sampling ﬁxed size buﬀer used represent observed data storing randomly sampled examples introduced idea stream subsampling replacement buﬀer update strategy. although online algorithms demonstrated good performance using simple online gradient descent approaches geometrical knowledge observed data. adaoam overcomes limitation employing adaptive gradient method exploits knowledge historical gradients. variant sadaoam proposed design sparse model online maximization task. proposed one-pass optimization algorithm considering square loss optimization task. squared error loss algorithm needs store ﬁrst second order statistics observed data. main drawback online methods discussed learn linear classiﬁer exploit learning power kernel methods. address issue yang investigated online imbalanced learning kernels informative support vectors stored buﬀer. buﬀer update strategies first-in-first-out reservoir sampling investigated. conducting experiments real-world datasets demonstrated kernel methods maximization performed better linear classiﬁer counterparts. proposed method however online algorithm. joachims presented structural framework optimizing batch mode. formulating -slack structural problem joachims solved dual problem cutting plane method. method though initially designed linear classiﬁers easily extended nonlinear classiﬁers. numerical experiments showed that ranking learning problems method slower others state-of-the-art methods solve directly learning rank important supervised learning problem application variety domains information retrieval online advertising. treating instances query number same preference pairs kernel ranksvm problem discussed used trust region newton method solve problem. method stores full kernel matrix repeated kernel evaluations bottleneck kernel ranksvm. method drawbacks learn sparse nonlinear classiﬁer model binary classiﬁcation problem imbalanced data distributions classes. discuss approach solve similar problem formulation used solve problem learning rank algorithm designed also applicable setting. alleviated diﬃculty computing loss term involves summation preference pairs using order-statistic trees. although cost computing required quantities reduced kernel evaluations amount time reduced kernel matrix maintained throughout optimization algorithm. implementation store full kernel matrix dense matrix size however large datasets impractical store full kernel matrix main memory. further huge datasets resulting classiﬁer sparse thereby making inference slow. therefore desired devise diﬀerent approach solve design sparse classiﬁer. motivated success matching pursuit approach presented keerthi design sparse classiﬁers propose eﬃcient algorithm solve using matching pursuit ideas. algorithm requires compute maintain kernel matrix size dmax helps reduce memory requirement considerably. dataset observed dmax suﬃcient achieve good performance test set. also demonstrate eﬃcient computations objective function gradient hessianvector product computations done using simple techniques like sorting binary search hashing require sophisticated data structures order-statistic trees. experimental results show proposed approach faster approach applied maximization problem achieves comparable generalization performance using small number support vectors. reformulation borrowing ideas presented maintain greedily chosen kernel basis functions design sparse non-linear classiﬁer. cardinality denoted dmax user speciﬁed positive integer. denote index basis functions. experiments choose deﬁned parameter vector represented airola used orderstatistic trees eﬃciently compute ranksvm. problem maximizing require order-statistic trees. enough sorting searching hashing methods. details given algorithm given deﬁne ordered pairs contributes empirical loss objective function every example training ﬁnding violating examples class examples compute empirical loss term computations done eﬃciently using sorting hashing searching complexity algorithm better naive computation pairwise losses further experiments implemented steps algorithm multicore setting. empirical evaluation discussed next section shows resulted signiﬁcant speed algorithm. {|qi preference pairs queries problem requires either store full kernel matrix requires many kernel evaluations become bottleneck. hand solution problem requires store matrix size dmax makes scalable. work solve using matching pursuit ideas approach starting training example chosen l)\\j inclusion results maximum improvement objective function. optimization problem solved respect procedure repeated till dmax holds true. algorithm gives pseudo-code procedure. step algorithm computationally expensive section discuss approaches make eﬃcient eﬃciency algorithm depends eﬃcient computation objective function value gradient hessian-vector product vector r|j|. pairwise indexing matrix denotes indexing matrix violating pairs contribute loss function deﬁning step algorithm solving fully times computationally expensive. instead choosing every work smaller subset size smola suggested random subset choice gaussian process regression successfully. even number random examples method selection still quite computationally heavy. method method solve dimensional problem optimizing solve completely. instead good idea solve determine problem easy solve one-dimensional problem. keerthi showed solve dimensional problem time using newtonraphson type iterations. case complexity solving dimensional problem also instead choosing choose random sample size truncated newton optimization method function consider optimized using second order optimization method. truncated newton optimization method instead classical newton method classical newton method update step computation hessian inverse computational intensive task. therefore reduce computation time uses truncated newton iteration optimize current linear conjugate gradient iteration method approximate uses hessian-vector product vector discussed section that compute hessian-vector product eﬃciently overall complexity discuss details linear conjugate gradient iteration. details steps involved linear conjugate gradient algorithm found many variations around rely hessian vector multiplications. basis selection discuss choose kernel basis functions given problem. approach greedy starts empty training example chosen inclusion results maximum improvement objective function. optimization problem solved respect procedure repeated till dmax holds true. algorithm gives details procedure. eﬃciency procedure depends optimization method used solve discuss methods basis functions computational complexity assuming kernel matrix stored memory computation loss term require computation time. hand corresponding term require computation time large datasets feasible store main memory. therefore datasets kernel ranksvm resorts several block wise computations result increased training time. problem arise approach maximum sub-matrix needs store size dmax. section discuss experimental evaluations proposed algorithm sparse classiﬁer design. particular demonstrate proposed sparse kernel algorithm results sparser classiﬁer gives comparable generalization performance kernel ranksvm algorithm. further batch learning algorithms perform better online learning algorithms majority real world datasets. experiments used gaussian kernel function experiments. kernel parameter regularization hyper-parameter tuned using crossvalidation. this grid values searched. performance corresponding pair gave best validation performance reported. value dmax proposed algorithm terminated dmax true signiﬁcant change validation performance. experiments compare following methods sparse kernel proposed sparse optimization approach discuss section kernel ranksvm extension kernel ranksvm method discussed optimization problem. online imbalanced learning kernels adaptive gradient method online maximization performance methods compared terms score test test explicitly available score validation averaged independent ﬁve-fold splits dataset reported. since paper design nonlinear sparse classiﬁer model using optimization report number support vectors present ﬁnal model batch learning methods sparse kernel kernel ranksvm. methods online learning approach fair compare number support vectors obtained using obtained using batch learning methods. time comparison batch learning methods sparse kernel kernel ranksvm done implementations done using matlab programming language respectively. compare computational complexity methods section iv-d. used benchmark datasets compare proposed method sparse kernel three methods. dataset details given table datasets available libsvm dataset repository. multi-class datasets converted class imbalanced binary datasets. given table training+test splits available datasets. eﬀect retraining make algorithm eﬃcient good idea perform conjugate gradient optimization step time time. experimented retraining strategies step executed addition every basis function basis functions basis functions. results presented ﬁgure clear ﬁgure that always retraining increases training time. similar generalization performance achieved cases retraining. found good choice across many datasets used experiments. table validation performance maximum number basis functions comparison various methods. performance numbers oilkrs adaoam reported respectively. possible basis function chose subset examples possible candidate basis functions. diﬀerent values tried. results shown ﬁgure although values resulted similar steady state generalization performance observed steady state generalization performance achieved faster. discussion tables observed generalization performance proposed sparse kernel method comparable kernel ranksvm method. batch learning methods perform signiﬁcantly better oilk method ionosphere fourclass satimage datasets.the kernel based methods sparse kernel kernel ranksvm oilkrs perform better linear classiﬁer based method majority datasets. proposed method required smaller number basis functions required kernel ranksvm achieve comparable performance. thus proposed method recommended designing sparse classiﬁers large datasets. experiments multi-core setting study speed-up proposed algorithm multi-core environment parallelized steps algorithm speed-up studied three large datasets gradually increasing number cores figure depicts time comparison. clear ﬁgure signiﬁcant speed-up obtained running method multi-core environment. speed-up noticeably large datasets like ijcnn. detailed investigation however needed study parallelization complete proposed algorithm. ding peilin zhao steven yew-soon ong. adaptive subgradient methods online maximization. arxiv preprint arxiv. fawcett. using rule sets maximize performance. data mining icdm proceedings ieee international conference pages ieee alan herschtal bhavani raskutti. optimising area curve using gradient descent. proceedings twenty-ﬁrst international conference machine learning page paper studied eﬃcient learning algorithm design sparse nonlinear classiﬁer using maximization. algorithm tackles challenge lengthy training times kernel methods greedily adding required number basis functions model. demonstrated resulting sparse classiﬁer achieved comparable generalization performance achieved full models. many large datasets observed proposed algorithm results using signiﬁcantly small number basis functions model. also demonstrated batch learning algorithms optimization perform better online algorithms many datasets. currently investigating extension ideas distributed setting. matlab code paper available dropbox link https//www.dropbox.com/s/hawolhbbn/icdm_auccode.tar.gz?dl= purushottam bharath sriperumbudur prateek jain harish karnick. generalization ability online learning algorithms pairwise loss functions. arxiv preprint arxiv. sathiya keerthi olivier chapelle dennis decoste. building support vector machines reduced classiﬁer complexity. journal machine learning research wojciech kotlowski krzysztof dembczynski eyke huellermeier. bipartite ranking minimization univariate loss. proceedings international conference machine learning pages", "year": 2016}