{"title": "Multimodal Task-Driven Dictionary Learning for Image Classification", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Dictionary learning algorithms have been successfully used for both reconstructive and discriminative tasks, where an input signal is represented with a sparse linear combination of dictionary atoms. While these methods are mostly developed for single-modality scenarios, recent studies have demonstrated the advantages of feature-level fusion based on the joint sparse representation of the multimodal inputs. In this paper, we propose a multimodal task-driven dictionary learning algorithm under the joint sparsity constraint (prior) to enforce collaborations among multiple homogeneous/heterogeneous sources of information. In this task-driven formulation, the multimodal dictionaries are learned simultaneously with their corresponding classifiers. The resulting multimodal dictionaries can generate discriminative latent features (sparse codes) from the data that are optimized for a given task such as binary or multiclass classification. Moreover, we present an extension of the proposed formulation using a mixed joint and independent sparsity prior which facilitates more flexible fusion of the modalities at feature level. The efficacy of the proposed algorithms for multimodal classification is illustrated on four different applications -- multimodal face recognition, multi-view face recognition, multi-view action recognition, and multimodal biometric recognition. It is also shown that, compared to the counterpart reconstructive-based dictionary learning algorithms, the task-driven formulations are more computationally efficient in the sense that they can be equipped with more compact dictionaries and still achieve superior performance.", "text": "trained using separate sources. classiﬁer fusion well-studied topic fewer studies done feature fusion mainly incompatibility feature sets naive feature fusion stack features longer however approach usually suffers curse dimensionality limited number training samples even scenarios abundant training samples concatenation feature vectors take account relationship among different sources contain noisy redundant data degrade performance classiﬁer however limitations mitigated feature fusion potentially result improved classiﬁcation performance sparse representation classiﬁcation recently attracted interest many researchers input signal approximated linear combination dictionary atoms successfully applied several problems robust face recognition visual tracking transient acoustic signal classiﬁcation approach structured dictionary usually constructed stacking training samples different classes. method also expanded efﬁcient feature-level fusion usually referred multi-task learning among different proposed sparsity constraints joint sparse representation shown signiﬁcant performance improvement several multi-task learning applications target classiﬁcation biometric recognitions multiview face recognition underlying assumption multimodal test input simultaneously represented dictionary atoms training samples multimodal dictionary represents modalities therefore resulting sparse coefﬁcients sparsity pattern. however dictionary constructed collection training samples suffer limitations. first number training samples increases resulting optimization problem becomes computationally demanding. second dictionary constructed optimal neither reconstructive tasks discriminative tasks recently shown learning dictionary overcome limitations signiﬁcantly improve performance several applications including image restoration face recognition object recognition learned dictionaries usuabstract—dictionary learning algorithms successfully used reconstructive discriminative tasks input signal represented sparse linear combination dictionary atoms. methods mostly developed single-modality scenarios recent studies demonstrated advantages feature-level fusion based joint sparse representation multimodal inputs. paper propose multimodal task-driven dictionary learning algorithm joint sparsity constraint enforce collaborations among multiple homogeneous/heterogeneous sources information. taskdriven formulation multimodal dictionaries learned simultaneously corresponding classiﬁers. resulting multimodal dictionaries generate discriminative latent features data optimized given task binary multiclass classiﬁcation. moreover present extension proposed formulation using mixed joint independent sparsity prior facilitates ﬂexible fusion modalities feature level. efﬁcacy proposed algorithms multimodal classiﬁcation illustrated four different applications multimodal face recognition multi-view face recognition multi-view action recognition multimodal biometric recognition. also shown that compared counterpart reconstructive-based dictionary learning algorithms task-driven formulations computationally efﬁcient sense equipped compact dictionaries still achieve superior performance. index terms—dictionary learning multimodal classiﬁcawell established information fusion using multiple sensors generally result improved recognition performance provides framework combine local information different perspectives tolerant errors individual sources fusion methods classiﬁcation generally categorized feature fusion classiﬁer fusion algorithms. feature fusion methods aggregate extracted features different sources single feature used classiﬁcation. hand classiﬁer fusions algorithms combine decisions individual classiﬁers work achieved bahrampour jenkins department electrical engineering pennsylvania state university university park usa; nasrabadi army research laboratory adelphi bahrampour bosch research technology center palo alto nasrabadi computer science electrical engineering department west virginia university. retrieval task relevant samples modalities given unimodal query. however proposed formulation cannot readily applied information fusion task label given multimodal query. moreover joint sparsity prior used couple similarly labeled samples within modality utilized extract cross-modality information essential information fusion furthermore dictionaries learned generative minimizing reconstruction error data across modalities therefore necessary optimal discriminative tasks multimodal dictionary learning algorithms multimodal task-driven dictionary learning algorithm proposed classiﬁcation using homogeneous heterogeneous sources information. information different modalities fused feature level using joint sparse representation decision level combining scores modal-based classiﬁers. proposed formulation simultaneously trains multimodal dictionaries classiﬁers joint sparsity prior order enforce collaborations among modalities obtain latent sparse codes optimized features different tasks binary multiclass classiﬁcation. fig. presents overview proposed framework. unsupervised multimodal dictionary learning algorithm also presented byproduct supervised version. differentiability bi-level optimization problem main difﬁculty proposing formulation solution corresponding joint sparse coding problem differentiable respect dictionaries. joint sparse coding non-smooth cost function shown locally differentiable resulting bi-level optimization task-driven multimodal dictionary learning smooth solved using stochastic compact fewer dictionary atoms number training samples dictionary learning algorithms generally categorized groups unsupervised supervised. unsupervised dictionary learning algorithms method optimal direction k-svd aimed ﬁnding dictionary yields minimum errors adapted reconstruction tasks signal denoising image inpainting although unsupervised dictionary learning also used classiﬁcation shown better performance achieved learning dictionaries adapted speciﬁc task rather data methods called supervised task-driven dictionary learning algorithms. classiﬁcation task example meaningful utilize labeled data minimize misclassiﬁcation error rather reconstruction error adding discriminative term reconstruction error minimizing trade-off proposed several formulations incoherent dictionary learning algorithm proposed another supervised formulation trains class-speciﬁc dictionaries minimize atom sharing different classes uses sparse representation classiﬁcation. fisher criterion proposed learn structured dictionaries sparse coefﬁcients small within-class large between-class scatters. unsupervised dictionary learning reformulated large scale matrix factorization problem solved efﬁciently supervised dictionary learning usually difﬁcult optimize. recently shown better optimization tool used tackle supervised dictionary learning achieved formulating bilevel optimization problem particular stochastic gradient descent algorithm proposed efﬁciently solves dictionary learning problem uniﬁed framework different tasks classiﬁcation nonlinear image mapping compressive sensing. majority existing dictionary learning algorithms including task-driven dictionary learning applicable single source data. view-speciﬁc dictionaries common dictionary learned application multi-view action recognition. view-speciﬁc dictionaries trained exploit view-level correspondence common dictionary trained capture common patterns shared among different views. proposed formulation belongs class dictionary learning algorithms leverages labeled samples learn class-speciﬁc atoms minimizing reconstruction error. moreover cannot used fusion heterogeneous modalities. generative multimodal dictionary learning algorithm proposed extract typical templates multimodal features. templates represent synchronous transient structures modalities used localization applications. recently multimodal dictionary learning algorithm joint sparsity prior proposed multimodal dictionary learning widely used various tasks reconstruction classiﬁcation compressive sensing contrast principal component analysis variants dictionary learning algorithms generally impose orthogonality condition ﬂexible allowing well-tuned training data. rn×n collection training samples assumed statistically independent. dictionary rn×d obtained minimizer following empirical cost fusion extension proposed framework presented facilitates ﬂexible fusion modalities feature level allowing modalities different sparsity patterns. extension provides framework tune trade-off independent sparse representation joint sparse representation among modalities. improved performance multimodal classiﬁcation proposed methods achieve stateof-the-art performance range different multimodal classiﬁcation tasks. particular provided extensive performance comparison proposed algorithms competing methods literature four different tasks multimodal face recognition multi-view face recognition multimodal biometric recognition multiview action recognition. experimental results datasets demonstrated usefulness proposed formulation showing proposed algorithm readily applied several different application domains. improved efﬁciency sparse-representation based classiﬁcation shown that compared counterpart sparse representation classiﬁcation algorithms proposed algorithms computationally efﬁcient sense equipped compact dictionaries still achieve superior performance. optimal value sparse coding problem regularizing parameters. usually zero exploit sparsity using makes optimization problem strongly convex resulting differentiable cost function index used emphasize dictionary learning formulation unsupervised method. well-known often interested minimizing expected risk rather perfect minimization empirical cost efﬁcient online algorithm proposed dictionary minimizer following stochastic cost convex trained dictionary used reconstruct input. reconstruction error shown robust measure classiﬁcation tasks another given trained dictionary feature extraction sparse code obtained solution used feature vector representing input signal classical expected risk optimization training classiﬁer rest paper organized follows. section unsupervised supervised dictionary learning algorithms single source information reviewed. joint sparse representation multimodal classiﬁcation also reviewed section. section proposes taskdriven multimodal dictionary learning algorithms. comparative studies several benchmarks concluding results presented section section respectively. vectors denoted bold lower case letters matrices bold upper case letters. given vector element. given ﬁnite indices vector formed elements indexed symbol used distinguish vectors column vectors i.e. given matrix column matrix represented respectively. given ﬁnite indices matrix formed columns indexed matrix formed rows indexed similarly given ﬁnite sets indices xγ→ψ matrix formed rows columns indexed respectively. element several advantages ﬁxed dictionary consisting training data. importantly potentially remove redundant noisy information representing training data compact form. also using supervised formulation expects dictionaries well-adapted discriminative tasks. unsupervised multimodal dictionary learning derived extending optimization problem characterized using joint sparse representation enforce collaborations among modalities. minimum joint sparse coding deﬁned cost regularizing parameters. additional frobenius norm compared guarantees unique solution joint sparse optimization problem. special case optimization reduces well-studied elastic-net optimization natural extension optimization problem unsupervised multimodal dictionaries obtained assumed data drawn ﬁnite probability distribution optimization problem solved using classical projected stochastic gradient algorithm consists sequence updates follows gradient step time orthogonal projector onto algorithm converges stationary point decreasing sequence typical choice shown next section. problem also solved using online matrix factorization algorithm noted stochastic gradient descent converge guaranteed converge global minimum non-convexity optimization problem however stationary point empirically found sufﬁciently good practical applications ground truth class label associated input model parameters regularizing parameter convex loss function measures well predict given feature vector classiﬁer parameters expectation taken respect probability distribution labeled data. note dictionary ﬁxed independent given task class label task-driven dictionary learning hand supervised formulation used ﬁnds optimal dictionary classiﬁer parameters jointly solving following optimization problem index convex loss function used emphasize dictionary learning formulation supervised. learned task-driven dictionary shown result superior performance compared unsupervised setting setting sparse codes indeed optimized latent features classiﬁer. joint sparse representation provides efﬁcient tool feature-level fusion sources information ﬁnite available modalities feature vector modality. also rns×d corresponding dictionary modality. assumed multimodal dictionaries constructed collections training samples different modalities i.e. atom dictionary training sample modality. given multimodal input optimal sparse matrix rd×s obtained solving following -regularized reconstruction problem regularization parameter. sthcolumn corresponds sparse representation modality. different algorithms proposed solve optimization problem efﬁcient alternating direction method multipliers prior encourages sparsity i.e. encourages collaboration among modalities enforcing dictionary atoms different modalities present event used reconstructing inputs. term also added cost function extend general framework sparsity also sought within rows discussed section iii-d. shown joint sparse representation result superior performance fusing multimodal sources information compared information fusion techniques interested learning multimodal dictionaries joint sparsity prior. multiclass classiﬁcation multiclass classiﬁcation formulated using collections binary classiﬁers one-vs-one one-vs-all setting. multiclass classiﬁcation also handled all-vs-all setting using softmax regression loss function. scheme label belongs softmax regression loss deﬁned another all-vs-all setting multiclass classiﬁcation task turned regression task scaler label changed binary vector coordinate corresponding label rest coordinates zero. setting deﬁned choosing one-vs-all setting independent multimodal dictionaries trained class multiclass formulation multimodal dictionaries shared classes points considered. one-vs-all setting total number dictionary atoms equal k-class classiﬁcation multiclass setting number equal noted multiclass setting larger dictionary generally required achieve level performance capture variations among classes. however generally observed size dictionaries multiclass setting required grow linearly number classes increases atom sharing among different classes. another point consider class-speciﬁc dictionaries onevs-all approach independent obtained parallel. paper multiclass formulation used allow feature sharing among classes. main challenge optimizing nondifferentiability however shown although sparse coefﬁcients obtained solving non-differentiable optimization problem discussed section unsupervised setting take account label training data dictionaries obtained minimizing reconstruction error. however classiﬁcation tasks minimum reconstruction error necessarily result discriminative dictionaries. section multimodal task-driven dictionary learning algorithm proposed enforces collaboration among modalities feature level using joint sparse representation decision level using decision scores. propose learn dictionaries ds∀s classiﬁer parameters ws∀s shortly denoted jointly solution following optimization problem minimizer optimization problem convex loss function measures well classiﬁer parametrized predict observing expectation taken respect joint probability distribution multimodal inputs label note acts hidden/latent feature vector corresponding input generated learned discriminative dictionary general chosen convex function twice continuously differentiable possible values examples given binary multiclass classiﬁcation tasks. classiﬁer parameters. optimal obtained multimodal sample lsu. simplicity intercept term linear model omitted here easily added. also bilinear model where instead vectors matrices learned multimodal sample accordingly -norm regularization needs replaced matrix frobenius norm. bilinear model richer linear model sometimes result better classiﬁcation performance needs careful training avoid over-ﬁtting. proof proposition given appendix. stochastic gradient descent algorithm optimal dictionaries classiﬁers described algorithm stochastic gradient descent algorithm guaranteed converge assumptions mildly stricter paper improve convergence proposed stochastic gradient descent algorithm classic mini-batch strategy used small batch training data sampled batch instead sample parameters updated using averaged updates batch. additional advantage corresponding factorization admm solving sparse coding problem computed whole batch. special case proposed algorithm reduces single-modal taskdriven dictionary learning algorithm selecting strictly positive guarantees linear equations unique solution. words easy show matrix positive deﬁnite given however practice observed solution joint sparse representation problem numerically stable since becomes full-column rank sparsity sought sufﬁciently large zero. noted assumption full column rank matrix common assumption sparse linear regression non-convex optimization algorithm algorithm initialized properly yield poor performance. similar dictionaries  initialized solution unsupervised multimodal dictionary learning algorithm. upon assignment initial dictionaries parameters classiﬁers solving respect convex optimization problem. present extension proposed algorithm ﬂexible structure sparse codes. joint sparse representation relies fact modalities share sparsity pattern which multimodal training sample selected reconstruct input modalities within training sample active. however group sparsity constraint imposed norm stringent applications example scenarios modalities different noise levels heterogeneity modalities imposes different sparsity levels reconstruction task. natural relaxation joint sparsity prior multimodal inputs share full active achieved replacing norm combination norms norm. following formulation section iii-b minimizer unction deﬁned differentiable therefore gradients computable. gradient respect optimality condition optimization ﬁxed point differentiation show differentiable non-zero rows. without label admits ﬁnite values deﬁned eqs. algorithm derived scenario belongs compact subset ﬁnitedimensional real vector space couple mild assumptions required prove differentiability direct generalizations required single modal scenario listed below assumption multimodal data admit ﬁrst assumption reasonable dealing signal/image processing applications acquired values obtained sensors bounded. also given examples previous section satisfy second assumption. stating main proposition paper below term active deﬁned. algorithm stochastic gradient descent algorithm multimodal task-driven dictionary learning. input regularization parameters learning rate parameters number iterations initial dictionaries ds}s∈s initial model parameters s}s∈s. performance proposed multimodal dictionary learning algorithms evaluated face database multi-pie dataset ixmas action recognition dataset multimodal dataset algorithms chosen quadratic loss handle multiclass classiﬁcation. experiments observed using multiclass formulation achieves similar classiﬁcation performance compared using logistic loss formulation one-vs-all setting. regularization parameters selected using crossvalidation sets .k|k respectively. observed number dictionary atoms kept small compared number training samples arbitrarily small value e.g. normalized inputs. mixed norm used regularization parameters selected cross-validation parameter zero experiments except using prior section iv-b small positive value required convergence. selected according learning parameter constants. results constant learning rate ﬁrst iterations annealing strategy rest iterations. observed choosing total number iterations whole training works well experiments. different values tried ﬁrst iterations results minimum error small validation retained. equal experiments. observed empirically selection parameters quite robust small variations values affect considerably obtained results. also used mini-batch size experiments. also noted design parameters competitive algorithms also selected using cross-validation fair comparison. regularization parameter added norm terms selection inﬂuences sparsity pattern intuitively increases group constraint becomes dominant collaboration enforced among modalities. hand small values encourage independent reconstructions across modalities. extreme case zero optimization problem separable across modalities. formulation brings added ﬂexibility cost additional design parameter obtained paper using cross-validation. present algorithm modiﬁed solve supervised multimodal dictionary learning problem mixed constraint. proof obtaining algorithm similar norm brieﬂy discussed appendix. algorithm solution optimization problem active rows. s|λ|} indices non-zero i.e. consists non-zero entries entries active rows deﬁned algorithm then updated table comparison reconstructive-based proposed discriminative-based classiﬁcation algorithms obtained using joint sparsity prior different numbers dictionary atoms class dataset. seven images ﬁrst session training samples seven images second session test samples. small randomly selected portion training used validation optimizing design parameters. fusion taken modalities left right periocular nose mouth whole face modalities similar setup test sample dataset extracted modalities shown fig. pixels ﬁrst pca-transformed normalized zero mean unit norm. dictionary size dictionary learning algorithms chosen four class resulting dictionaries overall atoms. classiﬁcation using whole face modality classiﬁcation results using whole face modality shown table results obtained using linear support vector machine multiple kernel learning logistic regression sparse representation classiﬁcation unsupervised supervised dictionary learning algorithms algorithm linear polynomial kernels used. equipped quadratic classiﬁer results best performance. sparse priors multimodal classiﬁcation straightforward utilizing single-modal dictionary learning algorithms namely multimodal classiﬁcation train independent dictionaries classiﬁers modality combine individual scores fused decision. fusion equivalent using norm enforce sparsity sparse coefﬁcients. denote corresponding unsupervised supervised multimodal dictionary learning algorithms using norm umdl smdl respectively. similarly proposed unsupervised supervised multimodal dictionary learning algorithms using norm denoted umdl smdl. table compares performance multimodal dictionary learning algorithms priors. shown proposed algorithms prior enforces collaborations among modalities better fusion performances prior. particular smdl signiﬁcantly better performance smdl fusion ﬁrst second modalities. agrees intuition modalities highly correlated learning multimodal dictionaries jointly indeed improves recognition performance. comparison fusion methods performances proposed fusion algorithms different sparsity priors compared several stateof-the-art decision-level feature-level fusion algorithms. addition priors evaluate proposed supervised multimodal dictionary learning algorithm mixed norm denoted smdl−. achieve decision-level fusion train independent classiﬁers modality aggregate outputs either adding corresponding scores modality come fused decision using majority voting among independent decisions obtained different modalities. approaches abbreviated respectively used classiﬁers decision-level fusion. proposed methods also compared feature-level fusion methods including joint sparse representation classiﬁer joint dynamic sparse representation classiﬁer mkl. jsrc jdsrc dictionary consists training samples. table compares performance proposed algorithms fusion algorithms dataset. expected multimodal fusion results signiﬁcant performance improvement compared using whole face modality. moreover proposed smdl smdl− achieve superior performances. used recognition task important realtime applications. clear reconstructive model result comparable performance dictionary size chosen relatively large. hand smdl algorithm over-ﬁtted large number dictionary atoms. terms computational expense test time discussed time required solve optimization problem expected linear dictionary size using efﬁcient admm required matrix factorization cashed beforehand. typical computational time solve given multimodal test sample shown fig. different dictionary sizes. expected increases linearly size dictionary increases. illustrates advantage smdl algorithm results state-of-the-art performance compact dictionaries. classiﬁcation presence disguise dataset also contains occluded samples session overall images faces disguised using glasses scarf. additional images evaluate robustness proposed algorithms. similar previous experiments images session used training samples images session used test data. classiﬁcation performance different sparsity priors shown table expected smdl− achieves best performance. presence occlusion modalities less coupled joint sparsity prior among modalities stringent also reﬂected results. performance proposed algorithm evaluated multi-view face recognition using multi-pie dataset consists large numdataset face images different illuminations viewpoints expressions recorded four sessions several months. subjects imaged using cameras different view-angles head height. illustrations multiple camera conﬁgurations well sample multi-view images shown fig. multi-view face images subjects sparsity prior comparison algorithms joint sparsity priors table indicates proposed smdl algorithm equipped dictionaries size achieves relatively better results jsrc uses dictionaries size results conﬁrm idea using supervised formulation compared using reconstruction error achieve better classiﬁcation performance even compact dictionaries. comparison experiment performed correct classiﬁcation rates reconsturtive discriminative formulations compared dictionary sizes kept equal. given number dictionary atoms class dictionaries jsrc thus constructed random selection train samples different classes. different standard jsrc utilized results table training samples used construct dictionaries moreover utilize available training samples reconstructive approach make meaningful comparison unsupervised multimodal dictionary learning algorithm train class-speciﬁc subdictionaries minimizes reconstruction error approximating training samples given class. sub-dictionaries stacked construct ﬁnal dictionaries similar approach call algorithm jsrc-udl indicate dictionaries indeed learned reconstructive formulation. table summarizes recognition performance jsrc jsrc-udl comparison proposed smdl enjoys discriminative formulation different number dictionary atoms class. seen smdl outperforms reconstructive approaches especially number dictionary chosen relatively small. main advantage smdl compared reconstructive approaches compact dictionaries present sessions. face regions poses extracted manually resized similar protocol used images session views {◦±◦±◦±◦} used training samples. test images obtained available view angles session realistic scenario testing poses available training set. handle multi-view recognition using multi-modal formulation divide available views three sets forms modality. test sample constructed randomly selecting image modality. thousand test samples generated way. dictionary size dictionary learning algorithms chosen atoms class. classiﬁcation results obtained using individual modalities shown table expected better classiﬁcation performance obtained using frontal view. results multi-view face recognition shown table vii. proposed supervised dictionary learning algorithms outperform corresponding unsupervised methods fusion algorithms. smdl− results state-of-the-art performance. consistently observed studied applications multimodal dictionary learning algorithm mixed prior results better performance individual prior. however requires additional regularizing parameter tuned. rest paper performance proposed dictionary learning algorithms reported individual priors. multi-view action recognition section presents results purpose multi-view action recognition using ixmas dataset action recorded simultaneously cameras different viewpoints considered modalities experiment. multimodal sample ixmas dataset shown fig. dataset contains action classes action repeated three times actors resulting sequences view. dataset include actions dense trajectories features generated using publicly available code word codebook generated random subset trajectories k-means clustering note wang used descriptors addition dense trajectories. however dense trajectory descriptors used. number dictionary atoms proposed dictionary learning algorithms chosen atoms class resulting dictionary atoms view. dictionaries jsrc constructed using training samples thus dictionary corresponding different view atoms. table viii shows average accuracies classes obtained using existing algorithms state algorithms. wang algorithm uses dense trajectories feature similar setup. wang algorithm however uses descriptors spatio-temporal pyramids addition trajectory descriptor. results show proposed smdl algorithm achieves superior performance smdl algorithm achieves second best performance. indicates sparse coefﬁcients generated trained dictionaries indeed discriminative engineered features. resulting confusion matrix smdl algorithm shown fig. dataset consists different biometric modalities ﬁngerprint iris palmprint hand geometry voice subjects different gender ethnicity. challenging data many samples corrupted blur occlusion sensor noise. paper irises four ﬁngerprint modalities used. evaluation done subset subjects four samples nally proposed biometric recognition systems seen smdl algorithm outperforms competitive algorithms achieves state-of-the-art performance using irises modalities rank recognition rate respectively. using ﬁngerprints performance smdl close best performing algorithm jsrc. results suggest using joint sparsity prior indeed improves multimodal classiﬁcation performance extracting coupled information among modalities. comparison algorithms joint sparsity priors indicates proposed smdl algorithm equipped dictionaries size achieves comparable mostly better results jsrc uses dictionary size similar experiment section iv-a compared reconstructive discriminating algorithms based joint sparsity prior number dictionary atoms class kept equal. fig. summarizes results different fusion scenarios. seen smdl signiﬁcantly outperforms jsrc jsrc-udl number dictionary atoms class chosen results consistent table dataset indicating proposed supervised formulation equipped compact dictionaries achieves superior performance reconstructive formulation studied biometric recognition applications. problem multimodal classiﬁcation using sparsity models studied task-driven formulation proposed jointly optimal dictionaries classiﬁers joint sparsity prior. shown fig. confusion matrix obtained smdl algorithm ixmas dataset. actions check watch cross arms scratch head down turn around walk wave punch kick pick modalities. samples different modalities shown fig. training formed randomly selecting four samples subject overall samples. remaining samples used testing. features used described pca-transformed. dimension input data preprocessing ﬁngerprint iris modalities respectively. inputs normalized zero mean unit norm. number dictionary atoms dictionary learning algorithms chosen class resulting dictionaries overall atoms. dictionaries jsrc jdsrc constructed using training samples. classiﬁcation results obtained using individual modalities different splits data training test samples shown table shown ﬁnger strongest modality recognition task. algorithms achieve best results. noted dictionary size twice sdl. multimodal classiﬁcation consider fusion ﬁngerprints fusion irises fusion modalities. table summarizes correct classiﬁcation rates several fusion algorithms using ﬁngerprints irises modalities obtained different training test splits. fig. shows corresponding cumulative matched score curves competitive methods. performance measure similar origiresulting bi-level optimization problem smooth stochastic gradient descent algorithm proposed solve corresponding optimization problem. algorithm extended general scenario sparsity prior combination joint independent sparsity constraints. simulation results studied image classiﬁcation applications suggest unsupervised dictionaries used feature learning sparse coefﬁcients generated proposed multimodal task-driven dictionary learning algorithms usually discriminative therefore result improved multimodal classiﬁcation performance. also shown that compared sparse-representation classiﬁcation algorithms proposed algorithms achieve signiﬁcantly better performance compact dictionaries utilized. fig. comparison reconstructive-based proposed discriminative-based classiﬁcation algorithms obtained using joint sparsity prior different numbers dictionary atoms class dataset. utilizes stochastic gradient algorithm learning rate carefully chosen convergence algorithm. experiments heuristic used control learning rate. topics future research include developing better optimization tools fast convergence guarantee non-convex setting. moreover developing taskdriven dictionary learning algorithms proposed structured sparsity priors multimodal fusion tree-structured sparsity prior another future research topic. future research also include adapting proposed algorithms multimodal tasks multimodal retrieval multimodal action recognition using kinect data image super-resolution. taken -related optimization though involved. since active locally constant using optimality condition implicitly differentiate respect non-active rows differential zero. active rewritten cardinality matrices consisting active columns active rows respectively. rest proof work active symbols dropped ease notation. taking partial derivative sides respect element ith-row jth-column taking transpose have proposition assumption hold. then part continuous function . proof part special case equivalent elastic problem already shown proof follows similar steps. assumption guarantees bounded. therefore restrict optimization problem compact subset rd×s. since unique cost function continuous element deﬁned compact continuous function . part part statements proved converting optimization problem equivalent group lasso problem using recent results block-diagnoal collection atoms dictionaries. also rewritten proof proposition proposition implies differentiable almost everywhere. know prove proposition easy show differentiable respect assumption fact twice differentiable. also differentiable respect given assumption twice differentiability fact differentiable everywhere except measure zero obtain derivative respect using chain rule. steps similar setting noting complete proof. derivation algorithm mixed prior obtained similarly. active solution optimization problem mixed prior active modalities non-zeros entries. optimality condition active varshney multisensor data fusion intell. problem solving. methodologies approaches. springer berlin heidelberg siegel stiefelhagen yang sensor fusion using dempster-shafer theory proc.  vehicle classiﬁcation multi-sensor smart cameras using featuredecision-fusion proc.distributed smart cameras zhang zhang nasrabadi huang jointstructured-sparsity-based classiﬁcation multiple-measurement transient acoustic signals ieee trans. soheil bahrampour received m.sc. degree electrical engineering university tehran iran received m.sc. degree mechanical engineering degree electrical engineering supervision w.k. jenkins pennsylvania state university university park respectively. bahrampour currently research scientist bosch research technology center palo alto research interests include nasser nasrabadi received b.sc. ph.d. degrees electrical engineering imperial college science technology london england respectively. october december worked senior programmer. worked philips research laboratory member technical staff. assistant professor department electrical engineering worcester polytechnic institute worcester associate professor department electrical computer engineering state university york buffalo buffalo since september senior research scientist army research laboratory since august professor lane dept. computer science elecrical engineering. nasrabadi served associate editor ieee transactions image processing ieee transactions circuits systems video technology ieee transactions neural networks. current research interests image processing computer vision biometrics statistical machine learning theory sparsity robotics neural networks applications image processing. also fellow spie ieee. asok received ph.d. degree mechanical engineering northeastern university boston graduate degrees disciplines electrical engineering mathematics computer science. joined pennsylvania state university university park july currently distinguished professor mechanical engineering mathematics graduate faculty electrical engineering graduate faculty nuclear engineering. prior joining penn state held research academic positions massachusetts institute technology cambridge carnegie-mellon university pittsburgh well management research positions strategic systems division westborough charles stark draper laboratory cambridge mitre corporation bedford authored coauthored research publications including scholarly articles refereed journals research monographs. also fellow american society mechanical engineers fellow world innovative foundation senior research fellow nasa glenn research center national academy sciences award. kenneth jenkins received b.s.e.e. degree lehigh university m.s.e.e. ph.d. degrees purdue university. research scientist associate communication sciences laboratory lockheed research laboratory palo alto joined university illinois urbana-champaign faculty member electrical computer engineering jenkins director coordinated science laboratory. served professor head electrical engineering penn state university returned rank professor electrical engineering. jenkins current research interests include fault tolerant highly scaled vlsi systems adaptive signal processing multidimensional array processing computer imaging bio-inspired optimization algorithms intelligent signal processing fault tolerant digital signal processing. coauthored book advanced concepts adaptive signal processing published kluwer past associate editor ieee transaction circuits systems past president society. served general chairman midwest symposium circuits systems general chairman thirty second annual asilomar conference signals systems. served board directors electrical computer engineering department heads association president ecedha since january serving member ieee-hkn board governors. jenkins life fellow ieee recipient distinguished service award ieee circuits systems society. received golden jubilee medal ieee circuits systems society millennium award ieee. named co-winner international award thegeorge monteﬁore foundation outstanding career contributions ﬁeld electrical engineering electrical science awarded shaler area high school distinguished alumnus award honored ieee midwest symposium circuits systems anniversary award received ecedha robert janowiak outstanding leadership service award.", "year": 2015}