{"title": "Feature Weight Tuning for Recursive Neural Networks", "tag": ["cs.NE", "cs.AI", "cs.CL", "cs.LG"], "abstract": "This paper addresses how a recursive neural network model can automatically leave out useless information and emphasize important evidence, in other words, to perform \"weight tuning\" for higher-level representation acquisition. We propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural Network (BENN), which automatically control how much one specific unit contributes to the higher-level representation. The proposed model can be viewed as incorporating a more powerful compositional function for embedding acquisition in recursive neural networks. Experimental results demonstrate the significant improvement over standard neural models.", "text": "paper addresses recursive neural network model automatically leave useless information emphasize important evidence words perform weight tuning higher-level representation acquisition. propose models weighted neural network binary-expectation neural network automatically control much speciﬁc unit contributes higher-level representation. proposed model viewed incorporating powerful compositional function embedding acquisition recursive neural networks. experimental results demonstrate signiﬁcant improvement standard neural models. recursive neural network models constitute type neural structure obtaining higherlevel representations beyond word-level phrases sentences. works bottom-up fashion tree structures long-term dependency extent captured. figure gives brief illustration recursive neural models work obtain distributed representation short sentence movie wonderful. suppose hwonderful embeddings tokens wonderful. representation parent node second layer given tasks obtained embeddings could task-speciﬁc machine learning models parameters optimized. take sentiment analysis example could feed aforementioned sentence embedding logistic regression model classify either positive negative. embeddings sometimes capable capturing latent semantic meanings syntactic rules within text manually developed features many tasks would beneﬁt type structure suffers sorts intrinsic drawbacks. revisit figure common sense tells tokens like movie contribute much sentiment decision word wonderful part unfortunately intrinsic structure recursive neural networks makes less ﬂexible inﬂuence less sentiment-related tokens. keyword wonderful hides deep parse tree example sentence studied russia moscow table brief comparison standard neural network models sentence-level sentiment classiﬁcation using date neural network models trained regularization using adagrad minibatches parameters trained based -fold cross validation training data. report best performance searching optimum regularization parameter optimum batch size mini-batches convolutional function. word embeddings borrowed glove dimensionality generates better performance wordvect senna rnnlm family think winter wonderful takes quite convolution steps keyword ‘wonderful comes surface consequence inﬂuence ﬁnal sentence representation could trivial. issue usually referred gradient vanishing speciﬁc recursive models deep learning architectures. compare neural models notable weakness big-of-word based inability considering words combined form meanings interestingly downside comes advantage resilience feature managing optimization ﬂat-expanded. weights assigned lessinformative evidence could pushed zero regularization. table gives brief comparison unigram based neural network models sentence-level sentiment prediction pang al.’s dataset seen task standard neural network models underperform svm. revisit form equ. straws grasp deal aforementioned problem expecting learned feature embeddings less useful words exert little inﬂuence expecting compositional parameters extremely powerful. former sometimes hard mostly borrow word embeddings trained large corpus senna rather training embeddings task-speciﬁc objective functions neural models easily ﬁtted given small amount training data. regarding latter issue several alternative compositional functions proposed enable varieties composition cater. recent proposed approaches include example matrixvector represents every word vector matrix rntn allows greater interactions input vectors algorithm presented note results comparable socher al.’s work obtains state-of-art performance sentiment classiﬁcation labels sentence-level constitute sort supervision neural network models there cases example task-speciﬁc word embeddings learned. requires sufﬁcient training data avoid ﬁtting. example socher al.’s work labels every single node positive/negative/neutral along parse trees paper borrow idea weight tuning feature based incorporate idea neural architectures. achieve goal propose recursive neural architectures weighted neural network binary-expectation neural network major idea involved proposed approaches associate node recursive network additional parameters indicating important ﬁnal decision. example would expect type structure would dilute inﬂuence tokens like movie magniﬁes impact tokens like wonderful great sentiment analysis tasks. parameters associated proposed models automatically optimized objective function manifested data. proposed model combines capability neural models capture local compositional meanings weight tuning approach reduce inﬂuence undesirable information time yield better performances range different tasks compared standard neural models. rest paper organized follows section brieﬂy describes related work. details benn illustrated section experimental results presented section followed brief conclusion. distributed representations calculated based neural frameworks extended beyond tokenlevel represent n-grams phrases sentences discourse paragraphs documents recursive recurrent models constitute types commonly used frameworks sentence-level embedding acquisition. different variations recurrent/recursive models proposed cater different scenarios recently proposed approaches included sentence compositional approach proposed paragraph/sentence vector representations optimized predicting words within sentence. neural network architecture sometimes requires vector representation input token. various deep learning architectures explored learn embeddings unsupervised manner large corpus might different generalization capabilities able capture semantic meanings depending speciﬁc task hand. proposed architectures work inspired long short-term memory model ﬁrst proposed hochreiter schmidhuber back process time sequence data long time lags unknown size important events. lstm associates time series gates determine whether information early timesequence forgotten current information allowed memory. lstm could partially address gradient vanishing problem recurrent neural models widely used machine translation could phrases sentences etc. denote sequence token word associated speciﬁc vector embedding denotes dimensionality word embedding. wish compute vector representation sentence denoted based parse trees using recursive neural models. parse tree sentence obtained stanford parser node parse tree associated representation basic idea associate node additional weight variable range denote importance current node. technically used pushing output representation not-useful node towards direction retain relatively important information. expect information regarding importance current node embedded representation convolution function enable type information emerge surface following compositional functions dimensional matrix bias vector. dimensional intermediate vector. implementation viewed using three-layer neural model latent neurons output projected space. model thinks much relevant information embedded value would small pushing output vector towards representations parents example figure therefore computed follows denotes dimensional matrix output] denotes concatenation vectors. optimum situation mthe mmovie take values around leading representation node around-zero vector. training illustration purpose binary classiﬁcation task show train wnn. note described training approach applies situations minor adjustments. binary classiﬁcation task sequence associated gold-standard label takes value positive otherwise. standardly determine value feed representation logistic regression model benn associates node binary variable sampled binary distribution parameterized scalar ﬁxed range indicating possibility current node pass information ancestors. obtained similar ways using convolution project current representation scalar lying within perform experiments better understand behavior proposed models compared standard neural models achieve this implement model problems require ﬁxed-length vector representations phrases sentences. sentence-level labels ﬁrst perform experiments dateset setting binary labels sentence constitute resource supervision neural models adopt settings fair comparison regularization gradient decent based adagrad mini batch size tuned parameters regularization -fold cross validation. standard neural models implement settings standard word embeddings directly ﬁxed glove standard word embeddings treated parameters optimize framework. additionally implemented recent popular variations recursive models sophisticatedly designed compositional functions including mv-rnn proposed represents every node parse tree vector matrix. given vector representation matrix representation child node child node vector representation matrix representation parent given report results table discussed earlier standard neural models underperform word models. note derivations standard neural models standard mvrnn many parameters learn performance even worse over-ﬁtting. benn although signiﬁcantly output words generates better results yielding signiﬁcant improvement standard neural models existing revised versions. figure illustrates automatic learned muted factor regarding different nodes parse tree based recursive network. observe model capable learning proper weight vocabularies assigning larger weight values important sentiment indicators suppressing inﬂuence less important ones. attribute better performance proposed models standard neural models automatic weight-tuning ability. note scenario claiming generate state-of-art results using proposed model. sophisticated bag-of-word models example generate better performance proposed models achieve. point wish illustrate proposed models provide promising perspective standard neural models weight tuning property. cases detailed data available capture compositionally proposed models hold promise generate compelling results illustrate socher al’s setting sentiment analysis. socher al’s settings consider socher al’s dataset sentiment analysis contains gold-standard labels every phrase node parse tree. task could considered either -way ﬁne-grained classiﬁcation task labels verynegative/negative/neutral/positive/very-positive -way coarse-way positive/negative based labeled dataset. follow experimental protocols described work consider labeling full sentences. addition varieties neural models mentioned socher al’s work also report performance recently proposed paragraph vector model ﬁrst obtains sentence embeddings unsupervised manner predicting words within context feeds pre-obtained embeddings logistic regression model. paragraph vector achieves current state-of-art performance regarding socher al’s dataset. performances reported table seen proposed approach slightly underperforms current state-of-art performance achieved paragraph vector outperforms versions recursive neural models indicating adding weight tuning parameters indeed leads better compositionally. note comprehensive dataset rely obtain favorable task-speciﬁc word embeddings compositionally plays important role deciding whether review positive negative harnessing local word order information. case neural models exhibit power capturing local evidence composition leading signiﬁcantly better performance bag-of-words based models move sentiment analysis document level. imdb dataset proposed maas dataset consists movie reviews taken imdb movie review contains several sentences. follow experimental protocols described ﬁrst train word vectors wordvect using training documents. next train compositional functions using labeled documents keeping word embedding ﬁxed. ﬁrst obtain sentence-level representations using wnn/benn review contains multiple sentences convolute sentence representations single vector using table performance proposed model compared approaches binary classiﬁcation imdb dataset. results baselines reported note reported results underperform current state-of-the-art performances. paragraph vectors reported accuracy terms imdb dataset. results approach baselines reported table seen long documents bag-of-words perform quite well difﬁcult beat. standard neural models generate competent results compared word models task. incorporating weighted tuning mechanism much better performance roughly compared standard neural models. although benn still underperform current state-of-art model paragraph vector produces better performance bag-of-word models. sentiment analysis forces semantic perspective meaning. next turn syntactic oriented task obtain sentence-level representations based proposed model decide coherence given sequence sentences. corpora widely employed coherence prediction contains reports airplane accidents national transportation safety board contains reports earthquakes associated press. standardly pairs articles containing original document order assumed coherent used positive examples random permutation sentences document treated not-coherent examples. follow protocols introduced considering window approach feeding concatenation representations adjacent sentences logistic regression model classiﬁed either coherent non-coherent. test time assume model makes right decision original document gets score higher random permutations. current state-of-art performance regarding task obtained using standard recursive network described table illustrates performance different models. entity-grid model generates state-of-art performance among non-neural network models. seen neural models perform pretty well task compared existing feature based algorithm. reported results better sentence representations obtained incorporating weighted tuning properties pushing state task accuracy paper propose revised versions neural models benn obtaining higher level feature representations sequence tokens. proposed framework automatically incorporates concept weight tuning architectures lead better higher-level representations generate better performance standard neural models multiple tasks. still underperforms bag-of-word models cases newly proposed paragraph vector approach provides alternative existing recursive neural models representation learning. note limit attentions recursive models work idea weight tuning benn associates nodes neural models additional weighed variables general extended many deep learning models minor adjustment.", "year": 2014}