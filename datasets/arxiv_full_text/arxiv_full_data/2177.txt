{"title": "Deep Robust Kalman Filter", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "A Robust Markov Decision Process (RMDP) is a sequential decision making model that accounts for uncertainty in the parameters of dynamic systems. This uncertainty introduces difficulties in learning an optimal policy, especially for environments with large state spaces. We propose two algorithms, RTD-DQN and Deep-RoK, for solving large-scale RMDPs using nonlinear approximation schemes such as deep neural networks. The RTD-DQN algorithm incorporates the robust Bellman temporal difference error into a robust loss function, yielding robust policies for the agent. The Deep-RoK algorithm is a robust Bayesian method, based on the Extended Kalman Filter (EKF), that accounts for both the uncertainty in the weights of the approximated value function and the uncertainty in the transition probabilities, improving the robustness of the agent. We provide theoretical results for our approach and test the proposed algorithms on a continuous state domain.", "text": "robustness effective environmental safety issues arise example autonomous cars important account environmental uncertainties weather road conditions. robust driving policy account uncertainties must subsequently adjust agent’s driving behavior accordingly. additional motivation model-robust methods agent seeks optimize coherent risk measure follow risk-sensitive policy example investing stock markets requires deﬁning model dynamics stock prices. model based historical data noisy insufﬁcient induces uncertainty model. robust investing policy would account uncertain model avoid dangerous decisions cause loss large amount money. control tasks beneﬁt robustness well state transitions depend several parameters robust agent would consider different possible values parameters ensure satisfactory performance real world. robust markov decision process sequential decision making model accounts uncertainty parameters dynamic systems. uncertainty introduces difﬁculties learning optimal policy especially environments large state spaces. propose algorithms rtd-dqn deep-rok solving large-scale rmdps using nonlinear approximation schemes deep neural networks. rtd-dqn algorithm incorporates robust bellman temporal difference error robust loss function yielding robust policies agent. deep-rok algorithm robust bayesian method based extended kalman filter accounts uncertainty weights approximated value function uncertainty transition probabilities improving robustness agent. provide theoretical results approach test proposed algorithms continuous state domain. sequential decision making stochastic environments often modeled markov decision processes order optimize policy achieves maximal expected accumulated reward given model parameters namely transition probabilities reward function agent optimal policy. many cases true model unknown advance parameters estimated data. deviation estimated model true model cause degradation performance learned policy tainties model parameters looking optimal policy. framework policy associated known uncertainty transition probabilities. optimal policy maximizes worst case value function associated uncertainty set. state space large solving robust optimization problems difﬁcult task looking method solving large-scale rmdps on-line nonlinear value function approximation. existing methods solving rmdps several limitations linear approximation off-line estimation restrictive assumptions transition probabilities. review methods section compare robust bayesian approach propose paper. distinguish types uncertainty model parameters uncertainty refers possible transition probability distributions stepping state state taking action denote clear context omit subscript type uncertainty origins bayesian assumption weights approximated value function. denote weight uncertainty inspired success deep q-network agents estimating large-scale nonlinear value functions propose robust temporal difference algorithm replaces nominal bellman temporal difference error involved optimized objective function robust bellman error. show algorithm captures model uncertainty improves robustness agent. kalman ﬁlter variant nonlinear approximations extended kalman ﬁlter used on-line tracking estimating states dynamic environments indirect observations. methods successfully applied numerous control dynamic systems navigation tracking targets. kalman ﬁlter also used weights estimation approximation functions weights constitute states dynamic systems. suggest extend rtd-dqn algorithm using bayesian approach account uncertainty weights value function approximation addition uncertainty transition probabilities. present approach deep robust kalman ﬁlter algorithm. estimating model parameters potential error estimates i.e. uncertainty model parameters introduces variance estimates value function governed value function weights. figure illustrate robust bayesian approach. serves bayesian learning algorithm receives information transition probabilities uncertainty propagates weights uncertainty approach provides robust efﬁcient estimation demonstrate experiments. contributions bayesian approach online nonlinear approximation value function rmdps. connect robust bellman error updates achieve robust policies rmdps; propose algorithms rtd-dqn deep-rok solve large scale rmdps; provide theoretical guarantees proposed methods; demonstrate performance algorithms continuous state domain. paper related several areas research namely rmdps deep q-learning networks bayesian approach weight uncertainty neural networks work ﬁrst solve rmdps combining scalability large state spaces on-line estimation nonlinear q-function approximations robustness uncertainty transition probabilities bayesian approach account uncertainty approximation weights. table compare different approaches proposed algorithms deep-rok rtd-dqn. tamar used approximate dynamic programming method linear value function approximation. convergence analysis based restrictive assumption transitions exploration policy transitions policy evaluause kalman ﬁlters solve reinforcement learning problems proposed geist pietquin formulation called kalman temporal difference serves base formulation algorithms propose. introduce several differences work ours re-formulate observation function observation agent time target label meaning immediate reward discounted next state optimal q-function. formulation observation function simply q-function current state action; used nominal bellman error using robust version extended kalman ﬁlter opposed unscented kalman ﬁlter approximate nonlinear functions formulation observation function differential allowing ﬁrst order taylor expansion linearization used gradient descent optimization methods. shown superior performance applications however computational cost much greater computational cost requirement sampling weights training step number times equals double weights dimension. moreover requires evaluate observation function samples every training step. unfortunately tractable deep weights might high-dimensional. rmdp tuple {sap ﬁnite states ﬁnite actions deterministic bounded reward function discount factor probability measure denotes uncertainty transition probabilities state action discrete time step system stochastically steps state state taking action transition associated immediate reward agent chooses actions according policy maps state probability distribution actions set. transitions system according probability distribution assumed known uncertainty q-function state-action pair policy state transition model represents expected discounted returns starting executt= γtrt|s state-action distribution induced transitions policy rmdps interested ﬁnding policy maximizes worst case q-function optimal robust qfunction unique solution robust bellman observation time step typically stored experience replay traditionally q-function trained stochastic gradient descent estimating loss experience encountered yielding update weights learned maximum likelihood estimation using stochastic gradient descent methods θmle maxθ bayesian approach uses bayes rule suggests adding prior knowledge weights calculate maximum a-posteriori estimator section brieﬂy outline extended kalman ﬁlter standard technique estimating state nonlinear dynamic system learning weights nonlinear approximation function. paper focus latter role meaning estimating considers weights evaluated time observation time nonlinear observation function. evolution noise observation noise modeled additive white noises covariance variance respectively. seen model presented equation treats weights random variables similarly bayesian approach. according perspective weights belong uncertainty governed mean covariance weights distribution. estimation time denoted ˆθt|· conditional expectation weights respect observed data. formulation distinguishes estimates based observations time successive state taking action state iyengar showed agent restricted stationary deterministic markov policies without affecting achievable robust reward. paper focus \u0001-greedy exploration strategy agent takes random action probability follows optimal policy probability solution minimization problem equation computationally demanding. fortunately families sets solution tractable. popular uncertainty sets presented iyengar nilim ghaoui constructed approximations conﬁdence regions associated probability density estimation. choice seems natural uncertainty statistical errors estimating states transition probabilities using historical data. q-learning method aims directly ﬁnding optimal policy updating q-function optimal greedy policy maxa therefore learning optimal policy reduced learning optimal q-function. many problems state space large thus q-function typically approximated parametric models denotes weights approximation function. weight ﬁxed weights normally called target network. composed stable periodic copy trained weights. denote joint distribution experiences current policy. possible next states state taking action drawn uncertainty note yrobust variation robust bellman function optimal q-function presented equation looks worst case transitions reduce value expected q-function sets robust target label value according minimal expectation. return agent receives robust target labels learns optimally transitions. approach solving rmdps presented algorithm rtd-dqn. based algorithm incorporates robust target label instead nominal one. rtd-dqn initializes weights small random values holds experience reply ﬁnite capacity environment initialized beginning episode. denote estimation process subscript observations episode denoted subscript episode agent observes uncertainty next possible states mini-batch sample agent computes yrobust updates weights according gradient descent step yrobust note environment tranjθ sitions state drawn true unknown model parameter however actions taken agent adhere robust policy based robust target labels. output rtd-dqn algorithm weights estimator rtd-dqn algorithm incorporates robust target label weights update account uncertainty weights. uncertainty important robustness properties. next section suggest purpose. considers several statistics interest time step prediction observation function observation innovation covariance weights error innovation covariance innovation kalman gain deﬁned respectively equations ﬁrst step solving large scale rmdps on-line nonlinear approximations change nbtde presented equation robust bellman error δrobust minimize following objective function time step here instead using prior weights present equivalent derivation posterior weights conditioned based likelihood single observation posterior conditioned estimating using common make following assumptions regarding likelihood posterior assumption likelihood p|θt) assumed gaussian yt|θt pnt). assumption posterior distribution assumed gaussian θt|ot− notation general target label serves observation formulation. based gaussian assumptions derive following theorem theorem assumptions ˆθekf minit|t mizes time step following regularized objective function note objective function regularized version objective function equation weights weighted according error covariance matrix pt|t−. nbtde δnominal equation nominal target label equation observation noise variance interpreted regularization coefﬁcient examine points view looking amount conﬁdence observations observations noisy consider larger values pnt. treating regularization coefﬁcient observations noisy would like larger impact weights prior increasing pnt. propose improve performance rtd-dqn accounting uncertainty weights addition uncertainty transition probabilities. suggest solving deep q-leaning networks. purpose using formulation presented section observation time simply nominal target label ynominal observation function state-action q-function formulation model bayesian approach becomes interesting note kalman gain interpreted adaptive learning rate individual weight implicitly incorporates uncertainty weight. approach resembles familiar stochastic gradient optimization methods adagrad adadelta rmsprop adam different choices pt|t− pnt. refer reader ruder figure block diagram deep-rok algorithm. kalman gain propagates information transition probabilities uncertainty weights uncertainty using rbtde δrobust looking weights update equation definition kalman gain equation combing rbtde formulation kalman gain propagates information robust target label back weight uncertainty combining estimated weight value. bayesian approach solving rmdps presented algorithm deep-rok. deep-rok receives input initial prior error covariance evolution noise covariance observation noise variance uncertainty discount factor observations similar ones described rtd-dqn algorithm weights update different deep-rok uses updates robust target label yrobust based uncertainty output deep-rok algorithm weights estimator ˆθmap error covariance matrix pt|t. deeprok suitable prior including priors assume correlations weights. figure presents block diagram illustrates weights updates deep-rok algorithm. test phase output deep-rok algorithm provides ﬂexibility agent. choose point estimate ˆθt|t single performs tests. recall ˆθt|t incorporates information regarding weights uncertainty transitions uncertainty however agent ability take advantage additional output pt|t test results ensemble sampling weights distribution mean ˆθt|t covariance pt|t. ready combine bayesian method rbtde δrobust approach incorporates uncertainty q-function weights allows propagation uncertainty transition probabilities uncertainty weights approach utilized solve large scale rmdps nonlinear approximation on-line fashion. practically change objective function presented equation adding regularization according formulation replacing nbtde rbtde δrobust theorem assumptions ˆθrobust minimizes time step following regularized robust objective function demonstrate performance rtd-dqn deep-rok algorithms classic environment cartpole nonlinear q-function approximations. cart-pole domain agent’s goal balance pole atop cart controlling direction force applied cart. action space discrete contains possible actions applying constant force right left. state space continuous state fourdimensional consisting respectively cart position cart velocity pole angle pole’s angular velocity. time step agent receives immediate reward pole fallen cart right left boundary screen. cases occur agent receives reward episode terminated. transitions follow dynamic model system based parameters {cart mass pole length}. additional technical details regarding experiment found supplementary material. order introduce robustness cart-pole domain assumed parameters precisely known advance rather known uncertainty turn introduces uncertainty transition probabilities deﬁned considered range meters pole length cart mass. episode uniformly sampled values parameters ranges. samples aided building used implementation cartpole-v contained openai added uncertainty parameters implementation. trained agent rtd-dqn deeprok algorithms using rbtde. compare performance double-dqn agent uses nbtde. agents trained episodes results drawn testing trained models mtest episodes. figure show performance three agents different values pole length. figure show success three agents different values pole length cart mass. double-dqn agent performs well parameters trained however performs poorly extreme values parameters. double-dqn agent learned policy highly optimized speciﬁc parameters trained brittle parameter mismatch. rtd-dqn agent high performance also parameter values nominal values taken account uncertainty rbtde. figure cumulative reward different values pole length success different value pole length cart mass. successful episode deﬁned cumulative reward nominal values double-dqn agent marked red. deep-rok agent robust results keeps high performance large range pole lengths cart masses. bayesian approach algorithm provides agent robustness uncertain parameters. introduced algorithms solving large scale rmdps using on-line nonlinear estimations. rtddqn algorithm incorporates robust bellman temporal difference error robust loss function yielding robust policies agent. deep-rok algorithm uses robust bayesian approach based extended kalman filter account uncertainty weights value function uncertainty transition probabilities. proved deep-rok algorithm outputs weights minimize robust lost function. demonstrated performance algorithms cart-pole domain showed robust approach performs better comparing nominal agent. believe real-world domains autonomous driving investment strategies beneﬁt using robust approach improve agents performances accounting uncertainties models. future work address accounting changes conﬁdence level evaluation procedure directed exploration using uncertainty estimates robustness policy gradient algorithms. chow yinlam tamar aviv mannor shie pavone marco. risk-sensitive robust decision-making cvar optimization approach. advances neural information processing systems chunyuan stevens andrew chen changyou yunchen carin lawrence. learning weight uncertainty stochastic gradient mcmc shape classiﬁcation. proceedings ieee conference computer vision pattern recognition mnih volodymyr kavukcuoglu koray silver david graves alex antonoglou ioannis wierstra daan riedmiller martin. playing atari deep reinforcement learning. arxiv preprint arxiv. rajeswaran aravind ghotra sarvjeet levine sergey ravindran balaraman. epopt learning robust neural network policies using model ensembles. arxiv preprint arxiv. st-pierre mathieu gingras denis. comparison between unscented kalman ﬁlter extended kalman ﬁlter position estimation module integrated navigation information system. intelligent vehicles symposium ieee ieee tieleman tijmen hinton geoffrey. lecture .rmsprop divide gradient running average recent magnitude. coursera neural networks machine learning eric merwe rudolph. unscented kalman ﬁlter nonlinear estimation. adaptive systems signal processing communications control symposium as-spcc. ieee ieee weights evaluated time observation time nonlinear observation function. evolution noise white covariance observation noise white variance sets estimation weights time according conditional expectation typically chosen previous estimation weights time ˆθt|t−. linearization helps computing statistics interest. recall expectation random variable ˆθt|t− estimating using common make following assumptions regarding likelihood posterior equation assumption likelihood p|θt) assumed gaussian yt|θt pnt). assumption posterior distribution assumed gaussian θt|ot− following calculations means covariances assumptions likelihood p|θt) based gaussian assumptions derive following theorem theorem assumptions ˆθekf minit|t mizes time step following regularized objective function used derivation fact argument maximizes posterior argument maximizes posterior. addition argument also minimizes negative log. replace current observation current target label receive proof. proof theorem follows arguments proof theorem difference target label deﬁnition. yrobust instead ynominal depend random varitθ able ﬁxed taking derivative objective function respect describe technical details regarding cart-pole experiment. algorithms rtd-dqn deep-rok) used feed forward fully connected neural network hidden layers tanh activation function. input dimension output dimension hidden layer composed neurons. total number weights network networks trained mini-batches transitions samples mini-batch. used discount factor algorithms trained episodes. double-dqn rtd-dqn trained adam optimizer learning rate deep-rok trained using learning rate observation noise evolution noise error covariance prior results presented paper mean standard deviation cumulative reward obtained applying \u0001greedy policy learned network episodes. double-dqn algorithm trained nominal values pole length=.m cart mass=.kg. uncertainty {pole length cart mass} used rtd-dqn deep-rok algorithms constructed sampling uniformly values range pole length .}m; cart mass }kg. beginning episodes sampled values ranges constructed uncertainty episode. pears using matrix inversion lemma bcbc)−cb square symmetric positive-deﬁnite matrix. purpose assume error covariance matrix pt|t− symmetric positive-deﬁnite. possible next states state taking action drawn uncertainty theorem assumptions ˆθrobust minimizes time step following regularized robust objective function", "year": 2017}