{"title": "Building DNN Acoustic Models for Large Vocabulary Speech Recognition", "tag": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Deep neural networks (DNNs) are now a central component of nearly all state-of-the-art speech recognition systems. Building neural network acoustic models requires several design decisions including network architecture, size, and training loss function. This paper offers an empirical investigation on which aspects of DNN acoustic model design are most important for speech recognition system performance. We report DNN classifier performance and final speech recognizer word error rates, and compare DNNs using several metrics to quantify factors influencing differences in task performance. Our first set of experiments use the standard Switchboard benchmark corpus, which contains approximately 300 hours of conversational telephone speech. We compare standard DNNs to convolutional networks, and present the first experiments using locally-connected, untied neural networks for acoustic modeling. We additionally build systems on a corpus of 2,100 hours of training data by combining the Switchboard and Fisher corpora. This larger corpus allows us to more thoroughly examine performance of large DNN models -- with up to ten times more parameters than those typically used in speech recognition systems. Our results suggest that a relatively simple DNN architecture and optimization technique produces strong results. These findings, along with previous work, help establish a set of best practices for building DNN hybrid speech recognition systems with maximum likelihood training. Our experiments in DNN optimization additionally serve as a case study for training DNNs with discriminative loss functions for speech tasks, as well as DNN classifiers more generally.", "text": "better neural network architecture different optimization technique. work aims address concerns systematically exploring several strategies improve acoustic models. view acoustic modeling component classiﬁer draw inspiration recent classiﬁcation research tasks predominantly image classiﬁcation. unlike many tasks acoustic models lvcsr simply classiﬁers instead sub-component larger speech transcription system. complex relationship downstream task performance word error rate proximal task training acoustic model classiﬁer. complexity unclear improvements acoustic models ultimately result improved performance across range lvcsr tasks. work empirically examines several aspects acoustic models attempt establish best practices creating models. further seek understand aspects training impact downstream task performance. knowledge guide rapid development acoustic models speech corpora languages computational constraints language understanding task variants. furthermore analyze task performance also quantify differences various dnns transform represent data. understanding dnns process information helps understand underlying principles improve dnns classiﬁers components large artiﬁcial intelligence systems. work serves case study dnns generally classiﬁers components larger systems. ﬁrst perform experiments standard switchboard corpus. corpus analyze effect size task performance although hours training data cause dnns overﬁt task increasing model size. investigate several techniques reduce over-ﬁtting including popular dropout regularization technique. next analyze neural network architecture choices comparing deep convolutional neural networks deep locally untied neural networks standard dnns. comparison also evaluates alternative input features since convolutional approaches rely input features meaningful time frequency dimensions. explore performance fewer constraints imposed over-ﬁtting next build baseline lvcsr system combining switchboard fisher corpora. results roughly hours training data represents largest collections conversational speech available abstract—deep neural networks central component nearly state-of-the-art speech recognition systems. building neural network acoustic models requires several design decisions including network architecture size training loss function. paper offers empirical investigation aspects acoustic model design important speech recognition system performance. report classiﬁer performance ﬁnal speech recognizer word error rates compare dnns using several metrics quantify factors inﬂuencing differences task performance. ﬁrst experiments standard switchboard benchmark corpus contains approximately hours conversational telephone speech. compare standard dnns convolutional networks present ﬁrst experiments using locally-connected untied neural networks acoustic modeling. additionally build systems corpus hours training data combining switchboard fisher corpora. larger corpus allows thoroughly examine performance large models times parameters typically used speech recognition systems. results suggest relatively simple architecture optimization technique produces strong results. ﬁndings along previous work help establish best practices building hybrid speech recognition systems maximum likelihood training. experiments optimization additionally serve case study training dnns discriminative loss functions speech tasks well classiﬁers generally. tremendous improvements large vocabulary continuous speech recognition recent years. initial research hypothesized dnns work well unsupervised pre-training however dnns random initialization yield state-of-the-art lvcsr results several speech recognition benchmarks instead appears modern dnn-based systems quite similar longstanding neural network acoustic modeling approaches modern systems build fundamental approaches utilize increased computing power training corpus size function optimization heuristics. paper offers large empirical investigation performance lvcsr tasks understand best practices important design decisions building acoustic models. recent research acoustic models lvcsr explores variations network architecture optimization techniques acoustic model training loss functions. system differences across research groups difﬁcult example determine whether performance improvement academic research. larger corpus allows explore performance much larger models times larger typically used lvcsr. using larger corpus also evaluate impact optimization algorithm choice number hidden layers used ﬁxed number total free parameters. analyze results terms ﬁnal task performance also compare sub-components task performance across models. finally quantify differences different architectures process information. section outlines steps involved building neural network acoustic models lvcsr describes previous work step. process outline contextualizes questions addressed investigations present section iii. section describes neural network architectures optimization algorithms evaluated paper. section presents experiments switchboard corpus focus regularization network dense versus convolutional architectural choices. present experiments combined switchboard fisher corpora section explore performance larger deeper architectures. compare quantify representational properties section conclude section dnns acoustic models hidden markov model speech recognition systems using hybrid approach. hybrid system largely resembles standard approach speech recognition using gaussian mixture model acoustic models. full overview lvcsr systems beyond scope work instead refer previous articles overview hmm-based speech recognition systems work focuses acoustic modeling component lvcsr system. acoustic model approximates distribution ppx|yq probability observing given short span acoustic features conditioned state label acoustic input features represent audio lvcsr systems. state labels lvcsr senones clustered contextdependent sub-phonetic states. hybrid system uses neural network approximate ppx|yq place gmm. neural network explicitly model distribution ppx|yq required hmm. instead train neural networks estimate ppy|xq allows view neural network classiﬁer senones given acoustic input. bayes’ rule obtain ppx|yq given neural network output distribution ppy|xq distribution ppyq prior distribution senones approximate empirical distribution senone occurrence training set. easy obtain simply normalized count senones training set. usually tractably estimate probability acoustic features ppxq. represents probability observing particular span acoustic features difﬁcult distribution model. however acoustic features ﬁxed decoding term ppxq constant albeit unknown scaling factor. result drop term instead provide unscaled acoustic model score term properly formed acoustic model probability sufﬁcient perform decoding maximize combination acoustic language model scores. decoding procedure introduces acoustic model scaling term empirically adjust scaling offset introduced using un-normalized probabilities. using neural networks acoustic models hmm-based speech recognition introduced years much original work developed basic ideas hybrid hmm-dnn systems used modern state-of-the-art systems. however much recently neural networks standard component highest performing lvcsr systems. computational constraints amount available training data severely limited pace possible make progress neural network research speech recognition. instead gaussian mixture models standard choice acoustic modeling researchers worked reﬁne architecture decoding frameworks signal processing challenges associated building high-performance speech recognizers. gmms extensions produced gains benchmark lvcsr tasks span many years resulting systems became increasingly complex. many complexities introduced focused purely increasing representational capacity acoustic models. parallel effort resurgence interest neural networks branding deep learning within machine learning community. work area focused overcoming optimization issues involved training dnns applying unsupervised pre-training obtain better initialization supervised learning tasks dnns provided interesting path forward acoustic modeling neural networks offer direct path increasing representational capacity provided possible good parameters. early experiments dnns used fairly small phoneme recognition tasks using monophone recognition systems small datasets like timit researchers demonstrated dnns also applied lvcsr systems context-dependent triphone states rather monophone states. innovation coupled larger representational capacity dnns compared gmms yielded substantial reductions multiple challenging lvcsr tasks within years acoustic models showed gains challenging tasks within lvcsr systems microsoft google several factors attributed success modern approaches compared previous work hybrid acoustic models. speciﬁcally large total number network parameters increased number hidden layers initialization pre-training thought drive performance modern hybrid systems. researchers quickly established hybrid hmms work much better using contextdependent triphones place monophones initializing weights unsupervised pre-training initially thought important good performance researchers later found purely supervised training random initial weights yields nearly identical ﬁnal system performance using dnns many hidden layers many total parameters generally found beneﬁcial role hidden layers total network size generally understood. deﬁned hybrid system neural network output ppy|xq within complete lvcsr system next focus build neural networks model senone distribution ppy|xq. better understand detailed aspects related building using neural network acoustic models lvcsr break process series modeling algorithmic choices. steps allows better contextualize previous work convey aspects process fully understood. deﬁne process steps label set. labels acoustic model deﬁned baseline hmm-gmm system choose use. early work neural network acoustic models used context-independent monophone states. recent work acoustic models established context-dependent states critical success generally true modern lvcsr systems. several variants context-dependent states exist tried acoustic models. work context-dependent triphone senones created baseline hmm-gmm system. forced alignment. training data originally contains word-level transcriptions without time alignments words. must assign senone label acoustic input frame training utterance. forced alignment ground-truth transcriptions generate sequence senone labels utterance consistent word transcription utterance. generating forced alignment standard step training hmm-based system. standard approach hybrid speech recognition creates forced alignment training data using hmm-gmm system aligned data used train neural network acoustic model. previous work found using trained hmm-dnn system realign training data second round training produces small gains overall system performance process recently generalized yield hmmdnn training procedure starts forced alignment repeatedly uses realign training data experiments used single forced alignment produced baseline hmm-gmm system standard approach building acoustic models. largest difference modern hmm-dnn systems used modern dnns hidden layer making deep. general property depth important feature success modern dnns. several groups recently found replacing standard sigmoidal hidden units rectiﬁed linear units dnns leads gains simpler training procedures deep architectures neural networks single hidden layer perform worse deeper counterparts variety speech tasks even total number model parameters held ﬁxed whether deeper always better deep network must obtain good performance well understood speech recognition classiﬁcation tasks generally. total number parameters used modern dnns typically times greater neural networks used original hybrid experiments. increased model size translates increased representational capacity critical success modern dnn-hmm system. clear push model size depth continue increasing lvcsr performance. size depth fundamental architectural choices dnns also consider variety alternative neural network architectures aside series densely-connected hidden layers. dcnns alternative densely-connected networks intended leverage meaningful time frequency dimensions certain types audio input features. recent work dcnns found useful ﬁrst phoneme recognition tasks also lvcsr tasks used addition standard acoustic model dcnns change ﬁrst sometimes second hidden layers neural network architecture otherwise utilize denselyconnected multilayer architecture dnns. perhaps larger architectural change dnns deep recurrent neural networks introduce temporally recurrent hidden layer hidden layers. resulting architecture outputs longer process input context window independently reﬂecting temporal coherence correlation speech signals. drnns modern extension time-delay neural network ﬁrst used phoneme recognition recurrent network approach modern recurrent network approaches acoustic modeling shown initial success large vocabulary tasks tasks limited training data available long term impact drnns hmm-drnn systems clear drnn reason temporal dynamics input introduce redundancy interference. researchers continue propose compare many architectural variants acoustic modeling speech-related tasks neural network loss function. given training utterances accompanied frame-level senone labels must choose loss function training acoustic model. space possible loss functions large also includes possible regularization terms might control over-ﬁtting training. default choice acoustic models cross entropy loss function corresponds maximizing likelihood observed label given input. cross entropy standard choice training dnns classiﬁcation tasks ignores component larger system. account aspects overall system discriminative loss functions introduced tasks. discriminative loss functions initially developed acoustic models recently applied acoustic model training discriminative training acoustic models begins standard cross entropy training achieve strong initial solution. discriminative loss function used either second step additively combined standard cross entropy function. view discriminative training task-speciﬁc loss function produces acoustic model better sub-component overall system. whatever loss function choose additionally apply regularization terms form ﬁnal training objective function. regularization especially important dnns easily increase models’ representational capacity. simplest form regularization widely applied dnns weight norm penalty often used -norm penalty. generally effective developing regularization techniques dnns area active research. dropout regularization recently introduced effective regularization technique training. recent work applied dropout regularization acoustic models found beneﬁcial combined architectural changes optimization algorithm. non-trivial neural network model leads non-convex optimization problem. this choice optimization algorithm impacts quality local minimum found optimization. little general case optimization since possible global minimum estimate particular local minimum best possible solution. standard approach optimization stochastic gradient descent many variants practitioners typically choose particular variant empirically. provides robust default choice optimizing dnns researchers continue work improving optimization algorithms dnns. nearly optimization algorithms popular gradient-based recent work shown advanced quasi-newton methods yield better results tasks generally well acoustic modeling quasi-newton similar methods tend computationally expensive update methods improved optimization performance sometimes distributed across multiple processors easily necessary loss functions difﬁcult optimize well techniques. recently algorithms like adagrad nesterov’s accelerated gradient applied dnns tasks outside speech recognition tend provide superior optimization compared still computationally inexpensive compared traditional quasi-newton methods amount time required training important practical consideration optimization tasks. several groups designed implemented neural network optimization procedures utilize graphics processing units clusters dozens hundreds computers clusters gpus indeed training time neural networks persistent issue throughout history researchers often utilized whatever specialized computing hardware available time modern parallelized optimization approaches often achieve ﬁnal solution similar quality nonparallelized optimization algorithm capable less time larger models compared non-parallelized approaches. stage neural network acoustic model design training tremendous breadth depth prior work. researchers often focus improving particular component pipeline holding components ﬁxed. unfortunately well-established baseline acoustic model building pipeline performance improvements example particular architectural variant difﬁcult assess examining literature. examines relative importance several acoustic model design training decisions. systematically varying several critical design components able test limits certain architectural choices uncover variations among baseline systems relevant lvcsr performance. speciﬁcally address following questions work aspects neural network architecture important acoustic modeling tasks? investigate total network size number hidden layers using corpora avoid overﬁtting confounding factor. build dnns times total number free parameters dnns used previous work. also compare optimization algorithms test whether modern approaches stochastic gradient descent driving factor building large acoustic models. additionally compare much broader architectural choice locally-connected models versus standard densely-connected models. recent work found improvements using dcnns combined dnns acoustic modeling applying dcnns audio features sufﬁcient pre-processing types input features compare dnns dcnns. present ﬁrst experiments dlunns acoustic modeling. dlunns generalized version dcnns still locally connected learn different weights location input features rather sharing weights locations. improve test generalization acoustic models? experiments architecture choices reveal increasing model size easily leads overﬁtting issues. evaluate several modiﬁcations training improve generalization performance large dnns. include dropout recently introduced regularization technique well early stopping used neural network training many years. finally propose evaluate early realignment training technique speciﬁc acoustic modeling path towards improving generalization performance. large deep dnns differ shallow smaller dnns terms phonetic confusions information processing metrics? acoustic models clearly successful application understand perform well might improved. analyze classiﬁcation errors made large acoustic models test improvements sub-tasks ultimately lead overall system improvements. further look information encoding metrics quantify information encoding changes larger deeper dnns. address questions separate experiments using switchboard hour corpus combined hour corpus appropriate experiment. section describe dcnn dlunn architecture computations used work. section addresses questions model size overﬁtting switchboard corpus section uses baseline switchboard system compare dcnn dlunn architectures dnns baseline gmms. section presents experiments using larger training corpus explore issues model size depth optimization algorithm. sections viii analyze performance coding properties dnns trained large combined corpus better understand large dnns encode information integrate lvcsr systems. address stated research questions employ three different classes neural network architecture. architecture amounts different equations convert input features predicted distribution output classes. describe speciﬁcs architecture along loss function optimization algorithms use. experiments utilize cross entropy classiﬁcation loss function. experiments apply regularization techniques addition cross entropy loss function improve generalization performance. many loss functions speciﬁc speech recognition tasks exist topic active research. choose focus cross entropy training cross entropy almost always ﬁrst step additional loss function criterion experimenting task-speciﬁc loss functions. additionally cross entropy loss function standard choice classiﬁcation tasks using allows experiments serve case study large scale classiﬁcation tasks generally. cross entropy loss function consider utterance entirety. instead deﬁned individual samples acoustic input senone label cross entropy objective function single training pair cross entropy convex approximation ideal loss classiﬁcation. however training acoustic models perfect classiﬁcation level short acoustic spans ultimate goal. instead wish minimize word error rate ﬁnal lvcsr system. measures mistakes word level possible perfectly transcribe words utterance without perfectly classifying state present time step. constraints present word sequence probabilities language model correct minor errors state-level observation estimates. conversely acoustic spans equal importance obtaining correct word-level transcription. relationship classiﬁcation accuracy rate frame level overall system complex well understood. experiments always report frame-level error metrics system-level elicit insights relationship loss function performance overall system performance. series fully connected hidden layers transform input vector probability distribution estimate output class. thus acts function approximator conditional distribution ppy|xq. parametrizes function using layers series hidden layers followed output layer. figure shows example dnn. weight matrix bias vectors respectively ﬁrst hidden layer. formulation column matrix corresponds weights single hidden unit ﬁrst hidden layer. fully connected real-valued matrix forms valid weight matrix. instead choose impose partial fully-connected architecture presented thus serves primary neural network acoustic modeling choice modern speech recognition tasks. contrast neural networks computer vision tasks often deep convolutional neural networks exploit spatial relationships input data using spectrogram ﬁlter bank representations speech data analogous time-frequency relationships exist. dcnn architecture allows parameter sharing exploiting local time-frequency relationships improved classiﬁcation performance. dcnns follow convolutional layer pooling layer hard-code invariance slight shifts time frequency. like fully connected neural network acoustic models idea using localized time-frequency regions speech recognition introduced years along modern resurgence interest neural network acoustic models researchers taken modern approach dcnn acoustic models. formulation consistent recent work dcnn acoustic models evaluate specialized feature post-processing combining dnns dcnns form ensemble acoustic models. instead whether dcnns replace dnns robust baseline recipe building neural network acoustic models. like dcnn feed-forward model computes conditional distribution ppy|xq. initial layers dcnn convolutional layers place standard fully-connected layers present dnns. convolutional layers originally developed enable neural networks deal large image inputs computer vision tasks. convolutional model restrict total number network parameters using hidden units connect small localized region input. localized hidden units applied many different spatial locations obtain hidden layer representations entire input. addition controlling number free parameters reusing localized hidden units different locations leverages stationary nature many input domains. computer vision domain amounts reusing edge-sensitive hidden units location image rather forcing model learn type hidden unit location separately. figure shows convolutional hidden layer connected input features time frequency axes. single weight matrix connects region input compute hidden unit activation value using rectiﬁer nonlinearity presented equation apply procedure possible locations input moving step time across input dimensions. process produces feature hidden activation values location input. feature meaningful time frequency axes preserve dimensions convolve across input compute hidden unit activations. convolutional hidden layer feature redundancies apply hidden units location slide across input. following convolutional layer apply pooling operation. pooling acts connectivity effectively constraining certain entries subsequent hidden layers compute hidden activation vector hpiq using hidden activations previous layer hpi´q hidden layers apply point-wise nonlinearity function σpzq part hidden layer computation. traditional approaches neural networks typically sigmoidal function. however work rectiﬁed linear units recently shown lead better performance hybrid speech recognition well classiﬁcation tasks rectiﬁer nonlinearity deﬁned using softmax nonlinearity obtain output vector well-formed probability distribution output classes. distribution used loss function stated equation loss functions. chosen loss function speciﬁed computation equations compute sub-gradient loss function respect network parameters. note using rectiﬁer nonlinearities true gradient rectiﬁer function non-differentiable practice treat sub-gradient would true gradient apply gradient-based optimization procedures settings dnn’s parameters. formulation fairly standard compared work speech recognition community. choice rectiﬁer nonlinearities beneﬁt reproduced several research groups. fully connected neural networks widely used acoustic modeling years issues total size depth thoroughly studied. fig. convolution pooling ﬁrst layer architecture. ﬁlter size pooling dimension pooling regions non-overlapping. note ﬁlters applied position convolution step constrained same. max-pooling maximum value grid extracted. down-sampling step hard-codes invariance slight translations input. like localized windows used convolutional layer pooling layer connects contiguous localized region input feature produced convolutional hidden layer. pooling layer overlapping regions. apply pooling function local regions feature map. recall feature contains hidden unit activations single hidden unit. thus using pooling select activation values hidden unit separately forcing different hidden units compete another. work pooling applies function inputs single pooling region. pooling common choice pooling function neural networks computer vision acoustic modeling tasks widely used alternative pooling replaces function averaging function. results pooling average pooling often comparable. overall architecture dcnn consists layers convolution followed pooling followed densely connected hidden layers softmax classiﬁer. essentially build convolution pooling layers input rather building original input features. possible interleave densely connected convolutional hidden layers densely connected hidden layer preserve spatial time-frequency relationships hidden layer representations. dcnn architecture contains hyper-parameters standard must select number convolutional layers input region size convolution pooling layers pooling function. additional hyper-parameters choices depth hidden layer size common types deep neural network architectures. multiple hidden units. need apply architectural ideas simultaneously. deep local untied neural network utilize locally-connected hidden units share weights different regions input. figure shows example dlunn architecture differs dcnn architecture using different weights location ﬁrst hidden layer. applying local untied hidden layer mel-spectrum timefrequency input features hidden units process different frequency ranges using different hidden units. allows network learn slight variations occur feature occurs lower frequency versus higher frequency. dlunns architecture convolutional network except ﬁlters applied different regions input constrained same. thus untied neural networks thought convolutional neural networks using locally connected computations without weight-sharing. results large increase number parameters untied layers relative dcnns. following locally united layer apply pooling layer behaves identically pooling layers dcnn architecture. grouping units together pooling function often results hidden weights similar post-pooling activations invariant feature detects similar time-frequency pattern different regions input. deﬁned several neural network architectures loss function wish optimize must specify gradient-based algorithm local minimum loss function. consider stochastic gradient techniques work batch optimization requires computing gradient across entire dataset step impractical datasets use. several variants stochastic gradient techniques many different convergence properties applied convex optimization problems. neural network training non-convex fig. locally connected untied ﬁrst layer architecture. ﬁlter size pooling dimension pooling regions non-overlapping. unlike convolutional layer shown figure network learns unique weights location. pooling layer otherwise behaves identically pooling layer convolutional architecture. problem make general statements optimality optimization methods. instead consider choice optimization algorithm heuristic lead better performance practice. consider popular stochastic gradient techniques neural network training. ﬁrst optimization algorithm consider stochastic gradient classical momentum technique probably standard optimization algorithm choice modern neural network research. minimize cost function fpθq classical momentum updates amount denotes accumulated gradient update velocity learning rate momentum constant governs accumulate velocity vector time. setting close expect accumulate gradient information across larger past updates. however shown extremely ill-conditioned problems high momentum classical momentum method might actually cause ﬂuctuations parameter updates. turn result slower convergence. recently nesterov’s accelerated gradient technique found address issues encountered training neural networks methods follow intuition accumulating gradient updates along course optimization help speed convergence. accumulates past gradients using alternative update equation ﬁnds better objective function value less sensitivity optimization algorithm hyper-parameters neural network tasks. update rule deﬁned optimization looking ahead gradient along update direction. detailed explanation intuition underlying optimization neural network tasks figure work treat optimization algorithm choice empirical question compare acoustic modeling task establish performance differences. ﬁrst carry lvcsr experiments telephone speech corpus hour switchboard conversational baseline system forced alignments created using kaldi open-source toolkit baseline recognizer sub-phone states gaussians. trained estimate state likelihoods used standard hybrid hmm/dnn setup. input features dnns mfccs context frames. per-speaker cmvn applied speaker adaptation done using fmllr. features also globally normalized prior training dnn. overall baseline system setup largely follows existing kaldi recipe defer previous work details recognition evaluation report test consisting switchboard callhome subsets data well subset training consisting utterances. ﬁrst experiment perhaps direct approach improving performance dnns making dnns larger adding hidden units. increasing number parameters directly increases representational capacity model. indeed representational scalability drives much modern interest applying dnns large datasets might easily saturate types models. many existing experiments acoustic models focus introducing architecture loss function variants specialize dnns speech tasks. instead question whether model size alone drive signiﬁcant improvements overall system performance. additionally experiment using larger context window frames input also serve direct path improving frame classiﬁcation performance dnns. experiments explore three different model sizes varying total number parameters network. number hidden layers ﬁxed altering total number parameters affects number hidden units layer. hidden layers single network number hidden units. hidden layer sizes respectively yield models approximately million parameters. output classes results output layer largest single layer networks. dnns size typically studied literature output layer often consumes majority total parameters network. example parameter model output layer comprises parameters. contrast output layer model total parameters. many output classes occur rarely devoting large fraction network parameters class-speciﬁc modeling wasteful. previous work explores factoring output layer increase relative number shared parameters effect occurs naturally substantially increasing network size. larger models experiment standard input context frames additionally models trained context frames. models hidden units rectiﬁed linear nonlinearity. optimization nesterov’s accelerated gradient smooth initial momentum schedule clamp maximum stochastic updates mini-batches examples. epoch full pass data anneal learning rate half. training stopped improvement cross entropy objective evaluated held development falls small tolerance threshold. order efﬁciently train models size mentioned above distribute model computation across several gpus using distributed neural network infrastructure proposed cluster distributed training software capable training billion parameter dnns. restrict attention models parameter range. preliminary experiments found dnns parameters representative dnns billion parameters task. train models paper model-parallel fashion distributing parameters across four gpus. single pass training parameter takes approximately days. table shows frame-level evaluations acoustic models varying size compared baseline recognizer. frame-level metrics. parameter halves development cross entropy cost smaller parameter substantial reduction. increase model size approximately absolute increase frame classiﬁcation accuracy. frame-level metrics improved using larger context windows. cases model trained larger context window outperforms smaller context counterpart. best overall model terms frame-level metrics parameter context window frames. however frame-level performance always good proxy performance ﬁnal system. evaluate subset training data well ﬁnal evaluation sets. large acoustic models substantially reduce training set. indeed results suggest training reductions possible continuing increase model size. however gains observe training translate large performance gains evaluation sets. small beneﬁt using models larger baseline size building models larger parameters prove beneﬁcial task. discussion better understand dynamics training large acoustic models plot training evaluation performance training. figure shows performance parameter dnns epoch cross entropy training. training reduces fairly dramatically ﬁrst continues decrease slower still meaningful rate. contrast nearly evaluation performance realized within ﬁrst epochs training. important practical implications large training speech recognition. first large acoustic models beneﬁcial exhibit strong over-ﬁtting effect evaluation performance improves awhile becoming increasingly worse. second possible utilize large dnns without prohibitively long training times utilizing ﬁnding performance comes ﬁrst epochs even models scale. finally although increasing context window size improves training metrics gains translate improved test performance. seems increasing context window size provides easy path better ﬁtting training function result learning meaningful generalizable function. dropout recently-introduced technique prevent overﬁtting training dropout technique randomly masks hidden unit activations training prevents co-adaptation hidden units. example observed training unit activation zero probability several experiments demonstrate dropout good regularization technique tasks computer vision natural language processing found reduction using dropout parameter acoustic model hour broadcast frame-wise error metrics evaluated frames held training set. models differ total number parameters. dnns hidden layers either hidden units hidden units networks employ dropout training found effective studies authors perform control experiments measure impact dropout alone. directly compare baseline architecture trained dropout. experiment tests whether dropout regularization mitigate poor generalization performance large dnns observed section v-a. experiments train acoustic models dropout compare generalization performance dnns presented section probability dropout hyper-parameter training. preliminary experiments found setting yield best generalization performance evaluating several possible values dnns presented dropout training otherwise follow training evaluation protocol used thus built using results table shows test performance acoustic models varying size trained dropout. dnns trained dropout improve baseline model acoustic model sizes evaluate. improvement consistent reduction absolute test set. beneﬁcial dropout seems insufﬁcient fully harness representational capacity largest models. additionally note hyper-parameter selection critical ﬁnding gain using dropout. poor setting dropout probability preliminary experiments found gain often worse results training dropout. early stopping regularization technique neural networks halts loss function optimization completely converging lowest possible function value. evaluate early stopping another standard regularization technique improve generalization performance large acoustic models. previous work found early stopping training networks large capacity produces generalization performance better generalization smaller network. further work found that using back-propagation optimization early training large capacity network behaves similarly smaller capacity network. finally early stopping regularization technique similar weight norm penalty another standard approach regularization neural network training. results analyzing training test curves figure observe best-case performance early stopping approach improving generalization. select lowest test system achieves optimization parameter achieves subset better parameter baseline system. early stopped model achieves absolute reduction much smaller parameter dnn. suggests early stopping beneﬁcial perhaps insufﬁcient yield full possible beneﬁts large acoustic models. regularization technique leverages process training labels created acoustic model training. acoustic model training data labeled forced alignment wordlevel transcriptions. test whether re-labeling training data training using partially-trained leads improved generalization performance. short acoustic span associated state label form supervised learning problem training. recall labels generated forced alignment word-level ground truth labels acoustic signal forced alignment uses existing lvcsr system generate labeling consistent word-level transcription system used generate forced alignment course imperfect overall speech recognition framework’s ability account variations pronunciation. leads dataset supervised training pairs contain labels imperfect. consider label corrupted version true label corruption function maps difﬁcult specify certainly independent identically distributed level individual samples. complex corruption function difﬁcult analyze address standard machine learning techniques label noise. hypothesize however noisy labels sufﬁciently correct make signiﬁcantly corrupted labels appear outliers respect true labels assumption outline approach improving generalization based dynamics performance training optimization. neural networks exhibit interesting dynamics optimization. work early stopping found networks high capacity exhibit behavior similar smaller limited capacity networks early phases optimization combining ﬁnding generally smooth functional form hidden output units suggests early training large capacity smooth output function ignores label noise course large enough completely corruptions present optimization converges. studies learning dynamics dnns hierarchical categorization tasks additionally suggest coarse high-level output classes ﬁrst training optimization realignment generating forced alignment using improved acoustic model standard tool lvcsr system training. baseline lvcsr systems using acoustic models realign several times training iteratively improve. iterative realignments helpful improving system performance single-layer ann-hmm hybrid models realignment typically used large acoustic models long training times dnns. however realignment using fully trained acoustic model often produce small reduction ﬁnal system evaluate early realignment generates forced alignment early optimization continues training labels. large capacity dnns begin accurately predicting labels much earlier training early realignment save days training time. further hypothesize less fully converged network remove label distortions completely trained already ﬁtting corrupt labels given imperfect alignment. experiments begin training initial using hmm-gmm forced alignments non-regularized training procedures presented thus far. training using initial hmm-gmm alignments ﬁxed number epochs hmm-dnn system generate forced alignment entire training set. training proceeds using weights newlygenerated training labels. regularization experiments hold rest training evaluation procedures ﬁxed directly measure impact early realignment training. train parameter hidden layer dnns build models realigning either epochs. preliminary experiments found realignment epoch disruptive training resulted quality models. similarly found starting fresh randomly initialized realignment performed worse continuing training weights used generate realignment. found important reset stochastic gradient learning rate initial value realignment occurs. without annealing schedule sets learning rate optimization procedure fully adjust newly-introduced labels. control experiment found resetting learning rate alone without realignment improve system performance. results table compares ﬁnal test performance dnns trained early realignment baseline model well dnns trained dropout regularization. realignment epochs beneﬁcial compared baseline realignment epoch must train additional three epochs total eight match performance trained early realignment. dnns scale translates several days compute time. training time reduction dnns early realignment comes cost implementing performing realignment course standard training technique. realignment requires specializing training speech recognition domain modern lvcsr system already contain infrastructure generate forced alignment hmm-dnn system. overall conclude early realignment effective technique improve performance acoustic models minimal additional training time. experiments thus modify training adding various forms regularization. experiment alternative neural network architectures deep convolutional neural networks deep local untied neural networks trained dcnn dlunn acoustic models using switchboard training data used acoustic model experiments facilitate direct comparisons across architectures. evaluate ﬁlter bank features addition fmllr features used training ﬁlter bank features meaningful spectro-temporal dimensions local receptive ﬁeld computations. models hidden layers trained using nesterov’s accelerated gradient smoothly increasing momentum schedule capped step size halving step size epoch. dcnn dlunn acoustic models chose receptive ﬁeld non-overlapping pooling regions dimension models convolutional layers ﬁrst layer ﬁlter pooling sizes. second layer uses ﬁlter size pooling. parameters selected using results preliminary experiments well results previous work dcnns convolutional layer used followed four densely connected layers equal number hidden units similarly dlunns. depth number hidden units selected models approximately parameters. dcnns convolutional ﬁrst layer depth applied input frame context. following dense hidden layers hidden units. convolutional layer dcnn uses feature maps convolutional layers dense layers hidden units each. dlunns ﬁlters location ﬁrst layer hidden layers hidden units. fig. function training epoch systems acoustic models trained without label realignment epoch re-generates training labels forced alignment early optimization generalizes much better test data converges original labels. system slightly worse system realigns epochs training. early realignment leads better performance models evaluated trained dropout early stopping. makes early realignment overall best regularization technique evaluated switchboard corpus. note early realignment outperforms dropout regularization trained realignment epochs performs comparably size trained dropout. discussion figure shows training test curves parameter acoustic models trained early realignment baseline realignment. note realignment train test increase brieﬂy. surprising realignment substantially changes distribution training examples. trained realignment trains three epochs following realignment begins outperform baseline system. quantify much labeling realignment differs original labeling computing fraction labels changed. early realignment labels changed realignment labels changed realign trained epochs. ﬁnding matches intuition large capacity trains converges corrupted training samples extremely well. thus realign training data fully trained large capacity previously observed labels reproduced nearly perfectly. realigning earlier optimization mimics realigning higher bias model relabels training smoother approximate function. taken together results suggest early realignment leverages high bias characteristics initial phases training reduce requiring minimal additional training time. features temporal dimension meaningful frequency dimension whereas fbank features meaningful time-frequency axes. additional control train dcnn features randomly permuted remove along frequency only pooling along frequency time overlapping pooling regions settings gave better performance. experiments context window frames found results worse results obtained context window frames report frame context results. table shows frame-level ﬁnal system performance results acoustic models built dnns dcnns dlunns. using ﬁlter bank features dcnns dlunns achieve improvements dnns. dcnn models narrowly outperform dlunn models. locally connected acoustic models appears constraint tied weights convolutional models advantageous compared allowing different localized receptive ﬁelds learned different time-frequency regions input. dlunns outperform dcnns experiments fmllr features. indeed dlunn performs well dnn. dcnn harmed lack meaningful relationships along frequency dimension input features whereas ﬂexible architecture dlunn able learn useful ﬁrst layer parameters. also note fmllr features yield much better performance models compared models trained ﬁlter bank features. order examine much beneﬁt using dcnns leverage local correlations acoustic signal yields control experiments ﬁlter bank features randomly permuted along frequency time axes. results show harms performance convolutional architecture still obtain fairly competitive word error rates. control experiment conﬁrms locally connected models indeed leverage localized properties input features achieve improved performance. dcnn dlunn models promising compared models ﬁlter bank features results ﬁlter bank features overall worse results models utilizing fmllr features. note ﬁlter bank features used fairly simple compared fmllr features ﬁlter bank features contain signiﬁcant post-processing speaker adaptation. performing feature transformations give improved performance call question initial motivation using dcnns automatically discover invariance gender speaker time-frequency distortions. fmllr features compare include much higher amounts specialized post-processing appears beneﬁcial neural network architectures evaluated. conﬁrms recent results previous work found dcnns alone typically superior dnns complement acoustic model used together achieve competitive results increased amounts post-processing applied ﬁlter bank features summary conclude dcnns dlunns sufﬁcient replace dnns default reliable choice acoustic modeling network architecture. additionally conclude dlunns warrant investigation alternatives dcnns acoustic modeling tasks. switchboard hour corpus observed limited beneﬁts increasing model size acoustic modeling even variety techniques improve generalization performance. next explore performance using substantially larger training corpus. experiments explores expect acoustic models behave training size limiting factor. setting overﬁtting large dnns less problem thoroughly explore architecture choices large dnns rather regularization techniques reduce over-ﬁtting improve generalization small training corpus. maximize amount training data conversational speech transcription task combine switchboard corpus larger fisher corpus fisher corpus contains approximately hours training data transcriptions slightly less accurate switchboard corpus. baseline acoustic model trained features obtained splicing together frames -dimensional mfccs projecting dimensions using linear discriminant analysis mfccs normalized zero mean speaker. obtaining features also single semi-tied covariance transform features. moreover speaker adaptive training done using single feature-space maximum likelihood linear regression transform estimated speaker. models trained full combined fisher+switchboard training contain tied triphone states gaussians. language model baseline system trained combination fisher transcripts switchboard mississippi state transcripts. kneser-ney smoothing applied ﬁne-tune back-off probabilities minimize perplexity held transcript sentences fisher transcripts. preliminary experiments interpolated transcript-derived language model language model built large collection page text found gains compared using transcript-derived language model alone. evaluation sets experiments corpus. first hub’ corpus used evaluate systems switchboard task. evaluation serves reference point compare systems built combined corpus trained switchboard alone. second evaluation frequently used literature evaluate fisher-trained systems. performance baseline hmm-gmm system shown table table experiments train several dnns hidden layers layer hidden units. results dnns roughly total free parameters typical size acoustic models used conversational speech transcription research literature. classical momentum nesterov’s accelerated gradient optimization techniques hyper-parameters initial learning rate maximum momentum µmax. cases decrease learning rate factor every iterations. learning rate annealing chosen preliminary experiments overall performance appear signiﬁcantly affected annealing schedule. common anneal learning rate pass dataset. dataset quite large found annealing epoch leads much slower convergence good optimization solution. results table shows performance classiﬁcation accuracy dnn-based systems various optimization algorithm settings. ﬁrst evaluate effect optimization algorithm choice. evaluated dnns µmax optimization algorithms dnns achieve best performance setting µmax terms frame level accuracy optimizer narrowly outperforms optimizer performance across evaluation sets nearly identical. optimization algorithms high value µmax important good performance. note previous work hybrid acoustic models µmax appear optimal experiments. also found larger initial learning rate beneﬁcial. experiments using report results dnns diverged optimization process. similarly models trained absolute higher test compared architecture trained thus omit results models trained results table. remainder experiments optimizer µmax settings achieve best performance overall initial experiments generally found optimizer somerobust optimizer producing good parameter solutions. avoid exhaustively searching architecture training parameters simultaneously ﬁrst establish impact optimization algorithm choice holding architecture ﬁxed. train networks optimization algorithms described section iv-e determine optimization algorithm rest experiments corpus. next evaluate performance dnns function total number model parameters keeping network depth optimization parameters ﬁxed. approach directly assesses hypothesis improving performance function model size sufﬁcient training data available. train dnns hidden layers keep number hidden units constant across hidden layer. varying total free parameters thus corresponds adding hidden units hidden layer. table shows frame classiﬁcation performance hidden layer dnns containing total free parameters. difﬁcult exactly reproduce results dnns architecture trained varying optimization algorithms. primarily compare stochastic settings maximum momentum table contains results learning rate since produces optimization procedures make training code available online training code comprises lines python code total facilitate easy comparison training frameworks. overall parameter model performs best terms frame classiﬁcation across evaluation sets. unlike smaller switchboard training corpus experiments increasing model size lead signiﬁcant over-ﬁtting problems wer. however gain increasing model size increase somewhat limited. eval evaluation observe relative gain compared dnn. moving relative gain finally model size increase total parameters yields relative gain clearly diminishing returns increase model size. trend diminishing relative gains also occurs evaluation although relative gains evaluation somewhat smaller overall. frame classiﬁcation rates corpus much lower overall compared switchboard corpus dnns. believe corpus challenging overall acoustic variation errors induced quick transcriptions. even largest leaves room improvement terms frame classiﬁcation. section viii explore thoroughly frame classiﬁcation performance dnns presented here. next compare performance systems keeping total model size ﬁxed varying number hidden layers dnn. optimal architecture neural network change total number model parameters changes. priori reason believe hidden layers optimal model sizes. furthermore good general heuristics select number hidden layers particular task. table shows system striking distinction terms frame classiﬁcation performance gain deep models versus single hidden layer. single hidden layer models perform much worse dnns hidden layers more. among deep models much smaller gains function depth. models hidden layers show clear gain hidden layers little gain hidden layer model compared hidden layer model. results suggest task hidden layers deep enough achieve good performance depth taken increase performance. it’s also interesting note depth much larger impact performance total size. task much important select appropriate number hidden layers choose appropriate total model size. total model size slight decrease frame classiﬁcation layer dnns compared hidden layer dnns. trend decreasing frame-level performance also present training suggests networks become deep difﬁcult minimize training objective function. evidence potential confounding factor building dnns. theory deeper dnns able model complex functions shallower counterparts practice found depth regularizer difﬁculties optimizing deep models. decompose task performance metrics frame classiﬁcation accuracy constituent components gain deeper understanding models compare another. analysis attempts uncover differences models achieve similar aggregate performance. example systems ﬁnal different rates substitutions deletions insertions constituent components metric. results dnns varying total model size depth. report performance eval test contains switchboard callhome evaluation subsets. also evaluate performance switchboard test comparison fisher corpus systems. additionally report frame-level classification accuracy figure shows decomposed performance hmmdnn systems varying size. hmm-dnn system uses hidden layers hmmdnn systems reported table decreases overall function model size largely driven lower substitution rates. insertions deletions remain relatively constant across systems generally smaller components overall wer. decreased substitution rates fairly direct result improving acoustic model quality system becomes conﬁdent matching audio features senones. three sub-components linked possible insertions deletions artifact system shortcomings vocabulary words pronunciation dictionary adequately capture pronunciation variations. next analyze performance terms frame-level classiﬁcation grouped phoneme. understanding senone classiﬁcation think possible senone labels leaves trees. phoneme acts root different tree leaves tree correspond senones associated tree’s base phoneme. figure shows classiﬁcation percentages senones grouped base phoneme. dnns analyzed hidden layer models presented analysis figure table total height reﬂects percentage occurrence data. broken three components correct classiﬁcations errors within base phoneme errors outside base phoneme. errors within base phoneme correspond examples true label senone particular base phone e.g. network predicts incorrect senone label also rooted type error possible predicting senone different base phoneme. together three categories correct base phone different base phone rate correct classiﬁcations non-decreasing function model size base phoneme. overall increasing accuracy larger dnns comes small correctness increases spread across many base phonemes. across phonemes substantial differences withinbase-phoneme versus out-of-base-phoneme error rates. higher rate within-baseexample phoneme errors compared fairly similar vowel similarly consonants varying rates within-base versus out-of-base errors despite similar total rates base phoneme occurrence data. note dnns generally exhibit similar error patterns fig. senone accuracy hidden layer systems varying total parameter count. accuracy grouped base phone report percentage correct mis-classiﬁcations chose senone base phone mis-classiﬁcations chose senone different base phone. total size combined indicates occurrence rate base phone data set. base phone bars representing performance different layer dnn. bars show performance dnns size left right. show non-speech categories silence laughter noise comprise frames sampled. observed acoustic models smaller corpora however challenging nature corpus observe overall lower phone accuracies found previous work. performance function model size appears change gradually fairly uniformly across phonemes rather larger models improving upon speciﬁc phonemes perhaps expense performance others. experiments focus task performance varying levels granularity. metrics address question dnns capable classiﬁers integrated speech decoding infrastructure. however completely addressed question various architectures achieve various levels task performance. computation equations presented section describe algorithmic steps necessary compute predictions many possible settings free parameters model. section offer descriptive analysis trained dnns encode information. analysis aims uncover quantiﬁable differences models various sizes depths encode input data transform make ﬁnal prediction. ﬁrst analysis focuses sparsity patterns units hidden layer dnn. compute empirical lifetime sparsity hidden unit forward propagating examples dnn. consider unit active output non-zero compute fraction examples unit active lifetime activation probability. value gives empirical probability particular unit activate given random input drawn sample distribution. hidden layer network plot hidden units’ lifetime activation probabilities sorted decreasing order sense distribution activation probabilities within layer. plotting technique sometimes called scree plot helps understand information coding distributed across units hidden layer. figure shows scree plots hidden layer dnns varying total model size. coding theory perspective researchers often discuss dnns learning efﬁcient codes sparse dispersed. sparsity generally refers relatively hidden units hidden layer active response input. sparsity efﬁcient seems natural given modern structures hidden layer size often much larger input vector dimensionality. dispersion refers units within hidden layer equally sharing responsibility coding inputs. representation perfect dispersion would appear scree plot. scree plot also visualizes sparsity average height representation units axis. generally model sizes sparsity increases deeper layers dnn. ﬁrst hidden layer noticeably active average compared every layer cases almost factor two. beyond ﬁrst layer activation probability layer decreases slightly look deeper layers dnn. changes activation probability layer within deeper hidden layers fairly minor suggest representation transformed continually compressed. fig. empirical activation probability hidden units hidden layer layer hidden layer dnns. hidden units sorted probability activation. consider positive value active sub-ﬁgure corresponds different model size total parameters left right. generally representations appear fairly disperse mostly curve hidden layer units large percentage inputs tail. appear slight trend increasing dispersion deeper layers especially larger models. importantly observe signiﬁcant permanently inactive units dnns grow total number parameters. larger dnns representation remains fairly disperse small units active less inputs. important metric adding parameters useful parameters actually used encoding transforming inputs. given task performance differences observed function depth ﬁxed number total parameters also compare scree plots function depth better understand coding properties. figure shows scree plots dnns hidden layers dnns total size observe general trend average activation probability decreasing subsequent hidden layers dnns size. true however models hidden layers slightly less sparse activations average layers compared layer compare models across total model size larger models sparse smaller models. larger models also tend slightly dispersed average compared smaller models. sparsity dispersion metrics serve indicators hidden units within layer behave. focus code length analyzes hidden layer transformed representation input rather focusing individual units hidden layer. given input compute number non-zero hidden unit activations hidden layer. compute average code length compare code length across models varying total parameter size larger dnns hidden units layer encode information hidden layer. trend especially evident ﬁrst hidden layer parameter models nearly twice code length compared models. deeper layers observe models parameters greater code length. unclear extend longer codes capturing information input turn enable greater classiﬁcation accuracy versus redundancy multiple hidden units encode overlapping information. code length deeper versus shallow models total size exhibit interesting trend. dnns increasing depth show generally decreasing constant code length layer except case hidden layer dnns. hidden layer dnns deepest models trained code length decreases reaches minimum layer increases layers trend evident models total parameters. note trend decreasing code length followed increasing code length correlated lack improvement hidden layer models compared hidden layer models. experiments needed establish whether code length deeper models generally correlated diminishing task performance. multi-step process building neural network acoustic models comprises large design space broad range previous work. work sought address fundamental design decisions relevant ﬁnal system performance. found increasing model size depth simple effective ways improve performance certain point. switchboard corpus found regularization improve performance large dnns otherwise suffer overﬁtting problems. however much larger gain achieved utilizing combined training corpus opposed applying regularization less training data. experiments suggest architecture quite competitive specialized architectures dcnns dlunns. architecture outperformed architecture variants frame classiﬁcation ﬁnal system wer. previous work used specialized features locally connected models note dnns enjoy beneﬁt making assumptions input features meaningful time frequency properties. enables build dnns whatever features choose rather ensuring features match assumptions neural network. found dlunns performed slightly better dcnns interesting approach specialized acoustic modeling tasks. example locally untied models work well robust reverberant recognition tasks particular frequency ranges experience interference distortion. trained acoustic models parameters hidden layers comprising largest models evaluated date acoustic modeling. trained simple optimization procedure large dnns achieved clear gains frame classiﬁcation training corpus large. analysis performance coding properties revealed fairly gradual change properties move smaller larger models rather ﬁnding phase transition large models begin encode information differently smaller models. overall total network size depth critical factor found experiments. depth certainly important regards hidden layer differences among dnns multiple hidden layers fairly small regards metrics evaluated. certain point increasing depth yields performance gains indeed start harm performance. applying acoustic models tasks appears sufﬁcient ﬁxed optimization algorithm suggest cross-validate total network size using least three hidden layers ﬁve. based results procedure instantiate reasonably strong baseline system experiments modifying whatever components acoustic model building procedure researchers choose explore. finally note driving factor uncertainty around acoustic model research stems training acoustic model isolation rest larger system. models trained paper used cross entropy criterion perform well dnns trained discriminative loss functions previous work. hypothesize large dnns become increasingly useful researchers invent loss functions entrust larger components task neural network. allows utilize function ﬁtting capacity simply acoustic inputs states. believe better understanding task performance coding properties guide research improved architectures loss functions. trained dnns using approximately lines python code demonstrating feasibility fairly simple architectures optimization procedures achieve good system performance. hope serves reference point improve communication reproducibility highly active research area neural networks speech language understanding. dahl deng acero context-dependent pre-trained deep neural networks large vocabulary speech recognition ieee transactions audio speech language processing hinton deng dahl mohamed jaitly senior vanhoucke nguyen sainath kingsbury deep neural networks acoustic modeling speech recognition ieee signal processing magazine vol. november kingsbury sainath soltau scalable minimum bayes risk training deep neural network acoustic models using distributed hessian-free optimization interspeech bourlard morgan connectionist speech recognition hybrid approach. norwell kluwer academic publishers hermansky ellis sharma tandem connectionist feature ieee jurafsky martin speech language processing introduction natural language processing computational linguistics speech recognition. prentice hall vincent larochelle lajoie bengio manzagol stacked denoising autoencoders learning useful representations deep network local denoising criterion journal machine learning research vol. raina madhavan large-scale deep unsupervised learning using graphics processors. icml vol. dean corrado monga chen devin ranzato senior tucker yang large scale distributed deep networks icml chilimbi suzue apacible kalyanaraman project adam building efﬁcient scalable deep learning training system usenix symposium operating systems design implementation chung sainath ramabhadran picheny gunnels austel chauhari kingsbury parallel deep neural network training data blue gene/q proceedings international conference high performance computing networking storage analysis. plaut experiments learning back propagation. rumelhart hinton williams learning internal representations error propagation parallel distributed processing explorations microstructures cognition volume psychological biological models vol. povey ghoshal boulianne burget glembek vesel´y goel hannemann motlicek qian schwarz silovsky stemmer kaldi speech recognition toolkit asru liao mcdermott senior large scale deep neural network acoustic modeling semi-supervised training data youtube video transcription asru sainath kingsbury sindhwani arisoy ramabhadran low-rank matrix factorization deep neural network training high-dimensional output targets icassp cieri miller walker ﬁsher corpus resource next generations speech-to-text. lrec vol. huang gong comparative analytic study gaussian mixture context dependent deep neural network hidden markov models interspeech dahl sainath hinton improving deep neural networks lvcsr using rectiﬁed linear units dropout icassp zeiler ranzato monga yang nguyen senior vanhoucke dean hinton rectiﬁed linear units speech processing icassp sainath kingsbury mohamed dahl saon soltau beran aravkin ramabhadran improvements deep convolutional neural networks lvcsr asru sainath kingsbury saon soltau rahman mohamed dahl ramabhadran deep convolutional neural networks large-scale speech tasks neural networks available http//www.sciencedirect.com/science/article/ pii/s waibel hanazawa hinton shikano lang phoneme recognition using time-delay neural networks acoustics speech signal processing ieee transactions vol. vinyals heigold senior mcdermott monga sequence discriminative distributed training long shortterm memory recurrent neural networks interspeech fig. empirical activation probability hidden units hidden layer layer dnns varying numbers hidden layers. contains dnns total parameters left right sub-ﬁgure shows hidden layers. hidden units sorted probability activation. consider positive value active fig. effective code length hidden layer dnns varying total size depth. compute number non-zero hidden unit activations given input average large sample inputs. plots show average number units active hidden layer dnns varying depth total size. within sub-plot layers ordered left right ﬁrst ﬁnal hidden layer.", "year": 2014}