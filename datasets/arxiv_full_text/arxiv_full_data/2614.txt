{"title": "Learning to reinforcement learn", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.", "text": "wang kurth-nelson tirumala soyer leibo munos blundell kumaran botvinick deepmind london gatsby computational neuroscience unit london institute cognitive neuroscience london recent years deep reinforcement learning systems attained superhuman performance number challenging task domains. however major limitation applications demand massive amounts training data. critical present objective thus develop deep methods adapt rapidly tasks. present work introduce novel approach challenge refer deep meta-reinforcement learning. previous work shown recurrent networks support meta-learning fully supervised context. extend approach setting. emerges system trained using algorithm whose recurrent dynamics implement second quite separate procedure. second learned algorithm differ original arbitrary ways. importantly learned conﬁgured exploit structure training domain. unpack points series seven proof-of-concept experiments examines aspect deep meta-rl. consider prospects extending scaling approach also point potentially important implications neuroscience. recent advances allowed long-standing methods reinforcement learning newly extended complex large-scale task environments atari enabling breakthrough development techniques allowing stable integration non-linear function approximation deep learning resulting deep methods attaining humanoften superhuman-level performance expanding list domains however least aspects human performance starkly lack. first deep typically requires massive volume training data whereas human learners attain reasonable performance wide range tasks comparatively little experience. second deep systems typically specialize restricted task domain whereas human learners ﬂexibly adapt changing task conditions. recent critiques invoked differences posing direct challenge current deep research. present work outline framework meeting challenges refer deep meta-reinforcement learning label intended link distinguish previous work employing term meta-reinforcement learning concept standard deep techniques train recurrent neural network recurrent network comes implement free-standing procedure. shall illustrate right circumstances secondary learned procedure display adaptiveness sample efﬁciency original procedure lacks. following sections review previous work employing recurrent neural networks context meta-learning describe general approach extending methods setting. present seven proof-of-concept experiments highlights important ramiﬁcation deep meta-rl setup characterizing agent performance light framework. close discussion challenges next-step research well potential implications neuroscience. flexible data-efﬁcient learning naturally requires operation prior biases. general terms biases derive sources; either engineered learning system acquired learning. second case explored machine learning literature rubric meta-learning standard setup learning agent confronted series tasks differ another also share underlying regularities. meta-learning deﬁned effect whereby agent improves performance task rapidly average past tasks architectural level meta-learning generally conceptualized involving learning systems lower-level system learns relatively quickly primarily responsible adapting task; slower higher-level system works across tasks tune improve lower-level system. variety methods pursued implement basic meta-learning setup within deep learning community beyond particular relevance approach introduced hochreiter colleagues recurrent neural network trained series interrelated tasks using standard backpropagation. critical aspect setup network receives step within task auxiliary input indicating target output preceding step. example regression task step network receives input value desired output corresponding network also receives input disclosing target value preceding step scenario different function used generate data training episode functions drawn single parametric family system gradually tunes consistent structure converging accurate outputs rapidly across episodes. interesting aspect hochreiter’s method process underlies learning within task inheres entirely dynamics recurrent network rather backpropagation procedure used tune network’s weights. indeed initial training period network improve performance tasks even weights held constant second important aspect approach learning procedure implemented recurrent network structure spans family tasks network trained embedding biases allow learn efﬁciently dealing tasks family. importantly hochreiter’s original work well subsequent extensions addressed supervised learning present work consider implications applying approach context reinforcement learning. here tasks make training series interrelated problems example series bandit problems varying parameterization. rather presenting target outputs auxiliary inputs agent receives inputs indicating action output previous step critically quantity reward resulting action. reward information parallel deep procedure tunes weights recurrent network. previously). supervised case approach successful dynamics recurrent network come implement learning algorithm entirely separate used train network weights. again sufﬁcient training learning occur within task even weights held constant. however procedure recurrent network implements full-ﬂedged reinforcement learning algorithm negotiates exploration-exploitation tradeoff improves agent’s policy based reward outcomes. point emphasize follows learned procedure differ starkly algorithm used train network’s weights. particular policy update procedure differ dramatically involved tuning network weights learned procedure implement approach exploration. critically supervised case learned procedure statistics spanning multi-task environment allowing adapt rapidly task instances. formalism write distribution markov decision processes want demonstrate meta-rl able learn prior-dependent algorithm sense perform well average mdps drawn slight modiﬁcations appropriately structured agent embedding recurrent neural network trained interacting sequence environments episodes. start episode task initial state task sampled internal state agent reset. agent executes action-selection strategy environment certain number discrete time-steps. step action executed function whole history agent interacting current episode network weights trained maximize observed rewards steps episodes. training agent’s policy ﬁxed evaluated mdps drawn either distribution slight modiﬁcations distribution internal state reset beginning evaluation episode. since policy learned agent history-dependent exposed environment able adapt deploy strategy optimizes rewards task. order evaluate approach learning described conducted series proof-of-concept experiments present along seventh experiment originally reported related paper particular point interest experiments whether meta-rl could used learn adaptive balance exploration exploitation demanded fully-ﬂedged procedure. second still important focus question whether meta-rl give rise learning gains efﬁciency capitalizing task structure. order examine questions performed four experiments focusing bandit tasks additional experiments focusing markov decision problems. experiments employ common methods minor implementational variations. experiments agent architecture centers recurrent neural network feeding soft-max output representing discrete actions. detailed below parameters network core well architectural details varied across experiments however important emphasize comparisons speciﬁc architectures outside scope paper. main illustrate validate meta-rl framework general way. experiments used high-level task setup previously described training testing organized ﬁxed-length episodes involving task randomly sampled predetermined task distribution lstm hidden state initialized beginning episode. task-speciﬁc inputs table list hyperparameters. coefﬁcient entropy regularization loss; exps. annealed course training. coefﬁcient value function loss reward last action current time step current observation. exp. bandits independent arms exp. bandits dependent arms exp. bandits dependent arms exp. restless bandits exp. two-step task exp. learning abstract task structure action outputs described conjunction individual experiments. experiments except speciﬁed input included scalar indicating reward received preceding time-step well one-hot representation action sampled time-step. reinforcement learning conducted using advantage actor-critic algorithm detailed mnih mirowski details training including entropy regularization combined policy value estimate loss closely follow methods detailed mirowski exception experiments used single thread unless otherwise noted. full listing parameters refer table figure advantage actor-critic recurrence. architectures reward last action additional inputs lstm. non-bandit environments observation also lstm either one-hot passed encoder model followed fully connected layer units relu non-linearity. details mirowski bandit experiments current time step also input. policy; value function. distributed multi-threaded asynchronous version advantage actor-critic algorithm single threaded. architecture used experiments convolutional-lstm architecture used experiment stacked lstm architecture convolutional encoder used experiments initial setting evaluating meta-rl studied series bandit problems. except limited bandit environments intractable compute bayesian-optimal strategy. demonstrate recurrent system trained bandit environments drawn i.i.d. given distribution environments produces bandit algorithm performs well problems drawn distribution certain extent generalizes related distributions. thus meta-rl learns prior-dependent bandit algorithm. speciﬁc bandit instantiation general meta-rl procedure described section deﬁned follows. training distribution bandit environments. meta-rl system trained sequence bandit environments episodes. start episode lstm state reset bandit task sampled. bandit task deﬁned distributions rewards sampled. agent plays bandit environment certain number trials trained maximize observed rewards. training agent’s policy evaluated bandit tasks drawn test distribution either slight modiﬁcation evaluate resulting performance learned bandit algorithm cumulative regret measure loss suffered playing sub-optimal arms. writing expected reward bandit environment maxa optimal arm) optimal expected reward deﬁne cumulative regret chosen time experiment also depends report performance either terms cumulative ﬁrst consider simple two-armed bandit task examine behavior meta-rl conditions theoretical guarantees exist general purpose algorithms apply. distributions independent bernoulli distributions parameters sampled independently uniformly denote corresponding distribution independent bandit environments beginning episode bandit task sampled held constant trials. training lasted episodes. network given input last reward last action taken trial number subsequently producing action next trial training evaluated episodes learning rate zero across model instances randomly sampled learning rate discount following mnih ﬁgures plotted average runs randomly sampled hyperparameter settings agents selected ﬁrst half evaluation episodes performance plotted second half. measured cumulative expected regret across episode comparing several algorithms tailored independent bandit setting gittins indices thompson sampling model simulations conducted pymabandits toolbox custom matlab scripts. shown figure meta-rl outperforms thompson sampling although performs less well compared gittins verify critical importance providing reward information lstm removed input leaving inputs before. expected performance chance levels bandit tasks. figure performance independentcorrelated-arm bandits. report performance cumulative expected regret test episodes averaged hyperparameters agent-task conﬁguration determined based performance separate test episodes. lstm trained evaluated bandits independent arms compared theoretically optimal models. single agent playing medium difﬁculty task distribution suboptimal pulls trials depicted episodes. lstm trained evaluated bandits dependent uniform arms trained medium bandit tasks tested easy trained medium tested hard task cumulative regret possible combinations training testing environments emphasized property meta-rl gives rise learned algorithm exploits consistent structure training distribution. order garner empirical evidence point tested agent ﬁrst experiment structured bandit task. speciﬁcally trained system two-arm bandits reward distributions correlated. setting unlike studied previous section experience either provides information other. standard bandit algorithms including thompson sampling perform suboptimally setting designed exploit correlations. cases possible tailor algorithms speciﬁc structures extensive problem-speciﬁc analysis typically required. approach aims learn structure-dependent bandit algorithm directly experience target bandit domain. consider bernoulli distributions parameters arms correlated sense consider several training test distributions. uniform means easy means similarly call medium hard denote corresponding induced distributions bandit environments. addition also considered independent uniform distribution independently. agents trained tested distributions bandit environments validation names given task distributions results show easy task easier learn medium easier hard compatible general notion hardness bandit problem inversely proportional difference expected reward optimal sub-optimal arms. note withholding reward input lstm resulted chance performance even easiest bandit task expected. figure reports results possible training-testing regimes. observing cumulative expected regrets make following observations agents trained structured environments develop prior knowledge used effectively tested structured distributions performing comparably gittins superiorly compared agents trained independent arms structured tasks test agent trained independent rewards learned exploit reward correlations useful structured tasks. conversely previous training structured distribution hurts performance agents tested independent distribution makes sense training correlated arms produce policy relies speciﬁc reward structure thereby impacting performance problems structure exists. iii) whilst previous results emphasize point meta-rl gives rise separate learnt algorithm implements prior-dependent bandit strategies results also provide evidence generalization beyond exact training distribution encountered example agents trained distributions perform well tested much wider structured distribution further evidence suggests generalization training easier tasks testing hardest task similar even marginally superior performance compared training hard distribution itself. contrast training hard distribution results relatively poor generalization structured distributions suggesting training purely hard instances result learned algorithm constrained prior knowledge perhaps difﬁculty solving original problem. previous experiment agent could outperform standard bandit algorithms making learned dependencies arms. however could always choosing believes highest-paying arm. next examine problem information gained paying short-term reward cost. similar problems examined providing challenge standard bandit algorithms contrast humans animals make decisions sacriﬁce immediate reward information gain experiment agent trained -armed bandits strong dependencies arms. arms deterministic payouts. nine non-target arms reward target reward meanwhile always informative target indexed times reward thus payouts ranged episode index target randomly assigned. ﬁrst trial episode agent could know target informative returned expected reward every target returned expected reward choosing informative thus meant foregoing immediate reward compensation valuable information. episodes steps long. again reward previous trial provided additional observation agent. facilitate learning encoded -hot format. results shown figure agent learned optimal long-run strategy sampling informative once despite short-term cost using resulting information exploit high-value target arm. thompson sampling supplied true prior searched potential target arms exploited target found. performed worse sampled every even target found early. figure learned procedure pays immediate cost gain information improve long-run returns. task lower-paying provides perfect information arms highest-paying. remaining nine arms intermediate reward. index informative ﬁxed episodes index highest-paying randomized episodes. ﬁrst trial trained agent samples informative arm. subsequent trials agent uses information gained deterministically exploit highest-paying arm. thompson sampling able take advantage dependencies arms. previous experiments considered stationary problems agent’s actions yielded information task parameters remained ﬁxed throughout episode. next consider bandit problem reward probabilities change course episode different rates change different episodes. perform well agent must track best also infer volatility episode adjust learning rate accordingly. environment learning rates higher environment changing rapidly past information becomes irrelevant quickly tested whether meta-rl would learn ﬂexible policy using two-armed bernoulli bandit task reward probabilities value changed slowly episodes quickly high episodes. agent knowing type episode except reward history within episode. figure shows example high episodes. reward magnitude ﬁxed episodes steps long. thompson sampling implemented comparison. conﬁdence bound term parameter selected empirically good performance data set. thompson sampling’s posterior update included knowledge gaussian random walk ﬁxed volatility episodes. previous experiment meta-rl achieved lower regret test thompson sampling rescorla-wagner learning rule best ﬁxed learning rate test whether agent adjusted effective learning rate match environments different volatility levels models agent’s behavior concatenating episodes blocks block consisted high episodes. considered four different models encompassing different combinations three parameters learning rate softmax inverse temperature lapse rate account unexplained choice variance related estimated value economides model included included included included three. parameters estimated separately block episodes. models free ﬁxed respectively. model comparison bayesian information criterion indicated meta-rl’s behavior better described model different learning rates block model ﬁxed learning rate across blocks. control performed model comparison behavior produced best agent ﬁnding beneﬁt allowing different learning rates across episodes models parameter estimates meta-rl’s behavior strongly related volatility episodes indicating meta-rl adjusted learning rate volatility episode whereas model ﬁtting behavior simply recovered ﬁxed parameters figure learned procedure adapts learning rate environment. agents trained two-armed bandits perfectly anti-correlated bernoulli reward probabilities example episodes shown. changed within episode fast poisson jump rate high episodes slow rate episodes. trained lstm agent outperformed thompson sampling rescorla-wagner learner ﬁxed learning rate models maximum likelihood behavior behavior lstm. models including learning rate could vary episodes outperformed models without free parameters lstm’s data r-w’s data. addition lapse parameter improved model lstm’s data suggesting algorithm implemented lstm exactly rescorla-wagner. lstm’s r-w’s estimated learning rate higher volatile episodes. small jitter added visualize overlapping points. foregoing experiments focused bandit tasks actions affect task’s underlying state. turn mdps actions inﬂuence state. begin task derived neuroscience literature turn task originally studied context animal learning requires learning abstract task structure. previous experiments focus examining meta-rl adapts invariances task structure. wrap reviewing experiment recently reported related paper demonstrates meta-rl scale large-scale navigation tasks rich visual inputs. examine meta-rl setting widely used neuroscience literature distinguish contribution different systems viewed support decision making speciﬁcally paradigm known two-step task developed dissociate model-free system caches values actions states q-learning; sutton barto model-based system learns internal model environment evaluates value actions time decision-making look-ahead planning interest whether meta-rl would give rise behavior emulating model-based strategy despite model-free algorithm train system weights. used modiﬁed version two-step task designed bolster utility model-based model-free control task’s structure diagrammed figure ﬁrst-stage state action leads second-stage states probability respectively action leads probabilities second-stage state yielded reward probability yielded reward probability identity higher-valued state assigned randomly episode. thus expected values ﬁrst-stage actions either three states represented one-hot vectors transition model held constant across episodes i.e. expected value second stage states changed episode episode. applied conventional analysis used neuroscience literature dissociate model-free model-based control focuses stay probability probability ﬁrst-stage action selected trial following second-stage reward trial function whether trial involved common transition rare transition standard interpretation model-free control predicts main effect reward first-stage actions tend repeated followed reward regardless transition type actions tend repeated followed non-reward contrast model-based control predicts interaction reward transition type reﬂecting goal-directed strategy takes transition structure account. intuitively receive second-stage reward following rare transition maximize chances getting reward next trial based knowledge transition structure optimal ﬁrst stage action results stay-probability analysis performed agent’s choices show pattern conventionally interpreted implying operation model-based control previous experiments reward information withheld level network input performance chance levels. interpreted following standard practice neuroscience behavior model experiment reﬂects surprising effect training model-free gives rise behavior reﬂecting model-based control. hasten note different interpretations observed pattern behavior available point return below. however notwithstanding caveat results present experiment provide illustration point learning procedure emerges meta-rl differ starkly original algorithm used train network weights takes form exploits consistent task structure. ﬁnal experiment conducted took step towards examining scalabilty meta-rl studying task involves rich visual inputs longer time horizons sparse rewards. additionally experiment studied meta-learning task requires system tune abstract task structure series objects play deﬁned roles system must infer. task adapted classic study animal behavior conducted harlow trial original task harlow presented monkey visually contrasting objects. covered small well containing morsel food; covered empty well. animal chose freely objects could retrieve food reward present. stage hidden left-right positions objects randomly reset. trial began animal choosing freely. process continued number trials using objects. completion trials entirely unfamiliar objects substituted original process began again. importantly within block trials object chosen consistently rewarded consistently unrewarded. harlow observed that substantial practice monkeys displayed behavior reﬂected understanding task’s rules. objects presented monkey’s ﬁrst choice necessarily arbitrary. observing outcome ﬁrst choice monkey ceiling thereafter always choosing rewarded object. figure three-state modeled two-step task states actions. trials start state transition probabilities taking actions depicted graph. result expected rewards predictions choice probabilities given either model-based strategy model-free strategy speciﬁcally model-based strategies take account transition probabilities would predict interaction amount reward received last trial transition observed. agent displays perfectly model-based proﬁle given reward input. anticipated meta-rl give rise pattern abstract one-shot learning. order test this adapted harlow’s paradigm visual ﬁxation task follows. pixel input represented simulated computer screen beginning trial display blank except small central ﬁxation cross agent selected discrete left-right actions shifted view approximately degrees corresponding direction small momentum effect completion trial required performing tasks saccading central ﬁxation cross followed saccading correct image. agent held ﬁxation cross center ﬁeld view minimum four time steps received reward ﬁxation cross disappeared images drawn randomly imagenet dataset resized appeared left right side display agent’s task select images rotating center image aligned center visual ﬁeld view images selected images disappeared intertrial interval time-steps ﬁxation cross reappeared initiating next trial. episode contained maximum trials steps. following mirowski implemented action repeat meaning selecting image took minimum three independent decisions completed ﬁxation. noted however rotational position agent limited; degree rotations could occur simulated computer screen subtended degrees. although imagenet images chosen beginning episode images re-used across trials within episode though randomly varying left-right placement similar objects harlow’s experiment. experiment image arbitrarily chosen rewarded image throughout episode. selection image yielded reward image yielded reward test learning rate zero imagenet images drawn separate held-out never presented training. figure learning abstract task structure visually rich environment. a-c) example single trial beginning central ﬁxation followed images random left-right placement. average performance seeds training. maximum expected performance indicated black dashed line. performance episode random seeds decreasing order performance. probability selecting rewarded image function trial number single stacked lstm agent range training durations nature task requires one-shot image-reward memory together maintenance information relatively long timescale assessed performance convolutional-lstm architecture receives reward action additional input also convolutional-stacked lstm architecture used navigation task discussed agent performance illustrated figure d-f. whilst single lstm agent relatively successful solving task stacked-lstm variant exhibited much better robustness. random seeds best hyperparameter performed ceiling compared single lstm. like monkeys harlow’s experiment networks converge optimal policy agent successfully ﬁxate begin trial starting second trial episode invariably selects rewarded image regardless image selected ﬁrst trial. reﬂects impressive form one-shot learning reﬂects implicit understanding task structure observing trial outcome agent binds complex unfamiliar image speciﬁc task role. experiments reported elsewhere conﬁrmed recurrent system also able solve substantially difﬁcult version task. task image randomly designated either rewarding item selected unrewarding item avoided presented every trial episode image presented novel every trial. experiments using harlow task demonstrate capacity meta-rl operate effectively within visually rich environment relatively long time horizons. consider related experiments recently reported within navigation domain discuss recast examples meta-rl attesting scaleability principle typical settings pose challenging problems dynamically changing sparse rewards. figure view i-maze showing goal object alcoves following initial exploration agent repeatedly goes goal performance stacked lstm feedforward architectures episode averaged across hyperparameters. following initial goal discovery value function occurs well advance agent seeing goal hidden alcove. figure used permission mirowski speciﬁcally consider setting environment layout ﬁxed goal changes location randomly episode although layout relatively simple labyrinth environment richer ﬁnely discretized resulting long time horizons; trained agent takes approximately steps reach goal ﬁrst time given episode. results show stacked lstm architecture receives reward action additional inputs equivalent used harlow experiment achieves near-optimal behavior showing one-shot memory goal location initial exploratory period followed repeated exploitation evidenced substantial decrease latency reach goal ﬁrst time compared subsequent visits notably feedforward network receives single image observation unable solve task whilst interpreted mirowski provides clear demonstration effectiveness meta-rl separate algorithm capability one-shot learning emerges training ﬁxed incremental algorithm meta-rl viewed allowing agent infer optimal value function following initial exploration additional lstm providing information currently relevant goal location lstm outputs policy extended timeframe episode. taken together meta-rl allows base model-free algorithm solve challenging problem might otherwise require fundamentally different approaches approach recently extended santoro demonstrated utility leveraging external memory structure. idea crossing meta-learning reinforcement learning previously discussed schmidhuber work appears introduced term meta-rl differs involve neural network implementation. recently however surge interest using neural networks learn optimization procedures using range innovative meta-learning techniques recent work chen particularly close spirit work presented here viewed treating case inﬁnite bandits using meta-learning strategy broadly analogous pursued. present research also bears close relationship different body recent work framed terms meta-learning. number studies used deep train recurrent neural networks navigation tasks structure task varies across episodes ﬁnal experiment presented above drawn example. extent experiments involve ingredients deep meta-rl neural network memory trained series interrelated tasks almost certain involve kind meta-learning described present work. related work provides indication meta-rl fruitfully applied larger scale problems ones studied experiments. importantly indicates ingredient scaling approach incorporate memory mechanisms beyond inherent unstructured recurrent neural networks work part suggests untapped potential deep recurrent agents meta-learn quite abstract aspects task structure discover strategies exploit structure toward rapid ﬂexible adaptation. completion present research closely related work reported duan like duan colleagues deep train recurrent network series interrelated tasks result network dynamics learn second procedure operates faster time-scale original algorithm. compare performance learned procedures conventional algorithms number domains including bandits navigation. important difference parallel work former’s primary focus relatively unstructured task distributions main interest contrast structured task distributions setting system learn biased therefore efﬁcient procedure exploits regular task structure. perspectives regard nicely complementary. current challenge artiﬁcial intelligence design agents adapt rapidly tasks leveraging knowledge acquired previous experience related activities. present work reported initial explorations believe promising avenue toward goal. deep meta-rl involves combination three ingredients deep algorithm train recurrent neural network training includes series interrelated tasks network input includes action selected reward received previous time interval. result emerges naturally setup rather specially engineered recurrent network dynamics learn implement second procedure independent potentially different algorithm used train network weights. critically learned algorithm tuned shared structure training tasks. sense learned algorithm builds domain-appropriate biases allow operate greater efﬁciency general-purpose algorithm. bias effect particularly evident results experiments involving dependent bandits system learned take advantage task’s covariance structure; study harlow’s animal learning task recurrent network learned exploit task’s structure order display one-shot learning complex novel stimuli. experiments illustrated point system trained using model-free algorithm develop behavior emulates model-based control. comments result warranted. noted presentation simulation results pattern choice behavior displayed network considered cognitive neuroscience literatures reﬂecting model-based control tree search. however remarked recent work pattern arise model-free system appropriate state representation indeed suspect network fact operates. however ﬁndings suggest explicitly model-based control mechanism emerge similar system trained diverse tasks. particular ilin showed recurrent networks trained random mazes approximate dynamic programming procedures time stressed consider important aspect deep meta-rl yields learned algorithm capitalizes invariances task structure. result faced widely varying still structured environments deep meta-rl seems likely generate procedures occupy grey area model-free model-based two-step decision problem studied section derived neuroscience believe deep meta-rl important implications arena notion meta-rl discussed previously neuroscience narrow sense according meta-learning adjusts scalar hyperparameters learning rate softmax inverse temperature recent work shown deep meta-rl account wider range experimental observations providing integrative framework understanding respective roles dopamine prefrontal cortex biological reinforcement learning. would like thank following colleagues useful discussion feedback nando freitas david silver koray kavukcuoglu daan wierstra demis hassabis matt hoffman piotr mirowski andrea banino ritter neil rabinowitz peter dayan peter battaglia alex lerchner lillicrap greg wayne. marcin andrychowicz misha denil sergio gomez matthew hoffman david pfau schaul nando freitas. learning learn gradient descent gradient descent. arxiv preprint arxiv. peter auer nicolo cesa-bianchi paul fischer. finite-time analysis multiarmed bandit problem. deng dong richard socher li-jia fei-fei. imagenet large-scale hierarchical image database. computer vision pattern recognition cvpr ieee conference pages ieee duan john schulman chen peter bartlett ilya sutskever pieter abbeel. fast reinforcement learning slow reinforcement learning. arxiv preprint arxiv. http//arxiv. marcos economides kurth-nelson annika lübbert marc guitart-masip raymond dolan. modelbased reasoning humans becomes automatic training. plos computational biology alex graves greg wayne malcolm reynolds harley danihelka agnieszka grabska-barwi´nska sergio gómez colmenarejo edward grefenstette tiago ramalho john agapiou hybrid computing using neural network dynamic external memory. nature roman ilin robert kozma paul werbos. efﬁcient learning cellular simultaneous recurrent neural networks-the case maze navigation problem. ieee international symposium approximate dynamic programming reinforcement learning pages ieee jaderberg volodymir mnih wojciech czarnecki schaul joel leibo david silver koray kavukcuoglu. reinforcement learning unsupervised auxiliary tasks. arxiv preprint arxiv. http//arxiv.org/abs/.. emilie kaufmann nathaniel korda rémi munos. thompson sampling asymptotically optimal ﬁnite-time analysis. algorithmic learning theory international conference pages mehdi khamassi stéphane lallée pierre enel emmanuel procyk peter dominey. robot cognitive control neurophysiologically inspired reinforcement learning model. frontiers neurorobotics kunikazu kobayashi hiroyuki mizoue takashi kuremoto masanao obayashi. meta-learning method based temporal difference error. international conference neural information processing pages springer yann lecun yoshua bengio geoffrey hinton. deep learning. nature daeyeol xiao-jing wang. mechanisms stochastic decision making primate frontal cortex single-neuron recording circuit modeling. neuroeconomics decision making brain pages jitendra malik. learning optimize. arxiv preprint arxiv. piotr mirowski razvan pascanu fabio viola hubert soyer andy ballard andrea banino misha denil ross goroshin laurent sifre koray kavukcuoglu dharshan kumaran raia hadsell. learning navigate complex environments. arxiv preprint arxiv. http//arxiv.org/abs/. volodymyr mnih adrià puigdomènech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. proc. int’l conf. machine learning icml danil prokhorov feldkamp ivan tyukin. adaptive behavior ﬁxed weights overview. proceedings ieee international joint conference neural networks pages robert rescorla allan wagner theory pavlovian conditioning variations effectiveness reinforcement nonreinforcement. classical conditioning current research theory russo benjamin roy. learning optimize information-directed sampling. advances adam santoro sergey bartunov matthew botvinick daan wierstra timothy lillicrap. meta-learning memory-augmented neural networks. proceedings international conference machine learning pages david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser mastering game deep neural networks tree search. nature david silver hado hasselt matteo hessel schaul arthur guez harley gabriel dulac-arnold david reichert neil rabinowitz andre barreto thomas degris. predictron end-to-end learning planning. submitted int’l conference learning representations iclr jane wang kurth-nelson dhruva tirumala joel leibo hubert soyer dharshan kumaran matthew botvinick. meta-reinforcement learning bridge prefrontal dopaminergic function. cosyne abstracts jason weston sumit chopra antoine bordes. memory networks. arxiv preprint arxiv. steven younger peter conwell neil cotter. fixed-weight on-line learning. ieee transactions", "year": 2016}