{"title": "Multiple Source Domain Adaptation with Adversarial Training of Neural  Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "While domain adaptation has been actively researched in recent years, most theoretical results and algorithms focus on the single-source-single-target adaptation setting. Naive application of such algorithms on multiple source domain adaptation problem may lead to suboptimal solutions. As a step toward bridging the gap, we propose a new generalization bound for domain adaptation when there are multiple source domains with labeled instances and one target domain with unlabeled instances. Compared with existing bounds, the new bound does not require expert knowledge about the target distribution, nor the optimal combination rule for multisource domains. Interestingly, our theory also leads to an efficient learning strategy using adversarial neural networks: we show how to interpret it as learning feature representations that are invariant to the multiple domain shifts while still being discriminative for the learning task. To this end, we propose two models, both of which we call multisource domain adversarial networks (MDANs): the first model optimizes directly our bound, while the second model is a smoothed approximation of the first one, leading to a more data-efficient and task-adaptive model. The optimization tasks of both models are minimax saddle point problems that can be optimized by adversarial training. To demonstrate the effectiveness of MDANs, we conduct extensive experiments showing superior adaptation performance on three real-world datasets: sentiment analysis, digit classification, and vehicle counting.", "text": "zhao†∗ shanghang zhang‡∗ guanhang joão costeira josé moura‡ geoffrey gordon† †machine learning department carnegie mellon university pittsburgh ‡department electrical computer engineering carnegie mellon university pittsburgh robotics institute carnegie mellon university pittsburgh department electrical computer engineering instituto superior técnico lisbon portugal domain adaptation actively researched recent years theoretical results algorithms focus single-source-single-target adaptation setting. naive application algorithms multiple source domain adaptation problem lead suboptimal solutions. step toward bridging propose generalization bound domain adaptation multiple source domains labeled instances target domain unlabeled instances. compared existing bounds bound require expert knowledge target distribution optimal combination rule multisource domains. interestingly theory also leads efﬁcient learning strategy using adversarial neural networks show interpret learning feature representations invariant multiple domain shifts still discriminative learning task. propose models call multisource domain adversarial networks ﬁrst model optimizes directly bound second model smoothed approximation ﬁrst leading data-efﬁcient task-adaptive model. optimization tasks models minimax saddle point problems optimized adversarial training. demonstrate effectiveness mdans conduct extensive experiments showing superior adaptation performance three real-world datasets sentiment analysis digit classiﬁcation vehicle counting. success machine learning algorithms partially attributed rich datasets abundant annotations unfortunately collecting annotating large-scale training data prohibitively expensive time-consuming. solve limitations different labeled datasets combined build larger synthetic training data generated explicit inexpensive annotations however possible shift training test samples learning algorithms based cheaper datasets still suffer high generalization error. domain adaptation focuses problems establishing knowledge transfer labeled source domain unlabeled target domain exploring domain-invariant structures representations bridge theoretical results algorithms proposed. recently algorithms based deep neural networks produce breakthrough performance learning transferable features theoretical results algorithms respect focus single-source-single-target adaptation setting however many application scenarios labeled data available come multiple domains different distributions. result naive application single-source-single-target algorithms lead suboptimal solutions. problem calls efﬁcient technique multiple source domain adaptation. paper theoretically analyze multiple source domain adaptation problem propose adversarial learning strategy based theoretical results. speciﬁcally prove generalization bound domain adaptation multiple source domains labeled instances target domain unlabeled instances. theoretical results build seminal theoretical model domain adaptation introduced ben-david divergence measure known h-divergence proposed measure distance distributions based given hypothesis space result generalizes bound case multiple source domains. bound interesting interpretation reduces source domain. technically derive bound ﬁrst proposing generalized h-divergence measure sets distributions multi-domains. prove bound target risk bounding empirical source risks using tools concentration inequalities theory compared existing bounds bound require expert knowledge target domain distribution optimal combination rule multiple source domains results also imply always beneﬁcial naively incorporate source domains training verify true experiments. interestingly bound also leads efﬁcient implementation using adversarial neural networks. implementation learns domain invariant task discriminative feature representations multiple domains. speciﬁcally propose models using neural networks rich function approximators instantiate generalization bound derive proper transformations models viewed computationally efﬁcient approximations generalization bound goal optimize parameters networks order minimize bound. ﬁrst model optimizes directly generalization bound second smoothed approximation ﬁrst leading data-efﬁcient task-adaptive model. optimization problem model minimax saddle point problem interpreted zero-sum game participants competing learn invariant features. models combine feature extraction domain classiﬁcation task learning training process. mdans generalization popular domain adversarial neural network reduce source domain. propose stochastic optimization simultaneous updates optimize parameters iteration. demonstrate effectiveness mdans well relevance theoretical results conduct extensive experiments real-world datasets including natural language vision tasks. achieve superior adaptation performances tasks validating effectiveness models. ﬁrst introduce notation used paper review theoretical model domain adaptation source target domain idea h-divergence measure discrepancy distributions. theoretical models exist choose work model distance measure particularly natural interpretation well approximated using samples domains. notations domain represent distribution input space labeling function setting source target domain adaptation denote source target domain respectively. hypothesis binary classiﬁcation function error hypothesis w.r.t. labeling function distribution deﬁned ex∼ds also hypothesis deﬁnition reduces probability disagrees ex∼ds ex∼ds prx∼ds deﬁne risk hypothesis error w.r.t. true labeling function domain true risk empirical risk target domain. h-divergence deﬁned follows deﬁnition hypothesis class instance space collection subsets support hypothesis i.e. distance distributions based a∈ah hypothesis class contains possible measurable functions reduces familiar total variation. given hypothesis class deﬁne symmetric difference w.r.t. operation. optimal hypothesis achieves minimum combined risk source target domains ben-david blitzer proved following generalization bound target risk terms source risk discrepancy source domain target domain theorem hypothesis space c-dimension distance probability least choice samples generalization bound depends optimal combined risk achieved hypothesis intuition large cannot hope successful domain adaptation. notable feature bound empirical discrepancy distance samples usually approximated discriminator distinguish instances domains. generalization bound multiple source domain adaptation section ﬁrst generalize deﬁnition discrepancy function appropriate domains. generalized discrepancy function derive generalization bound multisource domain adaptation. conclude section discussion comparison bound existing generalization bounds multisource domain adaptation refer readers appendix proof details mainly focus discussing interpretations implications theorems. {dsi}k source domains target domain respectively. deﬁne discrepancy function induced measure distance domains {dsi}k follows deﬁnition i.e. minimum risk achieved following lemma holds theorem maxi∈ remark. take closer look generalization bound make small discrepancy measure target domain multiple source domains need small. otherwise cannot hope successful adaptation using labeled instances source domains. case hypothesis performs well source domains target domain. worth pointing second term third term together introduce tradeoff complexity hypothesis class namely restricted second term large discrepancy term small. hand rich expect optimal error small discrepancy measure dh∆h large. ﬁrst term standard source risk term usually appears generalization bounds pac-learning framework later shall upper bound term corresponding empirical risk. discrepancy distance dh∆h reduce inﬁnite hypothesis space ﬁnite space acting ﬁnite samples. theorem follows standard union bound concentration inequalities. equivalently following corollary holds corollary {dsi}k {dsi}k probability least have note multiple source domains increase sample complexity drastically square root term corollary. appears. similarly usually access true error maxi∈ source domains again thm. proved combination concentration inequalities reduction inﬁnite space ﬁnite space along subadditivity function. equivalently following corollary hold corollary {dsi}k source distributions hypothesis class empirical distributions {dsi}k generated i.i.d. samples domain then probability least have remark. thm. nice interpretation term ﬁrst term measures worst case accuracy hypothesis source domains second term measures discrepancy target domain source domains. domain adaptation succeed multiple sources setting expect terms small pick hypothesis based source training errors generalize discrepancy sources target small. third term optimal error hope achieve. hence large hope generalization error small training source domains. last term bounds additional error incur possible bias ﬁnite samples. also worth pointing four terms appearing generalization bound also capture tradeoff using rich hypothesis class limited discussed above using richer hypothesis class ﬁrst third terms bound decrease value second term increase; hand choosing limited hypothesis class decrease value second term incur additional source training errors large simplicity interesting prediction implied thm. performance target domain depends worst empirical error among multiple source domains i.e. always beneﬁcial naively incorporate source domains training. experiment indeed case many real-world problems. comparison existing bounds first easy that upto multiplicative constant bound reduces thm. source domain hence thm. treated generalization thm. blitzer give generalization bound semi-supervised multisource domain adaptation where besides labeled instances multiple source domains algorithm also access fraction labeled instances target domain. although general bound incomparable instructive connections differences them hand multiplicative constants discrepancy measure optimal error bound half blitzer bound leading tighter bound; hand access labeled instances target domain bound expressed relative optimal error rate target domain terms empirical error source domain. finally thanks generalized deﬁnition need manually specify optimal combination vector unknown practice. mansour also give generalization bound multisource domain adaptation assumption target distribution mixture sources target hypothesis represented convex combination source hypotheses. distance measure assumes loss function generalized discrepancy measure also applied losses functions section shall describe neural network based implementation minimize generalization bound derive thm. idea reformulate generalization bound minimax saddle point problem optimize adversarial training. figure mdans network architecture. feature extractor domain classiﬁer task learning combined training process. hard version source achieves minimum domain classiﬁcation error backpropagated gradient reversal; smooth version domain classiﬁcation risks source domains combined backpropagated adaptively gradient reversal. suppose given samples drawn source domains {dsi} contains instance-label pairs. additionally also access unlabeled instances sampled target domain hypothesis class last terms generalization bound ﬁxed; hence hope minimize bound minimizing ﬁrst terms i.e. maximum source training error discrepancy source domains target domain. idea train neural network learn representation following properties indistinguishable source domains target domain; informative enough desired task succeed. note requirements necessary without second property neural network learn trivial random noise representations domains representations cannot distinguished discriminator; without ﬁrst property learned representation necessarily generalize unseen target domain. taking properties consideration propose following optimization problem terms exactly correspond criteria proposed ﬁrst term asks informative feature representation desired task succeed second term captures notion invariant feature representations different domains. inspired ganin gradient reversal layer effectively implement backpropagation. network architecture shown figure. pseudo-code listed alg. notable drawback hard version alg. iteration algorithm updates parameter based gradient domains. data inefﬁcient waste computational resources forward process. improve this approximate function log-sum-exp function frequently used smooth approximation trick smooths objective also provides principled adaptive combine gradients source domains. words says gradient mdan convex combination gradients domains. larger error domain larger combination weight ensemble. summarize algorithm smoothed version alg. note algorithms including hard version smoothed version reduce dann algorithm source domain. evaluate hard soft mdans compare state-of-the-art methods three real-world datasets amazon benchmark dataset sentiment analysis digit classiﬁcation task includes datasets mnist mnist-m svhn synthdigits public largescale image dataset vehicle counting city cameras details network architecture training parameters proposed baseline methods detailed dataset description introduced appendix. domains within dataset consist reviews speciﬁc kind product reviews encoded dimensional feature vectors unigrams bigrams binary labels indicating sentiment. conduct experiments them pick product target domain rest source domains. source domain labeled examples target test examples. training randomly sample number unlabeled target examples source examples mini-batch. implement hard-max soft-max methods according alg. compare three baselines mlpnet marginalized stacked denoising autoencoders dann dann cannot directly applied multiple source domains setting. order make comparison protocols. ﬁrst combine source domains single train using dann denote second protocol train multiple danns separately corresponds source-target pair. among danns report achieving best performance target domain. denote experiment fair comparison models built basic network structure input layer three hidden layers cdann performs slightly better soft-max methods perform close other. hard-max typically slightly worse soft-max. mainly data-efﬁciency hard-max model argue training iterations performance hard-max improved. results verify effectiveness mdans multisource domain adaptation. validate statistical signiﬁcance results non-parametric wilcoxon signed-ranked test task compare soft-max competitors shown table cell corresponds p-value wilcoxon test soft-max methods null hypothesis paired samples mean. p-values soft-max convincingly better methods. following setting combine four popular digits datasets build multisource domain dataset. take mnist-m svhn mnist target domain turn rest sources. source domain labeled images target test examples. compare hard-max soft-max mdans baselines best-single-source. basic network trained source domain without domain adaptation tested target domain. among three models report achieves best performance test set. ii). combine-source. basic network trained combination three source domains without domain adaptation tested target domain. iii). best-single-dann. train danns source-target domain pair test target. again report best score among three. iv). combine-dann. train single dann combination three source domains target-only. basic network trained tested target data. serves upper bound algorithms. mdans baseline methods built basic network structure equal footing. results analysis classiﬁcation accuracy shown table results show naive combination different training datasets sometimes even decrease performance. furthermore observe adaptation svhn dataset hard. case increasing number source domains help. conjecture large dissimilarity svhn data others. combined sources mdans always perform better source-only baseline however directly training dann combination multiple sources leads worse performance compared approach fact strategy even lead worse results source-only baseline surprisingly using single domain sometimes achieve best result. means domain adaptation quality data much important quantity conclusion experiment demonstrates effectiveness mdans multiple source domains available naive combination multiple sources using dann hurt generalization. webcamt public dataset vehicle counting large-scale city camera videos resolution frame rate high occlusion. frames annotated vehicle bounding count divided training testing sets frames respectively. demonstrate effectiveness mdans count vehicles unlabeled target camera adapting multiple labeled source cameras select cameras labeled images evaluations. shown fig. located different intersections city different scenes. among cameras randomly pick cameras take camera target camera cameras sources. compute proxy a-distance source camera target camera approximate divergence them. rank source cameras high choose ﬁrst cameras form source domains. thus proposed methods baselines evaluated different numbers sources implement hard-max soft-max mdans according alg. based basic vehicle counting network compare method baselines basic network without domain adaptation dann implemented basic network. record mean absolute error true count estimated count. results analysis counting error different methods compared table hardmax version achieves lower error dann settings target cameras. soft-max approximation outperforms baselines hard-max settings demonstrating effectiveness smooth adaptative approximation. lowest achieved soft-max means around vehicle miscount frame fig. shows counting results soft-max target cameras source cameras setting. proposed method accurately counts vehicles target camera long time sequences. adding source cameras always help improve performance target camera? answer question analyze counting error vary number source cameras shown fig. curves counting error goes source cameras beginning goes sources added end. phenomenon corresponds prediction implied thm. performance target domain depends worst empirical error among multiple source domains i.e. always beneﬁcial naively incorporate source domains training. illustrate prediction better show newly added camera fig. observing counting error performance target degrade newly added source camera large divergence target camera. number adaptation approaches studied recent years. theoretical aspect several theoretical results derived form upper bounds generalization target error learning source data. keypoint theoretical frameworks estimating distribution shift source target. kifer proposed h-divergence measure similarity domains derived generalization bound target domain using empirical error source domain h-divergence source target. idea later extended multisource domain adaptation corresponding generalization bound developed well. ben-david provide generalization bound domain adaptation target risk generalizes standard bound source risk. work formalizes natural intuition reducing distributions ensuring error source domain justiﬁes many algorithms. based work mansour introduce divergence measure discrepancy distance whose empirical estimate based rademacher complexity theoretical works also studied derives generalization bounds target error taking robustness properties introduced details. following theoretical developments many algorithms proposed instancebased methods feature-based methods parameterbased methods general approach domain adaptation starts algorithms focus linear hypothesis class linear assumption relaxed extended non-linear setting using kernel trick leading reweighting scheme efﬁciently solved quadratic programming recently availability rich data powerful computational resources non-linear representations hypothesis classes increasingly explored line work focuses building common robust feature representations among multiple domains using either supervised neural networks unsupervised pretraining using denoising auto-encoders recent studies shown deep neural networks learn transferable features bousmalis develop domain separation networks extract image representations partitioned subspaces domain private component cross-domain shared component. partitioned representation utilized reconstruct images domains improving performance. reference enables classiﬁer adaptation learning residual function reference target classiﬁer. main-task work limited classiﬁcation problem. ganin propose domain-adversarial neural network learn domain indiscriminate main-task discriminative features. although works generally outperform non-deep learning based methods focus single-source-single-target problem much work rather empirical design without statistical guarantees. hoffman present domain transform mixture model multisource based non-deep architectures difﬁcult scale adversarial training techniques build feature representations indistinguishable source target domains proposed last years speciﬁcally central ideas neural networks powerful function approximators approximate distance measure known h-divergence domains overall algorithm viewed zero-sum two-player game network tries learn feature representations fool network whose goal distinguish representations generated source domain generated target domain. goal algorithm nash-equilibrium game stationary point min-max saddle point problem. ideally equilibrium state feature representations source domain share distributions target domain result better generalization target domain expected training models using labeled instances source domain. derive generalization bound setting multiple source domains labeled instances target domain unlabeled instances. bound interesting interpretation reduces existing bound source domain. following theoretical results propose mdans learn feature representations invariant multiple domain shifts time discriminative learning task. hard soft versions mdans generalizations popular dann case multiple source domains available. empirically mdans outperform state-of-the-art methods three real-world datasets including sentiment analysis task digit classiﬁcation task visual vehicle counting task demonstrating effectiveness multisource domain adaptation.", "year": 2017}