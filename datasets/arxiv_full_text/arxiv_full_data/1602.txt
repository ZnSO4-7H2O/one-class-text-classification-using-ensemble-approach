{"title": "Dependency Recurrent Neural Language Models for Sentence Completion", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Recent work on language modelling has shifted focus from count-based models to neural models. In these works, the words in each sentence are always considered in a left-to-right order. In this paper we show how we can improve the performance of the recurrent neural network (RNN) language model by incorporating the syntactic dependencies of a sentence, which have the effect of bringing relevant contexts closer to the word being predicted. We evaluate our approach on the Microsoft Research Sentence Completion Challenge and show that the dependency RNN proposed improves over the RNN by about 10 points in accuracy. Furthermore, we achieve results comparable with the state-of-the-art models on this task.", "text": "recent work language modelling shifted focus count-based models neural models. works words sentence always considered left-to-right order. paper show improve performance recurrent neural network language model incorporating syntactic dependencies sentence effect bringing relevant contexts closer word predicted. evaluate approach microsoft research sentence completion challenge show dependency proposed improves points accuracy. furthermore achieve results comparable stateof-the-art models task. language models commonly used score sequence tokens according probability occurring natural language. essential building block variety applications machine translation speech recognition grammatical error correction. standard evaluating language model calculate perplexity large corpus. however evaluation assumes output language model probabilistic observed perplexity always correlate downstream task performance. reasons zweig burges proposed sentence completion challenge task pick correct word complete sentence candidates. performance evaluated accuracy thus probabilistic non-probabilistic models syntactic dependency parse sentence. syntactic dependencies bring relevant contexts closer word predicted thus enhancing performance shown gubbins vlachos count-based language models. dependency model published simultaneously another model introduced extend long-short term memory architecture tree-structured network topologies evaluate sentence-level sentiment classiﬁcation semantic relatedness tasks language model. adapting syntactic dependency structure required reset network paths dependency parse tree given sentence maintaining count often token appears paths. furthermore explain incorporate dependency labels features. results show dependency language model proposed outperforms proposed mikolov points accuracy. furthermore improves upon count-based dependency language model gubbins vlachos achieving slightly worse recent state-of-the-art results mnih kavukcuoglu finally make code preprocessed data available facilitate comparisons future work. count-based language models operate assigning probabilities sentences factorizing likelihood n-grams. neural language models embed word lowdimensional vector representation recurrent neural network mikolov suggested recurrent neural networks model long-range dependencies words restricted ﬁxed context length like feedforward neural network hidden representation word position sentence follows ﬁrst order auto-regressive dynamic matrix connecting hidden representation previous word current one-hot index current word matrix containing embeddings words vocabulary nonlinearity typically logistic sigmoid function +exp. time step generates word probability vector next word using output word embedding matrix softmax nonlinearity maximum entropy model mikolov combined rnns maximum entropy model essentially adding matrix directly connects input words’ n-gram context output word probabilities. practice large vocabulary size designing matrix computationally prohibitive. instead hash-based implementation used word context notation make distinction word token position sentence one-hot vector representation note i-th word token breadth-ﬁrst traversal dependency parse tree. training rnns rnns trained using maximum likelihood gradient-based optimization stochastic gradient descent annealed learning rate backpropagation time variant enables sum-up gradients consecutive time steps updating parameters handle long-range temporal dependencies hidden output sequences. loss function cross-entropy generated word distribution target one-hot word distribution involves log-likelihood terms speed-up estimation output word probabilities done using hierarchical softmax outputs i.e. class-based factorization word assigned class corresponding log-likelihood effectively index word among words belonging class experiments binned words found training corpus classes according frequency roughly corresponding square root vocabulary size. dependency rnns designed process sequential data iteratively presenting word generating next word’s probability distribution time step. reset beginning sentence setting values hidden vector zero. dependency parsing generates sentence parse tree single root many leaves unique path root leaf illustrated figure note {wi}i word tokens appearing parse tree sentence. order notation derives breadth-ﬁrst traversal tree unrolls seen different sequence words {wi} starting single root visited takes speciﬁc path parse tree. propose simple transformation algorithm process dependency parse trees. reset independently unroll. detailed next paragraph evaluating log-probability sentence word token appear multiple unrolls log-likelihood counted once. training avoid over-training network word tokens appear unroll word token given weight discount based number unrolls token appears since optimized using updated every time-step contribution word token discounted multiplying learning rate discount factor αiλ. sentence probability dependency given word deﬁne ancestor sequence subsequence words taken subset {wk}i− describing path root node parent example figure ancestors word token binoculars strong. assuming word conditionally independent words outside ancestor sequence given ancestor sequence gubbins vlachos showed probability sentence could written means conditional likelihood word given ancestors needs counted calculation sentence likelihood even though word appear multiple unrolls. modeling sentence using state used generate distribution words represents vector embedding history ancestor words therefore count term computing likelihood sentence. model presented dependency labels. purpose adapted context-dependent handle additional mdimensional label input features features require matrix connects label features word vectors thus yielding dynamical model matrix connects label features output word probabilities. full model becomes follows modiﬁed feature-augmented toolkit adapted handle tree-structured data. speciﬁcally instead sequentially entire training corpus word tokens unrolls sentences books corpus. reset beginning unroll sentence. calculating log-probability sentence contribution word token counted unrolls sentence processed log-probability sentence per-token logprobabilities hash-table. also training corpus consists century novels project gutenberg processing performed using stanford corenlp toolkit test contains sentences completed. sentence consists ground truth impostor sentences speciﬁc word replaced syntactically correct semantically incorrect impostor word. dependency trees generated sentence candidate. split using ﬁrst sentences validation latter sentences test set. during training start annealing learning rate decay factor soon classiﬁcation error validation starts increase. table shows accuracy obtained using simple -dimensional hidden word representation frequency-based word classes notices adding direct word context target word connections enables jump poor performance accuracy test accuracy essentially matching accuracy reported good-turing n-gram language models zweig modelling -grams yields even better results closer accuracy reported rnns best accuracy achieved deprnn combined development test sets used report results previous work best reported results sentence completion challenge achieved log-bilinear models variant neural language models accuracy conjecture superior performance might stem fact lbls like n-grams take account order words context thus model higher-order markovian dynamics simple ﬁrst-order autoregressive dynamics rnns. deprnn proposed ignores left-to-right word order thus likely combination approaches result even higher accuracies. gubbins vlachos developed countbased dependency language model achieving accuracy. finally mikolov report achieved accuracy ensemble rnns without giving details. related work mirowski incorporated syntactic information neural language models using tags additional input lbls obtained small reduction word error rate speech recognition task. similarly bian enriched continuous bag-ofwords model mikolov incorporating morphology tags entity categories -dimensional word embeddings trained gutenberg dataset increasing sentence completion accuracy work incorporating syntax language modeling include chelba pauls klein however none approaches considered neural language models count-based ones. levy goldberg zhao proposed train neural word embeddings using skip-grams cbows dependency parse trees extend approach actual language models evaluate word embeddings word completion tasks. note assume dependency tree supplied prior running limits scope dependency scoring complete sentences next word prediction nevertheless common speech recognition machine translation conventional decoder produce n-best list likely candidate sentences re-score language model. propose similar approach ours learning long short-term memory rnns dependency parse tree network topologies. architectures designed predict next-word probability distributions language model classify input words measure similarity hidden representations relative improvement performance tasks smaller ours probably because lstms better rnns storing long-term dependencies thus beneﬁt form word ordering dependency trees much rnns. similar vein ours micelibarone attardi simply propose enhance rnn-based machine translation permuting order words source sentence match order words target sentence using source-side dependency parsing. outperform word completion tasks. illustrated fig. validation perplexity keeps decreasing monotonically whereas validation accuracy rapidly reaches plateau oscillates. observation conﬁrms that went training epochs change perplexity longer good predictor change word accuracy. presume log-likelihood word distribution training objective crafted precision perplexity reduction happens middle tail word distribution. paper proposed novel language model dependency incorporates syntactic dependencies formulation. evaluated performance sentence completion task showed improves points accuracy achieving results comparable state-of-the-art. work include extending dependency tree language modeling long short-term memory rnns handle longer syntactic dependencies. thank anonymous reviewers valuable feedback. also thanks geoffrey zweig daniel voinea francesco nidito davide gennaro sharing original featureaugmented toolkit microsoft research website insights code well bhaskar mitra milad shokouhi andriy mnih enlighting discussions word embedding sentence completion. references yoshua bengio r´ejean ducharme pascal vincent christian janvin. neural probabilistic language model. journal machine learning research jiang bian tie-yan liu. knowledge-powered deep learning word machine learning knowledge embedding. discovery databases lecture notes computer science volume pages ciprian chelba david engle frederick jelinek victor jimenez sanjeev khudanpur lidia mangu harry printz eric ristad ronald rosenfeld andreas stolcke structure performance dependency language proceedings eurospeech volume model. pages joseph gubbins andreas vlachos. dependency language models sentence completion. proceedings conference empirical methods natural language processing. christopher manning mihai surdeanu john bauer jenny finkel steven bethard david mcclosky. stanford corenlp natural language processing toolkit. proceedings annual meeting association computational linguistics system demonstrations pages valerio miceli-barone giuseppe attardi. nonprojective dependency-based pre-reordering recurrent neural network machine translation. annual meeting association computational linguistics international joint conference asian federation natural language processing. tomas mikolov anoop deoras daniel povey lukas burget cernocky. strategies training large scale neural automatic speech network language models. recognition understanding ieee workshop pages ieee. piotr mirowski sumit chopra suhrid balakrishnan srinivas bangalore. feature-rich continuous language models speech recognition. spoken language technology workshop ieee pages ieee. andriy mnih geoffrey hinton. three graphical models staproceedings tistical language modelling. international conference machine learning page andriy mnih koray kavukcuoglu. learning word embeddings efﬁciently noise-contrastive estimation. c.j.c. burges bottou welling ghahramani k.q. weinberger editors advances neural information processing systems pages curran associates inc. andriy mnih teh. fast simple algorithm training neuproceedings probabilistic language models. international conference machine learning pages adam pauls klein. faster smaller n-gram language models. proceedings annual meeting association computational linguistics human language technologies pages association computational linguistics. adam pauls klein. large-scale syntactic language modeling proceedings annual meettreelets. association computational linguistics long papers-volume pages association computational linguistics. sheng richard socher improved semantic christopher manning. representations tree-structured long short-term memory networks. annual meeting association computational linguistics international joint conference asian federation natural language processing. yinggong zhao shujian huang xinyu jianbing zhang jiajun chen. learning word embeddings dependency relaproceedings asian language protions. cessing geoffrey zweig christopher burges. challenge advancing language modeling. proceedings naacl-hlt workshop ever really replace n-gram model? future language modeling pages association computational linguistics. john platt christopher meek christopher burges ainur yessenalina qiang liu. computational approaches sentence completion. proceedings annual meeting association computational linguistics pages", "year": 2015}