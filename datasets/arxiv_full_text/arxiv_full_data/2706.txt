{"title": "A Distributional Perspective on Reinforcement Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.", "text": "paper argue fundamental importance value distribution distribution random return received reinforcement learning agent. contrast common approach reinforcement learning models expectation return value. although established body literature studying value distribution thus always used speciﬁc purpose implementing risk-aware behaviour. begin theoretical results policy evaluation control settings exposing signiﬁcant distributional instability latter. distributional perspective design algorithm applies bellman’s equation learning approximate value distributions. evaluate algorithm using suite games arcade learning environment. obtain state-of-the-art results anecdotal evidence demonstrating importance value distribution approximate reinforcement learning. finally combine theoretical empirical evidence highlight ways value distribution impacts learning approximate setting. major tenets reinforcement learning states that otherwise constrained behaviour agent maximize expected utility value bellman’s equation succintly describes value terms expected reward exment learning. speciﬁcally main object study random return whose expectation value random return also described recursive equation distributional nature distributional bellman equation states distribution characterized interaction three random variables reward next state-action random return analogy wellknown case call quantity value distribution. although distributional perspective almost bellman’s equation reinforcement learning thus subordinated speciﬁc purposes model parametric uncertainty design risk-sensitive algorithms theoretical analysis contrast believe value distribution central role play reinforcement learning. contraction policy evaluation bellman operator. basing results r¨osler show that ﬁxed policy bellman operator value distributions contraction maximal form wasserstein metric. particular choice metric matters operator contraction total variation kullback-leibler divergence kolmogorov distance. instability control setting. demonstrate instability distributional version bellman’s optimality equation contrast policy evaluation case. speciﬁcally although optimality operator contraction expected value contraction metric distributions. results provide evidence favour learning algorithms model effects nonstationary policies. better approximations. algorithmic standpoint many beneﬁts learning approximate distribution rather approximate expectation. distributional bellman operator preserves multimodality value distributions believe leads stable learning. approximating full distribution also mitigates effects learning nonstationary policy. whole illustrate practical beneﬁts distributional perspective context arcade learning environment modelling value distribution within agent obtain considerably increased performance across gamut benchmark atari games fact achieve stateof-the-art performance number games. results echo veness obtained extremely fast learning predicting monte carlo returns. supervised learning perspective learning full value distribution might seem obvious restrict ourselves mean? main distinction course setting given targets. instead bellman’s equation make learning process tractable; must sutton barto learn guess guess. belief guesswork ultimately carries beneﬁts costs. consider agent interacting environment standard fashion step agent selects action based current state environment responds reward next state. model interaction time-homogeneous markov decision process usual respectively state action spaces transition kernel discount factor reward function work explicitly treat random variable. stationary policy maps state probability distribution action space bellman’s equations return discounted rewards along agent’s trajectory interactions environment. value function policy describes expected return taking action state acting according figure distributional bellman operator deterministic reward function next state distribution policy discounting shrinks distribution towards reward shifts projection step equation unique ﬁxed point optimal value function corresponding optimal policies maxa q∗). view value functions vectors rx×a expected reward function vector. context bellman operator optimality operator operators useful describe expected behaviour popular learning algorithms sarsa q-learning. particular contraction mappings repeated application initial converges exponentially respectively paper take away expectations inside bellman’s equations consider instead full distribution random variable view mapping state-action pairs distributions returns call value distribution. ﬁrst gain understanding theoretical behaviour distributional analogues bellman operators particular less well-understood control setting. reader strictly interested algorithmic contribution choose skip section. distributional equation indicates random variable distributed according without loss generality reader understand sides distributional equation relating distributions independent random variables. distributional equations used reinforcement learning engel morimura among others operations research white inﬁmum taken pairs random variables respective cumulative distributions inﬁmum attained inverse c.d.f. transform random variable uniformly distributed whenever unambiguous; believe greater legibility justiﬁes technical inaccuracy. finally extend metric vectors random variables value distributions using corresponding norm. consider scalar random variable independent need following additional property makes independence assumptions variables. proof later results given appendix. lemma random variables describing partition i.e. exactly random variables. policy evaluation setting interested value function associated given policy analogue value distribution section characterize study behaviour policy evaluation operator emphasize describes intrinsic randomness agent’s interactions environment rather measure uncertainty environment itself. view reward function random vector deﬁne transition operator contraction consider process starting expect limiting expectation {zk} converge exponentially quickly usual show process converges stronger sense contraction implies moments also converge exponentially quickly. using lemma conclude using banach’s ﬁxed point theorem unique ﬁxed point. inspection ﬁxed point must deﬁned assume moments bounded sufﬁcient conclude sequence {zk} converges conclude remark distributional metrics equal; example chung sobel shown contraction total variation distance. similar results derived kullback-leibler divergence kolmogorov distance. difference v)|. however fact contraction variance general contraction centered moment centered moments iterates {zk} still converge exponentially quickly proof extends result r¨osler thus considered ﬁxed policy studied behaviour associated operator understand distributional operators control setting seek policy maximizes value corresponding notion optimal value distribution. optimal value function notion intimately tied optimal policy. however optimal policies attain value case section show distributional analogue bellman optimality operator converges weak sense optimal value distributions. however operator contraction metric distributions general much temperamental policy evaluation operators. believe convergence issues outline symptom inherent instability greedy updates highlighted e.g. tsitsiklis recently harutyunyan optimal policies. begin characterizing mean optimal value distribution. deﬁnition optimal value distribution v.d. optimal policy. optimal value distributions emphasize value distributions expectation optimal must match full distribution return optimal policy. maximization corresponds greedy policy. although policy implicit cannot ignore distributional setting. call distributional bellman optimality operator operator implements greedy selection rule i.e. inspecting lemma might expect con∗. unforverges quickly ﬁxed point tunately convergence neither quick assured reach ﬁxed point. fact best hope pointwise larger convergence even nonstationary optimal value distributions. deﬁnition nonstationary optimal value distribution value distribution corresponding sequence optimal policies. n.o.v.d. theorem measurable suppose ﬁnite. comparing theorem lemma reveals signiﬁcant difference distributional framework usual setting expected return. mean converges exponentially quickly distribution need well-behaved emphasize difference provide number negative results concerning proposition operator contraction. consider following example states unique transition action yields reward optimal action yields equal probability. actions terminal. unique optimal policy therefore unique ﬁxed point consider given figure distance proposition ﬁxed point insufﬁcient guarantee convergence {zk} theorem paints rather bleak picture control setting. remains seen whether dynamical eccentricies highlighted actually arise practice. open question whether theoretically stable behaviour derived using stochastic policies example conservative policy iteration section propose algorithm based distributional bellman optimality operator. particular require choosing approximating distribution. although gaussian case previously considered best knowledge ﬁrst rich class parametric distributions. bution parametrized vmin vmax whose support atoms vmin vmax−vmin sense atoms canonical returns distribution. atom probabilities given parametric model using discrete distribution poses problem bellman update parametrization almost always disjoint supports. analysis section would seem natural minimize wasserstein metric also conveniently robust discrepancies support. however second issue prevents this practice typically restricted learning sample transitions possible wasserstein loss instead project sample bellman update onto support effectively reducing bellman update multiclass classiﬁcation. greedy policy w.r.t. given sample transition compute bellman update immediate neighbours component projected update readily minimized e.g. using gradient descent. call choice distribution loss categorical algorithm. simple one-parameter alternative call bernoulli algorithm. note that algorithms appear unrelated wasserstein metric recent work hints deeper connection. cade learning environment deterministic stochasticity occur number guises state aliasing learning nonstationary policy approximation errors. used training games testing games. study architecture output atom probabilities instead action-values chose vmax −vmin preliminary experiments training games. call resulting architecture categorical dqn. replace train network minimize loss. simple \u0001-greedy policy expected actionvalues; leave future work many ways agent could select actions basis full distribution. rest training regime matches mnih al.’s including target network figure illustrates typical value distributions observed experiments. example three actions lead agent releasing laser early eventually losing game. corresponding distributions reﬂect this assign signiﬁcant probability safe actions similar distributions example helps explain approach successful distributional update keeps separated low-value losing event high-value survival event rather average expectation. surprising fact distributions concentrated values spite ale’s determinism often close gaussians. believe discretizing diffusion process induced began studying algorithm’s performance training games relation number atoms experiment data clear using atoms lead poor behaviour always increases performance; immediately obvious expected saturate network capacity. difference performance -atom version particularly striking latter outperformed games seaquest attain state-of-the-art performance. additional point comparison single-parameter bernoulli algorithm performs better games notably robust asterix. compare algorithm double dueling architecture prioritized replay comparing best evaluation score achieved training. signiﬁcantly outperforms algorithms fact surpasses current state-of-the-art large margin number games notably seaquest. particularly striking fact algorithm’s good performance sparse reward games example venture private eye. suggests value distributions better able propagate rarely occurring events. full results provided appendix. also include appendix comparison averaged seeds showing number games training performance outperforms fullytrained human players. results continue show dramatic improvements representative agent’s average performance. within million frames outperformed fully trained agent games. suggests full million training frames ensuing computational cost unnecessary evaluating reinforcement learning algorithms within ale. recent version contains stochastic execution mechanism designed ward trajectory overﬁtting.speciﬁcally frame environment rejects agent’s selected action probability although mostly robust stochastic execution games performance reduced. score scale normalized respect random agents obtains mean median score improvements respectively conﬁrming beneﬁts beyond deterministic setting. figure learned value distribution episode space invaders. different actions shaded different colours. returns shown agent assigns virtually probability them. interesting outcome experiment method pick stochasticity. pong exhibits intrinsic randomness exact timing reward depends internal registers truly unobservable. clearly reﬂected agent’s prediction consecutive frames value distribution shows modes indicating agent’s belief receive reward. interestingly since agent’s state include past rewards cannot even extinguish prediction receiving reward explaining relative proportions modes. performance -atom agent training games presented last section particularly remarkable given involved none algorithmic ideas present state-of-the-art agents. next asked whether incorporating common hyperparameter choice namely smaller training could lead even better results. speciﬁcally furthermore every million frames work sought complete picture reinforcement learning involves value distributions. found learning value distributions powerful notion allows surpass gains previously made atari without algorithmic adjustments. surprising that policy aims maximize expected return difference performance. distinction wish make learning distributions matters presence approximation. outline possible reasons. reduced chattering. results section highlighted signiﬁcant instability bellman optimality operator. combined function approximation instability prevent policy converging gordon called chattering. believe gradient-based categorical algorithm able mitigate effects effectively averaging different distri† unreal results altogether comparable generated asynchronous setting per-game hyperparameter tuning butions similar conservative policy iteration chattering persists integrated approximate solution. state aliasing. even deterministic environment state aliasing result effective stochasticity. mccallum example showed importance coupling representation learning policy learning partially observable domains. example state aliasing pong agent could exactly predict reward timing. again explicitly modelling resulting distribution provide stable learning target. richer predictions. recurring theme artiﬁcial intelligence idea agent learning multitude predictions distributional approach naturally provides rich auxiliary predictions namely probability return take particular value. unlike previously proposed approaches however accuracy predictions tightly coupled agent’s performance. framework inductive bias. distributional perspective reinforcement learning allows natural framework within impose assumptions domain learning problem itself. work used distributions support bounded treating support hyperparameter allows change optimization problem treating extremal returns equivalent. surprisingly similar value clipping signiﬁcantly degrades performance games. take another example interpreting discount factor proper probability authors argued leads different algorithm. well-behaved optimization. well-accepted divergence categorical distributions reasonably easy loss minimize. explain empirical performance. early experiments alternative losses divergence continuous densities fruitful part divergence insensitive values outcomes. closer minimization wasserstein metric yield even better results presented here. authors acknowledge important role played colleagues deepmind throughout development work. special thanks whye alex graves joel veness guillaume desjardins schaul david silver andre barreto jaderberg mohammad azar georg ostrovski bernardo avila pires olivier pietquin audrunas gruslys stepleton aaron oord; particularly chris maddison comprehensive review earlier draft. thanks also marek petrik pointers relevant literature mark rowland ﬁne-tuning details ﬁnal version. camera-ready copy paper incorrectly reported mean score corrected ﬁgure stands remains higher comparable baselines. median score remains unchanged error evaluation episodes game lasting minutes; comparison results presented episodes minutes standard. previously reported score atlantis million; -minute score believe close achievable maximum time frame. capping minutes brings human-normalized score atlantis mere unfortunately enough noticeably affect mean score whose sensitivity outliers well-documented. references azar mohammad gheshlaghi munos r´emi kappen hilbert. sample complexity reinforcement learning generative model. proceedings international conference machine learning bellemare marc naddaf yavar veness joel bowling michael. arcade learning environment evaluation platform general agents. journal artiﬁcial intelligence research hoffman matthew freitas nando doucet arnaud peters jan. expectation maximization algorithm continuous markov decision processes arbitrary reward. proceedings international conference artiﬁcial intelligence statistics jaderberg mnih volodymyr czarnecki wojciech marian schaul leibo joel silver david kavukcuoglu koray. reinforcement learning unsupervised auxiliary tasks. proceedings international conference learning representations mnih volodymyr kavukcuoglu koray silver david rusu andrei veness joel bellemare marc graves alex riedmiller martin fidjeland andreas ostrovski georg human-level control deep reinforcement learning. nature veness joel bellemare marc hutter marcus chua alvin desjardins guillaume. compress control. proceedings aaai conference artiﬁcial intelligence wang ziyu schaul hessel matteo hasselt hado lanctot marc freitas nando. dueling network architectures deep reinforcement learning. proceedings international conference machine learning morimura tetsuro hachiya hirotaka sugiyama masashi tanaka toshiyuki kashima hisashi. parametric return proceeddensity estimation reinforcement learning. ings conference uncertainty artiﬁcial intelligence morimura tetsuro sugiyama masashi kashima hisashi hachiya hirotaka tanaka toshiyuki. nonparametric return distribution approximation reinforcement learning. proceedings international conference machine learning nair arun srinivasan praveen blackwell alcicek cagdas fearon rory maria alessandro panneershelvam vedavyas suleyman mustafa beattie charles petersen stig massively parallel methods deep reinforcement learning. icml workshop deep learning schaul quan john antonoglou ioannis silver proceedings david. prioritized experience replay. international conference learning representations sutton r.s. modayil delp degris pilarski p.m. white precup horde scalable real-time architecture learning knowledge unsupervised sensorimotor interaction. proceedings international conference autonomous agents multiagents systems tieleman tijmen hinton geoffrey. lecture .-rmsprop divide gradient running average recent magnitude. coursera neural networks machine learning toussaint marc storkey amos. probabilistic inference solving discrete continuous state markov decision processes. proceedings international conference machine learning best knowledge work closest papers studying distributional bellman equation perspective cumulative distribution functions. authors propose parametric nonparametric solutions learn distributions risk-sensitive reinforcement learning. also provide theoretical analysis policy evaluation setting including consistency result nonparametric case. contrast also analyze control setting emphasize distributional equations improve approximate reinforcement learning. variance return extensively studied risk-sensitive setting. note tamar analyze linear function approximation learn variance policy evaluation prashanth ghavamzadeh estimate return variance design risk-sensitive actor-critic algorithm. mannor tsitsiklis provides negative results regarding computation variance-constrained solution optimal control problem. distributional formulation also arises modelling uncertainty. dearden considered gaussian approximation value distribution modelled uncertainty parameters approximation using normal-gamma prior. engel leveraged distributional bellman equation deﬁne gaussian process unknown value function. recently geist pietquin proposed alternative solution problem based unscented kalman ﬁlters. believe much analysis provide here deals intrinsic randomness environment also applied modelling uncertainty. work based number foundational results particular concerning alternative optimality criteria. early jaquette showed moment optimality criterion imposes total ordering distributions achievable deﬁnes stationary optimal policy echoing second part theorem sobel usually cited ﬁrst reference bellman equations higher moments return. chung sobel provides results concerning convergence distributional bellman operator total variation distance. white studies nonstandard criteria perspective optimizing stateaction pair occupancy. number probabilistic frameworks reinforcement learning proposed recent years. planning inference approach embeds return graphical model applies probabilistic inference determine sequence actions leading maximal expected reward. wang considered dual formulation reinforcement learning optimizes stationary distribution subject constraints given transition function particular relationship linear approximation. related dual compress control algorithm veness describes value function learning return distribution using density models. aims work address question left open work whether could design practical distributional algorithm based bellman equation rather monte carlo estimation. speciﬁcally left-hand side equation inﬁmum r.v.’s whose cumulative distributions respectively right-hand side inﬁmum sequences r.v’s whose cumulative distributions faiu faiv respectively. prove upper bound consider c.d.f. thus right-hand side inﬁmum additional constraint must preserve conditional c.d.fs particular another instead having freedom completely reorder mapping reorder within element gist proof theorem consists showing every state time greedy policy w.r.t. mostly optimal. clearly expose steps involved ﬁrst assume unique optimal policy later return general case; denote optimal action notational convenience write gzk. supz∈z γkb. ﬁrst deﬁne states whose values must sufﬁciently close time hence every must eventually lemma allows guarantee existence iteration sufﬁciently many states wellbehaved sense greedy policy states chooses optimal action. call states solved. fact require states solved also successors successors those formalize notion follows deﬁne therefore exists time remains member xki+ states conclude xki+ also. statement follows induction. proof theorem proof policy similar iteration-type results requires care dealing metric possibly inﬁnite state space. deﬁne write similarly overload notation write zk). finally xk+i+ begin using lemma separate transition solved term unsolved term ﬁnite exists ﬁxed uniform convergence result follows. prove uniqueness ﬁxed point selects actions according ordering note optimal value distribution greedy policies denote policy coming ﬁrst ordering unique ﬁxed proposition ﬁxed point insufﬁcient guarantee convergence {zk} provide sketch result. consider single state actions ﬁrst action yields reward either yields equal probability actions optimal. take write received rewards. consider stochastic policy takes action probability return hence take ﬁnally large enough make proof extends considering additional application consider general case multiple optimal policies. expand deﬁnition follows suppose follow nonstationary policy takes ﬁrst step inspection return uniformly distributed interval correspond return value imagine operator alternates depending exact value distribution applied would turn converge nonstationary optimal value distribution. figure wasserstein distance ground truth distribution approximating distributions varying number atoms approximation training target loss function. approximate cumulative distributions representative states cliffwalk. training regime closely follows adam instead rmsprop gradient rescaling. also performed hyperparameter tuning ﬁnal results. speciﬁcally evaluated hyperparameters training games choose values performed best. hyperparameter values considered vmax \u0001adam ./l} minibatch size. found vmax \u0001adam performed best. used step-size value pseudo-code categorical algorithm given algorithm apply bellman update atom separately project nearest atoms original support. transitions terminal state handled lemma proves ﬁxed policy distributional bellman operator γ-contraction therefore converge distribution true distribution returns section empirically validate results cliffwalk domain shown figure dynamics problem match given sutton barto also study convergence distributional bellman operator sampled wasserstein loss categorical projection folcompute ground-truth distribution returns using monte-carlo rollouts state. perform experiments approximating value distribution state discrete distributions. ﬁrst experiment perform supervised learning using either wasserstein loss categorical projection cross-entropy loss. supervised target perform sweeps states ensure approaches converged. second experiment loss functions training target comes one-step distributional bellman operator sampled transitions. vmin vmax sample updates perform times many sweeps state space. fundamentally experiments investigate well training regimes figure show ﬁnal wasserstein distance learned distributions ground-truth distribution vary number atoms. graph shows categorical algorithm indeed minimize wasserstein metric supervised sample bellman setting. also highlights minimizing wasserstein loss stochastic gradient descent general ﬂawed conﬁrming intuition given proposition repeat experiments process converged different values suggesting presence local minima figure provides additional insight sampled wasserstein distance perform poorly. here cumulative densities approximations learned losses different states along safe path cliffwalk. wasserstein converged ﬁxedpoint distribution captures true distribution well. comparison categorical algorithm captures variance true distribution much accurately. figure provide links supplemental videos showing agent training various atari games. figure shows relative performance course training. figure provides table evaluation results comparing state-of-theart agents. figures depict particularly interesting frames. games alien amidar assault asterix asteroids atlantis bank heist battle zone beam rider berzerk bowling boxing breakout centipede chopper command crazy climber defender demon attack double dunk enduro fishing derby freeway frostbite gopher gravitar h.e.r.o. hockey james bond kangaroo krull kung-fu master montezuma’s revenge pac-man name game phoenix pitfall pong private q*bert river raid road runner robotank seaquest skiing solaris space invaders star gunner surround tennis time pilot tutankham venture video pinball wizard yars’ revenge zaxxon figure q*bert left right predicting actions unrecoverably fatal. bottom-left value distribution shows steep consequences wrong actions. bottom-right agent made huge mistake. figure space invaders top-left multi-modal distribution high uncertainty. top-right subsequent frame certain demise. bottom-left clear difference actions. bottom-middle uncertain survival. bottom-right certain success.", "year": 2017}