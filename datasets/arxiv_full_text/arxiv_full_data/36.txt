{"title": "Natural-Parameter Networks: A Class of Probabilistic Neural Networks", "tag": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "stat.ML"], "abstract": "Neural networks (NN) have achieved state-of-the-art performance in various applications. Unfortunately in applications where training data is insufficient, they are often prone to overfitting. One effective way to alleviate this problem is to exploit the Bayesian approach by using Bayesian neural networks (BNN). Another shortcoming of NN is the lack of flexibility to customize different distributions for the weights and neurons according to the data, as is often done in probabilistic graphical models. To address these problems, we propose a class of probabilistic neural networks, dubbed natural-parameter networks (NPN), as a novel and lightweight Bayesian treatment of NN. NPN allows the usage of arbitrary exponential-family distributions to model the weights and neurons. Different from traditional NN and BNN, NPN takes distributions as input and goes through layers of transformation before producing distributions to match the target output distributions. As a Bayesian treatment, efficient backpropagation (BP) is performed to learn the natural parameters for the distributions over both the weights and neurons. The output distributions of each layer, as byproducts, may be used as second-order representations for the associated tasks such as link prediction. Experiments on real-world datasets show that NPN can achieve state-of-the-art performance.", "text": "neural networks achieved state-of-the-art performance various applications. unfortunately applications training data insufﬁcient often prone overﬁtting. effective alleviate problem exploit bayesian approach using bayesian neural networks another shortcoming lack ﬂexibility customize different distributions weights neurons according data often done probabilistic graphical models. address problems propose class probabilistic neural networks dubbed natural-parameter networks novel lightweight bayesian treatment allows usage arbitrary exponential-family distributions model weights neurons. different traditional takes distributions input goes layers transformation producing distributions match target output distributions. bayesian treatment efﬁcient backpropagation performed learn natural parameters distributions weights neurons. output distributions layer byproducts used second-order representations associated tasks link prediction. experiments real-world datasets show achieve state-of-the-art performance. recently neural networks achieved state-of-the-art performance various applications ranging computer vision natural language processing however trained stochastic gradient descent variants known suffer overﬁtting especially training data insufﬁcient. besides overﬁtting another problem comes underestimated uncertainty could lead poor performance applications like active learning. bayesian neural networks offer promise tackling problems principled way. early works include methods based laplace approximation variational inference monte carlo sampling widely adopted lack scalability. recent advances direction seem shed light practical adoption bnn. proposed method based monte carlo estimate lower bound marginal likelihood used infer weights. recently used online version expectation propagation called ‘probabilistic back propagation’ bayesian learning proposed ‘bayes backprop’ viewed extension based ‘reparameterization trick’ recently interesting bayesian treatment called ‘bayesian dark knowledge’ designed approximate teacher network simpler student network based stochastic gradient langevin dynamics although recent methods practical earlier ones several outstanding problems remain addressed methods require sampling either training time test time incurring much higher cost ‘vanilla’ mentioned methods based online involve sampling need compute predictive density integrating parameters computationally inefﬁcient; methods assume gaussian distributions weights neurons allowing ﬂexibility customize different distributions according data done probabilistic graphical models address problems propose natural-parameter networks class probabilistic neural networks input target output weights neurons modeled arbitrary exponential-family distributions instead limited gaussian distributions. input distributions layers linear nonlinear transformation deterministically producing distributions match target output distributions shows providing distributions input corrupting data noise plays role regularization). byproducts output distributions intermediate layers used second-order representations associated tasks. thanks properties exponential family distributions deﬁned corresponding natural parameters learned efﬁciently backpropagation. unlike explicitly propagates estimates uncertainty back forth deep networks. uncertainty estimates layer neurons readily available associated tasks. experiments show information helpful neurons intermediate layers used representations like autoencoders summary main contributions propose class probabilistic neural networks. model combines merits terms computational efﬁciency ﬂexibility customize types distributions different types data. leveraging properties exponential family sampling-free backpropagationcompatible algorithms designed efﬁciently learn distributions weights learning natural parameters. unlike probabilistic models obtains uncertainty intermediate-layer neurons byproducts provide valuable information learned representations. experiments real-world datasets show achieve state-of-the-art performance classiﬁcation regression unsupervised representation learning tasks. exponential family refers important class distributions useful algebraic properties. distributions exponential family form exp{ηt random variable denotes natural parameters vector sufﬁcient statistics normalizer. given type distributions different choices lead different shapes. example univariate gaussian distribution corresponds input multiplies matrix random distribution goes nonlinear transformation outputs another distribution. since three distributions process speciﬁed natural parameters learning prediction network actually operate space natural parameters. example element-wise gamma distributions weights neurons counterpart vanilla network needs twice number free parameters neurons since natural parameters univariate gamma distribution. boldface uppercase letters like denote matrices boldface lowercase letters like vectors. similarly boldface number represents vector matrix identical entries. used denote values neurons layer nonlinear transformation values nonlinear transformation. mentioned above tries learn distributions variables rather variables themselves. hence letters without subscripts denote ‘random variables’ corresponding distributions. subscripts used denote natural parameter pairs similarly subscripts mean-variance pairs. note clarity many operations used implicitly element-wise example square division gamma function logarithm factorial resemble ae’s denoising effect.) input network denotes output targets following text drop subscript clarity. bracket denotes concatenation pairs vectors. ﬁrst introduce linear form general npn. simplicity assume distributions natural parameters section. speciﬁcally factorized distributions weight matrices dij) corresponding natural parameters. assume similar factorized distributions. traditional linear transformation follows output previous layer. deterministic variables exponential-family distributions meaning result also distribution. convenience subsequent computation desirable approximate using another exponentialfamily distribution. matching mean variance. speciﬁcally computing mean denotes element-wise product bijective function maps natural paramed gamma distributions). ters distribution mean variance denote inverse transformation. mean variance obtained natural parameters. computed subsequently facilitate feedforward computation nonlinear transformation described section obtain linearly transformed distribution deﬁned natural parameters element-wise nonlinear transformation imposed. resulting activation distribution po))|v− factorized distribution deﬁned computational challenge computing integrals equation closed-form solutions needed efﬁcient computation. gaussian distribution closedform solutions exist common activation functions like tanh unfortunately case distributions. leveraging convenient form exponential family possible design activation functions integrals non-gaussian distributions also expressed closed form. theorem assume exponential-family distribution exp{ηt vector activadetailed proof provided supplementary material. theorem remains constants make strictly increasing bounded example equation efﬁcient inference algorithms also exist special structure. similarly markov property enables efﬁciently trained end-to-end backpropagation learning fashion space natural parameters. known ﬂexible sense choose different distributions depict different relationships among variables. major drawback scalability especially deep. different stacks relatively simple computational layers learns parameters using backpropagation computationally efﬁcient algorithms pgm. potential best worlds. terms ﬂexibility different types exponential-family distributions chosen weights neurons. using gamma distributions weights neurons leads deep nonlinear version nonnegative matrix factorization bernoulli distribution sigmoid activation resembles bayesian treatment sigmoid belief networks poisson distributions chosen neurons becomes neural analogue deep poisson factor analysis note similar weight decay divergence prior distributions learned distributions weights error regularization chosen prior distributions correspond priors bayesian models learned distributions correspond approximation posterior distributions weights. note generative story assumed weights sampled prior output generated weights. section introduce three variants different properties demonstrate ﬂexibility effectiveness npn. note practice transformed version natural parameters referred proxy natural parameters here instead original ones computational efﬁciency. example gamma distributions γ−dcxc− proxy natural parameters computation rather natural parameters gamma distribution support positive values important member exponential family. corresponding probability density function γ−dcxc− natural parameters proxy natural parameters). assume gamma distributions formed becomes deep nonlinear version nonnegative matrix factorization this note activation zero biases equivalent ﬁnding factorization matrix denotes middle-layer neurons nonnegative entries learned gamma distributions. gamma parameters following algorithm detail algorithm follows linear transformation since gamma distributions assumed here function nonlinear transformation proxy natural parameters gamma distributions mean nonlinearly transformed distribution would obtained equation following theorem closed-form solutions possible constants. using activation function figure predictive distributions dropout npn. shaded regions correspond standard deviations. black curve data-generating function blue curves show mean predictive distributions. stars training data. different gamma distribution support positive values only gaussian distribution also exponential-family distribution describe real-valued random variables. makes natural choice npn. refer variant gaussian distributions weights neurons gaussian npn. details algorithm gaussian follows linear transformation besides support real values another property gaussian distributions mean variance used proxy natural parameters leading identity mapping function cuts computation cost. function compute probabilistic linear transformation equation nonlinear transformation sigmoid activation +exp used equation would activation tanh since tanh relu activation used techniques obtain ﬁrst moments gaussian random variables. full derivation tanh left supplementary material. poisson distribution another member exponential family often used model counts hence text modeling natural assume poisson distributions neurons npn. interestingly design poisson seen neural analogue poisson factor analysis models besides closed-form nonlinear transformation another challenge poisson pair poisson distributions. according central limit theorem section evaluate variants state-of-the-art methods four real-world datasets. matlab implement variants ‘vanilla’ trained dropout baselines theano library mxnet gain insights start regression task predicted mean variance visualized. following generate points dimension uniform distribution interval target outputs sampled function data gaussian figure shows predicted mean variance along mean provided dropout variance diverges farther away training data. npn’s bdk’s predictive distributions accurate enough keep curve inside shaded regions relatively variance. interesting observation training data points become scattered ideally variance start diverging happens npn. however sensitive enough capture dispersion change. another dataset boston housing root mean square error mnist digit dataset consists training images test images. images labeled digits. train models images images validation. networks structure used methods since works best dropout also dropout twice number hidden neurons fair comparison. directly quote results implement using hyperparameters whenever possible. gaussian priors used shown table achieve comparable performance dropout included comparison since supports regression only) gamma slightly outperforms dropout gaussian able achieve lower error rate note gaussian priors achieve error rate result using gaussian mixture priors. reference error rate dropout neurons hidden layer time cost epoch respectively. note matlab. evaluate npn’s ability bayesian treatment avoid overﬁtting vary size training compare test error rates. shown table margin gaussian dropout increases training shrinks. besides verify effectiveness estimated uncertainty split test subsets according npn’s estimated variance sample show accuracy subset figure uncertain lower accuracy indicating estimated uncertainty well calibrated. besides classiﬁcation regression also consider problem unsupervised representation learning subsequent link prediction task. three real-world datasets citeulike-a citeulike-t arxiv used. ﬁrst datasets collected separately citeulike different ways mimic different real-world settings. third arxiv snap datasets citeulike-a consists documents terms links citeulike-t consists documents terms links. last dataset arxiv consists documents terms links. task perform unsupervised representation learning feeding extracted representations bayesian algorithm stacked autoencoder stacked denoising autoencoder variational autoencoder baselines different variants form autoencoders input output targets bag-of-words vectors documents. network structure models please refer supplementary material detailed hyperparameters. major advantage sdae learned representations distributions instead point estimates. since representations contain mean variance call secondorder representations. note although also produces second-order representations variance part simply parameterized multilayer perceptrons npn’s variance naturally computed propagation distributions. -dimensional representations mean variance bayesian algorithm link prediction links among nodes train bayesian links test set. link rank used evaluation metrics. link rank average rank observed links test nodes training nodes. compute every test node report average values. deﬁnition lower link rank higher indicate better predictive performance imply powerful representations. table shows link rank different models. fair comparison also baselines double budget report whichever higher accuracy. treating representations distributions rather points vector space able achieve much lower link rank baselines including variance information. numbers brackets show link rank discard variance information. performance gain variance information veriﬁes effectiveness variance estimated npn. among different variants gaussian seems perform better datasets fewer words like citeulike-t poisson natural choice model text achieves best performance datasets words performance consistent terms link rank verify effectiveness estimated uncertainty plot reconstruction error variance data point citeulike-a figure higher uncertainty often indicates higher reconstruction error also higher variance introduced family models called natural-parameter networks novel class probabilistic combine merits pgm. regards weights neurons arbitrary exponential-family distributions rather point estimates factorized gaussian distributions. ﬂexibility enables richer descriptions hierarchical relationships among latent variables adds another degree freedom customize different types data. efﬁcient sampling-free backpropagation-compatible algorithms designed learning npn. experiments show achieves state-of-the-art performance classiﬁcation regression representation learning tasks. possible extensions would interesting connect arbitrary form fully bayesian deep learning models allowing even richer descriptions relationships among latent variables. also worth noting cannot deﬁned generative models unlike model cannot used support multiple types inference address limitations future work. section provide list exponential-family distributions corresponding activation functions could lead close-form expressions ﬁrst moments namely theorem need constants make exp) monotonically increasing bounded. mentioned paper activation function gamma poisson npn. figure plots function different function similar shape positive half tanh note activation function similar tanh. beta distributions since support domain activation function also case reasonable activation function figure shows function differnt since expect nonlinearly transformed distribution another beta distribution domain function ﬁeld criteria tanh might better activation function tanh. shown ﬁgure different leads different shapes function. rayleigh distributions support positive reals qe−τ proper activation function domain figure plots function different function also similar shape positive half tanh. means derivatives left right hand sides equation respect equal. approaches negative inﬁnity derivatives zero implies constant integration zero. hence equation holds. since mapping function involves gaussian approximation poisson distribution start proving connection gaussian distributions poisson distributions. lemma assume poisson random variable mean variance independent poisson random variables mean have proof. concept moment generating functions deﬁned random variable prove lemma. moment generating function poisson random variable mean variance equation fact independent. equation result using moment generating functions poisson distributions. since exactly moment generating function poisson random variable mean variance deﬁnition have feedforward computation poisson obtaining mean parameters case factorized poisson distribution since direct divergence involves computation inﬁnite series entropy term poisson distribution closed-form solutions available. address problem gaussian distribution proxy poisson distribution mean speciﬁcally gaussian distribution gaussian result mapping. simplicity consider univariate case gaussian distribution approximate divergence section show different models link prediction task. table above result consistent link rank able achieve much higher sdae vae. among different variants gaussian seems perform better datasets fewer words like citeulike-t poisson natural choice model text achieves best performance datasets words link prediction task also split data different compare performance different models. speciﬁcally randomly select observed links training others test set. results consistent original data-splitting method. regression task networks hidden layer containing neurons relu activation gaussian divergence loss isotropic gaussian priors precision weights priors used experiments. preprocessing following pixel values normalized range variants hyperparameters minibatch size number epochs learning rate adadelta used. note since dropout-compatible dropout effective regularization. training testing dropout similar vanilla dropout models preprocess vectors normalizing range although theoretically poisson need preprocessing since poisson distributions naturally model word counts practice normalizing vectors increase stability training predictive performance. simplicity poisson gaussian sigmoid activation used. hyperparameters mnist experiments. proxy natural parameters gamma distributions mean variance nonlinearly transformed distribution would obtained. mentioned before using traditional activation functions like tanh tanh relu could give closed-form solutions integrals. following theorem closed-form solutions possible constants. function similar shape positive half tanh saturation point controlling slope. computation procedure feedforward phase gradients respect parameters derived used backpropagation. note ensure positive entries parameters function log) exp. example parameters learn instead divergence learned distribution prior distribution weights objective function regularize gamma npn. isotropic gaussian prior entry weights compute divergence entry different previous distributions support nonnegative integers. poisson distribution takes form single natural parameter single natural parameter makes learning poisson trickier. text modeling assuming poisson distributions neurons natural model counts words topics documents. assume factorized poisson distribution ensure positive natural parameters gamma distributions weights. interestingly design poisson seen neural analogue poisson factor analysis models computed mean back proxy natural parameters poisson same obviously case minimizing divergence factorized poisson distribution gaussian distribution justiﬁcations) ﬁnding nonlinearly transformed distribution. case gamma traditional activation functions give closed-form solutions. fortunately activation also works poisson npn. speciﬁcally real-valued entries loss could used error note normalized target output error used gaussian npn. besides loss term divergence term equation regularize poisson npn. backpropagation gradients computed update parameters guaranteed nonnegative model still works even directly parameters though resulting models exactly same. experiments poisson bayesian autoencoder feed extracted second-order representations bayesian algorithm link prediction.", "year": 2016}