{"title": "Training Deep Convolutional Neural Networks with Resistive Cross-Point  Devices", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "In a previous work we have detailed the requirements to obtain a maximal performance benefit by implementing fully connected deep neural networks (DNN) in form of arrays of resistive devices for deep learning. This concept of Resistive Processing Unit (RPU) devices we extend here towards convolutional neural networks (CNNs). We show how to map the convolutional layers to RPU arrays such that the parallelism of the hardware can be fully utilized in all three cycles of the backpropagation algorithm. We find that the noise and bound limitations imposed due to analog nature of the computations performed on the arrays effect the training accuracy of the CNNs. Noise and bound management techniques are presented that mitigate these problems without introducing any additional complexity in the analog circuits and can be addressed by the digital circuits. In addition, we discuss digitally programmable update management and device variability reduction techniques that can be used selectively for some of the layers in a CNN. We show that combination of all those techniques enables a successful application of the RPU concept for training CNNs. The techniques discussed here are more general and can be applied beyond CNN architectures and therefore enables applicability of RPU approach for large class of neural network architectures.", "text": "repeated many times convergence criterion met. single fully connected layer inputs neurons connected output neurons forward cycle involve computing vector-matrix multiplication vector length represents activities input neurons matrix size stores weight values pair input output neurons. resulting vector length processed performing non-linear activation single layer also involves vector-matrix multiplication transpose weight matrix vector length represents error calculated output neurons vector length processed using derivative neuron non-linearity passed previous layers. finally update cycle weight matrix updated performing outer operations performed weight matrix implemented crossbar array two-terminal resistive devices rows columns stored conductance values crossbar array form matrix forward cycle input vector transmitted voltage pulses columns resulting vector read current signals rows vector-matrix product computed transpose weight matrix finally update cycle voltage pulses representing vectors simultaneously supplied columns rows. generally considered volume dimensions width height pixels depth channels corresponding different input components kernel. since exists different kernels output becomes volume dimensions passed following layers processing. column vector length stacking different kernels separate rows parameter matrix size formed stores trainable parameters associated single convolutional repeated vector-matrix multiplication equivalent matrix-matrix multiplication matrix dimensions input neuron activities repetition resulting matrix dimensions results corresponding output volume. similarly using transpose parameter matrix backward cycle convolutional layer also expresses matrix-matrix multiplication matrix dimensions error signals corresponding error volume. furthermore transmitted voltage pulses columns results read rows. repetition operation columns completes computations required forward cycle. similarly backward cycle input vector corresponding single column kernels hyperbolic tangent activation functions. first layer kernels second layer kernels. convolutional layer followed subsampling layer implements pooling function non-overlapping pooling windows size output second pooling layer consisting neuron activations feeds fully connected layer consisting neurons connected -way output layer. training performed repeatedly using mini-batch size unity images training dataset constitutes single training epoch. learning rate used throughout training stored separate arrays dimensions first convolutional layers following fully connected layers. name arrays subscript denotes layer’s location used length stochastic stream ∆ch=i change weight value single coincidence event gain factors used stochastic translation columns rows respectively. rpu-baseline ∆ch=i change weight values achieved conductance change devices; therefore order capture device imperfections ∆ch=i assumed cycle-to-cycle deviceto-device variations fabricated devices also show different amounts change positive negative updates device. average value ratio among devices assumed unity achieved global adjustment voltage pulse durations/heights. however device-to-device mismatch unavoidable therefore baseline model table performs poorly achieves test error shown black curve. value significantly higher fp-baseline value also higher error rate achieved model fully connected network final array shown green curve model without analog noise backward cycle infinite bounds reaches respectable test error eliminate noise keeping bounds model follow modest training epoch error rate suddenly increases reaches value similarly eliminate bounds maximal pulse duration represents unity pulse durations scaled accordingly depending values scheme works optimally forward cycle activations including bias term however assumption result ‘>hrk effectively reduces impact noise significantly small error rates >hrk noise management scheme allows propagate error signals arbitrarily small maintains fixed signal noise ratio independent range numbers curve figure beginning training network successfully learns test errors achieved; however right around epoch misleading error signals signal bounding forces network learn unwanted features hence error rate suddenly increases. addressed proper digital design. technique >hrk needs searched element value needs divided multiplied >hrk value. computations require additional comparison division multiplication operations performed digital domain. however given circuits need compute error selectively eliminating various device imperfections different layers. summary results shown figure average test error achieved epochs reported y-axis along error represents standard deviation interval. black four layers achieves test error note improvement comes convolutional layers similar test error achieved model device variations whereas model without device variations fully connected layers remains level. among convolutional layers clear influential compared test errors achieved respectively models device variations eliminated interestingly repeated similar experiments eliminating different layers similar device-to-device variation imbalance parameter ∆ch=i trend observed shown data points. results highlights important device igure average test error achieved epochs various models varying device variations. black data points corresponds experiments device-to-device cycle-toeliminated different layers. data points corresponds experiments device-toeliminated different layers. green points corresponds models multiple devices mapped second convolutional layer using models mapping influential layer repeated devices. indeed multi-device mapping approach reduces test error device mapping cases respectively shown green data points figure number devices used weight effectively reduces device variations factor proportional note -device mapping effectively reduces device variations factor cost increase array dimensions however assuming arrays fabricated equal number columns rows multi-device mapping rectangular matrixes introduce models presented stochastic update scheme length amplification factors equally distributed columns rows values choice values dictated learning rate hyperparameters. models used fixed learning rate ∆ch=i summary results shown figure first models varied value used equally amplification factors interestingly increasing improve network performance whereas reducing boosted performance larger unity single update pulse generated sure. makes updates deterministic earlier clipping. note weight value move single ∆ch=i update cycle. however also note convolutional layers receive receives single stochastic update image. although interaction terms tradeoffs non-trivial clear architecture favors whereas used favored results emphasize importance designing models assume used updates; however possible arbitrarily larger smaller long product satisfies update management scheme values probability generating pulses columns rows roughly order. achieved rescaling amplification factors ratio m>hrk/hrk scheme amplification factors written m\"/. although method yield improvement error rate achieved; hence shows proposed update management scheme columns rows different; i.e. many elements close whereas elements close zero case used updates become row-vise correlated. although unlikely generation pulse result many coincidences along many pulses generated different columns since many values close unity. reduced length updates model reaches test error finally combination management techniques -device mapping second convolutional brings model’s test error level. performance final model almost indistinguishable largest array size limited array size measurement time /hqrs derived considering acceptable noise threshold value dominated thermal noise devices. however small array devices acceptable noise threshold dominated thermal noise hence /hqrs reduced faster", "year": 2017}