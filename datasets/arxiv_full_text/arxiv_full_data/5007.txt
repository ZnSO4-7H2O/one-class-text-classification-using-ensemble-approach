{"title": "Pre-training Neural Networks with Human Demonstrations for Deep  Reinforcement Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "Deep reinforcement learning (deep RL) has achieved superior performance in complex sequential tasks by using a deep neural network as its function approximator and by learning directly from raw images. A drawback of using raw images is that deep RL must learn the state feature representation from the raw images in addition to learning a policy. As a result, deep RL can require a prohibitively large amount of training time and data to reach reasonable performance, making it difficult to use deep RL in real-world applications, especially when data is expensive. In this work, we speed up training by addressing half of what deep RL is trying to solve --- learning features. Our approach is to learn some of the important features by pre-training deep RL network's hidden layers via supervised learning using a small set of human demonstrations. We empirically evaluate our approach using deep Q-network (DQN) and asynchronous advantage actor-critic (A3C) algorithms on the Atari 2600 games of Pong, Freeway, and Beamrider. Our results show that: 1) pre-training with human demonstrations in a supervised learning manner is better at discovering features relative to pre-training naively in DQN, and 2) initializing a deep RL network with a pre-trained model provides a significant improvement in training time even when pre-training from a small number of human demonstrations.", "text": "order deep solving real-world problems need speed learning. method using humans provide demonstrations. using human demonstrations however recently area gain traction possible speeding deep speeding deep reinforcement learning achieved addressing problems trying accomplish feature learning policy learning. work focus addressing problem feature learning order speed learning deep pretraining learn underlying features hidden layers network. bring deep common technique widely used speed training deep learning pre-training network however success technique supervised deep learning attributed large datasets available used pre-train networks. work propose approach speeding deep reinforcement learning algorithms using relatively small amount non-expert human demonstrations. approach starts pre-training deep neural network using human demonstrations supervised learning. similar work shown step would learn imitate human demonstrator however interesting underlying features learned even small amount data. test approach deep q-network asynchronous advantage actor-critic evaluated performance using pong freeway beamrider atari domain. results show speed cases. improvement pong freeway quite large ac’s improvement pong especially large. generality approach means easily incorporated multiple deep algorithms. work directly transfer learning similar transfer learning methods deep learning. training deep neural networks image classiﬁcation yosinski shown transferring deep reinforcement learning achieved superior performance complex sequential tasks using deep neural network function approximator learning directly images. drawback using images deep must learn state feature representation images addition learning policy. result deep require prohibitively large amount training time data reach reasonable performance making difﬁcult deep real-world applications especially data expensive. work speed training addressing half deep trying solve learning features. approach learn important features pre-training deep network’s hidden layers supervised learning using small human demonstrations. empirically evaluate approach using deep q-network asynchronous advantage actor-critic algorithms atari games pong freeway beamrider. results show that pre-training human demonstrations supervised learning manner better discovering features relative pre-training naively initializing deep network pretrained model provides signiﬁcant improvement training time even pre-training small number human demonstrations. recent resurgence neural networks reinforcement learning attributed widespread success deep reinforcement learning uses deep neural networks function approximation besides deep rl’s state-of-the-art results impressive accomplishments ability learn directly images. however order bring success deep virtual environments realworld applications must address lengthy training time required learn policy. deep suffers poor initial performance like classic algorithms since learns tabula rasa addition deep inherently takes longer learn besides learning policy also learns directly images instead using hand-engineered features deep needs learn construct relevant high-level features images. problems consequential features learned existing models allow models learn faster particularly datasets similar. work deep learning classiﬁer source network classiﬁer’s network used initialize agent’s network. existing work pre-training shown improvement pre-training network. however networks much smaller number parameters state dynamics domains network input. approach images domain network input agent also needs learn latent features learning policy. approach using supervised learning pretraining also similar spirit anderson elliott pre-train learning predict state dynamics. instead pre-train network using game’s image frames human demo training data individually labeled action taken human demonstrator. setup similar could derive policy learning demonstration another approach pre-training learn latent features using unsupervised learning deep belief networks although pretraining approach differs falls different machine learning paradigm goals similar approach pre-trained networks learn better using random initialization. recent works leverage humans deep christiano human feedback learn reward function. another recent work similarly pre-trains network human demonstrations however pre-training combines large margin supervised loss temporal difference loss tries closely imitate demonstrator. work cross-entropy loss focus learned features. work silver trained human demonstrations supervised learning used supervised learner’s network initialized rl’s policy network. tested approach single domain used huge amount data train supervised learner data human experts. work ﬁrst provide comparative analysis approach impacts deep reinforcement learning algorithms well approach complement existing deep algorithms human demonstrations available. work also focuses optimizing policy learned humans paper focuses learning underlying features. work shows required huge amount data gain improvements supervised learner still learn important latent features even demonstrated human data non-experts dataset small. agent explores unknown environment taking action action lead agent certain state reward given based action took next state lands goal agent learn maximize expected rek= γkrt+k state time discount factor determines relative importance future immediate rewards. deep q-network recent development deep gained great attention ability generalize solve problems different domains. ﬁrst method deep q-network learns solve atari games directly screen pixels combining q-learning deep convolutional neural network. classic q-learning instead learning value states learns value state-action pairs expected discounted reward determined performing action state thereafter perform optimally. optimal policy deduced following actions maximum value maxπqπ. directly computing value feasible state space large continuous algorithm uses convolutional neural network function approximator estimate function network’s weight parameters. iteration trained minimize mean-squared error q-network target γmaxaq weight parameters target network generated previous iterations. reward uses reward clipping scales scores clipping rewards positive negative rewards unchanged. loss function iteration expressed state-action samples drawn experience replay memory minibatch size experience replay memory along target network reward clipping help stabilize learning. training agent also behaves following \u0001-greedy policy obtain sufﬁcient exploration state space. asynchronous advantage actor-critic drawbacks using experience replay memory algorithm. first store experiences space-consuming could slow learning. second using replay memory limits off-policy algorithms q-learning. asynchronous advantage actor-critic algorithm proposed overcome problems. benchmark deep since surpass dqn’s performance playing atari games also applied continuous control problems. combines actor-critic algorithm deep learning. differs value-based algorithms value function learned actor-critic policy-based policy function value function maintained. policy function called actor takes actions based current policy value function called critic serves baseline evaluate quality action returning state value current state policy policy directly parameterized improved policy-gradient. reduce variance policy gradient advantage function used calculated time step action state expected return time loss function actor-learners running parallel copies environment parameters policy value function. enables algorithm explore different parts environment observations correlated. mimics function experience replay memory efﬁcient space training time. actor-learner pair performs update parameters every tmax actions terminal state reached similar using minibatches done dqn. updates synchronized master learner maintains central policy value function ﬁnal policy upon completion training. deep reinforcement learning divided subtasks feature learning policy learning. deep already quite successful learning tasks parallel. however learning tasks also makes learning deep slow. believe addressing feature learning task would allow deep agents focus learning policy. learn features pre-training deep rl’s network using human demonstrations nonexperts. refer approach pre-trained model. pre-trained model method similar technique transfer deep learning existing previously trained model’s parameters used initialize model solve different problem. case pre-train network multi-classiﬁcation problem using deep learning human demonstrations training data. assume humans provide correct labels actions demonstrated playing game. ﬁrst apply pre-trained model approach refer pre-trained model pmfdqn train multiclass-classiﬁcation deep neural network softmax cross entropy loss function. loss minimized using adam optimization following hyperparameters step size stability constant using tensorﬂow’s default exponential decay rates network architecture classiﬁcation follows exactly structure hidden layers three convolutional layers fully connected layer network’s output layer also single output valid action uses cross-entropy loss instead loss. learned weights biases classiﬁcation model’s hidden layers used initialization dqn’s network instead using random initialization. also tested transferring layers including output layer experiments. transferring output layer normalization parameters output layer necessary achieve positive transfer. normalize output layer keep track value output layer training used divisor weights biases initial transfer. without normalization values output layer tend explode. also loaded human demonstrations replay memory thus removing need take uniform random action frames initially populate replay memory pre-trained model method also applied refer pre-trained model pmfac pre-trained multiclassclassiﬁer using hyperparameters optimization method mentioned pmfdqn experimented different types network structure. ﬁrst network uses hidden layers used three convolutional layers fully connected layer without lstm cells. output layer follows exact described pmfdqn. second network inspired one-vs-all multiclassclassiﬁcation multitask learning differs ﬁrst network uses multiple heads output layers class action output layer. individual output layer becomes one-vs-all classiﬁcation. training iteration uses uniform probability distribution select output layer train iteration gradients backpropagated shared hidden layers network. multiclass networks hidden layers used initialize ac’s network. since uses experience replay memory also possible pre-train network loading human demonstrations replay memory. refer experiment pre-training naive incorporate human demonstrations interesting method pre-train allows agent learn features policy without interaction actual atari environment. however pretraining method generalize and/or deep algorithms replay memory. would like address future work applying naive approach alternative version uses replay memory lastly conducted additional experiments combines pmfdqn pidqn goal exploring whether combined approach would achieve much greater performance dqn. limited computational resources tested approach three atari games pong freeway beamrider shown figure games actions respectively. openai gym’s deterministic version atari environment action repeat network architecture hyperparameters done previously follow lstm-variant work closely replicates results original paper however note differences original work. first using network architecture three convolutional layers fully connected layer modiﬁed units connect lstm cells followed. second tmax instead tmax actor-learner threads experiments. four recent game frames input network frame preprocessed done also evaluation technique average reward steps taken. addition evaluated deterministic policy agent uses \u0001-greedy action selection method evaluated stochastic policy uses output policy action probabilities. collection human demonstration used openai gym’s keyboard interface allow human demonstrator interact atari environment. demonstrator provided game rules valid actions corresponding keyboard keys game. action repeat provide smoother transitions game play. demonstration collect every fourth frame game play saving game state using game’s image action taken reward received game’s current state terminal state. format stored data follows exact structure experience replay memory used dqn. non-expert human demonstrator demonstrator plays games. game play maximum minutes playing time. demonstration ends game play reaches time limit using pmfdqn trained three multiclass-classiﬁcation networks atari game human demonstration dataset. training done using batch size training iterations. number training iterations determined shortest number iterations training loss games converges approximately zero. trained classiﬁcation networks provide pre-trained models. pre-trained model consists weights biases used initialize dqn’s network. results figure shows pmfdqn speeds training three atari games. also tested pidqn number pre-training iterations pmfdqn. figure shows using pidqn provides varying results three games worse compared results pmfdqn. additionally figure shows although pre-training million iterations provides better result compared shorter pre-training iterations still performed much worse pmfdqn. indicates relevant features better learned supervised learning. addition improve pre-training pmfdqn followed pidqn pre-training iterations each. results experiment surprising since pre-training results better. however figure shows lesser improvement compared pmfdqn alone pong freeway beamrider similar results pmfdqn. believe high initial exploration rate training. setting agent would taking entirely random actions value decayed much lower exploration rate. decayed million steps resulting replay memory ﬁlled experiences random actions believe adverse effect already learned pre-training steps. therefore initialize using pmfdqn followed pidqn. results games shown figure reveal combining pmfdqn pidqn initial exploration rate equally good pmfdqn even better freeway. result beneﬁcial especially applying realworld applications since remove high exploablation studies consider modiﬁcations pfdqn analyze performance. ﬁrst ablation study replaced human demonstrations random demonstrations. interested knowing important human demonstrations comparison using random agent. conducted experiment pong results figure show pre-training random demonstrations worse baseline. experiment indicates need level competency demonstrator order extract useful features pre-training. second ablation study excluded second fully connected layer initializing network pre-trained model. allow know supervised learning learn important features particularly hidden layers. experiments without transferring output layer pretrained model. empirically results figure show besides losing initial jumpstart beginning training time reach convergence different time using layers. indicates actually features hidden layers provide improvement training speed. surprising since output layer classiﬁer trying learn predict action take given state without consideration maximizing reward. additionally learning small amount data human performance relatively poor classiﬁer’s policy would optimal. using pre-trained multiclassclassiﬁcation networks atari game human demonstrations similar pmfdqn batch training iterations. since network lstm-variant uses lstm cells output layers initialize ac’s network pre-trained model’s hidden layers. figure results show improvements training time pong beamrider much higher improvement pong. however improvement freeway. attribute poor baseline performance freeway original work approach focuses learning features without addressing improvements policy improvements freeway approach expected. freeway needs better exploring states order learn near-optimal policy game. something address future work. strong improvements observed still gain improvements pre-train classiﬁcation network longer? tried longer training using one-vsmulticlass-classiﬁcation network shared hidden layers. since class action trained independently observe different convergence training loss class. allowed technique training training loss classes approxfigure performance evaluation baseline pretraining using dqn. x-axis training epoch epoch corresponds million steps. y-axis average testing score three trials shaded regions correspond standard deviation. figure performance evaluation ablation studies pong using dqn. results average testing score three trials shaded regions correspond standard deviation. imately zero. using one-vs.-all classiﬁcation pretrain iterations pong iterations beamrider. training longer results slight improvement beamrider pong shows large improvement shown figure last experiment conducted test whether important features could still learned even much smaller number demonstrations case single game play minutes demonstration. one-vs-all classiﬁcation network pre-train pong game frames training iterations similarly beamrider game frames training iterations. figure results pong beamrider shows high improvement still achievable small amount demonstration. even remarkable beamrider results equally good pre-training full human demonstrations. additional analysis order understand accomplished pretraining look closer ﬁlters determine much pre-trained features contribute ﬁnal features learned. thus investigate similar initial weights deep network ﬁnal weights layer learning near-optimal policy. quantify similarity ﬁnding difference weights using mean squared error layer’s arbitrarily small means higher similarity. table shows high similarity pretraining approach compared random weight initialization. furthermore looked visualization hidden layer observed weights learned classiﬁcation used initial values deep rl’s network provided features retained even training deep figure performance baseline pre-training using x-axis number training steps also number visited game frames among parallel works. y-axis average testing score three trials shaded regions correspond standard deviation. figure visualization normalized weights pong’s ﬁrst convolutional layer using pmfac. weights pre-trained classiﬁcation network trained iterations ﬁnal weights million training steps better illustrate similarity weights provided zoomed-in images particular ﬁlter pre-trained conv ﬁnal conv table evaluation similarity features hidden layer. mean squared error computed weights randomly initialized network ﬁnal weights. similarly using pre-trained model initial weights. figure shows visualization ﬁrst convolutional layer. high similarity weights observed layers suggests pre-training classiﬁcation able learn important features useful deep pre-training approach worked well pong. success explained human demonstration data classiﬁer pre-trained with simplicity pong’s game. pong’s states highly repetitive compared game environments dynamic. beamrider complex environments among three games different levels varying difﬁculty. although freeway’s game state also repetitive base agent’s inability learn good policy problem leans towards policy learning addressed approach. human demonstrations part success approach. import understand demonstrator’s performance amount demonstration data affect beneﬁts pre-training network future work. would also look using recently released human demonstration dataset atari starcraft human demonstrations suffer highly imbalanced classes attributed sparsity actions like torpedo action beamrider limited three uses level actions closely related like beamrider left right action plus combined actions left-ﬁre right-ﬁre demonstrator would usually native actions left right action alone action itself games default no-operation action. imbalance problem addressed classiﬁer learn policy tends bias towards majority classes. interesting classiﬁer still able learn important features without handling issue. however interesting future work handling class imbalance would know ends learning better features improvements observed approach. investigate ways improve approach know limit much improvement pre-training provide without addressing policy learning. approach already trained model policy tries imitate human demonstrator extend work simply using pretrained model’s policy provide advice agent overall learning directly images deep neural networks major factor learning slow deep demonstrated method initializing deep rl’s network pre-trained model signiﬁcantly speed learning deep implementation https//github.com/miyosuda/async_deep_ reinforce. authors would like thank sahil sharma kory matthewson providing useful insights actor-critic method. thank kamiak computing resources used running experiments. mnih badia mirza graves lillicrap harley silver kavukcuoglu asynchronous methods deep reinforcement learning. international conference machine learning silver huang maddison guez sifre driessche schrittwieser antonoglou panneershelvam lanctot mastering game deep neural networks tree search. nature vinyals ewalds bartunov georgiev vezhnevets makhzani k¨uttler agapiou schrittwieser starcraft challenge reinforcement learning. arxiv preprint arxiv..", "year": 2017}