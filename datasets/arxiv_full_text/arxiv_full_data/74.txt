{"title": "A Hierarchical Latent Variable Encoder-Decoder Model for Generating  Dialogues", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "I.5.1; I.2.7"], "abstract": "Sequential data often possesses a hierarchical structure with complex dependencies between subsequences, such as found between the utterances in a dialogue. In an effort to model this kind of generative process, we propose a neural network-based generative architecture, with latent stochastic variables that span a variable number of time steps. We apply the proposed model to the task of dialogue response generation and compare it with recent neural network architectures. We evaluate the model performance through automatic evaluation metrics and by carrying out a human evaluation. The experiments demonstrate that our model improves upon recently proposed models and that the latent variables facilitate the generation of long outputs and maintain the context.", "text": "sequential data often possesses hierarchical structure complex dependencies subsequences found utterances dialogue. effort model kind generative process propose neural network-based generative architecture latent stochastic variables span variable number time steps. apply proposed model task dialogue response generation compare recent neural network architectures. evaluate model performance automatic evaluation metrics carrying human evaluation. experiments demonstrate model improves upon recently proposed models latent variables facilitate generation long outputs maintain context. deep recurrent neural networks recently demonstrated impressive results number difﬁcult machine learning problems involving generation sequential structured outputs including language modelling machine translation dialogue speech recognition advances impressive underlying rnns tend fairly simple structure sense variability stochasticity model occurs output sampled. often inappropriate place inject variability especially true sequential data speech natural language possess hierarchical generation process complex intra-sequence dependencies. instance natural language dialogue involves least levels structure; within single utterance structure dominated local statistics language across utterances distinct source uncertainty characterized aspects conversation topic speaker goals speaker style. paper introduce novel hierarchical stochastic latent variable neural network architecture explicitly model generative processes possess multiple levels variability. evaluate proposed model task dialogue response generation compare recent neural network architectures. evaluate model qualitatively manual inspection quantitatively using human evaluation amazon mechanical turk using automatic evaluation metrics. results demonstrate model improves upon recently proposed models. particular results highlight latent variables help facilitate generation long utterances information content maintain dialogue context. model processes observation recursively. time step model observes element updates internal hidden state parametrized non-linear function hyperbolic tangent lstm gating unit gating unit hidden state acts sufﬁcient statistic summarizes past sequence parametrizes output distribution model assume outputs within discrete vocabulary assumption language model simplest possible generative discrete sequences parametrizes output distribution using softmax function applied afﬁne transformation hidden state model parameters learned maximizing training log-likelihood using gradient descent. hierarchical recurrent encoder-decoder model extension rnnlm. extends encoder-decoder architecture natural dialogue setting. hred assumes output sequence modelled two-level hierarchy sequences sub-sequences sub-sequences tokens. example dialogue modelled sequence utterances utterance modelled sequence words. similarly natural-language document modelled sequence sentences sentence modelled sequence words. hred model consists three modules encoder context decoder rnn. sub-sequence tokens deterministically encoded real-valued vector encoder rnn. given input context updates internal hidden state reﬂect information point time. context deterministically outputs real-valued vector decoder conditions generate next sub-sequence tokens. additional details recent literature observed rnnlm hred similar models based architectures critical problems generating meaningful dialogue utterances believe root cause problems arise parametrization output distribution rnnlm hred imposes strong constraint generation process source variation modelled conditional output distribution. detrimental perspectives probabilistic perspective stochastic variations injected level model encouraged capture local structure sequence rather global long-term structure. random variations injected lower level strongly constrained line immediate previous observations weakly constrained line older observations future observations. think random variations injected i.i.d. noise variables added deterministic components example. noise injected higher level representation spanning longer parts sequence effects could correspond longer-term dependencies. second computational learning perspective state rnnlm summarize past information time step order generate probable next token simultaneously occupy position embedding space sustains realistic output trajectory order generate probable future tokens vanishing gradient effect shorter-term goals inﬂuence ﬁnding compromise disparate forces likely lead training procedure model parameters focus much predicting next output token. particular high-entropy sequences models likely favour short-term predictions opposed long-term predictions easier learn predicting next token compared sustaining long-term trajectory every time step perturbed highly noisy source motived previous discussion introduce latent variable hierarchical recurrent encoder-decoder model. model augments hred model latent variable decoder trained maximizing variational lower-bound log-likelihood. allows model hierarchically-structured sequences two-step generation process—ﬁrst sampling latent variable generating output sequence—while maintaining long-term context. sequence consisting sub-sequences n’th sub-sequence m’th discrete token sequence. vhred model uses stochastic latent variable sub-sequence conditioned previous observed tokens. given model next generates n’th sub-sequence tokens multivariate normal distribution mean covariance matrix rdz×dz constrained diagonal matrix. vhred model contains three components hred model. encoder deterministically encodes single sub-sequence ﬁxed-size real-valued vector. context deterministically takes input output encoder encodes previous sub-sequences ﬁxed-size real-valued vector. vector two-layer feed-forward neural network hyperbolic tangent gating function. matrix multiplication applied output feed-forward network deﬁnes multivariate normal mean µprior. similarly diagonal covariance matrix σprior different matrix multiplication applied net’s output followed softplus function ensure positiveness model’s latent variables inferred maximizing variational lower-bound factorizes independent terms sub-sequence kullback-leibler divergence distributions distribution approximate posterior distribution aims approximate intractable true posterior distribution σposterior) µposterior deﬁnes approximate posterior mean σposterior deﬁnes approximate posterior covariance matrix function previous sub-sequences current sub-sequence posterior mean µposterior covariance σposterior determined prior matrix multiplication output feed-forward network softplus function applied covariance. test time conditioned previous observed sub-sequences sample drawn prior σprior) sub-sequence. sample concatenated output context given input decoder hred model generates sub-sequence token-by-token. training time sample drawn approximate posterior σposterior) used estimate gradient variational lower-bound given approximate posterior parametrized one-layer feed-forward neural network takes input output context current time step well output encoder next sub-sequence. vhred model greatly helps reduce problems generation process used rnnlm hred model outlined above. variation output sequence modelled ways sequence-level conditional prior distribution subsequence-level conditional distribution tokens variable helps model long-term output trajectories representing high-level information sequence turn allows variable primarily focus summarizing information token intuitively randomness injected variable corresponds higher-level decisions like topic sentiment sentence. consider problem conditional natural language response generation dialogue. interesting problem applications areas customer service technical support language learning entertainment also task domain requires learning generate sequences complex structures taking account long-term context consider tasks. task model given dialogue context consisting utterances goal model generate appropriate next response dialogue. ﬁrst perform experiments twitter dialogue corpus task generate utterances append existing twitter conversations. dataset extracted using procedure similar ritter split training validation test sets containing respectively dialogues. dialogue contains utterances tokens average. dialogues fairly long compared recent large-scale language modelling corpora billion word language model benchmark focus modelling single sentences. also experiment ubuntu dialogue corpus contains dialogues extracted ubuntu internet relayed chat channel. users enter chat channel ubuntu-related technical problem users help them. details appendix chose corpora large different purposes—ubuntu dialogues typically goal driven twitter dialogues typically contain social interaction optimize models using adam choose hyperparameters early stop patience using variational lower-bound test time beam search beams outputting responses decoders vhred models sample latent variable condition executing beam search decoder. ubuntu word embedding dimensionality size twitter word embedding dimensionality size models trained learning rate mini-batches containing training examples. variant truncated back-propagation apply gradient clipping. details given appendix baselines twitter ubuntu compare lstm model hidden units. ubuntu hred model hidden units encoder context decoder rnns respectively. encoder standard rnn. twitter hred model encoder bidirectional encoder forward backward rnns hidden units context decoder hidden units. reference also include non-neural network baseline speciﬁcally tf-idf retrieval-based model proposed vhred encoder context rnns vhred model parametrized corresponding hred models. difference parametrization decoder context output vector concatenated generated stochastic latent variable. furthermore initialize feed-forward networks prior posterior distributions values drawn zero-mean normal distribution variance biases equal zero. also multiply diagonal covariance matrices prior posterior distributions make training stable high variance makes gradients w.r.t. reconstruction cost unreliable fatal beginning training process. vhred’s encoder context rnns initialized parameters corresponding converged hred models. also heuristics proposed bowman drop words decoder ﬁxed drop rate multiply terms scalar starts zero linearly increases ﬁrst training batches twitter ubuntu respectively. applying heuristics helped substantially stabilize training process make model stochastic latent variables. experimented batch normalization training procedure feed-forward neural networks. found made training unstable without substantial gains performance w.r.t. variational bound. evaluation accurate evaluation dialogue system responses difﬁcult problem inspired metrics machine translation information retrieval researchers begun adopting wordoverlap metrics however show metrics little correlation human evaluations response quality. therefore carry human evaluation compare responses different models. also compute several statistics automatic metrics model responses characterize differences model-generated responses. carry human study twitter dialogue corpus amazon mechanical turk conduct experiments ubuntu evaluating responses usually requires technical expertise prevalent among users. evaluation study series pairwise comparison experiments. show human evaluators dialogue context along potential responses generated model participants choose response appropriate dialogue context. evaluators indifferent either responses cannot understand dialogue context choose neither response. pair models conduct experiments whhhhhhyyyy suddenly cold tallahassee today?? ﬂorida dream beach ﬁngers icey cold?? sittin deck looking lake travis austin enjoying birthday reaches approaches warily buddy. swings doin? tilts head wariness reaches face vhred that’s thinking. going one. lstm want hred don’t know don’t know vhred haha geen something maar geen something meer something lstm haha something something something hred something something something something something something something something vhred too. going wait come back. excited. lstm sure will. sure she’ll ﬁne. hred sure she’ll sure she’ll ﬁne. vhred need hair done lstm hred example contexts contain least unique tokens contain least tokens helps compare well model integrate dialogue context response since previously hypothesized long contexts hierarchical rnns models fare better screenshots details experiments appendix results show vhred clearly preferred majority experiments. particular vhred strongly preferred hred tf-idf baseline models short long context settings. vhred also preferred lstm baseline model long contexts; however lstm preferred vhred short contexts. believe lstm baseline tends output much generic responses since doesn’t model hierarchical input structure lstm model shorter memory span thus must output response based primarily last utterance. ‘safe’ responses reasonable wider range contexts meaning human evaluators likely rate appropriate. however argue model outputs generic responses undesirable dialogue leads uninteresting less engaging conversations. conversely vhred model explicitly designed long contexts output diverse responses sampling latent variable. thus vhred model generates longer sentences semantic content lstm model ‘riskier’ longer utterances likely contain small mistakes lead lower human preference single utterance. however believe response diversity crucial maintaining interesting conversations dialogue literature generic responses used primarily ‘back-off’ strategies case agent interesting response relevant context hypotheses conﬁrmed upon qualitative assessment generated responses vhred generates longer meaningful responses compared lstm model generates mostly generic responses. additionally observed vhred model learned better model smilies slang even continue conversations different languages aspects measured human study. further vhred appears better generating stories imaginative actions compared generative baseline models last example table case vhred generated response interesting less preferred humans slightly incompatible context compared generic lstm response. next section back examples quantitatively showing vhred model learns generate longer responses information content share semantic similarity context ground-truth response. show vhred responses on-topic share semantic similarity ground-truth response consider three textual similarity metrics based word embeddings. embedding average metric projects model response ground truth response separate real-valued vectors taking mean word embeddings response computes cosine similarity metric widely used measuring textual similarity. embedding extrema metric similarly embeds responses taking extremum dimension afterwards computes cosine similarity them.the embedding greedy metric ﬁne-grained; uses cosine similarity word embeddings closest word human-generated response word model response. given alignment words responses mean cosine similarities computed pair questions since metric takes account alignment words accurate long responses. metrics strongly correlate human judgements generated responses interpret measuring topic similarity model generated response similar semantic content ground truth human response metrics yield high score. ease reproducibility publicly available wordvec word embeddings trained google news corpus. compute metrics settings models generate single response generate next three consecutive utterances overall vhred seems better capture ground truth response topic either lstm hred models. fact vhred better particular setting model generates three consecutive utterances strongly suggests hidden states decoder context rnns vhred models better able follow trajectories remain on-topic w.r.t dialogue context. supports computational hypothesis stochastic latent variable helps modulate training procedure achieve better trade-off short-term long-term generation. also observed trend computing similarity metrics model generated responses corresponding context reinforces hypothesis. show vhred responses contain information content model responses compute average response length average entropy w.r.t. maximum likelihood unigram model generated responses unigram entropy computed preprocessed tokenized datasets. vhred produces responses higher entropy word ubuntu twitter compared hred lstm models. vhred also produces longer responses overall twitter translates responses containing average bits information hred model. since actual dialogue responses contain even information word generative models reasonable assume higher entropy desirable. thus vhred compares favourably recently proposed models literature often output extremely low-entropy responses don’t know finally fact vhred produces responses higher entropy suggests responses average diverse responses produced hred lstm models. implies trajectories hidden states vhred model traverse larger area space compared hidden states hred lstm baselines supports hypothesis stochastic latent variable helps vhred model achieve better trade-off short-term long-term generation. stochastic latent variable learned maximizing variational lower bound inspired variational autoencoder models used predominantly generating images continuous domain however also recent work applying architectures generating sequences variational recurrent neural networks applied speech handwriting synthesis stochastic recurrent networks applied music generation motion capture modeling. vrnn storn incorporate stochastic latent variables architectures unlike vhred sample separate latent variable time step decoder. exploit hierarchical structure data thus model higher-level variability. similar work variational recurrent autoencoder variational autoencoder language model apply encoder-decoder architectures generative music modeling language modeling respectively. vhred model different following ways. vhred latent variable conditioned previous sub-sequences enables model generate multiple sub-sequences also makes latent variables co-dependent observed tokens. vhred model builds hierarchical architecture hred model makes model applicable generation conditioned long contexts. direct deterministic connection context decoder allows model transfer deterministic pieces information components. crucially vhred also demonstrates improved results beyond autoencoder framework objective input reconstruction conditional generation next utterance dialogue. introduced novel latent variable neural network architecture called vhred. model uses hierarchical generation process order exploit structure sequences trained using variational lower bound log-likelihood. applied proposed model difﬁcult task dialogue response generation demonstrated improvement previous models several ways including quality responses measured human study. empirical results highlight advantages hierarchical generation process modelling high-entropy sequences. finally worth noting proposed model general. principle applied sequential generation task exhibits hierarchical structure document-level machine translation query prediction multi-sentence document summarization multi-sentence image caption generation others.", "year": 2016}