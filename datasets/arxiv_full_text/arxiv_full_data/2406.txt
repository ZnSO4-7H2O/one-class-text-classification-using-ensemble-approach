{"title": "The Z-loss: a shift and scale invariant classification loss belonging to  the Spherical Family", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Despite being the standard loss function to train multi-class neural networks, the log-softmax has two potential limitations. First, it involves computations that scale linearly with the number of output classes, which can restrict the size of problems we are able to tackle with current hardware. Second, it remains unclear how close it matches the task loss such as the top-k error rate or other non-differentiable evaluation metrics which we aim to optimize ultimately. In this paper, we introduce an alternative classification loss function, the Z-loss, which is designed to address these two issues. Unlike the log-softmax, it has the desirable property of belonging to the spherical loss family (Vincent et al., 2015), a class of loss functions for which training can be performed very efficiently with a complexity independent of the number of output classes. We show experimentally that it significantly outperforms the other spherical loss functions previously investigated. Furthermore, we show on a word language modeling task that it also outperforms the log-softmax with respect to certain ranking scores, such as top-k scores, suggesting that the Z-loss has the flexibility to better match the task loss. These qualities thus makes the Z-loss an appealing candidate to train very efficiently large output networks such as word-language models or other extreme classification problems. On the One Billion Word (Chelba et al., 2014) dataset, we are able to train a model with the Z-loss 40 times faster than the log-softmax and more than 4 times faster than the hierarchical softmax.", "text": "despite standard loss function train multi-class neural networks log-softmax potential limitations. first involves computations scale linearly number output classes restrict size problems able tackle current hardware. second remains unclear close matches task loss top-k error rate nondifferentiable evaluation metrics optimize ultimately. paper introduce alternative classiﬁcation loss function z-loss designed address issues. unlike log-softmax desirable property belonging spherical loss family class loss functions training performed efﬁciently complexity independent number output classes. show experimentally signiﬁcantly outperforms spherical loss functions previously published investigated. furthermore show word language modeling task also outperforms log-softmax respect certain ranking scores top-k scores suggesting z-loss ﬂexibility better match task loss. qualities thus makes z-loss appealing candidate train efﬁciently large output networks word-language models extreme classiﬁcation problems. billion word dataset able train model z-loss times faster log-softmax times faster hierarchical softmax. classiﬁcation tasks usually associated loss function interest task loss minimize ultimately. task losses classiﬁcation error rate time non-differentiable case differentiable surrogate loss designed minimized gradient-descent. surrogate loss proxy task loss minimizing hope minimize task loss. common surrogate loss multi-class classiﬁcation negative log-softmax corresponds maximizing log-likelihood probabilistic classiﬁer computes class probabilities softmax. despite ubiquitous remains unclear degree matches task loss softmax used rather alternative normalizing functions. traditionally loss functions also used train neural networks classiﬁcation mean square error sigmoid targets cross-entropy sigmoid corresponds output modeled independently bernoulli variable. multi-class generalisation margin losses ranking losses also used probabilistic interpretation required. although loss functions appear perform similarly small scale problems seem behave differently larger output problems neural language models therefore order better evaluate difference loss functions decided focus experiments language models large number output classes note computations loss functions scale linearly number output classes. paper introduce loss function z-loss which contrary log-softmax mentioned alternatives desirable property belonging spherical family loss functions algorithmic approach vincent allows compute exact gradient updates time memory complexity independent number classes. denote dimension last hidden layer number output classes spherical loss exact updates output weights computed instead naive implementation i.e. independently number output classes gist algorithm replace costly dense update output matrix sparse update factored representation maintain summary statistics allow computing loss refer reader aforementioned paper detailed description approach. several spherical loss functions already investigated seem perform well log-softmax large output problems. several workarounds proposed tackle computational cost huge softmax layers divided main approaches. ﬁrst sampling-based approximations compute tiny fraction output’s dimensions second hierarchical softmax modiﬁes original architecture replacing large output softmax heuristically deﬁned hierarchical tree chen benchmarked many methods language modeling task among tried found large vocabularies hierarchical softmax fastest best ﬁxed budget training time. therefore also compare z-loss hierarchical softmax. notations rest paper consider neural network outputs. denote output pre-activations i.e. result last matrix multiplication network representation last hidden layer. represents index target class whose corresponding output activation thus section brieﬂy describe different loss functions compare z-loss. log-softmax loss function standard loss function multi-class classiﬁcation log-softmax corresponds minimizing negative log-likelihood softmax model. softmax activation function models output network categorical distribution component deﬁned exp. note softmax invariant shifting constant scaling. maximizing log-likelihood model corresponds minimizing classic log-softmax loss function softmaxk. intuitively mimimizing whose gradient loss corresponds maximizing minimizing note gradient components zero reﬂecting competition activations previously investigated spherical loss functions recently vincent proposed novel algorithmic approach compute exact updates output weights efﬁcient fashion independently number classes provided loss belongs particular class functions called spherical family. family composed functions expressed using squared norm whole output mean square error linear mapping simplest member spherical family. deﬁned form gradient similar log-softmax components also sums zero values known slow training. log-taylor-softmax several loss functions belonging spherical family recently investigated brébisson vincent among taylor softmax retained best candidate. obtained replacing exponentials softmax second-order taylor expansions around zero although taylor softmax performs slightly better softmax small output problems mnist cifar scale well number output classes hierarchical softmax chen benchmarked many different methods train neural language models. among strategies tried found large vocabularies hierarchical softmax fastest best ﬁxed budget training time. therefore also compared z-loss hierarchical softmax modiﬁes original architecture replacing softmax heuristically deﬁned hierarchical tree. hyperparameters controlling scaling position vector z-loss seen function single variable plotted figure z-loss clearly belongs spherical family described section decomposed three successive operations normalization scaling/shift softplus. analyse three stages successively. z-normalization normalization call z-normalization essential order involve different output components ﬁnal loss. without loss would depend resulting null gradient respect thus thanks normalization pre-activations compete three interlinked ways increase either increase decrease decrease behavior similar log-softmax. furthermore standardization makes z-loss invariant shifting scaling outputs whereas log-softmax invariant shifting. note rank classes unaffected global shifting scaling pre-activations rank-based task losses precision since z-loss similarly invariant log-softmax sensitive scale make z-loss better surrogate rank-based task losses. gradient z-loss respect simply gradient times derivative softplus. gradient respect written +exp. like sigmoid denotes logistic sigmoid function deﬁned sigmoid components still z-loss reaches minimum inﬁnite number corresponding vectors possible unlike z-loss log-softmax ﬁxed points result minimization could potentially push extreme values. note z-normalization different used batch normalization applies across dimensions output example whereas batch normalization separately normalizes output dimension across minibatch. scaling shifting normalized activations scaled shifted afﬁne hyperparameters essential allow z-score better match task loss ultimately interested particular later optimal values parameters signiﬁcantly vary depending speciﬁc task loss optimize. controls softness softplus large making softplus closer rectiﬁer function max). note effect changing cannot cancelled correspondingly modifying output layer weights contrasts classic loss functions log-softmax effect could undone reciprocal rescaling discussed section softplus softplus ensures derivative respect tends towards zero grows. without derivative would always would strongly push towards extreme values potentially employ unnecessary capacity network. also random variable representing class example take values consider multi-label setup model output δy=k bernoulli whose parameter given sigmoid sigmoid. then probability class written probability minimizing negative log-likelihood model leads following cross-entropy-sigmoid loss minimize ﬁrst term ignore others values would systematically decrease network would learn. instead keep ﬁrst term apply z-normalization beforehand obtain z-loss deﬁned equation claim z-normalization compensates approximation ignored term likely stay approximatively constant invariant shift scaling experiments evaluate along z-loss. generaliszation z-normalization classic loss functions z-normalization could potentially applied classic loss functions therefore also compared z-loss z-normalized version log-softmax shifting parameter useless softmax shift-invariant. denote ls−z corresponding z-normalized loss function figure top-k error rates mean reciprocal rank obtained best models loss function penn tree bank language modeling task. mean-square-error worst performance followed taylorsoftmax. cross-entropy-sigmoid lowest top- error rate surprisingly outperforms log-softmax. z-loss lowest top-{} error rates brébisson vincent already conducted experiments several spherical losses showed that work well problems classes outperformed log-softmax problems large number output classes. therefore focused experiments problems particular word-level language modeling tasks large datasets publicly available. task word-language modeling consists predicting next word following sequence consecutive words called n-gram length sequence. example eats apple\" -gram eats used input sequence context predict target word \"apple\". neural language models tackles classiﬁcation task neural network whose number outputs size vocabulary. z-loss produce probabilities cannot compute likelihood perplexity scores comparable naturally computed log-softmax model. therefore instead evaluated different loss functions following scores top-{} error rates mean reciprocal rank deﬁned below. rank pre-activation among take values point well-classiﬁed. top-k error rate top-k error rate deﬁned mean boolean random variable deﬁned measures often target among highest predictions network. mean reciprocal rank deﬁned mean perfect classiﬁcation would lead examples thus identical mean average precision context classiﬁcation. popular score measures ranking ﬁeld information retrieval. penn tree bank ﬁrst trained word-level language models classic penn tree bank corpus split training validation testing words validation words test words. vocabulary words. trained typical feed-forward neural language models vanilla stochastic gradient descent mini-batches size using input context words. loss function tuned embedding size number hidden layers number neurons layer learning rate hyperparameters z-loss. figure reports ﬁnal test scores obtained best models loss evaluation metric. seen z-loss signiﬁcantly outperforms considered losses respect top-{} error rates. measure extent hyperparameters control z-loss matches task losses trained several times architecture different values results reported figure figure shows training curves best z-score models top-{} error rates respectively. hyperparameters drastically modify training dynamics thus extremely important particular evaluation metric interest. figure top-{} error rates z-loss models trained penn tree bank dataset differ value hyperparameter precisely value separate model trained scratch. models. three curves different shapes different minima showing gives surrogate z-loss ﬂexibility better task loss. figure evolution validation top-{} error rates training penn tree bank dataset four z-loss language models different combinations hyperparameters four combinations chosen minimize particular top-k error rate. example dashed green curve corresponds best model obtained respect top- error rate. particular best top- model worst top- model vice versa. high variations top-k plots show hyperparameters allow better match task loss. contrast classic log-softmax lack ﬂexibility hyperparameters. figure reports test scores obtained best z-normalized versions log-softmax. previously explained section z-normalization enables adding scaling hyperparameters also help log-softmax better match top-k evaluation metrics much z-loss. billion word also trained word-level neural language models billion word dataset considerably larger dataset penn tree bank. composed billion words belonging vocabulary words. given size dataset able extensively tune architecture models hyperparameters. therefore ﬁrst compared loss functions ﬁxed architecture almost identical chen -grams concatenated embeddings representing layer neurons figure comparison top-{} test error rates obtained best models z-loss loss functions hyperparameters penn tree bank language modeling task. hyperparameters added log-softmax cross-entropy seem effect important z-loss still improve slightly ﬁnal scores. z-loss z-normalization crucial removing would prevent meaningful learning. three hidden layers sizes refer architecture net. experiments chen expect days would required train naive log-softmax layer convergence among workarounds chen benchmarked showed hierarchical softmax fastest best method ﬁxed budget time. therefore compared z-loss hierarchical softmax architecture ﬁxed tuned initial learning rate loss function periodically decreased validation score stoped improving. table report timings convergence scores reached three loss functions architecture net. although hierarchical softmax yields slightly better top-k performance z-loss model times faster converge. allows train bigger z-loss models amount time hierarchical softmax thus trained bigger z-loss model architecture size less days required hierarchical softmax architecture converge. seen table model better top- error rate hierarchical softmax days. likely another hyperparameters would yield lower top- error rates well. table training timings process epoch training data billion word dataset different loss functions architecture i.e. feedforward network composed layers sizes batch size used nvidia titan intel .ghz. give timings whole model output layer only. timed thousands minibatches extrapolated timings whole epoch. table final test top- top- error rates billion word language modeling task. \"constant\" line corresponds constant classiﬁer predicting frequencies words. hierarchical softmax reaches ﬁnal perplexity hyperparameters z-loss model architectures tuned maximize top- error rate. used nvidia titan cross-entropy sigmoid outperforms log-softmax experiments penn tree bank dataset respect top-{} error rates. surprising cross-entropy sigmoid models multi-label distribution rather multi-class one. might explain z-loss seen approximation cross-entropy sigmoid performs well slightly worse log-softmax top- error outperforms softmax cross-entropy sigmoid top-k. signiﬁcantly outperforms investigated spherical loss functions namely taylor softmax mean square error. results show hyper-parameters z-loss essential allow certain evaluation metrics accurately log-softmax. also hyperparameters traditional loss function applying z-normalization beforehand. particular hyperparameters slightly improve performance log-softmax even though effect important z-loss practice hyperparameters z-loss simple tune found running search ﬁrst iterations sufﬁcient. top-k error rates hyperparameter important higher better top-k scores high vice versa. billion word language modeling task z-loss models train considerably faster hierarchical softmax slightly worse respect ﬁnal top-k scores. thanks speed z-loss able train signiﬁcantly larger architecture faster hierarchical softmax smaller architecture obtain slightly better top- error rate. z-loss top- score good hyperparameters tune top-. introduced loss function z-loss aims address potential limitations naive log-softmax speed problem large amount output classes discrepancy task loss ultimately interested contrary log-softmax z-loss desirable property belonging spherical family allows train output layer efﬁciently independently number classes. billion word dataset classes ﬁxed standard network architecture training z-loss model times faster naive log-softmax version times faster hierarchical softmax. ﬁxed budget around days able train better z-loss model hierarchical softmax respect top- error rate. complexity-wise number classes computations hierarchical softmax scale theory z-loss independent output size suggests z-loss would better suited datasets even classes hierarchical softmax would slow. addition huge speedups z-loss also addresses problem discrepancy task loss surrogate loss. thanks shift scale invariant z-normalization z-loss beneﬁts hyperparameters adjust extent well surrogate z-loss matches task loss. showed experimentally hyperparameters drastically improve resulting task loss values making desirable. penn tree bank z-loss models yield signiﬁcantly lower top-{} error rates log-softmax. research focus updating hyperparameters automatically training ensure loss function dynamically matches task loss close possible. beyond z-loss z-normalization interesting applied classic loss functions log-softmax allowing hyperparameters loss function potentially mitigating discrepancy task loss. research investigate generalizations z-normalization general framework z-loss. gutmann hyvarinen. noise-contrastive estimation estimation principle unnormalized statistical models. proceedings thirteenth international conference artiﬁcial intelligence statistics ioffe szegedy. batch normalization accelerating deep network training reducing internal covariate shift. proceedings international conference machine learning", "year": 2016}