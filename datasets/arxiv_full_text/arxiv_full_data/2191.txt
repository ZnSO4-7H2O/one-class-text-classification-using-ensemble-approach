{"title": "Estimating Accuracy from Unlabeled Data: A Probabilistic Logic Approach", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We propose an efficient method to estimate the accuracy of classifiers using only unlabeled data. We consider a setting with multiple classification problems where the target classes may be tied together through logical constraints. For example, a set of classes may be mutually exclusive, meaning that a data instance can belong to at most one of them. The proposed method is based on the intuition that: (i) when classifiers agree, they are more likely to be correct, and (ii) when the classifiers make a prediction that violates the constraints, at least one classifier must be making an error. Experiments on four real-world data sets produce accuracy estimates within a few percent of the true accuracy, using solely unlabeled data. Our models also outperform existing state-of-the-art solutions in both estimating accuracies, and combining multiple classifier outputs. The results emphasize the utility of logical constraints in estimating accuracy, thus validating our intuition.", "text": "propose efﬁcient method estimate accuracy classiﬁers using unlabeled data. consider setting multiple classiﬁcation problems target classes tied together logical constraints. example classes mutually exclusive meaning data instance belong them. proposed method based intuition that classiﬁers agree likely correct classiﬁers make prediction violates constraints least classiﬁer must making error. experiments four real-world data sets produce accuracy estimates within percent true accuracy using solely unlabeled data. models also outperform existing state-of-theart solutions estimating accuracies combining multiple classiﬁer outputs. results emphasize utility logical constraints estimating accuracy thus validating intuition. estimating accuracy classiﬁers central machine learning many ﬁelds. accuracy deﬁned probability system’s output agreeing true underlying output thus measure system’s performance. existing approaches estimating accuracy supervised meaning labeled examples required estimation. able estimate accuracies classiﬁers using unlabeled data important many applications including autonomous learning system operates supervision well crowdsourcing applications multiple workers provide answers questions correct answer unknown. furthermore tasks involve making several predictions tied together logical constraints abundant machine learning. example classiﬁers never ending language learning carnegie mellon university pittsburgh microsoft research redmond carnegie mellon university pittsburgh usa. correspondence emmanouil platanios <e.a.platanioscs.cmu.edu>. project predict whether noun phrases represent animals cities respectively know something cannot animal city cases hard observe predictions system violate least constraints least system’s components must making error. paper extends intuition presents unsupervised approach estimating accuracies able information provided logical constraints. furthermore proposed approach also able available labeled data thus also applicable semi-supervised settings. consider multiple approximations problem setting several different approximations target boolean classiﬁcation functions wish know true accuracies different approximations using unlabeled data well response true underlying functions value characterizes different domain domain interpreted class category objects. similarly function approximations interpreted classifying inputs belonging categories. consider case logical constraints deﬁned domains. note that contrast related work allow function approximations provide soft responses interval thus allowing modeling certainty. example setting often refer throughout paper consider part nell input space functions space possible noun phrases target function returns boolean value indicating whether input belongs category city animal categories correspond domains. also exist logical constraints categories hard soft example city animal mutually exclusive case function approximations correspond different classiﬁers logical constraints make system inputs. representation logical constraints terms function approximation error rates described section logical constraints blue arrows represent subsumption constraints labels connected dashed line represent mutually exclusive set. given inputs ﬁrst step grounding described section ground rules correspond logic implies. then inference performed order infer likely truth values unobserved ground predicates given observed ones ground rules results constitute outputs system include estimated error rates likely target function outputs features different views input data) return probability belonging class instead binary value. goal estimate accuracies classiﬁers using unlabeled data. order quantify accuracy deﬁne error rate classiﬁer domain binary case true underlying distribution input data. note accuracy equal minus error rate. deﬁnition relaxed case representing probability )pd= resembles expected probability error. literature covers many projects related estimating accuracy unlabeled data. setting considering previously explored collins singer dasgupta bengio chapados madani schuurmans balcan parisi among others. approaches made strong assumptions assuming independence given outputs assuming knowledge true distribution outputs. none previous approaches incorporated knowledge form logical constraints. collins huynh review many methods proposed estimating accuracy medical tests absence gold standard. effectively problem considering applied domains medicine biostatistics. present method estimating accuracy tests tests applied multiple different populations assuming accuracies tests across populations test results independent conditional true output. similar assumptions ones made several papers already mentioned idea applying tests multiple populations interesting. platanios proposed method relaxing assumptions. formulated problem estimating error rates several approximations function optimization problem uses agreement rates approximations unlabeled data. dawid skene ﬁrst formulate problem terms graphical model moreno proposed nonparametric extension model applied crowdsourcing. tian proposed interesting max-margin majority voting scheme combining classiﬁer outputs also applied crowdsourcing. however approaches outperformed models platanios similar work dawid skene moreno best knowledge work ﬁrst logic estimating accuracy unlabeled data shown experiments outperforms competing methods. logical constraints provide additional information estimation method partially explains performance boost. proposed method method consists deﬁning logic rules modeling logical constraints known logical constraints performing probabilistic inference using rules priors order obtain likely values observed. intuition behind method constraints violated function approximation outputs least functions making error. example nell case function approximations respond belongs city animal categories respectively least making error. deﬁne form logic rules section describe perform probabilistic inference section overview system shown ﬁgure next section introduce notion probabilistic logic fuses logic probabilistic reasoning forms backbone method. probabilistic logic classical logic predicates indicating whether mammal variable) rules deﬁned terms predicates animal interpreted implies). refer predicates rules deﬁned particular instantiation variables ground predicates ground rules respectively mammal animal). ground predicates rules take boolean values instead boolean representing probability corresponding ground predicate rule true. case boolean logic operators implies need redeﬁned. next section assume classical logical interpretation. model described earlier goal estimate true accuracies function approximations using unlabeled data well response true underlying functions deﬁne logic rules perform inference order achieve goal. rules deﬁned terms following predicates function approximation outputs deﬁned approximations inputs corresponding function approximation provided response. note values ground predicates probabilistic nature observed. target function outputs deﬁned inputs note that purely unsupervised setting none ground predicate values observed contrast semi-supervised setting. function approximation error rates goal logic rules deﬁne two-fold combine function approximation outputs single output value account logical constraints domains. achieve goals accounting error rates function approximations. ﬁrst deﬁne rules relate function approximation outputs true underlying function output. call rules ensemble rules describe following section. discuss account logical constraints domains. words ﬁrst rules state function approximation making error output match output target function second rules state function approximation making error output match output target function. interesting point make ensemble rules effectively constitute weighted majority vote combining function approximation outputs weights determined error rates approximations. error rates implicitly computed based agreement function approximations. related work platanios there authors answer question whether consistency outputs approximations implies correctness. directly agreement rates approximations order estimate error rates. thus exists interesting connection work also implicitly agreement rates estimate error rates results even though improving upon signiﬁcantly reinforce claim. dant structured prediction problems machine learning motivated method context nell mutual exclusion domains mutually exclusive implies example nell setting belongs city category cannot also belong animal category. subsumption subsumes must example nell setting belongs category must also belong animal category. mutual exclusion rule. ﬁrst deﬁne predicate indicating domains mutually exclusive. predicate value domains mutually exclusive value otherwise truth value observed values furthermore note symmetric meaning true also true. deﬁne mutual exclusion logic rule identiﬁability. consider ﬂipping values error rates target function responses. then ensemble logic rules would evaluate value therefore error rates target function values identiﬁable logical constraints. next section constraints sometimes help resolve issue often corresponding logic rules exhibit kind symmetry. however cases symmetry exists resolve assuming function approximations error rates better chance done considering following rules note rules imply discussed section probabilistic frameworks rules weighted real value rules given weight represents signiﬁcance strength. framework consider using smaller weight prior belief rules compared remainder rules would simply correspond regularization weight. weight tunable even learnable parameter. subsumption rule. ﬁrst deﬁne predicate indicating domain subsumes domain predicate value domain subsumes domain otherwise truth value always observed. note that unlike mutual exclusion predicate symmetric. deﬁne subsumption logic rule deﬁned logic rules comprise model describe perform inference probabilistic logic model next section. inference case comprises determining likely truth values unobserved ground predicates given observed predicates rules comprise model. variable corresponds soft truth value ground predicate. function corresponds measure distance satisﬁability logic rule. rules used characterizes particular model. rules represent prior knowledge might problem trying solve. model rules deﬁned section mentioned above variables allowed take values interval thus need deﬁne mean truth value rule distance satisﬁability. logical operators implies definitions łukasiewicz logic max{p min{p min{ note operators simple continuous relaxation corresponding boolean operators booleanvalued variables corresponding false true equivalent. writing logic rules form easy observe distance satisﬁability rule evaluates note rules ﬁrst-order predicate logic represented form minimizing quantity amounts making rule more satisﬁed. order complete system description need describe obtain ground rules predicates logic rules form presented section observed ground predicates deﬁne objective function equation solve optimization problem equation obtain likely truth values unobserved ground predicates. steps described following sections. grounding process computing possible groundings logic rule order construct inference problem variables objective function. already described section variables correspond ground predicates functions correspond ground rules. easiest ground logic rules would create ground rule instance possible value arguments. however rule depends variables variable take possible values ground rules would gen.. inference section introduced notion probabilistic logic deﬁned model terms probabilistic predicates rules. section discuss detail implications using probabilistic logic perform inference model. exist various probabilistic logic frameworks making different assumptions. arguably popular framework markov logic networks inference performed constructed markov random field based model logic rules. potential function corresponds ground rule takes arbitrary positive value ground rule satisﬁed value otherwise variable boolean-valued corresponds ground predicate. mlns thus direct probabilistic extension boolean logic. turns discrete nature variables mlns inference np-hard thus inefﬁcient. part goal paper method applicable large scale thus resorted probabilistic soft logic thought convex relaxation mlns. note model proposed previous section also primary contribution paper used various probabilistic logic frameworks. choice described section motivated scalability. could easily perform inference model using mlns framework. models composed logic rules represented using hinge-loss markov random ﬁelds case inference amounts solving convex optimization problem. variables hl-mrf correspond soft truth values ground predicates. speciﬁcally hl-mrf probability density random variables domain corresponding unobserved ground predicate values. additional variables known values domain corresponding observed ground predicate values. ﬁnite continuous potential functions form })pj linear function soon functions relate ground rules model. given above non-negative free parameters hl-mrf density deﬁned erated. example mutual exclusion rule equation depends meaning d×|x| ground rule instances would generated denotes number values take. applies predicates; would result d×|x| ground instances would become variables optimization problem. approach would thus result huge optimization problem rendering impractical dealing large scale problems nell. scaling grounding procedure notice many possible ground rules always satisﬁed irrespective values unobserved ground predicates depend upon. ground rules would therefore inﬂuence optimization problem solution safely ignored. since model dealing small predeﬁned logic rule forms devised heuristic grounding procedure generates ground rules predicates inﬂuence optimization. procedure shown algorithm based idea ground rule useful function approximation predicate appears body observed. turns approach orders magnitude faster existing state-of-the-art solutions grounding solution used large problems objective function equation potentially millions terms involving small variables. method used solve optimization problem based consensus alternating directions method multipliers approach consists handling term separate optimization problem using copies corresponding variables adding constraint copies variable must equal. allows solving subproblems completely parallel thus scalable. algorithm summarized algorithm details algorithm convergence properties found latest paper propose stochastic variation consensus admm method even scalable. stochastic consensus admm. iteration instead solving subproblems aggregating solutions consensus variables sample subproblems solve. probability sampling subproblem proportional distance variable copies respective consensus variables. intuition motivation behind approach solution optimization problem variable copies agreement consensus variables. therefore prioritizing subproblems whose variables greater disagreement consensus variables might facilitate faster convergence. indeed modiﬁcation inference algorithm allowed apply method nell data obtain results within minutes instead hours. following paragraphs describe data sets used experiments methods compare against evaluation metrics used results obtained. implementation well experiment data sets available http//anonymous. data sets. first considered following data sets constraints domains nell- classify noun phrases belonging category categories considered data figure illustration nell- data constraints. represents label blue arrow represents subsumption constraint labels connected dashed line represents mutually exclusive labels. example animal subsumes vertebrate bird fish mammal mutually exclusive. data sets total classiﬁers function approximations described note classiﬁers provide response every input order show applicability method cases logical constraints domains also replicated experiments platanios unell task nell- without considering constraints using categories classiﬁers category. ubrain classify second long story passages corresponds unlabeled second time series functional magnetic resonance imaging neural activity. classiﬁers used domain case deﬁned different locations brain examples. additional details found methods. methods compare explicitly estimate error rates. rather combine classiﬁer outputs produce single label. methods produce estimate error rate using labels compare estimate. majority vote intuitive method consists simply taking common output among provided function approximation responses combined target function output. method dawid skene agreement rates method platanios estimates error rates infer combined label. weighted majority vote classiﬁers’ predictions weighted according error rates order produce single output label. also compare method denoted experiments method except pairwise function approximation agreements evaluation. compute sample error rate estimates using true target function labels compute three metrics domain average domains error rank rank function approximations estimates sample estimates produce vectors ranks. compute mean absolute deviation vectors mean norm vectors’ difference. error vector estimates vector sample estimates vector indexed function approximation index. results. first note largest execution time method among data sets minutes using -inch macbook pro. second overall best performing method hcbee required minutes. highlights scalability method. results shown table divided analysis nell- nell- data sets case exist logical constraints domains thus results relevant central research claims paper since method motivated logical constraints. clear method outperforms existing methods including state-ofthe-art signiﬁcant margin. mads error rate estimation aucs target function response estimation signiﬁcantly better. unell ubrain data sets case exist logical constraints domains. method still almost always outperforms competing methods speciﬁcally always terms error rate estimation mad. results makes clear method also used effectively cases logical constraints. table mean absolute deviation error rate rankings error rate estimates area curve label estimates best results experiment across methods shown bolded text results proposed method highlighted blue. conclusion future work introduced approach estimate accuracy unlabeled data based probabilistic logic several function approximations several underlying true functions. contrast previous efforts approach able information provided logical constraints exist outputs underlying functions. proposed method also enables inference likely outputs underlying true functions. furthermore capable scaling cases many functions millions observations efﬁcient framework performing inference combined heuristic grounding algorithm stochastic variation consensus admm. order explore ability proposed approach estimate error rates realistic settings without domain-speciﬁc tuning used four different data sets experiments. methods shown outperform current state-of-the-art tasks estimating error rates inferring likely single label using exist several potential directions work. straightforward extension would model complete confusion matrix instead simply error rates deal discrete-valued functions opposed booleanvalued functions. long-term goal error estimation context self-reﬂection mitchell deﬁned ability system reﬂect learning process improve. method could useful context system able evaluate itself. acknowledgements would like thank abulhair saparov otilia stretcu useful feedback provided early versions paper. research performed internship microsoft research also supported part award part presidential fellowship carnegie mellon university. references bach huang london getoor. hinge-loss markov random fields convex inference structured prediction. conference uncertainty artiﬁcial intelligence madani pennock flake. co-validation using model disagreement unlabeled data validate classiﬁcation algorithms. neural information processing systems mitchell cohen hruschka pratim talukdar betteridge carlson dalvi gardner kisiel krishnamurthy mazaitis mohamed nakashole platanios ritter samadi settles wang wijaya gupta chen saparov greaves welling. neverending learning. association advancement artiﬁcial intelligence", "year": 2017}