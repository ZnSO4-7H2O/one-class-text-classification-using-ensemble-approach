{"title": "Rationalizing Neural Predictions", "tag": ["cs.CL", "cs.NE"], "abstract": "Prediction without justification has limited applicability. As a remedy, we learn to extract pieces of input text as justifications -- rationales -- that are tailored to be short and coherent, yet sufficient for making the same prediction. Our approach combines two modular components, generator and encoder, which are trained to operate well together. The generator specifies a distribution over text fragments as candidate rationales and these are passed through the encoder for prediction. Rationales are never given during training. Instead, the model is regularized by desiderata for rationales. We evaluate the approach on multi-aspect sentiment analysis against manually annotated test cases. Our approach outperforms attention-based baseline by a significant margin. We also successfully illustrate the method on the question retrieval task.", "text": "stand underlying basis decisions. ideally complex neural models would yield improved performance would also offer interpretable justiﬁcations rationales predictions. paper propose novel approach incorporating rationale generation integral part overall learning problem. limit extractive rationales. perspective rationales simply subsets words input text satisfy properties. first selected words represent short coherent pieces text second selected words must alone sufﬁce prediction substitute original text. concretely consider task multi-aspect sentiment analysis. figure illustrates product review along user rating terms categories aspects. model case predicts star rating color also identify phrase pleasant ruby red-amber color rationale underlying decision. prediction without justiﬁcation limited applicability. remedy learn extract pieces input text justiﬁcations rationales tailored short coherent sufﬁcient making prediction. approach combines modular components generator encoder trained operate well together. generator speciﬁes distribution text fragments candidate rationales passed encoder prediction. rationales never given training. instead model regularized desiderata rationales. evaluate approach multi-aspect sentiment analysis manually annotated test cases. approach outperforms attention-based baseline significant margin. also successfully illustrate method question retrieval task. many recent advances problems come formulating training expressive elaborate neural models. includes models sentiment classiﬁcation parsing machine translation among many others. gains accuracy have however come cost interpretability since complex neural models offer little transparency concerning inner workings. many applications medicine predictions used drive critical decisions including treatment options. necessary cases able verify undertion must learned entirely unsupervised manner. therefore assume model rationales trained data original neural models without access additional rationale annotations. words target rationales never provided training; intermediate step rationale generation guided desiderata discussed above. model composed modular components call generator encoder. generator speciﬁes distribution possible rationales encoder maps text task speciﬁc target values. trained jointly minimize cost function favors short concise rationales enforcing rationales alone sufﬁce accurate prediction. notion counts rationale ambiguous contexts task selecting rationales therefore challenging evaluate. focus domains ambiguity minimal ﬁrst scenario concerns multi-aspect sentiment analysis exempliﬁed beer review corpus smaller test corpus identiﬁes aspect sentence relate aspect. therefore directly evaluate predictions sentence level caveat model makes selections ﬁner level terms words complete sentences. second scenario concerns problem retrieving similar questions. extracted rationales capture main purpose questions. therefore evaluate quality rationales compressed proxy full text terms retrieval performance. model achieves high performance tasks. instance sentiment prediction task model achieves extraction accuracy compared obtained bigram neural attention baseline. developing sparse interpretable models considerable interest broader research community. need interpretability even pronounced recent neural models. efforts area include analyzing visualizing state activation decision trees trained networks. recently ribeiro propose modelagnostic framework proxy model learned target sample thus ensuring locally valid approximations. work differs terms meant explanation derived. case explanation consists concise sufﬁcient portion text mechanism selection learned jointly predictor. attention based models offer another means explicate inner workings neural models models successfully applied many problems improving prediction accuracy well visualization interpretability introduced stochastic attention mechanism together standard soft attention image captioning task. rationale extraction understood type stochastic attention although architectures objectives differ. moreover compartmentalize rationale generation downstream encoding expose knobs directly control types rationales acceptable facilitate broader modular applications. finally contrast work rationale-based classiﬁcation seek improve prediction relying richer annotations form human-provided rationales. work rationales never given training. goal learn generate them. formalize task extractive rationale generation illustrate context neural models. consider typical task provided sequence words input namely {x··· denotes vector representation word. learning problem input sequence target vector example multi-aspect sentiment analysis coordinate target vector represents response rating pertaining associated aspect. text retrieval hand target vectors used induce similarity assessments input sequences. broadly speaking solve associated learning problem estimating complex parameterized mapping input sequences target vectors. call mapping encoder. training signal vectors obtained either directly similarities challenge complex neural encoder reveals little internal workings thus offers little justiﬁcation particular prediction made. extractive rationale generation goal select subset input sequence rationale. order subset qualify rationale satisfy criteria selected words interpretable ought sufﬁce reach nearly prediction original input. words rationale must short sufﬁcient. assume short selection interpretable focus optimizing sufﬁciency cardinality constraints. encapsulate selection words rationale generator another parameterized mapping input sequences shorter sequences words. thus must include words enc) result nearly target vector original input passed encoder enc. think generator tagging model word input receives binary pertaining whether selected included rationale. case generator probabilistic speciﬁes distribution possible selections. rationale generation task entirely unsupervised sense assume explicit annotations words included rationale. another rationale introduced latent variable constraint guides interpret input sequence. encoder generator trained jointly end-to-end fashion function well together. multi-aspect sentiment prediction guiding example instantiate components encoder generator. framework generalizes tasks. encoder given training instance {xt}l input text sequence length target m-dimensional sentiment vector neural encoder predicts enc. trained encoder would minimize discrepancy predicted sentiment vector gold target vector squared error sentiment loss function encoder could realized many ways recurrent neural network. example denote parameterized recurrent unit mapping input word previous state next state target vector generated basis ﬁnal state reached recurrent unit processing words input sequence. speciﬁcally generator rationale generator extracts subset text original input function interpretable summary. thus rationale given sequence equivalently deﬁned terms binary variables {z··· indicates whether word selected not. specify binary selections thus actual rationale generated generator synonymous generator guided ways learning. first rationale produces must sufﬁce replacement input text. words target vector arising rationale close gold sentiment. corresponding loss function given second must guide generator realize short coherent rationales. select words selections form phrases rather represent isolated disconnected words. therefore introduce additional regularizer selections ﬁrst term penalizes number selections second discourages transitions note regularizer also depends generator indirectly selected rationale. easier assess rationale produced rather directly guide obtained. denote parameters encoder generator respectively collection training instances. joint objective encourages generator compress input text coherent summaries work well associated encoder trained with. minimizing expected cost challenging since involves summing possible choices rationales summation could potentially made feasible additional restrictive assumptions generator encoder. however assume possible efﬁciently sample generator. component distributions modeled using shared bi-directional recurrent neural network. speciﬁcally forward backward recurrent unit respectively independent context dependent selection words often sufﬁcient. however model unable select phrases refrain selecting word already chosen. also introduce dependent selection words joint objective rationale deﬁnition corresponds selected words i.e. {xk|zk shorthand rationale thus refers target vector obtained applying encoder rationale input. goal formalize rationale made short meaningful function well conjunction encoder. generator encoder learned jointly interact well treated independent units modularity. doubly stochastic gradient derive sampled approximation gradient expected cost objective. sampled approximation obtained separately input text work well overall stochastic gradient method. consider therefore training pair parameters generator evaluate proposed joint model applications multi-aspect sentiment analysis product reviews similar text retrieval askubuntu question answering forum. multi-aspect sentiment analysis dataset beeradvocate review dataset used prior work dataset contains million reviews written website users. reviews naturally multiaspect contains multiple sentences describing overall impression particular aspect beer including appearance smell palate taste. addition written text reviewer provides ratings aspect well overall rating. ratings fractional normalize scores supervision regression. last term expected gradient expectation taken respect generator distribution rationales therefore simply sample rationales generator resulting average gradient overall stochastic gradient method. sampled approximation gradient respect encoder parameters derived similarly choice recurrent unit employ recurrent convolution reﬁnement local-ngram based convolution. rcnn attempts learn n-gram features necessarily consecutive average features dynamic fashion. speciﬁcally bigrams rcnn computes follows table precision selected rationales ﬁrst three aspects. precision evaluated based whether selected words sentences describing target aspect based sentence-level annotations. best training epochs selected based objective value development table comparing neural encoders bigram model. mean squared error test set. amount data used training development. stands hidden dimension denotes depth network denotes number parameters table shows several statistics beer review dataset. sentiment correlation pair aspects quite high getting average maximum directly training model model confused strong correlation. therefore perform preprocessing step picking less correlated examples dataset. gives de-correlated subset aspect containing reviews. development set. focus three aspects since fourth aspect taste still gets correlation overall sentiment. sentiment prediction training joint model worth assessing neural encoder separately check accurately neural network predicts sentiment. compare neural encoders bigram model training medium large models using speciﬁcally aspect train simple linear regression model predict rating aspect given ratings four aspects. keep picking reviews largest prediction error sentiment correlation selected subset increases dramatically. reviews respectively. shown table recurrent neural network models outperform model sentiment prediction also require less training data achieve performance. lstm rcnn units obtain similar test error getting mean squared error respectively. rcnn unit performs slightly better uses less parameters. based results choose rcnn encoder network stacking layers hidden states. train joint model also rcnn unit states forward backward recurrent unit generator gen. dependent generator additional recurrent layer. layer states dependent version still number parameters comparable independent version. versions generator parameters respectively. figure shows performance joint dependent model trained predict sentiment aspects. vary regularization show various runs extract different amount text rationales. joint model gets performance close best encoder words extracted. figure examples extracted rationales indicating sentiments various aspects. extracted texts appearance smell palate shown blue green color respectively. last example shortened space. rationale selection evaluate supporting rationales aspect train joint encodergenerator model de-correlated subset. cardinality regularization values extracted rationale texts neither long short. simplicity encourage local coherency extraction. comparison bigram model implement attention-based neural network model. model successively extracts unigram bigram highest feature. attention-based model learns normalized attention vector input tokens model averages encoder states accordingly attention feed averaged vector output layer. similar model attention-based model selects words based attention weights. table presents precision extracted rationales calculated based sentence-level aspect annotations. regularization hyper-parameter tuned versions model extract similar number words rationales. attention-based model constrained similarly comparison. figure shows precision different amounts text extracted. again model corresponds changing regularization. shown table ﬁgure encoder-generator networks extract text pieces describing target aspect high precision ranging across three aspects appearance smell palate. baseline performs poorly achieving around accuracy. attention-based model achieves reasonable worse performance rationale generator suggesting potential directly modeling rationales explicit extraction. figure shows learning curves model smell aspect. early training epochs independent dependent selection models fail produce good rationales getting precision result. epochs exploration however models start achieve high accuracy. observe dependent version learns quickly general versions obtain close results end. finally conduct qualitative case study extracted rationales. figure presents several reviews highlighted rationales predicted model. rationale generator identiﬁes phrases adjectives indicate sentiment particular aspect. similar text retrieval forum dataset second application real-world askubuntu dataset used recent work contains unique questions useridentiﬁed similar question pairs. following previous work data used train neural encoder learns vector representation input question optimizing cosine distance similar questions random non-similar ones. one-versusall hinge loss encoder similar during development testing model used score candidate questions given query question total query-candidate question pairs annotated evaluation. task/evaluation setup question descriptions often long fraught irrelevant details. set-up fraction original question text sufﬁcient represent content used retrieving similar questions. therefore evaluate rationales based accuracy question retrieval task assuming better rationales achieve higher performance. performance context also report accuracy full body question used well titles alone. latter constitutes upper bound model performance dataset titles provide short informative summaries question content. evaluate rationales using mean average precision retrieval. results table presents results rationale model. explore range hyper-parameter values. include runs version. ﬁrst achieves highest development second selected compare models roughly question text also show results different runs figure rationales achieve getting close using titles. models also outperform baseline using noisy question bodies indicating models’ capacity extracting short important fragments. figure shows rationales several questions askubuntu domain using recurrent version around extraction. interestingly model always select words question title. reasons question body contain even complementary information useful retrieval. indeed rationale fragments shown ﬁgure error messages dropout proposed novel modular neural framework automatically generate concise sufﬁcient text fragments justify predictions made neural networks. demonstrated encoder-generator framework trained end-to-end manner gives rise quality rationales absence explicit rationale annotations. approach could modiﬁed extended various ways applications types data. choices gen. encoder generator realized numerous ways withchanging broader algorithm. instance could convolutional network deep averaging network boosting classiﬁer encoder. rationales expected conform repeated stereotypical patterns text simpler encoder consistent bias work better. emphasize that paper rationales ﬂexible explanations vary substantially instance another. generator side many additional constraints could imposed guide acceptable rationales. additional constraints generator output helpful alleviating problems exploring potentially large space possible rationales terms interaction encoder. could also apply variance reduction techniques increase stability stochastic training thank prof. julian mcauley sharing review dataset annotations. also thank group reviewers helpful comments. work supported arabic language technologies group qatar computing research institute within iyas project. opinions ﬁndings conclusions recommendations expressed paper authors necessarily reﬂect views funding organizations.", "year": 2016}