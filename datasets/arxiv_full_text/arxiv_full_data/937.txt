{"title": "MADE: Masked Autoencoder for Distribution Estimation", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder's parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.", "text": "recent interest designing neural network models estimate distribution examples. introduce simple modiﬁcation autoencoder neural networks yields powerful generative models. method masks autoencoder’s parameters respect autoregressive constraints input reconstructed previous inputs given ordering. constrained autoencoder outputs interpreted conditional probabilities product full joint probability. also train single network decompose joint probability multiple different orderings. simple framework applied multiple architectures including deep ones. vectorized implementations gpus simple fast. experiments demonstrate approach competitive stateof-the-art tractable distribution estimators. test time method signiﬁcantly faster scales better autoregressive estimators. distribution estimation task estimating joint distrit= bution examples {x}t deﬁnition general problem. many tasks machine learning formulated learning speciﬁc properties joint distribution. thus good distribution estimator used many scenarios including classiﬁcation denoising missing input imputation data synthesis many others. nature distribution estimation also makes particular challenge machine learning. essence curse dimensionality distinct impact because number dimensions input space grows volume space model must provide good answer exponentially increases. fortunately recent research made substantial progress task. speciﬁcally learning algorithms variety neural network models proposed algorithms showing great potential scaling high-dimensional distribution estimation problems. work focus attention autoregressive models computing exactly test example tractable models. however computational cost operation still larger typical neural network predictions d-dimensional input. previous deep autoregressive models evaluating costs times simple neural network point predictor. paper’s contribution describe explore simple adapting autoencoder neural networks makes competitive tractable distribution estimators faster existing alternatives. show mask weighted connections standard autoencoder convert distribution estimator. masks designed output autoregressive given ordering inputs i.e. input dimension reconstructed solely dimensions preceding ordering. resulting masked autoencoder distribution estimator preserves efﬁciency single pass regular autoencoder. implementation straightforward making method scalable. single hidden layer version made corresponds previously proposed autoregressive neural network bengio bengio here exploring deep variants model. also explore training made work simultaneously multiple orderings input observations hidden layer connectivity structures. test extensions across range binary datasets hundreds dimensions compare statistical performance scaling comparable methods. brief description basic autoencoder work builds upon required clearly grasp follows. paper assume given training concentrate case binary examples {x}t observations every d-dimensional input input dimension belongs motivation learn hidden representations inputs reveal statistical structure distribution generated them. matrices vectors nonlinear activation function sigm thus represents connections input hidden layer represents connections hidden output layer. cross-entropy understood taking form negative log-likelihood function. training autoencoder corresponds optimizing parameters reduce average loss training examples usually stochastic gradient descent. output layers. main disadvantage representation learns trivial. instance hidden layer least large input hidden units learn copy single input dimension reconstruct inputs perfectly output layer. obvious consequence observation loss function equation isn’t fact proper log-likelihood function. indeed since perfect reconstruction could achieved interesting question property could impose autoencoder output used obtain valid probabilities. speciﬁcally we’d like able write could computed based output properly corrected autoencoder. connection provides deﬁne autoencoders used distribution estimation. output must function taking input outputting probability observing value dimension. particular autoencoder forms proper distribution output unit depends previous input units units refer property autoregressive property computing negative log-likelihood equivalent sequentially predicting dimension input question modify autoencoder satisfy autoregressive property. since output must depend preceding inputs means must computational path output unit input units words paths least connection must convenient zeroing connections elementwisemultiply matrix binary mask matrix whose entries correspond connections wish remove. single hidden layer autoencoder write impose autoregressive property ﬁrst assign unit hidden layer integer inclusively. hidden unit’s number gives maximum number input units connected. disallow since hidden unit would depend inputs could used modelling conditionals similarly exclude would create constant hidden units. number network paths output unit input unit thus demonstrate autoregresˆ sive property need show strictly lower diagonal i.e. deﬁnition matrix product have constructing masks requires assignment values hidden unit. could imagine trying assign equal number units legal value experiments instead sampling uniform discrete distribution deﬁned integers independently hidden units. parameter connection matrix mask matrix. satisfy autoregressive property simply needs strictly lower diagonal matrix ﬁlled otherwise ones. used direct connections experiments well. overall need encode constraint output unit connected therefore output weights connect output hidden units i.e. units connected input units. constraints encoded output mask matrix notice that rule hidden units connected ﬁrst output unit desired. mask constructions easily demonstrate corresponding masked autoencoder satisﬁes autoregressive property. first note that since masks represent network’s connectivity matrix product mvmw represents connectivity input output layer. speciﬁcally advantage masked autoencoder framework described previous section naturally generalizes deep architectures. indeed we’ll assigning maximum number connected inputs units across deep network masks similarly constructed satisfy autoregressive property. networks hidden layers superscripts index layers. ﬁrst hidden layer matrix denoted second hidden layer matrix number hidden units hidden layer similarly indexed hidden layer index. also generalize notation maximum number connected inputs unit layer we’ve already discussed deﬁne ﬁrst layer’s mask matrix ensures unit connected inputs. impose property second hidden layer must simply make sure unit connected ﬁrst layer units we’ve assumed conditionals modelled made consistent natural ordering dimensions however might interested modelling conditionals associated arbitrary ordering input’s dimensions. speciﬁcally uria shown training autoregressive model orderings beneﬁcial. refer approach order-agnostic training. achieved sampling ordering stochastic/minibatch gradient update model. advantages approach. firstly missing values partially observed input vectors imputed efﬁciently invoke ordering observed dimensions before unobserved ones making inference straightforward. secondly ensemble autoregressive models constructed exploiting fact conditionals different orderings guaranteed exactly consistent ensemble easily obtained sampling orderings computing probability ordering averaging. conveniently made ordering simply represented vector speciﬁcally corresponds position original dimension product conditionals. thus random ordering obtained randomly permuting ordered vector values ﬁrst hidden layer mask matrix created. order-agnostic training randomly permuting last value sufﬁcient obtain random ordering. advantage order-agnostic training effectively allows train many models orderings using common parameters. exploited creating ensembles models test time. made addition choosing ordering also choose hidden unit’s connectivity constraint thus could imaging training made also agnostic connectivity pattern generated constraints. achieve this instead sampling values units layers training actually resample training example minibatch. still practical since operation creating masks easy parallelize. denoting assuming element-wise parallel implementation operation vectors matrix figure left conventional three hidden layer autoencoder. input bottom passed fully connected layers point-wise nonlinearities. ﬁnal layer reconstruction speciﬁed probability distribution inputs produced. distribution depends input itself standard autoencoder cannot predict sample data. right made. network structure autoencoder connections removed input unit predicted previous ones using multiplicative binary masks example ordering input changed change explained section necessary understanding basic principle. numbers hidden units indicate maximum number inputs unit layer depends. masks constructed based numbers masks ensure made satisﬁes autoregressive property allowing form probabilistic model example connections light gray correspond paths depend input dark gray connections depend inputs. also taking mean input layer deﬁning deﬁnition also applies ﬁrst hidden layer weights. output mask simply need adapt deﬁnition using connectivity constraints last hidden layer instead ﬁrst resampling connectivity hidden units every update hidden unit constantly changing number incoming inputs training. however absence connection indistinguishable instantiated connection zero-valued unit could confuse neural network training. similar situation uria informed hidden unit units providing input binary indicator variables connected additional learnable weights. considered applying similar strategy using companion weight matrices also masked connected constant one-valued vector analogous parametrization output layer also employed. connectivity conditioning weights sometimes useful. experiments treated choice using hyperparameter. moreover we’ve found experiments sampling masks every example could sometimes over-regularize made provoke underﬁtting. issue also considered sampling ﬁnite list masks. training made cycles list using every update. test time average probabilities obtained masks list. algorithm details computed made well obtain gradient stochastic gradient descent training. simplicity pseudocode assumes order-agnostic connectivity-agnostic training doesn’t assume conditioning weight matrices direct input/output connections. figure also illustrates example two-layer made network along values masks. recent work exploring feed-forward autoencoder-like neural networks probabilistic generative models. part motivation behind research test common assumption models probabilistic latent variables intractable partition functions necessary evil designing powerful generative models high-dimensional data. work neural autoregressive distribution estimator nade illustrated feed-forward architectures fact used form stateof-the-art even tractable distribution estimators. recently deep extension nade proposed improving even state-of-the-art distribution estimation work introduced randomized training procedure nearly cost iteration standard autoencoder. unfortunately deep nade models still require feed-forward passes network evaluate probability d-dimensional test vector. computation ﬁrst hidden layer’s activations shared across passes deep autoregressive networks also provide probabilistic models roughly training costs standard autoencoders. darn’s latent representation consist binary stochastic hidden units. simulating models fast evaluation exact test probabilities requires summing conﬁgurations latent representation exponential computation. monte carlo approximation thus recommended. main advantage made evaluating probabilities retains efﬁciency autoencoders minor additional cost simple masking operations. table lists computational complexity exact computation probabilities various models. darn rbms exponential dimensionality hiddens data whereas nade made polynomial. made requires pass autoencoder rather passes required nade. practice also observe single-layer made order magnitude faster one-layer nade hidden layer size despite nade sharing computation asymptotic scaling. nade’s computations cannot vectorized efﬁciently. deep versions made also better scaling nade test time. training costs made darn deep nade similar. work nade bengio bengio proposed neural network architecture corresponds special case single hidden layer made model without randomization input ordering connectivity. contribution work beyond special case exploring deep variants order/connectivity-agnostic training. interesting interpretation autoregressive mask sampling structured form dropout regularization speciﬁcally bears similarity masking dropconnect networks exception masks generated must guaranty autoregressive property autoencoder element mask generated independently. test performance model considered different benchmarks suite binary datasets binarized mnist dataset. code reproduce experiments paper available https//github.com/mgermain/made/releases/tag/icml. results reported average negative loglikelihood test respective dataset. experiments made using stochastic gradient descent mini-batches size lookahead early stopping. binary evaluation suite ﬁrst together larochelle murray it’s collection relatively small datasets university california irvine machine learning repository ocr-letters dataset stanford lab. table gives overview scale datasets split. experiments networks units hidden layer using adadelta learning update decay hyperparameters varied table indicates. note masks number different masks made cycles training. limit case masks sampled never explicitly reused unless re-sampled chance. situation validation test time sampled masks used averaging probabilities. results reported table made among best performing models half datasets competitive otherwise. reduce clutter reported standard deviations fairly small consistent across models. however completeness report standard deviations separate table supplementary materials. analysis hyperparameters selected dataset reveals clear winner. however table mask sampling helps helps quite impact negligible ocrletters. another interesting note conditioning weights almost inﬂuence except nips-- helped. version mnist used binarized salakhutdinov murray mnist hand written digits pixels. split larochelle murray consisting training validation test set. experiments using adagrad learning update epsilon since made much efﬁcient nade considered varying hidden layer size units. seeing increasing number units tended always help used even large hidden layer implementation made quite efﬁcient. using single mask training epoch requires seconds hidden layer hidden layer made respectively. using sampled masks training time increases respectively. timings less implementation hidden units nade model requires seconds epoch. timings obtained nvidia gpu. taken literature. again despite tractability made competitive models. note fact best made model outperforms single layer nade network otherwise best model among requiring single feed-forward pass compute probabilities. experiments clearly observed overregularization phenomenon using many masks. four orderings used deeper variant made always yielded better results. layer model adding masks training helped point negative log-likelihood started increase. observed similar pattern single layer model case around masks. figure illustrates behaviour precisely single layer made hidden units trained varying number masks used size mini-batches randomly sampled digits best performing model table compared nearest neighbor training ensure generated samples simple memorization. digits uses different mask seen training time network. table negative log-likelihood test results different models multiple datasets. best result well result overlapping conﬁdence interval shown bold. note since variance darn available considered zero. proposed made simple modiﬁcation autoencoders allowing used distribution estimators. made demonstrates possible direct cheap estimates high-dimensional joint probabilities single pass autoencoder. like standard autoencoders extension easy vectorize implement gpus. made evaluate high-dimensional probably distributions better scaling before maintaining state-of-the-art statistical performance. references bastien fr´ed´eric lamblin pascal pascanu razvan bergstra james goodfellow bergeron arnaud bouchard nicolas bengio yoshua. theano features speed improvements. deep learning unsupervised feature learning nips workshop bengio yoshua bengio samy. modeling highdimensional discrete data multi-layer neural networks. advances neural information processing systems press bengio yoshua laufer eric alain guillaume yosinski jason. deep generative stochastic networks trainable backprop. proceedings annual international conference machine learning jmlr.org bergstra james breuleux olivier bastien fr´ed´eric lamblin pascal pascanu razvan desjardins guillaume turian joseph warde-farley david bengio yoshua. theano math expression compiler. proceedings python scientiﬁc computing conference june oral presentation. duchi john hazan elad singer yoram. adaptive subgradient methods online learning stochastic optimization. technical report eecs department university california berkeley goodfellow pouget-abadie jean mirza mehdi bing warde-farley david ozair sherjil courville aaron bengio yoshua. generative adversarial nets. advances neural information processing systems curran associates inc. gregor karol danihelka mnih andriy blundell charles wierstra daan. deep autoregressive networks. proceedings annual international conference machine learning jmlr.org larochelle hugo murray iain. neural autoreproceedings gressive distribution estimator. international conference artiﬁcial intelligence statistics volume lauderdale jmlr w&cp. rezende danilo jimenez mohamed shakir wierstra daan. stochastic backpropagation approximate inference deep generative models. proceedings international conference machine learning salakhutdinov ruslan murray iain. quantitative analysis deep belief networks. proceedings annual international conference machine learning omnipress schmah tanya hinton geoffrey zemel richard small steven strother stephen generative versus discriminative training rbms classiﬁcation fmri images. advances neural information processing systems srivastava nitish hinton geoffrey krizhevsky alex sutskever ilya salakhutdinov ruslan. dropout simple prevent neural networks overﬁtting. journal machine learning research uria benigno murray iain renals steve valentinibotinhao cassia. modelling acoustic feature dependencies artiﬁcial neural networks trajectory-rnade. proceedings ieee international conference acoustics speech signal processing ieee signal processing society zeiler matthew zhang sixin lecun yann fergus rob. regularization neural networks usproceedings internaing dropconnect. tional conference machine learning", "year": 2015}