{"title": "Learning Compact Convolutional Neural Networks with Nested Dropout", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Recently, nested dropout was proposed as a method for ordering representation units in autoencoders by their information content, without diminishing reconstruction cost. However, it has only been applied to training fully-connected autoencoders in an unsupervised setting. We explore the impact of nested dropout on the convolutional layers in a CNN trained by backpropagation, investigating whether nested dropout can provide a simple and systematic way to determine the optimal representation size with respect to the desired accuracy and desired task and data complexity.", "text": "recently nested dropout proposed method ordering representation units autoencoders information content without diminishing reconstruction cost however applied training fully-connected autoencoders unsupervised setting. explore impact nested dropout convolutional layers trained backpropagation investigating whether nested dropout provide simple systematic determine optimal representation size respect desired accuracy desired task data complexity. supervised convolutional neural networks learn representations effective wide range tasks. drawback current approaches however selection architectures largely optimized hand researchers explicitly searching architecture hyperparameters cross-validation. model selection network architectures explored context learning network connectivity dating back optimal brain damage lecun continued explored context learning optimal sparse models. knowledge deep visual network capable increasing representation capacity based complexity available data tasks. recently proposed nested dropout method implicitly accomplishes this learning deep representation units incremental fashion. investigate whether approach applicable visual models propose visual model learn scale capacity according complexity data presented network training. standard dropout units layer independently dropped probability namely output unit zero. traditionally applied convolutional fully-connected layers training time shown regularizer discouraging over-ﬁtting training data though also applied entire channels convolutional layer output tompson empirically training large networks trained imagenet drop necessary avoid ﬁtting nested dropout hand randomly draws unit indices geometric distribution drops units follow number drawn e.g. number drawn units kept remaining units dropped. applied single layer semi-linear autoencoder technique proven enforce ordering units information capacity decreasing ﬂexibility representation quality resulting solution primary contributions paper demonstrate nested dropout successfully applied convolutional layers trained back-propagation propose nested dropout advantageous method learn cnns adapt task data complexity deep learning setting provide implementation caffe widely used deep learning framework upon publication. nested dropout algorithm convolutional layer channels follows sample mini-batch draw number geometric distribution drop latter channels output layer. nested dropout also applied multiple layers network applying nested dropout layer iteratively. first number ﬁlters layer determined nested dropout. ﬁxing number ﬁlters layer nested dropout used determine number ﬁlters layer trained cnns using stochastic gradient descent mini-batches samples. dropped units determined drawing geometric distribution units index rarely dropped thus converge quickly whereas latter units frequently dropout thus learned slowly. filters incrementally ﬁxed converged remaining ﬁlters considered drawing numbers geometric distribution. though incrementing sweeping index could done upon ﬁlter convergence achieved satisfactory results simply incrementing unit sweeping index number iterations. implement caffe framework added nested dropout layer follow layer standard dropout layer. also added customizations solver support unit sweeping training. figure accuracy function number ﬁlters conv without nested dropout. oracle consists separate networks whereas nested dropout curve requires training single network. brain damaged baseline trained without nested dropout best ﬁlters used test. nested dropout network achieves maximum accuracy compact representation ﬁlters requiring considerably fewer iterations brute force approach. apply nested dropout ﬁrst convolutional layer trained classify images cifar- dataset using default caffe architecture training ﬁxed learning rate. figure show test accuracy single network trained nested dropout function number conv ﬁlters. report accuracy ﬁlters using partially-trained network ﬁlter converged testing last ﬁlters dropped out. compare naive approaches select number ﬁlters conv. ﬁrst approach simply trains separate networks differ number conv ﬁlters. second approach trains single network ﬁlters test time varying number conv ﬁlters used remaining dropped out. note inherent ordering learned representation without nested dropout removing ﬁlter severely damages network resulting accuracies. experiments figure demonstrate nested dropout efﬁciently determines relationship model capacity test accuracy. nested dropout implementation requires iterations training whereas brute force approach requires signiﬁcantly more millions iterations. example training separate networks completion requires iterations. figure conv ﬁlters networks trained nested dropout without note latter ﬁlters carry little information networks achieve similar performance giving indication many units necessary model. figure visualize ﬁlters learned without nested dropout. though latter ﬁlters carry little information test accuracy sets ﬁlters comparable test accuracy network trained nested dropout test accuracy baseline network. applying nested dropout second convolutional layer yielded similar results experiments conv. training network nested dropout applied conv ﬁlters determined conv ﬁlters necessary achieve maximum classiﬁcation accuracy network. next train network nested dropout follows conv unit sweeping index learning conv ﬁlters training accuracy converges thus using nested dropout reduce total number parameters ﬁrst layers ﬁlters maintaining similar accuracy. summary provided simple method determining compact representation convolutional layers. experiments learned representation achieved classiﬁcation accuracy using conv ﬁlters conv ﬁlters rather baseline each within optimization framework. main advantage method enables network gradually increase network capacity training. additionally hope that future ordering parameters provide insights optimization deep convolutional neural networks network architecture impacts performance. work supported part darpa’s msee smisc programs awards iis- iis- toyota berkeley vision learning center. chelsea finn supported berkeley eecs fellowship lisa anne hendricks ndseg fellowship. references hinton geoffrey srivastava nitish krizhevsky alex sutskever ilya salakhutdinov improving neural networks preventing co-adaptation feature detectors. corr yangqing shelhamer evan donahue jeff karayev sergey long jonathan girshick ross guadarrama sergio darrell trevor. caffe convolutional architecture fast feature emproceedings international conference multimedia bedding. krizhevsky alex sutskever ilya hinton geoffrey imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems tompson jonathan goroshin ross jain arjun lecun yann bregler christoph. efﬁcient object localization using convolutional networks. corr abs/. http //arxiv.org/abs/..", "year": 2014}