{"title": "Neural Networks for Complex Data", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "Artificial neural networks are simple and efficient machine learning tools. Defined originally in the traditional setting of simple vector data, neural network models have evolved to address more and more difficulties of complex real world problems, ranging from time evolving data to sophisticated data structures such as graphs and functions. This paper summarizes advances on those themes from the last decade, with a focus on results obtained by members of the SAMM team of Universit\\'e Paris 1", "text": "abstract artiﬁcial neural networks simple efﬁcient machine learning tools. deﬁned originally traditional setting simple vector data neural network models evolved address difﬁculties complex real world problems ranging time evolving data sophisticated data structures graphs functions. paper summarizes advances themes last decade focus results obtained members samm team universit´e paris artiﬁcial neural networks provide efﬁcient techniques machine learning data mining solutions mainly developed handle vector data analyzed theoretically context statistically independent observations. however last decade seen numerous efforts overcome limitations survey article resulting solutions. focus attention major artiﬁcial neural network models multi-layer perceptron self-organizing multi-layer perceptron well known artiﬁcial neural network model statistical point view considered parametric family regression functions. technically data consists vector observations object described vector output hidden layer perceptron hidden neurons given vectors real numbers equation bounded transfer function introduces linearity given training examples pairs learning process consists minimizing distance target value predicted value. given error criterion optimal value determined optimization algorithm leveraging well know backpropagation algorithm enables fast computation derivatives many real world applications machine learning related techniques data anymore standard simple tabular format object described common ﬁxed numerical attributes. standard vector model useful efﬁcient obvious limitations limited numerical attributes cannot handle objects uniform descriptions relations objects etc. addition quite common real world applications dynamic aspect sense data study results temporal process. then traditional hypothesis statistical independence observations hold anymore hypothesis theoretical analysis needed justify mathematical soundness machine learning methods context. well known since seminal paper efﬁcient solution modeling time series whenever linear model proves inadequate. simplest approach consists building linear auto-regressive model given real valued time series builds training pairs vector deﬁned used learn mapping regression problem. order avoid overlearning and/or large computation time question selecting correct number neurons generally question model selection arises immediately. standard methods used neural-networks community based pruning trains possibly large removes useless neurons and/or connection weights. heuristic solutions include optimal brain damage optimal brain surgeon statistically founded method introduced method relies minimization bayesian information criterion shortly after proved consistency case mlps hidden layer. results established time series allow generalize consistency results case. convergence properties generalized even further. ﬁrst extension given noise supposed gaussian transfer function supposed bounded three times derivable. shows mild hypothesis maximum likelihood-ratio test statistic converges toward maximum square gaussian process indexed class limit score functions. theorem establishes tightness likelihood-ratio test statistic particular consistency penalized likelihood criteria bic. practical applications methods found hypothesis noise relaxed noise longer supposed gaussian admit exponential moments. general assumption criterion still consistent note choice penalty term important. simulated data good results reported enlog also mention tightness lrts particular consistency criterion recently established complex neural-networks models mixtures mlps mixtures experts mentioned previous section useful tool modeling time series. however results cited available data stationary time series. order deal highly nonlinear nonstationary time series hybrid model involving hidden markov models multilayer perceptrons proposed consider homogeneous markov chain valued ﬁnite state-space {e··· observed time series. hybrid hmm/mlp model written follows fxt+ σxt+εt+ fxt+ {fe··· fen} regression function order case i-th model parameterized weight vector σxt+ {σe··· σen} strictly positive number sequence standard gaussian variables. estimating procedure well statistical properties parameter estimates established proposed model successfully applied modeling difﬁcult data sets ozone peaks ﬁnancial shocks trees graphs clustered versions using temporal coding structure. recent advances line research include e.g. speciﬁc adaptation include symbol strings described clustering algorithm used frequently supervised context component complex model. described brieﬂy model example complex time series processing som. consider time series time scales i.e. written subscripts. date denoted represents slow time scale corresponds instance corresponds observed values time series denoted assume addition slow time scale associated metadata. instance corresponds year knows week month etc. metadata supposed available prior prediction. original time series takes value dual time scale leads naturally vector valued time series representation point view given past vector valued time series predict future vector value complete vector values. could seen long term forecasting problem usual solution would iterate one-step ahead forecasts. however leads generally unsatisfactory solutions either squashing behaviour chaotic behaviour alternative solution explored consists forecasting separately hand mean variance time series next slow time scale step hand proﬁle fast time scale. prediction mean variance done classical technique. proﬁle used follows. vector values time series i.e. centred normalized respect fast time scale transformed proﬁles deﬁned general formula applies data space linear forms deﬁned give data space linear functions deﬁne general neuron help calculating generalization particularly suitable functional data data object described several functions type data quite common instance multiple time series setting spectrometry. functional neuron deﬁned calculating parameter function. results show based type neurons share many interesting properties classical universal approximation statistical consistency alternative functional neuron similar properties). addition parameter functions represented standard numerical leading hierarchical solution level functional data obtained using numerical functional neurons. experimental results show practical relevance technique. kohonen’s self-organizing well known artiﬁcial neural network model clustering visualization model vector observations mapped neurons organized dimensional prior structure mainly dimensional grid dimensional string. neuron associated codebook vector prototype based clustering methods represents data points assigned corresponding neuron sense close points distinctive feature prototype also somewhat representative data points assigned neurons based geometry prior structure neurons neighbours prior structure close data points assigned neuron contrary away prior structure data points assigned neuron inﬂuence prototype neuron. important consequences terms visualization capabilities illustrated instance. original algorithm designed vector data numerous adaptations complex data proposed. survey three speciﬁc extensions respectively time series functional data categorical data. another important extension covered proposed built upon processing multiple time series recursive versions som. authors show slow time scale. metadata vector predict matched metadata associated neurons assume instance metadata days week predict sunday. collects neurons sunday proﬁles assigned. finally weighted average matching prototypes computed rescaled according shown technique enables stable meaningful full predictions integrating numerical metadata. approach described transformation achieved using rows transformed tables thus trained. training provides organized clustering possible values categorical variables prior structure dimensional grid. moreover simultaneous representation individuals values needed coupled trained superimposed. aforementioned articles present various real-world cases socio-economic ﬁeld. dual time scale approach described previous section become standard dealing time series functional shown e.g. pointed section functional data arise naturally contexts spectrometry. then naturally adapted functional data contexts time series. contexts addition normalization technique described produces proﬁles functional transformation derivative calculations order drive clustering process shapes functions rather mainly average values another adaptation consists integrating optimal segmentation techniques represent functions time series simple models piecewise constant functions instance. main idea apply functional data using functional distance additional constraint prototypes must simple e.g. piecewise constant. leads interesting visualization capabilities complexity display automatically globally adjusted surveys quite standard collected answers categorical variables ﬁnite number possible values. case speciﬁc adaptation algorithm deﬁned multiple correspondence analysis related principal component analysis. precisely useful encoding methods categorical data burt table full contingency table between pairs categories variables complete disjunctive table contains answers individual coded dummy variables correspond categories variables. then multiple correspondence analysis nothing else principal component analysis previously transformed take account speciﬁc distance rows weighting individuals adapted categorical data using extensions artiﬁcial neural networks model described previous sections sense constructed using speciﬁc features data hand. strength also limitation universal given data type design adaptation general technique. present section present general versions based dissimilarity kernel input data. assuming existence measure weaker assuming data vector format. instance simple deﬁne dissimilarity/similarity vertices graph data structure frequent real world problems representing directly vertices vectors generally difﬁcult. assume data study belong dissimilarity deﬁned function maps pair objects negative real number measures different are. hypothesis minimal symmetric pointed above dissimilarities readily available sets vector data. classical example string edit distance deﬁnes distance symbol strings. general edit distances deﬁned instance graph edit distance measure distances graphs hypothesis minimal cannot assume anymore vector calculation possible set. then learning rules apply based linear combination prototypes data points. circumvent difﬁculty suggest chose values prototypes observations leads batch version proceeds alternative approach dissimilarities rely kernels. kernels seen generalization notion similarity. precisely kernel symmetric function satisﬁes positivity property ∀≤i≤n ∀≤i≤n shown e.g. kernels convenient extend standard machine learning methods arbitrary spaces. indeed feature space comes elementary operations linear combination inner product norm distance. then work feature space original data space. difﬁculty comes fact explicit general mainly follows. random initialization prototypes observation assigned neuron closest propotype prototypes updated. neuron updated chosen among observations minimizer following distortion xi’s neuron decreasing function distance neurons prior structure. modiﬁcation algorithm known median closely related earlier median version standard k-means algorithm case small sample constraint chose prototypes data seen strong. then suggests associate several prototypes neuron. neuron represented subset size different steps algorithm modiﬁed accordingly. fast implementation described successful application dissimilarity real world data concerns school-to-work transitions. interested identifying career-path typologies challenging topic economists working labor market. data issued generation’ survey cereq. data sample contained information young people graduated monitored months left school. labor-market statuses nine categories permanent contracts unemployed including military service inactivity higher education. dissimilarity matrix computed using optimal matching distances currently main stream economy sociology. striking opposition appeared career-paths leading stableemployment situations chaotic ones. stable positions mainly situated west region map. however north south regions quite different north-west region access permanent contract achieved ﬁxed-term contract south-west classes subject transitions military service education stability career paths getting worse moved east map. north-east region initial ﬁxed-term contract getting longer becoming precarious south-east region characterized excluding trajectories unemployment inactivity case batch version quite simple indeed assignments data points neurons based euclidean distance classical numerical case translates directly distance feature space calculated solely using kernel prototypes update performed weighted averages data points weights computed function introduced equation proxy prior structure. shown weights computed using assignments only sufﬁcient deﬁne prototypes plugged distance calculation without needing explicit calculation variants scheme especially stochastic ones studied also noted relational approach mentioned previous section seen relaxed kernel application similar algorithm situations function positive. kernels convenient positivity conditions might seem strong ﬁrst. indeed much stronger conditions imposed dissimilarity instance. nevertheless numerous kernels deﬁned complex data ranging kernels strings based substrings kernel vertices graph heat kernel based application kernel medieval data notarial acts). graphs also compared kernel based random walks subtrees comparisons present days data becoming complex according several criteria structure time evolution volume adapting artiﬁcial neural networks data continuous challenge solved mixing different strategies outlined paper adding complexity models enable tackle standard behavior theoretical guarantees limit risk overﬁtting models tailor made speciﬁc data structures graph functions generic kernel/dissimilarity models handle almost type data. ability combine strategies demonstrates ﬂexibility artiﬁcial neural network paradigm.", "year": 2012}