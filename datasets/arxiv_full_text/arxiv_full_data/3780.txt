{"title": "A Short Survey on Data Clustering Algorithms", "tag": ["cs.DS", "cs.CV", "cs.LG", "stat.CO", "stat.ML"], "abstract": "With rapidly increasing data, clustering algorithms are important tools for data analytics in modern research. They have been successfully applied to a wide range of domains; for instance, bioinformatics, speech recognition, and financial analysis. Formally speaking, given a set of data instances, a clustering algorithm is expected to divide the set of data instances into the subsets which maximize the intra-subset similarity and inter-subset dissimilarity, where a similarity measure is defined beforehand. In this work, the state-of-the-arts clustering algorithms are reviewed from design concept to methodology; Different clustering paradigms are discussed. Advanced clustering algorithms are also discussed. After that, the existing clustering evaluation metrics are reviewed. A summary with future insights is provided at the end.", "text": "iterative reﬁnement approach main steps. ﬁrst step choose means clusters centroids whereas second step assign data points nearest centroids. practice computational speed simplicity appeal people main drawback vulnerability random seeding technique. words initial seeding positions chosen correctly clustering result quality affected adversely. light that david arthur sergei vassilvitskii proposed method called k-means++ improve k-means section ..and observe steps k-means++ exactly k-means. main difference lies step seeding technique. seeding technique proposed replace arbitrary seeding technique k-mean. given seeds chosen seeding technique favors data points seeds already chosen. thus seeds chosen probabilistically dispersed possible. k-means++ extended version k-means method conducted numerical experiments evaluate compare performance replicate runs. better visual inspection visualization datasets performance values depicted tabulated fig. observe k-means++ perform better k-means ﬁrst three datasets. clustering score time taken improved. however performance comparison relatively complicated last dataset. abstract—with rapidly increasing data clustering algorithms important tools data analytics modern research. successfully applied wide range domains; instance bioinformatics speech recognition ﬁnancial analysis. formally speaking given data instances clustering algorithm expected divide data instances subsets maximize intra-subset similarity inter-subset dissimilarity similarity measure deﬁned beforehand. work state-of-the-arts clustering algorithms reviewed design concept methodology; different clustering paradigms discussed. advanced clustering algorithms also discussed. that existing clustering evaluation metrics reviewed. summary future insights provided end. nowadays support science technology large amounts data been continue accumulated. example single human genome accounts four gigabytes data space transaction logs ﬁnancial markets measured billions large amount data overwhelming prevents applying traditional analysis techniques. scalable methods need devised handle main analysis tools cluster analysis methods proposed separate large amount data clusters. data clustering methods unsupervised means label model training; even know exact number clusters beforehand. given data clustering method expected divide data several clusters itself. formally speaking given data instances data clustering method expected divide data instances subsets maximize intra-subset similarity inter-subset dissimilarity similarity measure deﬁned beforehand. since data clustering problems shown np-hard different methods proposed past. general methods categorized different paradigms partitional clustering hierarchical clustering density-based clustering grid-based clustering correlation clustering spectral clustering gravitational clustering herd clustering others. clustering classic bottom-up approach data points gradually agglomerated together form clusters. step pair-wise distances computed identify minimum. parties involved minimal pair-wise distance linked together. step repeated data points linked together. hierarchical tree end. tree constructed connect data points depth level chosen tree forming clusters. model data dynamically special hierarchical clustering method called chameleon proposed makes inter-connectivity closeness concept merge divide clusters. inter-connectivity closeness clusters higher within clusters clusters merged. apart well-known clustering methods different clustering paradigms. density-based clustering data clustered based connectivity density functions. example dbscan uses density-based notions deﬁne clusters. connectivity functions density-reachable density-connected proposed deﬁne data point either core point border point. dbscan visits points arbitrarily points visited. point core point tries expand form cluster around itself. based experimental results authors demonstrated robustness toward discovering arbitrarily shaped clusters. grid-based clustering data space divided multiple portions different granularity levels clustered individually. example clique automatically subspaces high density clusters. data distribution assumption made. empirical results demonstrated could scale well number dimensions. thus especially efﬁcient clustering highdimensional data. correlation clustering motivated document clustering problem pair-wise similarity function learned past data. goal partition current documents correlates much possible. words complete graph vertices edge labeled either goal produce partition vertices agrees edge labels. authors proved problem np-complete problem. hence proposed approximation algorithms achieve partitioning. ﬁrst method called cautious minimize disagreements whereas second method called ptas maximize agreements basically ideas methods ﬁrst method discussed detail work. first arbitrarily choose vertex pick positive neighbors vertex picked positive neighbors vertex perform pruning. ’vertex removal step’. step move check δ-bad positive neighbors vertex remove removal step next step ’vertex addition step’ back vertices δ-good chosen vertex vertices chosen cluster. steps repeated vertices left becomes empty. existing clustering approaches local minima require iterative algorithm good clusters using different initial cluster starting points. contrast spectral clustering relatively promising approach clustering based leading eigenvectors matrix derived distance matrix. main idea make spectrum similarity matrix data perform dimensionality reduction k-means clustering fewer dimensions. seminal work discussed work. beginning form afﬁnity matrix matrix total number data points. entry corresponds similarity measure data points scaling parameter controls rapidly falls distance formed afﬁnity matrix construct laplacian matrix normalized afﬁnity matrix leading eigenvectors form matrix stacking eigenvectors column. stacked eigenvectors form matrix normalize row. treat data vector k-means clustering algorithm cluster them. clustering results projected back onto original data distinct works mentioned gravitational clustering considered rather unique method. ﬁrst proposed wright method data instance considered particle within feature space. physical model applied simulate movements particles. described jonatan proposed gravitational clustering method using newton laws motion. simpliﬁed version gravitational clustering proposed long wang proposed local shrinking method move data toward medians nearest neighbors blekas lagaris proposed similar method called newtonian clustering newton’s equations motion applied shrink separate data followed gaussian mixture model building. molecular dynamics-like mechanism also applied clustering junlin tackle clustering problem novel clustering method herd clustering proposed wong novelties aspects inspired nature herd behavior commonly seen phenomenon real world including human mobility patterns thus intuitive easy understood good performance. also demonstrates cluster analysis done non-traditional making data alive. inspired herd behavior attraction model used guide data movements ﬁrst stage. data instance represented particle. coordinate position particle given values corresponding data instance represents. particles attract distances smaller threshold. particle velocity iteration velocity particle affected neighborhood particles. particles found particular direction velocity particle accelerated toward direction. iterations ﬁrst stage data instances well separated merged together. much easier clustered before. thus intuitive approach proposed cluster data second stage. list cluster centroids maintained. beginning centroid list empty. point check whether distance centroid smaller threshold. centroid detected point assigned cluster centroid. distances centroids higher equal threshold point added list start cluster around data instances scanned clustering result obtained. ﬁrst glance similar gravitation clustering data instances moved according model. nonetheless details totally different. instance model physical model following newton laws motion artiﬁcial model designed computational efﬁciency. particle acceleration decreases inter-particle distance increases independent calculus involved whereas computationally efﬁcient operations allowed lots clustering methods proposed past. instance maulik applied genetic algorithm search cluster centers globally incremental approach k-means reported celeux proposed novel method called gaussian parsimonious clustering models different distance measures incorporated objective function cluster arbitrary number clusters hierarchical agglomerative clustering methodology using symbolic objects described tsao used fuzzy kohonen network clustering fuzzy c-means algorithm developed described alternative pruning approach reduce noise effect also proposed fuzzy cmeans algorithm recent years several kernel methods developed clustering fuzzy-rough application microarray data also reported applied hierarchical clustering method active learning interestingly corsini trained neural network deﬁne dissimilarity measures subsequently used relational clustering gullo also proposed clustering methods uncertain data many works; details found previous clustering methods assume data static clustering. nonetheless modern data static necessarily. fact data transmitted streaming form; instance real-time ﬁnancial stock market data video surveillance data social media data. modern data keeps changing evolving course clustering. analysis data ability process data timely manner little memory crucial. light that different data stream clustering methods proposed. instance guha proposed ﬁrstknown method stream solve k-median problem streaming data constant-factor approximation incremental clustering method also proposed maintain hierarchical clustering tree streaming data fisher zhang proposed efﬁcient data clustering method large datasets thanks linear complexity single-pass nature also applied cluster data streams tree data structure tree hand incremental clustering method proposed data stream clustering problems. particular lower bound clustering performance also provided past years probabilistic graphical models successfully applied different problems gene clustering particular hidden markov model demonstrated successful clustering sequence data wide range domains description hidden markov model probabilistic graphical model assumes sequence symbols controlled generated corresponding sequence hidden states sequence length. particular markov property assumed sequence hidden states; words hidden state solely depends previous hidden state sequence. although markov assumption over-simpliﬁes independence different states work fairly well practice. moreover greatly reduces computational complexity learning expected probability state p-th position m-th sequence forward backward probability m-th sequence state p-th position calculated dynamic programming approach probability observing given existing model parameter calculated addition also calculate expected model parameters used next iteration. repeat e-step m-step alternatively model parameters changed anymore. words difference converges numerically negligible value local optimum found. clustering rand index purity f-measure normalized mutual information usually adopted performance benchmarking. rand index based intra-cluster similarity inter-cluster dissimilarity. intra-cluster similarity pair data vectors assigned cluster target result clustering result score increased one. illustrative purposes example hidden states depicted figure example hidden states. beginning sequence initialization probabilities hidden state representing chances ﬁrst hidden state. transition probabilities {aij} determine that next hidden state recursively. hidden state traversal depending current state symbol emitted appended form output sequence based emission probabilities {bi}. model learning learn sequences baum-welch algorithm usually applied learn unknown parameters mathematically baum-welch algorithm expectation maximization algorithm maximal likelihood estimates parameters. thus would like note baum-welch algorithm highly depends ﬁrst random initialization iteration trapped local optima. multiple runs usually adopted circumvent issues. mathematically baum-welch training algorithm described herein input sequences length sequence represented smsm...sml m}∀p method beginning baum-welch algorithm randomly initialize model parameters iteratively reﬁne iteration. expectation step l-th iteration calculate expected values state based current parameter nonetheless accuracy non-informative dataset imbalanced mis-clustering cost high. instance performance method positive class prediction practically interesting adopt precision sensitivity deﬁned follows alternatively f-measure applied combine precision sensitivity single performance metric. deﬁned harmonic mean precision sensitivity. duals precision sensitivity negative class clustering negative predictive value speciﬁcity respectively. although performance metrics described suitable evaluating discrete clustering predictions. nonetheless modern clustering methods usually assign conﬁdence value prediction. examine modern methods full spectrum receiver operating characteristics curves precision-recall curves proposed. different thresholds conﬁdence values observe performance trade-off method. instance tradesensitivity false positive rates observed curves whereas precision recall observed curves. area curves usually adopted benchmarking metric. typical evaluation procedure divide dataset sets training dataset testing dataset. training dataset used training clustering model testing dataset isolated reserved testing trained model. particular common procedure n-fold crossvalidation iterations. dataset randomly divided non-overlapping subsets. iteration subset rotated testing dataset others assigned corresponding training dataset. input data scarce costly leave-one-out cross-validation also applied. case data sample left testing others allocated training dataset iteration. inter-cluster dissimilarity pair vectors assigned different clusters target result clustering result score also increased one. contrary pair data vectors cluster target result clustering result score increased. checked possible pairs score normalized total number possible pairs. mathematically formula derived follows number data vectors data vector data vector cluster group data vector target result cluster group data vector clustering result. hand f-measure similar rand index exception true negatives taken account. mathematically formula derived follows measure iverson bracket. contrast purity solely measures intra-cluster similarity. nevertheless useful sense care quality individual clusters. mathematically purity cluster size deﬁned below. data instances cluster groups overall purity clustering result deﬁned number data instances class assigned cluster. account performance results normalized mutual information also used non-deterministic methods performance metrics taken averaging multiple runs. deterministic methods performance metrics taken running only. perspective predictive tasks clustering outcome categorized types. clustering outcome consistent truth called either true positive true negative depending actual value. otherwise called false positive false negative respectively. different problem domains depreciated weighted differently. instance tolerated human disease diagnosis. since existing clustering methods stochastic multiple replicate runs need executed comprehensive benchmarking means standard deviations performance metrics usually reported fair comparison. justify results statistical tests adopted assess statistical signiﬁcances; instance t-tests mann-whitney u-tests kolmogorov-smirnov test investigate performance difference methods four representative methods selected different datasets. k-means++ chosen simplicty superior performance traditional k-means method; correlation clustering selected represent algorithms solid theoretical support; unsupervised optimal fuzzy clustering chosen represent soft clustering algorithms; spectral clustering selected represent modern clustering algorithms. since methods selected stochastic replicate runs executed compute average performance metrics method dataset. parameters tuned algorithm dataset manually. results depicted fig. clustering methods exhibit different characteristics different datasets. general based performance metric spectral clustering found perform best among selected algorithms whereas performance correlation clustering relatively limited. based time taken kmeans++ fastest whereas correlation clustering slowest datasets. three datasets typical datasets. cluster forms globular shape. hard expect solved clustering algorithms. result turns concede expectation except correlation clustering. middle three datasets difﬁcult datasets. cluster irregular shape. within dataset cluster even guaranteed similar clusters. interestingly nearly perfect result obtained spectral clustering reﬂecting dimensional transformation ability within spectral clustering play role lowering difﬁculties handling irregular data shapes. bottom four datasets wellknown datasets taken machine learning repository. number attributes ranged number class labels ranged number instances ranged experiment algorithm managed perform well particular dataset. conclusive insights drawn result. data dependency clustering algorithms fully reﬂected datasets. growing data cluster algorithms become important tools analyzing data. book chapter reviewed existing clustering algorithms different paradigms partitional clustering hierarchical clustering density-based clustering grid-based clustering correlation clustering spectral clustering gravitational clustering herd clustering others. especially focused methodologies design concepts. advanced clustering methods also reviewed; instance data stream clustering sequence clustering. verify algorithms’ competitiveness different types performance metrics deﬁned reviewed. particular benchmark studies conducted observe empirical performance selected methods kmeans++ correlation clustering fuzzy clustering spectral clustering. numerical results reveal spectral clustering competitive edge methods low-dimensional datasets. high-dimensional datasets cannot observe signiﬁcant performance difference selected methods. computational scalability mentioned beginning book chapter recent advancements science technologies enable massive data generation recent years. existing computational methods scale large amount data. instance high computational complexity spectral clustering method longer practical current datasets. imperative develop scalable methods keep pace data generation speed. advanced learning methods book chapter provided overview clustering. undeniable machine learning methods applied well instance probabilistic graphical models developed applied capture/eliminate uncertainty noises real world data. domain knowledge existing clustering algorithms built general purposes. domain knowledge incorporated clustering algorithm applied speciﬁc task; instance data sparse sparse clustering algorithm applied boost execution speed. k.-c. wong k.-s. leung m.-h. wong effect spatial locality evolutionary algorithm multimodal optimization proceedings international conference applications evolutionary computation volume part ser. evoapplicatons’. berlin heidelberg springer-verlag available http//dx.doi.org/./---- k.-c. wong peng m.-h. wong k.-s. leung generalizing learning protein-dna binding sequence representations evolutionary algorithm soft comput. vol. aug. available http//dx.doi.org/./ s--- maillard monde sans criminalit ﬁnancire images editions stock gonzalez computational complexity clustering related problems system modeling optimization ser. lecture notes control information sciences drenick kozin eds. springer berlin heidelberg vol. ./bfb. available http//dx.doi.org/./ xiong chen k-means clustering versus validation measures data-distribution perspective ieee transactions systems cybernetics part vol. advantages careful seeding proceedings eighteenth annual acm-siam symposium discrete algorithms ser. soda philadelphia society industrial applied mathematics available http//portal.acm.org/citation.cfm?id= wunsch survey clustering algorithms ieee transactions neural networks vol. available http//dx.doi.org/./tnn.. karypis e.-h. kumar chameleon hierarchical clustering using dynamic modeling computer vol. august available http//dx.doi.org/./. ester h.-p. kriegel sander density-based algorithm discovering clusters large spatial databases noise proc. international conference knowledge discovery data mining agrawal gehrke gunopulos raghavan automatic subspace clustering high dimensional data data mining applications sigmod rec. vol. june available http//doi.acm.org/./. jordan weiss spectral clustering analysis algorithm advances neural information processing systems. press malik normalized cuts image segmentation ieee trans. pattern anal. mach. intell. vol. august available http//dx.doi.org/./. long l.-w. simpliﬁed gravitational clustering method multi-prototype learning based minimum classiﬁcation error training advances machine vision image processing pattern analysis ser. lecture notes computer science zheng jiang eds. springer berlin heidelberg vol. wang zamar clues non-parametric clustering method based local shrinking computational statistics data analysis vol. september available http//ideas.repec.org/a/eee/csdana/vyip-.html blekas lagaris newtonian clustering approach based molecular dynamics global optimization pattern recogn. vol. june available http//portal.acm.org/citation.cfm?id=. junlin hongguang molecular dynamics-like data clustering approach pattern recognition vol. available http//www.sciencedirect.com/science/article/ pii/s maulik bandyopadhyay genetic algorithm-based clustering technique pattern recognition vol. available http//www.sciencedirect.com/science/article/ bv-wk-//fbfaeefba likas vlassis verbeek global k-means clustering algorithm pattern recognition vol. available http//www.sciencedirect.com/science/ article/bv-ttjy-//bfdfdefcfbbafddfdd celeux govaert gaussian parsimonious clustering models pattern recognition vol. available http//www.sciencedirect.com/science/article/ bv-ygvn-//faeebe clustering competitive agglomeration pattern recognition vol. available http//www.sciencedirect.com/science/ article/bv-snvhwm-m//cecfadabaabff gowda diday symbolic clustering using dissimilarity measure pattern recognition vol. available http//www.sciencedirect.com/science/ article/bv-mpntw-js//aefccacfffcd c.-k. tsao bezdek fuzzy kohonen clustering networks pattern recognition vol. available http//www.sciencedirect.com/science/article/ bv-mppxd-yx//cfebbafbff generalized fuzzy cmeans clustering algorithm improved fuzzy partitions ieee transactions systems cybernetics part vol. june available http//dl.acm.org/citation. cfm?id=. j.-s. zhang y.-w. leung robust clustering pruning outliers. ieee transactions systems cybernetics part vol. available http//dblp.uni-trier.de/db/journals/tsmc/tsmcb.htmlzhangl filippone survey kernel spectral methods clustering pattern recognition vol. available http//www.scopus.com/ inward/record.url?eid=-s.-&partnerid=&md= dafcbdcebcab maybank unsupervised active learning based hierarchical graph-theoretic clustering ieee transactions systems cybernetics part vol. october available http//dl.acm.org/citation.cfm?id= corsini lazzerini marcelloni fuzzy relational clustering algorithm based dissimilarity measure extracted data ieee transactions systems cybernetics part vol. feb. gullo ponti tagarelli clustering uncertain data k-medoids proceedings international conference scalable uncertainty management berlin heidelberg springer-verlag available http//dx.doi.org/./---- k.-c. wong c.-h. peng zhang evolutionary multimodal optimization using principle locality inf. sci. vol. jul. available http//dx.doi.org/./j.ins... guha meyerson mishra motwani o’callaghan clustering data streams theory practice ieee trans. knowl. data eng. vol. mar. available http//dx.doi.org/./tkde.. zhang ramakrishnan livny birch efﬁcient data clustering method large databases proceedings sigmod international conference management data ser. sigmod york available http//doi.acm.org/./. charikar chekuri feder motwani incremental clustering dynamic information retrieval proceedings twenty-ninth annual symposium theory computing. frey mohammad morris zhang robinson mnaimneh chang rossant bruneau aubin blencowe hughes genome-wide analysis mouse transcripts using exon microarrays factor graphs nat. genet. vol. rabiner readings speech recognition waibel k.-f. eds. francisco morgan kaufmann publishers inc. tutorial hidden markov models selected applications speech recognition available http//dl.acm.org/citation.cfm?id=. halkidi batistakis vazirgiannis clustering validation techniques intell. inf. syst. vol. december available http//portal.acm.org/citation.cfm? id=. vinh epps bailey information theoretic measures clusterings comparison variants properties normalization correction chance mach. learn. res. vol. dec. available http//dl.acm.org/citation.cfm? id=.", "year": 2015}