{"title": "Random design analysis of ridge regression", "tag": ["math.ST", "cs.AI", "cs.LG", "stat.ML", "stat.TH"], "abstract": "This work gives a simultaneous analysis of both the ordinary least squares estimator and the ridge regression estimator in the random design setting under mild assumptions on the covariate/response distributions. In particular, the analysis provides sharp results on the ``out-of-sample'' prediction error, as opposed to the ``in-sample'' (fixed design) error. The analysis also reveals the effect of errors in the estimated covariance structure, as well as the effect of modeling errors, neither of which effects are present in the fixed design setting. The proofs of the main results are based on a simple decomposition lemma combined with concentration inequalities for random vectors and matrices.", "text": "abstract. work gives simultaneous analysis ordinary least squares estimator ridge regression estimator random design setting mild assumptions covariate/response distributions. particular analysis provides sharp results out-of-sample prediction error opposed in-sample error. analysis also reveals eﬀect errors estimated covariance structure well eﬀect modeling errors neither eﬀects present ﬁxed design setting. proofs main results based simple decomposition lemma combined concentration inequalities random vectors matrices. random design setting linear regression provided samples covariates responses sampled independently population random vectors random variables. typically pairs hypothesized linear relationship random design setting stands contrast ﬁxed design setting covariates ﬁxed responses treated random. thus covariance structure design points completely known need estimated simpliﬁes analysis standard estimators. however ﬁxed design setting directly address out-of-sample prediction primary concern many applications; instance prediction problems estimator computed initial sample population end-goal predictor given draw population. ﬁxed design analysis assesses accuracy data already seen random design analysis concerned predictive performance unseen data. work gives detailed analysis ordinary least squares ridge estimators random design setting quantiﬁes essential diﬀerences random ﬁxed design. particular analysis reveals simple decomposition eﬀect errors estimated covariance structure; eﬀect errors estimated covariance structure well eﬀect approximating eﬀect errors noise response. neither ﬁrst eﬀects present ﬁxed design analysis ridge regression random design analysis shows eﬀect errors estimated covariance structure minimal—essentially second-order eﬀect soon sample size large enough. analysis also isolates eﬀect approximation error main terms estimation error bound bound reduces scales noise variance approximation error vanishes. mathematics subject classiﬁcation. primary secondary words phrases. linear regression ordinary least squares ridge regression randomized approximation. spectrum second moment particular choice λ—the dimension covariate space enter explicitly except immediately obtain analysis ordinary least squares; aware random design analysis ridge estimator characteristic. generally convergence rate optimized appropriately setting based assumptions spectrum. finally analysis based operator-theoretical approach similar relies probabilistic tail inequalities modular gives explicit dependencies without additional boundedness assumptions assumed probabilistic bounds. outline. section discusses model preliminaries related work. section presents main results excess mean squared error ordinary least squares ridge estimators random design discusses relationship standard ﬁxed design analysis. section discusses application accelerating least squares computations large data sets. proofs main results given section bounds used proofs; main results work extended separable hilbert spaces mild assumptions using suitable inﬁnite-dimensional generalizations probabilistic bounds. denote dimensionality space stress results positive deﬁnite linear operator denote vector norm given kvkm =phv omitted assumed identity =phv denote outer product vector i.e. kmkf =ptr. self-adjoint kmkf =ptr. λmax λmin respectively without loss generality assume eigenvalues strictly positive since otherwise restrict attention vectors subspace assumption holds. achieve minimum mean squared error linear functions i.e. merely simplify certain probability tail inequalities main result peculiar case remark appears naturally arises standard ﬁxed design analysis ridge regression also used random design analyses ridge regression. easy dimension main condition requires squared length never constant factor greater expectation linear mapping sometimes called whitening reason considering case call mapping λ-whitening expectation e−/xk] small suﬃciently hard almost sure bound condition relaxed moment conditions simply using diﬀerent probability tail inequalities analysis. consider relaxation sake simplicity. also remark possible replace condition subgaussian condition subgaussian) lead sharper deviation bound certain cases. response model. response model considered work relaxation typical gaussian model; model speciﬁcally allows approximation error general subgaussian noise. deﬁne random variables hard almost sure bound condition easily relaxed moment conditions consider sake simplicity. also remark appears lower-order terms main bounds. ﬁrst last inequalities condition second inequality uses deﬁnition approxλ triangle inequality third inequality follows cauchy-schwarz. quantity related work. ridge ordinary least squares estimators classically studied ﬁxed design setting covariates ﬁxed vectors responses independent random variables mean variance analysis reviewed section reveals expected prediction error controlled bias term zero variance term bounded σdλ/n. discussed introduction random design analysis ridge estimator reveals essential diﬀerences ﬁxed random design comparing classical analysis. many classical analyses ridge ordinary least squares estimators random design setting actually show nonasymptotic convergence mean squared error best linear predictor dimension covariate space. rather error relative bayes error bounded multiple error optimal linear predictor relative bayes error plus term bounds appropriate non-parametric settings error optimal linear predictor also approaches bayes error rate. beyond classical results analyses ordinary least squares often come nonstandard restrictions applicability additional dependencies spectrum second moment operator comprehensive survey results); instance result gives bound excess mean squared error form work provides ridge regression bounds explicitly terms vector terms eigenspectrum second moment operator essential setting study previous analyses make unnecessarily strong boundedness assumptions fail give bound case eﬀective dimension scale deﬁned quantity bounded assuming thus dominant terms ﬁnal bound explicit dependences bapprox assume almost surely prove bound depends dominant term. moreover dominant term c′′′ given explicit decay conditions eigenspectrum bound also trivial quantity vanishes precisely bias term classical recently derived sharp risk bounds ordinary least squares ridge estimators random design setting mild moment assumptions using pac-bayesian techniques. nonasymptotic bound ordinary least squares holds relying stronger moment assumptions allow probability tail parameter large analysis also arguably transparent yields reasonable quantitative bounds. analysis ridge estimator established asymptotic sense therefore directly comparable provided here. finally although focus present work understanding ordinary least squares ridge estimators also mentioned number estimators considered literature nonasymptotic prediction error bounds indeed works propose estimators require considerably weaker moment conditions obtain optimal rates. review ﬁxed design analysis. informative ﬁrst review ﬁxed design analysis ridge estimator. recall that setting design points ﬁxed vectors assume eigenvectors {vj} corresponding eigenvalues σvji. i=hvj xiie ordinary least squares. analysis ordinary least squares estimator based simple decomposition excess mean squared error similar ﬁxed design analysis. state decomposition ﬁrst denote conditional expectation least squares estimator conditioned i.e. random design ridge regression. analysis ridge estimator random design based simple decomposition excess mean squared error. here denote conditional expectation given i.e. remark eﬀect approximation error isolated term bound scales fourth-moment quantity λβλk]; using looser bound overall simpliﬁed bound remark viewed remark applications ridge regression considered also analyzed using theorem speciﬁcally consider problem approximating periodic function smoothing splines results ordinary least squares estimator used analyze randomized approximation scheme overcomplete least squares problems goal randomized methods approximately solve least squares problem large full-rank design matrix rm×d vector note using standard method exactly solve least squares problem requires operations prohibitive large-scale problems. however approximate solution satisfactory signiﬁcant computational savings achieved randomization. motivation random rotation captured lemma shows that chosen randomly certain distributions rotation matrices applying creates equivalent least squares problem statistical leverage parameter small. consequently least squares problem approximately solved small random sample theorems without random rotation statistical leverage parameter could large small random sample rows likely miss crucial obtaining accurate approximation. role statistical leverage setting also pointed although lemma makes connection direct. note lemma analysis generalized case approximately orthogonal; standard distributions rotation matrices additional error terms arise aﬀect overall analysis. running time approximation scheme given time required apply random rotation operator original matrix randomly sample rows plus time solve least squares problem smaller design matrix size na¨ıvely applying arbitrary rotation matrix requires operations; however rotation matrices running time reduced considerable speed-up large. fact rows retained anyway computation reduced standard methods produce ordinary least squares estimator ridge regression estimator operations. therefore interested sample size suﬃces yield accurate approximation. analysis approximation scheme. approach analyzing approximation scheme treat random design regression problem. apply theorem setting give error bounds solution produced approximation scheme. probability least choice random rotation matrix henceforth condition solution original least squares problem ordinary least squares estimator computed random sample rows note that example distributed uniformly orthogonal matrices. random vector θ⊤ei distributed uniformly unit sphere §m−. random variable inequality fact vector matrix. random rotation component fast johnson-lindenstrauss transform also used especially important present application applied vectors operations signiﬁcantly faster running time na¨ıve matrix-vector multiplication. proof theorem omits term decomposition proposition fact uses slightly simpler argument handle eﬀect noise reduces number lower-order terms. diﬀerences proof theorem special case part exactly standard ﬁxed design analysis ridge regression. section employs probability tail inequalities spectral frobenius norms random matrices bound matrix errors proof. claim consequence tail inequality lemma proof lemma deﬁne self-adjoint linear operators inner product given note inner product induces frobenius norm kmkf mif. observe", "year": 2011}