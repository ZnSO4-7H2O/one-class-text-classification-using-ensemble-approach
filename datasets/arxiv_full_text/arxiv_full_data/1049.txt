{"title": "Using Fast Weights to Attend to the Recent Past", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These \"fast weights\" can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns.", "text": "recently research artiﬁcial neural networks largely restricted systems types variable neural activities represent current recent input weights learn capture regularities among inputs outputs payoffs. good reason restriction. synapses dynamics many different time-scales suggests artiﬁcial neural networks might beneﬁt variables change slower activities much faster standard weights. fast weights used store temporary memories recent past provide neurally plausible implementing type attention past recently proved helpful sequence-to-sequence models. using fast weights avoid need store copies neural activity patterns. ordinary recurrent neural networks typically types memory different time scales different capacities different computational roles. history sequence currently processed stored hidden activity vector acts short-term memory updated every time step. capacity memory number hidden units. long-term memory convert current input hidden vectors next hidden vector predicted output vector stored weight matrices connecting hidden units inputs outputs. matrices typically updated sequence capacity numbers input output units. long short-term memory networks complicated type work better discovering long-range structure sequences main reasons first compute increments hidden activity vector time step rather recomputing full vector. encourages information hidden states persist much longer. second allow hidden activities determine states gates scale effects weights. multiplicative interactions allow effective weights dynamically adjusted input hidden activities gates. however lstms still limited short-term memory capacity history current sequence. recently surprisingly little practical investigation forms memory recurrent nets despite strong psychological evidence exists obvious computational reasons needed. occasional suggestions neural networks could beneﬁt third form memory much higher storage capacity neural activities much faster dynamics standard slow weights. memory could store information speciﬁc history current sequence information available inﬂuence ongoing processing without using memory capacity hidden activities. hinton plaut suggested fast weights could used allow true recursion neural network schmidhuber pointed system kind could trained end-to-end using backpropagation neither papers actually implemented method achieving recursion. processes like working memory attention priming operate timescale minutes. simultaneously slow mediated neural activations without dynamical attractor states fast long-term synaptic plasticity mechanisms kick artiﬁcial neural network research typically focused methods maintain temporary state activation dynamics focus inconsistent evidence brain also—or perhaps primarily—maintains temporary state information short-term synaptic plasticity mechanisms brain implements variety short-term plasticity mechanisms operate intermediate timescale. example short term facilitation implemented leftover axon terminal depolarization short term depression implemented presynaptic neurotransmitter depletion zucker regehr spike-time dependent plasticity also invoked timescale plasticity mechanisms synapsespeciﬁc. thus accurately modeled memory capacity standard recurrent artiﬁcial recurrent neural nets lstms. main preoccupations neural network research early idea memories stored somehow keeping copies patterns neural activity. instead patterns reconstructed needed information stored weights associative network weights could store many different memories auto-associative memory weights cannot expected store real-valued vectors components each. close come upper bound depends storage rule use. hopﬁeld nets simple one-shot outer-product storage rule achieve capacity approximately binary vectors using weights require bits each. much efﬁcient made weights using iterative error correction storage rule learn weights retrieve pattern bits purposes maximizing capacity less important simple non-iterative storage rule outer product rule store hidden activity vectors fast weights decay rapidly. usual weights called slow weights learn stochastic gradient descent objective function taking account fact changes slow weights lead changes gets stored automatically fast associative memory. fast associative memory several advantages compared type memory assumed neural turing machine neural stack memory network first clear real brain would implement exotic structures models e.g. tape whereas clear brain could implement fast associative memory synapses appropriate dynamics. second fast associative memory need decide write memory read memory. fast memory updated time writes superimposed fast changing component strength synapse. every time input changes transition hidden state determined combination three sources information input slow input-to-hidden weights previous hidden state slow transition weights recent history hidden state vectors fast weights effect ﬁrst sources information hidden state computed maintained sustained boundary condition brief iterative settling process allows fast weights inﬂuence hidden state. assuming fast weights decay exponentially show effect fast weights hidden vector iterative settling phase provide additional input proportional recent hidden activity vectors scalar product recent hidden vector current hidden activity vector term weighted decay rate raised power long hidden vector occurred. fast weights like kind attention recent past strength attention determined scalar product current hidden vector earlier hidden vector rather determined separate parameterized computation type used neural machine translation models update rule fast memory weight matrix simply multiply current fast weights decay rate outer product hidden state vector multiplied learning rate next vector hidden activities computed steps. preliminary vector determined combined effects input vector previous hidden vector slow weight matrices nonlinearity used hidden units. preliminary vector used initiate inner loop iterative process runs steps progressively changes hidden state terms square brackets sustained boundary conditions. real neural could implemented rapidly changing synapses computer simulation uses sequences fewer time steps dimensionality less full rank efﬁcient compute term without ever computing full fast weight matrix assuming beginning sequence term square brackets scalar product earlier hidden state vector current hidden state vector iterative inner loop. iteration inner loop fast weight matrix exactly equivalent attending past hidden vectors proportion scalar product current hidden vector weighted decay factor. inner loop iterations attention become focussed past hidden states manage attract current hidden state. equivalence using fast weight matrix comparing stored hidden state vectors helpful computer simulations. allows explore done fast weights without incurring huge penalty abandon mini-batches training. ﬁrst sight mini-batches cannot used fast weight matrix different every sequence comparing stored hidden vectors allow mini-batches. potential problem fast associative memory scalar product hidden vectors could vanish explode depending norm hidden vectors. recently layer normalization shown effective stablizing hidden state dynamics rnns reducing training time. layer normalization applied vector summed inputs recurrent units particular time step. uses mean variance components vector re-center re-scale summed inputs. then applying nonlinearity includes learned neuron-speciﬁc bias gain. apply layer normalization fast associative memory follows denotes layer normalization. found applying layer normalization iteration inner loop makes fast associative memory robust choice learning rate decay hyper-parameters. rest paper fast weight models trained using layer normalization outer product learning rule fast learning rate decay rate unless otherwise noted. demonstrate effectiveness fast associative memory ﬁrst investigated problems associative retrieval mnist classiﬁcation compared fast weight models regular rnns lstm variants. applied proposed fast weights facial expression recognition task using fast associative memory model store results processing level examining sequence details ﬁner level hyper-parameters experiments selected grid search validation set. models trained using mini-batches size adam optimizer description training protocols hyper-parameter settings used found appendix. lastly show fast weights also used effectively implement reinforcement learning agents memory start demonstrating method propose storing retrieving temporary memories works effectively task well suited. consider task multiple key-value pairs presented sequence. sequence keys presented model must predict value temporarily associated key. used strings contained characters english alphabet together digits construct training sequence ﬁrst randomly sample character alphabet without replacement. ﬁrst key. single digit sampled associated value key. generating sequence character-digit pairs different characters selected random query network must predict associated digit. examples string sequences targets shown below token separate query key-value pairs. generated training examples validation examples test examples. solve task standard hidden activities somehow store key-value pairs keys values presented sequentially. makes signiﬁcant challenge models using slow weights. used neural network single recurrent layer experiment. recurrent network processes input sequence character time. input character ﬁrst converted learned -dimensional embedding vector provides input recurrent layer. output recurrent layer sequence processed another hidden layer relus ﬁnal softmax layer. augment relu fast associative memory compare lstm model architecture. although original lstms explicit long-term storage capacity recent work danihelka extended lstms adding complex associative memory. experiments compared fast associative memory lstm variants. figure table show number recurrent units small fast associative memory signiﬁcantly outperforms lstms number recurrent units. result hypothesis fast associative memory allows recurrent units effectively. addition higher retrieval accuracy model fast weights also converges faster lstm models. despite many successes convolutional neural networks computationally expensive representations learn hard interpret. recently visual attention models shown overcome limitations convnets. understand signals algorithm using seeing model looking. also visual attention model able selectively focus important parts visual space thus avoid detailed processing much background clutter. section show visual attention models fast weights store information object parts though restricted glimpses correspond natural parts objects. given input image visual attention model computes sequence glimpses regions image. model determine look next also remember seen working memory make correct classiﬁcation later. visual attention models learn multiple objects large static input image classify correctly learnt glimpse policies typically over-simplistic single scale glimpses tend scan image rigid way. human movements ﬁxations complex. ability focus different parts whole object different scales allows humans apply knowledge weights network many different scales requires form temporary memory allow network integrate discovered glimpses. improving model’s ability remember recent glimpses help visual attention model discover non-trivial glimpse policies. fast weights store glimpse information sequence hidden activity vector freed learn intelligently integrate visual information retrieve appropriate memory content ﬁnal classiﬁer. explicitly verify larger memory capacity beneﬁcial visual attention-based models simplify learning process following first provide pre-deﬁned glimpse control signal model knows attend rather learn control policy reinforcement learning. second introduce additional control signal memory cells attention model knows store glimpse information. typical visual attention model complex high variance performance need learn policy network classiﬁer time. simpliﬁed learning procedure enables discern performance improvement contributed using fast weights remember recent past. consider simple recurrent visual attention model similar architecture previous experiment. predict attend rather given ﬁxed sequence locations static input image broken four non-overlapping quadrants recursively scale levels. four coarse regions down-sampled along four quadrants presented single sequence shown figure notice glimpse scales form two-level hierarchy visual space. order solve task successfully attention model needs integrate glimpse information different levels hierarchy. solution model’s hidden states store integrate glimpses different scales. much efﬁcient solution temporary cache store unﬁnished glimpse computation processing glimpses ﬁner scale hierarchy. computation ﬁnished scale results integrated partial results higher level popping previous result cache. fast weights therefore neurally plausible cache storing partial results. slow weights model specialize integrating glimpses scale. slow weights shared glimpse scales model able store partial results several levels fast weights though demonstrated fast weights storage single level. evaluated multi-level visual attention model mnist handwritten digit dataset. mnist well-studied problem many techniques benchmarked. contains classes handwritten digits ranging task predict class label isolated roughly normalized image digit. glimpse sequence case consists patches pixels. table compares classiﬁcation results relu multi-level fast associative memory lstm gets sequence glimpses. result shows number hidden units limited fast weights give signiﬁcant improvement models. increase memory capacities multi-level fast associative memory consistently outperforms lstm classiﬁcation accuracy. unlike models must integrate sequence glimpses convolutional neural networks process glimpses parallel layers hidden units hold intermediate computational results. demonstrate effectiveness fast weights comparing three-layer convolutional neural network uses patches glimpses presented visual attention model. table multi-level model fast weights reaches similar performance convnet model without requiring biologically implausible weight sharing. investigate beneﬁts using fast weights multi-level visual attention model performed facial expression recognition tasks multi-pie face database dataset preprocessed align face eyes nose ﬁducial points. downsampled greyscale. full dataset contains photos taken cameras different viewpoints illumination expression identity session condition. used images taken three central cameras corresponding views since facial expressions discernible extreme viewpoints. resulting dataset contained images. identities appeared training remaining identities test set. given input face image goal classify subject’s facial expression different categories neutral smile surprise squint disgust scream. task realistic challenging previous mnist experiments. dataset unbalanced numbers labels expressions example squint disgust hard distinguish. order perform well task models need generalize different lighting conditions viewpoints. used multi-level attention model mnist experiments recurrent hidden units. model sequentially attends non-overlapping pixel patches different scales total glimpses. similarly designed layer convnet receptive ﬁelds. table multi-level fast weights model knows store information outperforms lstm irnn. results consistent previous mnist experiments. however convnet able perform better multi-level attention model near frontal face dataset. think efﬁcient weight-sharing architectural engineering convnet combined simultaneous availability information level processing allows convnet generalize better task. rigid predetermined policy glimpse eliminates main potential advantages multi-level attention model process informative details high resolution whilst ignoring irrelevant details. realize advantage need combine fast weights learning complicated policies. different kinds memory attention studied extensively supervised learning setting models learning long range dependencies reinforcement learning received less attention. compare different memory architectures partially observable variant game catch described game played screen binary pixels episode consists frames. trial begins single pixel representing ball appearing somewhere ﬁrst column pixel paddle controlled agent bottom row. observing frame agent gets either keep paddle stationary move right left pixel. ball descends single pixel frame. episode ends ball pixel reaches bottom agent receives reward paddle touches ball reward doesn’t. solving fully observable task straightforward requires agent move paddle column ball. make task partiallyobservable providing agent blank observations frame. solving partiallyobservable version game requires remembering position paddle ball frames moving paddle correct position using stored information. used recently proposed asynchronous advantage actor-critic method train agents three types memory different sizes partially observable catch task. three agents included relu lstm fast weights rnn. figure shows learning progress different agents variants game agent using fast weights architecture policy representation able learn faster agents using relu lstm represent policy. improvement obtained fast weights also signiﬁcant larger version game requires memory. paper contributes machine learning showing performance rnns variety different tasks improved introducing mechanism allows state hidden units attracted towards recent hidden states proportion scalar products current state. layer normalization makes kind attention work much better. form attention recent past somewhat similar attention mechanism recently used dramatically improve sequence-to-sequence rnns used machine translation. paper interesting implications computational neuroscience cognitive science. ability people recursively apply knowledge processing apparatus whole sentence embedded clause within sentence complex object major part object long used argue neural networks good model higher-level cognitive abilities. using fast weights implement associative memory recent past shown states neurons could freed knowledge connections neural network applied recursively. overcomes objection models recursion storing copies neural activity vectors biologically implausible. guo-qiang mu-ming poo. synaptic modiﬁcations cultured hippocampal neurons dependence spike timing synaptic strength postsynaptic cell type. journal neuroscience teuvo kohonen. correlation matrix memories. computers ieee transactions james anderson geoffrey hinton. models information processing brain. parallel models alex graves greg wayne danihelka. neural turing machines. arxiv preprint arxiv. edward grefenstette karl moritz hermann mustafa suleyman phil blunsom. learning transduce jason weston sumit chopra antoine bordes. memory networks. arxiv preprint arxiv. bahdanau bengio. neural machine translation jointly learning align translate. kiros hinton. layer normalization. arxiv. kingma adam method stochastic optimization. arxiv. danihelka greg wayne benigno uria kalchbrenner alex graves. associative long short-term kelvin jimmy ryan kiros aaron courville ruslan salakhutdinov richard zemel yoshua bengio. show attend tell neural image caption generation visual attention. international conference machine learning graves. generating sequences recurrent neural networks. arxiv. volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. international conference machine learning used single hidden layer recurrent neural network takes dimensional embedding vector input. compared fast weights memory three different architecture irnn standard lstm associative lstm. non-recurrent slow recurrent weights initialized uniform distribution number outgoing weights. slow weights learning rate tuned using validation examples. below provide speciﬁc hyper-parameter settings models used experiments fast weights fast weights learning rate fast weights decay rate fast weights updated every time step. experimented iterations inner loop performance similar. recurrent slow weights initialized identity matrix scaled relu activation recurrent layer. agents used recurrent networks represent policy. time step input passed hidden layer relu units passed recurrent core. agents used recurrent cells. output every step softmax valid actions single linear output estimate value function. used random search hyperparameters values learning rate number hebbian steps fast weight learning rate decay applicable. averaged results models. considered different ways performing inner loop settling. method inputs hidden units outer loop transition using stored provide sustained boundary conditions inner loop settling. method simply identity matrix fast weight matrix inner loop settling tends sustain hidden activity vector. relus methods equivalent fast weight matrix zero similar exactly equivalent fast weights non-zero. using layer normalization found method worked slightly better method method would much easier implement biological network.", "year": 2016}