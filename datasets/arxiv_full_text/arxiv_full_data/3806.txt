{"title": "Generating Chinese Classical Poems with RNN Encoder-Decoder", "tag": ["cs.CL", "cs.NE"], "abstract": "We take the generation of Chinese classical poem lines as a sequence-to-sequence learning problem, and build a novel system based on the RNN Encoder-Decoder structure to generate quatrains (Jueju in Chinese), with a topic word as input. Our system can jointly learn semantic meaning within a single line, semantic relevance among lines in a poem, and the use of structural, rhythmical and tonal patterns, without utilizing any constraint templates. Experimental results show that our system outperforms other competitive systems. We also find that the attention mechanism can capture the word associations in Chinese classical poetry and inverting target lines in training can improve performance.", "text": "figure -char quatrain generated system keyword input. tone character shown parentheses. represent ping tones respectively. means tone either. rhyming characters underlined. quatrain kind classical poetry rules forms mean besides necessary requirements grammars semantics general poetry quatrains must obey rules structure tone. figure shows quatrain generated system. quatrain contains four line consists seven five characters. archaic chinese characters divided kinds according tone namely ping. characters particular tone must take generation chinese classical poem lines sequence-to-sequence learning problem build novel system based encoder-decoder structure generate quatrains topic word input. system jointly learn semantic meaning within single line semantic relevance among lines poem structural rhythmical tonal patterns without utilizing constraint templates. experimental results show system outperforms competitive systems. also find attention mechanism capture word associations chinese classical poetry inverting target lines training improve performance. chinese classical poetry undoubtedly largest bright pearl chinese classical literature compared crown. kind literary form starting pre-qin period classical poetry stretches thousand years farreaching influence development chinese history. poets write poems record important events express feelings make comments. different kinds chinese classical poetry quatrain huge quantity high quality must considered quite important one. famous anthology classical particular positions makes poetry cadenced full rhythmic beauty. meanwhile according vowels characters divided different rhyme categories. first second last line quatrain must belong rhyme category enhances coherence poetry. paper mainly focus automatic generation quatrains. actually automatic generation poetry issue long time deep learning opens door makes computer longer rely prepared templates learn composition method automatically large number excellent poems. poetry composition machine beautiful wish. based poem generation system interesting applications developed used education chinese classical poetry literary researches. different semantically similar pairs machine translation tasks pair adjacent sentences quatrain semantically take poem generation sequence-to-sequence learning problem encoderdecoder learn semantic meaning within single line semantic relevance among lines rhythm jointly. furthermore attention mechanism capture character associations improve relevance input lines output lines. consisting three independent line generation blocks system generate quatrain user keyword. system evaluated based task quatrain generation kinds methods automatic manual. experimental results show system outperforms generation systems. rest paper organized follows. section introduce related methods systems. section give details system analyze advantages model. section give evaluation experiments design results. section draw conclusion point future work. research poetry generation started focus recent decades. manurung proposed three criteria automatically generated poetry grammaticality lines express meaningfulness poeticness. early methods based rules templates. example aspera uses changes accent templates fill words. haiku generation system expands haiku sentences according rules extracted corpus. methods mechanical match requirements grammaticality perform poorly meaningfulness poeticness. important approach generate poems evolutionary algorithms. process poetry generation described natural selection. genetic heredity variation good results selected functions however methods depend quality evaluation functions hard designed well. generally sentences perform better meaningfulness encoder-decoder structure suitable sequence-to-sequence learning tasks. machine translation tasks sentence pairs semantically similar model learns corresponding relations. chinese classical quatrains close semantic relevance adjacent lines. poem lines semantically relevant sequence pairs. encoder-decoder learn relevance used generate poem line given previous line. utilizing context information different levels build three poem line generation blocks generate whole quatrain. illustrated figure user inputs keyword topic show main content emotion poem convey. firstly generates line relevant keyword first line. takes first line input generates relevant second line. generates third line first lines input. finally takes second third lines input generates last line. another approach based methods generation kinds texts. generate poems method automatic summarization. first applied task couplets generation treat generation couplets kind machine translation tasks. apply method quatrain generation translating input sentence second sentence second third sentences generated method good relevance cannot obey rules forms. cross field deep learning natural language process becoming focused neural network applied poetry generation. zhang lapata compress previous information vector produce probability distribution next character generated. work differs previous work mainly follows. firstly encoder-decoder basic structure system compared method moreover he's system rhythm controlled externally results perform poorly tonal patterns. system learn things jointly. secondly compared model based bidirectional gated units instead simple rnn. besides zhang compress context information small vector losing much useful information. need translation models another rhythm template control semantic relevance tones. results system obey constraints naturally. finally attention mechanism element-wise multiplication. final hidden state t-th character encoder. reset gate update gate difference context vector hidden state t-th character decoder. computed tanh poems corpus high quality. therefore neural language model system improve meaningfulness make poem idiomatic modern people. combine probability distributions decoder language model furthermore shown figure training invert target lines reasons. first final character second line must level-tone. tail character third line oblique-tone. final character input line level-tone can't determine tone final character hinese. fortunately length chinese classical poem lines fixed five seven characters. words chinese classical poetry consist chinese characters. therefore method feasible. used line-to-line generation. generating second line available context information first line. therefore generate second line taking first line input. shown figure bidirectional attention mechanism proposed build spb. figure embedding learned poem line representations. circle square triangle represent frontier-style boudoir-plaint history-nostalgia poem lines respectively. shortcoming smt-based methods need another model generate first line. example expands user keywords constraint templates language model search line. encoder-decoder words sentences mapped vector space. since system based characters words considered short sequences. ideally generate relevant line taking word input. training pairs long sequences work well input short word. therefore train third encoderdecoder called word poem block based model parameters trained <word line> pairs train improve wpb's ability generating long sequences short sequences. output line pairs <line line> training data. tone controller spb. obviously control harm meaningfulness outputs. therefore invert target sentences training generate tail character first minimize damage meaningfulness possible. furthermore find inverting target lines improve performance lines source mentioned utilize context information build another encoder-decoder called structure similar spb. difference concatenate adjacent lines quatrain long input sequence third line target sequence training. model utilize information last lines generating current line. final characters second fourth line must rhyme final character third line must rhyme. generating fourth line can't determine rhyme. taking second line consideration learn rhyme. thus generate third fourth lines. zhang utilizes context compressing lines -dimensional vector causes severe loss semantic information. whereas method save information. generating current line model learn focus points attention rather context indiscriminately improve semantic relevance inputs outputs. concatenate lines reasons. firstly long sequences result performance. secondly relevance fourth line figure example word boundaries recognition gated units. every adjacent characters first second show reset tendency update tendency. upper plot input line lower output line. value t-th character. along direction hidden states propagation calculated difference reset values latter previous characters reset tendency. similarly update tendency. onducted several qualitative experiments. results show encoder-decoder capture semantic meanings relevance pairs relevance learning tasks well reason model build system. representation poem line. selected three types classical poetry frontier-style poetry boudoir-plaint poetry history-nostalgia poetry type obtained lines used barnes-hut-sne representations two-dimensional space. shown figure lines type gather together. representations capture semantic meanings poem lines well. shown table results simple string matchings results meaning input's. though explicit boundaries input sequences vector representations contain information whole words. example word. results left plot generates character strong dependence character chinese classical poetry symbol homesickness symbol travelers. association characters. also right plot besides first character dependencies second character since input line home-bound ships output line travelers. associations obvious pairs similar syntactic structures. great improvement inverting target lines training. obvious associations bottom plots compared results plots. think improvement inverting related attributive structures archaic chinese. quantitative evaluation results section also show inverting target lines leads higher scores. figure examples attention. pixel shows association characters. horizontal lines inputs vertical lines outputs. outputs plots inverted reasons described section characters tend entirety. whereas tend separated. line 一声秋雁连天远 words. reset tendency update tendency reflect word boundaries roughly. furthermore tendency gated units decoder similar encoder. nature makes vector representations contain information whole words makes output lines keep structures input lines. plots show associations input output lines generated trained inverted target lines. bottom plots results trained normal target lines. left plots outputs syntactically similar inputs. right ones output syntactically different bleu- score evaluate output line given previous line input. since words chinese classical poetry consist characters bleu- effective. hard obtain human-authored references poem lines used method extracted references line quatrain. compared system system human evaluation since poetry kind creative text human evaluation necessary. referring three criteria designed five criteria fluency coherence meaningfulness poeticness entirety. criterion scored compared four comparison systems. system. system. daoxiang poem creator system pioneer chinese classical poetry generation. developed years used hundred million times. human poems famous ancient poets containing given keywords. selected typical keywords generated quatrains keyword using four systems. means obtained quatrain total. invited experts chinese classical poetry evaluate quatrains. expert evaluated quatrains. description beautiful woman attention mechanism focused characters though color word input line attention mechanism chose focus instead generating character since chinese classical poetry often used describe beauty women. compared simple alignments words semantic meanings translation task attention mechanism learn associations helps system focus relevant information instead context results stronger relevance input line output line. corpus contains poems tang dynasty contemporary. earlier poetry regular tonal patterns formed tang dynasty. used poems testing training set. extracted three pairs quatrain. used pairs train pairs train cpb. training selected words words lines word appears finally obtained word-to-line pairs used whole training train neural language model. built system based groundhog. bleu score evaluation referring zhang used https//github.com/lisa-groundhog/groundhog. http//duilian.msra.cn/jueju/. http//www.poeming.com/web/index.htm. keywords limited used spb+attention spb+attention+src invert spb+attention+trg invert table bleu- scores quatrains. simple structure without attention mechanism inverting target lines. -char -char -char -char human table human evaluation results. kappa coefficient groups' scores since generations sequence-to-sequence invited another expert select best line top- list generation line. limited training data used pairs positions train spb. even much higher bleu scores positions moreover outputs generated system observe tonal constraints smt's outputs observe constraints. also simply adding attention mechanism system made performance combination attention great improvement. find reversing source sentences lstm’s performance improve conclude preprocessing introduce short term dependencies source target sentences make optimization problem easier. explanation inverting source sentences inverting target sentences equivalent. shown table inverting table shows bleu- scores. system generates poetry whole compared system single line generation task. given input line output often original adjacent next line ancient poets. fair removed lines ancient poets top-n lists system used best ones. chinese classical poetry relevance lines pair related position. therefore pairs different positions train corresponding position-sensitive models. lower bleu‐ scores system reported qualitative experiments shown feasibility system also conducted quantitative experiment compare cpb. trained another encoder-decoder called concatenates three adjacent lines input takes fourth line target. bleu used evaluate line-to-line generation task focuses semantic relevance lines concentrate relevance generated line context. thus bleu suitable comparing three blocks. another comparison method called generation probability ranking score taking sequence input generation probability specified line decoder. used testing data section inputting testing line block generation probabilities references references descending generation probability. calculated gprs input line number references. higher gprs indicates block greater chance generate original lines means block capture relevance generated lines context better. source sentences made little improvement task. think attributive structure chinese classical poetry. many words structure attributive central word chinese poetry. example character attributive character central word. normal generation order generated earlier many characters central word \"青云\" increases uncertainty generation whereas inverting target sentence reduce uncertainty since attributive often chinese classical poetry. generating t-th character information used determine shown table system higher scores systems expect human. scores char poems lower -char poems human evaluation bleu evaluation) composition -char quatrains difficult. poems generated higher scores -char poems benefiting gated units attention mechanism. scores closed scores human though still gap. also asked experts select best line quatrains evaluated. selected -char lines system poets. selected -char lines system poets. indicates system little difference poets least generating meaningful sentences. dzmitry bahdanau kyunghyun yoshua bengio. neural machine translation jointly learning align translate. proceedings international conference learning representations diego merriënboer gulcehre bahdanau bougares schwenk bengio. learning phrase representations using encoder-decoder statistical machine translation. conference empirical methods natural language processing pages doha qatar. paper take generation lines sequence-to-sequence poem learning problem build novel system generate quatrains based encoder-decoder. compared methods system learn semantic meanings semantic relevance rhythmical tonal patterns without utilizing constraint templates. automatic evaluation human evaluation show system outperforms systems still system ancient poets. show encoder-decoder also suitable learning tasks semantically sequences. attention mechanism captures character associations gated units recognize word roughly. moreover inverting target lines training lead better performance. hitch haiku interactive renku poem composition supporting tool applied sightseeing navigation system. proceedings entertainment computing pages paris france. xueqiang xiaoming poetautomatic chinese poetry composition generative summarization framework constrained optimization. proceedings international joint conference artificial intelligence pages beijing china. jing ming zhou long jiang. generating chinese classical poems statistical machine proceedings aaai conference artificial intelligence pages toronto canada. jiang ming zhou. generating chinese couplets using statistical approach. proceedings international conference computational linguistics pages manchester", "year": 2016}