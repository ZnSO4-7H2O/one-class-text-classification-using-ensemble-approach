{"title": "Inverse Reinforcement Learning via Nonparametric Spatio-Temporal Subgoal  Modeling", "tag": ["cs.LG", "cs.AI", "cs.RO", "cs.SY", "stat.ML"], "abstract": "Recent advances in the field of inverse reinforcement learning (IRL) have yielded sophisticated frameworks which relax the original modeling assumption that the behavior of an observed agent reflects only a single intention. Instead, the demonstration data is typically divided into parts, to account for the fact that different trajectories may correspond to different intentions, e.g., because they were generated by different domain experts. In this work, we go one step further: using the intuitive concept of subgoals, we build upon the premise that even a single trajectory can be explained more efficiently locally within a certain context than globally, enabling a more compact representation of the observed behavior. Based on this assumption, we build an implicit intentional model of the agent's goals to forecast its behavior in unobserved situations. The result is an integrated Bayesian prediction framework which provides smooth policy estimates that are consistent with the expert's plan and significantly outperform existing IRL solutions. Most notably, our framework naturally handles situations where the intentions of the agent change with time and classical IRL algorithms fail. In addition, due to its probabilistic nature, the model can be straightforwardly applied in an active learning setting to guide the demonstration process of the expert.", "text": "recent advances ﬁeld inverse reinforcement learning yielded sophisticated frameworks relax original modeling assumption behavior observed agent reﬂects single intention. instead demonstration data typically divided parts account fact diﬀerent trajectories correspond diﬀerent intentions e.g. generated diﬀerent domain experts. work step further using intuitive concept subgoals build upon premise even single trajectory explained eﬃciently locally within certain context globally enabling compact representation observed behavior. based assumption build implicit intentional model agent’s goals forecast behavior unobserved situations. result integrated bayesian prediction framework provides smooth policy estimates consistent expert’s plan signiﬁcantly outperform existing solutions. notably framework naturally handles situations intentions agent change time classical algorithms fail. addition probabilistic nature model straightforwardly applied active learning setting guide demonstration process expert. inverse reinforcement learning refers problem inferring intention agent called expert observed behavior. markov decision process formalism intention encoded form reward function provides agent instantaneous feedback situation encountered decision-making process. classical methods assume single global reward model explain entire demonstration data recent methods relax modeling assumption allowing intention agent change time hypothesizing data inherently composed several parts diﬀerent trajectories reﬂect intentions diﬀerent experts. work step start premise that even case single expert trajectory demonstrated behavior explained eﬃciently locally within certain context single global reward model. illustrative example consider task shown figure expert approaches intermediate goal positions ﬁnally heading toward global goal state. despite simplicity task encoding behavior global intention model requires reward structure comprises comparably large number redundant state-action based rewards. alternative solution strategies include task-dependent expansions agent’s state representation e.g. memorize last visited goal resort general decision-making frameworks like semi-mdps/options order achieve necessary level abstraction. contrast method presented paper learns rather simple task representation based intuitive concept subgoals allows eﬃciently encode expert behavior using task-appropriate partitionings state space/the data set. framework builds upon method bayesian nonparametric inverse reinforcement learning used build subgoal representation task demonstration data—however without learning underlying subgoal relationships constructing policy model predict actions demonstrator. order address limitation generalize bnirl model using insights previous works nonparametric subgoal modeling policy recognition explicitly describe local dependencies expert demonstrations underlying subgoal structure allows build compact intentional model expert’s behavior. result integrated bayesian prediction framework exploits spatio-temporal context demonstrations capable producing smooth policy estimates consistent expert’s plan. capturing full posterior information data enables apply proposed approach active learning setting data acquisition process controlled posterior predictive distribution model. experimental study compare proposed approach common baseline methods variety benchmark real-world scenarios. results reveal approach performs signiﬁcantly better original bnirl model alternative interestingly enough algorithm outperforms solutions considered tasks. idea decomposing complex behavior smaller parts around long researchers approached problem many diﬀerent ways. overall ﬁeld methods large covered here existing approaches clearly categorized according certain criteria. first methods diﬀer exact problem formulation i.e. distinguish active methods learning algorithm freely interact environment passive methods behavioral model trained solely observation second discriminate methods build intentional model considered task work directly control/trajectory level latter distinction sometimes referred intentional/subintentional methods order give concise summary work relevant ours restrict demonstration-based approaches focus intentional methods ﬁeld considerably smaller. overview reinforcement learning based methods refer existing literature e.g. work daniel first methods pursue decomposition observed behavior global level using trajectory-based approaches. example dimitrakakis rothkopf proposed hierarchical prior reward functions account fact diﬀerent trajectories data could reﬂect diﬀerent behavioral intentions e.g. generated diﬀerent domain experts. similarly babe¸s-vroman follow expectation-maximization based clustering approach group individual trajectories according underlying reward functions. choi generalized idea proposing nonparametric bayesian model number intentions priori unbounded. methods consider demonstration data global scale work concerned problem subgoal modeling often conducted form option-based reasoning instance tamassia proposed clustering approach based state distances minimal options explain expert behavior. method provides simple alternative hand-crafting options allow probabilistic treatment data involves many ad-hoc design choices. going direction daniel presented principled probabilistic option framework based expectation-maximization. framework capable automatically inferring sub-policies trajectory information also used reinforcement learning setting intra-option learning. however resulting behavioral model based point estimates policy parameters number sub-policies needs speciﬁed manually. latter problem solved krishnan proposed hierarchical nonparametric framework learn sequential representation demonstrated task based transition regions deﬁned local changes linearity observed behavior. however contrast work daniel inference performed jointly several isolated stages where again stage propagates point estimate associated model parameters. moreover temporal relationship demonstration data used identify local linearity changes considered ad-hoc fashion windowing function. another general class models explicitly addresses issue employs hidden markov model structure establish temporal relationship demonstrations. instance work presented nguyen regarded generalization model babe¸s-vroman extending expectationmaximization framework imposing markov structure reward model. similarly niekum extended segment demonstrations vector autoregressive models order learn suitable movement primitives. however learning primitives done post-processing step meaning quality ﬁnal representation crucially depends success initial segmentation stage. contrast method rueckert automatically learns position timing multiple subgoals form via-points number points assumed known system objective encoded global cost function. recently lioutikov presented related approach based probabilistic movement primitives jointly solves segmentation learning step unknown number primitives expectation-maximization framework. model operates purely trajectory level cannot provide insights latent intention demonstrator. another variant approach niekum explicitly addresses problem proposed surana srivastava paper authors propose replace emission model model order infer policy segmented trajectories rather recognize changes dynamics. model later extended ranchod augmented representation beta process model facilitate skill sharing across trajectories. resulting formulation ﬂexible major drawback model inference computationally expensive since involves multiple iterations gibbs step. contrast hmm-based solutions sequential nature focus temporal relationship subtasks approach presented paper establishes correlation structure demonstrations without committing temporal factorization subgoals using non-exchangeable distribution subgoal assignments. results compact model representation brings ﬂexibility capture both temporal spatial dependencies subtasks. organization remaining sections follows section summarize bnirl model forms basis work. section introduces intentional subgoal framework addresses shortcomings bnirl discussed section section derive sampling-based inference scheme model explain framework used subgoal extraction action prediction. purpose section recapitulate principle bayesian nonparametric inverse reinforcement learning. brieﬂy discussing building blocks model focus limitations framework motivates need extended model formulation ﬁnally leads inference approach presented afterwards section form state-action pairs consists state visited agent corresponding action taken. herein denotes size demonstration set. throughout rest paper shorthand notations {sd}d access collection expert states actions individually. note bnirl model makes assumptions temporal ordering demonstrations i.e. state-action pair considered arise speciﬁc arbitrary time instant agent’s decision-making process. come back point later section section contrast classical formalism frameworks bnirl presuppose observed expert behavior necessarily originates single underlying reward function. instead introduces concept subgoals underlying assumption that decision instant expert selects particular subgoal plan next action. subgoal herein represented certain reward function deﬁned system state space; simplest case itive constant although principle legitimate associate subgoal arbitrary reward structure order encode complex forms goal-oriented behavior restriction reward function class equation suﬃcient behavioral complexity synthesized combination several subgoals. made possible nonparametric nature bnirl model i.e. number subgoals assumed unbounded. simple reward model advantage however posterior inference subgoals targeted expert becomes computationally tractable explained section following therefore assume reward model equation subgoal assignment bnirl implemented using indicator variables annotate demonstration pair unique subgoal index. prior distribution herein modeled chinese restaurant process assigns event indicator points subgoal prior probability ˜z\\d {˜zd} shorthand notation collection indicator variables without ˜zd. further denotes number assignments subgoal ˜z\\d represents number distinct entries ˜z\\d parameter controlling expectation respect stochastic state sequence expert generated policy initial action executed starting state explicit notation st=n at=n used disambiguate temporal index decision-making process notice subgoal prior distribution original bnirl formulation take state variabe argument. nonetheless authors bnirl suggest restrict support prior visited states indeed implies conditioning structure distribution visualized form graphical model figure worth emphasizing π—although referred likelihood model state-action pairs original bnirl paper—is really model actions conditional states. contrast stated original paper distribution equation therefore takes form conditional distribution provide generative description state variables. subgoal-based inference well-motivated approach bnirl framework shown promising results variety real-world scenarios. original model formulation michini comes number signiﬁcant conceptual limitations explain detail following paragraphs. central limitation bnirl framework restricted pure subgoal extraction inherently provide suitable mechanism forecast expert behavior based inferred subgoals. reason lies particular design framework which heart treats subgoal assignments exchangeable random variables implication induced partitioning model agnostic covariate information data resulting behavioral model unable reasonably propagate expert knowledge situations. assume model perfectly inferred subgoals corresponding subgoal assignments demonstration set. denoting predicted action state bnirl model yields latent subgoal indicator belonging herein either represent softmax decision rule equation optimal policy subgoal g˜z∗ depending whether aspire describe noisy expert behavior want determine optimal action according inferred gathered information would likely subgoal targeted expert however bnirl distribution modeled without consideration query state observed variable. conditional independence distribution eﬀectively reduces prior intrinsic exchangeability property considers frequency readily inferred assignments clearly subgoal selection strategy based solely relative frequencies little comes predicting expert behavior state resulting subgoal assignment mechanism inevitably ignore structural information demonstration consistently produce subgoal assignment probabilities states irrespective agent’s actual situation. contrast reasonable assignment authors bnirl discuss action selection problem paper propose subgoal assignment strategy states based action marginalization approach provide satisfactory solution problem alleged conditioning query state eﬀect involved subgoal indicator variable shown equation remedy problem without modifying model external postprocessing scheme like waypoint method discuss next section. waypoint method described full length follow-up paper michini post-processing routine converts subgoals identiﬁed bnirl valid option model based spatio-temporal information contained demonstration set. purpose required initiation termination sets option-policies constructed state proximities identiﬁed subgoals temporal ordering prescribed expert. combined bnirl method allows synthesize behavioral model mimics observed expert behavior. however strategy comes number signiﬁcant drawbacks using waypoint method spatio-temporal context explored post-hoc fashion largely ignored actual inference procedure explained limitation lack context-awareness makes inference mechanism overly prone demonstration noise measuring proximities subgoals order determine visitation order requires form distance metric deﬁned state space. system states correspond physical locations constructing metric usually straightforward. however general case states encode arbitrary abstract information become diﬃcult design metric hand. unfortunately bnirl framefigure schematic illustrate implications exchangeability assumption bnirl. similar bag-of-words model bnirl partition assignment mechanism ignores spatio-temporal context data makes diﬃcult discriminate demonstration noise real change agent’s intentions. note schematic illustrates partitioning process simpliﬁed shows eﬀect prior neglects impact likelihood model latter indeed consider state context actions cannot account spatial temporal patterns data processes state-action pairs separately. work provide solution problem. waypoint method cannot applied multiple unaligned trajectories cases data contain temporal information. situation occurs instance data provided form trajectories given individual state-action pairs recorded arbitrary time points. assigning particular order inferred subgoals meaningful expert eventually reaches subgoals demonstration phase finding subgoals properties guaranteed constraining support subgoal prior distribution states near expert states reduces ﬂexibility model potentially disables compact encodings task reasoning intentions agent basic types behavior encounter either agent follows static strategy optimize ﬁxed objective intentions agent change time. latter clearly general case also poses diﬃcult inference problem requires both identify intentions agent understand temporal relationship. static scenario contrast implies exists optimal policy task form simple state-to-action figure diﬀerence local global subgoal search. trajectories approaching goal. bottom agent heading towards global goal gets temporarily distracted follows original plan. left observed trajectories. center example partitioning assumption expert reached subgoals demonstration. right example partitioning without restriction subgoal locations yielding compact encoding task. objective. important understand model actually distinguish described scenarios. explained limitation temporal aspect data explicitly modeled bnirl framework even though waypoint method subsequently tries capture overall chronological order events. consequence model tailored either settings hand ignores valuable temporal context needed time-varying case reliably discriminate demonstration noise real change agent’s intention. hand model agnostic predeﬁned time-invariant nature optimal policy static scenario. lack structure makes inference problem harder necessary cases; also allows model learn inconsistent data representations static case since state potentially assigned subgoal violating above-mentioned state-to-action rule apart discussed limitations bnirl partitioning model turns major issues concerning softmax likelihood model subsequent pages demonstrate speciﬁc form model encodes number properties contradict intuitive understanding subgoal inference problem. properties less critical actual prediction expert behavior drastically aﬀect localization subgoals. since cause eﬀects somewhat hidden equation defer detailed explanation section figure schematic comparison basic behavior types. time-varying intentions cause agent perform diﬀerent action revisiting state. contrast time-invariant intentions imply simple state-to-action policy agent incentive perform diﬀerent action already visited state since assumption underlying goals remained unchained. diverging actions occur trajectory crossing point therefore explained result suboptimal behavior. minor problem original bnirl problem formulation algorithm expects demonstration data provided form state-action pairs requires full access expert’s action record. assumption restrictive practical point view conﬁnes application framework settings laboratorylike conditions allow complete monitoring expert. reason important note estimate actions recovered bnirl help additional sampling stage provided know successor state reached expert decision step. marginalized sampling scheme described paper present corresponding action sampling stage section section introduce redesigned inference framework analogy bnirl refer distance-dependent bayesian nonparametric derive model making series modiﬁcations original bnirl framework address previously described shortcomings conceptual level. rethinking part bnirl model begin discussion commonly used softmax action selection strategy context subgoal inference ﬁnally leads redesign action likelihood model next focus subgoal allocation mechanism introduce closely related model formulations targeting basic behavior types described figure thereby addressing limitations time-invariant case begin intermediate model introduces subtle important structural modiﬁcation bnirl framework. second step generalize model account structure control problem itself ﬁnally allows extrapolate expert behavior unseen situations. context present state space metric arises naturally context subgoal inference lastly tackle time-varying case present variant model explicitly considers temporal aspect problem. solution limitation discussed later section contrast bnirl presented models used likewise subgoal extraction action prediction. moreover sticking bayesian methodology presented approach provides complete posterior information levels. like many approaches found literature bnirl exploits softmax weighting transform q-values optimal policy valid action likelihood model. softmax action rule origin reinforcement learning known boltzmann exploration strategy commonly applied cope exploration-exploitation dilemma recent years however also become facto standard modeling decision-making process agent following paragraphs focus implications model subgoal extraction problem show contradicts understanding characteristics reasonable subgoal model have. particular argue subgoal posterior distribution arising bnirl softmax model limited inferring latent intention agent subgoal artifacts caused system dynamics reﬂect evidence contained demonstration set. based insights propose alternative transformation scheme consistent subgoal principle. corresponding q-values since values linear underlying reward function softmax likelihood model implies expert’s ability maximize long-term reward—as reﬂected distribution probability mass π—rises magnitude assumed subgoal reward. words assuming higher goal reward virtually increases level conﬁdence expert even though diﬃculty underlying task optimal policy remain unchanged. nonetheless bnirl model requires readjust uncertainty coeﬃcient order keep models consistent. bnirl model provides reference level expert’s uncertainty across diﬀerent scenarios choice becomes non-trivial. parameter signiﬁcant impact granularity learned subgoal model trades purposeful goal-oriented behavior random decisions. note described eﬀect speciﬁc subgoal reward model equation arises setting arbitrary reward function example provide additional constant reward states. clearly reward function provides additional information underlying task hence aﬀect agent’s belief optimal choice actions based observations intuition tells seek model rationality invariant aﬃne transformations reward signal meaning reward functions second implication softmax likelihood model less immediate inherently tied dynamics system. explain problem consider generic scenario precise idea potential goals expert. example adopt grid world model described section consider simple upward-directed trajectory state-action pairs aspire explain using single goal. complete setting depicted figure intuitively shown demonstration give rise goals located upper region state space concentrated around vertical center line. moreover move away center line expect smooth decrease subgoal likelihood rate decay reﬂect assumed level conﬁdence expert. turns induced bnirl subgoal posterior distribution shown diﬀerent values contradicts intuition. particular observe likelihood model yields unreasonably high posterior values upper border states corners state space which according intuitive understanding subgoals cannot justiﬁed given demonstration set. cause eﬀect recall equation likelihood action grows q-value action. hence need causes q-values demonstrated actions large assume subgoal located corner states. using bellman’s principle express q-values given policy successor state inner expectation evaluates expected cumulative reward states reachable important note that—by construction q-function—only ﬁrst move agent state depends choice action figure comparison subgoal posterior distributions induced original bnirl likelihood model proposed normalized model based grid world dynamics described section uniform subgoal prior distribution. range shown color scheme understood subﬁgure. black squares indicate wall states. bnirl likelihood model yields unreasonably high subgoal posterior mass border states corners state space locally increased state visitation probabilities arising wall reﬂections well trajectory ending caused implicit proximity property model. eﬀects mitigated proposed normalized likelihood models action selection process expert using relative advantages available actions instead absolute returns. irrespective chosen action q-values large whenever subgoal induces high state visitation frequency location latter fulﬁlled chance reaching goal small number steps high eﬀect discounting small and/or subgoal-induced transition dynamics give rise high goal visitation frequency itself. note ﬁrst condition implies model generally prefers subgoals close demonstration set—a property cannot justiﬁed cases. example recording demonstrations could simply ended expert able reach goal however desired proximity property naturally attributed example figure observe eﬀects described conditions clearly. particular upward-directed policy implied shown demonstration state visitation distribution exhibits increased values exactly aforementioned border corner states arbitrary constant canceled equation contrast bellman state-action value q∗which describes return action normalized function assesses return action relation returns thus serve indicator relative quality action. such interpreted relative advantages normalized action likelihood model constructed analogously bnirl likelihood model moreover bottom figure reveals induced posterior distribution notably closer expectation. reason twofold ﬁrst likelihood computation based relative advantages mitigates inﬂuence transition dynamics discussed section described cumulation eﬀect state visitation distribution present returns actions thus reduced normalization. second since normalization diminishes eﬀect discounting posterior subgoal mass less concentrated around trajectory ending shows signiﬁcant probability along extrapolated path agent. property allows identify located states potential subgoals adds ﬂexibility inferred subgoal constellation example consider rightmost subﬁgures figure observe normalized model assigns high posterior mass states right three corridors since subgoals located areas explain demonstration equally well. here diﬀerence models even pronounced transition dynamics strong impact agent behavior added wall states. details refer section provide additional insights subgoal inference mechanism. redesigned likelihood model focus partitioning structure model. herein ﬁrst consider case intentions agent constant respect time. explained limitation setting consistent standard formalism meaning optimal policy considered task described form state-to-action mapping. underlying system state space replacing demonstration-based indicators {˜zd unlike variables operate directly data instead tied elements although formally intermediate model makes possible reason policy visited parts state space. model unable extrapolate gathered information unvisited states reasons explained section problem solved replacing exchangeable prior distribution subgoal assignments induced non-exchangeable order account explicitly covariate state information demonstrations. based insights bayesian policy recognition distance-dependent chinese restaurant process purpose allows intuitive handling covariate information explained below. note exist variety choices herein called self-link parameter process denotes distance state state monotone decreasing score function. note distances {∆ij} obtained suitable metric deﬁned setting suitable metric derived transition dynamics system turns canonical choice ddbnirl-s model. consider markov chain governing state process {st=n}∞ agent speciﬁc policy ordered pair states chain naturally induces value called hitting time represents expected number steps required state process initialized eventually reaches state ﬁrst time context subgoal problem natural quasi-metric measure directed distance states thus given time takes reach goal state starting state corresponding optimal subgoal policy maxa∈a i.e. i→j. ddbnirl-s choice particularly appealing since subgoal policies {πj} already available within inference procedure corresponding distances {∆ij} computed eﬃciently single policy figure relationship presented model structures illustrated form bayesian networks. shaded nodes represent observed variables deterministic dependencies highlighted using double strokes. case changing expert intentions need keep ﬂexibility bnirl model select subgoal decision instant instead restricting policy target unique subgoal state hence reuse basic bnirl structure deﬁne subgoal allocation mechanism using data-related indicator variables. however contrast bnirl makes assumptions temporal relationship subgoals allows arbitrary changes expert’s intentions design joint distribution favors smooth action plans expert persistently follows subgoal extended period time. again make ddcrp principle encode underlying smoothness assumption time using score function deﬁned temporal distance demonstration pairs. purpose require additional piece information namely unique time stamp indices range size demonstration set. herein denotes temporal distance data points before ∼-notation distinguish data-related partitioning variables since distance-dependent model contains classical special case speciﬁc choice distance metric score function temporal ddbnirl-t model considered strict generalization original bnirl framework neglect likelihood normalization spatial ddbnirl-s model generalizes intermediate model presented section however although bnirl model recovered ddbnirl important note sampling mechanisms models fundamentally diﬀerent. whereas bnirl sample subgoal assignments directly clustering structure ddbnirl deﬁned implicitly assignment variables respectively. explained blei frazier eﬀect markov chain underlying gibbs sampler mixes signiﬁcantly faster several cluster assignments altered single step eﬀectively realizing blocked gibbs sampler introduced ddbnirl framework explain used generalize expert behavior. ﬁrst focus task action prediction given test state explain second step extract necessary information demonstration data. along also give insights implicit intentional model learned framework. note order keep level redundancy minimum following considerations based time-invariant ddbnirl-s model. results ddbnirl-t follow straightforwardly; change equations subgoals referenced. reward model. however contrast existing methods approach based point estimates expert’s reward function takes account entire hypothesis space reward models order obtain full posterior predictive policy expert data. mathematically task formulated computing predictive action used fact prediction conditionally independent demonstration given state partitioning structure corresponding subgoal assigned joint distribution equation follows total number clusters deﬁned explained blei frazier indicator samples {c{n}} eﬃciently generated using fast-mixing gibbs chain. starting given ddcrp graph deﬁned subset indicators {cj}\\ci state already cluster state creates link clusters. latter case involved clusters merged corresponds merging associated sums equation according three cases conditional distribution gibbs procedure obtained figure insertion edge ddcrp graph. colors indicate cluster membership nodes deﬁned implicitly connected components graph. adding self-loop adding edge already connected nodes change clustering structure. adding edge unconnected components merges clusters. corresponds normalizing constant posterior distribution cluster subgoal accordingly fraction equation interpreted likelihood ratio partitioning deﬁned merged structure inserting edge important note inference method described previous sections based collapsed sampling scheme subgoals model marginalized out. fact ddbnirl framework diﬀers bnirl methods reward model expert never made explicit predicting actions. nonetheless desired estimate subgoal locations obtained post-hoc fashion subgoal posterior distribution equation given assignment examples provided figure mentioned section original bnirl algorithm requires complete knowledge expert’s action record limits range potential applications. reason generalize inference scheme case access state information only provided form alternative data refers state visited expert immediately setting inference which ﬁxed assignment recovers latent action information observed state transitions. note knowledge transition model required step provides necessary link expert actions observed successor states. extension possible ddbnirl-t model provided transition time demonstration set. further write refer state cluster access number demonstration data points associated cluster indicate number clusters current iteration |supp| shorthand variables i.e. ddbnirl-s ddbnirl-t. initialization phase common discussed models depend preceding planning phase compute—in parallel—the state-action value functions considered subgoals order construct action likelihood model overall computational complexity complexity used planning routine solve size total number actions. using value iteration algorithm instance done assume expert reaches subgoals demonstration phase restrict support subgoal prior visited states upper bounded min. note exist approximation techniques make start sampling procedure compute single-cluster likelihoods pairwise likelihoods {l)} according equation based initial cluster structure. likelihood computation cluster involves product data points needs calculated subgoals partition inference partition inference bulk computation lies repeated construction likelihood term equation needs updated whenever cluster structure changes. analyze complexity consider sampling step individual assignment variable worst case removing edge belongs ddcrp graph divide associated cluster parts single-cluster likelihoods need computed. upper bound number data points associated cluster clusters changing assignment additionally need track connected components underlying ddcrp graph. explained kapron done polylogarithmic worst-case time. action sampling order compute conditional probability distribution particular action need evaluate product involving actions belong cluster first compute product actions except itself number involved terms upper bounded appending term belongs possible action choices requires another operations. complexity order full gibbs cycle involves sampling action variables overall complexity hence order od). section present experimental results algorithm separate four parts proof concept conceptual comparison bnirl performance comparison related algorithms real data experiment conducted kuka robot active learning task. illustrate conceptual diﬀerences bnirl provide empirical insights latent intentional model learned framework begin data sets presented original bnirl paper considered system figure results bnirl data demonstration data sample partitionings generated inference algorithms. center subgoal posterior distributions corresponding partitions found ddbnirl-s ddbnirl-t. space reasons omit bnirl distributions bottom time-invariant ddbnirl-s policy model synthesized three detected subgoals temporal phases identiﬁed ddbnirl-t. missing generalization mechanism bnirl provide policy model black horizontal bar. valid states expert choose action comprising total eight actions initiating noisy state transition toward cardinal directions. observed state-action pairs depicted form arrows whose colors indicate partitioning learned bnirl. remaining subﬁgures show results ddbnirl framework computed posterior sample returned respective algorithm temperature simulated annealing schedule comparing obtained results observe following main diﬀerences original approach unlike bnirl proposed framework allows choose temporal spatial encoding demonstrated task order explicitly account type demonstrated behavior explained section context-unaware—yet principle temporal—vanilla bnirl inference scheme still included special case. exploiting spatial/temporal context data ddbnirl solution inherently robust demonstration noise giving rise notably smoother partitioning structures eﬀect particularly pronounced case real data shall later section state partition/trajectory segment obtain implicit representation associated subgoal form posterior distribution without need assigning point estimates striking posterior distribution corresponding green state partition comparably large spread upper side wall. explained intuitively fact subgoal located high posterior region could potentially caused green state sequence circumvents wall right. time green area high posterior values exhibits sharp boundary left side since subgoal located upper left region state space would likely resulted trajectory approaching left. contrast bnirl built-in generalization mechanism method returns predictive policy model comprising full posterior action information states. note show resulting policy estimates computed according equation additional results regarding posterior uncertainty shown later section example illustrates synthesis predictive model diﬀers between ddbnirl-s ddbnirl-t ddbnirl-t uses independent policy models describe diﬀerent identiﬁed behavioral phases ddbnirl-s maps entire subgoal schedule onto single time-invariant policy representation. looking closer learned models recognize ddbnirl-s solution fact realizes spatial combination three temporal ddbnirl-t components phase activated corresponding cluster region state space. provides alternative explanations task. expert trajectories length herein expert select optimal action probability random suboptimal action probability obtained state sequences passed diﬀerent algorithms compute normalized value loss since considered system belongs class mdps time-invariant reward functions ddbnirl-s lends natural choice model expert behavior. baseline methods adopt subintentional bayesian policy recognition framework well maximum-margin maximumentropy vanilla bnirl. missing generalization abilities bnirl waypoint method straightforwardly apply considered scenario multiple unaligned trajectories compare algorithm extension bnirl refer bnirlext. mimicking ddbnirl-s principle method accounts spatial context demonstrations assigning state bnirl subgoal targeted closest state-action pair—however assignments made actual subgoal inference. compared ddbnirl-s provides reference much eﬀectively gained modeling spatial relationship data explicitly. experiment ddbnirl-s bnirl augmented corresponding action sampling stages since action sequences expert discarded data order enable fair comparison remaining algorithms. figure shows value loss size demonstration diﬀerent reward settings. small bnirl ddbnirl-s signiﬁcantly outperform reference methods. sparse reward structure allows eﬃcient subgoal-based encoding expert behavior enables algorithms reconstruct policy even minimal amounts demonstration data. however bnirl solutions drastically deteriorate denser reward structures. particular observe clear diﬀerence performance cases account spatial information include post-processing step exploit inference demonstrates importance processing context information. tellingly ddbnirl-s outperforms baseline methods even dense reward regimes although subgoal-based encoding loses eﬃciency here. fact results reveal proposed approach combines merits model types i.e. sample eﬃciency intentional models required small data sizes well asymptotic accuracy fully probabilistic nature subintentional bayesian framework next experiment test ddbnirl framework various real data sets recorded kuka lightweight kinesthetic teaching. videos demonstrated tasks found supplementary material. system seven degrees freedom corresponding seven joints arm. joint equipped torque sensor angle encoder providing recordings joint angles velocities accelerations. experiments xy-cartesian coordinates spanning transverse plane considered computed measurements using forward kinematic model. data recorded sampling rate downsampled factor yielding eﬀective sample rate provided suﬃcient temporal resolution considered scenario. goal experiments learn high-level intentional models recorded behaviors partitioning data sets meaningful parts used predict desired motion direction expert. simplicity demonstrate algorithm’s robustness modeling errors adopt simplistic transition model section action containing eight cardinal motion directions. high measurement accuracy end-eﬀector position allows extract high-level actions directly data i.e. selecting directions comparably large loss small data sizes explained fact based general policy model expert behavior assumed inherently stochastic contrast considered setting stochasticity arises merely consequence suboptimal control. smallest angular deviations ground truth. underlying state space obtained discretizing part coordinate range covered measurements blocks predeﬁned size apart discretization step mentioned downsampling preprocessing applied. first consider case expert behavior described using time-invariant policy model aspire capture ddbnirl-s. example consider cycle task shown video. setting later analyzed using time-variant ddbnirl-t model allows direct comparison approaches task consists approaching number target positions indicated markers eventually returning initial state. setting regarded real-world version loop problem described michini explained paper classical algorithms holistic state-based reward model completely fail problem periodic nature task. background indicates learned partitioning structure computed low-temperature posterior sample. observe found state clusters clearly reveal modular structure task providing intuitive interpretable explanation data. however though induced policy model smoothly captures cyclic nature task cannot expect obtain trustworthy predictions center region state space lack additional demonstration data would required unveil expert’s true intention region. clearly point estimate shown policy cannot reﬂect prediction uncertainty since carry conﬁdence information. following bayesian approach naturally quantify prediction tempered chain. resulting entropy estimates summarized uncertainty overlaid original prediction result produce ﬁnal ﬁgure shown bottom right. note obtained posterior uncertainty information model used active learning setting demonstrated section figure result ddbnirl-s cycle task. measurements discretized demonstration data coloring background indicates partitioning structure obtained low-temperature posterior sample. maximum posteriori policy estimate returned model. visualization prediction uncertainty based entropy predictive action distribution. illustration ﬁnal predictive model comprising action information prediction uncertainty. supplementary video comprises diﬀerent time-dependent expert behaviors varying complexity. order obtain quantitative performance measure evaluation conducted manual segmentation recorded trajectories thereby producing ground truth subgoal labels observed decision instants. result segmentation step depicted appendix note ground truth subgoals assumed located immediately ends shown segments. left right column figure show respectively partitioning structures found bnirl ddbnirl-t based uniform subgoal prior distribution support visited states. underlying state discretization block size chosen figure subgoal localization errors ddbnirl-t bnirl demonstrated tasks shown supplement measured euclidean distance. black dots indicate change ground truth subgoal annotation. problem. quantitative comparison show instantaneous subgoal prediction errors models euclidean space figure ddbnirl-t directly return subgoal location estimate instead provides access full subgoal posterior distribution error computed respect subgoal locations using ddbnirl-t version equation black dots ﬁgure indicate time instants ground truth annotations change. time instants observe signiﬁcantly increased localization errors explained fact assumed ground truth annotation somewhat subjective around transition points also notice comparably high error beginning trajectories stems imperfect synchronization recording interval execution task capture accuracy single ﬁgure performance hence consider median localization error time series masks outliers thus provides realistic error quantiﬁcation sample mean. obtained values shown next error plots figure indicating ddbnirl-t localization error range discretization interval figure comparison random data acquisition active learning based monte carlo runs. curves show mean value losses obtained policy models number data queries. demonstrate basic procedure reconsider random problem section active learning context compare diﬀerent active learning strategies previously used random data acquisition scheme. initialization learning procedure request single state-action pair demonstrator store initial data herein state drawn uniformly random action generated according true expert policy continuing point considered active inducing sequence data sets next query state chosen respectively likely second likely action according considered distribution i.e. maxa∈a maxa∈a\\ˆa iteration compute value losses induced policy models compare loss results random data acquisition. resulting curves delineated figure expected learning speed model signiﬁcantly improved active acquisition schemes reduces required number expert demonstrations successfully learn observed task. building upon idea bayesian nonparametric inverse reinforcement learning proposed framework data-eﬃcient leverages context information demonstration learn predictive model expert behavior small amounts training data. central framework model architectures designed learning spatial subgoal plans capture time-varying intentions. contrast original bnirl model architectures explicitly consider covariate information contained demonstration giving rise predictive models inherently robust demonstration noise. original bnirl model recovered special case framework conducted experiments show drastic improvement vanilla bnirl approach terms achieved subgoal localization accuracy stems improved likelihood model context-aware clustering data. notably framework outperforms discussed reference methods analyzed benchmark scenarios additionally capturing full posterior information learned subgoal representation. resulting prediction uncertainty expert behavior reﬂected posterior predictive action distribution provides natural basis apply method active learning setting learning system request additional demonstration data expert. current limitation approach presented architectures require model discrete state action space. subgoal principle carries straightforwardly continuous metric spaces construction likelihood model becomes diﬃcult environments requires knowledge optimal state-action value functions potential subgoal locations. however bnirl exist several ways approximate likelihood cases concepts apply equally ddbnirl. thus interesting future study would compare eﬃcacy model types larger problems involving continuous spaces comes even natural follow distance-based approach. figure long motion sequences comprising large number sub-patterns overlapping parts separated considering temporal context. flower strokes performed absolute velocity. flower individual strokes performed alternating velocity.", "year": 2018}