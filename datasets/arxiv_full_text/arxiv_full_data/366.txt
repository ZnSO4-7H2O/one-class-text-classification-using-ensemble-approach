{"title": "TVAE: Triplet-Based Variational Autoencoder using Metric Learning", "tag": ["stat.ML", "cs.AI", "cs.CV", "cs.LG", "68T30 (Primary), 68T01 (Secondary)"], "abstract": "Deep metric learning has been demonstrated to be highly effective in learning semantic representation and encoding information that can be used to measure data similarity, by relying on the embedding learned from metric learning. At the same time, variational autoencoder (VAE) has widely been used to approximate inference and proved to have a good performance for directed probabilistic models. However, for traditional VAE, the data label or feature information are intractable. Similarly, traditional representation learning approaches fail to represent many salient aspects of the data. In this project, we propose a novel integrated framework to learn latent embedding in VAE by incorporating deep metric learning. The features are learned by optimizing a triplet loss on the mean vectors of VAE in conjunction with standard evidence lower bound (ELBO) of VAE. This approach, which we call Triplet based Variational Autoencoder (TVAE), allows us to capture more fine-grained information in the latent embedding. Our model is tested on MNIST data set and achieves a high triplet accuracy of 95.60% while the traditional VAE (Kingma & Welling, 2013) achieves triplet accuracy of 75.08%.", "text": "deep metric learning demonstrated highly effective learning semantic representation encoding information used measure data similarity relying embedding learned metric learning. time variational autoencoder widely used approximate inference proved good performance directed probabilistic models. however traditional data label feature information intractable. similarly traditional representation learning approaches fail represent many salient aspects data. project propose novel integrated framework learn latent embedding incorporating deep metric learning. features learned optimizing triplet loss mean vectors conjunction standard evidence lower bound vae. approach call triplet based variational autoencoder allows capture ﬁne-grained information latent embedding. model tested mnist data achieves high triplet accuracy traditional achieves triplet accuracy learning semantic similarity pairs images core part visual competence learning. applied proper embedding input data similarity metric functions euclidean distances mahalanobis distance cosine similarity result superior metric similarity measure reduce many complex classiﬁcation problems simple nearest neighbor problems. similarity metric functions would perform poorly applied complex input datasets. image embeddings learned part larger classiﬁcation task using deep nets various practical limitations several scenarios. extreme classiﬁcation problems number possible categories large possibly unknown conventional classiﬁcation learning approaches essentially useless since availability training examples class becomes scarce totally unavailable. hence line approach namely metric learning gained much popularity ability learn image embedding directly using concept relative distances rather relying speciﬁc category information. able learn metric space nearest neighbor based methods would naturally give superior performance higher quality representation input images learned embedding space. approach potential improve generative models variational autoencders learned. perform extremely efﬁcient approximate inference latent gaussian model latent embedding space learns lacks many salient aspects original data. motivated triplet network explained hoffer ailon project propose architecture loss function training capable tasks time learning latent image representations ﬁne-grained information stochastic inference. figure model overview. input triplet digit images given three identical encoder networks. mean latent vectors three input images used calculate triplet loss reconstructed images identical decoders used calculate reconstruction error. proposed hybrid model fig. motivated improve learn latent representation enriched ﬁne-grained information. achieve optimize network minimizing upper-bound expected negative log-likelihood data triplet loss simultaneously. encoder encodes image latent vector encoder decoder decodes latent vector back image decoder regularize encoder imposes prior latent distribution loss consists parts reconstruction loss divergence loss. reconstruction loss lrec −eq] negative expected log-likelihood observations kldivergence loss kl||p] characterizes distance distribution prior distribution. iteration training input triplet randomly sampled training anchor similar positive negative triplet three images encoder network simultaneously mean latent embedding deﬁne loss function ltriplet triplets model similarity structure images wang triplet loss expressed focus experiments preservation semantic structure learned latent embedding image generation ability compared original kingma welling experiments mnist adopted simple network structure fully connected layers encoder decoder used pixel-to-pixel distance loss function reconstruction loss. dimension latent embedding space visually explore learned embedding distribution mean vector. additional triplet loss term mean vectors different groups compactly clustered shown fig. hand without added triplet loss image clusters less compact seem spreading spatial space seen fig. case also observe images class likely divided multiple small clusters images different clusters overlaps often. order evaluate structure quality terms preserved relative distance among different classes analyze learned latent embedding unseen triplets. table calculate triplet accuracy deﬁned percentage triplets incur loss zero eq.. using tvae test triplets learned latent embedding maintain relative distances among classes. hand traditional preserve relative distances test triplets. figure comparison reconstructed images mnist dataset. ﬁrst input images mnist test set. second reconstructed images generated plain vae. third reconstructed images generated tvae. triplet based variational autoencoders provide tools learning latent embedding performing approximate inference leverage traditional deep metric learning techniques. incorporating triplet constraint learning process tvaes learn interpretable latent representation preserves semantic structure original dataset. method provides initial framework learning latent embedding would able encode various notions similarity. demonstrate tvae generates high quality samples good traditional encoding semantic structural information latent embedding. future work include analysis medical datasets. hyun song xiang stefanie jegelka silvio savarese. deep metric learning lifted structured feature embedding. proceedings ieee conference computer vision pattern recognition jiang wang yang song thomas leung chuck rosenberg jingbin wang james philbin chen ying learning ﬁne-grained image similarity deep ranking. proceedings ieee conference computer vision pattern recognition", "year": 2018}