{"title": "Recurrent Orthogonal Networks and Long-Memory Tasks", "tag": ["cs.NE", "cs.AI", "cs.LG", "stat.ML"], "abstract": "Although RNNs have been shown to be powerful tools for processing sequential data, finding architectures or optimization strategies that allow them to model very long term dependencies is still an active area of research. In this work, we carefully analyze two synthetic datasets originally outlined in (Hochreiter and Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store information over many time steps. We explicitly construct RNN solutions to these problems, and using these constructions, illuminate both the problems themselves and the way in which RNNs store different types of information in their hidden states. These constructions furthermore explain the success of recent methods that specify unitary initializations or constraints on the transition matrices.", "text": "spectral norm transition matrix signiﬁcantly different non-linear transition functions. spectral norm transition matrix greater gradients grow exponentially magnitude backpropagation known exploding gradient problem. spectral norm less gradients vanish exponentially quickly known vanishing gradient problem. recently simple strategy clipping gradients introduced proved effective addressing exploding gradient problem problem vanishing gradients shown difﬁcult various strategies proposed years address successful approach known long short-term memory units modify architecture hidden units introducing gates explicitly control information function state input. speciﬁcally signal stored hidden unit must explicitly erased forget gate otherwise stored indeﬁnitely. allows information carried long periods time. lstms become successful applications language modeling machine translation speech recognition methods proposed deal learning long-term dependencies adding separate contextual memory stabilizing activations using sophisticated optimization schemes recent methods propose directly address vanishing gradient problem either initializing parameterizing transition matrix orthogonal unitary matrices although rnns shown powerful tools processing sequential data ﬁnding architectures optimization strategies allow model long term dependencies still active area research. work carefully analyze synthetic datasets originally outlined used evaluate ability rnns store information many time steps. explicitly construct solutions problems using constructions illuminate problems rnns store different types information hidden states. constructions furthermore explain success recent methods specify unitary initializations constraints transition matrices. recurrent neural networks powerful models naturally suited processing sequential data. maintain hidden state encodes information previous elements sequence. classical version every timestep hidden state updated function input current hidden state. theory recursive procedure allows models store complex signals arbitrarily long timescales. designed pathologically difﬁcult require models store information long timescales different approaches solved problems varying degrees success. authors report hessianfree optimization based method solves addition task timesteps. authors reported method beat chance baseline adding task cases irnn reported solve addition task method proposed able solve copy task timesteps able completely solve addition task timesteps partially solves work analyze long-memory tasks construct explicit solutions them. solutions illuminate tasks provide theoretical justiﬁcation success recent approaches using orthogonal initializations unitary constraints transition matrix rnn. particular show classical elman transition non-linearity random orthogonal initialization high probability close explicit solution sequence memorization task network architecture identity initialization close explicit solution addition task. verify experimentally initializing correctly critical success tasks. finally show pooling used allow model choose random-orthogonal identity-like memory. several works studied properties orthogonal matrices relation neural networks. work gives exact solutions learning dynamics deep linear networks based analysis suggests orthogonal initialization scheme accelerate learning. authors study ability linear rnns store scalar sequences hidden state show memory capacity scales number hidden units. work complements providing related analysis discrete input sequences. review recurrent neural network architectures processing sequential data discuss modiﬁcations long memory problems. following notation input sequences denoted output sequences denoted srnn srnn consists transition matrix decoder matrix encoder matrix bias either output input categorical number classes one-hot representation. srnn ingests sequence keeps running updates hidden state using hidden state decoder matrix produces outputs input output hidden state respectively time great improvements training srnns since introduction shown powerful models tasks language modeling still difﬁcult train generic srnns information inputs hundreds timesteps previous computing current output following simpliﬁcation srnns makes sense less powerful models makes easier train solve simple long-memory tasks. namely placing non-linearity input hidden state rather hidden state output obtain rnns linear transitions call lt-rnns. update equations then lstm architecture designed improve upon srnn introduction simple memory cells gating architecture. work architecture originally proposed memory cell network computes output four gates update gate input gate forget gate output gate. outputs gates relatively common variation original lstm involves adding so-called peephole connections allows information cell state various gates. variant originally designed measure generate precise time intervals proven successful speech recognition sequence generation consider lt-rnns initialized either random orthogonal transition matrices identity transitions large difference behavior initializations. however architecture random orthogonal initialization behaves much closer identity initialization using pooling layer output. feed pooled unpooled hidden layer decoder model choose whether wants identity-like randomorthogonal like representation. pool size update equations model task tests network’s ability recall information seen many time steps previously. follow setup brieﬂy outline here. {ai}k symbols pick numbers input consists length vector categories starting entries sampled uniformly {ai}k sequence remembered. next inputs blank category. following input represents delimiter indicating network output initial entries input. last inputs ak+. required output sequence consists entries followed ﬁrst entries input sequence exactly order. task minimize average cross-entropy predictions time step amounts remembering categorical sequence length time steps. write lt-rnn solution problem. write descriptions equation note since inputs categorical assume non-linearity used. number pick random integer drawn uniformly block diagonal matrix. note iterating spins different rates synchronize multiples thus acts clock period matrix columns sampled uniformly unit sphere form appending zero columns extra entry entry entry. schematically q−juij small brieﬂy argue large enough w.r.t. repeatedly variance independent mean-zero random variables grows variances. denote pair coordinates column corresponding block; since uniform sphere expect thus small number choose large enough high probability finally weak dependence here; ﬁxed exponentially unlikely nearest neighbor close enough interfere. show operates starting high-level overview. last dimension hidden state divides state space regions model outputs blank symbol outputs ﬁrst symbols dictionary. model begins ﬁrst region remains encounters delimiter symbol sends second. symbols input sequence encoded hidden state rotation applied timestep. result rotation powers hides symbol encoded hidden state i.e. decorrelates current representation original one. periodicity timesteps different symbols input sequence surface hidden state time order seen symbols sequence whose representations rotations applied them remain hidden. causes output units symbols correct order. solution mechanism suggests random orthogonal matrix good starting point solving task. construction invariant rotations; always basis given orthogonal matrix block form basis. thus necessary descent nudge eigenvalues orthogonal matrix roots unity already basic form construction above. also gives good explanation performance models used copy problem since construction copy mechanism randomized provide experiment show solution degrades function strong dependence figure shows number successes runs note solution mechanism copy problem depends ﬁxed location regurgitating input. experiments below also discuss variant copy task symbol indicate memorized sequence must output randomly located considered variant task know bounded explicit lt-rnn srnn solution variable length problem adding problem requires network remember marked numbers long sequence them. specifically input consists dimensional sequence xt}. ﬁrst coordinate uniformly sampled second coordinate save two; entries required output comparison tasks note matrix mechanism adding problem identity. build redundant solution using larger identity matrix. describe identity using block structure matrix deﬁned copy task; namely hand copy task acts clock synchronizes ﬁxed number steps important mechanism described clock looks random time example instead used block mechanism would succeed. transition matrices addition task copy task thus opposites sense addition copy uniformly distributed unit circle possible. experiments below show hard lt-rnn learn adding task transition matrix initialized random orthogonal matrix easy initialized identity vice-versa copy task. uniﬁed solution pooling initialized matrix distributed uniformly decoder choose pooled hiddens adding task hiddens clock-like. based analysis hypothesize ltrnn random orthogonal initialization perform well sequence memorization problem lt-rnn identity initialization perform well addition task. test this conducted following experiment copy addition task different timescales. task timescale trained lt-ornns lt-irnns different random seeds. transformation matrices models intialized using gaussian distribution mean variance lt-ornns projected transition matrix nearest orthogonal matrix setting singular values experiments used rmsprop train networks ﬁxed learning rate decay rate preliminary experiments tried different learning rates chose largest loss diverge lt-rnn’s used also included lstms experiments baseline. used method lt-rnn pick learning rate ended experiments normalized gradients respect hidden activations denotes number timesteps. preliminary experiments also found lt-rnn models activations frequently exploded whenever largest singular value transition matrix became much greater therefore adopted simple activation clipping strategy rescaled activations magnitude whenever magnitude exceeded experiments chose figure shows results copy task lstm lt-ornn lt-irnn. networks trained hidden units. lstm difﬁculty beating baseline performance outputting empty symbol; however eventually converge solution however lt-ornn solves task almost immediately. note behavior similar urnn paramaterized makes easy recover explicit solution described above. ltirnn never able solution. figure shows results addition task timesteps. networks trained hidden units. trained single ltornn lt-irnn time constraints. contrast copy task lt-irnn able efﬁciently solve problem whereas lt-ornn able solve long time all. lstm also able easily solve task consistent original work authors report solving task timesteps. note lstm baseline differs reported difﬁculty solving addition task. hypothesize difference different variants lstm architecture peephole connections. next series experiments examine effect feeding pooled outputs decoder could obtain good performance copy addition tasks single architecture initialization. experiments added soft penalty transition matrix keep orthogonal throughout training. speciﬁcally every iteration applied step stochastic gradient descent minimize loss evaluated random points unit sphere. note requires operations regular update requires operations adding soft constraint negligible computational overhead. experiments minibatch size. pooling experiments used pool size stride results shown figure lt-ornn pooling easily able solve copy task timescales approximately solves addition task timescales well even though convergence slower lt-irnn. success copy task surprising since zeroing matrix equation solve problem solution regular lt-ornn. good performance adding task somewhat interesting. gain insight network stores information stable manner orthogonal transition matrix plotted activations hidden states time processes input sequence. displayed figure observe relatively constant activations ﬁrst marked number encountered triggers oscillatory patterns along certain dimensions. second marked number seen existing oscillations ampliﬁed ones emerge. suggests network stores information stably radius hidden state’s rotations along different -dimensional subspaces. information recovered phase discarded though pooling operation. thus model uniform clocklike oscillations perceived δ-like pooling. seen stark impact initialization performance lt-irnns lt-ornns copy addition task mitigation addition pooling layer tested models problem solution mechanism namely variable length copy task. figure shows performance lt-irnn lt-ornn lt-ornn pooling lstm variable length copy task timesteps. even though number timesteps significantly less tasks none lt-rnns able beat chance baseline whereas lstm able solve task even though convergence slow. experiment classic example detail construction synthetic benchmark favor model fails generalize tasks. work analyzed standard synthetic long-term memory problems provided explicit solutions them. found copy problem solved using transition matrix root identity matrix whose eigenvalues well distributed unit circle remarked random orthogonal matrices almost satisfy description. also addition problem solved transition matrix. showed correspondingly initializing allows linear-transition easily optimized solving addition task initializing random orthogonal matrix allows easy optimization copy task; ﬂipping leads poor results. suggests optimization difﬁculty transitioning oscillatory steady dynamics mitigated adding pooling layer allows model easily choose regimes. finally experiment variable length copy task illustrates although synthetic benchmarks useful evaluating speciﬁc capabilities given model success necessarily generalize across different tasks novel model architectures evaluated broad benchmarks well natural data. pascanu razvan mikolov tomas bengio yoshua. difﬁculty training recurrent neural networks. proceedings international conference machine learning icml atlanta june saxe andrew mcclelland james ganguli surya. exact solutions nonlinear dynamics learning deep linear neural networks http//arxiv.org/abs/.. cite arxiv.. sutskever ilya vinyals oriol quoc sequence sequence learning neural networks. advances neural information processing systems annual conference neural information processing systems december montreal quebec canada gers felix schraudolph nicol schmidhuber jürgen. learning precise timing lstm mach. learn. res. recurrent networks. march issn http//dx.doi.org/ graves alex mohamed abdel-rahman hinton speech recognition deep recurgeoffrey ieee international conrent neural networks. ference acoustics speech signal processing icassp vancouver canada ./icassp.. http//dx.doi.org/./ icassp... martens sutskever learning recurrent neural networks hessian-free optimization. proceedings international conference machine learning icml bellevue washington june july", "year": 2016}