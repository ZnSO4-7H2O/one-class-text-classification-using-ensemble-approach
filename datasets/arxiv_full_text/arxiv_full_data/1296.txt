{"title": "Real-time emotion recognition for gaming using deep convolutional  network features", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "The goal of the present study is to explore the application of deep convolutional network features to emotion recognition. Results indicate that they perform similarly to other published models at a best recognition rate of 94.4%, and do so with a single still image rather than a video stream. An implementation of an affective feedback game is also described, where a classifier using these features tracks the facial expressions of a player in real-time.", "text": "current state-of-the-art approach part-based model developed detect intensity facial action units applied emotions generally described literature combinations action units keypoints extracted faces features computed hand-designed sparse representation patches around keypoints. also report performances different sets dataset onset apex emotion. onset limited ﬁrst frames sequence makes task harder facial expressions subtle. performance reported onset whereas performance apex i.e. last frames high seen previously described papers approaches developed solve emotion recognition customized features short sequences facial expressions therefore require particular efforts might generalizable vision-related tasks. interest paper lies developing state-of-the-art system task investigating whether features object recognition task transfered achieve adequate results without training would indicate good generalizability. following sections system developed classify emotions still images video streams described well performance evaluated. implemementation written python ubuntu reproducible freely available software. extract features image convolutional network model used trained million images imagenet large scale visual recognition challenge described python implementation model distributed donahue integrated system currently described. parameters downloaded avoid retraining important consideration difference images imagenet challenge emotion recognition task. whereas imagenet challenge contains images showing wide variety objects differences across images datasets abstract—the goal present study explore application deep convolutional network features emotion recognition. results indicate perform similarly recently published models best recognition rate single still image rather video stream. implementation affective feedback game also described classiﬁer using features tracks facial expressions player real-time. popularity recent years vision-related applications since shown achieve highest accuracies image classiﬁcation tasks features extracted networks trained classifying objects also applied tasks successfully training style classiﬁcation photographs paintings ﬁndings point potential using features generic visual system. goal paper study whether features also perform well emotion recognition task. extended cohn-kanade dataset recent dataset compiled emotion recognition large number participants compared datasets commonly used order provide baseline performances implemented multiclass active appearance model features average across emotions found confusion matrix shows accuracy blatant weakness emotion. since publication many researchers evaluate models dataset. khan proposed model based human behavior data. studied human participants eye-tracking experiment. recording gaze shown images facial expressions determined salient regions human face individual emotions expressed dataset. model extracts features regions interest identiﬁed eye-tracking experiment classiﬁes svm. features computed pyramid histograms gradients report performances slightly dataset comparable approaches published previously. sequences dataset grayscale others color every sequence grayscaled training. face detection also applied images preprocessing step pixels within rectangle detected viola-jones detector implemented opencv processed. effect applying ignoring face detection images dataset reported results section. classify features extracted still images support vector machine model chosen commonly used classiﬁer ﬁeld research popular implementations liblinear libsvm packaged python library scikit-learn tested. strategies exist multiclassing svms commonly used compared later paper one-versus-one one-versus-all. one-versus-one method tested multiple kernels values soft margin parameter one-versus-all method part liblinear therefore limited linear kernel. dataset also unbalanced representing emotions surprise often others. scikit-learn offers option assigning class weights relation frequency automatically enabled order account unbalanced data. scheme leave-one-participant-out used producing training test sets classiﬁer would tested novel faces. evaluation method maximizes data available consistent baseline experiments performed lucey measure commonly reported average accuracy across emotions i.e. equal weights given seven emotions even instances noted performances different values parameters shared results section validation offered lucey performances reported papers seem validation select parameters. done prevent reporting single best performance would biased. enable real-time emotion recognition video gaming session multithreaded application requiring webcam developed. video game consists using arrow keys keyboard avoid incoming debris plane model consists seven layers convolutional remaining ones fully-connected. output every layer accessible features interest extracted layers layer six. respectively dimension noted trained ﬁlters ﬁrst layer become gabor ﬁlters visible figure subsequent layers providing higher-level representations features extracted ﬁrst layer. dataset chosen project extended cohnkanade dataset dataset consists sequences acted participants labelled judges following emotions anger contempt disgust fear happiness sadness surprise. picture taken http//nbviewer.ipython.org/github/ucb-icsi-visiongroup/decaf-release/blob/master/decaf/demos/notebooks/lena imagenet.ipynb simplicity. losing health collision detected. rate incoming debris controlled facial expression player decreasing player seems happy increasing otherwise forcing essentially player look happy order survive. hence called happiness game inspired facial feedback effect reported psychology smiling accentuate positive experience main thread application video stream captured player’s webcam continuously opencv library player’s face ﬁrst found using viola-jones detector implemented opencv. library’s implementation offers different classiﬁers parameters detector performance frontalface-alt found reliable used scale factor mininum number neighbors minimum size face also square pixels reduce computational load avoid detecting faces people standing webcam. frame according location found detector passed secondary thread features extracted grayscaled frame classiﬁed. processor tested phenom frames processed every second makes viable real-time application. result classiﬁcation passed main thread appends list keeping recent emotions detected. common emotion listed assigned player current emotion. done order keep current emotion stable single misclassiﬁed frames. fig. comparison layers given best multiclassing strategy. blue line one-versus-all method features layer green line one-versus-all method features layer best performance overall one-versus-one trained linear kernel value features taken ﬁfth layer. comparisons approaches made following discussion section. figure compares performances different values different multiclassing strategies tested features layer method one-versus-all performs better tested values effect around hand seen figure oneversus-all outperforms method consistently. comparisons done linear kernel. terms different kernels tested radial-basis function polynomial kernels experimented offered lower performance sensitive parameter selection experienced longer training times. therefore extensive evaluation performance reported here. order assess performance individual emotions table shows confusion matrix best model found approach described above true labels vertical axis predicted labels horizontal axis. emotions indicated ﬁrst letters. accuracies reported percentage. average accuracy across emotions removing face detection i.e. letting whole image processed convolutional network decreases performance models considerably. accuracy found one-versus-one trained linear kernel value features taken ﬁfth layer. figures confusion matrix best model without face detection shown without description. qualitative assessment real-time gaming experience affective feedback game solely tested myself writer paper therefore section highly subject bias proper experiments performed participants. still pointed application seemed accurately predict facial expressions time experience seemed uniform despite changes lighting conditions distances camera. models developed paper face detection images dataset suffer much wider imbalance across emotions decreases overall accuracy compared best model tested. face detection therefore seems useful preprocessing step even centered uniform pictures. main weaknesses model without face detection seem emotions fear sadness respectively. baseline system also lower fig. screenshot gaming session. number debris lowest since happiness detected shown terminal window left zoom single frame detected disgusted current emotion determined -snapshots window stable nonetheless. fig. comparison layers given best multiclassing strategy. blue line one-versus-all method features layer green line one-versus-all method features layer recognition rate emotions. partial explanation lower performances might related number sequences present emotions among emotions fewest instances. features outputted ﬁfth sixth layers network tested seventh layer ignored reasons. first performance degraded ﬁfth sixth seemed unlikely seventh layer would produce better features. second sixth seventh layers possibly optimized object classiﬁcation task trained fully-connected layers rather convolutional layers. seemed therefore likely output ﬁfth layer encoded image information unrelated speciﬁcally object classiﬁcation next layers transformed improve performance imagenet challenge. ﬁfth layer also shown perform better different tasks determining aesthetic rating picture sequences dataset recorded grayscale uniformize data images therefore grayscaled. change done performance observed slightly lower. however convolutional network trained color images might beneﬁt dataset color images. linear kernels shown perform better lower cost terms computational time compared polynomial radial basis function kernels. radial basis function seemed perform considerably worse others might limited grid-search done. differences performance explained high dimensionality features especially considering dimensions output ﬁfth layer. nonlinear kernels generally increase number dimensions order space dataset accurately separable therefore useful features dimension ones used khan approach namely -dimensional features given already high dimensionality convolutional features linear kernels therefore seem best option here. current approach processes apex frame sequences comparatively approaches video stream processed. using apex frame accentuates distinction emotions avoiding frames expressions subtle effect might balanced limiting system single frame rather motion information could extracted sequences. would possible motion information later version system multiple ways. possible method would similar employed real-time application aggregating predictions frame assigning commonly predicted label sequence. another method would stack features sequentially classiﬁer handle highdimensionality data points. dataset doesn’t seem offer information whether participants wearing glasses. testing real-time application glasses model trained without face detection used observed disgust often detected instead emotions glasses removed. surprising facial occlusions generally hard handle hand tested best model found glasses issue anymore. given observation would interesting test model participants present facial occlusions headwear glasses facial hair. present study investigated application pre-trained features deep convolutional neural network task emotion recognition. network initially trained object recognition training network took place emotion recognition task. simple preprocessing step detecting faces using viola-jones detector using features classiﬁes seven emotions accuracy karayev hertzmann winnemoeller agarwala darrell recognizing image style arxiv preprint arxiv. lucey cohn kanade saragih ambadar matthews extended cohn-kanade dataset complete dataset action unit emotion-speciﬁed expression computer vision pattern recognition workshops ieee computer society conference ieee jeni girard cohn torre continuous intensity estimation using localized sparse facial feature space automatic face gesture recognition ieee international conference workshops ieee viola jones rapid object detection using boosted cascade simple features computer vision pattern recognition cvpr proceedings ieee computer society conference vol. ieee pedregosa varoquaux gramfort michel thirion grisel blondel prettenhofer weiss dubourg vanderplas passos cournapeau brucher perrot duchesnay scikit-learn machine learning python journal machine learning research vol. zhao pietikainen dynamic texture recognition using local binary patterns application facial expressions pattern analysis machine intelligence ieee transactions vol. krizhevsky sutskever hinton imagenet classiﬁcation deep convolutional neural networks. nips vol. ciresan meier schmidhuber multi-column deep neural networks image classiﬁcation computer vision pattern recognition ieee conference ieee s´ebastien ouellet graduate student universit´e montr´eal mostly interested artiﬁcial intelligence. current line research inﬂuenced previous degree bachelor cognitive science carleton university.", "year": 2014}