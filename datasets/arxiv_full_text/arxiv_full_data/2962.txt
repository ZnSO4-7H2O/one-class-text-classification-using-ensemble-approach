{"title": "Probabilistic Zero-shot Classification with Semantic Rankings", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "In this paper we propose a non-metric ranking-based representation of semantic similarity that allows natural aggregation of semantic information from multiple heterogeneous sources. We apply the ranking-based representation to zero-shot learning problems, and present deterministic and probabilistic zero-shot classifiers which can be built from pre-trained classifiers without retraining. We demonstrate their the advantages on two large real-world image datasets. In particular, we show that aggregating different sources of semantic information, including crowd-sourcing, leads to more accurate classification.", "text": "paper propose non-metric rankingbased representation semantic similarity allows natural aggregation semantic information multiple heterogeneous sources. apply ranking-based representation zeroshot learning problems present deterministic probabilistic zero-shot classiﬁers built pre-trained classiﬁers without retraining. demonstrate advantages large real-world image datasets. particular show aggregating different sources semantic information including crowd-sourcing leads accurate classiﬁcation. standard multiclass classiﬁcation settings classes treated categorical without extra structure. side-information structure classes semantic relatedness information improve classiﬁcation itself transfer knowledge learned training domain solve problems domain. consider classiﬁcation problem following visual objects airplane automobile bird deer frog horse ship truck. many sources semantic information objects obtained. wordnet knowledge-base semantic hierarchies developed manually linguistic experts wordnet objects form hierarchical tree child object kind parent object. several similarity metrics deﬁned hierarchy shown figure twodimensional classical multidimensional scaling embedding. semantic relatedness also mined automatically existing corpora wikipedia google ngram corpus using search engines cosine angles co-occurrence vectors used similarity words. recently elaborate methods learning vectorial representations words also proposed figure example embedding representation seen ﬁgure similarity objects look different depending semantic source measure used. non-metric representation similarity. multiple sources semantic information potential complement improved classiﬁcation result. still best aggregate similarity inhomogeneous sources remains open problem. similarity measures different corpora methods directly comparable therefore simple averaging measures optimal. ﬁrst idea paper non-metric ranking-based representation semantic similarity instead numerical representation. words able distinguish truck closeness reference objects without using numerical similarity. special case similarity objects form semantic ranking cat. example semantic ranking figure representations semantic relatedness objects. left hierarchical tree wordnet. middle embedding using palmer metric. right embedding note embeddings different similarity measures look different labels hidden avoid clutter. according distance figure ordinal similarity sufﬁcient distinguishing truck also seems natural representation since ordinal similarity invariant scaling monotonic transformation numerical values therefore better chance consistent across different heterogeneous sources. moreover ordinal information obtained directly non-numerical comparisons. particular human subjects judge similarity objects easier subjects rank objects rather assign numerical scores similarity. zero-shot classiﬁcation without retraining. paper apply non-metric rankings-based representations semantic similarity zero-shot classiﬁcation problems zero-shot learning samples domain samples test domain goal construct classiﬁer using training data semantic knowledge domains standard approach classifying classes binary classiﬁcations one-vs-rest one-vs-one setting multiclass losses directly. already pre-trained classiﬁers training domain classes using settings classiﬁers ‘for free’ distinguish unseen classes truck without retraining training domain samples? figure provides intuition problem. consider multiple decision hyperplanes learned one-vs-one setting hyperplanes partition feature space ‘cells’ assigns ranking objects points inside interior. this note pairs objects compared cell transitivity follows metric triangle inequality. ranking unseen test sample assigned pre-trained classiﬁers compared semantic rankings truck zero-shot classiﬁcation assuming feature semantic similarities strongly correlated discussion). figure decision hyperplanes classifying objects partition feature space cells correspond rankings lines separating hyperplanes one-vs-one binary classiﬁcation setting. building idea present novel zero-shot classiﬁcation methods free re-training aggregate semantic information multiple sources. start proposing simple deterministic ranking-based method improve method introducing probability models rankings. probabilistic approach realvalued classiﬁcation scores mapped posterior probabilities rankings combined prior probability rankings learned semantic sources. advantage using probabilistic approach explained method experiment sections. posterior prior probabilities rankings classic probabilistic models ranking including plackett-luce mallows babingtonsmith models. experiment section demonstrate advantages approach numerically-based approach deterministic approach using well-known image databases animals-with-attributes cifar-/ particular demonstrate aggregating different semantic sources including crowd-sourcing leads accurate zero-shot classiﬁcation. remainder paper organized follows section present deterministic probabilistic rankingbased algorithms zero-shot classiﬁcation. section relate work others literature. section test methods real-world image databases conclude paper section zero-shot learning rankings notations. denote rankings items/classes denote ranking position item item number whose position write top-k ranking straightforward generalization ranking order ﬁrst items matter order remaining items ignored. abuse notation top-k ranking topk rankings well since full ranking special case partial order generalization ranking top-k ranking. ranking pair items satisfy either whereas neither partial order. addition partial order satisfy transitivity triple implies item positions general undeﬁned partial order. simple deterministic approach zero-shot learning using semantic rankings already outlined introduction. one-vs-one setting pre-trained classiﬁers assign ranking one-vs-rest binary classiﬁers assign real-valued scores test point according point’s distances decision hyperplanes. scores distance rankings. {cat dog} whose semantic rankexample ings respectively. unseen image classiﬁcation scores order πcat πtruck classify rather truck. kendall’s ranking distance number mismatching orders π}|. sometimes make sense compare closest items compare items. top-k version kendall’s distance proposed computed follows. sets reﬁne ranking-based algorithms considering probabilistic approach. several causes uncertainty ranking-based representation. first classiﬁer outputs test-domain sample conﬁdence since classiﬁers trained trainingdomain samples. second prior knowledge semantic rankings multiple semantic sources unanimous. third feature semantic similarities always coincide. reasons consider probability models rankings. discuss three models mallows plackett-luce babingtonsmith introduce needed reviews.) hunter proposed iterative method timation using minorization-maximization procedure generalizes expectation-maximization procedure converges global maximum solution certain condition data. experience simple gradient-based newton-raphson methods seem work well appropriate choice regularization parameter. mallows. mallows model full rankings deﬁned mode spread parameter kendall’s distance rankings. viewed discrete analog gaussian distribution ranking. distance written identity ranking vj’s deﬁned given samples rankings parameters mallows model total rankings also found mode known spread parameter found convex optimization owing fact log-likelihood concave function minimization also known kemeny optimal consensus aggregation problem known np-hard however known heuristic methods sequential transposition adjacent items admissible heuristics former method. starting average ranking initial value search adjacent items label sample dependent predicted ranking turn dependent sample probability test-domain label given sample obtained marginalizing latent ranking variable ﬁnal prediction label test sample made maxz terms probabilistic ranker prior semantic ranking first describe prior semantic ranking learned semantic sources section second describe probabilistic rankers based standard classiﬁers trained training-domain data section ﬁnal zero-shot classiﬁer unseen samples bringing learned components described section encode semantic similarity trainingtest-domain classes probabilistic ranking models training-domain classes test-domain class learn multiple instances rankings test-domain class. rankings come multiple linguistic corpora human-rated rankings directly. outline three popular models rankings plackett-luce mallows babington-smith models. plackett-luce. plackett-luce model probability observing top-k ranking non-negative parameters indicate relative chances ranked higher rest items invariant constant scaling v’s. interpretation generative procedure plackettluce model vase interpretation suppose vase inﬁnite number balls marked whose numbers proportional v’s. ﬁrst stage ball drawn recorded second stage another ball drawn recorded unless ball already selected case drawing tried again. procedure continued distinct balls drawn recorded. generative probability captured similar ranker evaluates probability ranking given taking account conﬁdence one-vspre-trained classione classiﬁers. ﬁers linear since ranker j=i+ wπ-π-. however different normalization term multiclass-loss classiﬁers. types classiﬁers accommodated. pre-trained classiﬁers multinomial logistic regression svms multiclass loss scores .... computed parameter vectors similar one-vs-rest case relation plackett-luce model gives ranker note original classiﬁer multinomial logistic regression fact direct generalization logistic regression also observed case trained parameters {wi} coincide optimal maximum likelihood parameters trained top- rankings ground truth labels training domain. summarize exist natural interpretations plackett-luce babington-smith models allow relate classiﬁcation scores parameters produce posterior probability rankings without training. place babington-smith. babington-smith model another probabilistic ranking model based pairwise comparisons. given items probability item ranked higher item given preferences {αij} probability ranking babington-smith model similar plackett-luce model probability product v’s. larger likely item ranked higher item however unlike plackett-luce model normalizing constant known closed form. modeling semantic prior probabilistic ranker next section. probabilistic ranker classiﬁers probabilistic ranker takes sample input probabilistically ranks similarity trainingdomain classes propose build rankers standard settings multiclass classiﬁers one-vs-rest one-vsone multiclass-loss classiﬁer output real-valued conﬁdence score used purpose. one-vs-rest binary classiﬁers. setting scores .... training-domain class. relate real-valued scores {fi} nonnegative parameters {vi} plackett-luce model setting instead producing single ranking deterministic approach ranker evaluates probability ranking given taking account conﬁdence one-vs-rest classiﬁers. one-vs-one binary classiﬁers. setting scores .... fc−c ﬁnal prediction label test sample made maxz parallels attribute-based approach direct similarity-based method also uses classiﬁcation scores probabilistic inference ours uses numerical similarity instead non-metric ranking presentation method. recently similarity-based approaches using semantic embedding proposed algorithms training test domain objects simultaneously embedded semantic space using multilayer neural networks. methods produce interesting results speciﬁc metric similarity models require retraining semantic model changes unlike method. mensink linear combination pre-trained classiﬁers classifying unseen data co-occurrence statistics semantic information whereas assume speciﬁc type similarity information. lastly method provides means aggregate multiple semantic sources addressed literature. datasets animals attributes dataset cifar-/ collected semantic similarity obtained wordnet distance searches wordvec glove amazon mechanical turk. table summarizes characteristics datasets types available semantic information used experiments. details data processing provided appendix. perform comprehensive tests probabilistic ranking-based zero-shot model three learning settings types semantic sources different prior models semantic rankings compare probabilistic ranking-based method decarlo sampling. alternatively uniform prior somewhat similar preliminary experiments mcmc-based summation showed inferior results simple version therefore omitted report. ﬁnal zero-shot classiﬁer classiﬁer major approaches zero-shot learning explored literature attribute-based similaritybased. classes training test domains assumed distinguishable common list attributes. attribute-based approaches often show excellent empirical performance however designing attributes discriminative common multiple classes correlated original feature time non-trivial task typically requires human supervision. similar arguments test-domain classes multiple heterogeneous sources. figure shows embeddings classical multidimensional scaling using distances. shows qualitative differences numerical similarity ranking embedding rankings better between-class separation within-class clustering embeddings numerical similarity suggesting non-metric order information consistent numerical similarity across different sources. also compute leave-one-out accuracy bayesian classiﬁcation rankings collected directly crowdsourcing animals cifar datasets. rankings ranking held remaining rankings used build semantic ranking probabilities using mallows plackett-luce models. prediction class held-out rankings made maximum classes. animals average accuracy ././. using mallows model ././. using plackett-luce model. cifar average accuracy ././. using mallows model ././. using plackettluce model. numbers show rankings obtained crowd-sourcing information discriminate test-domain classes accuracy. compare probabilistic ranking deterministic ranking direct similarity methods zero-shot classiﬁcation accuracy. three methods share image features linguistic sources semantic information different ways. uses probabilistic models combine multiple sources semantic similarity. inherently single source semantic similarity therefore multiple sources combined heuristically. ﬁrst normalize individual similarity sources range compute arithmetic geometric means multiple sources. note main difference uses rankings whereas uses numeric values. results shown table using averaged semantic similarity euclidean distance normalization kendall’s distance test-domain classes plotted different symbols colors. terministic ranking-based method direct similarity-based method closest state-of-the-art methods uses classiﬁer scores. also refer results literature comparison. regularization parameters classiﬁers determined validation partially manually avoid exhaustive cross-validation. test different hyperameters report results one-vs-rest one-vs-one trained svms followed platt’s probabilistic scaling multiclass used multinomial logistic regression. ﬁrst compare discriminability classes ranking numerical representations similarity without using image data. using linguistic sources animals compute pairwise distances similarity vectors. types distances computed euclidean distance numerical similarity withl normalization hausdorff distance top-k lists using kendall’s ranking distance note rankings obtained sorting numerically similarity. different representations average accuracy leave-one-out -nearest neighbor classiﬁcation shows ranking distances better euclidean distances discriminating table zero-shot classiﬁcation accuracy direct similarity deterministic ranking probabilistic ranking-based method tested different semantic source classiﬁer types. indiv averaged accuracy using individual semantic similarities arithm accuracy using arithmetic mean similarities geom accuracy using geometric mean similarities. best result method highlighted boldface. geom) better using individual similarity animals cifar datasets. plausible interpretation aggregate similarity reliable individual similarities despite using heuristic methods aggregation. highest accuracy whereas hight accuracy performs slightly better animals worse cifar. within accuracy affected much pre-trained classiﬁer type others. using linguistic sources highest accuracy much higher regardless whether single multiple sources used. suggests advantage using probabilistic models aggregate multiple semantic sources. within results one-vs-rest one-vs-one classiﬁers perform comparably multiclass logistic regression performs best. performs even better crowd-sourced semantic information linguistic sources animals opposite true cifar probably less reliability human subject ratings cifar literature accuracy attributes-based methods animals ranges compared method attributes. remind reader ﬁnding ‘good’ attributes non-trivial task. similarity attributes mined automatically corpora similarity-based methods perform much better attributed-based methods embedding-based method select pairs classes cifar although numbers directly comparable different settings performs noticeably better state-of-the-arts. fact distinguish auto deer deer ship truck accuracy without single training image categories. paper propose ranking-based representation semantic similarity alternative metric representation similarity. using rankings semantic information multiple sources aggregated naturally produce better representation individual sources. using representation probability models rankings present zero-shot classiﬁers constructed huang eric socher richard manning christopher andrew improving word representations global context multiple word prototypes. proceedings association computational linguistics mensink thomas verbeek jakob perronnin florent csurka gabriela. metric learning large scale image classiﬁcation generalizing classes near-zero cost. eccv mensink thomas gavves efstratios snoek cees costa co-occurrence statistics zero-shot classiﬁcation. computer vision pattern recognition ieee conference ieee pennington jeffrey socher richard manning christopher glove global vectors word representation. proceedings empiricial methods natural language processing torralba antonio fergus freeman william million tiny images large data nonparametric object scene recognition. ieee transactions pattern analysis machine intelligence direct similarity-based method implemented follows. probability modeled one-vs-rest binary classiﬁer followed platt’s probabilistic scaling trained training-domain feature label pairs. testing probability evaluated test image prediction test-domain class made estimation using animals attributes dataset collected processed training domain consists images types animals images used training validation sets. image dimensional features extracted test domain consists images types animals non-overlapping training-domain classes. semantic similarity animals provided computed different linguistic sources path distance wordnet co-occurrence wikipedia yahoo search yahoo image search flickr image search. collected cifar- cifar- training domain consists images types objects including animals plants household objects scenery. images cifar- training validation sets. test domain consists images types objects similar cifar- without overlap classes cifar-. images test data. compute features deep-trained neural network trained imagenet ilsvrc dataset consisting million images categories. apply cifar- cifar- training images network -dimensional output last hidden layer network features. semantic similarity cifar- cifar- compute wordnet path distance also used wordvec tools glove tools addition using linguistic sources amazon mechanical turk collect word similarity data crowdsourcing. participant survey shown word test domain classes asked sort words training domain according perceived similarity given word. initial order words randomized survey. pre-select closest words test-domain word found preliminary trials ordering words demanding time-consuming participants. animal closest words selected based average ranking words w.r.t. test-", "year": 2015}