{"title": "Deep, Convolutional, and Recurrent Models for Human Activity Recognition  using Wearables", "tag": ["cs.LG", "cs.AI", "cs.HC", "stat.ML"], "abstract": "Human activity recognition (HAR) in ubiquitous computing is beginning to adopt deep learning to substitute for well-established analysis techniques that rely on hand-crafted feature extraction and classification techniques. From these isolated applications of custom deep architectures it is, however, difficult to gain an overview of their suitability for problems ranging from the recognition of manipulative gestures to the segmentation and identification of physical activities like running or ascending stairs. In this paper we rigorously explore deep, convolutional, and recurrent approaches across three representative datasets that contain movement data captured with wearable sensors. We describe how to train recurrent approaches in this setting, introduce a novel regularisation approach, and illustrate how they outperform the state-of-the-art on a large benchmark dataset. Across thousands of recognition experiments with randomly sampled model configurations we investigate the suitability of each model for different tasks in HAR, explore the impact of hyperparameters using the fANOVA framework, and provide guidelines for the practitioner who wants to apply deep learning in their problem setting.", "text": "tion methods many cases relatively simple means sufﬁce obtain impressive recognition accuracies. however elaborate behaviours example interest medical applications pose significant challenge manually tuned approach work furthermore suggested dominant technical approach beneﬁts biased evaluation settings explain apparent inertia ﬁeld adoption deep learning techniques. deep learning potential signiﬁcant impact ubicomp. substitute manually designed feature extraction procedures lack robust physiological basis beneﬁts ﬁelds speech recognition. however practitioner difﬁcult select suitable deep learning method application. work promotes deep learning almost always provides performance best system rarely includes details seemingly optimal parameters discovered. single score reported remains unclear peak performance compares average parameter exploration. paper provide ﬁrst unbiased systematic exploration peformance state-of-the-art deep learning approaches three different recognition problems typical ubicomp. training process deep convolutional recurrent models described detail introduce novel approach regularisation recurrent networks. experiments investigate suitability model different tasks explore impact model’s hyper-parameters performance conclude guidelines practitioner wants apply deep learning application scenario. course experiments discover recurrent networks outperform state-of-the-art allow novel types real-time application sample-by-sample prediction physical activities. deep learning ubiquitous computing movement data collected body-worn sensors multivariate time-series data relatively high spatial temporal resolution analysis data ubicomp typically following pipeline-based approach ﬁrst step segment timehuman activity recognition ubiquitous computing beginning adopt deep learning substitute well-established analysis techniques rely hand-crafted feature extraction classiﬁcation techniques. isolated applications custom deep architectures however difﬁcult gain overview suitability problems ranging recognition manipulative gestures segmentation identiﬁcation physical activities like running ascending stairs. paper rigorously explore deep convolutional recurrent approaches across three representative datasets contain movement data captured wearable sensors. describe train recurrent approaches setting introduce novel regularisation approach illustrate outperform state-of-the-art large benchmark dataset. across thousands recognition experiments randomly sampled model conﬁgurations investigate suitability model different tasks explore impact hyperparameters using fanova framework provide guidelines practitioner wants apply deep learning problem setting. introduction deep learning represents biggest trend machine learning past decade. since inception umbrella term diversity methods encompasses increased rapidly continue driven resources academic commercial interests. deep learning become accessible everyone machine learning frameworks like torch signiﬁcant impact variety application domains ﬁeld beneﬁt deep learning human activity recognition ubiquitous computing dominant technical approach includes sliding window segmentation time-series data captured body-worn sensors manually designed feature extraction procedures wide variety classiﬁcafigure models used work. left right lstm network hidden layers containing lstm cells ﬁnal softmax layer top. bi-directional lstm network parallel tracks future direction past convolutional networks contain layers convolutions max-pooling followed fully-connected layers softmax group. fully connected feed-forward network hidden layers. series data contiguous segments either driven signal characteristics signal energy sliding-window segmentation approach. frames features extracted commonly include statistical features stem frequency domain. ﬁrst deep learning approach explored substitute manual feature selection corresponds deep belief networks auto-encoders trained generatively restricted boltzmann machines results mixed cases deep model outperformed combination principal component analysis statistical feature extraction process. subsequently variety projects explored pre-trained fully-connected networks automatic assessment parkinson’s disease emission model hidden markov models model audio data ubicomp applications popular approach ubicomp relies convolutional networks. performance variety activity recognition tasks explored number authors furthermore convolutional networks applied speciﬁc problem domains detection stereotypical movements autism signiﬁcantly improved upon stateof-the-art. individual frames movement data ubicomp usually treated statistically independent applications sequential modelling techniques like hmms rare however approaches able exploit temporal dependencies time-series data appear natural choice modelling human movement captured sensor data. deep recurrent networks notably rely long short-term memory cells recently achieved impressive performance across variety scenarios application explored settings. first investigated variety recurrent approaches identify individuals based movement data recorded mobile phone largescale dataset. secondly compared performance recurrent approach cnns datasets representing current state-of-the-art performance opportunity datasets utilised work. cases recurrent network paired convolutional network encoded movement data effectively employing recurrent network model temporal dependencies abstract level. recurrent networks applied model movement data lowest possible level sequence individual samples recorded sensor. comparing deep learning exploration deep models variety application scenarios still lack systematic exploration deep learning capabilities. authors report explore parameter space preliminary experiments usually omit details. overall process remains unclear difﬁcult replicate. instead single instantiations e.g. cnns presented show good performance application scenario. solely reporting peak performance ﬁgures does however reﬂect overall suitability method ubicomp remains unclear much effort went tuning proposed approach much effort went tuning approaches compared likely practitioner parameter conﬁguration works similarly well application? representative reported performance across models compared parameter exploration? parameters largest impact performance? questions important practitioner remain unanswered related work. paper provide ﬁrst unbiased comparison popular deep learning approaches three representative datasets ubicomp. include typical application scenarios like manipulative gestures repetitive physical activities medical application parkinson’s disease. compare three types models described below. explore suitability method chose reasonable ranges hyperparameters randomly sample model conﬁgurations. report performance across thousands experiments analyse impact hyperparameters approach. deep feed-forward networks implemented deep feed-forward networks correspond neural network hidden layers followed softmax-group. represents sequence non-linear transformations input data network. follow convention refer network hidden layers n-layer network. hidden layer contains number units corresponds linear transformation recitiﬁed-linear activation function. different regularisation techniques dropout during training unit hidden layer zero probability pdrop inference output unit scaled /pdrop max-in norm after mini-batch incoming weights unit network scaled maximum euclidean length din. limit number hyperparameters approach chose perform generative pre-training solely rely supervised learning approach. input data network corresponds frames movement data. frame consists varying number samples simply concatenated single vector rs∗d. model illustrated ﬁgure trained mini-batch approach mini-batch contains frames stratiﬁed respect class distribution training-set. minimise negative likelihood using stochastic gradient descent. convolutional networks cnns introduce degree locality patterns matched input data enable translational invariance respect precise location pattern within frame movement data. explore performance convolutional networks follow suggestions architecture regularisation techniques. overall architecture illustrated ﬁgure contains least temporal convolution layer pooling layer least fully connected layer prior top-level softmax-group. temporal convolution layer corresponds convolution input sequence different kernels width subsequent max-pooling looking maximum within region width corresponds subsampling introducing translational invariance system. width max-pooling ﬁxed throughout experiments. output max-pooling layer transformed using relu activation function. subsequent fully connected part effectively corresponds follows architecture outlined above. similar also perform max-in norm regularisation suggested input data corresponds frames movement data dnn. however instead concatenating different input dimensions matrix-structure retained trained using stratiﬁed mini-batches stochastic gradient descent minimise negative likelihood. recurrent networks order exploit temporal dependencies within movement data implemented recurrent neural networks based lstm cells vanilla variant contain peephole connections architecture recurrent connections within network form directed cycle current timestep considers states network previous timestep lstm cells designed counter effect diminishing gradients error derivatives backpropagated many layers through time recurrent networks lstm cell keeps track internal state represents it’s memory. time cells learn output overwrite null internal memory based current input history past internal states leading system capable retaining information across hundreds time-steps implement ﬂavours lstm recurrent networks deep forward lstms contain multiple layers recurrent units connected forward time bi-directional lstms contain parallel recurrent layers stretch future past current time-step followed layer concatenates internal states timestep practically ﬂavours differ signiﬁcantly application requirements. forward lstm contextualises current time-step based seen previously inherently suitable real-time applications where inference time future known. bi-directional lstms hand future past context interpret input timestep makes suitable ofﬂine analysis scenarios. work apply recurrent networks three different settings trained minimise negative likelihood using adagrad subject max-in norm regularisation. ﬁrst case input data network given time corresponds current frame movement data stretches certain length time whose dimensions concatenated denote model lstm-f. second application case forward lstms represents real-time application sample movement data presented network sequence recorded denoted lstm-s. ﬁnal scenario sees application bi-directional lstms sample-by-sample prediction problem denoted blstm-s. training rnns common applications rnns include speech recognition natural language processing. settings context input limited it’s surrounding entities training rnns usually treats contextualised entities whole example training complete sentences. context individual sample movement data well deﬁned least beyond immediate correlations neighbouring samples likely depends type movement wider behavioural context. problem well known ﬁeld affects choice window length sliding window segmentation order construct mini-batches used train initialise number positions start training-set. construct mini-batch extract samples follow position increase steps possibly wrapping around sequence. found important initialise positions randomly avoid oscillations gradients. approach retains ordering samples presented allow stratiﬁcation mini-batch w.r.t. class-distribution. training long sequences issue addressed work. approach outlined train sufﬁciently large memorise entire input-output sequence implicitly leading poor generalisation performance. order avoid memorisation need introduce breaks internal states reset zero mini-batch decide retain internal state carry-over probability pcarry reset zero otherwise. novel form regularisation rnns useful similar applications rnns. experiments different hyper-parameters explored work listed table last column indicates number parameter conﬁgurations sampled dataset selected represent equal amount computation time. conduct experiments three benchmark datasets representative problems tyical experiments machine three gpus model conﬁgurations except largest networks. epoch training evaluate performance model validation set. model trained least epochs maximum epochs. epochs training stops increase validation performance subsequent epochs. select epoch showed best validation-set performance apply corresponding model test-set. application har. ﬁrst dataset opportunity contains manipulative gestures like opening closing doors short duration non-repetitive. second pamap contains prolonged repetitive physical activities typical systems aiming characterise energy expenditure. last daphnet gait corresponds medical application participants exhibit typical motor complication parkinson’s disease known large inter-subject variability. detail dataset opportunity dataset consists annotated recordings on-body sensors participants instructed carry common kitchen activities. data recorded frequency locations body annotated mid-level gesture annotations subject data different runs recorded. used subset sensors show packet-loss included accelerometer recordings upper limbs back complete data feet. resulting dataset dimensions. subject validation replicate popular recognition challenge using runs subject test set. remaining data used training. frameby-frame analysis created sliding windows duration second overlap. resulting training-set contains approx. samples pamap dataset consists recordings participants instructed carry lifestyle activities including household activities variety exercise activities accelerometer gyroscope magnetometer temperature heart rate data recorded inertial measurement units located hand chest ankle hours resulting dataset dimensions. used runs subject validation runs subject test set. remaining data used training. analysis downsampled accelerometer data order temporal resolution comparable opportunity dataset. frame-by-frame analysis replicate previous work non-overlapping sliding windows seconds duration second stepping adjacent windows training-set contains approx. samples daphnet gait dataset consists recordings participants affected parkinson’s disease instructed carry activities likely induce freezing gait. freezing common motor complication affected individuals struggle initiate movements walking. objective detect freezing incidents goal inform future situated prompting system. represents twoclass recognition problem. accelerometer data recorded ankle knee trunk. resulting dataset dimensions. used subject validation runs subject test used rest training. analysis downsampled accelerometer data frameby-frame analysis created sliding windows second table best results obtained model dataset along baselines comparison. delta median refers absolute difference between peak median performance across experiments. results results illustrated ﬁgure graphs show cumulative distribution main performance metric dataset. graph illustrates effect category hyper-parameter estimated using fanova. overall observe large spread peak performances models mean f-score best performing approach worst pamap difference smaller still considerable best performing approach outperforms current state-of-the-art considerable margin mean f-score best discovered work outperforms previous results reported literature type model mean f-score weighted f-score good performance recurrent approaches model movement sample level holds potential novel applications alleviate need segmentation time-series data. duration overlap. training-set contains approx. samples order estimate impact hyperparameter performance observed across experiments apply fanova determines extent hyperparameter contributes network’s performance. builds predictive model model performance function model’s hyperparameters. non-linear model decomposed marginal joint interaction functions hyperparameters percentage contribution overall variability network performance obtained. fanova used previously explore hyperparameters recurrent networks practitioner important know aspect model crucial performance. grouped hyperparameters model three categories learning parameters control learning process; regularisation parameters limit modelling capabilities model avoid overﬁtting; iii) architecture parameters affect structure model. based variability observed hyperparameter estimate variability attributed parameter category higher order interactions between categories. performance metrics datasets utilised work highly biased require performance metrics independent class distribution. opted estimate mean f-score distributions performance scores differ models investigated work. cnns show characteristic behaviour fraction model conﬁgurations work remaining conﬁgurations show little variance performance. pamap example difference peak median performance mean f-score dnns show largest spread peak median performance approaches opp. forward rnns show similar behaviour across different datasets. practically conﬁgurations explored pamap non-trivial recognition performance. effect category hyperparameter recognition performance illustrated ﬁgure interestingly observe consistent effect parameters cnn. contrast expectation parameters surrounding learning process largest main effect performance. expected model rich choice architectural variants larger effect. dnns observe systematic effect category hyperparameter. pamap correct learning parameters appear crucial. architecture model. interestingly observed relatively shallow networks outperform deeper variants. drop performance networks hidden layers. related choice solely rely supervised training generative pretraining improve performance deeper networks. performance frame-based depends critically carry-over probability introduced work. always retaining internal state always forgetting internal state lead performance. found pcarry works well settings. ﬁndings merit investigation example carry-over schedule improve lstm performance. results sample-based forward lstms mostly conﬁrm earlier ﬁndings type model found learning-rate crucial parameter however bi-directional lstms observe number units layer discussion work explored performance state-of-the-art deep learning approaches human activity recognition using wearable sensors. described train recurrent approaches setting introduced novel regularisation approach. thousands experiments evaluated performance models randomly sampled hyperparameters. found bi-directional lstms outperform current state-of-the-art opportunity large benchmark dataset considerable margin. however interesting practitioner’s point view peak performance model process parameter exploration insights suitability different tasks har. recurrent networks outperform convolutional networks signiﬁcantly activities short duration natural ordering recurrent approach beneﬁts ability contextualise observations across long periods time. bi-directional rnns found number units layer largest effect performance across datasets. prolonged repetitive activities like walking running recommend cnns. average performance setting makes likely practitioner discovers suitable conﬁguration even though found rnns work similarly well even outperform cnns setting. recommend start exploring learning-rates optimising architecture network learning-parameters largest effect performance experiments. found models differ spread recognition performance different parameter settings. regular dnns model probably approachable practitioner requires signiﬁcant investment parameter exploration shows substantial spread peak median performance. practitioners therefore discard model even preliminary exploration leads poor recognition performance. sophisticated approaches like cnns rnns show much smaller spread performance likely conﬁguration works well iterations. references mohammad alsheikh ahmed selim dusit niyato linda doyle shaowei hwee-pink tan. deep activity recognition models triaxial accelerometers. arxiv. marc bachlin daniel roggen gerhard troster meir plotnik noit inbar inbal meidan talia herman marina brozgol eliya shaviv giladi potentials enhanced context awareness wearable assistants parkinson’s disease patients freezing gait syndrome. iswc ricardo chavarriaga hesam sagha alberto calatroni sundara tejaswi digumarti gerhard tr¨oster jos´e mill´an daniel roggen. opportunity challenge benchmark database pattern on-body sensor-based activity recognition. recognition letters nils hammerla thomas pl¨otz. let’s stick together pairwise similarity biases cross-validation activity recognition. ubicomp nils hammerla james fisher peter andras lynn rochester richard walker thomas pl¨otz. disease state assessment naturalistic environments using deep learning. aaai nicholas lane petko georgiev lorena qendro. deepear robust smartphone audio sensing unconstrained acoustic environments using deep learning. ubicomp pages natalia neverova christian wolf grifﬁn lacey fridman deepak chandra brandon barbello graham taylor. learning human identity motion patterns. arxiv. francisco javier ord´o˜nez daniel roggen. deep convolutional lstm recurrent neural networks multimodal wearable activity recognition. sensors thomas pl¨otz nils hammerla agata rozga andrea reavis nathan call gregory abowd. automatic assessment problem behavior individuals developmental disabilities. ubicomp nastaran mohammadian andrea bizzego seyed mostafa giuseppe jurman paola venuti cesare furlanello. convolutional neural network stereotypical motor movement detection autism. arxiv. charissa ronao sung-bae cho. deep convolutional neural networks human activity recognition smartphone sensors. neural information processing pages springer charissa ronaoo sungbae cho. evaluation deep convolutional neural network architectures human activity recognition smartproc. kiise korea computer phone sensors. congress pages nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. jmlr jian yang minh nhut nguyen phyo phyo xiao shonali krishnaswamy. deep convolutional neural networks multichannel time series human activity recognition. ijcai ming zeng nguyen mengshoel jiang pang juyong zhang. convolutional neural networks human activity recognition using mobile sensors. mobicase pages ieee", "year": 2016}