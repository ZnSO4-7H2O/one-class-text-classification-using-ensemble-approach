{"title": "MMD GAN: Towards Deeper Understanding of Moment Matching Network", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Generative moment matching network (GMMN) is a deep generative model that differs from Generative Adversarial Network (GAN) by replacing the discriminator in GAN with a two-sample test based on kernel maximum mean discrepancy (MMD). Although some theoretical guarantees of MMD have been studied, the empirical performance of GMMN is still not as competitive as that of GAN on challenging and large benchmark datasets. The computational efficiency of GMMN is also less desirable in comparison with GAN, partially due to its requirement for a rather large batch size during the training. In this paper, we propose to improve both the model expressiveness of GMMN and its computational efficiency by introducing adversarial kernel learning techniques, as the replacement of a fixed Gaussian kernel in the original GMMN. The new approach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN. The new distance measure in MMD GAN is a meaningful loss that enjoys the advantage of weak topology and can be optimized via gradient descent with relatively small batch sizes. In our evaluation on multiple benchmark datasets, including MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN significantly outperforms GMMN, and is competitive with other representative GAN works.", "text": "generative moment matching network deep generative model differs generative adversarial network replacing discriminator two-sample test based kernel maximum mean discrepancy although theoretical guarantees studied empirical performance gmmn still competitive challenging large benchmark datasets. computational efﬁciency gmmn also less desirable comparison partially requirement rather large batch size training. paper propose improve model expressiveness gmmn computational efﬁciency introducing adversarial kernel learning techniques replacement ﬁxed gaussian kernel original gmmn. approach combines ideas gmmn hence name gan. distance measure meaningful loss enjoys advantage weak∗ topology optimized gradient descent relatively small batch sizes. evaluation multiple benchmark datasets including mnist cifar- celeba lsun performance signiﬁcantly outperforms gmmn competitive representative works. essence unsupervised learning models underlying distribution data deep generative model uses deep learning approximate distribution complex datasets promising results. however modeling arbitrary density statistically challenging task many applications caption generation accurate density estimation even necessary since interested sampling approximated distribution. rather estimating density generative adversarial network starts base distribution gaussian distribution trains transformation network underlying distribution training gan-based algorithms require auxiliary network estimate distance different probabilistic metrics studied framework. instead training auxiliary network measuring distance generative moment matching network uses kernel maximum mean discrepancy centerpiece nonparametric two-sample test determine distribution distances. training trained pass hypothesis test shows even simple gaussian kernel enjoys strong theoretical guarantees however empirical performance gmmn meet theoretical properties. promising empirical results comparable challenging benchmarks computationally also requires larger batch size needs training considered less efﬁcient work improve gmmn consider using adversarially learned kernels instead ﬁxed gaussian kernels better hypothesis testing power. main contributions work section prove training learned kernels continuous differentiable guarantees model trained gradient descent. second prove distance measure kernel learning sensitive loss function distance empirically loss decreases distributions closer. section propose practical realization called learns generator adversarially trained kernel. propose feasible reduction speed stabilize training gan. section show computationally efﬁcient gmmn trained much smaller batch size. also demonstrate promising results challenging datasets including cifar- celeba lsun gmmn fails. best knowledge ﬁrst based work achieve comparable results works datasets. finally also study connection existing works section interestingly show wasserstein special case proposed certain conditions. uniﬁed view shows connections moment matching potentially inspire algorithms based well-developed tools statistics experiment code available https//github.com/octoberchang/mmd-gan. two-sample test gmmn assume given data {xi}n interested sampling necessary estimate density instead generative adversarial network trains generator parameterized transform samples measure similarity samples {x}n training trains discriminator parameterized help. learning done playing two-player game tries distinguish aims confuse generating similar hand distinguishing distributions ﬁnite samples known two-sample test statistics. conduct two-sample test kernel maximum mean discrepancy given distributions kernel square distance deﬁned theorem given kernel characteristic kernel gmmn example characteristic kernel gaussian kernel expx− based theorem propose generative moment-matching network trains reject imply chose threshold otherwise passes test indistinguishable test. please refer details. intuitively kernel cannot result high distance chance smaller unlikely reject null hypothesis ﬁnite samples implies distinguishable therefore instead training pre-speciﬁed kernel gmmn consider training takes different possible characteristic kernels account. hand could also view replacing ﬁxed kernel adversarially learned kernel maxk∈k stronger signal train refer interested readers rigorous discussions testing power increasing distances. however difﬁcult optimize characteristic kernels solve injective function characteristic resulted kernel still characteristic. family injective functions parameterized denoted able change objective mk◦fφ paper consider case combining gaussian kernels injective functions expfφ− fφ). example function class {fφ|fφ equivalent kernel bandwidth tuning. complicated realization discussed section next abuse notation distance given composition kernel gaussian kernel following. note considers linear combination characteristic kernels also incorporated discussed composition kernels. general kernel studied discuss different distances distributions adopted existing deep learning algorithms show many discontinuous jensen-shannon divergence total variation except wasserstein distance. discontinuity makes gradient descent infeasible training. train minimizing maxφ mfφ. next show maxφ also enjoys advantage continuous differentiable objective mild assumptions. assumption locally lipschitz denote evaluation convenience. given probability distribution satisﬁes assumption local lipschitz constants independent ez∼pz theorem generator function parameterized assumption ﬁxed distribution random variable space denote distribution maxφ continuous everywhere differentiable almost everywhere parameterized feed-forward neural network satisﬁes assumption trained gradient descent well propagation since objective continuous differentiable followed theorem technical discussions shown appendix theorem {pn} sequence distributions. considering means converging mild assumption maxφ distribution theorem shows maxφ sensible cost function distance distance decreasing getting closer beneﬁts supervision improvement training. proofs omitted appendix next section introduce practical realization training optimizing minθ maxφ mfφ. approximate neural networks parameterized expressive power. assumption locally lipschitz commonly used feed-forward neural networks satisfy constraint. also gradient bounded done clipping gradient penalty non-trivial part injective. injective function exists function f−)) approximated autoencoder. following denote parameter discriminator networks consists encoder train corresponding decoder regularize objective relaxed note ignore autoencoder objective train concise presentation. note empirical study suggests autoencoder objective necessary lead successful training show section even though injective property required theorem proposed algorithm similar aims optimize neural networks minmax formulation meaning objective different. discriminator classiﬁer distinguish distributions. proposed algorithm distinguishing distribution still done two-sample test adversarially learned kernel parametrized fφe. trained pass hypothesis test. connection difference related works discussed section similarity call proposed algorithm gan. present implementation weight clipping algorithm easily extend lipschitz approximations gradient penalty encoding perspective besides using kernel selection explain proposed viewing feature transformation function kernel two-sample test performed transformed feature space optimization ﬁnding manifold stronger signals two-sample test. perspective special case identity mapping function. circumstance kernel two-sample test conducted original data space. optimal solution still equivalent solving however hard solve constrained optimization problem backpropagation. relax constraint ordinal regression parameterizing neural networks assuming −fφ∀φ recovers wasserstein treat data transform function wgan interpreted ﬁrst-order moment matching aims match inﬁnite order moments gaussian kernel form taylor expansion theoretically wasserstein distance similar theoretically guarantee theorem practice show neural networks enough capacity approximate wasserstein distance. section demonstrate matching high-order moments beneﬁts results. also propose mcgan matches second order moment primal-dual norm perspective. however proposed algorithm requires matrix decompositions exact moment matching hard scale higher order moment matching. hand giving exact moment matching match high-order moments kernel tricks. detailed discussions appendix difference works autoencoders energy-based gans also utilizes autoencoder discriminator energy model perspective minimizes reconstruction error real samples maximize reconstruction error generated samples contrast uses approximate invertible functions minimizing reconstruction errors real samples generated samples also show ebgan approximates total variation drawback discontinuity optimizes distance. line works aims match codespace utilize decoder fdec. match distribution different distribution distances generate data fdec. match generate data fdec). proposed matches generates data directly gan. similar considers kl-divergence without showing continuity weak∗ topology guarantee prove section works addition discussed works several extended works gan. proposes using linear kernel match ﬁrst moment discriminator’s latent features. considers variance empirical score training. also improves latent feature matching using kernel instead proposing adversarial training framework studied section uses wasserstein distance match distribution autoencoder loss instead data. consider extend higher order matching based proposed gan. parallel work energy distance treated different kernel. however potential problems critic. discussion referred respectively. samples images generated ﬁxed noise random vectors cherry-picked. network architecture experiments follow architecture dcgan design generator discriminator except expanding output layer dimensions. kernel designs loss function implicitly associated family characteristic kernels. similar prior seminal papers consider mixture gaussian kernel bandwidth parameter tuning kernel bandwidth optimally still remains open problem. works ﬁxed left learn kernel hyper-parameters rmsprop learning rate fair comparison wgan suggested original paper ensure boundedness model parameters discriminator clipping weights point-wisely range required assumption dimensionality latent space manually according complexity dataset. thus mnist celeba cifar- lsun bedrooms. batch size datasets. start comparing gmmn standard benchmarks mnist cifar. consider variants gmmn. ﬁrst original gmmn trains generator minimizing distance original data space. call gmmn-d. compare also pretrain autoencoder projecting data manifold autoencoder feature transformation train generator minimizing distance code space. call gmmn-c. results pictured figure gmmn-d gmmn-c able generate meaningful digits mnist simple data structure. closer look nonetheless boundary shape digits figure often irregular non-smooth. contrast sample digits figure natural smooth outline sharper strike. cifar- dataset gmmn variants fail generate meaningful images resulting level visual features. observe similar cases complex large-scale datasets celeba lsun bedrooms thus results omitted. hand proposed successfully outputs natural images sharp boundary high diversity. results figure conﬁrm success proposed adversarial learned kernels enrich statistical testing power difference gmmn gan. increase batch size gmmn image quality improved however still competitive images appendix demonstrates proposed trained efﬁciently gmmn smaller batch size. comparisons gans several representative extensions gans. consider recent state-of-art wgan based dcgan structure connection discussed section results shown figure mnist digits generated wgan figure unnatural peculiar strikes. contrary digits figure enjoy smoother contour. furthermore wgan generate diversiﬁed digits avoiding mode collapse problems appeared literature training gans. celeba difference generated samples wgan gan. speciﬁcally observe varied poses expressions genders skin colors light exposure figure closer look observe faces wgan higher chances blurry twisted faces spontaneous sharp acute outline faces. lsun dataset could distinguish salient differences samples generated wgan. quantitatively measure quality diversity generated samples compute inception score cifar- images. inception score used gans measure samples quality diversity pretrained inception model models generate collapsed samples relatively score. table lists results samples generated various unsupervised generative models trained cifar- dataset. inception scores directly derived corresponding references. although wgan generate sharp images show section score better techniques except seems conﬁrm empirically higher order moment matching real data fake sample distribution beneﬁts generating diversiﬁed sample images. also note appears compatible method combing training techniques possible avenue future work. illustrate distance correlates well quality generated samples. figure plots evolution estimate distance training mnist celeba lsun datasets. report average ˆmfφ moving average smooth graph reduce variance caused mini-batch stochastic training. observe whole training process samples generated noise vector across iterations remain similar nature. qualitative observation indicates valuable stability training process. decreasing curve improving quality images supports weak∗ topology shown theorem also plot model converges quickly. figure example converges shortly tens thousands generator iterations celeba dataset. conduct time complexity analysis respect batch size time complexity iteration wgan proposed mixture kernels. quadratic complexity introduced computing kernel matrix sometimes criticized inapplicable large batch size practice. however point several recent works ebgan also matching pairwise relation samples batch size leading complexity well. empirically environment highly parallelized matrix operation tremendously alleviated quadratic time almost linear time modest figure compares computational time generator iterations versus different titan adapted training experiments setting time iteration wgan seconds respectively. used training gmmn references time iteration becomes seconds respectively. result coheres argument empirical computational time quadratically expensive compared wgan powerful parallel computation. used weight-clipping lipschitz constraint assumption another approach obtaining discriminator similar constraints approximates wasserstein distance gradient discriminator constrained generated data points. inspired alternative approach apply gradient constraint regularizer witness function different mmd. idea ﬁrst proposed energy distance shown correspond gradient penalty witness function rkhs-based mmd. undertake preliminary investigation approach also drop requirement algorithm injective observe necessary practice. show preliminary results training gradient penalty without auto-encoder figure preliminary study indicates generate satisfactory results lipschitz constraint approximation. potential future work conducting thorough empirical comparison studies different approximations. introduce deep generative model trained adversarially learned kernels. study theoretical properties propose practical realization trained much smaller batch size gmmn competitive performances state-of-theart gans. view ﬁrst practical step forward connecting moment matching network gan. important direction applying developed tools moment matching general works based connections shown gan. also section connect wgan ﬁrst-order inﬁnite-order moment matching. shows ﬁnite-order moment matching achieves best performance domain adaption. could extend using polynomial kernels. last theory injective mapping necessary theoretical guarantees. however observe mandatory practice show section conjecture usually learns injective mapping high probability parameterizing neural networks worth study future work. thank reviewers helpful comments. also thank dougal sutherland arthur gretton valuable feedbacks pointing error previous draft. work supported part national science foundation grants iis- iis. kelvin jimmy ryan kiros kyunghyun aaron courville ruslan salakhudinov rich zemel yoshua bengio. show attend tell neural image caption generation visual attention. icml fisher seff yinda zhang shuran song thomas funkhouser jianxiong xiao. lsun construction large-scale image dataset using deep learning humans loop. arxiv preprint arxiv. dougal sutherland hsiao-yu fish tung heiko strathmann soumyajit aaditya ramdas alexander smola arthur gretton. generative models model criticism optimized maximum mean discrepancy. iclr krikamol muandet kenji fukumizu bharath sriperumbudur bernhard schölkopf. kernel mean embedding distributions review beyonds. arxiv preprint arxiv. kenji fukumizu arthur gretton gert lanckriet bernhard schölkopf bharath sriperumbudur. kernel choice classiﬁability rkhs embeddings probability distributions. nips arthur gretton dino sejdinovic heiko strathmann sivaraman balakrishnan massimiliano pontil kenji fukumizu bharath sriperumbudur. optimal kernel choice large-scale two-sample tests. nips werner zellinger thomas grubinger edwin lughofer thomas natschläger susanne saminger-platz. central moment discrepancy domain-invariant representation learning. arxiv preprint arxiv. marc bellemare danihelka dabney shakir mohamed balaji lakshminarayanan stephan hoyer rémi munos. cramer distance solution biased wasserstein gradients. arxiv preprint arxiv. combining implies maxφ locally lipschitz continuous everywhere. last applying radamacher’s theorem proves maxφ differentiable almost everywhere completes proof. proof. proof assumes scalar vector case proved sketch. first ﬂipping sign change distance. parameterized neural network linear output layer realized ﬂipping sign weights last layer. simplify theorem proof standard distance show also continuous differentiable almost everywhere. propose counterexample show discontinuity assuming however known rkhs discussed counterexample appropriate. changing function class recover several distances total variation wasserstein distance distance. discriminator different existing works explained used solve different probabilistic metrics based function class {fhk rkhs associated kernel different form many distances total variation wasserstein distance analytical representation show section analytical representation gmmn need additional network estimating distance. also provide explanation proposed adversarially learned kernel framework. distance adversarially learned kernel represented inexact moment matching second term contains quadratic ﬁrst moment. difﬁcult match high-order moments deal high order tensors directly. hand easily match high-order moments kernel tricks enjoys strong theoretical guarantee.", "year": 2017}