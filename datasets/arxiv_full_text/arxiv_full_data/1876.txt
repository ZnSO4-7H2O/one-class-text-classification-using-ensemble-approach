{"title": "Classifying medical notes into standard disease codes using Machine  Learning", "tag": ["cs.LG", "cs.CL", "stat.AP", "stat.ML"], "abstract": "We investigate the automatic classification of patient discharge notes into standard disease labels. We find that Convolutional Neural Networks with Attention outperform previous algorithms used in this task, and suggest further areas for improvement.", "text": "problem assigning codes automatically discharge summaries previously studied. among recent research publications distinguish types efforts papers trying predict codes complexity numerous focus smaller domain. full codes perotte used mimic dataset predict original codes. experimented approaches treats code independently leverages hierarchical nature codes modeling used novel evaluation metrics reﬂected distances among source tree predicted codes locations tree. found hierarchy-based classiﬁer outperformed classiﬁer. simpliﬁed codes researchers focused efforts smaller number codes found best results using convolutional neural networks gerhman relabeled clinical notes mimic using labels. cnns outperform approaches based n-gram models natural entity recognition approach paper focused improving techniques applied simpliﬁed code problem. cnns seem well suited characteristics discharge notes raise possibilities approaches. medical notes describe temporal sequence events tests cnns oblivious contrary long short-term memory models. additionally notes long average length around words. large context also explored attention models seem applied problem domain before. investigate automatic classiﬁcation patient discharge notes standard disease labels. convolutional neural networks attention outperform previous algorithms used task suggest areas improvement. electronic health records grown signiﬁcantly years include unprecedented amount variety patient information including demographics vital sign measurements laboratory test results prescriptions procedures performed digitized notes imaging reports mortality etc. usually contain structured data well unstructured data provided processed information records especially unstructured data holds promise medical insights improved medical care faster detection epidemics identiﬁcation symptoms personalized treatment detailed understanding treatment outcomes. gains automated accurate report diseases. since world health organization developed international classiﬁcation diseases monitor incidence prevalence diseases observe reimbursements resource allocation trends keep track safety quality guidelines. currently labeling done manually administrative personnel based deﬁnitions subject interpretation errors. paper focus efforts automatic labeling discharge notes mimic database codes. public database ehrs contains data points patients intensive care units including notes close admissions. mimic already broadly break approach multilabel multi-class problem steps detailed below output labeling input preprocessing training output metrics algorithms. icd- nomenclature applied mimic contains numerical codes representing possible diagnoses procedures. distinct codes used describe hospital admissions database codes appearing once. creates issue classiﬁcation algorithms since many codes would need predicted example training set. fortunately codes organized hierarchical tree figure intuitively expect second approach perform better since codes represent different realities whereas common codes related distribution less balanced. additional beneﬁt access full dataset training instead subset annotated dataset manually take common codes note preprocessing database presents multiple clinical notes categories including things like radiology nutrition pharmacy social work. here focus discharge summaries already provide synthesis different aspects. process notes relatively common steps summarize brieﬂy here words lower case remove special characters separate contractions canonize numbers tokenized resulting words. results vectorized notes reach words. since algorithms require ﬁxed length input truncate notes output length words. done without loss generality since notes meet criteria. embedding unfortunately even previous steps wording still standardized. like unpublished papers least ways write hypercholesterolemia instance. solve issue would named entity recognition implementations exist tailored medical realm apache ctakes metamap. however previous papers embeddings perform better trusting embeddings’ ability make misspellings synonyms abbreviations original word learn similar embeddings. therefore used trainable embeddings sometimes pre-trained glove algorithm wiki mimic notes account vocabulary speciﬁcity. training loss function multi-label classiﬁcations like approach convert problem single binary classiﬁcation tasks. would work icd- codes since ones assigned clinical-note independent early procedure multi-label classiﬁcation using bp-mll uses novel pairwise ranking loss function training later research found cross entropy produces better results work latter. data split training validation test sets. models implemented using tensorﬂow keras. training optimizer used adam. models using regularizations dropouts baseline linear models baseline model simply predicts common icd- codes clinical note. performs better complicated alternatives example gehrmann used -gram logistic regression relatively poor results hence implement lstm model discharge note features make better candidate. since relatevely small start single layer lstm keep number parameters low. didn’t published papers regarding classifying clinical notes using lstm however report web. raffel displayed better performance many tasks long text using attention. here seek emulate results implementing algorithms based formulas presented yang lstm attention lstm cell returns last hidden state intermediate ones sent attention layer creates vector representing clinical note output layer classiﬁcation. attention maxpooling element network replaced attention layer order create vector representing relevant information taking account values. model like mentioned hierarchical attention model implemented based yang speciﬁcally targets document classiﬁcations. levels attention mechanisms ﬁrst creates vectors represent sentence using attention mechanism across words; second level creates vector represent document using attention mechanisms across sentences. yang uses bidirectional grus lstms fair comparision lstm models. threshold calibration resulting vector neural network interpreted probability individual icd- codes complete prediction must convert vector binary values. several methods selecting threshold used constant threshold maximizing overall f-score. future work could explore methods building model intermediate vector. performance metrics metric validation data evaluate performance models compare results previous work classifying mimic clinical notes text classiﬁcation general. results discussion comparing previous work order compare performance results model built gehrmann took consideration dataset size number classes. multi-label classiﬁcations sklearn offers several options used ’micro’ option calculates global counts true positives false negatives false positives. testing lstm attention improve initial result experiments different models identify promising. experiments notes ﬁrst level icd- codes using epochs. results presented table tested lstms without attention mechanisms without attention mechanisms hierarchical lstm model attention layers. results table models perform better lstm classifying mimic medical notes. also signiﬁcant improvement scores applying attention mechanism lstm models. lstm attention model outperforms standard lstm attention model outperforms standard model hand hierarchical lstm attention mechanisms small increase performance results regards flat lstm attention. smaller expected based similar classiﬁcation tasks yang difference reported larger datasets. model twice number parameters models would impact performance relatevely small ﬁles like using could reason small improvement score. also tried grus instead lstms compare yang results difference still same. another possible reason hierarchical model performing much better tokenization sentences. model bases predictions results sentence sentences identiﬁed correctly ﬁrst place rest model perform well. inspect suspicious long sentences incorrect reports. would inspect closely sentence tokenization process work. models could seen hierarchical convolutional sliding windows create segments document collapsed vectors representing higher level abstraction. sense ﬁnding best segments document regardless sentences separations. explain models getting better performance hierarchical models. anticipated earlier plain model executed records f-score outperforming model previous work larger dataset better separated labels iii) imbalanced label distribution however stage model still overﬁts even though highest score experimental runs records epochs didn’t reach best f-score running full data set. work would explore hyperparameters tuning evaluating number parameters attempt undoing ﬁtting situation. believe models still improved inspecting detail cases model predicted false positive false negative working hyper-parameters. would ﬁrst tasks work regarding models. using wiki glove pre-trained embeddings minor decrease performance compared empty embedding matrix could expected since medical clinical notes vocabulary differs wiki pages. fact half vocabulary found wiki glove pretrained embeddings. part future work think using pretrained embeddings millions clinical notes would improve performance models processing clinical notes example type work wenpeng hinrich schutze bing xiang bowen zhou abcnn attention-based convolutional neural network modeling sentence pairs. carnegie mellon university microsoft research redmond. johnson pollard shen lehman feng ghassemi moody szolovits celi mark mimic-iii freely accessible critical care database. scientiﬁc data ./sdata... references sebastian gehrmann franck dernoncourt yeran eric carlson jonathan welt john foote edward moseley david grant patrick tyler anthony celi. comparison rule-based deep learning models patient phenotyping. critical data laboratory computational physiology harvard john paulson school engineering applied sciences massachusetts institute technology harvard t.h. chan school public health philips research north america beth israel deaconess medical center massachusetts general hospital tufts university school medicine university massachusetts washington university school medicine adler perotte rimma pivovarov karthik natarajan nicole weiskopf frank wood nomie elhadad. diagnosis code assignment models evaluation metrics. department biomedical informatics columbia; university york york usa; newyork presbyterian hospital york;department engineering university oxford oxford m.l. zhang z.h. zhou. multi-label neural networks applications functional genomics text categorization. ieee transactions knowledge data engineering zichao yang diyi yang chris dyer xiaodong alex smola eduard hovy hierarchical attention networks document classiﬁcation. carnegie mellon university microsoft research redmond.", "year": 2018}