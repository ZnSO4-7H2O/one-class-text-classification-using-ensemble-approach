{"title": "From Virtual to Real World Visual Perception using Domain Adaptation --  The DPM as Example", "tag": ["cs.CV", "cs.AI"], "abstract": "Supervised learning tends to produce more accurate classifiers than unsupervised learning in general. This implies that training data is preferred with annotations. When addressing visual perception challenges, such as localizing certain object classes within an image, the learning of the involved classifiers turns out to be a practical bottleneck. The reason is that, at least, we have to frame object examples with bounding boxes in thousands of images. A priori, the more complex the model is regarding its number of parameters, the more annotated examples are required. This annotation task is performed by human oracles, which ends up in inaccuracies and errors in the annotations (aka ground truth) since the task is inherently very cumbersome and sometimes ambiguous. As an alternative we have pioneered the use of virtual worlds for collecting such annotations automatically and with high precision. However, since the models learned with virtual data must operate in the real world, we still need to perform domain adaptation (DA). In this chapter we revisit the DA of a deformable part-based model (DPM) as an exemplifying case of virtual- to-real-world DA. As a use case, we address the challenge of vehicle detection for driver assistance, using different publicly available virtual-world data. While doing so, we investigate questions such as: how does the domain gap behave due to virtual-vs-real data with respect to dominant object appearance per domain, as well as the role of photo-realism in the virtual world.", "text": "invited book chapter appear domain adaptation computer vision applications springer series advances computer vision pattern recognition edited gabriela csurka. written summer supervised learning tends produce accurate classiﬁers unsupervised learning general. implies training data preferred annotations. addressing visual perception challenges localizing certain object classes within image learning involved classiﬁers turns practical bottleneck. reason that least frame object examples bounding boxes thousands images. priori complex model regarding number parameters annotated examples required. annotation task performed human oracles ends inaccuracies errors annotations since task inherently cumbersome sometimes ambiguous. alternative pioneered virtual worlds collecting annotations automatically high precision. however since models learned virtual data must operate real world still need perform domain adaptation chapter revisit deformable part-based model exemplifying case virtualreal-world case address challenge vehicle detection driver assistance using diﬀerent publicly available virtualworld data. investigate questions domain behave virtual-vs-real data respect dominant object appearance domain well role photorealism virtual world. since machine learning essential tool solving computer vision tasks image classiﬁcation object detection instance recognition semantic segmentation among others general terms best performing machine learning algorithms tasks supervised; words data required also annotated information i.e. ground truth must provided training invited book chapter appear domain adaptation computer vision applications springer series advances computer vision pattern recognition edited gabriela csurka. written summer protocol. collecting annotations based human oracles collaborative software tools amazon’s mechanical turk labelme etc. known human-based annotation cumbersome task ambiguities inaccuracies. moreover kinds ground truth actually collected relying human annotators e.g. pixel-wise optical depth. non-expert reader feeling annotation eﬀort looking fig. typical annotation tasks namely bounding based object annotations delineation semantic contours classes interest. former case develop object detector latter develop pixel-wise multi-class classiﬁer i.e. perform so-called semantic segmentation image. century diﬀerent datasets created ground truth publicly available research. providing comprehensive list scope chapter cite meaningful pioneering examples related particular tasks worked actively namely pedestrian detection semantic segmentation; road scenarios either advanced driver assistance systems autonomous driving example daimler pedestrian dataset includeds bb-annotated pedestrians pedestrian-free images training images bbannotated pedestrians testing. another example corresponds pixel-wise class ground truth provided urban scenarios; giving rise wellknown camvid dataset considers semantic classes includes annotated images normally used training testing. years after kitti vision benchmark suite enormous contribution research focused adas/ad given high variability provided synchronized data ground truth human-designed features machine learning pipelines image search schemes etc.) solving computer vision tasks deep leaning ﬁnding become powerful tool today solving tasks. many researchers would point main breakthrough since deep convolutional neural networks showed astonishing performance data used imagenet large-scale visual recognition challenge imagenet contains million human-labeled high-resolution images roughly categories. thus imagenet gigantic human annotation eﬀort. ilsvrc uses subset imagenet images categories; overall images training validation testing. many deep cnns developed today rely imagenet pre-trained deep modiﬁed ﬁne-tuned solve task operate domain. research community agrees fact that addition powerful hardware train test deep cnns large dataset ground truth imagenet success. line recently released coco dataset per-instance object segmentation provided object types images total labeled instances. matter fact ﬁeld adas/ad would like datasets least variety information sources kitti ground truth size imagenet/coco. however looking ground truth kitti quantitative terms individually order magnitude adas/ad-oriented publicly available datasets proof need recently released cityscapes dataset tries beyond kitti several aspects. instance includes pixel-wise annotated images covering classes per-instance distinction odometry ambient temperature metadata. addition includes images annotations coarser regarding delineation instance/class contours. kind invited book chapter appear domain adaptation computer vision applications springer series advances computer vision pattern recognition edited gabriela csurka. written summer figure ground truth obtained human annotation left) framing rectangular bounding vehicle instances; right) delineating contours diﬀerent classes interest contained image even instance level. dataset diﬃcult collect since driving cities covering several months weather conditions required. moreover providing ground truth take minutes image human oracle case ﬁne-grained annotations depending image content. semantic segmentation task cityscapes goes order magnitude beyond kitti camvid. however annotation numbers imagenet coco. main reason two-fold. hand data collection itself i.e. cityscapes images collected on-board systems designed adas/ad downloaded internet source; moreover metadata vehicle odometry important mention possibility obtaining depth stereo. hand annotations must precise since ultimately main focus adas/ad reducing traﬃc accidents. case mentioned before interesting ground truth types possible really diﬃcult obtain human annotation e.g. pixel-wise optical depth eventually important cues adas/ad based visual perception. adas/ad context diﬃculties relevance large amounts data ground truth training debugging testing roughly since started explore diﬀerent approach. particular idea using realistic virtual worlds training vision-based perception modules. advantages clear forcing driving data acquisition situations needed; obtaining diﬀerent types pixel-wise ground truth generating data relatively fast generate images hour ground truths using standard consumer hardware); etc. course proposal also came doubts visual model learned virtual worlds operate well real-world environments? depend degree photo-realism?. pioneering paper used pedestrian detection based hog/linear-svm proof-of-concept last work i.e. synthia addressed pixel-wise semantic segmentation deep cnns continuously exploring idea learning virtual worlds operate real environments. synthetic data attracted attention researchers recently specially massive adoption deep cnns perform computer vision tasks data hungry nature. models used train visual models pose estimation object detection recognition indoor scene understanding virtual racing circuit used generating diﬀerent types pixel-wise ground truth videogames used traininvited book chapter appear domain adaptation computer vision applications springer series advances computer vision pattern recognition edited gabriela csurka. written summer deep cnns purpose semantic segmentation depth estimation synthetic scenarios used also evaluating performance diﬀerent feature descriptors training testing optical and/or scene computation methods stereo algorithms well trackers even using synthetic clones real-world areas interest synthetic lidar-style data used object detection ﬁnally virtual worlds used learning high-level artiﬁcial behavior playing atari games reproducing human behavior playing shooter games driving/navigating end-to-end even learning unwritten common sense beginning work clear domain virtual real worlds. however also clear case using images coming diﬀerent camera sensors environments. words domain virtual-to-real issue rather general sensor-to-sensor environment-toenvironment problem researchers conﬁrmed fact addressing related different visual tasks since then training visual models virtual worlds applying domain adaptation techniques real-world scenarios come hand-by-hand fact authors followed approach performing explicit step virtualreal-world domain adaptation without exhaustive list reader address illustrative examples. showed virtualreal-world domain adaptation possible holistic models based hog+lpb/linear-svm paradigm well haar+eoh/adaboost former case proof-of-concept experiments adapting rgb-style synthetic images infrared ones reported positive results moreover deformable part-based model also proposed virtual worlds domain adaptation cases focused supervised domain adaptation i.e. relatively amount annotated target-domain data used adapt model learned source-domain data holistic models focused mixing source target data collected active learning model adaptation termed corresponding feature space cool world; focused using source-domain model together target-domain data i.e. without revisiting source-domain data. terms modern deep cnns former case would similar mixing source target data mini-batches latter case spirit socalled ﬁne-tuning. rest chapter going focus state-of-the-art object detection breakthrough deep cnns. priori good proxy deep cnns regarding speciﬁc experiments want address deep cnns eventually require domain adaptation obviously based hogstyle features point much data would really translate better accuracy keep training data order thousands here. hand note reformulated deep end-toend learning. moreover domain adaptation techniques proposed used core technology hierarchical domain adaptation well weakly supervised incremental domain adaptation particular going rely domain adaptation method termed structureaware adaptive structural gave best performance chapter compliment experiments mainly addressing questions role photo-realism virtual world well domain behave virtual-vs-real data respect dominant object appearance domain. moreover sake analyzing cases instead focusing pedestrian detection using virtual data invited book chapter appear domain adaptation computer vision applications springer series advances computer vision pattern recognition edited gabriela csurka. written summer learned. domain adaptation context term model learned source-domain data sa-ssvm domain adaptation method takes relatively annotated target-domain data learn model expected perform better target domain. reader referred mathematical technical details sa-ssvm works. however explain idea support example fig. consists components half body full body well persons seen diﬀerent viewpoints. component consists root parts adapt target domain decompose stands transpose note component contain appearance deformation parameters decomposed model parameters adapted target doeter control relative penalty. finally standard terms objective function number target-domain samples used adaptation. optimizing objective function split sets actually training testing. testing used purpose follow so-called moderate encodes appearance objects’ constituent parts together holistic object representation termed root. contrast part models allows parts located diﬀerent positions respect root. plausible relative locations known deformations also encoded. appearance deformations learned. appearance parts learned double resolution root. triplet rootparts-deformations known component. order avoid blurred models allows learn mixture components. diﬀerent components correspond diﬀerent object views poses specially implies diﬀerent aspect ratios corresponding root fig. pictorial intuition. invited book chapter appear domain adaptation computer vision applications springer series advances computer vision pattern recognition edited gabriela csurka. written summer invited book chapter appear domain adaptation computer vision applications springer series advances computer vision pattern recognition edited gabriela csurka. written summer setting considering vehicles mandatory detect. training consider four datasets addition mentioned split kitti detection challenge thus total. namely kitti tracking dataset synthesized clone virtual kitti synthia course synthia dataset diﬀerent types ground truth selected cars images experiments. case semi-automatically annotated cars. table shows number samples dataset. figures show images sampled kitti-det kitti track virtual kitti synthia respectively. virtual kitti synthia based development framework i.e. unityd. images photo-realistic ones used synthia virtual kitti. synthia images always corresponding forward facing on-board virtual camera case virtual kitti gta. details datasets reader refer corresponding papers. order show accuracy vehicle detectors plot curves false positive image miss rate according caltech protocol overlap detection ground truth bbs. training testing consider moderate cases according deﬁnition given kitti detection challenge vehicles non-occluded partially occluded regarding application sa-ssvm followed settings reported producing best results. namely adapted structures correspond root parts i.e. components; since domain adaptation experiments require random sampling target domain training data three times mean fppi-mr curve standarddeviation based intervals plotted three repetitions instead ﬁve). test. kitti-det train kitti-det test coming domain since correspond splits done original dataset. learned detectors tested kitti-det test diﬀerence among data used training. accordingly reported experiments follows invited book chapter appear domain adaptation computer vision applications springer series advances computer vision pattern recognition edited gabriela csurka. written summer table used samples dataset. images stands number images them vehicles stands number annotated vehicles using bounding box. negative samples selected background areas images. kitti-det test kitti-det train refer splits training kitti detection training set. kitti-det test testing used experiments chapter rest datasets used training. kitti track virtual kitti sequences training datasets. synthia-sub refers subset randomly sampled synthia. invited book chapter appear domain adaptation computer vision applications springer series advances computer vision pattern recognition edited gabriela csurka. written summer worst case src∈ {kitti-track virtual kitti} since average miss rate src∈ {synthia gta} points. note related number vehicle samples since virtual kitti virtual kitti turn contains vehicles kitti-track basically equal. since virtual kitti synthesized clone kitti-track think main reason accuracy drop virtual-to-real nature training images typical vehicle poses backgrounds reﬂected training datasets i.e. comparing virtual kitti/kitti-track kitti-det train. words kitti-det train represents better kitti-det test since built data set. note addition kitti-track come camera sensor kitti-det train test avoid accuracy gap. moreover synthia come virtual worlds still produce detector performs better using kitti-track. invited book chapter appear domain adaptation computer vision applications springer series advances computer vision pattern recognition edited gabriela csurka. written summer heuristics tried alleviate manual annotation eﬀort. however observe pre-training automatically collected virtual-word data using sa-ssvm adapting model figure compares vehicle detections based synthia samples using chapter result applying sa-ssvm tar. cases setting threshold model classiﬁer operate fppi= regime. note sa-ssvm allows obtain better results. tar. tar-all baages annotated without reward performance. course although annotationtraining-test loop followed avoid useless vehicle annotations priori diﬃcult know stop manual annotations. hand even using tar. data starting preinvited book chapter appear domain adaptation computer vision applications springer series advances computer vision pattern recognition edited gabriela csurka. written summer invited book chapter appear domain adaptation computer vision applications springer series advances computer vision pattern recognition edited gabriela csurka. written summer invited book chapter appear domain adaptation computer vision applications springer series advances computer vision pattern recognition edited gabriela csurka. written summer invited book chapter appear domain adaptation computer vision applications springer series advances computer vision pattern recognition edited gabriela csurka. written summer invited book chapter appear domain adaptation computer vision applications springer series advances computer vision pattern recognition edited gabriela csurka. written summer figure vehicle detections operating fppi= regime. left based synthia data considered chapter middle using tar. version kitti-det train. right adapting using tar. applying sa-ssvm. invited book chapter appear domain adaptation computer vision applications springer series advances computer vision pattern recognition edited gabriela csurka. written summer tar.. looking synthia-sub similar number samples argue could probably reach performance synthia would double vehicles. case remarkable eﬀective dpms pre-trained virtual worlds doubling number manually annotated target-domain images i.e. least assuming manual annotation procedure free prior knowledge current vehicle detector. even i.e. combining data used train tar-all pre-training virtual worlds able improve performance tar-all alone. points improvement respect tar-all being overall best result. using eventually provide improvement summary results presented discussed reinforce take home messages highlighted previous works namely combining models/data pre-trained virtual worlds reasonable amount real-world data domain adaptation really practical paradigm worth explore learning diﬀerent kinds models. matter fact according literature reviewed sections nowadays approach widely adopted computer vision community. another interesting question addressed refers degree photo-realism i.e. higher degree would imply learn accurate models eventually even requiring domain adaptation. important question since extreme photo-realism require hours rendering images degree photo-realism virtual worlds presented achieved real time using standard modern gamer sensors operates real virtual worlds) nature scenario train test images acquired this believe photo-realistic world would another sensor still diﬀerent real-world therefore domain gaps would persists. note experiments presented chapter reinforce hypothesis using virtual kitti kittitrack gives rise domain-adapted detectors similar performance cases i.e. despite fact kitti-track relies real-world sensor kitti-det train test virtual kitti consists synthesized data; moreover despite fact contains images photo-realistic synthia using similar number samples performance corresponding domain-adapted detectors basically same. recent works reinforce idea that basic photo-realism achieved adding photo-realism relevant impact. thus opinion evidences collected virtual kitti synthia suﬃciently photo-realistic tasks addressing another interesting point analysis better mixing virtualreal-world data ﬁne-tuning pre-trained model virtual-world data real-world samples. former called cool world sa-ssvm example later. experiment synthia tar-all seen fig. case mixed data standard learning using sa-ssvm. moreover training time much longer sa-ssvm since uses samples domains training scratch also requires iterations converge. extrapolate experiments deep cnns paradigm priori would think ﬁne-tuning proper approach. however working i.e. semantic segmentation based deep invited book chapter appear domain adaptation computer vision applications springer series advances computer vision pattern recognition edited gabriela csurka. written summer cnns using appropriate mini-batch scheme weight relevance samples function domain obtained better performance ﬁne-tuning. therefore regarding topic clear conclusions yet. course advantage ﬁne-tuning would avoiding revisit source data; thus point keep researching. overall research research presented computer vision community insist adoption paradigm virtual worlds domain adaptation techniques used train desired models. moreover think degree photo-realism like presented already datasets virtual kitti synthia suﬃcient task. addition although chapter focused dpm-based vehicle detection think conclusions extrapolated computer vision tasks visual appearance important course worth note moment best results kitti detection challenge dominated approaches based deep cnns providing astonishing high performances moderate setting beyond approaches. benchmark seems challenging enough still small proportion real-world real challenge deep cnns. therefore also think conclusions extrapolated powerful models deep cnns addressing challenging scenarios; note sect. mentioned already even deep cnns require domain adaptation. hand expected deep cnns would require less domain adaptation since models capacity generalize across domains. proof-of-concept believe conclusions extrapolate visual tasks based complex models deep cnns. presented results suggest extreme photo-realism necessary i.e. degree photo-realism already achieved datasets virtual kitti synthia suﬃcient provided domain adaptation would necessary even relying photorealistic datasets looking future think best practice would design sets relatively controlled virtual-world scenarios designed train debug test visual perception capabilities words knowledge accumulated building gigantic virtual world avoid domain issues. would really diﬃcult build handle. prefer pursue domain adaptation save existing virtual-to-real world gap. however think research must direction unsupervised domain adaptation allowing systems trained virtual worlds self-adapt real-world scenarios. example line approach presented manual annotations required train domain adapted pedestrian detector on-board moving camera setting. however approach performs adaptation oﬀ-line perfectly right many applications real challenge on-line. acknowledgments authors want thank next funding bodies spanish project tra-c--r people programme fp/- grant agreement agency competitiveness companies government catalonia accio generalitat catalunya project -sgr- nvidia corporation generous support form diﬀerent hardware units.", "year": 2016}