{"title": "Deep Probabilistic Programming", "tag": ["stat.ML", "cs.AI", "cs.LG", "cs.PL", "stat.CO"], "abstract": "We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations---random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.", "text": "propose edward turing-complete probabilistic programming language. edward deﬁnes compositional representations—random variables inference. treating inference ﬁrst class citizen modeling show probabilistic programming ﬂexible computationally efﬁcient traditional deep learning. ﬂexibility edward makes easy model using variety composable inference methods ranging point estimation variational inference mcmc. addition edward reuse modeling representation part inference facilitating design rich variational models generative adversarial networks. efﬁciency edward integrated tensorflow providing signiﬁcant speedups existing probabilistic systems. example show benchmark logistic regression task edward least faster stan faster pymc. further edward incurs runtime overhead fast handwritten tensorflow. paper design compositional representations probabilistic programming. probabilistic programming lets users specify generative probabilistic models programs compile models inference procedures. probabilistic models also compositional nature much work enabled rich probabilistic programs compositions random variables less work however considered analogous compositionality inference. rather many existing probabilistic programming languages treat inference engine black abstracted away model. cannot capture probabilistic inferences reuse model’s representation—a idea recent advances variational inference generative adversarial networks also classic inferences propose edward turing-complete probabilistic programming language builds compositional representations—one random variables inference. treating inference ﬁrst class citizen modeling show probabilistic programming ﬂexible computationally efﬁcient traditional deep learning. ﬂexibility show edward makes easy model using variety composable inference methods ranging point estimation variational inference mcmc. efﬁciency show integrate edward existing computational graph frameworks tensorflow frameworks like tensorflow provide computational beneﬁts like distributed training parallelism vectorization support free. example show benchmark task edward’s hamiltonian monte carlo many times faster existing software. further edward incurs runtime overhead fast handwritten tensorflow. probabilistic programming languages typically trade expressiveness language computational efﬁciency inference. side languages emphasize expressiveness representing rich class beyond graphical models. employs generic inference engine scales poorly respect model data size. side languages emphasize efﬁciency restricted speciﬁc class models inference algorithms optimized efﬁcient class. example infer.net enables fast message passing graphical models augur enables data parallelism gpus gibbs sampling bayesian networks edward bridges gap. turing complete—it supports computable probability distribution—and supports efﬁcient algorithms leverage model structure scale massive data. prior research efﬁcient algorithms turing-complete languages. venture anglican design inference collection local inference problems deﬁned program fragments produces fast program-speciﬁc inference code build neither system supports inference methods programmable posterior approximations inference models data subsampling. concurrent work webppl features amortized inference unlike edward webppl reuse model’s representation; rather annotates original program leverages helper functions less ﬂexible strategy. finally inference designed program transformations kiselyov ´scibior zinkov enables ﬂexibility composing inference inside probabilistic programs. edward builds idea compose inference within modeling also modeling within inference ﬁrst develop compositional representations probabilistic models. desire criteria integration computational graphs efﬁcient framework nodes represent operations data edges represent data communicated invariance representation graph representation reused inference. edward deﬁnes random variables compositional representation. class objects methods example compute density sample. further random variable associated tensor represents single sample association embeds random variable onto computational graph tensors. design’s simplicity makes easy develop probabilistic programs computational graph framework. importantly computation represented graph. enables compose random variables complex deterministic structure deep neural networks diverse math operations third party libraries build framework. design also enables compositions random variables capture complex stochastic structure. bernoulli latent probability shared across data points random variable -dimensional parameterized random tensor fetching object runs graph simulates generative process outputs binary vector elements. computation registered symbolically random variables execution. symbolic representations require reifying full model leads unreasonable memory figure variational auto-encoder data pixel images graphical model dotted lines inference model; probabilistic program -layer neural networks. computational graphs also natural build mutable states within probabilistic program. typical computational graphs states deﬁne model parameters; tensorflow given tf.variable. another case building discriminative models features input training test data. program written independent data using mutable state graph. training testing feed placeholder appropriate values. appendix provide examples bayesian neural network classiﬁcation latent dirichlet allocation gaussian matrix factorization present others below. figure implements variational auto-encoder edward. comprises probabilistic model data variational model designed approximate former’s posterior. random variables construct probabilistic model variational model; inference data points latent variables program uses keras deﬁne neural networks. probabilistic model parameterized -layer neural network hidden units generates pixel images. variational model parameterized -layer inference network hidden units outputs parameters normal posterior approximation. probabilistic program concise. core elements vae—such distributional assumptions neural architectures—are extensible. model compositionality embed complicated models learning tasks inference compositionality embed complicated algorithms expressive variational approximations alternative objectives random variables also composed control operations. example figure implements bayesian recurrent neural network variable length. data sequence inputs outputs length time step. applies update previous hidden state feed hidden state output’s likelihood normal place standard normal prior parameters rh×h rd×h implementation dynamic differs ﬁxed length pads unrolls computation. random variables also placed control itself enabling probabilistic programs stochastic control ﬂow. stochastic control deﬁnes dynamic conditional dependencies known literature contingent existential dependencies figure depend given execution. appendix stochastic control implement dirichlet process mixture model. tensors stochastic shape also possible example tf.zeros) deﬁnes vector zeros length given poisson draw rate stochastic control produces difﬁculties algorithms graph structure relationship conditional dependencies changes across execution traces. computational graph however provides elegant teasing static conditional dependence structure dynamic dependence structure perform model parallelism static structure gpus batch training. generic computations handle dynamic structure. described random variables representation building rich probabilistic programs computational graphs. describe compositional representation inference. desire criteria support many classes inference form inferred posterior depends algorithm; invariance inference computational graph posterior composed part another model. explain approach simple hierarchical model running example. figure displays joint distribution data local variables global variables ideas extend expressive programs. inference abstract class takes inputs. ﬁrst collection latent random variables beta associated posterior variables qbeta respectively. second collection observed random variables associated realizations x_train. class methods available ﬁnely control inference. calling inference.initialize builds computational graph update calling inference.update runs computation update call method loop convergence. importantly efﬁciency lost edward’s language computational graph handwritten speciﬁc model. means runtime same; also experiments section concept edward distinct model inference block. model simply collection random variables inference modifying parameters collection subject another. reductionism offers signiﬁcant ﬂexibility. example infer parts model infer parts used multiple models plug posterior model design inference general. describe subclasses represent many algorithms below variational inference monte carlo generative adversarial networks. variational inference posits family approximating distributions ﬁnds closest member family posterior edward build variational family graph; figure running example family mutable variables parameters normal categorical. speciﬁc variational algorithms inherit variationalinference class. deﬁnes methods loss function gradient. example represent maximum posteriori estimation approximating family pointmass random variables i.e. probability mass concentrated point. inherits variationalinference deﬁnes negative joint density loss function; uses existing optimizers inside tensorflow. section experiment multiple gradient estimators black variational inference estimator implements loss different update rule monte carlo approximates posterior using samples monte carlo inference approximating family empirical distribution parameters figure monte carlo algorithms proceed updating sample time empirical approximation. speciﬁc samplers determine update rules gradients hamiltonian monte carlo graph structure sequential monte carlo edward also supports non-bayesian methods generative adversarial networks figure model posits random noise data points dimensions; random noise feeds generative_network function neural network outputs real-valued data addition discriminative_network takes data input outputs probability data real build ganinference; running optimizes parameters inside neural network functions. approach extends many advances gans finally design algorithms would otherwise require tedious algebraic manipulation. symbolic algebra nodes computational graph uncover conjugacy relationships random variables. users integrate variables automatically derive classical gibbs mean-ﬁeld updates exact inference. algorithms currently developed edward. core edward’s design inference written collection separate inference programs. demonstrate variational e-step local variables m-step global variables. instantiate algorithms conditions inferences other alternate update extends many cases exact exponential families contrastive divergence pseudo-marginal methods gibbs sampling within variational inference also write message passing algorithms solve collection local inference problems example classical message passing uses exact local inference expectation propagation locally minimizes kullback-leibler divergence stochastic optimization scales inference massive data algorithms stochastic gradient langevin dynamics stochastic variational inference idea cheaply estimate model’s joint density unbiased way. step subsamples data {xm} size scales densities respect local variables support stochastic optimization represent subgraph full model. prevents reifying full model lead unreasonable memory consumption initialization pass dictionary properly scale arguments. figure conceptually scale argument represents scaling random variable’s plate seen random variable many times. example appendix shows implement stochastic variational inference edward. approach extends naturally streaming data dynamic batch sizes data structures working subgraph immediately apply section illustrate main beneﬁts edward ﬂexibility efﬁciency. former show easy compare different inference algorithms model. latter show easy signiﬁcant speedups exploiting computational graphs. without analytic analytic entropy score function gradient normalizing ﬂows hierarchical variational model importance-weighted auto-encoders iwae objective rényi divergence table inference methods probabilistic decoder binarized mnist. edward convenient research platform making easy develop experiment many algorithms. latent variables data point optimize using adam. study different components setup using different methods; appendix complete script. training evaluate held-out likelihoods lower bounds true value. table shows results. ﬁrst method uses figure next three methods apply different gradient estimators reparameterization gradient without analytic reparameterization gradient analytic entropy; score function gradient typically leads optima different convergence rates. score function gradient slowest. gradients analytic entropy produced difﬁculties around convergence switched stochastic estimates entropy approached optima. also hierarchical variational models normalizing prior; produced similar results normalizing latent variable space better importance-weighted auto-encoders also study novel combinations hvms iwae objective gan-based optimization decoder rényi divergence decoder. gan-based optimization enable calculation log-likelihood; rényi divergence directly optimize log-likelihood perform well. point edward convenient research platform easy modiﬁcations given script. benchmark runtimes ﬁxed number hamiltonian monte carlo iterations modern hardware -core intel .ghz nvidia titan gpu. apply logistic regression covertype dataset using edward stan pymc iterations leapfrog updates iteration step size single precision. figure illustrates program edward. previous version paper reported pymc took caused preventing pymc correctly handling single-precision ﬂoating point. predict porting edward’s design theano would feature similar speedups. addition speedups highlight edward runtime overhead fast handwritten tensorflow. following section computational graphs inference fact edward handwritten code. addition edward also release probability community repository pre-trained probability models posteriors. inspired model caffe provides many pre-trained discriminative neural networks making large-scale deep learning transparent accessible. also inspired forest provides examples probabilistic programs. described edward turing-complete compositional representations probabilistic models inference. edward expands scope probabilistic programming ﬂexible computationally efﬁcient traditional deep learning. ﬂexibility showed edward variety composable inference methods capture recent advances variational inference generative adversarial networks ﬁnely control inference algorithms. efﬁciency showed edward leverages computational graphs achieve fast parallelizable computation scales massive data incurs runtime overhead handwritten code. present work applying edward research platform developing probabilistic models inference algorithms language design edward makes tradeoffs pursuit ﬂexibility speed research. example open challenge edward better facilitate programs complex control recursion. possible represent unknown enable ﬂexible inference strategies. addition open expand edward’s design dynamic computational graph frameworks—which provide ﬂexibility programming paradigm—but sacriﬁce performance. crucial next step probabilistic programming leverage dynamic computational graphs maintaining ﬂexibility efﬁciency edward offers. thank probabilistic programming community—for sharing enthusiasm motivating work—including developers church venture gamalon hakaru webppl. also thank stan developers providing extensive feedback developed language well thomas wiecki experimental details. thank google bayesflow team—joshua dillon langmore ryan sepassi srinivas vasudevan—as well ahmed matthew johnson hung rajesh ranganath maja rudolph francisco ruiz helpful feedback. work supported iis- n--- darpa fa--- darpa n--c- adobe google nserc pgs-d sloan foundation. martín abadi paul barham jianmin chen zhifeng chen andy davis jeffrey dean matthieu devin sanjay ghemawat geoffrey irving michael isard manjunath kudlur josh levenberg rajat monga sherry moore derek murray benoit steiner paul tucker vijay vasudevan pete warden martin wicke yuan xiaoqiang zhang. tensorflow system largescale machine learning. arxiv preprint arxiv. carpenter andrew gelman matthew hoffman daniel goodrich michael betancourt marcus brubaker jiqiang peter allen riddell. stan probabilistic programming language. journal statistical software goodfellow jean pouget-abadie mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. neural information processing systems karol gregor danihelka alex graves danilo rezende daan wierstra. draw recurrent neural network image generation. international conference machine learning yangqing evan shelhamer jeff donahue sergey karayev jonathan long ross girshick sergio guadarrama trevor darrell. caffe convolutional architecture fast feature embedding. arxiv preprint arxiv. danilo rezende shakir mohamed daan wierstra. stochastic backpropagation approximate inference deep generative models. international conference machine learning danilo jimenez rezende shakir mohamed danihelka karol gregor daan wierstra. oneshot generalization deep generative models. international conference machine learning dustin tran kucukelbir adji dieng maja rudolph dawen liang david blei. edward library probabilistic modeling inference criticism. arxiv preprint arxiv. jean-baptiste tristan daniel huang joseph tassarotti adam pocock stephen green steele. augur data-parallel probabilistic modeling. neural information processing systems many examples available http//edwardlib.org including models inference methods complete scripts. describe several model examples; appendix describes inference example appendix describes complete scripts. examples paper comprehensive leaving import statements ﬁxed values. companion webpage paper examples machine-readable format runnable code. -layer neural network whose weights biases form latent variables deﬁne prior weights biases standard normal. figure data points features hidden units. figure note program written illustration. recommend vectorization practice instead storing scalar random variables lists lists prefer represent random variables many dimensions. shape dirichletprocess random variable returns sample_n=n draws shape given base distribution normal. essential component deﬁning dirichletprocess random variable stochastic loop. deﬁne below. edward’s code base involved version base distribution. subgraph setting data subsampling working subgraph full model. setting necessary data model memory. scalable algorithm’s computational complexity memory complexity independent data size. klqp variational method minimizes divergence measure instantiate algorithms global inference given subset local inference subset given also pass tensorflow placeholder x_ph data change data step. iteration also reinitialize parameters inference local variational factors batch. demo readily applies inference algorithms sgld simply replace qbeta empirical random variables; call ed.sgld instead ed.klqp. note data model memory you’d still like perform data subsampling fast inference recommend deﬁning subgraphs. reify full model simply index local variables placeholder. placeholder runtime determine local variables update time. mnist input_data.read_data_sets data x_ph} inference ed.klqp optimizer tf.train.rmspropoptimizer inference.initialize tf.initialize_all_variables.run n_epoch n_iter_per_epoch range figure example uses data subsampling priors conditional likelihoods deﬁned minibatch data. similarly variational model models embeddings used given minibatch. tensorflow variables contain embedding vectors entire vocabulary. tensorflow placeholders ensure correct embedding vectors used variational parameters given minibatch. bernoulli variables y_pos y_neg ﬁxed respectively. model whether word indeed target word given context window drawn negative sample. without regularization objective optimize identical negative sampling.", "year": 2017}