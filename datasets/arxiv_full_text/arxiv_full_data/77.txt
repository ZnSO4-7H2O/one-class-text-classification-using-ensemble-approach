{"title": "Embracing data abundance: BookTest Dataset for Reading Comprehension", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "abstract": "There is a practically unlimited amount of natural language data available. Still, recent work in text comprehension has focused on datasets which are small relative to current computing possibilities. This article is making a case for the community to move to larger data and as a step in that direction it is proposing the BookTest, a new dataset similar to the popular Children's Book Test (CBT), however more than 60 times larger. We show that training on the new data improves the accuracy of our Attention-Sum Reader model on the original CBT test data by a much larger margin than many recent attempts to improve the model architecture. On one version of the dataset our ensemble even exceeds the human baseline provided by Facebook. We then show in our own human study that there is still space for further improvement.", "text": "practically unlimited amount natural language data available. still recent work text comprehension focused datasets small relative current computing possibilities. article making case community move larger data step direction proposing booktest dataset similar popular children’s book test however times larger. show training data improves accuracy attention-sum reader model original test data much larger margin many recent attempts improve model architecture. version dataset ensemble even exceeds human baseline provided facebook. show human study still space improvement. since humans amass generally available data form unstructured text would useful teach machines read comprehend data understanding answer questions. signiﬁcant amount research recently focused answering particular kind questions answer depends understanding context document. cloze-style questions require reader missing word sentence. important advantage questions generated automatically suitable text corpus allows produce large-scale datasets recently proposed researchers google deepmind facebook cnn/daily mail dataset children’s book test respectively. attracted attention research community state-of-the-art model coming every weeks. time models compared several standard datasets publications often presenting minuscule improvements performance. large-scale billion word corpus dataset appeared allowed jozefowicz train much larger lstm models almost halved state-of-the-art perplexity dataset. think time make similar step area text comprehension. hence introducing booktest dataset similar children’s book test times larger enable training larger models even domain text comprehension. furthermore methodology used create data later used create even larger datasets need arises thanks technological progress. show evaluate model trained dataset standard children’s book test dataset improvement accuracy much larger research groups achieved enhancing model architecture itself training dataset reduce prediction error almost third. named-entity version brings ensemble models level human baseline reported facebook however ﬁnal section show human study still room improvement beyond performance model. natural testing reader’s comprehension text question answer deduced text. hence task trying solve consists answering clozestyle question answer depends understanding context document provided question. model also provided possible answers correct selected. formalized follows training data consist tuples question document contains answer question possible answers ground-truth answer. sequences words vocabulary also assume possible answers words vocabulary cnn/daily mail datasets also true ground-truth answer appears document. exploited many machine learning models however explicitly depend property crucial condition applying deep-learning techniques huge amount data available training. question answering speciﬁcally means large number documentquestion-answer triples available. unlimited amount text available coming relevant questions corresponding answers extremely labour-intensive done human annotators. efforts provide human-generated datasets e.g. microsoft’s mctest however scale suitable deep learning without pretraining data google deepmind managed avoid scale issue generating documentquestion-answer triples automatically closely followed facebook similar method. brieﬂy introduce resulting datasets whose properties summarized table daily mail datasets datasets exploit useful feature online news articles many articles include short summarizing sentence near page. since information summary sentence also presented article body nice cloze-style question note vocabulary distinct model’s dictionary. instance model replace certain rare words generic unknown-word tags hence reducing dictionary size. table statistics standard text comprehension datasets booktest dataset introduced paper. stands common nouns stands named entites. statistics taken statistics provided data set. dataset’s authors also replaced named entities dataset anonymous tokens shufﬂed batch. forces model rely solely information context document able transfer meaning named entities documents. restricts task speciﬁc aspect context-dependent question answering useful however moves task real application scenario would like model information available answer questions. furthermore chen suggested make questions unanswerable even humans. also claim half question sentences mere paraphrases exact matches single sentence context document. raises question extent dataset test deeper understanding articles. children’s book test children’s book test uses different source books freely available thanks project gutenberg. since summary available example consists context document formed consecutive sentences story together question formed subsequent sentence. dataset comes four ﬂavours depending type word omitted question sentence. based human evaluation done seems named entities common nouns context dependent types prepositions verbs. therefore lambada dataset designed measure progress understanding common-sense questions short stories easily answered humans cannot answered current standard machine-learning models dataset useful measuring humans machine learning algorithms. however contrast booktest dataset allow track progress towards performance baseline systems examples machine learning show super-human performance. also lambada diagnostic dataset provide ready-to-use questionanswering training data plain-text corpus moreover include copyrighted books making potentially problematic purposes. providing ready training data consisting copyright-free books only. squad dataset based wikipedia who-did-what dataset based gigaword news articles factoid question-answering datasets multi-word answer extracted context document. contrast previous datasets including cnn/dm lambada dataset require single-word answers. datasets however provide less training questions orders magnitude less dataset does. story cloze test provides crowd-sourced corpus commonsense stories training testing stories right wrong endings. hence dataset rather small. similarly lambada story cloze test designed easily answerable humans. wikireading dataset context document formed wikipedia article question-answer pair taken corresponding wikidata page. entity wikidata contain number property-value pairs form datasets’s questionanswer pairs. dataset certainly relevant community however questions limited variety properties covering dataset. furthermore many frequent properties mentioned spot within article make task easier machines. trying provide varied dataset. although several datasets related task aiming solve differ sufﬁciently dataset bring value community. biggest advantage size furthereasily upscaled without expensive human annotation. finally emphasizing differences models could certainly beneﬁt diverse collection datasets possible. ﬁrst major work applying deep-learning techniques text comprehension hermann work followed application memory networks task later three models emerged around time including attention reader model reader inspired several subsequent models sub-component diverse ensemble extend hierarchical structure compute attention context document every word query two-way context-query attention mechanism every word context query similar possible directions improvements accuracy machine learning tasks enhanced either improving machine learning model using in-domain training data. current state models improve reader’s accuracy datasets percent absolute. suggests current techniques limited room improvement algorithmic side. possibility improve performance simply training data. importance training data highlighted frequently quoted mercer’s statement there data like data. observation having data often important better algorithms frequently stressed since step direction exploiting potential data domain text comprehension created dataset called booktest similar much larger widely used cnn/dm datasets. similarly booktest dataset derived books available project gutenberg. used copyright-free books extract examples books examples comparison dataset extracted books. creating dataset follow procedure used create dataset detect whether sentence contains either named entity common noun already appeared preceding twenty sentences. word replaced sentence hence turned clozestyle question. preceding sentences used context document. common noun quote attributed robert mercer fred jelinek reader introduced reader time publication signiﬁcantly outperformed architectures datasets. model built leverage fact answer single word context document. similarly many models uses attention document intuitively measure relevant word answering question. however previous models used attention weights calculate blended representation answer word simply attention across occurrences unique words simply select word highest ﬁnal answer. simple trick seems improve accuracy speed-up training. adopted basic structure words document question ﬁrst converted vector embeddings using look-up matrix document read bidirectional network concatenation hidden states forward backward grus word used contextual embedding word intuitively representing context word appearing. also understand representing questions word answer. similarly question read bidirectional case ﬁnal hidden states concatenated form question embedding. attention word context calculated product contextual embedding question embedding. attention normalized softmax function summed across occurrences answer candidate. candidate accumulated attention selected ﬁnal answer. out-of-vocabulary words past experiments datasets unique word training validation test datasets look-up matrix however radically increased dataset size would result extremely large number model parameters decided limit vocabulary size frequent words. example unique out-of-vocabulary word mapped anonymous tokens randomly initialized untrained. fixing embeddings anonymous tags proved signiﬁcantly improve performance. instances model. tried initializing context encoder gru’s hidden state letting encoder read question ﬁrst proceeding read context document. intuitively allows encoder know advance look reading context document. table shows accuracy reader architectures validation test data. last rows show performance reader trained booktest dataset; models trained original training data. take best reader ensemble trained baseline improving model architecture continuing original training data lead improvements absolute named entities common nouns respectively. contrast inﬂating training dataset provided boost using model. ensemble models even exceeded human baseline provided facebook common noun dataset. firstly since amount data practically unlimited could even generate resulting continuous learning similar neverending language learning carnegie mellon university speed training determines much data model able see. since training data signiﬁcantly help model performance focusing speeding algorithm important ever before. instance inﬂuence decision whether regularization dropout seem somewhat improve model performance however usually cost slowing training. thanks simplicity reader seems training fast example around seven times faster models proposed chen hence reader particularly suitable training large datasets. second challenge generalize performance gains large data speciﬁc target domain. huge amounts natural language data general case domain want ultimately apply model. highlighted observations applying model trained booktest children’s book test test data. move model training joint ne+cn training data subset booktest size drop accuracy around test datasets. hence even though children’s book test booktest datasets almost close disjoint datasets transfer still imperfect rightly choosing data augment in-domain training data certainly problem worth exploring future work. results show given enough data reader able exceed human performance reported facebook. however hypothesized system still achieving full potential decided examine room improvement small human study. this also suggests increase accuracy using data strictly domain original training data results performance increase even larger reporting cbt. however scenario look additional data elsewhere realistic focusing article direction. decided explore remaining space improvement testing humans random subset named entity common noun validation questions reader ensemble could answer correctly. questions answered non-native english speakers research laboratory disjoint subset questions.. participants unlimited time answer questions told questions correctly answered machine providing additional motivation prove better computers. results human study summarized table show majority questions system could answer fact answerable. suggests original human baselines might underestimated however might also case examples answered machines humans; still space improvement. system would answer correctly every time either ensemble human answered correctly would achieve accuracy percent validation test datasets datasets. hence still makes sense ways improving model performance solidly established using training data. believe principle somewhat neglected recent research text comprehension. practically unlimited amount data available ﬁeld research performed unnecessarily small datasets. gentle reminder community shown simply infusing model data yield performance improvements several attempts improve model architecture training data given gains compared best ensemble result. experiments small datasets certainly bring useful insights. however believe community also embrace real-world scenario data abundance. references michele banko eric brill. scaling large corpora natural language disambiguation. proceedings annual meeting association computational linguistics pages kyunghyun bart merrienboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. empirical methods natural language processing jenny rose finkel trond grenager christopher manning. incorporating nonlocal information information extraction systems gibbs sampling. proceedings annual meeting association computational linguistics karl moritz hermann tomas kocisky edward grefenstette lasse espeholt mustafa suleyman phil blunsom. teaching machines read comprehend. advances neural information processing systems pages fr´ed´eric bastien pascal lamblin razvan pascanu james bergstra goodfellow arnaud bergeron nicolas bouchard yoshua bengio. theano features speed improvements. deep learning unsupervised feature learning nips workshop. daniel hewlett alexandre lacoste llion jones illia polosukhin andrew fandrianto matthew kelcey david berthelot. wiki reading novel large-scale language understanding task wikipedia. pages ciprian chelba tomas mikolov mike schuster thorsten brants phillipp koehn tony robinson. billion word benchmark measuring progress statistical language modeling. proceedings annual felix hill antoine bordes sumit chopra jason weston. goldilocks reading children’s books exprinciple arxiv preprint plicit memory representations. arxiv.. sosuke kobayashi tian naoaki okazaki kentaro inui. dynamic entity representation max-pooling improves machine reading. proceedings north american chapter association computational linguistics human language technologies mitchell cohen hruschka talukdar betteridge carlson dalvi gardner kisiel krishnamurthy mazaitis mohamed nakashole platanios ritter samadi settles wang wijaya gupta chen saparov greaves welling. never-ending learning. proceedings twenty-ninth aaai conference artiﬁcial intelligence mostafazadeh nathanael chambers xiaodong devi parikh dhruv batra lucy vanderwende pushmeet kohli james allen. corpus evaluation framework deeper understanding commonsense stories. proceedings naacl. denis paperno angeliki lazaridou quan ngoc pham raffaella bernardi sandro pezzelle marco baroni gemma boleda raquel fern. lambada dataset word prediction requiring broad discourse context. proceedings acl. razvan pascanu tomas mikolov yoshua bengio. difﬁculty training recurrent neural networks. proceedings international conference machine learning pages matthew richardson christopher burges erin renshaw. mctest challenge dataset open-domain machine comprehension text. empirical methods natural language processing pages andrew saxe james mcclelland surya ganguli. exact solutions nonlinear dynamics learning deep linear neural networks. international conference learning representations. kristina toutanova klein christopher manning. feature-rich part-of-speech tagging cyclic dependency network. proceedings conference north american chapter association computational linguistics human language technology volume pages adam trischler zheng xingdi yuan jing phillip bachman kaheer suleman. parallel-hierarchical model machine comprehension sparse data. proceedings acl. merrienboer dzmitry bahdanau vincent dumoulin dmitriy serdyuk david warde-farley chorowski yoshua bengio. blocks fuel frameworks deep learning. pages table best hyperparameters range values tested booktest train dataset. report number hidden units unidirectional gru; bidirectional twice many hidden units. started best performing model according validation performance. step tried adding best performing model previously tried. kept ensemble improve validation performance discarded otherwise. gradually tried model once. call resulting model greedy ensemble. used booktest validation dataset procedure. train model used stochastic gradient descent adam update rule learning rates best learning rate experiments minimized negative log-likelihood training objective. initial weights word-embedding matrix drawn randomly uniformly interval weights networks initialized random orthogonal matrices biases initialized zero. also used gradient clipping threshold batches sizes increasing batch seems signiﬁcantly improve performance large dataset something observe original data. increasing batch size much currently difﬁcult memory constraints gpu. training randomly shufﬂed examples beginning epoch. speed training always pre-fetched batches worth examples sorted according document length. hence batch contained documents roughly length.", "year": 2016}