{"title": "Semantic Vector Machines", "tag": ["cs.LG", "cs.AI"], "abstract": "We first present our work in machine translation, during which we used aligned sentences to train a neural network to embed n-grams of different languages into an $d$-dimensional space, such that n-grams that are the translation of each other are close with respect to some metric. Good n-grams to n-grams translation results were achieved, but full sentences translation is still problematic. We realized that learning semantics of sentences and documents was the key for solving a lot of natural language processing problems, and thus moved to the second part of our work: sentence compression. We introduce a flexible neural network architecture for learning embeddings of words and sentences that extract their semantics, propose an efficient implementation in the Torch framework and present embedding results comparable to the ones obtained with classical neural language models, while being more powerful.", "text": "ﬁrst present work machine translation used aligned sentences train neural network embed n-grams diﬀerent languages d-dimensional space n-grams translation close respect metric. good n-grams n-grams translation results achieved full sentences translation still problematic. realized learning semantics sentences documents solving natural language processing problems thus moved second part work sentence compression. introduce ﬂexible neural network architecture learning embeddings words sentences extract semantics propose eﬃcient implementation torch framework present embedding results comparable ones obtained classical neural language models powerful. first would like thank labs allowing come months united states work interesting project great environment. wonderful experience personal professional levels. interesting people opportunity attend fascinating talks along project. would also like thank ronan collobert support professional personal life. gave great advice feedback along project always needed support fantastic torch framework created. also showed beautiful places jersey york state even shared love quebec great trip there. jason weston help ﬁrst part project. prof. martin hasler accepting supervise master thesis epfl taking time read monthly reports providing helpful feedback. europarl parallel corpus using words network architecture training algorithm loss function performance evaluation results network architecture results discussion extraction auto-encoder networks network architecture local optimization global optimization results second approach ranking network architecture loss function implementation results look resulting embedding finding best tree greedy algorithm report covers master thesis project laboratories america princeton jersey done september march project machine learning natural language processing. machine learning ﬁeld studies design implementation algorithms allow computers improve performances time based data whereas natural language processing ﬁeld computer sciences concerned interaction computers human languages. intersection ﬁelds many tasks found semantic role labeling name entity recognition etc. focused them machine translation sentence compression. machine translation become popular last years mostly huge quantity data available internet languages. people want able read latest news japanese website understand recipe blog peruvian chef. need tools translate texts various foreign languages mother tongue. address problem ﬁrst part project building automated tool learns translate language another. sentence compression generally semantic extraction ﬁeld become even needed translation. indeed information electronic devices ever part lives generate huge amounts data positions receivers images cellphones cameras texts blogs temperatures sensors etc. continuous data become impossible humans interpret even sort thus need tools able make sense pile unlabeled data organize semantic extraction aims extract representation data gives information meaning. second part project build tool embeds text d-dimensional space representation contains features needed meaning. tool like that would able easily categorize sort cluster tons information unsupervised way. chapter describe ﬁrst part project build neural network learns translate language another embedding words sentences d-dimensional space using corpus aligned sentences languages. obtained results presented conclude chapter considerations limitations model reasons redeﬁning problem trying approach. chapter discuss second part project focused sentence compression. propose approach still relies embedding present main problems associated solution presented performances compared previous work ﬁeld language models. neural network mathematical model based biological observations used several diﬀerent tasks common ones classiﬁcation regression. built using layers interconnected artiﬁcial neurons. artiﬁcial neurons simply take input values return weighted them sometimes applied non-linear function neural network layer quite simple adding second layer allows network approximate function assuming enough neurons layer. thus neural networks seen universal approximators. general express network sequence matrix-vector operations applications non-linear functions illustrated ﬁgure example following equation describes network inputs neurons second layer hyperbolic tangent transfer function outputs loss function measures performance network respect given input current weights cannot minimize expected risk directly know distribution however proven suﬃcient minimize approximation expected risk ﬁnite training samples weight layer then simply apply scaling factor gradient called learning rate subtract weight. gradient descent algorithm well-known optimization technique allows local minima pretty quickly means legitimately consider layer networks black module takes inputs gives outputs. forward data backpropagated gradient without actually knowing observation origin modular architecture torch framework used project described later chapter explained section want optimize approximation risk training samples. ﬁrst forward whole dataset network gradients sample ﬁnally update weights network weights network current iteration weights next iteration. practice however technique called stochastic gradient descent details). technique consists estimating gradient whole dataset gradient sample. means instead considering training examples updating weights them results faster convergence speed. indeed calculating gradients whole dataset ﬁrst iterations waste time network randomly initialized thus certainly huge error sample. reason used stochastic gradient descent experiments. common widely used transfer function hidden neurons non-linear neural network hyperbolic tangent. project following work ronan collobert chose variant function called hard hyperbolic tangent deﬁned follows functions compared ﬁgure substitution allows hidden neurons saturate faster thus puts network non-linear mode sooner training process. moreover much simpler implement tanh hardt faster compute thus makes experiments quickly. words something usable multi-layer perceptrons? could simply replace word index dictionary. approach would work main disadvantage reduces greatly information contained input simple number clearly cannot hold information contained word. thus need representation still contains features original word understandable processable neural network. solution problem introduced yoshua bengio article presents mapping words vectors d-dimensional space mapping words vectors included neural network architecture values vectors learned training process. collobert reﬁned method technique implemented module torch named lookuptable. allows learn compute mapping diﬀerent words d-dimensional space. practice getting weight vector corresponding given word simply consists lookup table. however easily written matrix operation later included matrix calculations formalize multi-layer perceptron explained section implementation part project done using torch opensource machine learning framework written lua. framework originally developed ronan collobert idiap research institute switzerland. provides matlab-like environment state-of-theart machine learning algorithms eﬃcient implementation. moreover thanks modular architecture allows easily algorithms change behavior existing component. chapter small introduction framework gives insight power ease use. framework divided several packages provides tools speciﬁc tasks. main package used contains everything needed building feed-forward neural network training using backpropagation techniques. using multi-layer networks layer considered separate entity. indeed data need forwarding given layer output values previous layer. similarly backpropagating error network need update given layer gradient values coming next layer explained section observation allows layer network module module generic methods used forwarding data backpropagating gradients. allows black boxes chain them regardless content package provides containers modules allow build sequence several layers figure example basic torch usage ﬁrst create sequential container allows build sequence layers. then successively linear layer three inputs outputs non-linearity ﬁnally linear layer inputs output. resulting network pictured ﬁgure creating network feed random-valued input corresponding output. torch also provides eﬃcient implementation matrices dimension operators performing several mathematical statistical manipulation operations them. finally module implements forwarding backwarding methods allows train created network easily using gradient descent technique. create custom loss function criterions already implemented torch figure shows code snippet describing create simple non-linear multi-layer perceptron three inputs hidden neurons hyperbolic tangent transfer function output. resulting network illustrated ﬁgure give reader idea simple word neural network using torch. figure non-linear multi-layer perceptron corresponding code snippet ﬁgure tree inputs hidden neurons hyperbolic tangent transfer function output. ﬁrst part project machine translation. ﬁeld research high interest many years. internet everybody access resources several diﬀerent languages. moreover cheap travels become easy foreign exotic countries often completely diﬀerent language even alphabet. thus need eﬃcient translation tools greater ever. diﬀerent approaches exist problem less success. state-ofthe-art translation tools google translation mainly based mapping tables given sentence translate looks huge database sentence knows translation. then combines known translations build ﬁnal sentence. solution gives satisfying results main drawback requires possess mapping table. companies like google enough available data build database infrastructure store clearly feasible individuals. solution translation problem train neural network embed words sentences d-dimensional space. space words sentences close meaning close other regardless language belong translating word would easy ﬁnding closest word language embedding space. many available internet fortunately countries several oﬃcial languages thus translate oﬃcial documents them. holds also european union publishes oﬃcial documents language country belongs great opportunity machine translation provides parallel datasets translation diﬀerent languages whereas available data usually limited common languages europarl parallel corpus extracted european parliament proceedings contains versions eleven diﬀerent languages. distributed structured form also provides tools generate sentence aligned data details). data consists text language line correspond sentence thus allowing learn translate others. needed able assess quality results experiments focused english/french translations. however approach language independent thus could used languages aligned sentences available. ﬁrst approach used seen words model sentence consider words separately without taking account order. described section embed words d-dimensional space process them. weights vectors representing word randomly initialized trained learning process. represent sentence embedding space simply mean words. figure shows diagram representing architecture multi-layer perceptron used learn translation task. input receives pair sentences language. word represented index global dictionary computed oﬄine constant language. ﬁrst stage network runs list indices lookup table replaces word d-dimensional representation embedding figure architecture multi-layer perceptron used learn english/french translations. sentences language forwarded parallel lookup table replace words d-dimensional vectors. then take mean vectors sentence representation compute distance representations score. basic idea used training sentences translation representations close embedding space whereas random sentences train network composed copies network described above shown ﬁgure line aligned corpus generate samples positive contains english sentence figure global architecture neural network. take instances network described ﬁgure feed positive sample negative compare scores obtained output want score positive sample higher score negative one. xpos xneg respectively positive negative samples described above ﬁxed margin. loss functions optimizes network scores positive samples bigger scores negatives sample given margin. value margin arbitrary weights networks dynamically adjusted. however experiences shown convergence speed actually aﬀected value. thus empirically chose square root dimension margin experiments. rank given pair corresponding sentences english french deﬁned follows rank french closest french sentence english. algorithm describes rank actually computed. translations prefect would average rank meaning english sentence closest french sentence actual corresponding sentence. however sentences train nearly sentences test computing real rank sentences pair would quite time consuming even restricted test only. therefore needed approximation actual rank fast compute. thus came method statistically estimating average rank. limited number randomly chosen french sentences instead whole dataset count often corresponding sentences shorter distance english sentence random french sentence. algorithm describes process details. experiments diﬀerent values discovered actually suﬃcient compare corresponding sentences random pair decent estimation performance thanks great number train test samples. ﬁrst results obtained shown ﬁgure plot displays estimated performance evaluated test sentences diﬀerent dimensions embedding space. best result obtained shows performance means would translate incorrectly sentence time. look like good result earlier experiments made jason weston showed error rate could reached. comparing setups quickly found diﬀerence experiments using distance measure whereas using product equivalent distance. measure gave importance great errors misaligned sentences data prevented network converge correctly. figure performance estimation network using words product measure distance function number iterations several dimensions interestingly yellow curve shows using margin instead figure also showed something interesting experiments margin instead square root dimension gave much worse results others. tests inﬂuence value margin. results presented ﬁgure indicate empirical choice margin criterion good. certainly still converging time obtained much satisfying results illustrated ﬁgure ﬁgure shows estimated performance test network using dimension embedding space diﬀerent measures. obtained error rate product measure distance using distance instead allowed reduce checking hand remaining errors discovered compared results distance several dimensions ﬁgure could reach better performances higher embedding dimensions. dimensions smaller ﬁfty clearly showed reduced performances results suggest dimensions hundred above performances constrained lack complexity network rather words approach. figure shows english words closest french word embedding space dimension translations look correct really interesting result obtained looking closest words languages pictured ﬁgure embedding learned networks groups words similar related. obtained satisfying results considering words only extended architecture include features learning process. indeed explained above using mean sentence representation embedding space makes approach similar words sense lose information order words relation other. order address problem much information possible solution figure sample embedding results header lines show english words closest french word embedding space. lines show nine closest words languages corresponding english words header line. figure shows modiﬁed architecture network presented ﬁgure simply added second network parallel time using pairs words instead words only lookup tables. pair replaced index dictionary mapped representation embedding space. words only mean representations taken representation sentence. process applied english french sentences distance encoded representations computed. finally distances obtain global score particular pair sentences using words pairs words. actually added networks train translation pairs words learn similarities n-grams diﬀerent sizes language. note could easily take account n-grams simply adding networks parallel lookup tables given size n-gram. network still trained criterion pictured ﬁgure copies network described above feed ﬁrst corresponding sentences given english sentence random french sentence. want score corresponding pair bigger score random pair. moreover accelerate training train networks sample rather randomly select network train example between figure shows estimated performance extended version network diﬀerent dimensions embedding space. dimension estimated performance using words only like simple version network using words pairs words compute computing distance described above. second evaluation gives ﬁrst worse results using words only regardless dimension certainly gives information thus needs time network adjust. enough iterations however gives results using words only shows even better performance afterwards also smaller dimensions limited allow network fully represent complexity problem dimensions higher approximatively performances. figure estimated performance extended network function number iterations diﬀerent dimensions embedding space. experiment estimate performance using words only using words pairs. figure samples embedding space learned extended network dimension header lines show chosen english pairs along closest english word next lines show closest pairs words chosen pairs. pairs words allowing directly translate english pairs french pairs vice-versa. moreover look results language i.e. english n-grams close given english n-gram. figure shows chosen english pairs closest english words. simply variations words pair examples like especially close particular makes think network actually learned meaning words themselves instead simply building english/french mapping. convinced could even better results network simply letting train longer results test clearly still increasing. however obtained good translations words pairs words translating sentences obvious. indeed presented english sentence embed dimensional space using neural network. could actually corresponding french sentence? clearly cannot list possible sentences pick closest. present ideas solving problem chapter point realized learned good mappings n-grams n-grams approach diﬀerent ones presented introduction chapter. embeddings compressed representations captured meaning sentences felt enough. indeed interesting focus solely learning semantics sentences harder problem would greatly help large number ﬁelds including machine translation. fact understand sentence want translate correctly. thus decided work translation dive world sentence compression. chapter present second part work focused compressing sentences using neural networks. explained previous chapter work translation related compression taking sentences arbitrary length mapping features vector dimension later compare vectors sentences. basically features vector seen compressed representation sentence. however words approach used taking mean words sentence lost information. thus decided approach still based idea embedding words d-dimensional space time diﬀerent purpose translation semantic compression. want encoded representation gives information meaning sentence. huge amount data available everywhere ever need solutions indexing sorting grouping comparing text documents. however comparing texts diﬃcult could short summarized long verbose still talking subject. comparing word occurrences extract keywords help similarities texts would certainly fail comparing complicated article wikipedia english sibling simple english. simple producing encoded representation sentences synthesizes meaning semantics would help greatly direction. instead trying compare text would work points d-dimensional space allowing well known tools algorithms clustering sorting another element space iteratively group words sentence element left. order group elements vary example used tree shown upper-left part ﬁgure could used other. compresses elements d-dimensional space another element space. then would apply function iteratively sentence reduce single element embedding space vector encoded representation sentence. example process shown ﬁgure therefore main aspects problem indeed shown ﬁgure many ways grouping sentence two. quality resulting representation depend order function applied maybe grouping ﬁrst words related better grouping sentence randomly left right approach? however ﬁnding function best grouping words time would hard. therefore ﬁrst used ﬁxed left right tree ﬁnally shuﬄing them. result random list nine millions paragraphs totalizing hundred ninety millions words. dataset built dictionary diﬀerent words sorted frequency. finally thresholded words keep frequent discarded words training dataset. sentences delimitation dataset cannot actual sentences training. instead slide window ﬁxed size paragraph extract n-grams contain thresholded word. means obtains n-grams split actual sentences ideally consider these make less sense actual sentence willing price less perfect data exchange large quantity network trained using regular backpropagation techniques identity output equal input data training converged split network parts illustrated ﬁgure taking output compression part compressed representation input. then extraction part original data back encoded representation. linear architecture similar pictured ﬁgure bourlard shown autoencoder network equivalent applying principal component analysis data. means instead training network could simply express principal component analysis involves mathematical procedure transforms number possibly correlated variables smaller number uncorrelated variables called principal components. details solution problem explicitly terms input data using techniques like svd. would sure optimal solution whereas classical error backpropagation algorithm could stuck local minima. also possible hidden layers encoded representation time non-linear transfer functions like hyperbolic tangent. common misperception neural network community even non-linearities auto-encoder networks trained backpropagation equivalent linear methods claimed japkowicz shown case non-linear autoassociators beactually diﬀerently linear methods outperform projection classiﬁcation tasks even able perform non-linear classiﬁcation. ﬁrst tried optimize network locally time compress vectors extract back encoded representation want extractions close possible original vectors. mean squared error criterion deﬁned follows figure example training network sentence length ﬁrst word vectors forwarded compression extraction part mean squared error backpropagated. done third fourth word vectors. finally outputs compression part obtained taken forwarded auto-encoder mean squared error backpropagated. output third compression used encoded representation sentence. chose local approach felt global would hard train. indeed sentence words already implies forward nine compression networks nine extraction networks. backpropagating error deep architecture would small gradients ﬁrst layers resulting ﬁrst networks never trained. local approach however like greedy solution minimizes error intermediate step compression instead globally. performance network evaluated looking mean squared distance input vectors auto-encoder output vectors test set. measure indicates network correctly training want give information whether could correctly extract sentence not. thus used second measure test phase proportion correct extractions. proportion deﬁned percentage words closest vector extracted word vector embedding space original word vector itself. algorithm shows computed. obtained ﬁrst results observed decreasing mean squared error translate better proportions correct extractions. besides proportions good short sentences quickly decreased considering longer sentences suggesting greedy solution optimizing mean squared error step compression right choice good extraction performances. thus changed training method end-to-end technique instead considering compression step separately backpropagating error locally performed full compression extraction process sentence evaluated distance directly original word vectors corresponding extracted vectors. then backpropagated error whole extraction compression networks. figure illustrates training scheme. figure second version training scheme contrary pictured ﬁgure error computed original word vectors ﬁnal extracted vectors backpropagated directly whole network. figure shows mean squared error original word vectors extracted ones function number iterations linear non-linear networks diﬀerent sentences length figure shows corresponding proportions correct extractions. results particularly noisy clearly decreasing global mean squared error increases proportion correct extractions especially non-linear networks. moreover extracting nearly perfectly pairs words linear autoencoder quickly shows limits increasing length sentence. non-linear diﬃcult train thus takes longer good results able extract longer sentences much higher success rate. again observing slopes diﬀerent curves graphs suggest could obtained better results training longer. evaluating proportion correct extractions pretty costly implies computing distance word dictionary extracted word vector. thus words dictionary testing limited size thus resulting variance measure. actually wanted learning semantics. verify this look embedding space compared words closest neighbors similarly previous chapter. everything worked well learned that example women similar concepts thus similar representation embedding space. results totally diﬀerent word embeddings made sense moved randomly generated position assigned initialization network. suspect instead making network learn semantics criterion allowed away applying tricks inputs still obtain good results. thus another criterion would compel network understand meanings sentences instead simply applying tricks vectors. second approach based diﬀerent training network instead relying distance input output introduce notion score score attributed encoded representation sentence then train network score sentence dataset higher score sentence word randomly replaced. score seen indicator whether given sentence correct not. teaching network diﬀerentiate encoded representation correct sentence wrong sentence hopefully make deduce embedding words makes sense. compression part used linear linear ﬂavors score function simply implemented linear layer inputs output appended last compression step shown ﬁgure again train network end-to-end manner forwarding positive negative examples network obtain encoded representations computing score evaluating error ﬁnally backpropagating whole network. started group words diﬀerently point using randomly generated trees addition left right trees make sure network take advantage regularity words grouped concatenation tricks build encoded representation. also tried diﬀerent schemes generated wrong sentences either replacing always last word sentence random word randomly choosing sentence word replace. evaluate performance network computed rank sentence test set. rank deﬁned similarly rank used previous chapter given sentence generate possible negative samples replacing word words dictionary compute score negative samples. then sort decreasing score position correct sentence sorted list rank. hence rank means correct sentence better score negative samples. however cannot achieved sentences negative samples actually sentences correct e.g. replacing word house building sentence result something grammatically correct english high score. thus want implemented compression extraction networks modules torch. allow kind tree grouping words kind compression/extraction network. moreover coded following architecture module package means compression/extraction trees included modules existing networks transparently trained modules are. thus allows easily build kind networks used experiments illustrated ﬁgure without much eﬀort someone familiar torch framework. figure shows results obtained. used embedding dimension dictionary words sentence length non-linear version compression network hundred ﬁfty hidden units setup shown best results previous experiments small enough train reasonably fast. ﬁrst observation made looking results left right tree better random trees. give kind results conﬁguration. however good thing means network trick depends regularity words combined. moreover training n-grams ﬁxed length clearly gives better results training random lengths least achieves faster. main reason using ﬁxed length always trains network harder case whereas random lengths make train example pairs words quarter time really easier. finally results show replacing always last word generate negative samples choosing word replace randomly sentence inﬂuence results. major result previous attempts collobert case language models never able learn similar tasks replacing words randomly ﬁxed position order make network converge. overall best performance obtained ﬁxed length random trees choosing position word replace randomly generate negative samples. setup best average rank worst rank explained before could expect average rank figure example usage compresstree module create network composed lookup table compression tree score layer then create sentence forward network score. figure average rank sentences test function number iterations dictionary words diﬀerent ways grouping words generating negative samples. compression part non-linear hidden units trees position negative word random dimension embedding space n-grams lengths also tried verify network actually able learn limited small lengths. used embedding dimension non-linear compression network hidden units. time dictionary size words used random trees random position changing word negative samples. best average rank obtained n-grams length quite encouraging. detailed results shown ﬁgure results comparatively worse obtained smaller dictionary explained fact task diﬃcult. moreover experiments project still slightly decreasing slope training errors suggests still room improvement. figure average rank sentences test function number iterations dictionary words diﬀerent n-grams lengths. compression part non-linear hidden units trees position negative word random dimension embedding space figure selected words closest neighbors embedding space built network trained n-grams ﬁxed length dictionary words embedding space dimension random trees random position word replace generating random samples. finally exploration embedding space promising results obtained actually meant network built sensible embedding words. ﬁrst took selected words looked closest neighbors embedding space presented ﬁgure words semantically grouped least related. embedding captures several features words whether adjectives nouns singular plural etc. kind results obtained collobert architecture ﬂexible consider n-grams ﬁxed length training support replacing middle word sentence kind bag-of-word approach support kind grouping replacing sentences. furthermore language model assess relations words architecture analyze relations n-grams size. basically compute encoded representation able compare directly regardless represent pair words full sentence even whole document. caveat compute existing n-grams closest one. means closest pair words given pair using dictionary words compute score diﬀerent pairs sort closest one. feasible n-grams length small dictionaries clearly costly longer sentences real-world dictionaries. however proof concept selected pairs words looked closest pairs embedding space using limited dictionary. figure shows figure selected pairs closest neighbors embedding space built network trained n-grams ﬁxed length dictionary words embedding space dimension random trees random position word replace generating random samples. dictionary limited frequent words computing possible pairs. finally decent candidate compression function tried best grouping words. important observation iterative process start sentence length select consecutive words group apply replace resulting encoded representation. algorithm make notion score introduced last section. score seen mark attributed encoded representation qualiﬁes quality correct sentences good encoded representation wrong sentences one. hence make score build simple algorithm greedily select step best pair words group. method detailed algorithm illustrated ﬁgure opportunity algorithm using well-trained compression ranking network project thus cannot yield many results really assess performance. however achieve average rank using tree computed algorithm compress sentences dictionary words encouraging results compared best rank obtained random trees. figure shows examples trees selected greedy algorithm presented figure illustration greedy algorithm presented sentence compute score compressed representations pairs consecutive words select best replace words compressed representation repeat sentence element left. chapter concludes work. beginning document presented machine translation sentence compression ﬁelds explained challenges awaiting them would take ﬁrst succeeded building neural network able learn translate n-grams n-grams languages provided aligned sentences available training. however clearly showed limitations translating full sentences instead simple n-grams. propose extensions circumvent limitation next section translation remains opinion open problem. good translation tools need step start actually understand true meanings trying translate. second part work comes play. built ﬂexible network able embed sentence d-dimensional space iteratively applying compression function elements two. kind network compression phase combine words order. able obtain good candidates compression function able optimal combining words. again directions improving part next section. however architecture provides good starting point experiments powerful ﬂexible traditional neural networks language models. translation system built chapter really suited translating whole sentences. however still wanted translate whole sentences give hints direction follow. explained system perform good n-grams n-grams translations. thus presentend sentence length translate could imagine taking n-grams size computing corresponding closest n-grams size language. then ﬁnding best translation would matter selecting n-grams combining them maybe rearranging order close possible original sentence embedding space. section presented simple version greedy algorithm used good ways grouping words. approach obtained lower average rank using random trees makes construction choices locally optimal thus chances global optimum. could instead design algorithm makes decisions global based following observation grouping words sentence seen sequence decisions. indeed suppose start stack contains ﬁrst word sentence. consider word sentence successively starting second one. step possible decisions attribute cost choices algorithm similar viterbi algorithm globally optimal sequence decisions. costs still deﬁned feel approach achieve interesting results. olivier bousquet ulrike luxburg editors advanced lectures machine learning lecture notes artiﬁcial intelligence lnai pages springer verlag berlin collobert weston. uniﬁed architecture natural language processing deep neural networks multitask learning. international conference machine learning icml ronan collobert jason weston. uniﬁed architecture natural language processing deep neural networks multitask learning. international conference machine learning icml", "year": 2011}