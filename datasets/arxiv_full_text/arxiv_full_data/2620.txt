{"title": "Exploration for Multi-task Reinforcement Learning with Deep Generative  Models", "tag": ["cs.AI", "cs.LG", "stat.ML", "I.2; I.5"], "abstract": "Exploration in multi-task reinforcement learning is critical in training agents to deduce the underlying MDP. Many of the existing exploration frameworks such as $E^3$, $R_{max}$, Thompson sampling assume a single stationary MDP and are not suitable for system identification in the multi-task setting. We present a novel method to facilitate exploration in multi-task reinforcement learning using deep generative models. We supplement our method with a low dimensional energy model to learn the underlying MDP distribution and provide a resilient and adaptive exploration signal to the agent. We evaluate our method on a new set of environments and provide intuitive interpretation of our results.", "text": "exploration multi-task reinforcement learning critical training agents deduce underlying mdp. many existing exploration frameworks rmax thompson sampling assume single stationary suitable system identiﬁcation multi-task setting. present novel method facilitate exploration multi-task reinforcement learning using deep generative models. supplement method dimensional energy model learn underlying distribution provide resilient adaptive exploration signal agent. evaluate method environments provide intuitive interpretation results. learning solve multiple tasks simultaneously multi-task reinforcement learning problem. mtrl solved either planning deducing current ignoring deduction learning policy mdps combined. example marker environment determines obstacle structure goal locations. agent visit marker learn environment structure hence deducing task solved becomes important part agents policy. mtrl different transfer learning aims generalize knowledge tasks solve similar task. mtrl also involves learning common representation tasks makes attempt generalize tasks. conventional reinforcement learning algorithms like q-learning sarsa fail identify decision making sub-problem. also learn sub-optimal policies non-stationary reward structure since goal location varies episode episode. address issues current research mtrl driven towards using model possible mdps deduce current using form memory store past observations reason based history. methods able deduce happen markers make effort actively search deduce mdp. methods incentivize agent actively seek markers deduce providing smooth adaptive exploration signal pseudo-reward obtained generative model using deep neural networks. since method relies computing jacobian state representation respect input deep neural networks learn state representation. best knowledge ﬁrst propose using jacobian exploration bonus. exploration bonus added intrinsic reward like bayesian exploration bonus. focus grid-worlds colors markers. clarity redeﬁne state single grid-location pixel value observation. methods however generalize arbitrarily complex distributions observations also assume agent deduce rewards transition probabilities observation. extensive research ﬁeld exploration strategies reducing uncertainty mdp. rmax examples widely used exploration strategies. bayesian exploration bonus assigns pseudo reward states calculated using frequency state visitation. thomson sampling samples posterior distribution computed using evidence obtains trajectories. follow similar approach sample mdps. recent advances domain involve sampling using dropout neural networks. however algorithms assume exists single stationary episode. algorithm addresses multi-task problem episode uses sampled arbitrary distribution mdps. contrary strl exploration strategies exploration bonus designed mark states potentially useful agent improve certainty current mdp. recent advances mtrl transfer learning algorithms like value iteration networks actor-mimic networks attempt identify common structure among tasks generalize learning tasks similar structure. context mtrl transfer learning environments give image-like observations value iteration networks employ recurrent neural networks value iteration learn kernel functions estimate reward transition probabilities state immediate surroundings. effect easily generalizing tasks share structure. work attempt learn common structure across mdps purpose transfer learning. instead attempt learn input distribution deduce current given observations. proposes novel method using deep recurrent memory networks learn policies minecraft multi-task environments. used ﬁxed memory past observations. step context vector generated memory network queried relevant information. model successfully learns policies i-shaped environments color marker cell determines goal location. experiments i-world pattern-recognition world identiﬁer states close agents starting position. another class mtrl algorithms focuses deducing current using bayesian reasoning. multi-class models proposed attempt assign class labels current given sequence observations made hierarchical bayesian model learn conditional distribution class labels given observations. agent samples posterior distribution manner similar thomson sampling chooses action. follow procedure action selection incorporate exploration bonuses well. proposes multi-class multi-task learning non-parametric hierarchical bayesian model learn inherent structure value functions class. mcmtl clusters mdps classes learns posterior distribution mdps given observed evidence. similar work explicitly incentivize agent visit marker states. contributions fold. first propose deep generative model allow sampling posterior distribution. second propose novel exploration bonus using models posterior distribution. variational auto encoders attempt learn distribution generated data vaes like standard autoencoders encoder decoder component. generative models attempt estimate likelihood objective function formally objective function written deﬁned σi). gradient-motivated learning requires approximation integral samples. high-dimensional z-space could lead large estimation errors likely concentrated around select would take infeasible number samples proper estimate. vaes circumvent problem introducing distribution sample from. reduce parameters functions approximated deep network form encoder component vae. represented using sampling function forms decoder component vae. mathematical sleight hand account learning equations provides intuitive understanding equations) obtain following formulation loss function rbms used widely learn energy models input distribution undirected complete bipartite probabilistic graphical model hidden units visible units gaussian-binary rbms hidden units binary units capable representing total combinations visible units gaussian distribution. network parametrized edge weights matrix node bias vectors respectively. given visible state hidden state obtained sampling posterior given since rbms model conditional distributions conditional distributions closed form marginal joint distributions impossible compute without explicit summation combinations. parameters learnt using contrastive divergence). learning however proved unstable) hence treat hyperparameter inv. corresponding state observed agent. denote pixel mask denoted respectively. episodes agent visit entire grid-world hence since several views ground-truth need able reconstruct ground-truth multiple observations several episodes. single-task done tabular fashion. mtrl however potentially inﬁnite possible mdps becomes hard build association different views mdp. deep convolutional vaes infer association different views dimensional energy model sample mdps given observational evidence. here locality world features warrant convolution layers vae. figure shows setup learn associations infer ground truth given observations. setup train model another allow back sampling mdps. call train query models. method scaled large state spaces vae. figure deep generative model train model requires mask inputs account missing observations. query model involves value iteration determine best action sampled mdps. given partial observation sample posterior obtain samples. doesn’t enough evidence skew posterior favour single encoding produced encodings ground-truth mdps z-space. obtain mixture mdps sample posterior. solving could result agent following policy unsuitable component mdps isolation. circumvent problem train probability distribution embeddings zin. -mdp environments gaussian-boltzmann cluster inputs ﬁxed-variance gaussians. algorithm sample gaussians. value function given samples model posterior perform action selection using aggregate value function samples. deﬁne state aggregate value function algorithm sample mdps given result mdps sampled model posterior compute sample hidden states posterior calculate estimate maxz decode estimates samples denotes value function state obtained using standard planning algorithms value iteration. action selection done using \u0001-greedy mechanism since recomputing value functions step computationally infeasible selected action persists steps. note value functions used need exact approximate used steps. quicker estimate obtained using monte-carlo methods state-space large. incentivize agent visit decisive pixels/locations introduce bonus based change embedding intuitively embedding highest change detects changes relevant distribution modelling. bonus summarised follows denotes list observations made state tanh transfer function bound activations produced jacobian thus mainitaining numerical stability. bonus used ways pseudo reward actual reward deduced agent. methods showed improvement latter worked better since total reward states already gave high reward increased. since changes drastically observations recomputed every time recomputed. also memory-less i.e. doesn’t carry information episode next. back world goal location alternates depending marker location color marker location ﬁxed paths start goal. domain demonstrates advantage gained using probabilistic model mdps. back world setting bw-h marker location paths start goal. domain demonstrates advantage provided jacobian exploration bonus generative model. strl using visible portions environment unstable hence pseudo reward. unseen location provide pseudo reward step annealed factor episode terminated steps agent hadn’t reached goal. using pseudo reward agent forcefully terminated fewer times. worlds become challenging partial visibility. kernel clipped corners agent always assumed center. step environment tracks locations agent seen presents agent action taken. experiments consider average number steps goal measure loss average reward measure performance. figure worlds used experiments white indicates start position agent. green yellow marker locations. locations failures. blue locations successes. gray areas kernel visible agent. white cell kernel agents position. shown optimal path considers deduction sub-problem. table gives average reward agent. table gives average episode length. also impose forceful termination steps episode completed. results infer following. mtrl-α shows better results bw-h. expected mtrl- makes attempt visit marker locations. mtrl-α motivated jacobian bonus visit marker locations thereby deducing mdp. mtrl- performs good mtrl-α bw-e marker locations paths goal. however since fails understand signiﬁcance marker locations markers bw-h paths goal results longer episodes lower reward. visualize training used bw-e environment. used random agent environment navigate collect sample seen environment episode. encoded samples using used train rbm. figure shows clusters means gaussian rbm. since possible bw-e environments gaussians encoded samples clustered around same. training done minibatches samples hidden unit rbm. figure visualization training encoded bw-e world samples points means ﬁtted gaussians. blue points data points minibatch. black points samples gaussians ﬁtted rbm. spread black points measure variance ﬁtted gaussians. distinct clusters snapshot. iteratively reﬁnes parameters means close encoded sample clusters. perfect reconstruction spread encoded samples. visualize training setup visualizing training. used samples bw-e training done mini-batches samples. test samples randomly chosen reconstruction recorded epochs training. figure shows training progress bw-e samples. figure visualization training bw-e epochs learnt general structure grid-world learn colors. epochs learnt colors marker pixels training examples learn goal state colors. epochs learn colors goal state learn colors corners present samples. epochs learning complete. presented method using deep generative model provide exploration bonus solving multi-task reinforcement learning problem. modiﬁcation loss function allows learn partial inputs also associations different views environment. rbms learn distribution allows sample actual mdps instead mixture mdps. introduced intuitive exploration bonus shown improvements existing baselines. drawback jacobian bonus doesn’t reward structure mdps. bonus could yield sub-optimal policies environments multiple markers associated rewards. would like incorporate reward structure jacobian bonus form utility interpretation. deep generative model scalable would like explore learning larger worlds extend method work minecraft-like environments.", "year": 2016}