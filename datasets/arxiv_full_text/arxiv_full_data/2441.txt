{"title": "Generalization and Exploration via Randomized Value Functions", "tag": ["stat.ML", "cs.AI", "cs.LG", "cs.SY"], "abstract": "We propose randomized least-squares value iteration (RLSVI) -- a new reinforcement learning algorithm designed to explore and generalize efficiently via linearly parameterized value functions. We explain why versions of least-squares value iteration that use Boltzmann or epsilon-greedy exploration can be highly inefficient, and we present computational results that demonstrate dramatic efficiency gains enjoyed by RLSVI. Further, we establish an upper bound on the expected regret of RLSVI that demonstrates near-optimality in a tabula rasa learning context. More broadly, our results suggest that randomized value functions offer a promising approach to tackling a critical challenge in reinforcement learning: synthesizing efficient exploration and effective generalization.", "text": "sampling statistically plausible value functions broader speciﬁc algorithm beyond proposal study rlsvi. view important role paper establish broad concept promising approach tackling critical challenge synthesizing efﬁcient exploration effective generalization. present computational results comparing rlsvi lsvi action-dithering schemes. case studies algorithms generalize using identical linearly parameterized value functions distinguished explore. results demonstrate rlsvi enjoys dramatic efﬁciency gains. further establish bound expected regret episodic tabula rasa learning context. bound denote cardinalities state action spaces denotes time elapsed denotes episode duration. matches worst case lower bound problem logarithmic factors interesting contrast known bounds provably efﬁcient tabula rasa algorithms adapted context. knowledge results establish rlsvi ﬁrst algorithm provably efﬁcient tabula rasa context also demonstrates efﬁciency generalizing linearly parameterized value functions. sizable literature algorithms provably efﬁcient tabula rasa contexts literature algorithms generalize explore provably efﬁcient manner sparser. work model-based algorithms apply speciﬁc model classes computationally intractable. value function generalization approaches potential overcome computational challenges offer practical means synthesizing efﬁcient exploration effective generalization. relevant line work establishes efﬁcient value function generalization reduces efﬁcient kwik online propose randomized least-squares value iteration reinforcement learning algorithm designed explore generalize efﬁciently linearly parameterized value functions. explain versions least-squares value iteration boltzmann \u0001-greedy exploration highly inefﬁcient present computational results demonstrate dramatic efﬁciency gains enjoyed rlsvi. further establish upper bound expected regret rlsvi demonstrates nearoptimality tabula rasa learning context. broadly results suggest randomized value functions offer promising approach tackling critical challenge reinforcement learning synthesizing efﬁcient exploration effective generalization. design reinforcement learning algorithms explore intractably large state-action spaces efﬁciently remains important challenge. paper propose randomized least-squares value iteration generalizes using linearly parameterized value function. prior algorithms generalize require worst case learning times exponential number model parameters and/or planning horizon. rlsvi aims overcome inefﬁciencies. rlsvi operates manner similar least-squares value iteration also shares much spirit closely related approaches lstd sarsa fundamentally distinguishes rlsvi algorithm explores randomly sampling statistically plausible value functions whereas aforementioned alternatives typically applied conjunction action-dithering schemes boltzmann \u0001-greedy exploration lead highly inefﬁcient learning. concept explorregression however known whether kwik online regression problem solved efﬁciently. terms concrete algorithms optimistic constraint propagation provably efﬁcient algorithm exploration value function generalization deterministic systems c-pace provably efﬁcient algorithm generalizes using interpolative representations. contributions represent important developments suitable stochastic systems highly sensitive model mis-speciﬁcation generalizing effectively high-dimensional state spaces calls methods extrapolate. rlsvi advances research agenda leveraging randomized value functions explore efﬁciently linearly parameterized value functions. work know involving exploration random sampling value functions work proposed algorithm tabula rasa learning; algorithm generalize state-action space. episodic reinforcement learning ﬁnite-horizon ﬁnite state space ﬁnite action space number periods encodes transition probabilities encodes reward distributions state distribution. episode initial state sampled period h=··· state action selected next state sampled reward sampled episode terminates state reached terminal reward sampled represent history actions observations multiple episodes often index variables episode period. example respectively denote state action reward observed period episode policy sequence functions mapping policy deﬁne value function consider scenario agent models that span rsa×k. abuse notation denote cardinalities state action spaces. refer matrix generalization matrix denote matrix associated state-action pair write column refer basis function. refer contexts agent’s belief correct coherent learning refer alternative agnostic learning. lsvi applied episode estimate optimal value function data gathered previous episodes. form algorithm based lsvi must specify agent selects actions. common scheme selectively take actions random call approach dithering. appendix presents algorithms resulting combining lsvi common schemes \u0001-greedy boltzmann exploration. literature efﬁcient shows dithering schemes lead regret grows exponentially and/or provably efﬁcient exploration schemes require exploration directed towards potentially informative state-action pairs consistent multiple timesteps. literature provides several intelligent exploration schemes provably efﬁcient apply tabula rasa little prior information available learning considered efﬁcient even time required scales cardinality state-action space. sense rlsvi represents synthesis ideas efﬁcient tabula rasa reinforcement learning value function generalization methods. motivate beneﬁts rlsvi figure provide simple example highlights failings dithering methods. setting lsvi boltzmann \u0001-greedy exploration requires exponentially many episodes learn optimal policy even coherent learning context even small number basis functions. environment made long chain states step agent transition left right. actions left deterministic actions right succeed probability otherwise left. states zero reward except right gives reward episode length agent begin episode state optimal policy right every step receive expected reward episode policies give reward. example establishes that choice basis function lsvi \u0001-greedy boltzmann exploration lead regret grows exponentially similar result holds policy gradient algorithms. figure dithering schemes highly inefﬁcient. example ﬁrst episode state visited. easy furthermore either \u0001-greedy boltzmann exploration actions sampled uniformly random episodes thus episode node reached probability p∗−h. follows regret consider alternative approach exploration involves randomly sampling value functions rather actions. speciﬁc scheme kind propose randomized least-squares value iteration present algorithm obtain algorithm simply select greedy actions episode speciﬁed algorithm manner rlsvi explores inspired thompson sampling shown explore efﬁciently across general class online optimization problems thompson sampling agent samples posterior distribution models selects action optimizes sampled model. rlsvi similarly samples distribution plausible value functions selects actions optimize resulting samples. distribution thought approximation posterior distribution value functions. rlsvi bears close connection psrl maintains samples posterior distribution mdps direct application thompson sampling psrl satisﬁes regret bounds scale dimensionality rather cardinality underlying however psrl accommodate value function generalization without planning feature expect great practical importance. rlsvi algorithm designed efﬁcient exploration large mdps linear value function generalization. algorithms analytical regret bounds setting. fact common methods provably inefﬁcient demonstrated example regardless choice basis function. section establish expected regret bound rlsvi tabular setting without generalization basis functions bound expectation respect probability space deﬁne random variables consider respect probability space. assume deterministic drawn prior distribution. assume lemma conditional data q-values generated rlsvi stochastically optimistic true q-values proof. data available backwards induction write amount visits datapoint write empirical mean reward mean transitions based upon data write posterior mean rewards transitions rewards drawn independent dirichlet values transitions dirichlet analytical techniques exist extend similar results general bounded distributions; example theorem algorithm executed φh=i ..h− surprisingly scalings better state optimistic algorithms speciﬁcally designed efﬁcient analysis would admit regret important result since demonstrates rlsvi provably-efﬁcient contrast popular dithering approaches \u0001-greedy provably inefﬁcient. central analysis notion stochastic optimism induces partial ordering among random variables. deﬁnition real-valued random variables stochastically optimistic ur→r convex increasing notation express relation. worth noting stochastic optimism closely connected second-order stochastic dominance second-order stochastically dominates repoduce following result establishes relation involving gaussian dirichlet random variables appendix lemma dirichlet bound regret ﬁrst showing rlsvi generates optimistic estimates ∆opt nonpositive expectation history available prior episode remaining term ∆conc vanishes estimates generated rlsvi concentrate around lemma means rlsvi generates stochastically optimistic q-values history remains prove |hl] concentrate around remaining estimates optimistic bias terms rlsvi. terms emerge since rlsvi shrinks estimates towards zero rather dirichlet prior rewards transitions. next note that conditional rewrite martingale difference. allows decompose error policy estimation error states actions actually visit. also note that conditional data true independent sampling process rlsvi. means that analysis section shows rlsvi tabular basis functions acts effective gaussian approximation psrl. demonstrates clear distinction exploration randomized value functions dithering strategies example however motivating rlsvi tabular environments several provably efﬁcient algorithms already exist instead large systems require generalization. believe that conditions possible establish polynomial regret bounds rlsvi value function generalization. stimulate thinking topic present conjecture result possible appendix present series experiments designed test applicability scalability rlsvi exploration generalization. experiments divided three sections. first present series didactic chain environments similar figure show rlsvi effectively synthesize exploration generalization coherent agnostic value functions intractable dithering scheme. next apply algorithm learning play tetris. demonstrate rlsvi leads faster learning improved stability superior learned policy large-scale video game. finally consider business application simple model recommendation system. show algorithm improve upon even optimal myopic bandit strategy. rlsvi learns optimal strategy dithering strategies not. consider series environments modelled example dithering strategies exploration provably inefﬁcient. importantly unlike tabular setting section algorithm interact basis function generalize across states. examine empirical performance rlsvi efﬁciently balance exploration generalization didactic example. linear subspace spanned gaussian ∈rsa. form projecting onto renormalize component equal -norm. figure presents empirical regret rlsvi σ=.λ= \u0001-greedy agent seeds. scales gracefully further marginal effect decrease dim=k approaches dim=. include local polynomial regression blue highlight trend. importantly even large performance superior dithering tabular bounds. figure shows rlsvi consistently learns optimal policy roughly episodes. dithering strategy would take least episodes result. state upper bounds efﬁcient optimistic algorithm ucrl given appendix kick after suboptimal episodes. rlsvi able effectively exploit generalization prior structure basis functions learn much faster. examine learning scales change chain length number basis functions observe rlsvi essentially maintains optimal policy discovers rewarding state. number episodes rewards proxy learning time. report average random seeds. figure examines time learn vary chain length ﬁxed basis functions. include dithering lower bound dashed line lower bound scaling tabular learning algorithms solid line sa>. rlsvi demonstrates scalable generalization exploration outperform bounds. figure examines scalings logarithmic scale. data experiments consistent polynomial learning hypothesized appendix results remarkably robust several orders magnitude present detailed analysis sensitivies appendix unlike example above practical problems typically agnostic. true value function within vhk. examine rlsvi setting generate basis functions adding gaussian noise true value function parameter determines scale noise. problem coherent typically case. i=.. rlsvi episodes ρ=i/ random seed. figure presents number episodes rewards value large values extremely misspeciﬁed basis rlsvi effective. however region learning remains remarkably stable. simple example gives hope rlsvi chain bounds sa>. h∈{} represents signiﬁcant noise. note turn attention learning play iconic video game tetris. game random blocks fall sequentially grid rows columns. step agent move rotate object subject constraints grid. game starts empty grid ends square becomes full. however becomes full removed bricks move downward. objective maximize score attained game. tetris something benchmark problem approximate dynamic programming several papers topic focus much learn high-scoring tetris player instead demonstrate rlsvi offers beneﬁts forms exploration lsvi. tetris challenging huge state space states. order tackle problem efﬁciently benchmark features. featurs give height column absolute difference height column maximum height column number holes constant. well known superior linear basis functions mirror approach. order apply rlsvi tetris ﬁxed episode length made natural modiﬁcations algorithm. first approximate timehomogeneous value function. also keep recent transitions linear growth memory computational requirements similar details provided appendix figure present learning curves rlsvi λ=σ= lsvi tuned \u0001-greedy exploration schedule averaged seeds. results signiﬁcant several ways. basis functions algorithms reach higher ﬁnal performance respectively) best level lspi also reach performance many fewer games unlike lspi collapse ﬁnding peak performance. believe improvements mostly memory replay buffer stores bank recent past transitions rather lspi purely online. second rlsvi lsvi learn scratch lspi required scoring initial policy begin learning. believe improved exploration schemes lspi completely greedy struggles learn without initial policy. lsvi tuned schedule much better. however signiﬁcant improvement exploration rlsvi even compared tuned scheme. details available appendix show efﬁcient exploration generalization helpful simple model customer interaction. consider agent recommends products sequentially customer. conditional probability customer likes product depends product items better others. however also depends user observed liked disliked. represent products customer seen product indicate preferences {dislike like} respectively. customer observed product write model probability customer like product logistic transformation linear importantly model reﬂects customers’ preferences evolve experiences change. example customer much likely watch second season show breaking watched ﬁrst season liked whose goal maximize cumulative amount items liked time customer. agent know initially learn estimate parameters interactions across different customers. customer modeled episode horizon length cold start previous observed products simulations sample random problem instance sampling independently although setting simple number possible states +}|h exponential learn time less crucial exploit generalization states equation problem constuct following simple basis functions xn{a period form dimension function class exponentially smaller number states. however barring freak event simple basis lead agnostic learning problem. figure show performance rlsvi compared several benchmark methods. figure plot cumulative regret rlsvi compared lsvi boltzmann exploration identical basis features. rlsvi explores much efﬁciently boltzmann exploration wide range temperatures. optimal myopic policy. bernoulli thompson sampling learn much even episodes since algorithm take context account. linear contextual bandit outperforms rlsvi ﬁrst. surprising since learning myopic policy simpler multi-period policy. however data gathered rlsvi eventually learns richer policy outperforms myopic policy. appendix provides pseudocode computational study. note problems states; allows solve exactly compute regret. result averaged problem instances problem instance repeat simulations times. cumulative regret rlsvi lsvi boltzmann exploration plotted figure rlsvi clearly outperforms lsvi boltzmann exploration. simulations extremely simpliﬁed model. nevertheless highlight potential value multiarmed bandit approaches recommendation systems customer interactions. algorithm outperform even even optimal myopic system particularly large amounts data available. settings efﬁcient generalization exploration crucial. established regret bound afﬁrms efﬁciency rlsvi tabula rasa learning context. however real promise rlsvi lies potential efﬁcient method exploration large-scale environments generalization. rlsvi simple practical explores efﬁciently several environments state approaches ineffective. believe approach exploration randomized value functions represents important concept beyond speciﬁc implementation rlsvi. rlsvi designed generalization linear value functions many great successes come highly nonlinear deep neural networks backgammon atari insights approach gained rlsvi still useful nonlinear setting. example might adapt rlsvi instead take approximate posterior samples nonlinear value function nonparametric bootstrap references abbasi-yadkori yasin szepesv´ari csaba. regret bounds adaptive control linear quadratic systems. journal machine learning research proceedings track brafman ronen tennenholtz moshe. r-max general polynomial time algorithm near-optimal reinforcement learning. journal machine learning research dann christoph brunskill emma. sample complexity episodic ﬁxed-horizon reinforcement learning. advances neural information processing systems gabillon victor ghavamzadeh mohammad scherrer bruno. approximate dynamic programming ﬁnally advances performs well game tetris. neural information processing systems liang yitao machado marlos talvitie erik bowling michael state control atari games using shallow reinforcement learning. corr abs/. http//arxiv.org/ abs/.. lsvi algorithm iterates backwards time periods planning horizon iteration ﬁtting value function immediate rewards value estimates next period. value function ﬁtted least-squares note vectors satisfy algorithms produced synthesizing boltzmann exploration \u0001-greedy exploration lsvi presented algorithms algorithms temperature parameters boltzmann exploration \u0001-greedy exploration control degree random perturbations distort greedy actions. computational results suggest that coupled generalization rlsvi enjoys levels efﬁciency beyond achieved boltzmann \u0001-greedy exploration. leave open problem establishing efﬁciency guarantees contexts. stimulate thinking topic forth conjecture. conjecture reward distributions unique support φhθh rk×h satisfying exists polynomial poly would hope algorithm generalizes bound depend number states actions. instead dependence number basis functions. appendix present empirical results consistent conjecture. present full details algorithm generates random coherent basis functions rsa×k algorithm standard notation indexing vector elements. rm×n write element column. placeholder repesent entire axis that example ﬁrst column algorithm generating random coherent basis input output rsa×k sample rhsa×k stack rhsa form projection ψ−ψt sample rhsa×k project rhsa×k scale reshape reshape rh×sa×k return rsa×k reason rescale value function step algorithm resulting random basis functions similar scale completely arbitrary choice scaling exactly replicated similar rescalings robustness figures present cumulative regret ﬁrst episodes several orders magnitude combinations parameters learning remains remarkably stable. scaling number bases figure demonstrated rlsvi seems scale gracefully number basis features chain length figure reproduce reults chains several different lengths. highlight overtrend present local polynomial regression chain length. roughly speaking numbers features number episodes required learning appears increase linearly number basis features. however marginal increase basis features seems decrease almost plateau number features reaches maximum dimension problem approximate polynomial learning simulation results empirically demonstrate learning appears polynomial inspired results figure present learning times different together quadratic regression separately large values lead slowers learning since bayesian posterior concentrates slowly data. however stochastic domains found choosing small might cause rlsvi posterior concentrate quickly fail sufﬁciently explore. similar insight previous analyses thompson sampling matches ﬂavour theorem small memory requirement note that apart number holes every feature reward positive integer inclusive. number holes positive integer could store information transitions every possible action using less memory. present results rlsvi ﬁxed corresponds bayesian linear regression known noise variance algorithm actually found slightly better performance using bayesian linear regression inverse gamma prior unknown variance. conjugate prior gaussian regression known variance. since improvements minor slightly complicates algorithm omit results. however believe using wider prior variance robust application rather picking speciﬁc figure show rlsvi outperforms lsvi even highly tuned annealing scheme for\u0001. however results much extreme didactic version mini-tetris. make tetris board rows pieces. problem much difﬁcult highlights need efﬁcient exploration extreme way. figure present results mini-tetris environment. expected example highlights beneﬁts rlsvi lsvi dithering. rlsvi greatly outperforms lsvi even tuned schedule. rlsvi learns faster reaches higher convergent policy. algorithm present natural adaptation rlsvi without known episode length still regular episodic structure. algorithm experiments tetris. lsvi algorithms formed way. algorithm simply approximates time-homogenous value function using bayesian linear regression. found discount rate helpful stability rlsvi lsvi. subsection describe linear contextual bandit algorithm. linear contextual bandit algorithm similar rlsvi without backward value propagation feature rlsvi. straightforward linear contextual bandit algorithm aims learn myopic policy. algorithm speciﬁed algorithm notice algorithm implemented incrementally hence computationally efﬁcient. computational study basis functions rlsvi algorithm parameters brieﬂy discuss couple possible extensions version rlsvi proposed algorithm incremental version computationally efﬁcient. addresses continual learning inﬁnite horizon discounted markov decision process. sense rlsvi shares much lsvi distinguished approach exploration extensions share much least-squares q-learning note algorithm batch learning algorithm sense that episode though σlh’s computed incrementally needs past observations compute ¯θlh’s. thus per-episode compute time grows undesirable algorithm applied many episodes. problem derive incremental rlsvi updates ¯θlh’s σlh’s using summary statistics past data observations made recent episode. approach computing setting ¯θl+h step size controls inﬂuence past observations ¯θlh. ˜θlh’s computed actions chosen based algorithm another approach would simply approximate solution numerically random sampling stochastic gradient descent similar works non-linear architectures per-episode compute time incremental algorithms episode-independent allows deployment large scale. hand expect batch version rlsvi data efﬁcient thus incur lower regret. finally propose version rlsvi inﬁnite-horizon time-invariant discounted mdps. discounted identiﬁed sextuple discount factor. deﬁned similarly ﬁnite horizon case. speciﬁcally time state action selected subsequent state sampled reward sampled also denote optimal state value function denote optimal action-contingent value function. note depend case. including states actions rewards observed previous time steps well state space action space discount factor possible prior information. consider scenario agent prior knowledge lies within linear space spanned generalization matrix r|s||a|×k. version rlsvi continual learning presented algorithm note values computed algorithm previous time period. initialize similarly algorithm algorithm randomly perturbs value estimates directions signiﬁcant uncertainty incentivize exploration. note random matrices consecutive perturbations differ slightly. period computed greedy action selected. avoiding frequent abrupt changes perturbation vector important allows agent execute multi-period plans reach poorly understood state-action pairs. goal subsection prove lemma reproduced below dirichlet begin lemma recapping basic equivalences stochastic optimism. lemma following equivalent properties well known theory second order stochastic dominance re-derived using elementary integration parts. second order stochastic dominant order prove lemma ﬁrst prove intermediate result shows particular beta distribution optimistic prove result ﬁrst state basic result gamma distributions. lemma independent random variables gamma gamma previous section showed matched beta distribution would optimistic dirichlet show normal random variable optimistic complete proof lemma unfortunately unlike case beta dirichlet quite difﬁcult show optimism relationship gaussian beta directly. instead make appeal stronger dominance relationship single-crossing cdfs. deﬁnition real-valued random variables cdfs respectively. single-crossing dominates crossing point that repeated application mean value theorem want prove cdfs cross sufﬁcient prove pdfs cross twice interval. strategy show mechanical calculus known densities pdfs cross twice lament proof stands laborious attempts elegant solution unsucessful. remainder appendix devoted proving double-crossing property manipulation pdfs different values write density normal density beta respectively. know boundary represents left right limits respectively. since densities postive interval consider pdfs instead. since injective increasing could show solutions interval would done. instead attempt prove even stronger condition solution interval. necessary actually want show sufﬁcient easier deal since ignore annoying constants. therefore done proof. case dealt similarly. g... convex function case know convex function solve proved statement. write convenience. attempt solve shown somehow large enough away certianly proved result g... final region produce ﬁnal argument even remaining region pdfs double crossing. argument really different before difﬁculty enough look derivatives likelihoods need bound normalizing constants bounds. symmetry problem sufﬁce consider case result follows similarly. region interest know since concave therefore totally maximum whole region consider regions know consideration tails root segment must least three crossings. three crossings second derivative difference must least root region. however know convex show cannot possible. similar argument complete proof lengthy amounts calculus. using results previous sections complete proof lemma gaussian beta dominance possible piecing together lemma lemma completes proof lemma imagine much elegant general proof method available future work.", "year": 2014}