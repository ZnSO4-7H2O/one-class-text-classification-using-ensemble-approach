{"title": "Learning 6-DOF Grasping Interaction with Deep Geometry-aware 3D  Representations", "tag": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "abstract": "This paper focuses on the problem of learning 6-DOF grasping with a parallel jaw gripper in simulation. We propose the notion of a geometry-aware representation in grasping based on the assumption that knowledge of 3D geometry is at the heart of interaction. Our key idea is constraining and regularizing grasping interaction learning through 3D geometry prediction. Specifically, we formulate the learning of deep geometry-aware grasping model in two steps: First, we learn to build mental geometry-aware representation by reconstructing the scene (i.e., 3D occupancy grid) from RGBD input via generative 3D shape modeling. Second, we learn to predict grasping outcome with its internal geometry-aware representation. The learned outcome prediction model is used to sequentially propose grasping solutions via analysis-by-synthesis optimization. Our contributions are fourfold: (1) To best of our knowledge, we are presenting for the first time a method to learn a 6-DOF grasping net from RGBD input; (2) We build a grasping dataset from demonstrations in virtual reality with rich sensory and interaction annotations. This dataset includes 101 everyday objects spread across 7 categories, additionally, we propose a data augmentation strategy for effective learning; (3) We demonstrate that the learned geometry-aware representation leads to about 10 percent relative performance improvement over the baseline CNN on grasping objects from our dataset. (4) We further demonstrate that the model generalizes to novel viewpoints and object instances.", "text": "additional shape signals focus learning boost performance. notion using shape geometry pioneered grasping research inspired approaches propose concept deep geometry-aware representation grasping. approach ﬁrst build mental representation recognizing reconstructing geometry scene rgbd input demonstrated figure built-in geometry-aware representation hallucinate local view object’s geometric surface gripper perspective directly useful grasping interaction. contrast black-box models explicit notion geometry prior shape-based grasping approaches approach following features performs shape reconstruction auxiliary task; hallucinates local view using learning-free physical projection operator; explicitly reuses learned geometry-aware representation grasping outcome prediction. work design end-to-end deep geometryaware grasping network learning representation. geometry-aware network components shape generation network grasping outcome prediction network. shape generation network learns recognize reconstruct geometry scene image encoder voxel decoder. image encoder transforms rgbd input high-level geometry representation involves shape location orientation object. voxel decoder network takes geometry representation outputs occupancy grid object. hallucinate local view gripper perspective propose novel learningfree image projection layer similar building abstract— paper focuses problem learning -dof grasping parallel gripper simulation. compared existing approaches specialized three-dimensional grasping using -dof grasping model allows robot learn richer grasping interactions given less physical constraints; hence potentially enhancing robustness grasping robot dexterity. however learning -dof grasping challenging high dimensional state space difﬁculty collecting large-scale data many variations object’s visual appearance propose notion geometry-aware representation grasping based assumption knowledge geometry heart interaction. idea constraining regularizing grasping interaction learning geometry prediction. speciﬁcally formulate learning deep geometry-aware grasping model steps first learn build mental geometry-aware representation reconstructing scene rgbd input generative shape modeling. second learn predict grasping outcome internal geometry-aware representation. learned outcome prediction model used sequentially propose grasping solutions analysis-by-synthesis optimization. contributions fourfold best knowledge presenting ﬁrst time method learn -dof grasping rgbd input; build grasping dataset demonstrations virtual reality rich sensory interaction annotations. dataset includes everyday objects spread across categories additionally propose data augmentation strategy effective learning; demonstrate learned geometry-aware representation leads relative performance improvement baseline grasping objects dataset. demonstrate model generalizes novel viewpoints object instances. learning interact grasp objects fundamental challenging problem robot learning combines perception motion planning control. problem challenging requires understanding geometry also requires estimating physical properties weight density friction. furthermore requires invariance illumination object location viewpoint. handle this current data-driven approaches hundreds thousands examples learn solution. upon shape generation network grasping outcome prediction network learns produce grasping outcome based action current visual state learned geometry-aware representation. unlike end-to-end multi-objective learning framework existing datadriven grasping pipelines viewed models without shape generation component. require either additional camera capture global object shape extra processing steps object detection patch alignment. furthermore methods learn constrained grasp space typically either -dof -dof. relax constraint learn fully generalized -dof grasp poses. built large database consisting everyday objects around grasping demonstrations virtual reality human augmented synthetic interactions. object collect grasping attempts parallel gripper right-handed users. attempt record pre-grasping status includes location orientation object gripper well grasping outcome acquire sufﬁcient data learning generate additional synthetic data perturbing gripper location orientation human demonstrations using pybullet plan open-source dataset well tensorﬂow implementation deep geometry-aware grasping network. build database rich visual sensory data grasping annotations virtual reality system propose data augmentation strategy effective learning modest amount human demonstrations. proposed geometry-aware grasping network able learn shape well grasping outcome signiﬁcantly better models without notion geometry. common approach robotic grasping detect optimal grasping location visual inputs earlier work studied planar grasping problem using visual features extracted sensory input adopted logistic regression ﬁtting optimal grasping location visual features. lenz proposed two-step detection pipeline deep neural networks. pinto gupta built robotic system learning grasping large-scale real-world trialand-error experiments. work deep convolutional neural network trained hours robotic grasping data collected system. fine-grained grasping planning control often involves modeling object shape modeling dynamics robot hands local surface modeling work focused analytic modeling robotic grasps known object shape information varley proposed shape completion model reconstructs occupancy grid robotic grasping partial observations ground-truth occupancy grid used model training. comparison approach require full volume supervision training similar work learned shapecontext help predict grasps. unlike work shape build virtual global geometric representation along local gripper centric model sequentially propose evaluate grasp proposals. investigated hand pose estimation robotic grasping decoupling contact points hand conﬁguration parametrized object shape. building upon compositional aspect everyday objects vahrenkamp proposed part-based model robotic grasping better generalization novel object. recently effort also made building dexnet large-scale point cloud database planar grasping addition general robotic grasping several recent work investigated semantic task-speciﬁc grasping contrast existing learning frameworks applied robotic grasping approach features providing method learn grasping network rgbd input end-to-end deep learning framework generative shape modeling leveraging predictive grasping interaction learning-free projection layer links observations object shape allows learning shape representation without explicit volume supervision. able recognize reconstruct geometry given rgbd input important step grasping planning. formulation propose reconstruction occupancy grid encodes shape location orientation object geometry-aware representation. previous work generate normalized occupancy grids centered origin. formulated geometry-aware representation differs takes location orientation consideration volume rh×w related transformation matrix here width height depth input output volume respectively. deﬁne dense sampling step channel-wise ﬂattening step follows following section network trained match predictions groundtruth please note in-network projection layer learning-free implements exact ray-tracing algorithm without extra free parameters involved. learning reconstruct geometry single-view rgbd sensory input challenging task computer vision shape ambiguity. solution enforce viewpoint-invariant constraint based shape consistency across multi-view observations constraint image taken viewpoint shares representation image taken another viewpoint. thus provide multi-view observations scene model training single-view observation rgbd input testing. given series observations ii··· scene reconstruction formulated {ii}n similarly projection operator i-th viewpoint v×pi depth camera transformation matrix corresponding viewpoint respectively. finally deﬁne shape reconstruction loss lshape demonstrated previous work learn interactions demonstrations prediction future state metric understanding physical interaction. grasping setting deﬁne rgbd input current state pre-grasping parameters action grasping outcome future state. future prediction task solved learning functional invariant camera viewpoint distance given rgbd input corresponding occupancy grid task learn functional mapping simply following formulation previous work supervision obtained reasonable quality generating normalized volumes using thousands shape instances. however problem setting methods would require even data considering entangled factors shape location orientation. recent breakthroughs reconstructing geometry supervision suggest quality reconstructed geometry good previous work supervision; learned representation generalizes better novel settings previous work supervision; learning becomes efﬁcient supervision. inspired ﬁndings tackle reconstruction weakly supervised manner without explicit shape supervision. innetwork projection layer introduced shape learning masks unfortunately silhouette usually insufﬁcient supervision signal reconstruct objects concave parts reasons chose depth signal shape reconstruction. additionally rgbd sensors commonly available robot platforms. enable depth supervision shape generation component propose novel in-network opengl projection operator utilizes depth supervision signal learning reconstruct geometry. formulate projection operation transforms shape depth camera transformation matrix here camera transformation matrix decomposes camera intrinsic matrix camera rotation matrix camera translation vector. implementation also silhouette object mask learning. empirically additional objective makes learning stable efﬁcient. similar transformer networks proposed projection considered performing dense sampling input volume output volume ﬂattening spatial output across dimension. input volume again point output baseline refer method mapping baseline grasping interaction prediction model basis several recent state-of-the-art grasping methods using deep learning work managed learn mapping either millions randomly generated grasps additional view eye/hand perspective additional processing steps object detection image alignment. comparison geometry-aware model endto-end architecture constrains prediction geometry information. learn reconstruct geometry argue local surface view directly inferred viewpoint-invariant geometry-aware representation dlocal here treat gripper virtual camera transformation matrix world-space coordinates given pregrasping parameters addition local view geometry-aware representation provides global view scene takes shape prior location orientation object consideration. finally given current observation proposed action inferred shape representation geometry−aware i×a×v functional mapping binary outcome. implement components proposed previous sections introduce deep grasping network composed shape generation network outcome prediction network. shape generation network convolutional shape encoder deconvolutional shape decoder followed global projection layer. shape encoder network takes rgbd images resolution corresponding -by- camera view matrices input; network outputs identity units intermediate representation. shape decoder deconvolutional neural network outputs voxels resolution implemented projection layer transforms voxels back foreground object silhouettes depth maps input resolution here purpose generative pre-training learn viewpoint invariant units object segmentation depth prediction. outcome prediction network convolutional state encoder fully connected outcome predictor additional local shape projection layer. state encoder takes rgbd input resolution corresponding actions outputs state units intermediate representation. outcome predictor takes current state geometry features consideration. note local dense-sampling transforms surface area around gripper ﬁngers foreground silhouette depth resolution plan releasing tensorﬂow implementation deep geometry-aware grasping network. section describes data collection augmentation process well experimental evaluation grasping outcome prediction grasping trials. demo video approach available https//goo.gl/gpzphm. human demonstrations collected grasping demonstrations seven categories objects include total everyday objects. collect grasping demonstrations vive system virtual reality assign target objects randomly right-handed users total human grasps demonstrated average grasps object randomly split objects three sets make sure covers seven categories deep geometry-aware model. adopted twostage training procedure first pre-trained shape generation model using adam optimizer learning rate iterations mini-batch size batch sample random viewpoints purpose multiview supervision training time. observed setting stable shape generation performance compared single-view training. addition used loss foreground depth prediction loss silhouette prediction coefﬁcients second stage ﬁne-tuned state encoder outcome predictor using adam optimizer learning rate iterations mini-batch size used cross-entropy objective function since grasping prediction formulated binary classiﬁcation task. experiments models trained using workers parameter servers asynchronized updates. baseline geometry-aware model adopt convolutional encoder-decoder architecture residual connections. bottleneck layer dimensional vector. evaluate quality shape generation model visualizing geometry representations shape encoder decoder network. evaluations used single-view rgbd input corresponding camera view matrix input network. shown figure shape generation model able generate detailed occupancy grid single-view input without supervision training. shown figure model demonstrates reasonable generalization quality even novel object instances. advantage shape generation component obtain additional local geometry information geometry-aware representation. difference work related work require additional camera gripper. geometry part intermediate representation hallucinate local geometry running projection gripper’s perspective understand advantages shape generation component visualized intermediate local geometry projected generated occupancy grid. shown figure shape generation component provides accurate local geometry estimation useful grasping outcome prediction. evaluate actual advantages grasping outcome prediction modeling computed average classiﬁcation accuracy demonstrations novel object instances diverse observation viewpoints. human demonstration generated data augmentation order collect sufﬁcient grasping demonstrations model training evaluation generate synthetic grasps perturbing human demonstrations using pybullet signiﬁcantly helps increasing number grasps adding perturbations demonstrations. total collected grasping demonstrations covering objects. figure illustrates examples objects dataset successful unsuccessful grasping trials human demonstrations synthetic grasps successful unsuccessful trials generated augmentation process. details described appendix. demonstration take snapshot pregrasping scene randomly setting camera distance draw camera target position normal distribution mean object center desired variance furthermore camera around target position different azimuth angles adjust elevation different angles finally save state scene without gripper used shape pre-training; referred static scene throughout paper. include elevation angles training leaving rest evaluation. deep baseline. adopt current datadriven framework grasping baseline removing shape encoder shape decoder deep geometryaware grasping model. baseline interpreted grasping quality without additional view top-down camera. trained model using adam optimizer learning rate iterations mini-batch size ablation study added view static scene additional input channel baseline model didn’t observe signiﬁcant improvements. fig. visualization shape generation single-view rgbd. performance training objects. performance testing objects. local geometry inference generated occupancy grid. synthetic grasps perturbation computed average accuracy grasps investigate model performance viewpoint changes repeat evaluation experiment four different elevation angles parallel computing resources evaluation entire evaluation took day. results summarized table table overall deep geometry-aware model consistently outperforms deep baseline grasping outcome classiﬁcation. teapot plate comparatively challenging categories outcome prediction since teapot irregular shape parts plate fairly shape. comes novel elevation angles deep geometry-aware model less affected especially categories teapot plate viewpoint-invariant shape understanding crucial. improve classiﬁcation accuracy grasping outcome natural question whether improvement used guide better grasping planning. given grasping proposal seed conducted grasping planning sequentially adjusting grasping pose guided deep grasping network grasp success. optimization step performed cross-entropy method follows. initialized failure grasp order force model better grasping pose. obtain gradient direction space sample random directions selected based score returned neural network repeat iterations success conducted grasping explore evaluation baseline deep geometry-aware model. account variations observation viewpoints initial seeds repeat evaluation eight times testing demonstration dataset reported average success rate iterations baseline experimental results demonstrated improved performance outcome prediction thanks generative shape modeling. guided geometry-aware representation obtained better planning analysis-by-synthesis grasping optimization. repeat random grasping exploration times. draw grasp pose normal distribution mean value demonstrated grasp pose desired variance switch simulation mode open gripper ﬁnger place drawn random pose close gripper lift check whether object still gripper ﬁngers. based outcome pose list successful failed grasps reset simulation environment previously stored state. protocol collected total grasping synthetic grasps based human demonstrations. shown figure visualize gripper positions using colored dots green ones representing successful grasps ones representing failure grasps. fig. visualization grasping optimization based grasping prediction output. selected three representative steps grasping optimization represents failure grasp green represents successful grasp. shown table guided geometry-aware model performance consistently better baseline model. believe improved performance comes explicit modeling geometry intermediate representation deep geometry-aware model. model achieved signiﬁcant improvement bottle category since bottle shape relatively easy reconstruct. improvement bowl category less signiﬁcant partly difﬁculty predicting concave shape novel object instances. figure demonstrates example grasping planning trajectories different objects. baseline less robust compared deep geometry-aware model likely transit side object side clear notion geometry. work studied problem learning grasping interaction deep geometry-aware representation. proposed deep geometry-aware network performs shape generation well grasping outcome prediction learning-free physical projection layer. compared vahrenkamp westkamp yamanobe aksoy asfour. part-based grasp planning familiar objects. humanoid robots ieee-ras international conference pages ieee zhang freeman tenenbaum. learning probabilistic latent space object shapes generative-adversarial advances neural information processing systems modeling. pages song khosla zhang tang xiao. shapenets deep representation volumetric shapes. proceedings ieee conference computer vision pattern recognition pages perspective transformer nets learning single-view object reconstruction without supervision. advances neural information processing systems pages yang reed m.-h. yang lee. weakly-supervised disentangling recurrent transformations view synthesis. advances neural information processing systems pages girdhar fouhey rodriguez gupta. learning predictable generative vector representation objects. european conference computer vision pages springer johns leutenegger davison. deep learning grasp function grasping gripper pose uncertainty. intelligent robots systems ieee/rsj international conference pages ieee levine pastor krizhevsky ibarz quillen. learning hand-eye coordination robotic grasping deep learning large-scale data collection. international journal robotics research page mahler liang niyaz laskey doan ojea goldberg. dex-net deep learning plan robust grasps synthetic point clouds analytic grasp metrics. arxiv preprint mahler pokorny roderick laskey aubry kohlhoff kr¨oger kuffner goldberg. dex-net cloud-based network objects robust grasp planning using multi-armed bandit model correlated rewards. ieee international conference robotics automation pages ieee maturana scherer. voxnet convolutional neural network real-time object recognition. intelligent robots systems ieee/rsj international conference pages ieee pinto gandhi y.-l. park gupta. curious robot learning visual representations physical interactions. european conference computer vision pages springer pinto gupta. supersizing self-supervision learning grasp robotics automation tries robot hours. ieee international conference pages ieee rezende eslami mohamed battaglia jaderberg heess. unsupervised learning structure images. advances neural information processing systems pages", "year": 2017}