{"title": "Analyzing Knowledge Transfer in Deep Q-Networks for Autonomously  Handling Multiple Intersections", "tag": ["cs.LG", "cs.AI"], "abstract": "We analyze how the knowledge to autonomously handle one type of intersection, represented as a Deep Q-Network, translates to other types of intersections (tasks). We view intersection handling as a deep reinforcement learning problem, which approximates the state action Q function as a deep neural network. Using a traffic simulator, we show that directly copying a network trained for one type of intersection to another type of intersection decreases the success rate. We also show that when a network that is pre-trained on Task A and then is fine-tuned on a Task B, the resulting network not only performs better on the Task B than an network exclusively trained on Task A, but also retained knowledge on the Task A. Finally, we examine a lifelong learning setting, where we train a single network on five different types of intersections sequentially and show that the resulting network exhibited catastrophic forgetting of knowledge on previous tasks. This result suggests a need for a long-term memory component to preserve knowledge.", "text": "fig. investigate types knowledge transfer different types intersections. knowledge handle intersection represented deep q-network trained simulation data. every intersection makes decision either wait intersection analyze directly copying deep network intersection tuning previously trained deep network intersection whether tuning destroys intersection knowledge reverse transfer lifelong learning multiple intersections single deep neural network. paper focus knowledge type intersection represented deep q-network translates types intersections first look direct copy well network trained task performs task second analyze performance network initialized task tuned task compares randomly initialized network exclusively trained task third investigate reverse transfer network pre-trained task ﬁne-tuned task preserves knowledge task finally explore training network tasks sequentially lifelong learning scenario. paper organized follows. providing brief literature survey section present problem formulation section examining various knowledge sharing strategies section explaining experimental setup section present results section concluding section vii. abstract— analyze knowledge autonomously handle type intersection represented deep qnetwork translates types intersections view intersection handling deep reinforcement learning problem approximates state action function deep neural network. using trafﬁc simulator show directly copying network trained type intersection another type intersection decreases success rate. also show network pre-trained task ﬁne-tuned task resulting network performs better task network exclusively trained task also retained knowledge task finally examine lifelong learning setting train single network different types intersections sequentially show resulting network exhibited catastrophic forgetting knowledge previous tasks. result suggests need long-term memory component preserve knowledge. companies increasing spending automated driving technology recent years good cause promises greatly reduce accidentrelated fatalities increase productivity society whole. although technology made important strides last couple years current technology still ready large scale roll-out. urban environments especially pose signiﬁcant challenges unpredictable nature pedestrians vehicles city trafﬁc. handling intersections safely efﬁciently challenging problems urban rule-based methods provide predictable method handle intersections. however rule-based intersection handling approaches don’t scale well becomes increasingly harder design hand-crafted rules scene complexity increases. moreover algorithm designer come hand-crafted rules parameters different types intersections. different intersection types mean single multi-lane right left turns forward passing. goal research explore machine learning based method generalizes various types intersections. machine learning particularly deep learning growing ﬁeld tremendous impact applications computer vision speech recognition language translation increasingly used decision making. model vehicle learning agent learns positive negative experiences reinforcement learning framework. recently increased interest using machine learning techniques control autonomous vehicles. imitation learning policy learned human driver online planners based partially observable monte carlo planning shown handle intersections existence accurate generative model available markov decision processes used ofﬂine address intersection problem additionally machine learning used optimize comfort safe trajectories machine learning greatly beneﬁted training large amounts data. helps system learn general representations prevents ﬁtting based incidental correlations sampled data. absence huge datasets training multiple related tasks give similar improvement gains large breadth research investigated transferring knowledge system another machine learning general reinforcement learning speciﬁcally training time sample complexity deep networks make transfer methods particularly appealing prompted depth investigation help understand behavior recent work deep reinforcement learning looked combining networks different tasks share information efforts made enable uniﬁed framework learning multiple tasks changes architecture design modiﬁed objective functions address known problems like catastrophic forgetting view intersection handling reinforcement learning problem deep q-network learn state action value q-function. assume vehicle intersection path known network tasked choosing actions wait every time step. agent decides follows intelligent driver model keeping distance vehicles front. reinforcement learning agent state takes action according policy parameterized agent transitions state receives reward collection deﬁned experience typically formulated markov decision process states actions agent execute state transition function reward function discount factor adds preference earlier rewards provides stability case inﬁnite time horizons. mdps follow markov assumption probability transitioning state given current state action independent previous states actions q-learning deﬁnes optimal action-value function maximum expected return achievable following policy given state action maxπe. follows dynamic programming properties bellman equation state values q∗a) known optimal strategy select maximizes expected value γq∗a) q∗a)|sa] deep q-learning optimal value function approximated neural network parameters learned using bellman equation iterative update qia)|sa] minimizing error expected return state-action value predicted network. gives loss individual experience deep q-network practice poor estimate early make learning slow since many updates required propagate reward appropriate preceding states actions. make learning efﬁcient n-step return γrt+ +··· γn−rt+n− maxat+n learning ε-greedy policy followed selecting random action probability promote exploration otherwise greedily selecting best action maxaq according current network. order improve effectiveness random exploration make dynamic frame skipping. frequently repeated actions required several time steps. recently shown allowing agent select actions extended time periods improves learning time agent example rather explore trial error build series learning steps eight time steps appropriate amount time agent wait pass agent need discover wait eight steps action appropriate. dynamic frame skipping viewed simpliﬁed version options recently starting explored deep community. uses convolutional neural network convolution layers fully connected layer. ﬁrst convolutional layer ﬁlters stride second convolution layer ﬁlters stride fully connected layer nodes. layers leaky relu activation functions ﬁnal linear output layer outputs single action wait action experience replay buffers allotment experiences. learning iteration samples batch experiences. since experience replay buffer imposes off-policy learning able calculate return state-action pair trajectory prior adding step replay buffer. allows train directly n-step return forgo added complexity using target networks state space represented grid global coordinates. epsilon governing random exploration reward used successfully navigating intersection collision step cost. case forgetting retention describes amount previous knowledge retained network training task. value difﬁcult deﬁne formally since must exclude relevant knowledge source tasks obtained training target task additionally retention might include aspects quantiﬁable weight conﬁgurations network. example network might exhibit catastrophic forgetting fact retained weight conﬁguration greatly reduces training time needed retrain source task. difﬁculty deﬁning retention deﬁne empirical retention difference direct copy tuned direct copy network. interested sharing knowledge different driving tasks. sharing knowledge different tasks reduce learning time create general capable systems. ideally knowledge sharing extended involve system continues learn deployed enable system accurately predict appropriate behavior novel situations examine behavior various knowledge sharing strategies autonomous driving domain. demonstrate extent transfer show difference tasks train network single source task iterations. unmodiﬁed network evaluated every task. repeat process using different task source task. starting network trained iterations source task tune network additional iterations second target task. iterations demonstrates substantial learning suboptimal order emphasize possible beneﬁts gained transfer. fine tuning demonstrates jumpstart asymptotic performance described taylor stone. network tuned target task evaluate performance network source task. training later task improves performance earlier task known reverse transfer. known lifelong learning process learning multiple tasks sequentially goal optimize performance every task combination information previous tasks used jumpstart learning task. reciprocal fashion learning task potentially reﬁne existing knowledge previous tasks. single system handles tasks system able handle broader problems likely generalize better problems. examine deep q-network performs learning sequence tasks. order tasks encountered impact learning several groups investigated effects ordering experiments task ordering demonstrates forgetting hold ﬁxed experiments. experiments using sumo simulator open source trafﬁc simulation package. package allows users model road networks road signs trafﬁc lights variety vehicles pedestrians simulate trafﬁc conditions different types scenarios. importantly purpose testing evaluation autonomous vehicle systems sumo provides tools facilitate online interaction vehicle control. trafﬁc scenario users control vehicle’s position velocity acceleration steering direction fig. retention. table examines performance network source task training target task. subtract baseline performance network trained exclusively target task. shows much source task training exists training target task. left network substantially better either single lane tasks challenge task. fine tuning figure shows tuning results. nearly cases pre-training different network gives signiﬁcant advantage jumpstart several cases asymptotic beneﬁt well. tuned networks re-applied source task performance looks similar direct copy shown figure reverse transfer performance source task drops tuning trend positive improvement compared direct copy. indicates information retained network. figure shows retention task pair showing percentage gain resulting initialization. left challenge tasks less overlap tasks state space possible aspects initialization left unchanged might explain simulate motion using basic kinematics models. trafﬁc scenarios like multi-lane intersections setup deﬁning road network along speciﬁcations control trafﬁc conditions. simulate trafﬁc users control types vehicles road paths vehicle density departure times. trafﬁc cars follow control motion. sumo randomness simulated varying speed distribution vehicles using parameters control driver imperfection simulator runs based predeﬁned time interval controls length every step. experiments using different intersection scenarios right left left forward challenge. scenarios depicted figure right scenario involves making right turn forward scenario involves crossing intersection left scenario involves making left turn left scenario involves making left turn across lanes challenge scenario involves crossing lane intersection. sumo trafﬁc simulator conﬁgured lane miles hour speed. begins stopped position. time step equal seconds. number steps trial capped steps equivalent seconds. trafﬁc density probability vehicle emitted randomly second. depart probability lane tasks. direct copy table shows results network trained task applied another. instance network trained different task better network trained matching task several tasks achieve similar performance transfer. particularly single lane tasks related consistently performers single lane tasks. additionally challenging multi-lane settings appear connected fig. fine-tuning comparison. network task initialized network different task. colored lines indicate initialization network. black line indicates performance network trained random initialization. initializing network network trained another task almost always advantageous. notice jumpstart beneﬁt every tested example observe several asymptotic improvements. fig. direct copy reverse transfer. axis denotes test condition. black bars show performance single task learning. light gray bars show average performance network trained task tested another. drop performance demonstrates difference tasks. dark gray indicates average performance reverse transfer network trained task ﬁne-tuned task evaluated task drop performance indicates catastrophic forgetting networks exhibit retention initial task. largest amount retention tasks. hypothesis supported fact task exhibits retention since tasks least overlap. lifelong learning results lifelong learning experiment shown figure every task initially beneﬁts learning ﬁrst task although performance left challenge settings beneﬁt less. cases training different task helps point training hurts tasks. example training approximately fig. lifelong learning results. background color indicates task trained. solid line depicts test performance different task. dotted lines indicate performance network trained single task equivalent period time. standard deviation across runs indicated envelope. several points learning process network shows evidence forgetting previous tasks. overall afﬁnity single lane tasks multi-lane tasks. training challenge task starts left beneﬁts single lane tasks exhibit catastrophic forgetting. training left task helps single lane tasks challenge decreases performance. single lane tasks. training right task much detrimental effect multi-lane tasks either forward left. suspect right turns ignore lanes trafﬁc matters tasks. overall negative effects catastrophic forgetting negate many positive effects transfer. paper view vehicle learning agent reinforcement learning setting analyze knowledge handling type intersection represented deep q-network translates types intersections. investigated compared four different transfer methods different intersections direct copy tuning reverse transfer lifelong learning. results several conclusions. first found success rates consistently network trained task directly tested task second network initialized network task ﬁnetuned task generally performed better randomly initialized network trained task third network initialized task ﬁne-tuned task tested back task performed better network directly copied task task finally examine lifelong learning domain train single network handle intersection scenarios show resulting network exhibited catastrophic forgetting previous task knowledge. brechtel gindele dillmann probabilistic decisionmaking uncertainty autonomous driving using continuous pomdps intelligent transportation systems ieee international conference razavian azizpour sullivan carlsson features off-the-shelf astounding baseline recognition computer vision pattern recognition workshops mar. kirkpatrick pascanu rabinowitz veness desjardins rusu milan quan ramalho grabska-barwinska overcoming catastrophic forgetting neural networks arxiv preprint arxiv. kulkarni narasimhan saeedi tenenbaum hierarchical deep reinforcement learning integrating temporal abstraction intrinsic motivation advances neural information processing systems mnih kavukcuoglu silver rusu veness bellemare graves riedmiller fidjeland ostrovski human-level control deep reinforcement learning nature vol. isele rostami eaton using task features zeroshot knowledge transfer lifelong learning proceedings international joint conference artiﬁcial intelligence ruvolo eaton active task selection lifelong machine learning. aaai conference artiﬁcial intelligence krajzewicz erdmann behrisch bieker recent development applications sumo–simulation urban mobility international journal advances systems measurements vol. krauss microscopic modeling trafﬁc investigation collision free vehicle dynamics ph.d. dissertation deutsches zentrum fuer luft-und raumfahrt", "year": 2017}