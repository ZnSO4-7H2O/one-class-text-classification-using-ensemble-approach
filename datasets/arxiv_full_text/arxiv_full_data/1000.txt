{"title": "Common Variable Learning and Invariant Representation Learning using  Siamese Neural Networks", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "We consider the statistical problem of learning common source of variability in data which are synchronously captured by multiple sensors, and demonstrate that Siamese neural networks can be naturally applied to this problem. This approach is useful in particular in exploratory, data-driven applications, where neither a model nor label information is available. In recent years, many researchers have successfully applied Siamese neural networks to obtain an embedding of data which corresponds to a \"semantic similarity\". We present an interpretation of this \"semantic similarity\" as learning of equivalence classes. We discuss properties of the embedding obtained by Siamese networks and provide empirical results that demonstrate the ability of Siamese networks to learn common variability.", "text": "consider statistical problem learning common source variability data synchronously captured multiple sensors demonstrate siamese neural networks naturally applied problem. approach useful particular exploratory data-driven applications neither model label information available. recent years many researchers successfully applied siamese neural networks obtain embedding data corresponds semantic similarity. present interpretation semantic similarity learning equivalence classes. discuss properties embedding obtained siamese networks provide empirical results demonstrate ability siamese networks learn common variability. many machine learning signal processing methods separate desired variability undesired variability variability interest known data supervised learning methods applied. alternatively data model available classic signal processing techniques used discard noise. assumption knowledge available always realistic; many cases particular exploring data clear identify represent interesting part phenomenon. example analysis epileptic seizures interested recovering patterns activity drive multiple areas brain even though patterns masked massive nonlinear non-additive eﬀects local activity model. purpose manuscript propose approach separating desired variability irrelevant variability absence data model label information. proposed approach purely unsupervised based coincidence co-occurrence source information desired variability recovered. speciﬁcally assume data measured multiple sensors desired variability recorded sensors irrelevant eﬀects sensor-speciﬁc idiosyncrasies coincidence recognize common source variability. learning performed using siamese neural networks. modern form siamese neural networks proposed obtain embedding input data corresponds semantic similarity euclidean proximity points embedding space implies points semantically similar. siamese networks used various tasks dimensionality reduction learning invariant representations learning hashing functions manuscript provides formal model mathematical interpretation embedding siamese networks obtain representing equivalence classes. training siamese networks requires collection pairs similar dissimilar input objects; refer choice training collections pairing. many works siamese networks pairing based information given classiﬁed samples knowledge data model. example pairing objects based class membership resulting representation shown invariant input transformations pose illumination faces images. siamese networks applied obtain dimensional embedding input data; approach based computing similarity graph input data; experiments graph computed using euclidean distance knowledge model generating data. assume model. furthermore discussed section euclidean proximity input space might always capture desired similarity. siamese networks used obtain hash maps given similarity measure; experiments manuscript rely class membership pairing. siamese networks applied obtain representations images people correspond pose invariant undesired variability identity clothing; case similarity calculation based images people imitating positions shown ﬁxed seed images i.e. known data model. variants siamese architectures applied textual objects; cases availability pairs objects labeled degree similarity assumed. contributions follows ﬁrst formulation common variable learning problem interpret siamese network speciﬁcally demonstrate siamese networks fact trained recognize equivalence classes equivalence relation deﬁned common variability learn. another embedding obtained siamese network corresponds quotient space relation. second demonstrate siamese networks exploit coincidence eﬃcient approach separating desired variability undesired variability absence neither data model label information. organization manuscript follows section describe common variable learning problem. section show coincidence used train siamese network brieﬂy review typical training algorithm. mathematical properties embedding siamese networks obtain discussed section section present experiments using synthetic data demonstrating common variable indeed learned siamese networks. brief conclusions presented section purpose section illustrate motivation learning coincidence. simpliﬁed example adapted example appears simple image processing problem easily treated domain knowledge intentionally refrain using domain knowledge order demonstrate siamese networks work without domain knowledge. experimental setup presented figure three objects yoda bulldog bunny placed spinning tables object spins independently objects. cameras used take simultaneous snapshots camera whose ﬁeld view includes yoda bulldog camera whose ﬁeld view includes bulldog bunny. setting rotation angle bulldog common hidden variable denote common variable manifested snapshot taken cameras. rotation angle yoda denote sensor-speciﬁc source variability manifested snapshots taken camera rotation angle bunny denote sensor-speciﬁc source variability manifested camera three rotation angles hidden sense measured directly snapshots taken cameras. given snapshots cameras goal obtain parametrization relevant common hidden variable i.e. rotation angle bulldog ignore superﬂuous sensor-speciﬁc idiosyncratic variables speciﬁc task performed speciﬁc knowledge problem masking irrelevant objects reducing problem learning variable. indeed variable inﬂuencing measurements various methods explore geometry however interesting case known a-priori abstract value inﬂuences measurements measurements need images scenario presented section simpliﬁed case data-driven modeling exploration systems; represents underlying phenomena would like investigate although don’t model phenomena variables inﬂuence measurements. particular labeled examples furthermore know advance possible isolate masking certain pixels isolating superﬂuous fact multiple instances measurements taken simultaneously cameras; case don’t know anything nature images know state cameras took snapshots. manuscript argue siamese networks used context proper adaptations attempt recover representation ignores sensor speciﬁc demonstrate siamese networks used unsupervised data-driven exploration phenomena little domain knowledge based mainly examples obtained simultaneously without prior model without class labels target values regression. figure dataset experiment. experiment setting yoda bulldog bunny spinning tables cameras. bottom sample snapshots taken cameras time. pictures bulldog state denote range respectively; ranges embedded high dimensional space. refer random variables measurement sensor measurement sensor respectively. i-th realization system consists hidden triplet corresponding pair measurements hidden available directly observable. note functions realization dataset composed pairs corresponding measurements ideally would like construct function recovers every every would however since unknown cannot expect recover precisely interested function recovers scaling bi-lipschitz transformation. particular require section discuss rationale siamese networks context problem formulated above brieﬂy review siamese networks algorithm context. algorithm given section typical variant siamese network training algorithm element approach implementation learning coincidence manifested construction positive negative datasets described sections rationale order satisfy equations would like depend invariant value crucial information provided dataset fact i-th pair functions value idea information learn maps implement function network denote function elements correspond realization common variable addition given dataset dneg negative pairs elements correspond diﬀerent realizations idea dpos input introduce network positive pair require outputs identical output whereas introduce dneg input require output network negative pair diﬀerent output process implemented computes approximate algorithm given dataset dpos npos pairs form realizations hence instance taken time diﬀerent sensors. refer dataset positive dataset. addition construct second dataset referred negative dataset dneg contains nneg false pairs form diﬀerent realizations diﬀerent values practice suﬃces suﬃciently high probability. dneg explicitly available training data siamese network obtained without assuming class membership data model label information. entire construction pairing based coincidence i.e. fact data measured multiple sensors sensors capturing variability interest. typical architecture siamese network presented figure network composed networks single output unit connected output layer networks. accept samples respectively. vector containing weight parameters positive pairs dpos would like close zero thus similarly negative pairs dneg would network trained implement proposed functions network bears superﬁcial resemblance classiﬁer determines whether measurements diﬀerent sensors share value however classiﬁers need construct representation common variable goal work. addition since class membership information entire training based coincidence proposed training approach purely unsupervised. said that experiments useful measure classiﬁcation accuracy network proxy quality learning. speciﬁcally since output network ranges classiﬁcation threshold estimating whether real fake pair. observe function satisﬁes yields value member equivalence class moreover function satisﬁes also yields diﬀerent value members diﬀerent equivalence classes practice continuity functions continuity computation operations networks here samples close would similar representations representation smooth. informally sensors might measure diﬀerent modalities audio signals images other framework proposed allows compare diﬀerent modalities terms common variable. several works propose anns learn representations inputs measured sources possibly diﬀerent modalities example audio video images texts works focus learning shared representation containing information modalities demonstrate modality provides information modality. works particularly interested recovering input modality input another modality example autoencoder. problem learning shared representation objects captured multiple sensors possibly diﬀerent modalities discussed also diﬀusion maps used obtain representation. however well sensor speciﬁc variability removed. manuscript discard modality-speciﬁc attributes learn common hidden variable underlies modalities. variable semantic similarity little similarity input space. example problem diﬀerent snapshots taken camera supposed equivalent bulldog happens place. however bunny actually larger dominant object appear state snapshots snapshots diﬀerent input space. words snapshots diﬀerent input space equivalent. similarly snapshots bunny appears similar state bulldog similar input space although equivalent sense common variable wish capture. therefore similarity input space might little similarity common variable consequently inappropriate tool collecting positive pairs context. siamese networks distance input space deﬁne pairs regularize distance output space; similarity input space demonstrated useful dimensionality reduction. however type pairing regularization cannot discard superﬂuous variables cannot distinguish superﬂuous variables common ones. therefore pairing based simultaneous measurements measurements available advantageous recovering common variable distilling hidden underlying phenomena. natural approach discovering common information dataset paired observations canonical correlation analysis ability standard discover information limited since considers linear transformations inputs. among linear versions deep-cca architecture proposed bares resemblance siamese network architecture. manuscript follow diﬀerent approach dataset architecture resembles siamese networks deep-cca. experiment section suggests approach proposed manuscript better suits common variable learning problem. invariant representation learning problem examples pairs diﬀerent randomly selected group actions operating randomly selected element cases examples negative pairs neural networks invariant speciﬁc input transformations translation rotation proposed example. however networks often designed invariant speciﬁc well modeled transformations rather unknown transformations. section present experimental results common variable learning. experiments involve synthetic datasets generated common variable deﬁned explicitly demonstrate embedding obtained indeed corresponds quotient space deﬁned common variable. also demonstrate siamese networks used learn invariant representations. gradient descent momentum dropout using l-bfgs optimization algorithms compute gradients using standard backpropagation classiﬁcation accuracy report measured test consisting positive negative examples introduced network training. experiment revisit setup described section here snapshot taken camera snapshot taken camera dataset dpos consists pairs snapshots taken simultaneously camera camera respectively. dataset dneg constructed pairing snapshots taken color images; positive negative diﬀerent times. samples examples presented figure training sets dpos dneg consisted examples each. three layers hidden layers network units output layers units. joint network trained using l-bfgs. -dimensional. used standard dimensionality reduction algorithms process output purpose visualization processing; figure present reduced representation obtained using diﬀusion maps found clearer representation obtained using pca. closed curve smooth transitions color embedding demonstrate algorithm recovered good representation figure dataset experiment. sample examples. positive example snapshots taken simultaneously containing diﬀerent views bulldog rotation angle. negative example snapshots cameras taken time hence correspond rotation angle bulldog. bottom embedding dataset. color point corresponds true common hidden variable i.e. rotation angle bulldog. catenation rotations image arbitrary angles measurement sensor dimensional vector vxizi entries vxizi sint+zi) deterministic function determines frequency sine determines phase. words common variable determines rotation left image ﬁrst sensor frequency sine second sensor; sensor speciﬁc variables rotation angle right image phase sine three layers units each. dpos dneg consisted figure present diﬀusion embedding outputs colored true value common variable smooth transition color furthermore points figure correspond output indistinguishable points correspond outputs words data goal learn maps rotation-invariant. group rotations images rotation degrees. used images caltech- dataset constructed datasets rotated images. positive dpos composed samples gi.si gi.si instances base image rotated randomly chosen angles sample negative dataset composed positive negative examples dataset ﬁrst layer weights presented networks three layers each; joint network trained using l-bfgs. figure modalities dataset. positive example dataset. measurement sensor concatenation rotations image angles measurement sensor sine frequency determined phase determined bottom embedding images audio signals test modalities experiment; color corresponds true value common variable data diﬀerent modalities mapped space parametrized common variable. figure rotation-invariance experiment. left sample examples generated caltech- dataset rotation-invariance experiment. positive example image rotation. negative example diﬀerent images rotated diﬀerent angles. right ﬁrst layer features rotation-invariance random variables deep algorithm maximized. implemented deep network applied dataset section network structure used experiment section diﬀusion embedding obtained last layer representation deep network presented figure observe experiment position along embedded manifold correspond value common variable i.e. rotation angle bulldog; moreover additional analysis indicated representation obtained deep experiment reﬂects sensor speciﬁc superﬂuous variables would like discard. figure diﬀusion embedding obtained deep network dataset. color point corresponds value common hidden variable embedding captures common variable would smooth transition color; embedding correspond value common hidden variable. manuscript presented siamese neural networks solution statistical problem common variable learning. demonstrated siamese neural networks learn equivalence relations input space. demonstrated coincidence used recovery common variables absence model labeled data using examples measurements equivalent related appropriate form coincidence using examples measurements equivalent unrelated. addition demonstrated experiments presented manuscript carefully designed illustrate theoretical arguments regarding embedding obtained siamese networks representing common variable regarding limited domain knowledge. demonstrated works domain knowledge available used designing network architecture example samples images natural convolutional networks.", "year": 2015}