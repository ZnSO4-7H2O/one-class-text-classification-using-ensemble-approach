{"title": "COBRA: A Fast and Simple Method for Active Clustering with Pairwise  Constraints", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Clustering is inherently ill-posed: there often exist multiple valid clusterings of a single dataset, and without any additional information a clustering system has no way of knowing which clustering it should produce. This motivates the use of constraints in clustering, as they allow users to communicate their interests to the clustering system. Active constraint-based clustering algorithms select the most useful constraints to query, aiming to produce a good clustering using as few constraints as possible. We propose COBRA, an active method that first over-clusters the data by running K-means with a $K$ that is intended to be too large, and subsequently merges the resulting small clusters into larger ones based on pairwise constraints. In its merging step, COBRA is able to keep the number of pairwise queries low by maximally exploiting constraint transitivity and entailment. We experimentally show that COBRA outperforms the state of the art in terms of clustering quality and runtime, without requiring the number of clusters in advance.", "text": "clustering inherently ill-posed often exist multiple valid clusterings single dataset without additional information clustering system knowing clustering produce. motivates constraints clustering allow users communicate interests clustering system. active constraint-based clustering algorithms select useful constraints query aiming produce good clustering using constraints possible. propose cobra active method ﬁrst over-clusters data running k-means intended large subsequently merges resulting small clusters larger ones based pairwise constraints. merging step cobra able keep number pairwise queries maximally exploiting constraint transitivity entailment. experimentally show cobra outperforms state terms clustering quality runtime without requiring number clusters advance. introduction clustering inherently subjective single dataset often clustered multiple ways different users prefer different clusterings. subjectivity motivations constraint-based clustering methods setting exploit background knowledge obtain clusterings aligned user’s preferences. often knowledge given form pairwise constraints indicate whether instances cluster traditional constraint-based clustering systems constraints assumed given priori practice pairs queried often selected randomly. contrast active clustering method decides pairs query. typically active work introduces active constraint-based clustering method named constraint-based repeated aggregation differs existing approaches several ways. first aims maximally exploit constraint transitivity entailment properties allow deriving additional constraints given constraints. this actual number pairwise constraints cobra works typically much larger number pairwise constraints queried user. secondly cobra introduces assumption exist small local regions data grouped together potential clusterings. clarify this consider example clustering images people taking different poses least natural clustering targets data might want cluster based identity pose. appropriate feature space expects images agree criteria close. need consider instances individually cluster targets user might interested cobra aims group instances super-instance treated jointly clustering process. substantially reduces number pairwise queries. thirdly cobra inherently active method constraints selected execution algorithm itself constraint selection algorithm execution intertwined. contrast existing approaches consist component selects constraints another uses clustering. experiments show cobra outperforms state-ofthe-art active clustering methods terms clustering quality runtime. furthermore distinct advantage require knowing number clusters beforehand competitors many realistic clustering scenarios number known running algorithm wrong number clusters often results signiﬁcant decrease clustering quality. discuss related work constraint-based clustering section section elaborate ideas cobra describe method detail. present experimental evaluation section conclude section background related work existing constraint-based methods extensions well-known unsupervised clustering algorithms. constraints either adapted clustering procedure learn similarity metric constraint-based extensions developed clustering algorithms including k-means spectral clustering dbscan basu introduce strategy select informative constraints prior performing single constraint-based clustering algorithm. show active constraint selection improve clustering performance. several selection strategies proposed since based classic approach uncertainty sampling. cobra also chooses pairs query consider active method experiments compare methods setting. note however cobra quite different existing methods active constrained clustering active learning general. cobra selecting pairs query inherent clustering procedure whereas methods selection strategy optional considered separate component. core cobra related hierarchical clustering follows procedure sequentially trying merge closest clusters. constraints used hierarchical clustering different ways. davidson example present algorithm clustering hierarchy consistent given constraints. nogueira propose active semi-supervised hierarchical clustering algorithm based merge conﬁdence. also related work campello developed framework extract given hierarchy clustering consistent given constraints. difference cobra starts super-instances i.e. small clusters produced kmeans merging decision settled pairwise constraint. idea working small number representatives instead individual instances used before different purposes. example speed unsupervised spectral clustering whereas reduce number pairwise queries. constraint-based repeated aggregation constraint-based clustering algorithms produce clustering dataset resembles unknown target clustering close possible. algorithm cannot query cluster labels directly query relation pairs instances. must-link constraint obtained instances cluster label cannot-link conseveral strategies used exploit constraints clustering. figure illustrates them. naive strategy query pairwise relations construct clusters sets instances connected must-link constraint though clearly good strategy scenario allows formulate baseline improvements. always results perfect clustering previous strategy improved exploiting constraint transitivity entailment well known properties constraint-based clustering must-link constraints known transitive thus every time constraint queried added constraints transitivity entailment applied expand set. strategy illustrated figure exploiting transitivity entailment signiﬁcantly reduces number pairwise queries needed obtain clustering without loss clustering quality. order constraints queried strongly inﬂuences number constraints derived. general better obtain must-link constraints early future query involving instances connected must-link also applies others. suggests querying closest pairs ﬁrst likely belong cluster hence connected mustlink constraint. strategy illustrated figure previous strategies obtain perfect clustering require high number queries makes inapplicable reasonably sized datasets. reduce number queries cobra groups similar instances super-instances clusters representatives i.e. medoids. assumes instances within super-instance connected must-link constraint. clustering medoids cobra uses previously discussed strategies querying closest pairs exploiting transitivity entailment. strategy illustrated figure results substantial reduction number queries. always result perfect clustering possible instances within particular super-instance grouped together w.r.t. target clustering. table illustrates extent improvements described reduces number queries. perform extensive evaluation quality clusterings cobra produces section algorithmic description figure illustration different querying strategies. colors indicate desired clustering. solid green lines indicate must-link ones cannot-link constraints. dashed lines indicate derived constraints color code. number next solid line indicates ordering queried constraints whereas number next dashed line indicates constraint number constraint derived. querying constraints exploiting entailment transitivity results querying constraints. querying closest pairs ﬁrst results constraints. introducing super-instances results queries. table table shows total number pairwise relations several datasets well number pairwise queries required exploiting transitivity entailment additionally querying closest pairs ﬁrst last column shows number pairwise queries cobra super-instances. instances clus. {xi}n tered. instances ﬁrst over-clustered disjoint subsets namely super-instances {si}ns over-clustering obtained running kmeans signiﬁcantly larger actual number clusters. super-instance represented medoid forming super-instance representatives sns}. pairwise queries performed super-instance representatives. goal cobra cluster representatives cluster super-instance representatives conceptually contains points corresponding super-instances. number clusters unknown priori determined clustering procedure. initially clusters containing single superinstance representative shown line algorithm clusters merged subsequent loop. next cobra loops pairs clusters checks whether merged. starts selecting closest pair super-instance representatives clusters asks whether cluster case clusters merged while-loop restarted case pair representatives added cannot-link constraints inner loop continues inspecting next pair clusters. execution stops clusters complete merge done anymore. number super-instances number queries exact number queries cobra needs depends extent querying closest pairs ﬁrst leads mustlink constraints actual number clusters. thus difﬁcult determine execution. however estimate terms lower upper bound posed beforehand thus serves means understand number queries might needed. figure compares lower bound actual number queries needed cobra clustering tasks tasks actual number pairwise queries relatively close lower bound. possible smaller number queries suggested lower bound happens single super-instance contains instances actual clusters experimental evaluation section discuss experimental evaluation cobra. existing constraint-based algorithms compare cobra following state-of-the-art constraint-based clustering algorithms mpckmeans hybrid constraint-based extension k-means uses metric learning adapted clustering procedure combining within-cluster squares cost violating constraints objective. implementation available wekaut package. constrained spectral clustering based spectral clustering optimizes modiﬁed objective also takes constraint violation account. code provided authors page. important note that contrast cobra cosc mpckmeans require number clusters input parameter. experiments true number clusters provided algorithms. many clustering applications however number typically known beforehand. thus cosc mpckmeans advantage. active selection strategies algorithms combined following active selection strategies minmax starts exploration phase neighborhoods cannot-links sought. subsequent consolidation phase neighborhoods expanded selecting uncertain instances determining neighborhood membership means pairwise constraints. width parameter gaussian kernel percentile distribution pairwise euclidean distances also based concept neighborhoods contrast minmax iterative method data clustered multiple times clustering used determine next pairs query. xiong show typically outperforms minmax terms clustering quality cost increased runtime. datasets experiment datasets iris wine dermatology hepatitis glass ionosphere optdigits ecoli breast-cancer-wisconsin segmentation column parkinsons spambase sonar yeast. datasets used earlier work constraint-based clustering optdigits contains digits handwritten digits data table wins losses aggregated clustering tasks. count report average margin cobra wins counts marked asterisk differences signiﬁcant according wilcoxon test duplicate instances removed datasets data normalized also perform experiments faces dataset contains images persons taking different poses different expressions without sunglasses. hence dataset target clusterings identity pose expression sunglasses. extract -value feature vector image running pre-trained inception-v network storing output second last layer. finally also cluster newsgroups text data. dataset consider tasks clustering documents newsgroups related topics clustering documents newsgroups different topics ﬁrst extract tf-idf features next apply latent semantic indexing reduce dimensionality brings total datasets clustering tasks deﬁned experimental methodology cross-validation procedure highly similar ones used e.g. folds instances aside test set. clustering algorithm entire dataset query pairwise constraints instances training set. evaluate quality resulting clustering compute adjusted rand index instances test set. measures similarity clusterings case produced constraint-based clustering algorithm indicated class labels. means clustering better random indicates perfect clustering. ﬁnal score algorithm particular dataset computed average folds. exact number pairwise queries known beforehand cobra super-instances generally results queries. evaluate cobra varying amounts user input super-instances. fold execute following steps make sure cobra queries pairs instances training medoid superinstance calculated based training instances super-instance rare event superinstance contains test instances merged nearest super-instance contain training instances. minmax selection strategies pairs involving instance test simply excluded selection. results results clustering tasks summarized tables table reports wins losses competitors. shows cobra tends produce better clusterings competitors. difference cosc signiﬁcant according wilcoxon test whereas difference mpckmeans not. table shows average ranks cobra competitors. friedman aligned rank test power friedman test number algorithms comparison indicates super-instances differences rank cobra competitors signiﬁcant using posthoc holm test table dataset algorithms ranked table shows average ranks super-instances. algorithms difference cobra signiﬁcant according friedman aligned rank test posthoc holm test marked asterisk. running competitors different numbers queries previous experiments competitors number queries cobra required cobra cannot ﬁxed beforehand. might wonder whether constitutes advantage cobra whether conclusions also hold competitors figure comparing clustering qualities cobra competitors wider range numbers constraints. cobra black marker shows average number questions cobra required particular number super-instances average show results minmax selection strategy conclusions drawn also hold npu. number super-instances cobra different numbers constraints. answer question cobra wider range superinstances competitors numbers constraints. figure shows results datasets conclusions drawn also hold others. ﬁrst conclusion datasets cobra outperforms competitors experiments discussed above also larger numbers constraints such results discussed previous section representative. secondly clustering quality quickly plateaus many datasets especially true mpckmeans explained strong spherical bias. contrast several datasets cobra cosc produce increasingly better clusterings constraints given selecting right number clusters cobra require specifying number clusters beforehand. often produces correct ﬁnds close correct one. contrast cosc mpckmeans require specifying experiments discussed above given correct found experimentally that majority cases running different reduces clustering quality often signiﬁcant amount. occasionally different improves results case typically small margin. results omitted paper lack space. runtime figure compares runtimes cobra competitors. cobra consists steps constructing super-instances grouping together form clusters. fast k-means used ﬁrst step second step applied small super-instances. seen figure runtimes mpckmeansminmax comparable cobra surprising built k-means. contrast coscminmax signiﬁcantly expensive built spectral clustering. used selection strategy mpckmeans cosc become much slower requires several runs clustering algorithm. figure shows runtime method datasets. ﬁgure shows results cobra super-instances ﬁgures superinstances comparable. cobra mpckmeans-minmax highly scalable always ﬁnish seconds. conclusion introduced cobra active constraint-based clustering method. unlike methods built extension existing unsupervised algorithm. instead cobra inherently constraint-based. selection strategy aims maximally exploit constraint transitivity entailment. experiments show cobra outperforms state terms clustering quality runtime even methods advantage given right number clusters. future work investigate whether appropriate number super-instances determined automatically e.g. incrementally reﬁning necessary. acknowledgements toon craenendonck supported agency innovation science technology flanders sebastijan dumanˇci´c supported research fund leuven noam shental aharon bar-hillel tomer hertz daphna weinshall. computing gaussian mixture models using equivalence constraints. advances neural information processing systems christian szegedy vincent vanhoucke sergey ioffe jonathon shlens zbigniew wojna. rethinking inception architecture computer vision. corr abs/. ulrike luxburg robert williamson isabelle guyon. clustering science art? workshop unsupervised learning transfer learning jmlr workshop conference proceedings kiri wagstaff claire cardie seth rogers stefan schroedl. constrained k-means clustering background knowledge. proc. eighteenth international conference machine learning pages eric xing andrew michael jordan stuart russell. distance metric learning application clustering side-information. advances neural information processing systems pages qianjun marie desjardins kiri wagstaff. active constrained clustering examining spectral eigenvectors pages springer berlin heidelberg berlin heidelberg donghui ling huang michael projordan. fast approximate spectral clustering. ceedings sigkdd international conference knowledge discovery data mining pages york acm. sugato basu misha bilenko raymond mooney. probabilistic framework semiproceedings supervised clustering. sigkdd international conference knowledge discovery data mining page january mikhail bilenko sugato basu raymond mooney. integrating constraints metric learning semi-supervised clustering. proc. international conference machine learning pages july ricardo campello davoud moulavi arthur zimek j¨org sander. framework semi-supervised unsupervised optimal extraction clusters hierarchies. data mining knowledge discovery davidson ravi. using instance-level constraints agglomerative hierarchical clustering theoretical empirical results. data mining knowledge discovery pages jason davis brian kulis prateek jain suvrit inderjit dhillon. informationtheoretic metric learning. proceedings international conference machine learning icml pages york acm. salvador garca alberto fernndez julin luengo francisco herrera. advanced nonparametric tests multiple comparisons design experiments computational intelligence data mining information sciences experimental analysis power. special issue intelligent distributed information systems.", "year": 2018}