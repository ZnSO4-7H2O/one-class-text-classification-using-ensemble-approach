{"title": "Learning for Adaptive Real-time Search", "tag": ["cs.AI", "cs.LG"], "abstract": "Real-time heuristic search is a popular model of acting and learning in intelligent autonomous agents. Learning real-time search agents improve their performance over time by acquiring and refining a value function guiding the application of their actions. As computing the perfect value function is typically intractable, a heuristic approximation is acquired instead. Most studies of learning in real-time search (and reinforcement learning) assume that a simple value-function-greedy policy is used to select actions. This is in contrast to practice, where high-performance is usually attained by interleaving planning and acting via a lookahead search of a non-trivial depth. In this paper, we take a step toward bridging this gap and propose a novel algorithm that (i) learns a heuristic function to be used specifically with a lookahead-based policy, (ii) selects the lookahead depth adaptively in each state, (iii) gives the user control over the trade-off between exploration and exploitation. We extensively evaluate the algorithm in the sliding tile puzzle testbed comparing it to the classical LRTA* and the more recent weighted LRTA*, bounded LRTA*, and FALCONS. Improvements of 5 to 30 folds in convergence speed are observed.", "text": "abstract. real-time heuristic search popular model acting learning intelligent autonomous agents. learning real-time search agents improve performance time acquiring reﬁning value function guiding application actions. computing perfect value function typically intractable heuristic approximation acquired instead. studies learning real-time search assume simple valuefunction-greedy policy used select actions. contrast practice high-performance usually attained interleaving planning acting lookahead search non-trivial depth. paper take step toward bridging propose novel algorithm learns heuristic function used speciﬁcally lookahead-based policy selects lookahead depth adaptively state gives user control trade-off between exploration exploitation. extensively evaluate algorithm sliding tile puzzle testbed comparing classical lrta* recent weighted lrta* bounded lrta* falcons. improvements folds convergence speed observed. keywords real-time heuristic search planning learning on-line learning adaptive lookahead search. problem formulation complete search methods ida* produce optimal solutions admissible heuristics. price optimality substantial running time often exponential problem dimension limits applicability complete search tasks large state spaces limited time action. body research real-time search trades optimality solution running time consequently techniques widely applied real-time path-planning game-playing control general decision-making. approaches building boundedly rational realtime decision-making agents interleave lookahead-based deliberation backing information select action accordingly efforts increase rationality agents fall three categories better hand-engineered automatically derived heuristic functions various lookahead control state pruning methods search extension techniques specialized hardware paper focus ﬁrst ways increasing rationality autonomous decision-making agents. namely consider framework learning real-time heuristic search henceforth referred lrts. attractive model decision-making autonomous agents since allows agent improve performance repeated trials environment. hence another view lrts light on-line reinforcement learning learning ability enables gradual improvement solution also allows agent incomplete model environment within non-stationary environment goals non-deterministic environment. consequently lrts model successfully applied numerous practical tasks including moving target search problems robot navigation localization robot exploration several important attributes lrts algorithms agents ﬁnal solution quality measured relation optimal solution quality. upper bounds cost solution agent eventually converges important performance guarantees; convergence speed measured total number actions agent executes converges ﬁnal solution; resource bounds imposed amount memory agent requires converge ﬁnal solution. crucial autonomous agents hardware particularly limited; exploration versus exploitation control. ﬁnding high-quality solutions requires extensive exploration state space usually trade-off ﬁnal solution quality speed convergence resources required. thus important attribute lrts algorithm whether trade-off user-controlled; convergence stability stems related exploration exploitation trade-off. namely fast convergence better solutions requires aggressive exploration state space. lrts agent usually demonstrates optimism face uncertainty optimistically abandoning already found solution eager exploration unknown regions state space. result solution quality vary orders magnitude consecutive trials. argued unacceptable depending application; integration learning planning critical real-world lrts agents high performance crucially depends extensive lookahead constitutes planning step cycle contrary popular belief deeper lookahead necessarily improve decision-making quality thus adaptive control lookahead integration heuristic function update appear promising. related research consider several lrts algorithms light attributes discussed previous section. lrta* early still widely used lrts algorithm. certain assumptions guaranteed convergence optimal solution ﬁnite number trials. optimality-seeking algorithm lrta* require prohibitive computational resources unstable convergence provide exploration exploitation control learn heuristic function tailored lookahead module several extensions lrta* proposed. weighted lrta* combination lrta* inadmissible initial heuristic function. heuristic required within factor optimal heuristic function parameter controls suboptimality ﬁnal solutions weighted lrta* converges solution cost within optimal. consequently larger values lead faster convergence smaller memory requirements. update bounded lrta* uses additional memory maintain upper bound true heuristic function explored state. user-set parameter used together upper bound control amount exploration thereby increasing convergence stability. hand bounded lrta* incurs additional memory running time overhead well additional complexity acquiring non-trivial upper bound reﬁning search. additionally optimality seeking nature lead intractable storage requirements. consequently combination weighted bounded lrta* extensions needed. resulting interplay parameters effects algorithm properties subject future research finally still explicit consideration lookahead learning module bounded lrta* clear well weighting upper-bounding techniques work lookahead search non-trivial depth. falcons maintains additional heuristic function lower bound cost current state initial state. designed speed convergence retaining optimality ﬁnal solution therefore require prohibitive storage. additionally offer user control exploration exploitation trade-off convergence stability. finally falcons make speciﬁc considerations lookahead learning module. novel learning real-time search algorithm intuition. past efforts often focused either gaining closer approximations true distance goal better management planning phase recognize tightly coupled heuristic function useful insomuch guidance decision-making module agent. consequently instead learning manually engineering heuristic functions approximate true distance goal well design learning module speciﬁcally adaptive lookahead search. intuition behind novel lrts algorithm since optimal solutions inherently intractable many non-trivial domains allow algorithm converge suboptimal solutions. results signiﬁcant speed-ups convergence savings memory. time similar weighted lrta* upper-bound converged solutions. deep lookahead wasteful states optimal action determined shallow search. thus algorithm adjusts lookahead depth adaptively based concept so-called traps introduced later paper. additionally execute variable-length sequence actions lookahead thereby reducing running time. propose tighter coupling heuristic function update lookahead modules updating heuristic function agent runs trap respect lookahead module. results fewer heuristic function updates smaller storage requirements. raps. lookahead based real-time agent rta* lrta* conducts lookahead ﬁxed depth expanding potentially large number states step. upon taking action updates heuristic function current state repeats process goal state found. effectively follows gradient heuristic function reaches local minimum. visualized ‘pit’ heuristic function surface heuristic values surrounding states together distances exceed heuristic value current state. lrta*-like agent continue moving inside ‘pit’ raising heuristic values ‘pit’ completely ‘ﬁlled’. ‘ﬁlling process’ take large number actions updates heuristic function. formalize ‘pits’ general concept γ-traps incorporate explicit efﬁcient trap detection recovery methods lrts algorithm. search problem. present algorithm search task needs formally deﬁned. lrts problem deﬁned tuple ﬁnite states; ﬁnite actions; cost function incurred cost taking action state initial state; goal states. adopt assumptions every action reversible every state every applicable action leads another state least goal state reachable true cost traveling state state denoted dist deﬁned minimal cumulative action cost agent incur traveling state true cost deﬁned minimal travel cost nearest goal minsg dist. heuristic approximation true cost called admissible h∗]. value state referred heuristic cost state depth child state state reachable minimum actions depth neighborhood state deﬁned state called γ-trap depth lacks child depth cost getting child adjusted plus heuristic cost child exceed heuristic cost current state ssd′ algorithm. figure presents lrts algorithm called γ-trap control policy outputting series actions state thus given search problem agent’s current state initialized heuristic function initialized admissible initial function γ-trap policy called repeatedly environment series actions returns used update current state goal state reached. process heuristic function updated policy well. goal state reached current problem-solving trial deemed completed current state reset initial location agent solve problem starting heuristic function previous problem-solving trial. convergence achieved problem-solving trial completed without updates heuristic function solution produced considered ﬁnal solution. cumulative cost actions ﬁnal solution cost. depth dmax speciﬁed user. iteration expansion process current state found non-trap lookahead search terminated series actions leading promising state within last expanded neighborhood returned ties resolved randomly. note operation exits policy call. agent move current state heuristic function updated. hand loop terminates naturally line current state γ-trap depth dmax. case heuristic function needs updated. order minimize number updates number actions agent takes ‘ﬁlling pit’ things. first increase heuristic maximum allowable amount line second backtrack previous state avoid wasteful wandering inside ‘pit’. note current state initial state backtracking performed current state changed. completeness ﬁnal solution quality. despite giving ﬁnal solution optimality γ-trap complete converges reasonable ﬁnal solution. precisely amount optimality loss upper-bounded theorem. search problem satisﬁes aforementioned conditions given admissible initial heuristic γ-trap ﬁnite solution every trial. additionally ﬁnite number repeated trials γ-trap converge ﬁnal solution cost upper-bounded proof available website cannot reproduced space constraints. also note actual performance γ-trap substantially better upper bound suggests. instance convergence experiments detailed average ﬁnal solution cost optimal substantially upper bound suggested theorem. mplemented. additionally since extended experimental setup used ﬁgures falcons bounded lrta* reported used comparisons well. extensions experiments reported follows used folds order measure standard deviation algorithms lookahead depth ﬁxed plies addition lookahead reported therein. extensive nature results space constraints paper subset ﬁndings reported herein. full results found technical report site. lrta* rta* reimplemented directly additionally version lrta* upgraded full hash-based lookahead pruning consistent table-based function forced monotonicity values upward updates implemented name ilrta*. used weighted lrta* experiments listed eilrta-x.x ﬁgures value likewise bounded lrta* listed bilrta-x.x graphs order investigate impact backtracking γ-trap also implemented version algorithm backtracking upon h-function update. instead algorithm always moves child node minimum dist value. plots backtracking version γ-trap listed gtrap-bt-x.x whereas non-backtracking version listed gtrap-x.x manhattan distance used initial heuristic function algorithms except weighted lrta* used manhattan distance multiplied convergence speed. within fold random hundred -puzzles generated. algorithms repeatedly puzzle instances convergence achieved convergence cost ﬁnal solution cost relative optimal averaged puzzles. mean standard deviation computed folds puzzles each. experiments repeated lookahead depths results lookahead shown figure data indicate γ-trap backtracking outperforms algorithms terms convergence cost approximately seven times better successful weighted lrta* times better falcons times better classic lrta*. time comparable others terms ﬁnal solution cost optimal case successful weighted lrta*. trend observed lookahead depths. memory requirements. paralleled experiments reported computed number -puzzles hundred puzzle published korf solved convergence within four million heuristic function values stored. results found figure lookahead one. experiments γtrap algorithm converging korf’s puzzles within four million nodes. time found highest quality solutions among algorithms fig. performance repeated trials lookahead one. number averaged independent folds random -puzzle instances each. puzzle solved convergence. total costs plotted graph ﬁnal solution costs relative optimal below. data bounded lrta* falcons taken computed fold number korf’s -puzzles solved convergence within four million stored heuristic values. bottom ﬁnal solution quality attained. lookahead ﬁxed one. convergence stability. even though γ-trap designed minimize oscillations solution cost convergence process evaluated standard algorithms along dimension. paralleling study adopted itae itse stability indices. reported paper deﬁned avgs cost cost) cost state soluavgs tion cost trial optimal cost. -puzzle results lookahead plotted figure similar improvements observed deeper lookaheads. data bounded lrta* falcons taken proper scaling factors itae itse observe γ-trap backtracking learns signiﬁcantly stable fashion algorithms including bounded lrta* speciﬁcally designed stable convergence. particular breaks past records nearly folds folds γ-trap without backtracking appears comparable weighted lrta*. first-trial performance. evaluated ﬁrst-trial performance lrts algorithms random sets -puzzles korf’s -puzzles. namely within single fold algorithm presented puzzles solve within thousand moves. experiments repeated independent folds lookahead results plotted figure included data reported shimbo ishida bottom graph. data suggest superior convergence speed memory requirements learning stability γ-trap come cost expensive ﬁrst-trial solution. behavior similar exhibited falcons. additionally note backtracking part γ-trap appears primary factor superior learning inferior ﬁrst-trial performance no-backtracking version similar weighted lrta* respects. future work conclusions learning real-time search appealing model study rational autonomous decision-making agents. real-time nature allows test various strategies interleaving planning acting learning feature enables agent cope unknown uncertain non-stationary environments. complete search methods ida* face intractability problem apparent even domains real-time search agents trade intractable optimal solutions effectively computable satisfactory solutions. paper discuss important attributes learning real-time search agent associated trade-offs. examine existing methods including lrta* extensions well falcons light desired properties. novel algorithm called γ-trap founded tighter integration adaptive lookahead-based planning learning modules. standard assumptions proved complete enjoys convergence satisfactory solutions. evaluate empirically existing methods show signiﬁcant improvement convergence speed stability well much reduced memory requirements. future research directions include deriving tighter upper bounds convergence speed stability ﬁnal solution cost controlling amount fig. learning stability folds hundred random -puzzles lookahead one. stability indices plotted. data bounded lrta* falcons collected single fold thus standard deviations plotted error bars. exploration ﬁrst trial automatic selection parameter applying γ-trap non-deterministic changing unknown environments. acknowledgements contributions valeriy bulitko david furcy masashi shimbo sven koenig holte rich korf stuart russell much appreciated. grateful support nserc university alberta alberta ingenuity centre machine learning. references korf real-time heuristic search. artiﬁcial intelligence russell wefald right thing studies limited rationality. press barto a.g. bradtke s.j. singh s.p. learning using real-time dynamic program. korf r.e. taylor l.a. finding optimal solutions twenty-four puzzle. proceedings thirteenth national conference artiﬁcial intelligence portland oregon aaai press press koenig simmons solving robot navigation problems initial pose uncertainty using real-time heuristic search. proceedings international conference artiﬁcial intelligence planning systems. bulitko greiner levner lookahead pathologies single agent search. proceedings international joint conference artiﬁcial intelligence. bulitko lookahead pathologies meta-level control real-time heuristic search.", "year": 2004}