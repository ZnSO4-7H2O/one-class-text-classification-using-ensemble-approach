{"title": "Sentence Pair Scoring: Towards Unified Framework for Text Comprehension", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "abstract": "We review the task of Sentence Pair Scoring, popular in the literature in various forms - viewed as Answer Sentence Selection, Semantic Text Scoring, Next Utterance Ranking, Recognizing Textual Entailment, Paraphrasing or e.g. a component of Memory Networks.  We argue that all such tasks are similar from the model perspective and propose new baselines by comparing the performance of common IR metrics and popular convolutional, recurrent and attention-based neural models across many Sentence Pair Scoring tasks and datasets. We discuss the problem of evaluating randomized models, propose a statistically grounded methodology, and attempt to improve comparisons by releasing new datasets that are much harder than some of the currently used well explored benchmarks. We introduce a unified open source software framework with easily pluggable models and tasks, which enables us to experiment with multi-task reusability of trained sentence model. We set a new state-of-art in performance on the Ubuntu Dialogue dataset.", "text": "review task sentence pair scoring popular literature various forms viewed answer sentence selection semantic text scoring next utterance ranking recognizing textual entailment paraphrasing e.g. component memory networks. argue tasks similar model perspective propose baselines comparing performance common metrics popular convolutional recurrent attentionbased neural models across many sentence pair scoring tasks datasets. discuss problem evaluating randomized models propose statistically grounded methodology attempt improve comparisons releasing datasets much harder currently used well explored benchmarks. introduce uniﬁed open source software framework easily pluggable models tasks enables experiment multi-task reusability trained sentence models. state-of-art performance ubuntu dialogue dataset. machine learning task often involves classifying sequence tokens sentence document i.e. approximating function large class problems involve classifying pair sentences typically function represents sort semantic similarity whether sequences semantically related. formulation allows measure tasks different topic relatedness paraphrasing degree entailment pointwise ranking task answer-bearing sentences next utterance classiﬁcation. work adopt working assumption exist certain universal type measures successfuly applied wide variety semantic similarity tasks case neural network models trained represent universal semantic comprehension sentences adapted given task ﬁne-tuning adapting output neural layer argument preferring pursuit fact sentence pair essentially complex label training sequence model therefore discern semantically rich structures dependencies. determining demonstrating universal semantic comprehension models across multiple tasks remains steps ahead since research landscape fragmented regard. model research typically reported within context single f-type task dataset requires sometimes substantial engineering work measurements possible results reported ways make meaningful model comparisons problematic. unify research within single framework employs task-independent models task-speciﬁc adaptation modules. improve methodology model evaluation terms statistics comparing strong non-neural baselines introducing datasets better characteristics. demonstrate feasibility pursuing universal task-independent models showing sec. outline possible speciﬁc tasks available datasets; sec. survey popular nonneural neural baselines context tasks; ﬁnally sec. present model-task evaluations within uniﬁed framework establish watermark future research well gain insight suitability models across variety tasks. sec. demonstrate transfer learning across tasks helpful powerfully seed models. conclude sec. summarizing ﬁndings outlining several future research directions. tasks aware phrased f-type problems listed below. general primarily focus tasks reasonably large realistically complex datasets freely available. contrary explicitly avoided datasets licence restrictions availability commercial usage. given factoid question candidate answer-bearing sentences encyclopedic style ﬁrst task rank higher sentences likely contain answer question. fundamentally information retrival task nature model performance commonly evaluated terms mean average precision mean reciprocial rank task popular research community thanks dataset introduced papers published february alone neural models substantially improving classical approaches based primarily parse tree edits. possibly main research testbed f-style task models. task also immediate applications e.g. question answering systems. candidate sentences often small quite uneven total number individual sentence pairs well questions relatively small. furthermore validation test small makes noisy performance measurements; splits also seem quite different nature questions since minimum correlation performance validation test sets calls parameter tuning procedures epoch selection early stopping question. alternative datasets wikiqa insuranceqa proposed encumbered licence restrictions. furthermore speculate suffer many problems above alleviate problems listed above introducing dataset yodaqa/large based extension curatedv question dataset denoisiﬁed mechanical turkers) candidate sentences retrieved yodaqa question answering system english wikipedia labelled matching gold standard answers passages. motivated another problem related yodaqa system also introduce another dataset wqmprop question sentences english labels properties make path within freebase knowledge base connects entity linked question correct answer. task evaluated identically previous task solutions often involving convolutional neural networks studied question answering literature sentences derived webquestions dataset extended moviese dataset questions property paths based freebase knowledge graph dump generated based entity linking exploration procedure yodaqa note wang yodaqa datasets however share common ancestry regarding questions overlaps even across train test splits. therefore mixing training evaluation wang yodaqa datasets within single model instance advisable. fig. compares critical characteristics datasets. furthermore apparent below baseline performances newly proposed datasets much lower suggests future model improvements apparent evaluation. proposed large-scale real-world ubuntu dialogue dataset f-style task ranking candidates next utterance chat dialog given dialog context. technical formulation task answer sentence selection semantically choosing best followup different concerns choosing answer-bearing sentence. recall top-ranked utterances either candidates reported; also propose reporting utterance aggregate measure. newly proposed ubuntu dialogue dataset based chat logs ubuntu community technical support channels contains casually typed interactions regarding computer-related problems. training consists individual labelled pairs evaluation followups given message ranked. sequences might tokens long. primary motivation using dataset size. numerical characteristics dataset shown table version dataset. research published dataset relies simple neural models. classic tasks boundary natural language processing artiﬁcial intelligence inference problem recognizing textual entailment given pair factual sentence hypothesis sentence determine whether hypothesis represents contradiction entailment neutral include current popular machine learning datasets task. stanford natural language inference snli dataset consists english sentence pairs facts based image captions pairs held validation test sets. sick- dataset introduced task semeval conference contrast snli geared speciﬁcally benchmarking semantic compositional methods aiming capture similarities purely language common knowledge level without relying domain knowledge named entities multi-word idioms; consists training pairs validation pairs testing pairs. sick- dataset also report results semantic textual similarity. task originates track semeval conferences involves scoring pairs sentences objective maximizing correlation manually annotated gold standard. goal text comprehension model focus neural network models architecture-wise. assume sequence transformed using n-dimensional word embeddings input employ models produce pair sentence embeddings sequences word embeddings unless noted otherwise siamese architecture used shares weights among sentenes. scorer module compares sentence embeddings produce scalar result connected model; speciﬁc task-model conﬁgurations either dot-product module module takes elementwise product embeddings feeds two-layer perceptron hidden layer width task follow score regression using motivation capture angle euclid distance multiple weighed sums. past literature uses absolute difference rather performed equally experiments adopted technical reasons. figure val.-test column shows inter-trial pearson’s validation test mrrs averaged across models benchmarked statistics shown evaluation portion datasets. last column includes relative standard deviation number candidate sentences question corresponds variation difﬁculty ranking task training ranking task bipartite ranking version ranknet objective; training task pearson’s formula objective; binary classiﬁcation tasks binary crossentropy objective. order anchor reported performance report several basic methods. weighed word overlaps metrics tf-idf inspired research provide strong baselines many tasks. treat query document counting number common words weighing appropriately. determined training set. metric represents baseline method using word embeddings proved successful e.g. simply taking mean vector word embedding sequence training weight matrix projects embeddings vector space tanh scorer compares them. training standard dropout applied input embeddings. simple extension deep averaging networks shown adequately replace much complex models tasks. dense perceptron layers stacked mean projection relu used instead tanh non-linearity word-level dropout used instead elementwise dropout. rectional network memory units direction; ﬁnal unit states summed across per-direction grus yield vector representation sentence. like baseline projection matrix applied representation ﬁnal vectors compared scorer. found applying massive dropout input output network helps avoid overﬁtting even early training. sentence-wide pooling layer also popular models processing sentences apply multi-channel convolution single-token channel convolutions -token channels convolutions each relu transfer function max-pooling whole sentence projection shared space scorer. dropout applied. rnn-cnn model aims combine recurrent convolutional networks using memory unit states token representation token convolutional network. inspired model allow model long-term dependencies model contextual representations words taking advantage pooling operation crisp selection gist sentence. parameters individual models dropout reducing number parameters using memory units direction. idea attention models attend preferrentially parts sentence building representation many ways model attention adopt model attn conceptually simple easy implement baseline. asymmetrically extends rnn-cnn model extra links output post-recurrent representation token determining attention level token weighed token vector elements focusing relevant segment transforming attention levels using softmax multiplying token representations attention levels convolutional network. convolutional network weights shared sentences convolutional network output projected applying scorer. used singlechannel convolution ﬁlters tokens wide. easily implement models dataset loaders task adapters modular fashion model easily f-type task created software package dataset-sts integrates variety datasets python dataset adapter pysts python library easy construction deep neural models semantic sentence pair scoring kerasts uses keras machine learning library framework available researchers open source github. dimensional glove embeddings matrix pretrained wikipedia gigaword keep adaptable training; words training included pretrained model initialized random vectors uniformly sampled match embedding standard deviation. chitti especially sentences contain named entities numeric data embedding available. workaround ensemble world overlap count neural model score typically used produce ﬁnal score. line idea answer sentence selection wang large datasets overlap baseline additional input scoring module prune scored samples based furthermore extend embedding input token several extra dimensions carrying boolean ﬂags bigram overlap unigram overlap whether token starts capital letter number. particular hyperparameters tuned primarily yodaqa/large dataset unless noted otherwise respective results table caption. apply regularization adam optimization standard parameters answer selection tasks train dataset epoch. training epoch best validation performance; sadly typically observe heavy overﬁtting training progresses rarely model later couple epochs. report model performance averaged across training runs consideration must emphasize randomness plays large role neural models terms randomized weight initialization stochastic dropout. example typical methodology reporting results wang dataset evaluate report single test tuning wang test empirical standard deviation across repeated runs attn model twice every successive papers pushing state-of-art dataset ∗marked sample fig. practical example phenomenon. furthermore complex tasks validation performance this reduces number training samples observed adverse effects that speeds training greatly models well typical information retrieval scenario fast pre-scoring candidates essential. allow comparison models therefore report also conﬁdence intervals model performance estimate determined empirical standard deviation using student’s t-distribution. fig. show cross-task performance models. observe effect analogous described dataset smaller models preferrable larger dataset allows models capture text comprehension task better. baselines provide strong competition ﬁnding ways ensemble models prove beneﬁcial future. especially apparent answer sentence selection datasets large number sentence candidates question. attention mechanism also highest impact kind information retrieval task. ubuntu dialog dataset even simple lstm model proposed setting beats baseline performance reported lowe rnn-cnn model establishes state-of-art beating three-hop memory network possible statistically determine relation models state-of-art wang answer sentence selection dataset. models clearly behind state-of-art tasks carefully tune parameters also employ data augmentation strategies like synonyme substitution might necessary good performance small datasets even using transfer learning. over larger number samples estimate converges normal distribution conﬁdence levels. note conﬁdence interval determines range true expected evaluation evaluation measured sample. sion trained model large ubuntu dialogue dataset transferred weights retrained model instance tasks. used model experiment conﬁguration dot-product scorer smaller dimensionality conﬁguration shown respective result tables ubu. consistently ranks best among best classiﬁers dramatically outperforing baseline model. experiments noticed important apply dropout retraining wasn’t applied source model training balance dataset labels used rmsprop training procedure since adam’s learning rate annealing schedule might appropriate weight re-training. also tried freezing weights layers never yielded signiﬁcant improvement. shown model transfer beneﬁcial reusing model trained snli dataset sick dataset. tried same shown snli improvement reusing task tasks worse ubuntu dialogue based transfer possibly ubu. task sees versatile less clean data. uniﬁed variety tasks single scientiﬁc framework sentence pair scoring demonstrated platform general modelling problem aggregate benchmarking models across many datasets. promising initial transfer learning results suggest quest generic neural model capable task-independent text comprehension becoming meaningful pursuit. open source nature framework implementation choice popular extensible deep learning library allows high reusability research easy extensions advanced models. figure model results answer sentence selection task measured wang yodaqa/large wqmprop datasets. wqmprop ensembling siamese. demonstration problematic single-measurement result reporting past literature outlier sample -trial attn benchmark would score state art; total three outliers trial scored better wide scope f-problem scope leave popular tasks datasets future work. popular instance sentence pair scoring question answering task memory networks realistic large question paraphrasing dataset based askubuntu stack overﬂow forum recently proposed multi-lingual context sentence-level quality estimation meta-task several available datasets. tasks semantic textual similarity paraphrasing right now) available within framework report results models behind state-of-art signiﬁcantly show little difference results. advancing models competitive remains future work. generalization proposed architecture could applied hypothesis evidencing task binary classiﬁcation hypothesis sentence based number memory sentences example within mctext dataset. also include several major classes models initial evaluation. notably includes serial rnns attention used e.g. task skip-thoughts method sentence embedding. figure model results ubuntu dialogue next utterance ranking task. models slightly speciﬁc conﬁguration much bigger dataset tokens considered input dropout applied memory units projection matrix dot-product scorer used comparison. attn model furthermore memory units ﬁlters. exact models reran version dataset note results dataset directly comparable. research models towards real-world allowing wider sentence variability less explicit supervision. particular believe models developed tested tasks long sentences wide vocabulary. terms models recent work many domains clearly points towards various forms attention modelling remove bottleneck compress full spectrum semantics single vector ﬁxed dimensionality. paper shown beneﬁt training model single dataset applying another dataset. open question whether could jointly train model multiple tasks simultaneously another option would include extra supervision similar token overlap features already employ; example answer sentence selection task datasets explicitly mark actual tokens representing answer. work ﬁnancially supported grant agency czech technical university prague grant sgs/ /ohk/t/ augur project forecast foundation. computational resources provided cesnet cerit scientiﬁc cloud provided programme projects large research development innovations infrastructures. we’d like thank tom´aˇs tunys rudolf kadlec ryan lowe cicero nogueira santos bowen zhou helpful discussions insights silvestr stanko jiˇr´ı n´advorn´ık software contributions. references eneko agirre carmen banea claire cardie daniel mona diab aitor gonzalez-agirre weiwei guof inigo lopez-gazpio montse maritxalar rada mihalcea semeval- task semantic textual similarity english spanish pilot interpretability. petr baudiˇs ˇsediv´y. modeling question answering task yodaqa system. experimental meets multilinguality multimodality interaction pages springer. samuel bowman gabor angeli christopher potts christopher manning. large annotated corpus learning natural language inference. proceedings conference empirical methods natural language processing association computational linguistics. chris burges shaked erin renshaw lazier matt deeds nicole hamilton greg hullender. learning rank using gradient descent. proceedings international conference machine learning pages acm. dagan oren glickman bernardo magnini. pascal recognising textual entailment challenge. machine learning challenges. evaluating predictive uncertainty visual object classiﬁcation recognising tectual entailment pages springer. jesse dodge andreea gane xiang zhang antoine bordes sumit chopra alexander miller arthur szlam jason weston. evaluating prerequisite qualities learning end-to-end dialog systems. corr abs/.. karl moritz hermann tomas kocisky edward grefenstette lasse espeholt mustafa suleyman phil blunsom. teaching machines read comprehend. advances neural information processing systems pages learning rank short text pairs convolutional deep neural networks. proceedings international sigir conference research development information retrieval pages acm. ryan kiros yukun ruslan salakhutdinov richard zemel raquel urtasun antonio torralba sanja fidler. skip-thought vectors. advances neural information processing systems pages ankit kumar ozan irsoy jonathan james bradbury robert english brian pierce peter ondruska ishaan gulrajani richard socher. anything dynamic memory networks natural language processing. corr abs/.. hrishikesh joshi regina barzilay tommi jaakkola kateryna tymoshenko alessandro moschitti llu´ıs m`arquez villodre. denoising bodies titles retrieving similar questions recurrent convolutional models. corr abs/.. ryan lowe nissan iulian serban joelle pineau. ubuntu dialogue corpus large dataset research unstructured multi-turn dialogue systems. corr abs/.. marco marelli luisa bentivogli marco baroni raffaella bernardi stefano menini roberto zamparelli. semeval- task evaluation compositional distributional semantic models full sentences semantic relatedness textual entailment. semeval-. jonas mueller aditya thyagarajan. siamese recurrent architectures learning sentence similarity. thirtieth aaai conference artiﬁcial intelligence. jeffrey pennington richard socher christopher manning. glove global vectors word representation. empirical methods natural language processing pages", "year": 2016}