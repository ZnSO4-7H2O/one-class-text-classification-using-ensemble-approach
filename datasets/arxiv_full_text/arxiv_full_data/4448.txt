{"title": "Deep Image Prior", "tag": ["cs.CV", "stat.ML"], "abstract": "Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, super-resolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs.  Apart from its diverse applications, our approach highlights the inductive bias captured by standard generator network architectures. It also bridges the gap between two very popular families of image restoration methods: learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors such as self-similarity. Code and supplementary material are available at https://dmitryulyanov.github.io/deep_image_prior .", "text": "deep convolutional networks become popular tool image generation restoration. generally excellent performance imputed ability learn realistic image priors large number example images. paper show that contrary structure generator network sufﬁcient capture great deal low-level image statistics prior learning. order show randomly-initialized neural network used handcrafted prior excellent results standard inverse problems denoising superresolution inpainting. furthermore prior used invert deep neural representations diagnose them restore images based ﬂash-no ﬂash input pairs. apart diverse applications approach highlights inductive bias captured standard generator network architectures. also bridges popular families image restoration methods learning-based methods using deep convolutional networks learning-free methods based handcrafted image priors self-similarity. deep convolutional neural networks currently state-of-the-art inverse image reconstruction problems denoising single-image super-resolution convnets also used great success exotic problems reconstructing image activations within certain deep networks descriptor generally convnets similar architectures nowadays used figure super-resolution using deep image prior. method uses randomly-initialized convnet upsample image using strucrture image prior; similar bicubic upsampling method require learning produces much cleaner results sharper edges. fact results quite close state-of-the-art superresolution methods convnets learned large datasets. deep image prior works well inverse problems could test. ability learn realistic image priors data. however learning alone insufﬁcient explain good performance deep networks. instance authors recently showed image classiﬁcation network generalizes well trained genuine data also overﬁt presented random labels. thus generalization requires structure network resonate structure data. however nature interaction remains unclear particularly context image generation. work show that contrary expectations great deal image statistics captured structure convolutional image generator rather learned capability. particularly true statistics required solve various image restoration problems image prior required integrate information lost degradation processes. show this apply untrained convnets solution several problems. instead following common paradigm training convnet large dataset example images generator network single degraded image. scheme network weights serve parametrization restored image. weights randomly initialized ﬁtted maximize likelihood given speciﬁc degraded image task-dependent observation model. show simple formulation competitive standard image processing problems denoising inpainting super-resolution. particularly remarkable aspect network learned data; instead weights network always randomly initialized prior information structure network itself. best knowledge ﬁrst study directly investigates prior captured deep convolutional generative networks independently learning network parameters images. addition standard image restoration tasks show application technique understanding information contained within activations deep neural networks. this consider natural pre-image technique whose goal characterize invariants learned deep network inverting natural images. show untrained deep convolutional generator used replace surrogate natural prior used dramatically improved results. since regularizer like norm learned data entirely handcrafted resulting visualizations avoid potential biases arising form powerful learned regularizers figure learning curves reconstruction task using natural image plus i.i.d. noise randomly scrambled white noise. naturally-looking images result much faster convergence whereas noise rejected. code vector image approach used sample realistic images random distribution furthermore distribution conditioned corrupted observation solve inverse problems denoising super-resolution paper investigate prior implicitly captured choice particular generator network structure parameters learned. interpreting neural network parametrization image r×h×w rc×h×w code tensor/vector network parameters. network alternates ﬁltering operations convolution upsampling non-linear activation. particular experiments performed using encoderdecorder hourglass architecture many million parameters choice data term dictated application discussed later. choice regularizer usually captures generic prior natural images difﬁcult subject much research. simple example total variation image encourages solutions contain uniform regions. work replace regularizer implicit prior captured neural network folminimizer obtained using optimizer gradient descent starting random initialization parameters. given minimizer result restoration process obtained note also possible optimize code usually initialize randomly keep ﬁxed. terms prior deﬁned indicator function images produced deep convnet certain architecture signals. since aspect network pre-trained data deep image prior effectively handcrafted like norm. show hand-crafted prior works well various image restoration tasks. parametrization high noise impedance. wonder high-capacity network used prior all. fact expect able parameters recovering possible image including random noise network impose restriction generated image. show that indeed almost image ﬁtted choice network architecture major effect solution space searched methods gradient descent. particular show network resists solutions descends much quickly towards naturally-looking images. result minimizing either results good-looking local optimum least optimization trajectory passes near one. order study effect quantitatively consider basic reconstruction problem given target image want value parameters reproduce image. setup optimization using data term comparing generated image figure shows value energy function gradient descent iterations four different choices image natural image image plus additive noise image randomly permuting pixels white noise. apparent ﬁgure optimization much faster cases whereas parametrization presents signiﬁcant inertia cases parametrization offers high impedance noise impedance signal. therefore applications restrict number iterations optimization process certain number iterations. resulting prior corresponds projection onto reduced images produced convnets parameters random initialization show experimentally proposed prior works diverse image reconstruction problems. space limitations evaluation main text restricted examples numbers. reader therefore strongly encouraged address supplementary material extensive evaluation extra details. denoising generic reconstruction. parametrization presents high impedance image noise naturally used ﬁlter noise image. denoising recover clean image noisy observation sometimes degradation model known follows particular distribution. however often blind denoising noise model unknown. work blindness assumption method easily modiﬁed incorporate information noise model. exact formulation eqs. given noisy image recover clean image substituting minimizer approach require model image degradation process needs revert. allows applied plug-and-play fashion image restoration tasks degradation process complex and/or unknown obtaining realistic data supervised training highly problematic. demonstrate capability several qualitative examples supplementary material approach uses quadratic energy leading formulation restore images degraded complex unknown compression artifacts. figure also demonstrates applicability method beyond natural images evaluate denoising approach standard dataset consisting colored images noise strength achieve psnr optimization steps. score improved additionally average restored images obtained last iterations averaged optimization runs method improves figure blind restoration jpeg-compressed image. approach restore image complex degradation optimization process progresses deep image prior allows recover signal getting halos blockiness eventually overﬁtting input evaluate super-resolution ability approach using datasets. scaling factor compare works show results scaling factors number optimization steps constant every image. qualitative comparison bicubic upsampling state-of-the learning-based methods srresnet lapsrn presented method fairly compared bicubic methods never data given low-resolution image. visually approach quality learning-based methods loss. gan-based methods srgan enhancenet intelligently hallucinate details image impossible method uses absolutely information world images. compute psnrs using center crops generated images. method achieves psnr datasets respectively. bicubic upsampling gets lower score srresnet psnr method still outperformed learning-based approaches considerably better bicubic upsampling. visually seems close bicubic state-of-the-art trained convnets inpainting. image inpainting given image missing pixels correspondence binary mask }h×w goal reconstruct missing data. corresponding data term given hadamard’s product. necessity data prior obvious energy independent values missing pixels would therefore never change initialization objective optimized directly pixel values before prior introduced optimizing data term w.r.t. reparametrization figure blind image denoising. deep image prior successful recovering man-made natural patterns. reference result state-of-the-art nonlearned denoising approach shown. super-resolution. goal super-resolution take resolution image r×h×w upsampling factor generate corresponding high resolution version r×th×tw solve inverse problem data term super-resolution ill-posed problem inﬁnitely many images reduce image regularization required order select among inﬁnite minimizers plausible ones. following regularize problem considering reparametrization optimizing resulting energy w.r.t. optimization still uses gradient descent exploiting fact neural network common downsampling operators lanczos differentiable. figure image super-resolution. similarly e.g. bicubic upsampling method never access data single low-resolution image produces much cleaner results sharp edges close state-of-the-art superresolution methods srresnet utilize networks trained large datasets. figure region inpainting. method able successfully inpaint large regions. despite using learning results comparable does. choice hyper-parameters important demonstrates sensitivity learning rate) good setting works well images. next considers inpainting masks randomly sampled according binary bernoulli distribution. first mask sampled drop pixels random. compare approach method based convolutional sparse coding. obtain results ﬁrst decompose corrupted image high frequency components similarly method high frequency part. fair comparison version method dictionary built using input image quantitative comparison standard data method given table showing strong quantitative advantage proposed approach compared convolutional sparse coding. present representative qualitative visual comparison also apply method inpainting large holes. non-trainable method expected work correctly highly-semantical large-hole inpainting works surprisingly well situations. compare learning-based method deep image prior utilizes context image interpolates unknown region textures figure comparison recent inpainting approaches. comparison shepard networks text inpainting example. bottom comparison convolutional sparse coding inpainting missing pixels. cases approach performs better images used respective papers. compare deep priors corresponding several architectures. ﬁndings seem suggest deeper architecture beneﬁcial skip-connections work well recognition tasks highly detrimental. natural pre-image. natural pre-image method diagnostic tool study invariances lossy function deep network operates natural images. ﬁrst several layers neural network trained perform image classiﬁcation. pre-image images result representation looking reveals information lost network finding pre-image points formulated minimizing data term however optimizing function directly artifacts i.e. non-natural images behavior network principle unspeciﬁed thus drive arbitrarily. meaningful visualization obtained restricting pre-image natural images called natural pre-image practice ﬁnding points natural pre-image done regularizing data term similarly inverse problems seen above. authors prefer norm weak natural image prior relatively unbiased. contrary papers learn invert neural network examples resulting better looking reconstructions however figure inpainting using different depths architectures. ﬁgure shows much better inpainting results obtained using deeper random networks. however adding skip connections resnet u-net highly detrimental. figure alexnet inversion. given image left show natural pre-image obtained inverting different layers alexnet using three different regularizers deep image prior norm prior network trained invert representations hold-out reconstructions obtained deep image prior many ways least good biased learning process. biased towards learn data-driven inversion prior. here propose deep image prior instead. handcrafted like tv-norm biased towards particular training set. hand results inversions least interpretable ones representations obtained considering progressively deeper subsets alexnet conv conv conv pre-images found either optimizing using structured prior. convnets convolutional sparse coding even deeper investigated context recognition networks recently single-layer convolutional sparse coding proposed reconstruction tasks. comparison approach however suggests using deep convnet architectures popular modern deep learning-based approaches lead accurate restoration results least circumstances. investigated role convolutional network architecture success recent convnet-based image restoration methods. teased apart contribution prior imposed architecture contribution information transfered external images learning. along shown simple approach ﬁtting randomly-initialized convnets corrupted images works swiss knife restoration problems. swiss knife require modeling degradation process pre-training. admittedly approach computationally heavy many ways results common narrative attributes recent successes deep learningbased methods imaging shift using handcrafted priors learning everything data. turns much success also attributed switching worse hand-crafted priors better hand-crafted priors validates importance developing deep learning architectures. norm still produces noisy images whereas structured regularizer produces images often still interpretable. approach also produces informative inversions learned prior clear tendency regress mean. flash-no ﬂash reconstruction. work focus single image restoration proposed approach extended tasks restoration multiple images e.g. task video restoration. therefore conclude application examples qualitative example demonstrating method applied perform restoration based pairs images. particular consider ﬂash-no ﬂash image pair-based restoration goal obtain image scene lighting similar no-ﬂash image using ﬂash image guide reduce noise level. general extending method image likely involve coordinated optimization input codes single-image tasks approach often kept ﬁxed random. case ﬂashno-ﬂash restoration found good restorations obtained using denoising formulation using ﬂash image input resulting approach seen non-linear generalization guided image ﬁltering results restoration given method obviously related image restoration synthesis methods based learnable convnets referenced above. time much related alternative group restoration methods avoid training hold-out set. group includes methods based joint modeling groups similar patches inside corrupted image particularly useful corruption process complex highly variable also group methods based ﬁtting dictionaries patches corrupted image well methods based convolutional sparse coding also statistical models similar shallow convnets reconstructed image work investigates model combines convnet self-similarity based denoising thus also bridges groups methods still requires training hold-out set. overall prior imposed deep convnets investigated work seems highly related selfsimilarity-based dictionary-based priors. indeed weights convolutional ﬁlters shared across entire spatial extent image ensures degree self-similarity individual patches generative convnet potentially produce. connections figure reconstruction based ﬂash no-ﬂash image pair. deep image prior allows obtain low-noise reconstruction lighting close no-ﬂash image. successful avoiding leaks lighting patterns ﬂash pair joint bilateral ﬁltering", "year": 2017}