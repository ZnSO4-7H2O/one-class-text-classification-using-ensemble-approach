{"title": "SquishedNets: Squishing SqueezeNet further for edge device scenarios via  deep evolutionary synthesis", "tag": ["cs.NE", "cs.AI", "cs.LG"], "abstract": "While deep neural networks have been shown in recent years to outperform other machine learning methods in a wide range of applications, one of the biggest challenges with enabling deep neural networks for widespread deployment on edge devices such as mobile and other consumer devices is high computational and memory requirements. Recently, there has been greater exploration into small deep neural network architectures that are more suitable for edge devices, with one of the most popular architectures being SqueezeNet, with an incredibly small model size of 4.8MB. Taking further advantage of the notion that many applications of machine learning on edge devices are often characterized by a low number of target classes, this study explores the utility of combining architectural modifications and an evolutionary synthesis strategy for synthesizing even smaller deep neural architectures based on the more recent SqueezeNet v1.1 macroarchitecture for applications with fewer target classes. In particular, architectural modifications are first made to SqueezeNet v1.1 to accommodate for a 10-class ImageNet-10 dataset, and then an evolutionary synthesis strategy is leveraged to synthesize more efficient deep neural networks based on this modified macroarchitecture. The resulting SquishedNets possess model sizes ranging from 2.4MB to 0.95MB (~5.17X smaller than SqueezeNet v1.1, or 253X smaller than AlexNet). Furthermore, the SquishedNets are still able to achieve accuracies ranging from 81.2% to 77%, and able to process at speeds of 156 images/sec to as much as 256 images/sec on a Nvidia Jetson TX1 embedded chip. These preliminary results show that a combination of architectural modifications and an evolutionary synthesis strategy can be a useful tool for producing very small deep neural network architectures that are well-suited for edge device scenarios.", "text": "deep neural networks shown recent years outperform machine learning methods wide range applications biggest challenges enabling deep neural networks widespread deployment edge devices mobile consumer devices high computational memory requirements. recently greater exploration small deep neural network architectures suitable edge devices popular architectures squeezenet incredibly small model size .mb. taking advantage notion many applications machine learning edge devices often characterized number target classes study explores utility combining architectural modiﬁcations evolutionary synthesis strategy synthesizing even smaller deep neural architectures based recent squeezenet macroarchitecture applications fewer target classes. particular architectural modiﬁcations ﬁrst made squeezenet accommodate -class imagenet- dataset evolutionary synthesis strategy leveraged synthesize efﬁcient deep neural networks based modiﬁed macroarchitecture. resulting squishednets possess model sizes ranging furthermore squishednets still able achieve accuracies ranging able process speeds images/sec much images/sec nvidia jetson embedded chip. preliminary results show combination architectural modiﬁcations evolutionary synthesis strategy useful tool producing small deep neural network architectures well-suited edge device scenarios without need compression quantization. consider deep neural networks successful machine learning methods outperforming many state-of-the-art machine learning methods wide range applications ranging image categorization speech recognition. major factor great recent successes deep neural networks availability powerful high performance computing systems great advances parallel computing hardware. enabled researchers signiﬁcantly increase depth complexity deep neural networks resulting greatly increased modeling capabilities. such majority research deep neural networks largely focused designing deeper complex deep neural network architectures improved accuracy. however increasing complexity deep neural networks become biggest obstacles widespread deployment deep neural networks edge devices mobile consumer devices computational memory power capacity signiﬁcantly lower high performance computing systems. given proliferation edge devices increasing demand machine learning applications devices increasing amount research exploration design smaller efﬁcient deep neural network architectures infer train faster well transfer faster onto embedded chips power edge devices. commonly employed approach designing smaller neural network architectures synaptic precision reduction number bits used represent synaptic strength signiﬁcantly reduced -bit ﬂoating point precision ﬁxed-point precision -bit precision -bit precision approach leads greatly reduced model sizes resulting deep neural networks often require specialized hardware support accelerated deep inference training embedded devices limit utility wide range applications. another approach designing smaller deep neural network architectures take principled approach employ architectural design strategies achieve efﬁcient deep neural network macroarchitectures exemplary case achieved using approach squeezenet three design strategies employed decrease number ﬁlters decrease number input channels ﬁlters downsample late network. resulted macroarchitecture composed fire modules possessed incredibly small model size smaller alexnet comparable accuracy imagenet classes. authors introduced squeezenet number ﬁlters well ﬁlter sizes reduced resulting less computation original squeezenet without sacriﬁcing accuracy thus considered state-of-the-art efﬁcient network architectures. inspired incredibly small macroarchitecture squeezenet motivated take step taking account majority applications machine learning edge devices mobile consumer devices quite specialized often require much fewer number target classes such study explores utility combining architectural modiﬁcations evolutionary synthesis strategy synthesizing even smaller deep neural architectures based squeezenet macroarchitecture applications fewer target classes. refer smaller deep neural network architectures squishednets. ﬁrst simplest strategy taken study perform simple architectural modiﬁcations macroarchitecture squeezenet accommodate scenarios dealing much fewer target classes. study explored classiﬁcation scenario number target classes reduced fewer target classes quite common many machine learning applications edge devices mobile consumer devices. given reduced number target classes modify conv layer ﬁlters. given conv layer contains parameters squeezenet architectural modiﬁcation resulted signiﬁcant reduction model size. leap imagination rather trivial modiﬁcation illustrates squeezenet great macroarchitecture modiﬁed even efﬁcient edge scenarios fewer target classes. second strategy taken study employ evolutionary synthesis strategy synthesize even efﬁcient deep neural network architectures achieved principled macroarchitecture design strategies. first proposed subsequently extended idea behind evolutionary synthesis automatically synthesize progressively efﬁcient deep neural networks successive generations stochastic synaptogenesis manner within probabilistic framework. speciﬁcally synaptic probability models used encode genetic information deep neural network thus mimic notion heredity. offspring networks synthesized stochastic manner given synaptic probability models computational environmental constraints inﬂuencing synaptogenesis thus forming next generation deep neural networks thus mimicking notions random mutation natural selection. offspring deep neural networks trained evolution synthesis process performed generations desired traits met. detailed description evolutionary synthesis strategy found brief mathematical description provided follows. genetic encoding network formulated network architecture generation inﬂuenced network architecture generation offspring network synthesized stochastic manner synthesis probability combines genetic encoding environmental factor model imposed since underlying goal inﬂuence synaptogenesis behaviour offspring deep neural networks progressively efﬁcient generation generation environmental factor model value less enforce resource-starved environment deep neural networks generation generation. study aforementioned modiﬁed macroarchitecture used ancestral precursor evolutionary synthesis process produce deep neural networks even smaller network architectures. study generations evolutionary synthesis performed produce ﬁnal squishednets. study utility combination architectural modiﬁcations evolutionary synthesis synthesizing small deep neural network architectures based squeezenet macroarchitecture well-suited edge device scenarios examine top- accuracies runtime speeds synthesized squishednets -class imagenet- dataset. imagenet- dataset used study subset imagenet dataset composed following target classes reported table performance results four different squishednets shown table number observations made based table first observed leveraging architectural modiﬁcations account fewer target classes well evolutionary synthesis results generation even efﬁcient network architectures evident squishednets model sizes range .mb. therefore smallest squishednet smaller squeezenet second signiﬁcant model size reductions squishednets able process speeds images/sec much images/sec nvidia jetson embedded chip signiﬁcant particularly edge scenarios mobile consumer devices. therefore ability achieve small model sizes also fast runtime speeds great beneﬁts used resource-starved environments limited computational memory energy requirements. terms top- accuracy squishednets able still achieve accuracies ranging high enough many edge applications. preliminary results show combination architectural modiﬁcations evolutionary synthesis strategy useful tool producing small deep neural network architectures well-suited edge device scenarios without need compression quantization.", "year": 2017}