{"title": "Anytime Neural Network: a Versatile Trade-off Between Computation and  Accuracy", "tag": ["cs.LG", "cs.AI"], "abstract": "Anytime predictors first produce crude results quickly, and then continuously refine them until the test-time computational budget is depleted. Such predictors are used in real-time vision systems and streaming-data processing to efficiently utilize varying test-time budgets, and to reduce average prediction cost via early-exits. However, anytime prediction algorithms have difficulties utilizing the accurate predictions of deep neural networks (DNNs), because DNNs are often computationally expensive without competitive intermediate results. In this work, we propose to add auxiliary predictions in DNNs to generate anytime predictions, and optimize these predictions simultaneously by minimizing a carefully constructed weighted sum of losses, where the weights also oscillate during training. The proposed anytime neural networks (ANNs) produce reasonable anytime predictions without sacrificing the final performance or incurring noticeable extra computation. This enables us to assemble a sequence of exponentially deepening ANNs, and it achieves, both theoretically and practically, near-optimal anytime predictions at every budget after spending a constant fraction of extra cost. The proposed methods are shown to produce anytime predictions at the state-of-the-art level on visual recognition data-sets, including ILSVRC2012.", "text": "anytime predictors ﬁrst produce crude results quickly continuously reﬁne test-time computational budget depleted. predictors used real-time vision systems streaming-data processing efﬁciently utilize varying test-time budgets reduce average prediction cost earlyexits. however anytime prediction algorithms difﬁculties utilizing accurate predictions deep neural networks dnns often computationally expensive without competitive intermediate results. work propose auxiliary predictions dnns generate anytime predictions optimize predictions simultaneously minimizing carefully constructed weighted losses weights also oscillate training. proposed anytime neural networks produce reasonable anytime predictions without sacriﬁcing ﬁnal performance incurring noticeable extra computation. enables assemble sequence exponentially deepening anns achieves theoretically practically near-optimal anytime predictions every budget spending constant fraction extra cost. proposed methods shown produce anytime predictions state-of-the-art level visual recognition data-sets including ilsvrc. increasing number machine learning applications require fast accurate responses. examples autonomous vehicles require real-time object detectors spam ﬁlters email servers desire latency. furthermore computational budget limits vary test-time e.g. autonomous vehicles need faster object detection high speed servers less time sample peak trafﬁc. hence often difﬁcult balance trade-off computation accuracy. approach problems anytime predictors output crude results ﬁrst quickly continuously reﬁne budgets depleted. versatile predictors automatically utilize available budgets also enable systems exit early easy samples order focus computation difﬁcult ones. using anytime predictions bolukbasi speed neural networks three times speed density-based clustering algorithm orders magnitude. produce competitive anytime predictions anytime predictor produces sequence predictions gradually expensive accurate e.g. wide array works studied order computation features predictions form sequence. meta-algorithms however rely assembling existing features predictors difﬁculty utilizing modern computationally intensive deep neural networks dnns typically greatly improve prediction performance cost long chains computation without intermediate outputs. hence would ideal gradually improving predictions within trade computation accuracy smoothly order adjust utilize test-time computational budget. achieve competitive anytime predictions dnns propose uniformly auxiliary predictions losses intermediate layers existing feed-forward networks form anytime neural networks shown fig. speciﬁcally desire ﬁnal prediction’s performance figure anytime neural networks contain auxiliary predictions losses intermediate feature unit work sequence residual building blocks; global average pooling followed linear prediction; cross-entropy loss. previous relative ﬁnal error rate optimal spends three times computation reach error rate cifar proposed ﬁnal reach original network. subject constraint desire anytime prediction performance cost near optimal deﬁne optimal performance network trained speciﬁcally output cost highlight importance ﬁnal performance previous works utilize auxiliary predictions using training strategies increase ﬁnal error rates relatively signiﬁcant shown fig. cost anytime networks three times computation regular reach accuracy. make following contributions achieve competitive anytime predictions auxiliary predictors. propose loss weighting schemes utilize high correlation among neighboring predictions maintain accuracy ﬁnal prediction competitive auxiliary predictions deeper layers. also propose oscillate loss weighting training improve individual anytime prediction avoiding spurious solutions optimal losses individual ones. furthermore thanks near-optimal late predictions assemble anns exponentially increasing depths perform near-optimally every budget theoretically practically. related works prediction test-time budget limits. various works studied predict efﬁciently meet test-time budget limits organizing test-time computation predictors features. apply boosting greedy methods order feature predictor computation. sequentially generate features empower ﬁnal predictor. form markov decision processes computation weak predictors features learn policies order them. cascade design speed test-time prediction early exiting easy samples focusing computation difﬁcult ones. however meta-algorithms easily compatible complex accurate predictors like cnns anytime predictions computation inaccurate intermediate results long computation cnn. work addresses problem learning competitive anytime predictions end-to-end within neural networks. anytime predictions also utilize gating functions adaptive models reduce average test-time computation. computationally efﬁcient neural networks. many works studied compress neural networks without sacriﬁcing accuracy. reducing redundant information neural networks prune network weights connections; quantize weights within networks reduce computation memory footprint. transfer knowledge deep networks shallow ones changing training target shallow networks. work addresses computational efﬁciency deep networks orthogonal approach instead making models small train networks adaptive varying budget limits. furthermore work compatible above-mentioned methods produce potentially efﬁcient predictors. networks auxiliary predictions. auxiliary predictions used neural networks various purposes. regularize networks training using auxiliary predictions. train auxiliary predictions target label sets instead one-hot labels improve ﬁnal predictions. increasing number works also appreciate high-quality auxiliary predictions i.e. anytime predictions analyzing images multiple scales speeding average test-time predictions. contribute works auxiliary predictions folds. first propose better-performing easy-to-implement strategies train auxiliary predictions end-to-end. particular proposed methods leverage correlation among neighboring auxiliary predictions achieve near-optimal level anytime predictions near ﬁnal outputs previous methods suffer. second instead competing ensembles networks previous works show assembling sequence exponentially deepening anns greatly improve cost-effectiveness predictions. training anytime neural networks illustrated fig. given sample initial feature subsequent feature transformations generate sequence intermediate features using parameter feature produce auxiliary prediction using prediction layer parameter auxiliary prediction incurs expected loss e∼d]. call feed-forward network auxiliary predictions losses anytime neural network instance base anns resnets contains number residual building blocks prediction period. outputs predicted label distribution using global pooling layer followed linear layer softmax; loss function cross-entropy loss. test-time computes sequentially interruption outputs latest available prediction. interruption issued users learned additional computations parameters full minθ optimal loss prediction. goal training single i.e. i={x minθ ideal exist general often exist practice. hence instead minimize weighted losses hopes approximating weight loss call various choices weight schemes experiment sec. previous works typically follow auxiliary predictions weights also exist works increase linearly instead unfortunately schemes increase ﬁnal error rate relatively show sec. sec. cost inefﬁcient. however experiments intuitive weight schemes provide important insights. first close minimum need high weighting particular spreading weights evenly keeps away possible respective minimum. second neighboring losses highly correlated high weighting possible reduce nearby loss even decrease observations imply weight scheme high values spread losses optimized high weights weights neighbors still improve thanks high correlations among neighbors. since shown fig. imperative maintain ﬁnal performance versatility networks anytime predictions always half total weightings split remaining half weightings high weights spread propose following weight schemes. sieve scheme ﬁrst adds unit weight means rounding. adds unit predictors non-zero weights. scaled figure proposed sieve exp-lin weight schemes anns eann computed order depth; anytime results reported current depth current higher depth previous results better validation sets. also propose oscillate training loss batch samples since minimizi= lead spurious solutions gradients zero bi∇i individual gradients not. choose iteration loss temporarily increase weight current iteration constant choose layer probability proportional weight weight scheme. call sampling strategy alternating show experimentally sec. improves anytime predictions budget limit. also learn sampling strategy no-regret online learning requires hyper-parameters almost performance gains experiments shown appendix. ﬁnding echoes recent results graves sampling according competitive baseline. since half total weights ﬁnal loss observe anns much closer optimal second half network ﬁrst half. order improve early predictions leverage accurate late predictions anns forming sequence anns whose depths grow exponentially shown eann sequentially computes independently-trained anns whose depths grow exponentially base interruption eann outputs better prediction ﬁnal prediction latest ﬁnished latest anytime prediction currently-running comparison determined validation set. intuitively competitive layers computed recursively delegate predictions early fraction cost. rely fraction network smaller network competitive predictions later fraction ann. furthermore depths grow exponentially constant fraction extra costs evaluate shallower anns ﬁrst formally following proposition proves eann competitive budget constant fraction additional computational complexity. proposition assume depth competitive anytime optimal depth layers computation eann prediction depth produces anytime predictions competitive optimal depth expectation eb∼unif supb proposition implies multiply allowed budget build anytime predictor competitive everywhere. furthermore stronger anytime model larger becomes worst case inﬂation rate supb shrinks average rate figure none-constant weights early layers utilize prediction correlations sieve improves half-end. optimal level ﬁnal prediction. eann const linear barely faster sieve exp-lin high errors much expensive errors. shrinks moreover number parallel workers instead speed eanns computing anns parallel ﬁrst-in-ﬁrst-out schedule effectively increase constant computing also worth noting form sequence using regular networks instead anns lose ability output frequently produce intermediate predictions instead predictions eann. larger cost inﬂation supb average cost inﬂation least defer proofs appendix. experiment anns cifar cifar svhn evaluate augmentations different resnets refer models number basic residual building blocks contains bn-relu-xconv bn-relu-addition input number channels initial convolution. network contains blocks conv layers. models prediction period i.e. anytime prediction expected every building block unless speciﬁed otherwise. defer detailed study prediction period appendix. optimize networks stochastic gradient descent momentum learning rate starts divided epochs. train epochs cifar epochs svhn. evaluate using single-crop. quantitatively evaluate training technique ﬁrst train optimal predictions total depth network optimal depth training speciﬁcally predict ˆyi. compute relative increase top- error rate four milestone depths. error rate bi-linearly interpolated doesn’t predict milestones. compare proposed weight schemes popular const scheme used simplicity linear scheme used zamir increases weight linearly also tried normalizing total weights schemes effects minimal. ad-hoc weight schemes? discuss sequence experiments cifar lead proposed weight schemes described fig. sec. discussed sec. fig. imperative ﬁnal performance anns. three data-sets consist colored images. cifar cifar classes training testing images. last training samples cifar cifar held validation hyper parameter tuning using network parameters used models three data-sets. adopt standard augmentation svhn contains around training around testing images numeric digits google street views. adopt pad-and-crop augmentations cifar svhn. table percentages experiments oscillating training loss outperforms targeting static loss. averages relative increases error rates various aanns percentages. achieve this ﬁnal prediction half total weightings. since desire predictions near-optimal budget natural distribute remaining half weightings evenly among them. call weighting scheme half-end plot anytime losses cifar fig. next determine away respective minimum train layer full weighting layer plot optimal curve note half-end persists layers smaller near high correlation neighboring predictions. utilize correlation reduce performance shift weighting loss total depths observe emphasizing locations able reduce performance shown fig. proposed sieve scheme weighting scheme emphasizes locations. sec. observe sieve scheme seems sacriﬁce much early prediction accuracy. reduce early performance propose exp-lin scheme ensures ﬁrst depths least hold aann general technique improve predictions showcase aann useful technique independent weight schemes. explained sec. aann samples layer probability proportional iteration iteration. choose experiments validation network. experiment settings compare aanns anns. table presents percentage settings aanns outperform anns. treating experimental settings independent coin ﬂips apply hypothesis testing understand percentages. entry table probability listed entry probability aann outperforms using associated weight scheme associated milestone cost. null hypothesis alternative hypothesis using hoeffding’s inequality since almost table milestones conclude sampling according form aanns allows improve predictions second half anns independent weight schemes. also suggests optimizing static weighted anytime losses hinders optimization late predictions early predictions. assume anns trained aann. weighting scheme comparisons table presents average relative performance various weight schemes. observe const scheme makes errors relative widens deeper depths. linear scheme also relative ﬁnal prediction. cifar relative ﬁnal error translates absolute error means ﬁnal performances anns const scheme often equivalent resnets cost e.g. cifar achieves ﬁnal error using const using linear four-times-cheaper achieves using exp-lin sieve. show ﬁnal performance gaps highly detrimental cost-efﬁciency eanns. intuitive weight schemes fail ﬁnal prediction possibly ﬁnal prediction difﬁcult train early ones assigned hand proposed sieve exp-lin schemes weighting order using hoeffding’s inequality experiments know figure performance eann cifar. proposed sieve exp-lin schemes achieve much better performance const linear ﬁnal performance gaps. eanns much efﬁcient aanns early predictions const linear irrelevant. enjoy small relative performance ﬁnal prediction furthermore also found anns proposed schemes outperform ﬁnal predictions expensive models trouble converging withdropouts. possibly regularization effect anytime predictions used szegedy zhao optimal-level ﬁnal performances proposed schemes however cost higher error rates ﬁrst half layers. since exp-lin designed higher weights depth outperforms sieve early layers. eann closing early performance gaps section leverage near-optimal ﬁnal predictions anns sieve exp-lin schemes close early performance using eann described sec. train individually models order form eann exponential base proposition eann compete optimal instead compare eann challenging opt+ ﬁrst collect four model also depth error rate opt+ minimum among collection also depth greater hence opt+ equates running parallel outputting best cost whereas eann computes aanns sequentially. noted sec. parallelism eann effectively exponentiate base number parallel workers discuss sec. fig. fig. compare eanns various weight schemes opt+. without parallelism eanns sieve exp-lin schemes shown fig. near opt+ every budget closing early-mid performance gaps aanns. aanns worse eanns early cost early predictions large aann less accurate ﬁnal predictions small aann since early aanns exponentially cheaper eanns constant fraction extra flops compute ﬁrst. fig. fig. also show combined eanns popular const linear schemes clearly higher error rate budgets schemes fail reach near-optimal ﬁnal predictions small aanns. instance achieve error rate const linear require times computation sieve exp-lin fig. shows better early-mid aann predictions const linear sieve exp-lin irrelevant since eann+sieve much cost-efﬁcient aanns. comparisons suggest imperative near-optimal ﬁnal predictions anns large anns ﬁnal performance gaps outperformed small anns without gaps like early aanns eann+sieve. latter also much suitable cost-efﬁcient eanns. figure ilsvrc performance individual anns used eann. aann- ﬁnishes error aann- aann- using const ﬁnal error aann- aann- higher absolutely using sieve. resnet dots mark errors costs four resnets designed bottlenecks hence expensive depths. eann easily parallelizable running aanns eann fifo manner workers reduce time accurate results million natural images validation images classes. report top- error rates validation using single-crop size resnet resnet designed residual bottleneck building blocks contains bn-relu-interleaved sequence xconv-xconv-xconv summation input. prediction period blocks networks. form eann also design resnet contains residual bottleneck blocks period aanns based resnet models trained using momentum epochs batch size learning rate starts divided epoch since anytime prediction costs global pooling followed linear product anytime predictions total cost total flops. fig. observe aann sieve scheme resnet achieve optimal level performance ﬁnal layer error rates near-optimal later half depths. fig. eann using three aanns achieves optimal performance constant fraction additional cost suggested proposition addition running aanns parallel fifo manner worker gpus speed eann signiﬁcantly supports theory sec. parallel workers effectively increase parameter also note replacing aanns regular networks causes improvement happen network much less frequent predictions without noticeable improvement accuracy speed. reference single implementation resnet+aanns using tensorﬂow following flops cost test-time wall-clock speed images flops resnet flops resnet flops resnet. conclusion discussion work propose weighting techniques achieve anytime predictions deep neural networks without degrading ﬁnal performance. anytime predictors combined using ensemble technique achieve near-optimal level performance budgets theory practice. versatile near-optimal anytime predictions allow applications adapt varying budget constraints throughput requirements data-sample difﬁculties. proposed eanns potentially improved multiple ways. first relative gaps complex models small performance plateaus consider skipping expensive networks entirely. potentially addressed budgeted prediction algorithms adaptive models predictor cascades anytime model selection budgeted prediction algorithms also learn early exit within individual aanns. second since predicts later layers anns learn early predictions focus late predictions only. third eann predictions features early anns late anns. zhaowei mohammad saberian nuno vasconcelos. learning complexity-aware cascades deep pedestrian detection. international conference computer vision forrest iandola song matthew moskewicz khalid ashraf william dally kurt keutzer. squeezenet alexnet-level accuracy fewer parameters <.mb model size. arxiv preprint yuval netzer wang adam coates alessandro bissacco andrew reading digits natural images unsupervised feature learning. nips workshop deep learning unsupervised feature learning olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein alexander berg fei-fei. imagenet large scale visual recognition challenge. ijcv sketch proof proposition proof. budget consumed compute cost optimal eann competitive against. goal analyze ratio ﬁrst eann depth optimal result eann same. assume eann depth number depth eann reuse result number cost spent optimal compete cost last form sequence regular networks grow exponentially depth instead worst case happen right prediction produced. hence ratio consumed budget cost optimal current anytime prediction compete right number network completed n→∞−−−−→ table tables anytime performance error rates various prediction period various models cifar. best metric bold. resnet residual blocks details cifar cifar svhn ﬁrst apply convolution output channels stride input image speciﬁed model name initial conv layer followed three groups residual blocks group basic residual building blocks. residual building block applies transformation input feature transformation input together form next feature i.e. basic residual block pre-activation bn-relu-xconv-bn-relu-xconv-bn. conv channel size input within group stride groups second conv previous residual block produces twice many channels stride input block i.e. also ﬁrst average pooled padded zero channels added second convolution result. ilsvrc ﬁrst apply xconv output channels stride input image. apply bn-relu max-pooling kernel size bottleneck residual block bn-relu-xconv-bn-relu-xconv-bn-relu-xconvbn. ﬁrst bottleneck residual block conv output channels third conv channels. bottleneck block scale transition ﬁrst conv outputs channel size input; xconv maintains channel size input; second conv outputs four times channel size input. scale transition ﬁrst conv outputs channel size input; conv output channel size multiplier relative inputs before; conv however uses stride input transition residual blocks ﬁrst pooled padded zero channels adding result note every residual bottleneck unit required compute anytime output possible allowing layers computed without interruption lead better predictions. section studies relationship frequency performance anytime predictions. expect model frequent anytime predictions perform worse optimizing extra auxiliary losses limits freedom network. additionally anytime outputs average weight anytime losses decreases assuming total loss weight constant. less relative weight layer less optimized layer general. following call makes anytime prediction every units feature transformations stack-i layers predict bottlenecks whose index written non-negative integer last layer always predict. fig. table demonstrate performances anns cifar different stack-i networks cases anytime predictions able achieve near-optimal ﬁnal predictions ﬁnal large weights sieve scheme. slightly larger network observe general trend improvement anytime performances metrics period increases. however increase without limit. instance relatively large period possible milestone cost right prediction anytime performance milestone suffers. applications choose large period long cause much waste budgets right milestones. section consider replace heuristic static strategy aann strategies learned training. inspired shalev-shwartz wexler choose amplify iteration applying no-regret online learner maximization following min-max optimization l-dimensional probability simplex heuristic loss objective choosing layer parameter note loss layer intuitively min-max optimization considered player zero-sum game player chooses layer using generate maximal loss player updates reduce chosen loss. suggested shalev-shwartz wexler applying no-regret online learner maximization w.r.t. leads no-regret strategy choosing layers static strategies. multiple options no-regret online learners. instance random weighted majority exponential gradient ascend variable layers consistently results high losses. ideally no-regret online learners discover high loss layer optimization focus eventually loss equal. also consider another heuristic sampling strategy called avgl sample layer probability proportional exponential average maximization objective explain choice heuristic loss objective minimization leads loss ideally would like maxi−∗ measure relative difference optimal fact apply intermediate loss form hyper parameter tune. however general tuning parameter difﬁcult cannot compute efﬁciently without training model speciﬁcally avoid dependency consider relative loss difference neighboring layers following min-max optimization select losses optimize constant chosen cross-validation. intuitively i−i+ represents relative reduction loss layer value high layer performing much worse successor structures relative same. suggests layer could improved signiﬁcantly optimization focus. ﬁnal loss objective maxi=...l− constant fraction maximal previous layers ﬁnal layer successor desire relative high probability choosing ﬁnal layer. suggested shalev-shwartz wexler player no-regret online learning algorithm update min-max optimization follows. iteration ﬁrst sample loss using according probability distribution hyper parameter represents probability algorithm hyper parameter. fraction total weight objective chosen signiﬁcant inﬂuence total loss. computing total next update network parameter gradient total. finally apply update rules update viexp normalize vector onto probability simplex random weighted majority instead procedure almost identical difference iteration update exponential gradient descent using signal three hyper parameters applying alternating optimization probability online learner explore uniformly random ratio reward ﬁnal layer maximum reward among previous layers additional weight chosen grid search cifar cifar validation network sort settings average error ﬁnal error. choose setting ﬁrst appears ranking true optimal parameter setting hard determine random nature randomized strategy itself luckily found validation performance overly sensitive parameters. momentum constant exponential gradient average avgl strategy fig. plot probability bottleneck unit chosen various strategies model overall behavior probabilities similar across models even performance ranking same. plot stack graph versus training epochs view strategy evolve time. found no-regret algorithms learned pick ﬁnal layer half probability. remaining probability roughly split evenly among layers. avgl learns weight ﬁnal layer layers well since avgl update probabilities gradient descent instead exponentiated gradient descent avgl select ﬁnal layer high probability. moreover avgl learns focus layers around layer around transition layers subsample feature maps double channel sizes convolutions. aann based sieve weight scheme naturally focus half weights ﬁnal layer spread remaining weights evenly. instead intermittent large weights small weights every layer somewhat focused correlation neighbors neighborhood ﬁnal layer overly focused table lists sampling strategy anytime evaluation metric percentage experiments table corresponding strategy better vanilla corresponding metric. missing experiments ignored. degrades performance every metric. able improve milestone degrades performance milestones. avgl able somewhat improve performance milestones. however cost ﬁnal prediction performance even though note difference usually small. contrast aann able improve performances every metric except ﬁnal prediction cannot improved already optimal level metric. overall dynamic strategies avgl effective proposed simple aann strategy samples layers according normalized static weights since dynamic strategies extra parameters tune chosen heuristic loss objective probably sub-optimal experiments cannot fully prove helpful improving performance ann. however conclude even helpful require extensive parameter tuning. table lists different models experiment sampling strategies table also contains performance sampling strategy milestone sample evenly total depths.", "year": 2017}