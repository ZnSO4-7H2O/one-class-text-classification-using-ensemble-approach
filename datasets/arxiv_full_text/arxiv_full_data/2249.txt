{"title": "Inductive Representation Learning in Large Attributed Graphs", "tag": ["stat.ML", "cs.AI", "cs.LG", "cs.SI"], "abstract": "Graphs (networks) are ubiquitous and allow us to model entities (nodes) and the dependencies (edges) between them. Learning a useful feature representation from graph data lies at the heart and success of many machine learning tasks such as classification, anomaly detection, link prediction, among many others. Many existing techniques use random walks as a basis for learning features or estimating the parameters of a graph model for a downstream prediction task. Examples include recent node embedding methods such as DeepWalk, node2vec, as well as graph-based deep learning algorithms. However, the simple random walk used by these methods is fundamentally tied to the identity of the node. This has three main disadvantages. First, these approaches are inherently transductive and do not generalize to unseen nodes and other graphs. Second, they are not space-efficient as a feature vector is learned for each node which is impractical for large graphs. Third, most of these approaches lack support for attributed graphs.  To make these methods more generally applicable, we propose a framework for inductive network representation learning based on the notion of attributed random walk that is not tied to node identity and is instead based on learning a function $\\Phi : \\mathrm{\\rm \\bf x} \\rightarrow w$ that maps a node attribute vector $\\mathrm{\\rm \\bf x}$ to a type $w$. This framework serves as a basis for generalizing existing methods such as DeepWalk, node2vec, and many other previous methods that leverage traditional random walks.", "text": "graphs ubiquitous allow model entities dependencies them. graph data often observed directly natural world constructed non-relational data deriving metric space entities retaining signiﬁcant edges learning useful feature representation graph data lies heart success many machine learning tasks classiﬁcation anomaly detection link prediction among many others. many existing techniques random walks basis learning features estimating parameters graph model downstream prediction task. examples include recent node embedding methods deepwalk nodevec well graph-based deep learning algorithms. however simple random walk used methods fundamentally tied identity node. three main disadvantages. first approaches inherently transductive generalize unseen nodes graphs second space-efﬁcient feature vector learned node impractical large graphs. third approaches lack support attributed graphs. make methods generally applicable propose framework inductive network representation learning based notion attributed random walk tied node identity instead based learning function maps node attribute vector type. framework serves basis generalizing existing methods deepwalk nodevec line many existing methods leverage traditional random walks. given directed graph framework consists general steps function mapping nodes types ﬁrst step learn function maps nodes types based matrix attributes. note given input and/or computed based structure graph. attributed random walks second step uses types derived function generating attributed random walks. attributed walk length deﬁned sequence adjacent node types associated sequence indices biogrid–human bn–cat bn–rat–brain bn–rat–cerebral ca–csphd eco–everglades eco–fweb–baydry ia–radoslaw–email –usair soc–anybeat soc–dolphins fb–yale fb–nips–ego web–epa evaluate effectiveness proposed framework generalize nodevec. additional details discussed experimental results provided table results table mean construct edge features learned node embedding vectors node similar results observed using operators. cases generalized approach outperforms methods across wide variety networks biology information social networks. experimental results table demonstrate effectiveness proposed framework generalizing existing methods making powerful practical attributed heterogeneous/typed networks well inductive learning tasks notably generalization enables methods leverage available intrinsic self-attributes well arbitrary structural features derived graph even relational features leverage graph structure well initial intrinsic/self-attributes furthermore learned embeddings approach naturally generalize longer tied identity instead represent general functions easily extracted another arbitrary graph. therefore approach naturally supports wide range inductive network representation learning tasks cross-temporal link prediction classiﬁcation dynamic networks acrossnetwork prediction modeling role discovery graph matching similarity among others finally generalized approach using proposed framework guaranteed perform least well original method since recovered special case framework. advantages framework discussed full version paper conclusion work proposed ﬂexible framework generalizing important class embedding representation learning methods graphs leverage random walks. framework serves basis generalizing existing methods attributed graphs unseen nodes inductive learning/graph-based transfer learning tasks also allowing signiﬁcantly larger graphs inherent space-efﬁciency approach. finally framework shown following desired properties space-efﬁcient accurate inductive able support graphs attributes", "year": 2017}