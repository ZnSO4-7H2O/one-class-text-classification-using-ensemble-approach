{"title": "Scalable Planning and Learning for Multiagent POMDPs: Extended Version", "tag": ["cs.AI", "cs.LG"], "abstract": "Online, sample-based planning algorithms for POMDPs have shown great promise in scaling to problems with large state spaces, but they become intractable for large action and observation spaces. This is particularly problematic in multiagent POMDPs where the action and observation space grows exponentially with the number of agents. To combat this intractability, we propose a novel scalable approach based on sample-based planning and factored value functions that exploits structure present in many multiagent settings. This approach applies not only in the planning case, but also in the Bayesian reinforcement learning setting. Experimental results show that we are able to provide high quality solutions to large multiagent planning and learning problems.", "text": "online sample-based planning algorithms pomdps shown great promise scaling problems large state spaces become intractable large action observation spaces. particularly problematic multiagent pomdps action observation space grows exponentially number agents. combat intractability propose novel scalable approach based samplebased planning factored value functions exploits structure present many multiagent settings. approach applies planning case also bayesian reinforcement learning setting. experimental results show able provide high quality solutions large multiagent planning learning problems. online planning methods pomdps demonstrated impressive performance large problems interleaving planning action selection. leading method partially observable monte carlo planning achieves performance gains extending sample-based methods based monte carlo tree search solve pomdps. online sample-based methods show promise solving pomdps large state spaces become intractable number actions observations grow. particularly problematic case multiagent systems. speciﬁcally consider multiagent partially observable markov decision processes assume agents share partially observable view world coordinate actions. mpomdp model centralized pomdp methods apply fact number joint actions observations scales exponentially number agents renders current pomdp methods intractable. combat intractability provide novel sample-based online planning algorithm exploits multiagent structure. method called factored-value partially observable monte carlo planning based pomcp ﬁrst mcts method exploits locality interaction many mass agents interact directly subset agents. structure enables decomposition value function overlapping factors used produce high quality solutions unlike previous approaches assume factored model value function approximately factored. present variants fvpomcp different amounts factorization value function scale large action observation spaces. fv-pomcp approach applicable large mpomdps potentially even important bayesian learning agents uncertainty underlying model modeled bayes-adaptive pomdps models translate learning problem planning problem since resulting planning problems inﬁnite number states scalable sample-based planning approaches critical solution. show experimentally approach allows planning learning signiﬁcantly efﬁcient multiagent pomdps. evaluation shows approach signiﬁcantly outperforms regular pomcp indicating fv-pomcp able effectively exploit locality interaction settings. mpomdp multiagent planning model unfolds number steps. every stage agents take individual actions receive individual observations. however mpomdp individual observations shared communication allowing team agents ‘centralized manner’. restrict setting communication free noise costs delays. mpomdp tuple s{ai} r{zi} with agents; states designated initial state distribution ×iai joint actions using action sets agent pr|s probability transitioning state state transition probabilities actions taken agents; reward function immediate reward state taking actions ×izi joint observations using observation sets agent observation probabilities oasz prz|as) probability seeing observations given actions taken resulting state horizon. mpomdp reduced pomdp single centralized controller takes joint actions receives joint observations therefore mpomdps solved pomdp solution methods described remainder section. however approaches exploit particular structure inherent many mass. sec. present ﬁrst online planning method overcomes deﬁciency. monte carlo tree search pomdps research pomdps focused planning given full speciﬁcation model determine optimal policy mapping past observation histories states called beliefs) actions. optimal policy extracted optimal q-value function maxa acting greedily. computing however pomcp scalable method extends monte carlo tree search solve pomdps. every stage algorithm performs online planning given current belief incrementally building lookahead tree contains algorithm however avoids expensive belief updates creating nodes belief simply action-observation history particular samples hidden states root node uses state sample trajectory ﬁrst traverses lookahead tree performs rollout. return trajectory used update statistics visited nodes. search tree enormous search directed relevant parts selecting actions maximize ‘upper conﬁdence bounds’ number times action taken history. pomcp shown converge \u0001-optimal value function. moreover method demonstrated good performance large domains limited number simulations. bayesian pomdps reinforcement learning considers realistic case model known advance. unfortunately effective pomdps difﬁcult. ross introduced framework called bayes-adaptive pomdp reduces learning problem planning problem thus enabling advances planning methods used learning problem. particular ba-pomdp utilizes dirichlet distributions model uncertainty transitions observations. intuitively agent could observe states observations could maintain vectors counts transitions observations respectively. transition count number times state resulted taking action state observation count representing number times observation seen taking action transitioning state counts induce probability distribution possible transition observation models. even though agent cannot observe uncertainty represented using states uncertainty actual count vectors pomdp formalism including count vectors part hidden state special pomdp called ba-pomdp. ba-pomdp extended multiagent setting yielding bayesadaptive multiagent pomdp framework. ba-mpomdps pomdps inﬁnite state space since inﬁnitely many count vectors. quality-bounded reduction ﬁnite state space possible problem still intractable; sample-based planning needed provide solutions. unfortunately current methods pomcp scale well multiple agents. pomcp directly suitable multiagent problems fact number joint actions observations exponential number agents. ﬁrst elaborate problems sketch approach mitigate exploiting locality agents. large number joint observations problematic since leads lookahead tree high branching factor. even though theoretically problem mdps partially observable settings particle ﬁlters leads severe problems. particular order good particle representation next time step actual joint observation received must sampled often enough planning previous stage. actual joint observation sampled frequently enough particle ﬁlter approximation results sampling starting initial belief again alternatively fall back acting using separate policy random one. issue large numbers joint actions also problematic standard pomcp algorithm will node maintain separate statistics thus separate upper conﬁdence bounds exponentially many joint actions. exponentially many joint actions selected least times reduce conﬁdence bounds principled problem cases combination individual actions lead completely different effects necessary least times. many cases however effect joint action factorizable effects action individual agents small groups agents. instance consider team agents ﬁghting ﬁres number burning houses illustrated fig. rewards depend amount water deposited house rather exact joint action taken problem lends natural factorization problems also factorized permit approximation. speciﬁes payoff components {qe} component associated subset agents. subsets interpreted -edges graph nodes agents. goal select joint action maximizes local payoff qeae). ﬁghting problem shown fig. follow cited literature assuming suitable factorization easily identiﬁable designer also learnable. even payoff function factor exactly approximated moment assuming stateless problem action-value function approximated qeae) performed efﬁciently variable elimination max-sum algorithms exponential number agents therefore enable signiﬁcant speed-ups larger number agents. algorithm exponential induced width coordination graph. applied given advance. exploit techniques context pomcp however case. such task consider maximum estimated qeae). note necessarily require best approximation entire instead seek estimation maximizing joint action close maximum actual q-value qa∗). purpose introduce technique called mixture experts optimization. contrast methods based linear approximation learn best-ﬁt factored function directly estimate maximizing joint action. main idea local action introduce expert predicts total value ˆqae) ae]. joint action responses—one payoff component ˆqae). mixture weights used predict maximization joint action maxa equation restricted-scope functions identical case linear approximation used perform maximization effectively. remainder paper integrate weights simply write ˆqeae) ˆqae). experts implemented maximum-likelihood estimators total value. expert keeps track mean payoff received performed done efﬁciently. additional beneﬁt approach allows efﬁcient estimation upper conﬁdence bounds also keeping track often local action performed turns facilitates easy integration pomcp describe next. section presents main algorithmic contribution factored-value pomcp online planning method pomdps exploit approximate structure value function applying mixture experts optimization pomcp lookahead search tree. introduce variants fv-pomcp. ﬁrst technique factored statistics addresses complexity introduced joint actions. second technique factored trees additionally addresses problem many joint observations. fv-pomcp ﬁrst mcts method exploit structure mass achieving better sample complexity using factorization generalize value function joint actions histories. method developed scale pomcp larger mpomdps terms number agents techniques beneﬁcial multiagent models factored pomdps. factored statistics ﬁrst introduce factored statistics directly applies mixture experts optimization node pomcp search tree. shown fig. tree joint histories remains same statistics retained history different. rather maintaining statistics node expected value joint action maintain statistic component estimates values corresponding upper conﬁdence bounds. since method retains fewer statistics performs joint action selection efﬁciently expect efﬁcient plain application pomcp ba-mpomdp. however complexity joint observations directly addressed joint histories used reuse nodes factored trees second technique called factored trees additionally tries overcome large number joint observations. decomposes local qe’s splitting joint histories local histories distributing factors. case introduce expert local pair. simulations agents know action selection conducted maximizing upper conﬁdence bounds uehe qehe )/nae. assume agents relevant actions histories component same generalized. approach reduces number statistics maintained increases reuse nodes mcts chance nodes trees exist observations seen execution. such increase performance increasing generalization well producing robust particle ﬁlters. type factorization major effect implementation rather constructing single tree construct number trees parallel factor shown fig. node tree component stores required statistics count local history counts actions taken local tree tree. finally point decentralization statistics potential reduce communication since components statistics decentralized fashion could updated without knowledge observation histories. here investigate approximation quality induced factorization techniques. desirable quality bounds would express performance relative ‘optimal’ i.e. relative pomcp converges probability \u0001-optimal value function. even one-shot case extremely difﬁcult method employing factorization based linear approximation equation corresponds special case linear regression. case write terms basis functions weights weae heae heae basis functions heae speciﬁes component such providing guarantees respect optimal qa)-value would require developing priori bounds approximation quality basis functions. difﬁcult problem good solution even though methods widely studied machine learning. however expect methods perform well arbitrary instead expect perform well nearly factored approximately holds since local actions contain enough information make good predictions. such analyze behavior methods samples come factored function contaminated zero-mean noise. cases show following. theorem estimate made mixture experts converges probability true value plus bπa). bias given biases induced sample policy dependent bias term ˆqa) pairs observe global reward given actions bias caused correlations sampling policy fact overcounting value components. overlap sampling policy ‘component-wise’ πae\\e|ae) πae\\e|a πae\\e) counting local actions similar reasoning used establish bounds performance case overlapping components subject assumptions properties true value function. denote neighborhood component components overlap theorem overlapping components ‘intersection action proﬁles’ ae∩ea intersection true value function satisﬁes analysis shows sufﬁciently local q-function effectively optimized using sufﬁciently local sampling policy. assumptions also derive guarantees sequential case. directly possible derive bounds fv-pomcp seems likely exploration leads effective policy nearly satisﬁes property. moreover since bias introduced interaction action correlations differences ‘non-local’ components even using policy correlations bias limited q-function sufﬁciently structured. factored tree case introduce strong result. histories agents outside factor included assume independence factors approximation quality suffer markov case local history such expected return local history depends future policy well past implies convergence longer guaranteed proof. ft-fv-pomcp corresponds general case monte carlo control linear function approximation greedy w.r.t. current value function. settings result divergence even though negative result guarantee convergence ft-fv-pomcp practice need problem; many reinforcement learning techniques diverge produce high-quality results practice e.g. therefore expect problem exhibits enough locality factored trees approximation allow good quality policies found quickly. finally analyze computational complexity. fv-pomcp implemented modifying pomcp’s simulate function maximization performed variable elimination complexity induced width |amax| size largest action set. addition algorithm updates components bringing total complexity call simulate here empirically investigate effectiveness factorization methods comparing nonfactored methods planning learning settings. experimental setup. test methods versions ﬁreﬁghting problem section sensor network problems. ﬁreﬁghting problems ﬁres suppressed quickly larger number agents choose particular house. fires also spread neighbor’s houses start house small probability. sensor network problems sensors aligned along discrete intervals axes rewards tracking target moves grid. types sensing could employed agent agent could nothing. higher reward given agents correctly sensing target time. ﬁreﬁghting problems broken overlapping factors agents sensor grid problems broken factors agents ﬁreﬁghting problem agents agents sensor network problems agents agents experiment given number simulations number samples used step choose action averaged number episodes. report undiscounted return standard error. experiments single core machine memory. cases compare factored representations version using pomcp. comparison uses code base directly shows difference using factorization. pomcp similar sample-based planning methods already shown state-of-the-art methods pomdp planning learning mpomdps. start comparing factored statistics factored tree versions fv-pomcp multiagent planning problems. here agents given true mpomdp model plan. setting compare baseline methods pomcp regular pomcp applied mpomdp random uniform random action selection. note pomcp converge \u0001-optional solution solution quality poor using small number simulations. results -agent -agent ﬁreﬁghting problems horizon shown figure -agent problem pomcp performs poorly simulations number simulations increases outperforms methods provides high-quality solution small number simulations resulting value plateaus approximation error. also provides high-quality solution small number simulations able converge solution near pomcp. -agent problem pomcp able generate solution slightly better random fv-pomcp methods able perform much better. fact performs similar results seen sensor grid problem. pomcp outperforms random policy number simulations grows produce much higher values available simulations. seems converge quality solution loss information target’s previous position longer known local factors. problem pomcp requires minutes episode simulations making reducing number simulations crucial problems size. results clearly illustrate beneﬁt fv-pomcp exploiting structure planning mass. ba-mpomdps. also investigate learning setting here episode state count vectors reset initial values. learning partially observable environments extremely hard many equivalence classes transition observation models indistinguishable learning. therefore assume reasonably good model transitions poor estimate observation model results four agent instance ﬁghting problem shown fig. cases variants approach pomcp value. small number simulations learns quickly providing signiﬁcantly better values methods better increased horizon. learns slowly value better number simulations increases full history. simulations horizon problem performance model improves factored methods still outperform increase less similar results seen four agent sensor grid problem. performs best small number simulations number increases outperforms methods. again problems ba-pomcp requires minutes episode largest number simulations showing need efﬁcient methods. experiments show even challenging multiagent settings state uncertainty methods learn effectively exploiting structure. mcts methods become popular games type multiagent setting action factorization exploited progressive widening double progressive widening success games large action spaces. progressive widening methods structure coordination graph order generalize value actions instead must correct joint action exponentially many available also designed fully observable scenarios address large observation space mpomdps. factorization history unlike linear function approximation state components td-search however contrast method particular factorization still apply aggressively search promising branches tree. methods based q-learning exploit action factorization assume agents observe individual rewards clear could incorporated uct-style algorithm. locality interaction also considered previously decentralized pomdp methods form factored dec-pomdps networked distributed pomdps models make strict assumptions information agents choose actions thereby signiﬁcantly lowering resulting value nd-pomdps also impose additional assumptions model mpomdp model contrast impose restrictions. instead mpomdps agent knows joint action-observation history different perspectives different agents. therefore factored dec-pomdp nd-pomdp methods apply mpomdps; specify mappings individual histories actions nd-pomdp methods assume value function exactly factored local values mpomdp value approximately factored perfect locality interaction allows natural factorization mpomdp value function method applied mpomdp furthermore current factored dec-pomdp nd-pomdp models generate solutions given model ofﬂine fashion consider online methods using simulator paper. approach builds upon coordination-graphs perform joint action optimization efﬁciently factorization one-shot problems considered settings too. amin present method optimize graphical bandits relates optimization approach. since approach replaced functionality obvious approach could integrated pomcp. moreover work focuses minimizing regret apply factorization hold. oliehoek present factored-payoff approach extends coordination graphs imperfect information settings agent knowledge. relevant current algorithm assumes joint observations received centralized decision maker could potentially useful relax assumption. presented ﬁrst method exploit multiagent structure produce scalable method monte carlo tree search pomdps. approach formalizes team agents multiagent pomdp allowing planning techniques pomdp literature applied. however since number joint actions observations grows exponentially number agents na¨ıve extensions single agent methods scale well. combat problem introduced fv-pomcp online planner based pomcp exploits multiagent structure using novel techniques—factored statistics factored trees— reduce number joint actions number joint histories considered. empirical results demonstrate fv-pomcp greatly increases scalability online planning mpomdps solving problems agents. investigation also shows scalability much complex learning problem four agents. methods could also used solve pomdps ba-pomdps large action observation spaces well recent bayes-adaptive extension self interested i-pomdp model", "year": 2014}