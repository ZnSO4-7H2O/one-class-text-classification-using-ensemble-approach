{"title": "Discriminative Bimodal Networks for Visual Localization and Detection  with Natural Language Queries", "tag": ["cs.CV", "stat.ML"], "abstract": "Associating image regions with text queries has been recently explored as a new way to bridge visual and linguistic representations. A few pioneering approaches have been proposed based on recurrent neural language models trained generatively (e.g., generating captions), but achieving somewhat limited localization accuracy. To better address natural-language-based visual entity localization, we propose a discriminative approach. We formulate a discriminative bimodal neural network (DBNet), which can be trained by a classifier with extensive use of negative samples. Our training objective encourages better localization on single images, incorporates text phrases in a broad range, and properly pairs image regions with text phrases into positive and negative examples. Experiments on the Visual Genome dataset demonstrate the proposed DBNet significantly outperforms previous state-of-the-art methods both for localization on single images and for detection on multiple images. We we also establish an evaluation protocol for natural-language visual detection.", "text": "pioneering works recurrent neural language models deep image representations localizing object referred text phrase given single image global spatial context left commonly used pick particular object. contrast johnson takes descriptions without global context queries localizing general visual entities visual genome dataset existing work performs localization maximizing likelihood generate query text given image regions using image captioning model whose output probability density needs modeled virtually inﬁnite space natural language. since hard train classiﬁer huge structured output space current captioning models constrained trained generative partially discriminative ways. however discriminative tasks localization detection usually favor models trained associating image regions text queries recently explored bridge visual linguistic representations. pioneering approaches proposed based recurrent neural language models trained generatively achieving somewhat limited localization accuracy. better address natural-language-based visual entity localization propose discriminative approach. formulate discriminative bimodal neural network trained classiﬁer extensive negative samples. training objective encourages better localization single images incorporates text phrases broad range properly pairs image regions text phrases positive negative examples. experiments visual genome dataset demonstrate proposed dbnet signiﬁcantly outperforms previous state-of-the-art methods localization single images detection multiple images. also establish evaluation protocol natural-language visual detection. object localization detection computer vision traditionally limited small number predeﬁned categories category-speciﬁc image region classiﬁers serve object detectors. however real world visual entities interest much diverse including groups objects object parts objects particular attributes and/or particular context. scalable annotation entities need labeled ﬂexible using text phrases. deep learning demonstrated uniﬁed learning framework text image representations. signiﬁcant progress made many related tasks image captioning visual question answering text-based ﬁnediscriminative objective better utilize negative samples. paper propose deep architecture natural-language-based visual entity localization call discriminative bimodal network architecture uses binary output space allow extensive discriminative training negative training sample potentially utilized. idea take text query condition rather output model directly predict text query image region compatible particular pathways deep architecture respectively extract visual linguistic representations. discriminative pathway built upon pathways fuse bimodal representations binary classiﬁcation inter-modality compatibility. compared estimated probability density huge space natural language score given binary classiﬁer likely calibrated. particular better calibrated scores comparable across different images text queries. property makes possible learn decision thresholds determine existence visual entities multiple images text queries making localization model generalizable detection tasks. examples natural-language visual detection showcased perform comprehensive quantitive ablative evaluations. proposed architecture convolutional neural networks visual textual representations. inspired fast r-cnn roi-pooling architecture induced large-scale image classiﬁcation networks efﬁcient feature extraction model learning image regions. textual representations develop character-level extracting phrase features. network image language pathways dynamically forms classiﬁers image region features depending text features outputs classiﬁer responses regions interest. main contributions follows develop bimodal deep architecture binary output space enable fully discriminative training natural-language visual localization detection. propose training objective extensively pairs text phrases bounding boxes discriminative objective deﬁned possible region-text pairs entire training non-mutually exclusive nature text phrases taken account avoid ambiguous training samples. experimental results visual genome demonstrate proposed dbnet signiﬁcantly outperforms existing methods based recurrent neural language models visual entity localization single images. also establish evaluation methods naturallanguage visual detection multiple images show state-of-the-art results. object detection. recent success deep learning visual object recognition constitutes backbone state-of-the-art object detection natural-language visual detection adapt deep visual representations single forward-pass computing framework r-fcn used existing work traditional object detection. however natural-language visual detection needs huge structured label space represent natural language ﬁnding proper mapping huge space visual representations difﬁcult. image captioning caption grounding. recurrent neural network based language model become dominant method captioning images text despite differences details network architectures language models learn likelihood picking word predeﬁned vocabulary given visual appearance features previous words introduced attention mechanism encourage rnns focus relevant image regions generating particular words. karpathy fei-fei used strong supervision text-region alignment well-grounded captioning. object localization natural language. recent work used conditional likelihood captioning image region given text localizing associated objects. proposed spatial-context recurrent convnet conditioned local visual features global contexts evaluating given captions. johnson combined captioning object proposal end-to-end neural network densely caption image regions localize objects. trained captioning model maximizing posterior localizing object given text phrase reduced ambiguity generated captions. however training objective limited ﬁguring single objects single images. simpliﬁed limited text queries subject-relationship-object triplets. rohrbach improved localization accuracy extra text reconstruction task. extended bounding localization instance segmentation using natural language queries. nagaraja explicitly modeled context referral expressions. text representation. neural networks also embed text ﬁxed-dimensional feature space. rnn-based methods cnn-based methods word-level one-hot encoding input. recently character-level also demonstrated effective paragraph categorization zero-shot image classiﬁcation max-pooling layers followed fully connected layers takes sequence ﬁxed length input produces textual representations ﬁxed dimension. input length long enough cover possible text phrases. avoid empty tailing characters input replicate text phrase reaching input length limit. empirically found sparse input easily lead over-sparse intermediate activations create large portion dead relus ﬁnally result degenerate solution. avoid problem adopt leaky relu keep hidden units active character-level cnn. text embedding methods also used dbnet framework. character-level simplicity ﬂexibility. compared word-based models uses lower-dimensional input vectors constraint word vocabulary size. compared rnns easily allows deeper architectures. discriminative pathway compared basic form bilinear function xtawφrgn discriminative pathway includes additional linear term text-dependent bias visual representation classiﬁer. natural modeling cross-modality correlation multiplication also source instability training. improve training stability introduce regu+|b| dynamic larization term γdynamic classiﬁer besides network weight decay γdecay dbnet drive training proposed twopathway bimodal binary classiﬁcation objective. pair image regions text phrases training samples. deﬁne ground truth binary label best-performing object detection framework terms accuracy generally veriﬁes candidate image region belongs particular category interest. though recent deep architectures propose regions conﬁdence scores time veriﬁcation model taking input image features exact proposed regions still serves boost accuracy. section develop veriﬁcation model natural-language visual localization detection. unlike classiﬁers small number predeﬁned categories traditional object detection model dynamically adaptable different text phrases. image coordinates region text phrase. veriﬁcation model outputs conﬁdence matched suppose binary label indicating positive negative region-text pair veriﬁcation model learns probability compatible i.e. section supplementary materials formalized comparison conditional captioning models. develop bimodal deep neural network model. particular composed single-modality pathways followed discriminative pathway. image pathway φrgn extracts drgn-dim visual representation image region language pathway φtxt extracts dtxt-dim textual representation phrase discriminative pathway parameters θdis dynamically generates classiﬁer visual representation according textual representation predicts matched full model speciﬁed visual linguistic pathways roi-pooling image network. suppose regions interest given existing region proposal method calculate visual representations image regions pass using fast r-cnn roi-pooling pipeline. state-of-the-art image classiﬁcation networks including -layer vggnet resnet- used backbone architectures. character-level textual network. english text phrase encode characters -dim one-hot vector alphabet composed printable characters including punctuations space. thus encoded -channel sequence stacking character encodings. character-level deep obtain high-level textual representation particular network convolutional layers interleavtween image region text phrase. obtain reliable training labels deﬁne positive labels conservative manner; then combine text similarity together spatial establish ambiguous text phrase reﬂects potential false negative labels. provide detailed deﬁnitions below. positive phrases. region positive text phrases constitute ηpos high enough threshold determine positive labels. positive phrases missing incomplete annotations. however recover false positive training labels introduced ambiguous phrases. still region collect text phrases whose ground truth regions moderate overlap ηneg lower bound largest ground truths phrase lies uncertain whether positive negative. words ambiguous respect region note contains phrases cover possible ambiguous phrases full text similarity measurement augment ﬁnalized ambiguous phrase pairs region ambiguous text phrases assigned uncertain label avoid false negative labels. figure illustrates region-text label arbitrary training image region. weighted training loss effective training sets. image effective training region-text pairs figure ground truth labels region-text pairs phrases categorized positive ambiguous negative sets based given region’s overlap ground truth boxes ambiguous phrases augmented text similarity shown visual clarity ηneg ηpos different rest paper. training region-text pair propose weighted training loss function training samples. given training images {}ni ground truth annotations number annotations coordinate region text phrase corresponding rij. region paired multiple phrases take pair separate entry denote regions considered j={rij} regions given proposal methods write annotated text phrases training text phrases. ground truth labels labeling criterion. assign possible training region-text pair ground truth label binary classiﬁcation. region image text phrase take largest overlap ground truth regions evidence determine label. denote intersection union. largest overlap deﬁned object detection limited number categories usually reliable enough assigning binary training labels given complete ground truth annotations categories. contrast text phrase annotations inevitably incomplete training set. image region intractable number valid textual descriptions including different points focus paraphrases description annotating infeasible. consequently cannot always reﬂect consistency besplit visual genome datasets images training validation testing; remaining images included text phrases annotated crowd sourcing included signiﬁcant portion misspelled words. corrected misspelled words using enchant spell checker abiword. that unique phrases training unique phrases testing set. test third phrases appeared training remaining thirds unseen. unique phrases annotated ground truth regions image. experimental results reported dataset. models. constructed fast r-cnn -style visual pathway dbnet based either -layer vggnet resnet- experiments used vggnet fair comparison existing works less evaluation time. resnet- used improve accuracy. compared dbnet image captioning based localization models densecap scrc dbnet visual pathway pretrained object detection using faster r-cnn pascal trainval linguistic pathway randomly initialized. pretrained vggnet imagenet ilsvrc classiﬁcation dataset used initialize densecap model trained match dense captioning accuracy reported johnson found faster r-cnn pretraining beneﬁt densecap scrc model additionally pretrained image captioning coco did. trained models using training visual genome evaluated localization single images detection multiple images. also assessed usefulness major components dbnet. localization task took ground truth text phrases annotated image queries localize associated objects maximizing network response proposed image regions. evaluation metrics. used region proposal method propose bounding boxes models used non-maximum suppression threshold localize boxes. performance evaluated recall ground truth regions query phrase origin phrase spos srest sneg negative region-text pairs containing phrases rest training per-image training loss notation convenience; binary classiﬁcation loss particular cross-entropy loss logistic regression. deﬁne training loss summation three parts freq frequency occurrences training set. normalize re-weight loss three subsets separately. particular λpos λneg λrest balance positive negative training loss. values λneg λrest implicitly determined numbers text phrases choose inside outside stochastic optimization. training loss functions existing work natural-language visual localization positive samples training similar solely using lpos method also considers negative case less ﬂexible extensible case lrest recurrent neural language model encourage certain amount discriminativeness word selection entire text phrases ours. full training objective. summing training loss images together weight decay whole neural network regularization text-speciﬁc dynamic classiﬁer full training objective table single-image object localization accuracy visual genome dataset. text phrase annotated test image taken query image. denotes overlapping threshold determining recall ground truth boxes. dc-rpn region proposal network densecap. discussion recall precision localization tasks). proposed bounding boxes top-k network responses large enough overlap ground truth bounding took successful localization. multiple ground truth boxes image required localized boxes match them. ﬁnal recall averaged test cases i.e. image text phrase. median mean overlap top- localized ground truth also considered. dbnet outperforms captioning models. summarize top- localization performance different methods table bounding boxes proposed testing. dbnet outperforms densecap scrc metrics. particular dbnet’s recall twice high methods threshold times higher johnson reported densecap’s localization accuracy much smaller test comparable exhaustive test settings also note different region proposal methods make difference localization performance. used edgebox rest evaluation. figure shows top-k recall curves. scrc slightly better densecap possibly global context features used scrc. dbnet outperforms consistently signiﬁcant margin thanks effectiveness discriminative training. dynamic bias term improves performance. textdependent bias term introduced makes method fusing visual linguistic representations different basic bilinear functions similar visual feature classiﬁer. table dynamic bias term relative improvement median relative improvement recall thresholds. transferring knowledge beneﬁts localization accuracy. pretraining visual pathway dbnet object detection pascal showed minor beneﬁt recall lower thresholds brought relative improvement recall threshold respectively. section supplementary materials results showed densecap beneﬁt technique. qualitative results. visually compared localization results dbnet densecap figure many cases dbnet localized queried entities reasonable locations. examples provided section supplementary materials. quantitative results. supplementary materials studied performance improvement learned table detection average precision using query three levels difﬁculties. mean text phrases. test cases. vggnet default visual methods. dbnet denotes dbnet resnet-. number negative images positive images level- number negative images either times number positive images test phrase included relatively negative images infrequent phrases. level went became challenging detector maintain precision negative test cases included. level- level- sets text phrases depicting obvious non-object stuff removed better detection task. then phrases remained. evaluation metrics. measured detection performance average precision particular computed independently query phrase test images reported mean query phrases. like traditional object detection score threshold detected region category/phrase-speciﬁc. practical natural-language visual detection query text known advance also directly computed test cases. term global models random guessing upper bound performance limitation region proposal methods also evaluated dbnet using queries constrained form high query complexity demonstrated signiﬁcant source failures natural language visual localization. detection task model needs verify existence quantity queried visual entities addition localizing them any. text phrases associated image regions exist query image evaluation metrics deﬁned extending used traditional object detection. query sets. huge total number possible query phrases practical test subset phrases test image. developed query sets three difﬁculty levels text phrase test image positive least ground truth region exists phrase; otherwise image negative. level- query localization task every text phrase tested positive images level- text phrase randomly chose implies universal decision threshold query phrase. table summarizes maps gaps under different overlapping thresholds models. dbnet shows higher per-phrase performance. dbnet achieved consistently stronger performance densecap scrc terms indicating dbnet produced accurate detection given phrase. even challenging threshold dbnet still showed reasonable performance. results suggest effectiveness discriminative training. dbnet scores better calibrated. achieving good performance challenging assumes phraseagnostic universal decision threshold. densecap scrc showed performance terms dbnet dramatically outperformed them. densecap scrc unsuccessful dbnet could produce certain degree positive results. results suggest responses dbnet much better calibrated among different text phrases captioning models supporting hypothesis distributions binary decision space easier model huge natural language space. robustness negative rare cases. performance models dropped query became difﬁcult. scrc appeared robust densecap negative test cases dbnet showed superior performance difﬁculty levels. particularly level- query densecap’s performance dropped signiﬁcantly compared level- case suggests probably failed handling rare phrases dbnet’s level- performance even better level- performance densecap scrc. test scrc level query high time consumption. qualitative results. showed qualitative results dbnet detection selected examples figure comprehensive examples provided section supplementary materials. dbnet could detect diverse visual entities including objects attributes objects context object parts groups objects level- query dbnet densecap cost process image using vggnet titan card. scrc takes nearly minutes setting. addition dbnet took seconds process image using level- query set. shown table performance basic training strategy better densecap scrc effectiveness discriminative training. ambiguous phrase pruning signiﬁcant performance gain improving correctness training labels pruning ambiguous phrases means setting quantitative analysis tuning text similarity threshold provided section supplementary materials. inter-image negative phrases beneﬁt localization performance since localization single-image task. however mechanism improved detection performance making model robust diverse negative cases. expected vision tasks ﬁnetuning pretrained classiﬁcation network boosted performance models. addition upgrading vggnet-based visual pathway resnet- another clear gain dbnet’s performance demonstrated importance discriminative learning natural-language visual localization. proposed discriminative bimodal neural network allow ﬂexible discriminative training objectives. further developed comprehensive training strategy extensively properly leverage negative observations training data. dbnet signiﬁcantly outperformed previous state-of-the-art based caption generation models. also proposed quantitative measurement protocols natural-language visual detection. dbnet showed robustness rare queries compared existing methods produced detection scores better calibration various text queries. method potentially improved combining discriminative objective generative objective image captioning. work funded software center samsung electronics well career iis- sloan research fellowship. thank nvidia donating titan gpus. also thank kibok binghao deng jimei yang ruben villegas helpful discussions.", "year": 2017}