{"title": "Long-term Blood Pressure Prediction with Deep Recurrent Neural Networks", "tag": ["cs.LG", "cs.AI", "math.DS", "stat.ML"], "abstract": "Existing methods for arterial blood pressure (BP) estimation directly map the input physiological signals to output BP values without explicitly modeling the underlying temporal dependencies in BP dynamics. As a result, these models suffer from accuracy decay over a long time and thus require frequent calibration. In this work, we address this issue by formulating BP estimation as a sequence prediction problem in which both the input and target are temporal sequences. We propose a novel deep recurrent neural network (RNN) consisting of multilayered Long Short-Term Memory (LSTM) networks, which are incorporated with (1) a bidirectional structure to access larger-scale context information of input sequence, and (2) residual connections to allow gradients in deep RNN to propagate more effectively. The proposed deep RNN model was tested on a static BP dataset, and it achieved root mean square error (RMSE) of 3.90 and 2.66 mmHg for systolic BP (SBP) and diastolic BP (DBP) prediction respectively, surpassing the accuracy of traditional BP prediction models. On a multi-day BP dataset, the deep RNN achieved RMSE of 3.84, 5.25, 5.80 and 5.81 mmHg for the 1st day, 2nd day, 4th day and 6th month after the 1st day SBP prediction, and 1.80, 4.78, 5.0, 5.21 mmHg for corresponding DBP prediction, respectively, which outperforms all previous models with notable improvement. The experimental results suggest that modeling the temporal dependencies in BP dynamics significantly improves the long-term BP prediction accuracy.", "text": "existing methods cufﬂess continuous estimation categorized groups namely physiological model i.e. pulse transit time model regression model decision tree support vector regression models suffers accuracy decay time especially multi-day continuous prediction. limitation become bottleneck prevents models practical applications. worth noting aforementioned models directly present input target ignoring important temporal dependencies dynamics. could root long-term inaccuracy. compared static prediction multi-day prediction generally much challenging. complex regulation mechanisms human body multi-day dynamics intricate temporal dependencies larger variation range. paper formulate prediction sequence learning problem propose novel deep model proved effective modeling long-range dependencies dynamics achieved state-of-the-art accuracy multi-day continuous prediction. goal arterial prediction multiple temporal physiological signals predict sequence. input features extracted electrocardiography photoplethysmogram signals denote target sequence. conditional probability factorized figure illustrates overview proposed deep model. deep consists bidirectional lstm bottom layer stack multilayered long shortterm memory residual connections. full network trained backpropagation time miniaturize difference prediction ground truth. abstract— existing methods arterial blood pressure estimation directly input physiological signals output values without explicitly modeling underlying temporal dependencies dynamics. result models suffer accuracy decay long time thus require frequent calibration. work address issue formulating estimation sequence prediction problem input target temporal sequences. propose novel deep recurrent neural network consisting multilayered long short-term memory networks incorporated bidirectional structure access larger-scale context information input sequence residual connections allow gradients deep propagate effectively. proposed deep model tested static dataset achieved root mean square error mmhg systolic diastolic prediction respectively surpassing accuracy traditional prediction models. multi-day dataset deep achieved rmse mmhg month prediction mmhg corresponding prediction respectively outperforms previous models notable improvement. experimental results suggest modeling temporal dependencies dynamics signiﬁcantly improves longterm prediction accuracy. leading risk factor cardiovascular diseases high blood pressure commonly used critical criterion diagnosing preventing cvd. therefore accurate continuous monitoring people’s daily life imperative early detection intervention cvd. traditional measurement devices e.g. omron products cuff-based therefore bulky discomfort available snapshot measurements. disadvantages restrict cuff-based devices long-term continuous measurement essential nighttime monitoring precise diagnosis different symptoms. feature cardiovascular system complex dynamic self-regulation involves multiple feedback control loops response variation mechanism gives dynamics temporal dependency nature. peng xiao-rong ding yuan-ting zhang jing zhao department electronic engineering chinese university hong kong hong kong miao laboratory health informatics chinese academy sciences shenzhen institutes advanced technology shenzhen china. variety experimental results suggested rnns deep architecture signiﬁcantly outperform shallow rnns. simply stacking multiple layers could readily gain expressive power. however full deep network could become difﬁcult train goes deeper likely exploding vanishing gradient problems inspired idea attaching identity skip connection adjacent layers shown good performance training deep neural networks incorporate residual connection lstm layer next model shown figure input hidden state lstm function respectively associated i-th lstm layer corresponding weight input element-wise added layer’s i-th lstm layer next lstm hidden state layer. lstm block residual connections implemented deep model created stacking multiple lstm blocks other output previous block forming input next. toplayer hidden state computed output obtained given multiple supervision signals like systolic diastolic mean closely related other adopt multi-task training strategy train single model predict fig. deeprnn architecture. rectangular lstm cell. green dashed bottom bidirectional lstm layer consisting forward backward lstm. orange dashed depicts lstm layer residual connections. first introduce basic block deep model one-layer bidirectional long short-term memory lstm designed address vanishing gradient problem conventional introducing memory cell state multiple gating mechanisms inside standard hidden state transition process. hidden state lstm generated respectively forget gate input gate output gate control much information forgotten accumulated outputted. terms denote weight matrices bias vectors respectively. tanh stand element-wise application logistic sigmoid function hyperbolic tangent function respectively denote element-wise multiplication. conventional lstms capture information past history present input access larger-scale temporal context input sequence also incorporate nearby future information inform downstream modeling process. bidirectional realize function processing data forward backward directions separate hidden layers merge output layer. illustrated bottom figure brnn computes forward hidden state backward hidden state static continuous dataset. dataset including obtained healthy people including males females. signal acquired biopac system reference continuous measured finapres system simultaneously experiment. data subject recorded sampling frequency minutes rest status. multi-day continuous dataset. similar dataset obtained healthy subjects including males female. data subject recorded minutes rest status multi-day period namely moth ﬁrst day. since primary goal paper prove importance modeling temporal dependencies dynamics accurate prediction simply select representative handcrafted features signals follows represents ground truth corresponding prediction. represents regulation model parameters corresponding penalty coefﬁcient. advantage multi-task training learning predict different values simultaneously could implicitly encode quantitative constrains among mbp. rnns inherently deep time hidden states transition. despite depth time proposed deep model also deep along layer structure. simplify analysis mainly focus gradient along depth layers. recursively updating equation have decomposed additive terms term propagates information directly without weight layers another term weight layers. ﬁrst term ensures supervised information could directly backpropagate shallower layer cannot always samples mini-batch gradient unlikely canceled out. implies gradients layer vanish even intermediate weights arbitrarily small. nice backpropagation property allows train deep model owns expressive power without worrying gradient vanishing problem. evaluate proposed model static multi-day continuous dataset. root mean square error used evaluation metric deﬁned t=zt datasets compare model following reference models table detailed analysis deep models comparison different reference models. deeprnn-xl represents layer model. models validated static continuous dataset. models trained using mini-batches size adam optimizer minibatch computed norm gradients gradients scaled gv/g model different number layers hidden state size layer. sequence length training sample could larger deeper model adopted. saving computational cost adopt bidirectional lstm ﬁrst layer. limited training samples prediction problem maximum depth deep model avoid overﬁtting. training dataset divided data used training validation test. normalized corresponding maximum respectively. evaluation multi-day continuous dataset deep models ﬁrst pretrained static dataset ﬁnetuned using part ﬁrst-day data ﬁnally tested rest ﬁrst-day data well following days’ data. validation static continuous dataset. shown table models yield slightly better results models show poorer performance kalman ﬁlter bidirectional lstm deep models. best accuracy obtained -layer deep model achieves rmse prediction respectively. bland-altman plots indicate deeprnnl predictions agreed well ground truth differences within agreement area. figure qualitatively shows deeprnn-l prediction result representative subject static continuous dataset. incorporating bidirectional structure model i.e. using bidirectional lstm prediction accuracy improved signiﬁcantly compared vanilla lstm decrease rmse decrease rmse. furthermore observed improvement prediction accuracy enhanced increasing depth deeprnn network. instance replacing deeprnn-l deeprnn-l results improvement prediction respectively. stack -layer deeprnn model tend overﬁt clear beneﬁts depth observed more. validation multi-day continuous dataset. figure compares prediction performance deep reference models. clearly seen deeprnn models yield much better performance compared regression models likely temporal dependencies modeling deeprnn models. kalman ﬁlter could model time dependencies sequence dose perform well deeprnn models. likely linearity assumption kalman ﬁlter state transition measurement functions linear. assumption limit capability model complex temporal dependencies dynamics. best accuracy obtained deeprnn-l model achieves rmse mmhg month prediction mmhg corresponding prediction respectively. shown figure models regression models kalman ﬁlter exhibit pronounced accuracy decay second day. although prediction accuracy deeprnn model also drops ﬁrst consistently provides lowest rmse values among models. figure qualitatively shows capability deeprnn track long-term variation. importance residual connections. investigate importance residual connections conduct ablation study static continuous dataset. shown table deeprnn model incorporated residual connections works considerably better counterpart. training found residual connections signiﬁcantly improve gradient backward pass make deep neural network easier optimize. accordingly better performance could obtained expressive deep structure. detailed reason computational beneﬁt explained section iii. importance multi-task training. table shows multi-task training strategy boost prediction performance compared separate training individual models. explained different training objectives involved task strongly correlated thus share data representations capture underlying factors learned model work demonstrated modeling temporal dependency dynamics signiﬁcantly improve longterm prediction accuracy challenging problems cufﬂess estimation. proposed novel deep incorporated bidirectional lstm residual connections tackle challenge. experimental results show deep model achieves state-of-the-art accuracy static multi-day continuous datasets. flaxman danaei shibuya adairrohani almazroa amann anderson andrews comparative risk assessment burden disease injury attributable risk factors risk factor clusters regions systematic analysis global burden disease study lancet vol. guyton coleman cowley scheel manning norman arterial pressure regulation overriding dominance kidneys long-term regulation hypertension american journal medicine vol. chen kobayashi ichikawa takeuchi togawa continuous estimation systolic blood pressure using pulse time intermittent calibration medical biological arrival engineering computing vol. fig. comparison ground truth deeprnn prediction representative subject multi-day continuous dataset. figure represent results month respectively. miao y.-t. zhang x.-r. ding hong novel continuous blood pressure estimation approach based data mining techniques ieee journal biomedical health informatics jain kumar majumdar sparse regression based approach cuff-less blood pressure measurement acoustics speech signal processing ieee international conference graves a.-r. mohamed hinton speech recognition deep recurrent neural networks acoustics speech signal processing ieee international conference ieee amodei ananthanarayanan anubhai battenberg case casper catanzaro cheng chen deep speech end-to-end speech recognition english mandarin international conference machine learning pascanu mikolov bengio difﬁculty training recurrent neural networks. icml vol. zhang deep residual learning image recognition proceedings ieee conference computer vision pattern recognition", "year": 2017}