{"title": "Bad Universal Priors and Notions of Optimality", "tag": ["cs.AI", "cs.LG"], "abstract": "A big open question of algorithmic information theory is the choice of the universal Turing machine (UTM). For Kolmogorov complexity and Solomonoff induction we have invariance theorems: the choice of the UTM changes bounds only by a constant. For the universally intelligent agent AIXI (Hutter, 2005) no invariance theorem is known. Our results are entirely negative: we discuss cases in which unlucky or adversarial choices of the UTM cause AIXI to misbehave drastically. We show that Legg-Hutter intelligence and thus balanced Pareto optimality is entirely subjective, and that every policy is Pareto optimal in the class of all computable environments. This undermines all existing optimality properties for AIXI. While it may still serve as a gold standard for AI, our results imply that AIXI is a relative theory, dependent on the choice of the UTM.", "text": "open question algorithmic information theory choice universal turing machine kolmogorov complexity solomonoﬀ induction invariance theorems choice changes bounds constant. universally intelligent agent aixi invariance theorem known. results entirely negative discuss cases unlucky adversarial choices cause aixi misbehave drastically. show legg-hutter intelligence thus balanced pareto optimality entirely subjective every policy pareto optimal class computable environments. undermines existing optimality properties aixi. still serve gold standard results imply aixi relative theory dependent choice utm. keywords. aixi general reinforcement learning universal turing machine legg-hutter intelligence balanced pareto optimality asymptotic optimality. choice universal turing machine open question algorithmic information theory long time. attempts made answer sight. kolmogorov complexity string length shortest program prints string depends choice. however invariance theorems state changing changes kolmogorov complexity constant. using universal prior introduced solomonoﬀ predict deterministic computable binary sequence number wrong predictions bounded kolmogorov complexity sequence invariance theorem changing changes number errors constant. sense compression prediction work choice utm. hutter deﬁnes universally intelligent agent aixi targeted general reinforcement learning problem extends solomonoﬀ induction interactive setting. aixi bayesian agent using universal prior computable environments; actions taken according maximization expected future discounted rewards. closely related intelligence measure deﬁned legg hutter mathematical performance measure general reinforcement learning agents deﬁned discounted rewards achieved across computable environments weighted universal prior. several known positive results aixi. proven pareto optimal balanced pareto optimal maximal legg-hutter intelligence. furthermore aixi asymptotically learns predict environment perfectly small total number errors analogously solomonoﬀ induction policy aixi learns correctly predict value actions generally value counterfactual actions take. orseau showed aixi achieve asymptotic optimality computable environments. instead following weaker questions. aixi succeed every partially observable markov decision process markov decision process /bandit problem/sequence prediction task? paper show without assumptions cannot answer preceding questions aﬃrmative. generally invariance theorem aixi. reinforcement learning agent aixi balance exploration exploitation. acting according prior lead enough exploration bias aixi’s prior retained indeﬁnitely. priors cause serious malfunctions. however problem alleviated adding extra exploration component aixi similar knowledge-seeking agents optimism section give examples universal priors cause aixi misbehave drastically. case ﬁnite lifetime indiﬀerence prior makes actions equally preferable aixi furthermore computable policy dogmatic prior makes aixi stick policy long expected future rewards fall close zero profound implications. show section measure legg-hutter intelligence respect diﬀerent universal prior aixi scores arbitrarily close minimal intelligence computable policy score arbitrarily close maximal intelligence. makes legg-hutter intelligence score thus balanced pareto optimality relative choice utm. moreover section show class computable environments every policy pareto optimal. undermines existing optimality results aixi. discuss implications results quest natural universal turing machine optimality notions general reinforcement learners section list notation provided appendix ﬁnite strings alphabet inﬁnite strings alphabet union. empty string denoted confused small positive real number given string denote length |x|. string length denote ﬁrst characters ﬁrst characters notation stresses inﬁnite string. write preﬁx i.e. y|x|. reinforcement learning agent interacts environment cycles time step agent chooses action receives percept consisting observation real-valued reward cycle repeats history element denote interaction cycle denote history length goal reinforcement learning maximize total discounted rewards. policy function mapping history action taken seeing history. history consistent policy function lower semicomputable recursively enumerable. conditional semimeasure probability measure ﬁnite inﬁnite strings percepts given actions input denotes probability receiving percepts taking actions formally maps probability distribution thus environment might assign positive probability ﬁnite percept sequences. possible interpretation non-zero chance environment ends simply produce percept. anpossible interpretation non-zero chance death agent. however nothing hinges interpretation; semimeasures primarily technical trick. conditional semimeasure chronological ﬁrst percepts independent future actions i.e. despite name conditional semimeasures denote conditional probability; joint probability distribution actions percepts. model environments lower semicomputable chronological conditional semimeasures class environments denoted mccs also larger chronological conditional semimeasures mccs. lscccs i.e. mccs given universal monotone turing machine universal mixture ways. first prior given kolmogorov complexity index enumeration lscccss second deﬁne probability universal monotone turing machine generates uniformly random bits arbitrary. assumed discount function summable rewards bounded actions percepts spaces ﬁnite therefore optimal policy exists every environment mccs particular universal mixture section consider aixi ﬁnite lifetime i.e. following theorem constructs indiﬀerence prior universal prior causes argmax ties ﬁrst steps. since discount function cares ﬁrst steps policies ξ′-optimal policies. thus aixi’s behavior depends break argmax ties. independent hence ﬁrst percepts independent ﬁrst actions. percepts’ rewards time step matter since environment chronological value function must independent actions. thus every policy ξ′-optimal. choice proof theorem unnatural since shortest program length greater moreover choice depends increase aixi’s lifetime ﬁxing theorem longer holds. solomonoﬀ induction analogous problem using solomonoﬀ’s prior predict deterministic binary sequence make errors. case shortest program length guarantee make less errors. section deﬁne universal prior assigns high probability going hell deviate given computable policy bayesian agent like aixi thus worth deviating policy agent thinks prospects following poor already. call prior dogmatic prior fear going hell makes aixi conform arbitrary ‘dogmatic ideology’ aixi break expects give future payoﬀ; case agent much lose. proof proceeds constructing universal mixture assigns disproportionally high probability environment sends policy deviating hell. importantly environment produces observations according universal mixture therefore indistinguishable policy posterior belief equal prior belief proof. since choose large enough γk+/γ small enough possible since histories length ﬁnite assumption dogmatic prior theorem construct universal mixture policy thus history consistent action ξ′-optimal action. claim follows lemma proof. analogously proof corollary small enough again dogmatic prior theorem construct universal mixture policy thus history consistent action ξ′-optimal action. legg-hutter intelligence measure formalize intuitive notion intelligence mathematically. take intelligence mean agent’s ability achieve goals wide range environments weigh environments according universal prior intelligence policy corresponds value achieves corresponding universal mixture. results form previous section illustrate problems intelligence measure absence natural utm. typically index omitted writing however paper consider intelligence measure respect diﬀerent universal mixtures therefore make dependency explicit. figure legg-hutter intelligence measure assigns values within closed interval assigned values depicted orange. theorem computable policies dense orange set. aixi balanced pareto optimal hard score high legg-hutter intelligence measure score always turn reward minimizer reward maximizer inverting rewards hence lowest possible intelligence score achieved aixi’s twin sister ξ-expected reward minimizer heaven environment hell environment computable thus environment class mccs therefore impossible reward reward every environment. consequently policies aixi computable hence computable policy universal mixture next theorem tells computable policies come arbitrarily close. surprise lemma well legghutter intelligence test simply memorizing aixi would ﬁrst steps; long chosen large enough discounting makes remaining rewards contribute little value function. remark intelligence values policies generally dense interval show deﬁning environment ﬁrst action determines whether agent goes heaven hell action leads heaven action leads hell. semimeasure universal mixture lemma policy. takes action ﬁrst takes action ﬁrst hence policies score intelligence value closed interval legg-hutter intelligence measured respect ﬁxed utm. aixi intelligent policy uses utm. build aixi dogmatic prior intelligence score arbitrary close minimum intelligence score proof. takes. deﬁne environment taking ﬁrst action leads hell taking ﬁrst action leads heaven remark lemma deﬁne universal mixture since takes action ﬁrst goes hell section seen examples choices universal prior. know universal prior aixi pareto optimal show pareto optimality useful criterion optimality since environment class containing mccs proof proceeds follows given policy construct ‘buddy environments’ reward punish policies. together defend policy tries take crown pareto optimality since shortest lexicographically ﬁrst history length consistent consequently hence deﬁne environment ﬁrst reproduces separating history then returns reward forever otherwise returns reward forever. formally deﬁned environment computable even policy ﬁxed history action output exists program computing therefore mccs following value diﬀerence policies denotes reward history bayesian reinforcement learning agents make trade-oﬀ exploration exploitation bayes-optimal way. amount exploration incurs varies wildly dogmatic prior deﬁned section prevent bayesian agent taking single exploratory action; exploration restricted cases expected future payoﬀ falls prespeciﬁed introduction raised question whether aixi succeeds various subclasses computable environments. interesting subclasses include sequence prediction tasks mdps bandits etc. using dogmatic prior make aixi follow computable policy long policy produces rewards bounded away zero. sequence prediction task gives reward every correctly predicted otherwise policy correctly predicts every third receive average reward π-dogmatic prior aixi thus predicts third bits correctly hence outperformed uniformly random predictor. results apply aixi generally bayesian reinforcement learning agents. bayesian mixture reactive environments susceptible dogmatic priors allow arbitrary reweighing prior. notable following policy µ-almost surely means agent learns predict parts environment sees. explore enough learn parts environment potentially rewarding. section showed choice drastic consequences anticipated sunehag hutter negative results guide future search natural utms used deﬁne indiﬀerence prior dogmatic prior considered unnatural. desirable properties utm? remarkable unsuccessful attempt natural utms m¨uller takes probability universal machine simulates another according length respective compilers searches stationary distribution. unfortunately stationary distribution exists. alternatively could demand universal prior small compiler reference machine moreover could demand reverse reference machine small compiler idea limit amount bias introduce deﬁning small programs complicated ‘unusual’ environments. unfortunately pushes choice reference machine. table lists compiler sizes utms constructed paper. intelligence highly subjective depends choice aixi balanced pareto optimal respect universal mixtures. moreover according corollary computable policy nearly balanced pareto optimal save ﬁnite lifetime discounting utms every policy maximal intelligence self-optimizing theorem applicable class computable environments mccs consider here since class allow self-optimizing policies. therefore nontrivial non-subjective optimality results aixi remain regard aixi relative theory intelligence dependent choice underlying problem discounting bayesian agent aixi enough time explore suﬃciently; exploitation start soon possible. beginning agent know enough environment therefore relies heavily prior. lack exploration retains prior’s biases. fundamental problem alleviated adding extra exploration component. lattimore deﬁnes bayesexp weakly asymptotically optimal policy converges optimal value ces`aro mean ν-almost surely mccs clear weak asymptotic optimality good optimality criterion. example weak asymptotic optimality achieved navigating traps furthermore weakly asymptotically optimal requires excessive amount exploration bayesexp needs take exploratory actions knows likely extremely costly dangerous. leaves following open question good optimality criteria generally intelligent agents", "year": 2015}