{"title": "Unifying Decision Trees Split Criteria Using Tsallis Entropy", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "The construction of efficient and effective decision trees remains a key topic in machine learning because of their simplicity and flexibility. A lot of heuristic algorithms have been proposed to construct near-optimal decision trees. ID3, C4.5 and CART are classical decision tree algorithms and the split criteria they used are Shannon entropy, Gain Ratio and Gini index respectively. All the split criteria seem to be independent, actually, they can be unified in a Tsallis entropy framework. Tsallis entropy is a generalization of Shannon entropy and provides a new approach to enhance decision trees' performance with an adjustable parameter $q$. In this paper, a Tsallis Entropy Criterion (TEC) algorithm is proposed to unify Shannon entropy, Gain Ratio and Gini index, which generalizes the split criteria of decision trees. More importantly, we reveal the relations between Tsallis entropy with different $q$ and other split criteria. Experimental results on UCI data sets indicate that the TEC algorithm achieves statistically significant improvement over the classical algorithms.", "text": "∗department computer science technology tsinghua university beijing china email wangysmails.tsinghua.edu.cn songchaobin.com xiastsz.tsinghua.edu.cn abstract—the construction efﬁcient effective decision trees remains topic machine learning simplicity ﬂexibility. heuristic algorithms proposed construct near-optimal decision trees. cart classical decision tree algorithms split criteria used shannon entropy gain ratio gini index respectively. split criteria seem independent actually uniﬁed tsallis entropy framework. tsallis entropy generalization shannon entropy provides approach enhance decision trees’ performance adjustable parameter paper tsallis entropy criterion algorithm proposed unify shannon entropy gain ratio gini index generalizes split criteria decision trees. importantly reveal relations tsallis entropy different split criteria. experimental results data sets indicate algorithm achieves statistically signiﬁcant improvement classical algorithms. decision tree non-parametric supervised learning method used classiﬁcation regression. although decision tree methods ﬁrst machine learning approaches remains actively researched domain machine learning. simple understand interpret also offers relatively good results computational efﬁciency ﬂexibility. general idea decision trees predict unknown input instances learning simple decision rules inferred several known training instances. decision trees often induced following topmanner. given data partitioned left right subset split criterion test attributes. highest scoring partition reduces average uncertainty mostly selected data partitioned accordingly child nodes growing tree making node parent newly created child nodes. procedure applied recursively stopping conditions e.g. maximum tree depth minimum leaf size reached. generally speaking split criterion fundamental issue decision trees induction. large number decision tree induction algorithms different split criteria proposed. example iterative dichotomiser algorithm based shannon entropy algorithm based gain ratio considered normalized shannon entropy classiﬁcation regression tree algorithm based gini index algorithms seem independent hard judge algorithm always outperforms others. actually reﬂects drawback kind split criteria lack adaptability data sets. numerous alternatives proposed adaptive entropy estimate statistical entropy estimates complex lose simplicity comprehensibility decision trees. best knowledge uniﬁed framework combining criteria together. addition series papers analyzed importance split criterion demonstrated different split criteria substantial inﬂuence generalization error induced decision trees. inspiration proposed split criterion unifying generalizing classical split criteria. address issue propose tsallis entropy framework paper. tsallis entropy generalization shannon entropy adjustable parameter ﬁrst introduced decision trees prior work tested performance tsallis entropy given relation tsallis entropy split criteria explored. uniﬁed framework also presented. paper propose tsallis entropy based decision tree induction algorithm called algorithm analyze correspondence tsallis entropy different split criteria. shannon entropy gini index speciﬁc cases tsallis entropy gain ratio also considered normalized tsallis entropy tsallis entropy indeed provides approach improve performance decision trees tunable uniﬁed framework. experimental results data sets indicate algorithm achieves statistically signiﬁcant improvement classical algorithms without losing strengths decision trees. rest paper organized follows. section presents background tsallis entropy. section outlines proposed algorithm. section exhibits experimental results. section summaries work. entropy measure disorder physical systems measure amount information needed specify full microstates system shannon adopted entropy information theory called shannon entropy measure uncertainty tsallis entropy convex. tsallis entropy non-convex non-concave. tsallis entropy concave satisfying similar properties shannon entropy instance maximal uniform distribution. additivity crucial difference fundamental property shannon entropy tsallis entropy. independent random variables shannon entropy additivity property besides tsallis conditional entropy tsallis joint entropy tsallis mutual information also derived similarly shannon entropy. conditional probability joint probability tsallis conditional entropy tsallis joint entropy denoted typical distributions observed macroscopic world exponential distribution family power-law heavy-tailed distribution family. however cannot characterize power-law heavy-tailed distribution maximizing shannon entropy subject normal mean variance. reason shannon entropy implicitly assumes certain trade-off contributions tails main mass distribution worthwhile control trade-off explicitly characterize distribution family. entropy measures depend powers probi= provide control. thus parameterized entropies proposed. well-known generalization concept tsallis entropy extends applications so-called non-extensive systems using adjustable parameter tsallis entropy explain physical systems complex behaviours long-range long-memory interactions described above tsallis entropy uniﬁes shannon entropy gain ratio gini index framework. following reveal relations tsallis entropy split criteria. iii. tsallis entropy criterion algorithm issue procedure decision tree induction split criterion. every step decision tree chooses pair attribute cutting point makes maximal impurity decrease split data grow tree. therefore attribute chosen split signiﬁcantly affects construction decision trees inﬂuences classiﬁcation performance. candidate cutting point attribute data belonging node partitioned child nodes would created partitioned function impurity criterion e.g. tsallis entropy computes labels data fall node. pair attribute cutting point chosen construct tree maximizes procedure applied recursively stopping conditions reached. stopping conditions consist three principles classiﬁcation achieved subset. attributes left selection. cardinality subset greater predeﬁned threshold. here summary proposed tsallis entropy criterion algorithm pseudo-code format algorithm compared classical decision tree induction algorithms difference split criterion. tsallis entropy replace classical split criteria e.g. shannon entropy gain ratio gini index. actually following subsection tsallis entropy uniﬁes shannon entropy gain ratio gini index different values represents shannon entropy. replaced tsallis entropy gain ratio generalized tsallis gain ratio. thus gain ratio also covered tsallis entropy adding normalized factor summary tsallis entropy uniﬁes three kinds split criteria e.g. shannon entropy gain ratio gini index generalizes split criterion decision trees. know ﬁrst time unify common split criteria parametric framework. also ﬁrst time reveal correspondence tsallis entropy different split criteria. optimal tsallis entropy obtained cross-validation usually equal implies better performance traditional split criteria. although optimal different different data sets associated properties data sets. parameter enables algorithm adaptability ﬂexibility. tsallis entropy indeed provides approach improve decision trees’ performance tunable uniﬁed framework. experiments section algorithm achieves higher accuracy classical algorithms appropriate illustrated section algorithm based tsallis entropy adjustable parameter consists tsallis entropy tsallis gain ratio split criteria. tsallis entropy split criterion degenerates shannon entropy gini index respectively. respect gain ratio tsallis gain ratio also degenerates gain ratio order quantitatively compare trees obtained different methods choose accuracy evaluate effectiveness tree total number tree nodes measure tree complexity. shown table data sets adopted evaluate proposed approaches. data sets consist three types namely numeric categorical mixed data sets. also data sets include kinds classiﬁcation problems binary multi-class classiﬁcation. decision trees different split criteria e.g. gain ratio shannon entropy gini index tsallis entropy tsallis gain ratio implemented python. refer cart algorithm implementation scikit-learn platform algorithm implementation weka data ﬁrst partition data training test randomly test holds training grid search using -fold cross-validation determine values tsallis entropy tsallis gain ratio. maybe optimal tsallis entropy tsallis gain ratio different fair comparison choose e.g. optimal tsallis entropy. besides minimal leaf size avoid overﬁtting. parameter selection best parameters ﬁxed. then decision tree trained training data without postpruning evaluated test data. procedure training-test data partition evaluation repeated times reduce inﬂuence randomness. figure gives intuitive exhibition inﬂuence different values parameter tsallis entropy glass data set. figure illustrates accuracy sensitive change highest accuracy obtained figure shows tree complexity different responds change accuracy lowest tree complexity achieved noted different strategies choose various purpose e.g. highest accuracy lowest complexity trade-off also reﬂection algorithm’s adaptability data sets. paper choose highest accuracy principle choice table reports accuracy complexity results different criteria different data sets. highest accuracy lowest complexity data boldface. expected performance outperforms cart fact tsallis entropy generalization shannon entropy gini index gain ratio. respect kinds algorithm e.g. tsallis entropy tsallis gain ratio prevail another absolutely. results indicates tsallis entropy prefers high accuracy tsallis gain ratio prefers complexity. reason lies normalized factor inﬂuence tree structure extent. addition compared shannon entropy gini index tsallis entropy achieves better performance accuracy complexity. tsallis gain ratio also obtains better results compared gain ratio. three wilcoxon signed ranked tests accuracy reject null hypothesis equal performance p-value less results show algorithm appropriate achieves average statistically signiﬁcant improvement accuracy maintains lower complexity. terms optimal value fuzzy trend table class number smaller value tended e.g. numeric type data sets yeast haberman increasing class number decreasing paper choose optimal value using cross-validation method conjecture values associated properties data sets. example data algorithms presents almost results reﬂects data sensitive parameter relation properties data sets discussed future work. paper present evaluate tsallis entropy enhancing decision trees fundamental issue e.g. split criterion. unify classical split criteria parametric framework propose algorithm tsallis entropy split criterion generalizes shannon entropy gain ratio gini index adjustable parameter reveal relations tsallis entropy different split criteria. experimental results indicate that appropriate algorithm achieves average statistically signiﬁcant improvement accuracy. nevertheless approaches limitations need addressed future estimate method parameter place current cross-validation method. furthermore tsallis entropy also potential applications beyond decision trees instance random forest bayesian network investigated future work. serrurier prade entropy evaluation based conﬁdence intervals frequency estimates application learning decision trees proceedings international conference machine learning maszczyk duch comparison shannon renyi tsallis entropy used decision trees proceedings international conference artiﬁcial intelligence soft computing springer pedregosa varoquaux gramfort michel thirion grisel blondel prettenhofer weiss dubourg vanderplas passos cournapeau brucher perrot duchesnay scikit-learn machine learning python journal machine learning research vol.", "year": 2015}