{"title": "Evolution of Covariance Functions for Gaussian Process Regression using  Genetic Programming", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "In this contribution we describe an approach to evolve composite covariance functions for Gaussian processes using genetic programming. A critical aspect of Gaussian processes and similar kernel-based models such as SVM is, that the covariance function should be adapted to the modeled data. Frequently, the squared exponential covariance function is used as a default. However, this can lead to a misspecified model, which does not fit the data well. In the proposed approach we use a grammar for the composition of covariance functions and genetic programming to search over the space of sentences that can be derived from the grammar. We tested the proposed approach on synthetic data from two-dimensional test functions, and on the Mauna Loa CO2 time series. The results show, that our approach is feasible, finding covariance functions that perform much better than a default covariance function. For the CO2 data set a composite covariance function is found, that matches the performance of a hand-tuned covariance function.", "text": "abstract. contribution describe approach evolve composite covariance functions gaussian processes using genetic programming. critical aspect gaussian processes similar kernel-based models covariance function adapted modeled data. frequently squared exponential covariance function used default. however lead misspeciﬁed model data well. proposed approach grammar composition covariance functions genetic programming search space sentences derived grammar. tested proposed approach synthetic data two-dimensional test functions mauna time series. results show approach feasible ﬁnding covariance functions perform much better default covariance function. data composite covariance function found matches performance hand-tuned covariance function. composition covariance functions non-trivial task described black hand critical tune covariance function data modeled primary option integrate prior knowledge learning process hand experience knowledge modeled system required correctly. frequently especially multi-dimensional data sets obvious covariance function structured. work discuss composition covariance functions gaussian processes used nonparametric machine learning tasks e.g. regression classiﬁcation context gaussian process used bayesian prior functions relating input variables target variable. gaussian process regression allows modeling non-linear functional dependencies diﬀerent covariance functions produces posterior probability distribution estimates target values instead point estimates only. paper describe idea using grammar covariance functions genetic programming search good covariance function given data set. also describe prototype implementation using grammarguided tree-based ﬁnally present results proof-of concept. evaluated diﬃculty problem genetic programming particular suited well kind problem. results experiments indicate idea feasible producing good covariance functions low-dimensional data sets. recent contribution problem structure identiﬁcation covariance functions approached using grammar rather rewriting rules basis searching composite covariance functions gaussian processes approach actually similar work; main difference work genetic programming search possible structures duvenaud enumerate composite functions starting standard functions. another recent contribution discusses ﬂexible families covariance functions instead composing covariance functions simple terms also related earlier work describes additive gaussian processes equivalent weighted additive composition base kernels calculated eﬃciently. genetic programming used previously evolve kernel functions svms mixed results latest contribution found genetic programming able rediscover multiple standard kernels signiﬁcant improvements standard kernels obtained results however transfered directly gaussian processes several major diﬀerences gaussian processes svms. particular case gaussian processes hyper-parameters optimized using ml-ii approach contrast svms hyper-parameter values usually tuned using crossvalidation grid-search. additionally contrast previous work simple embeddings covariance functions masking dimensions supported. gaussian process non-parametric model produces predictions solely speciﬁed mean covariance functions available training data inference function values observed input values based observations involves calculation covariance matrices inference multi-dimensional gaussian shown equation term necessary account gaussian distributed noise variance deﬁnition follows posterior multi-dimensional gaussian. model selection hyper-parameter learning marginal likelihood must calculated. model multidimensional gaussian analytical form likelihood derived. calculation marginal likelihood requires matrix inversion thus asymptotic complexity usually covariance function hyper-parameters must optimized. often accomplished simple ml-ii fashion optimizing hyper-parameters w.r.t. likelihood using quasi-newton method since gradients marginal likelihood hyper-parameters determined additional computational complexity hyper-parameter feasible gradient-based methods. drawback likelihood typically multimodal especially covariances many hyper-parameters optimizer converge local optimum. thus typically suggested execute several random restarts. better solution would include priors hyper-parameters optimizing w.r.t. posterior distribution however accomplished using mcmc approach computationally expensive. frequently used covariance functions gaussian processes include linear polynomial squared exponential rational quadratic mat´ern function. covariance functions combined complex covariance functions instance products sums diﬀerent covariance functions genetic programming generally refers automatic creation computer programs using genetic algorithms basic principle evolve variablelength structures frequently symbolic expression trees represent potential solutions problem. prominent applications genetic programming symbolic regression synthesis regression models without predetermined structure. genetic programming makes possible optimize structure solutions combination parameters. thus also possible synthesize composite covariance functions genetic programming. following grammar-guided genetic programming system make sure valid covariance functions produced. good survey grammar-guided genetic programming given grammar covariance functions derived rules composition kernels e.g. discussed noted grammar shown complete meaning several constructions would functions prod produce product multiple covariance functions composite covariance functions. scale operator used scaling factor covariance function. mask operator selects potentially empty subset input variables possible input variables. non-terminal symbol bitvector derived list zeros ones. vector used mask selected dimensions data eﬀectively reducing dimensionality. length mask match total number dimensions; checked resulting covariance function evaluated. finally non-terminal symbol terminalcov derived range default covariance functions. currently included isometric covariance functions covariance functions added grammar easily. grammar include hyper-parameters optimized genetic programming. instead hyper-parameters optimized potential solution using gradient-descent technique. experiments implemented gaussian processes commonly used covariance functions grammar covariance functions heuristiclab already provides implementation grammar-guided treebased genetic programming. experiments presented contribution mainly test feasibility idea. diﬀerent types data sets used experiments forecasts synthesized covariance functions compared default covariance functions also hand-tuned covariance functions. ﬁrst data univariate mauna atmospheric time series. data chosen hand-tuned covariance function data presented second experiment created several synthetic data sets sampled randomly two-dimensional gaussian process priors shown equation data generated functions diﬃcult model single isometric covariance function. multiple covariance functions combined correct dimension masking vectors training gaussian processes computationally expensive necessary optimize hyper-parameters evaluated covariance function time genetic programming algorithm grows quickly. therefore used restrictive parameter settings particular small population size individuals. parameter settings shown table parameter population size max. length height initialization parent selection mutation rate ml-ii iterations oﬀspring selection max. selection pressure max. generations results time series positive. algorithm able consistently covariance functions well training period accurate forecasts testing period structures exemplary solutions shown equation ﬁrst solution actually similar hand-tuned covariance solution proposed second covariance function complex slightly better likelihood. unfortunately genetic programming often leads overly complex solutions critical drawback approach. solutions found evaluated solution candidates achieve negative loglikelihood respectively. correlation coeﬃcients forecasts test partition figure shows output ﬁrst model. results synthetic two-dimensional data shown table experiment trained multiple gaussian process models using several frequently used covariance functions. trained many models using random restarts data covariance function report best negative log-likelihood pair. expected models isometric covariance functions well. contrast composite covariance functions produced genetic programming much better. comparison also report negative log-likelihood achieved optimal covariance function data set. experiments exact structure covariance could rediscovered thus evolved functions worse optimal solution. contribution described approach synthesis composite covariance function gaussian processes using grammar-guided genetic programming. proposed approach commonly used covariance functions table best negative log-likelihood achieved three synthetic two-dimensional test functions default covariance functions evolved composite covariance functions. used compose complex covariance functions using sums products several covariance functions. valid covariance functions deﬁned grammar genetic programming used search space possible derivations grammar. hyper-parameters covariance functions subject evolutionary search optimized w.r.t. likelihood using standard gradient-descent optimizer proposed approach tested types low-dimensional problems proof concept. found univariate mauna time series possible consistently good covariance functions genetic programming. identiﬁed solutions perform well hand-tuned covariance function problem. results two-dimensional synthetic functions show possible composite covariance functions perform much better default covariance functions data sets. contrast previous work genetic programming community focused mainly kernel synthesis svms contribution discusses kernel synthesis gaussian processes non-parametric fully bayesian models. gaussian process models hyper-parameters optimized standard gradient-descent approach strictly necessary execute cross-validation previous work either used grid-search crossvalidation tune hyper-parameters computationally expensive consider hyper-parameter optimization all. additionally using grammar compose covariance functions simple covariance functions instead evolving full function. statistics community recent contribution also discussed usage grammars composition covariance functions main diﬀerence work genetic programming used search derivations grammar. another relevant diﬀerence grammar contribution also supports simple embeddings masking function. noted analyzed genetic programming well suited task particular compare approach simple enumeration random search. another interesting topic future research look alternative ways searching space covariance functions deﬁned grammar. recently interesting approach described uses variational methods bayesian learning probabilistic context free grammars task idea could especially useful bayesian models gaussian processes. acknowledgments authors would like thank jeﬀrey emanuel initial idea leading contribution. work supported austrian research promotion agency behalf austrian federal ministry economy family youth within program josef ressel-centers.", "year": 2013}