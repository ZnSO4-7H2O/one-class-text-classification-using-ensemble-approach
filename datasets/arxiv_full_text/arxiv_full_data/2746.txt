{"title": "The Effects of Memory Replay in Reinforcement Learning", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Experience replay is a key technique behind many recent advances in deep reinforcement learning. Allowing the agent to learn from earlier memories can speed up learning and break undesirable temporal correlations. Despite its wide-spread application, very little is understood about the properties of experience replay. How does the amount of memory kept affect learning dynamics? Does it help to prioritize certain experiences? In this paper, we address these questions by formulating a dynamical systems ODE model of Q-learning with experience replay. We derive analytic solutions of the ODE for a simple setting. We show that even in this very simple setting, the amount of memory kept can substantially affect the agent's performance. Too much or too little memory both slow down learning. Moreover, we characterize regimes where prioritized replay harms the agent's learning. We show that our analytic solutions have excellent agreement with experiments. Finally, we propose a simple algorithm for adaptively changing the memory buffer size which achieves consistently good empirical performance.", "text": "experience replay technique behind many recent advances deep reinforcement learning. allowing agent learn earlier memories speed learning break undesirable temporal correlations. despite widespread application little understood properties experience replay. amount memory kept affect learning dynamics? help prioritize certain experiences? paper address questions formulating dynamical systems model q-learning experience replay. derive analytic solutions simple setting. show even simple setting amount memory kept substantially affect agent’s performance—too much little memory slow learning. moreover characterize regimes prioritized replay harms agent’s learning. show analytic solutions excellent agreement experiments. finally propose simple algorithm adaptively changing memory buffer size achieves consistently good empirical performance. reinforcement learning agent observes stream experiences uses experience update internal beliefs. example experience could tuple agent could experience update value function td-learning. standard algorithms experience immediately discarded it’s used update. recent breakthroughs leveraged important technique called experience replay experiences stored memory buffer certain size; buffer full oldest memories discarded. step random batch experiences sampled buffer update agent’s parameters. intuition experience replay breaks temporal correlations increases data usage computation efﬁciency combined deep learning experience replay enabled impressive performances alphago silver atari games mnih etc. despite apparent importance memory buffer popularity deep relatively little understood basic characteristics buffer size affect learning dynamics performance agent. practice memory buffer size determined heuristics ﬁxed agent. prioritized experience replay modiﬁcation whereby instead uniformly choosing experiences buffer update agent likely sample experiences surprising moore atkeson schaul empirical shown improve agent’s performance compared regular also lack good mathematical model per. contributions. paper perform ﬁrst rigorous study size memory buffer affects agent’s learning behavior. develop model experience replay prioritized replay. simple setting derive analytic solutions characterizing agent’s learning dynamics. solutions directly quantify effects memory buffer size learning rate. surprisingly even simple case value function model mismatch memory size non-monotonic effect learning rate. much little memory slow learning. moreover prioritized replay could also slow learning. conﬁrm theoretical predictions experiments. motivated develop simple adaptive experience replay algorithm automatically learn memory buffer size agent learning parameters. demonstrate consistently improves agent’s performance. related works. memory replay technique widely implemented experiments currently shown good performance different algorithms actor-critic algorithms wawrzy´nski deep q-network algorithms mnih double q-learning algorithms hasselt make good experience prioritized methods proposed algorithms moore atkeson peng williams main idea algorithm reinforcement learning experience replay input memory size minibatch size step size discount factor total steps initial weights update policy effect memory buffer extracted algorithm itself hidden mechanism hard perceive experiments black box. thus derive model simulate learning process. general results obtained numerically even analytically conﬁrmed good matches experiments. analytic approach enables systematically analyze replay memory affects learning process principle behind model corresponds continuous interpolation discrete learning step continuous approximation works well step size large i.e. dramatical change weights within steps. criterion often real experiments. details derivation appendix. prioritization sample transitions lead larger value change frequently. probability selecting experience determined relative magnitude temporal difference error reported effective many experiments moore atkeson seijen sutton schaul measures td-error also literature weight experience; examples include rewards tessler transition property peng performance experience replay similar batch incremental lagoudakis parr kalyanakrishnan stone ernst another approach reuse data called model-learning dyna architecture builds model simulate generate data sutton sutton method however induces extra computation cost modeling error data. task agent takes actions observes states receives rewards sequence interaction environment. goal learn strategy leads best possible reward. standard learning framework agent action-value function learn optimal behavior perform action selection. optimal action-value deﬁned maximum expected return agent starts state takes ﬁrst action satisﬁes practice state space usually large function approximation adopted estimate action-value function deep q-network example approach. learning step commonly-used td-learning method updates weight according able analyze learning process speciﬁcally weights state function learning step based experiment needed inﬂuence memory buffer parameters analyzed explicitly analytical solutions. theoretical model validated experiments based algorithms. prioritized replay proposes speed learning process sampling experience transitions according non-uniform probability distribution. commonly used model parametrize probability selecting transition starting game call linesearch analytically solve odes learning dynamics quantify effects memory. characterize settings helps hinders learning tasks compared finally show theoretical predictions excellent agreement experiments. model setup. ﬁrst deﬁne simple game linesearch space agent state dimensional reward function linear action binary {v−v} constant. transition next state determined adding action value current state i.e. discount factor real action-value function figure learning curves metrics weights agent real weights respectively. scattered blue dots orange squares represent experimental results using algorithm algorithm. blue orange curves numerical solutions based theoretical model. theoretical predictions excellent agreement experimental results learning dynamics. dependence ﬁnal absolute metric |∆θ| |∆θ| memory size different minibatch sizes. note smaller stands better performance. original setting discount factor indicate batch size. contour plot measure function memory size minibatch size discount factor stars denote optimal memory sizes given minibatch values. plots corresponds situations minibatch size region stands situations works better. rest white region situations settings behave similarly. precisely absolute difference less white area. plotted based theory predictions also experiments well similar learning process theory calculated based dynamic equations metrics i.e. noted weights obtained time detailed analytical solution discussion odes given appendix. effects memory size. solving odes found memory setting non-monotonic effect performance. able extract mechanism behind phenomenon analytic expressions. ﬁxed real value metric solved analytically turn general situation updates weights coupled together. examine performance deﬁne measure |∆θ| |∆θ| i.e. metrics absolute values learning step game. smaller measure better agent learns. fig. plots dependence memory size minibatch size step size learning performance affected non-monotonically memory size monotonic relation observed shown fig. example optimal memory size around exists denoted curve fig. contrast measure experiences monotonic decrease growth memory size plotted blue curve fig. inﬂuence memory setting arises trade-off overshooting weight update. term overshooting describes phenomenon weights updated wrong direction. example fig. actually moves away times also overshoots incurs negative bias. ﬁrst address settings small minibatches. memory size also small learning process likely overshoot limited memory capacity. replay memory enlarged overshooting effect mitigated. increase memory size averaged weight update ﬁrst becomes slowly slightly accelerates. balance contributions leads nonmonotonic nature. large still trade-off overshooting increasing weight update. however latter counteract former quick convergence induced large update. analytical expressions numerical results combined illustrations appendix. performance prioritized replay. compare discuss memory buffer affects based theoretical model. fig. plots learning curve exhibits similar property fig. compare performance algorithms fig. blue regions stand cases better white areas represent situations algorithms perform similarly. shown performs relatively worse memory size small particularly minibatch size large. also attributed trade-off between overshooting quick weight update. small memory size overshooting effect serious prioritized sampling large memory prioritized agents update weight quicker leads faster convergence. demonstrations given appendix. nonzero discount factor. discount factor previous subsections simplicity. show learning dynamics qualitatively similar case linesearch game. nonzero discount factor i.e. considering long-term effect real action-value function greedy policy algorithm adaptive memory size reinforcement learning experience replay input initial memory size minibatch size step size discount factor total steps initial weights update policy number checked oldest transitions nold memory adjustment internal |δold| observe initial state kept longer used future updates. hand error magnitude oldest transitions buffer starts decrease time older memories likely less useful agent decreases memory buffer accelerate learning process. intuition. lesson section useful adaptive algorithm increase memory capacity overshooting effect dominates shrink memory buffer weight update becomes slow. change absolute error oldest memories buffer proxy whether agent overﬁtting recent memories. intuition error magnitude oldest transitions buffer starts increase—i.e. data violate bellman equation severely agent learns—then sign agent might overshooting overﬁtting recent experiences. case increase memory buffer ensure older experiences algorithm description. memory size adaptively changed according error magnitude change oldest transitions characterized |δold|−|δold|. |δold| |δold| deﬁned absolute errors nold transitions memory nold hyperparameter denotes number data |δold| ﬁrst calculated. choose examine. steps derive |δold| compare |δold|. speciﬁcally every steps change absolute error magnitude transitions decreases i.e. |δold| |δold| memory shrinks otherwise increases given algorithm denoted aer. performance linesearch. ﬁrst analyze works linesearch game. minibatch size step size agent adaptively adjusts memory capacity depicted fig. compared setting ﬁxed memory size agent updates weights effectively overshooting effect mitigated indicated fig. used fully connected neural network value function approximation layer cartpole layers mountaincar acrobot. randomly initialize weights minibatch size memory adjustment internal checked transitions cover half initial memory size randomly sample experiences approximate |δold| cartpole discount factor cartpole mountaincar acrobot. step size cartpole mountaincar acrobot. first cartpole game starts initial memory size agent speeds learning adaptive adjustments memory size shown fig. fig. curve averaged games. larger static memory always better best performance achieved experience discarded corresponding full size second example mountaincar found optimal ﬁxed memory size around indicated fig. fig. result averaged games. smaller larger static memory sizes reduce learning. setting learns increase memory size initial memory small also learns decrease capacity initial memory large. trials outperforms averaged ﬁxed memory results initial size respectively. note dynamical change memory size also enables agent even perform better best static result. last also demonstrates good performance acrobot game relatively smaller ﬁxed memory preferred plotted fig. fig. curve averaged games. initial buffer size outperforms averaged ﬁxed memory approach experiments respectively. agent learns accelerate learning dynamically decreasing memory size. additional considerations little extra computation cost needed carry aer. taking cartpole game example every steps need compute forward passes neural network without backpropogation. examined batch size sampling minibatch update. simple algorithm tendency shrink memory already shows good performance experiments. goal learning process diminish error amplitude data updated |δold| likely less |δold| average sense. possible solution change criterion shrinking memory buffer |δold| |δold| could predeﬁned constant learned online averaged error amplitude change whole dataset previous |δold| |δold| value. analytic solutions conﬁrmed experiments demonstrate size memory buffer substantially affect agent’s learning dynamics even simple settings. perhaps surprisingly memory size effect non-monotonic even model mismatch between true value function agent’s value function. little much memory slow speed agent’s learning correct value function. developed simple adaptive memory algorithm evaluates usefulness older memories learns automatically adjust buffer size. shows consistent improvements current static memory size algorithms four settings evaluated. many interesting directions extend adaptive approach. example could adaptively learn prioritization scheme improves upon prioritized replay. paper focused simple settings order derive clean conceptual insights. systematic evaluation effects memory buffer large scale projects would also great interest. utilizing model able analyze learning properties inﬂuence replay memory systematically. choose replay memory settings memory size minibatch size discussed. note real action-value function qreal weight initialized initial metric correspondingly. metric function learning step calculated analytically learning process consists parts. beginning exponent gradually grows described replay memory gets full memory buffer becomes sliding window exponent evolves according metric approaches desired value exponentially exponent cubic function learning step fig. present learning curves exponent metric different memory sizes minibatch size step size whole learning process exponent grows monotonically metric decreases monotonically indicated agent learn certain amount knowledge used represent performance. total steps required reach chosen stand learning ability agent. value desired exponent value. instance agent thought learn well metric .∆θ. note agent viewed better learner uses less training time i.e. less learning steps achieve first always beneﬁcial larger minibatch case. hand exponent strictly proportional minibatch size hand exponent monotonically increases whole learning process. thus larger minibatch agent faster learns. mentioned real experiments minibatch cannot large cause gradient step size required small guarantee validity td-error update gradient step size proportional minibatch size deﬁned algorithm. second learning ability agent represented total steps agent takes initial exponent nonmonotonic dependence memory size plotted fig. learning proceeds weight update currently collected transition i.e. αδi∆θq ﬁrst decreases grows moment state increases time transition setting weight update always correct direction towards real value larger weight update always preferable. beginning game older transitions memory replay contribute weight update larger memory desirable. soon agent reaches region weight update current state large enough compared average past experience updating current transition becomes good strategy. illustrated blue curve fig. memory size grows slowly start velocity exponent increases gradually surpasses conditions different memory size grows rapidly among possible choices learning step thus optimal memory size increases growth target exponent abruptly falls certain kand remains thereafter demonstrated fig. prioritized method always outperforms uniformly selection approach ﬁxed-intercept case. fact that larger absolute td-error corresponds larger derivative action-value function respect weight furthermore corresponds larger weight update. mentioned above larger weight update always beneﬁcial. prioritized setting transitions larger td-error value frequently selected leading faster convergence real weight value. also demonstrated analytically difference exponent prioritized setting kpri original written equation suggests metric also decreases target value exponentially exponent proportional learning step strong contrast exponent third-order polynomial respect ﬁxed-intercept case. thus learning generally slower ﬁxed-slope case ﬁxed-intercept case. example minibatch size step size takes ﬁxed-slope agent learning steps achieve ∆θ/∆θ ﬁxed-intercept agent needs less steps learn. learning process results totally independent initial state velocity state changing importantly memory size. learning dynamics fully described step size minibatch size initial weights. fact transitions identically useful ﬁxed-slope situation. learning step easily proved td-error weight update transition possible state values. selection data update longer matters different replay memory settings performance. similarly prioritized method learning results original setting cause transitions equal sense weight update. conﬁrmed theoretical calculation yields exponent prioritized setting kpri original satisfy figure learning curve metric conditions prioritized method withprioritized method ﬁxed intercept case. memory size batch size illustration purpose. figure contour plot ﬁnal absolute metric |∆θ| |∆θ| function memory size minibatch size algorithms. smaller ﬁnal total metric stands better performance. stars denote optimal memory sizes given minibatch values. contour plot difference ﬁnal absolute metric original setting prioritized setting function memory size minibatch size i.e. result subtracting fig. fig. positive value represents prioritized method useful negative value denotes prioritized method harmful addressed metric converge agent changes state unidirectionally movement ﬁnally reaches balance weight update. thus order make metric near possible last crucial agent small value enters ﬁnal stage learning process. illustrate details memory setting affects learning performance trade-off overshooting weight update. memory size small overshooting likely take place owing limited memory size. depicted solid curves fig. memory size metric gradually ﬁtted positive negative remains positive ﬁrst stage. know that case absolute value approaches large conﬁrmed fig. growth memory size overshooting effect mitigated. case weight update ﬁrst decelerated slightly accelerates. optimal memory size reached around depicted fig. memory size continues increase agent suffers little overshooting issue. convergence slower optimal because smaller weight update shown fig. swift change metrics remain constant short period shown fig. note value changing behavior happens fast values given rely memory size policy minibatch size even step size. estimated values agent enters last stage. learning curve last stage illustrated fig. guess vary lot. rigorous result observe delayed effect fact agent fully enter last stage experiment time scale neglected terms derivation also contribute result. ﬁnal absolute metric memory size minibatch size exhibits similar behavior original setting causes also similar. nonmonotonic dependence memory given batch size less decreases monotonically increase memory. fig. depicts difference original prioritized settings depend memory size minibatch size. positive difference value means prioritized setting better. fig. derived principle algorithm found perform relatively worse memory size minibatch size small indicated fig. could also explained trade-off overshooting quick weight update similar situation fig. plotted fig. fig. memory size prioritized setting makes overshooting even worse; weights quickly updated prioritized agents converge faster. noted complicated situations two-dimensional-weight situation. prioritized scheme always results faster weight update necessarily hold true according deﬁnition weight update. example fig. learning step non-prioritized case actually learns faster prioritized case. figure learning curve metrics θ−θr weights agent real weights respectively. scattered blue dots orange squares represent experimental results based algorithm. blue orange curve theoretical solutions", "year": 2017}