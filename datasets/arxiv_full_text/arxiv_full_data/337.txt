{"title": "Generalization Error of Invariant Classifiers", "tag": ["stat.ML", "cs.AI", "cs.CV", "cs.LG"], "abstract": "This paper studies the generalization error of invariant classifiers. In particular, we consider the common scenario where the classification task is invariant to certain transformations of the input, and that the classifier is constructed (or learned) to be invariant to these transformations. Our approach relies on factoring the input space into a product of a base space and a set of transformations. We show that whereas the generalization error of a non-invariant classifier is proportional to the complexity of the input space, the generalization error of an invariant classifier is proportional to the complexity of the base space. We also derive a set of sufficient conditions on the geometry of the base space and the set of transformations that ensure that the complexity of the base space is much smaller than the complexity of the input space. Our analysis applies to general classifiers such as convolutional neural networks. We demonstrate the implications of the developed theory for such classifiers with experiments on the MNIST and CIFAR-10 datasets.", "text": "paper studies generalization error invariant classiﬁers. particular consider common scenario classiﬁcation task invariant certain transformations input classiﬁer constructed invariant transformations. approach relies factoring input space product base space transformations. show whereas generalization error non-invariant classiﬁer proportional complexity input space generalization error invariant classiﬁer proportional complexity base space. also derive suﬃcient conditions geometry base space transformations ensure complexity base space much smaller complexity input space. analysis applies general classiﬁers convolutional neural networks. demonstrate implications developed theory classiﬁers experiments mnist cifar- datasets. fundamental topics statistical learning theory generalization error given training hypothesis class learning algorithm chooses hypothesis based training minimizes empirical loss. loss calculated training also called training loss often underestimates expected loss. diﬀerence empirical loss expected loss. various approaches literature bounding complexity measures hypothesis class vc-dimenension fat-shattering dimension rademacher gaussian complexities another line work provides bounds based stability algorithms measuring sensitive output removal change single training sample finally recent work mannor bounds terms notion algorithmic robustness. important property bounds distribution agnostic i.e. hold distribution sample space. moreover bounds lead principled derivation learning algorithms guarantees e.g. support vector machine extension non-linear classiﬁcation kernel machines however design learning algorithms practice rely complexity measures hypothesis class also relies exploiting underlying structure present data. prominent example associated ﬁeld computer vision features learning algorithms designed invariant intrinsic variability data image classiﬁcation particular computer vision task requires representations invariant various nuisances/transformations viewpoint illumination variations commonly present natural images contain helpful information identity classiﬁed object. motivates develop theory learning algorithms invariant certain sets transformations. invariant methods studied vc-dimension abu-mostafa shown subset hypothesis class invariant certain transformations smaller general hypothesis class. therefore smaller vc-dimension. authors provide characterization much smaller vcdimension invariant method might similarly group symmetry data distribution also explored problem covariance estimation shown leveraging group symmetry leads gains sample complexity covariance matrix estimation various examples literature aims understand/leverage role invariance data processing. example convolutional neural networks known achieve state results image recognition speech recognition many tasks known possess certain invariances. invariance cnns achieved careful design architecture invariant various transformations rotation scale aﬃne deformations training augmented training meaning training augmented transformed versions training samples learned network approximately invariant another example translation invariant method scattering transform cnn-like transform based wavelets point-wise non-linearities also practice learning techniques achieve lower non-invariant counterparts. poggio anselmi study biologically plausible learning invariant representations connect results cnns. role convolutions pooling context natural images also studied cohen shashua motivated examples work proposes theoretical framework study invariant learning algorithms shows invariant learning technique much smaller non-invariant learning technique. moreover work directly relates diﬀerence bounds size transformations learning algorithm invariant approach signiﬁcantly diﬀerent focuses complexity data rather complexity hypothesis class. distribution deﬁned sample space {si}m learning algorithm takes training maps learned hypothesis asm. loss function hypothesis sample denoted empirical loss expected loss learned hypothesis deﬁned represents number classes represents dimension input signal represents parameters feature extractor. classiﬁer deﬁned feature extractor given classiﬁcation task goal learning hypothesis separates training samples different classes. model deﬁne score training sample measures conﬁdent prediction classiﬁer note large score training samples imply learned hypothesis small work leverage bounds provided sokoli´c providing bounds deﬁne notion learning algorithm stability notion covering number crucial bounds. leads covering number deﬁnition cover sets form take training invariance decision region implies stable invariant learning algorithm )-robust. transformations i.e. really interested access transformed samples clearly label. words class labels case reasonable leverage using invariant learning algorithm. deﬁnition learning algonote transformations implemented permutation matrices orthonormal. important implies considered sets satisfy condition examples transformed atoms shown figure invariance architecturegiven transformations group averaging function group leads invariant representation example conventional cnns pooling operators usually average translation group make cnns translation invariant. cohen welling generalize notion convolution translation group general groups leads architectures invariant arbitrary transformations form discrete groups. related approach involves normalization network input eliminates eﬀect aﬃne transformations input invariance learningalternative encoding invariances architecture train become invariant. particularly helpful cases know exactly characterize impose invariance manually network. approximate invariance achieved training cnns data augmentation involves training network transformed samples training examples. indicated lenc vedaldi showed cnns trained imagenet implicitly learn invariant ﬂips scalings rotations. provide example base space transformation provide example base space transformation example consider {cross circle corner curve} translation set. contains possible translations shapes veriﬁed therefore dimension take {corner curve} trans-rotation establish examples consider {cross circle} rotation set. therefore contains possible rotations circle cross figure clear circle cross already invariant rotation i.e. corresponds exactly shape. therefore condition holds section discuss implication theory cnns popular classiﬁcation. note particular example theory holds also possible classiﬁers. consider ways invariance achieved cnns appropriate construction arant conventional includes cyclic slice layer ﬁrst convolutional layer cyclic pool layer softmax layer. both cyclic slice layer cyclic pool layer proposed dieleman together ensure invariant rotations. particular cyclic slice layer takes input image creates copies rotated networks trained using stochastic gradient descent momentum training objective standard categorical cross entropy loss. batch size learning rate reduced epochs. networks trained epochs classiﬁcation accuracies reported figure reported figure ratio invariant conventional cnns shown figure note rotation invariant always higher classiﬁcation accuracy conventional cnn. moreover rotation invariant much smaller conventional cnn. diﬀerence signiﬁcant training small demonstrates importance invariance generalization learning algorithms. roughly same whereas conventional cnns higher datasets smaller explained fact rotated mnist dataset smaller complex larger number rotations. sizes transformation sets respectively. theorem predicts ratio invariant non-invariant cnns equal observe ratios obtained empirically roughly follow theoretical prediction. however training small conventional generalizes worse predicted theory training large conventional generalizes better predicted theory. conjecture conventional cnns learn partially invariant number training samples large. moreover current theory might capture relationship invariant non-invariant cnns entirely especially assumptions theorem hold. copies averaged obtain sample. theory suggests rotation invariant cnns case lower conventional dataset rotation invariant. fact given rotation invariant mnist dataset rotation invariant conventional equivalent. easily established observing cyclic slicing layer produces copies input identical. veriﬁed empirically rotation invariant non-invariant cnns perform rotation invariant mnist dataset. batches constructed follows ﬁrst half batch contains images training half mini batch contains transformed versions images ﬁrst half mini batch transformations chosen ranpromote invariance using regularizer chose regularize output last global pooling layer instead softmax output corresponding pairs batch compute classiﬁcation accuracies test augmented test cnns trained invariance regularization cnns trained without invariance regularization reported table training accuracies close cases. first observe invariance regularization leads lower cases. moreover testing augmented test even robust leads lower both regularized non-regularized cnns. note however cnns trained explicit invariance regularization performs better non-regularized network evaluated augmented test testing augmented test approximately times expensive conventional testing single image. experiment veriﬁes hypothesis enforcing network invariance explicitly lead smaller ratio trained data augmentation invariance regularization trained without data augmentation between note theory section apply directly trained withdata augmentation already invariant translations convolutional structure pooling formally demonstrated invariant learning algorithm much smaller non-invariant learning algorithm provided input space factorized product transformation base space covering number base space much smaller covering number input space. work offers important foundation study learning algorithms cnns extensions leverage symmetries data.", "year": 2016}