{"title": "Adversarial vulnerability for any classifier", "tag": ["cs.LG", "cs.CR", "cs.CV", "stat.ML"], "abstract": "Despite achieving impressive and often superhuman performance on multiple benchmarks, state-of-the-art deep networks remain highly vulnerable to perturbations: adding small, imperceptible, adversarial perturbations can lead to very high error rates. Provided the data distribution is defined using a generative model mapping latent vectors to datapoints in the distribution, we prove that no classifier can be robust to adversarial perturbations when the latent space is sufficiently large and the generative model sufficiently smooth. Under the same conditions, we prove the existence of adversarial perturbations that transfer well across different models with small risk. We conclude the paper with experiments validating the theoretical bounds.", "text": "despite achieving impressive often superhuman performance multiple benchmarks state-of-the-art deep networks remain highly vulnerable perturbations adding small imperceptible adversarial perturbations lead high error rates. provided data distribution deﬁned using generative model mapping latent vectors datapoints distribution prove classiﬁer robust adversarial perturbations latent space suﬃciently large generative model suﬃciently smooth. conditions prove existence adversarial perturbations transfer well across diﬀerent models small risk. conclude paper experiments validating theoretical bounds. deep neural networks powerful models achieve state-of-the-art performance across several domains bioinformatics speech computer vision though deep networks exhibited good performance classiﬁcation tasks recently shown unstable adversarial perturbations data fact small often imperceptible perturbations data samples suﬃcient fool state-of-the-art classiﬁers result incorrect classiﬁcation. discovery surprising vulnerability classiﬁers perturbations large body work attempts design robust classiﬁers. however advances designing robust classiﬁers accompanied stronger perturbation schemes easily defeat defenses fact date successful scalable strategy defend adversarial perturbations. leads following natural question main result prove data distribution deﬁned smooth generative model suﬃciently large latent space classiﬁer robust adversarial noise. addition prove existence perturbations transferable classiﬁers small risk intriguing property empirically observed speciﬁcally establish upper bounds classiﬁers’ robustness perturbations applicable classiﬁcation function. main assumption existence generative model transforms normally-distributed random latent vectors natural data. assumption motivated numerous previous works generative modelling whereby natural-looking images obtained transforming normal vectors deep neural network data model classiﬁer-agnostic bounds provide limits maximal robustness hope achieve. speciﬁcally ﬁrst focus robustness perturbations latent space; assuming latent space suﬃciently high dimensional prove classiﬁer existence small perturbations latent space cause misclassiﬁcation. then show existence perturbations latent space implies existence small adversarial perturbations image space provided generator satisﬁes smoothness condition speciﬁcally gives fundamental upper bounds robustness classiﬁer perturbations image space provided data follows model. main technical tool gaussian isoperimetric inequality gives lower bounds widenings arbitrary measurable sets gaussian space. noted bounds hold even generative model exactly model data distribution rather provides good approximation. assuming data distribution δ-close wasserstein sense obtained generative model upper bounds still hold. assumption inline recent advances generative models whereby generator provides good approximation true distribution exactly finally prove intriguing properties adversarial perturbations; transferability show arbitrary classiﬁers small risk shared adversarial perturbations latent space. existence transferable adversarial perturbations across diﬀerent models important implications security perspective opens door designing adversarial perturbations models known adversary. vulnerability classiﬁers perturbations ﬁrst highlighted studied context deep neural networks proven certain families classiﬁers exists adversarial perturbations cause misclassiﬁcation magnitude data dimension provided robustness random noise ﬁxed addition fundamental limits robustness classiﬁers derived simple classiﬁcation families risk dependent upper bounds apply classiﬁcation functions family highlight existence tradeoﬀ classiﬁcation robustness risk work instead derive bounds hold classiﬁcation function assume model data distribution recently lower bounds adversarial robustness diﬀerent classiﬁcation models shown contrast works stress goal instead show upper bounds robustness classiﬁer-agnostic. previous works focused proving conditions classiﬁer robust goal show classiﬁers robust provided conditions data met. causes behind high vulnerability state-of-the-art networks perturbations still completely understood several hypotheses proposed analyzing simple classiﬁers tasks simultaneously large number techniques recently proposed improve robustness classiﬁers perturbations adversarial training robust optimization regularization distillation stochastic networks unfortunately recent techniques shown fail whenever complex attack strategy used recently algorithms provide provably robust classiﬁers proposed tested small datasets large scale high dimensional datasets remains however unclear whether possible design robust classiﬁcation methods. goal show exists upper bounds robustness classiﬁer surpass. finally note robustness generative models empirically analyzed unlike works goal assess robustness generative models perturbations; instead analyze robustness classiﬁers data generated according generative models. problem setting arbitrary k-class classiﬁer operating space often refer space space images dimensionality refers number pixels image assume existence generative model maps latent vectors space images speciﬁcally generate image according distribution natural images generate random vector apply rm). words distribution images pushforward standard procedure used generate images state-of-the-art generative models. denote latent vectors generate images belonging class note {ci}i partition space classiﬁer clear context simply write note measure classes equiprobable remainder paper goal analyse limits maximal achievable robustness adversarial perturbations measuring robustness latent space image space interpret limits robustness crucial compare adversarial perturbations right quantity. image space perturbations often compared norm image perturbation deemed suﬃciently small whenever robustness much smaller norm image interpret robustness latent space follow similar methodology compare classiﬁer robust perturbations latent space assuming coordinates encode diﬀerent factors variation image condition indicates tiny changes latent factors variation cause data misclassiﬁcation. next section bounds robustness quantities arbitrary classiﬁers. comparison meaningful generative mode non-degenerate; dimension latent space dummy variables eﬀect norm increases dimension remains constant. assume remainder paper generative model non-degenerate; i.e. latent coordinates eﬀect image. remark dependence theorem shows increasing probability fooling number classes words easier adversarial perturbations setting number classes large binary classiﬁcation task. dependence conﬁrms empirical results whereby robustness observed decrease number classes. dependence captured bounds contrast previous bounds showed decreasing probability fooling classiﬁer larger number classes remark classiﬁcation-agnostic bound. bounds hold classiﬁcation function speciﬁc family classiﬁers. unlike work establishes bounds robustness speciﬁc classes functions appendix details calculation. compares general bound theorem seen checkerboard partition example probability fooling converges much quicker general result theorem hence classiﬁer creates many disconnected classiﬁcation regions much vulnerable perturbations linear classiﬁer latent space. taking typical value fig. reports numerical values upper bound function number classes note that similarly observation theorem upper bound gets smaller increasing number classes. compared theorem bounds robustness adversarial perturbations extra price transferable adversarial perturbations term small risk classiﬁers small. hence bounds provide theoretical explanation existence transferable adversarial perturbations previously shown exist existence transferable adversarial perturbations across several models small risk important security implications adversaries principle fool diﬀerent classiﬁers single classiﬁer-agnostic perturbation. existence perturbations signiﬁcantly reduces diﬃculty attacking machine learning models. goal derive bounds robustness image space. fundamental ingredient relate distances latent space distances image space. assuming generative model admits modulus continuity property this assumption simplify statement general statement easily derived way. this assumption extended random ease exposition however deterministic assumpnote assumption milder assuming lipschitz continuity. lipschitz continuity assumption corresponds choosing linear function diﬃcult satisfy practice. replaced \u0001/l. high probability bounds derived latent space therefore also valid bound robustness image space provided generator function admits modulus continuity. following extend generalize upper bound expectation image space. theorem provides bounds similar theorem expected robustness image space provided generator admits modulus continuity. moreover theorem shows similar upper bounds robustness even generative model approximates true unknown distribution. relaxed assumption practical relevance state-ofthe-art generative models known provide good approximations true distribution wasserstein sense even model exactly distribution natural images. theorem provides formal justiﬁcation showing results also hold even cases generative model perfectly model data distribution. finally note transferability result holds straightforward image space provided modulus continuity assumption holds. constrained belong data distribution easy bounds derived in-distribution robustness also hold unconstrained robustness since clearly holds wonder whether possible better upper bound directly. show possible require result therefore shows that classiﬁer-agnostic bounds concerned fundamental diﬀerence in-distribution unconstrained robustness. speciﬁcally classiﬁer in-distribution robustness construct classiﬁer unconstrained robustness hence tight upper bounds derived in-distribution robustness also tight unconstrained setting. words universal limits derived notions robustness essentially same. finally note procedure construct suggests robustify classiﬁer out-distribution perturbations. nearest neighbour strategy useful in-distribution robustness much larger unconstrained robustness allows essentially latter match former. approach recently implemented others shown eﬀective improve robustness. evaluate bounds svhn dataset dataset contains color images house numbers task classify digit center image. dataset contains training images test images train dcgan generative model dataset latent vector dimension consider several neural networks architectures classiﬁcation. classiﬁer empirical robustness computed z-space diﬀerent tested classiﬁers. suggests bounds z-space provide tight estimates robustness state-of-the-art deep networks. compute upper bound robustness image-space estimate empirically smoothness generator apply bound theorem image space theoretical prediction classiﬁer-agnostic bounds order magnitude larger empirical estimates. note however bound non-vacuous predicts norm required perturbation approximately norm images. potentially leaves room improving robustness image space. however noted that unlike comparisons ﬁgures in-distribution unconstrained robustness image space interestingly shows simple lenet architecture large exists quantities. however using complex classiﬁers in-distribution unconstrained robustness gets smaller. recall theorem says classiﬁer modiﬁed in-distribution robustness unconstrained robustness diﬀer factor preserving accuracy. modiﬁcation result much complicated classiﬁer compared original one; example starting linear classiﬁer modiﬁed classiﬁer general linear. thus natural expect consider ﬂexible family classiﬁers in-distribution unconstrained robustness gets smaller. interestingly experiments approaches showed existence classiﬁer-agnostic upper bounds robustness adversarial perturbations assuming data generated according smooth generative model suﬃciently large latent space. exceeding bounds possible classiﬁer; goal therefore approach bounds. list several implications paper used probabilistic version modulus continuity property required satisﬁed rather high probability accounted error probability bound. used gradient descent strategy worst-case leading largest possible robustify classiﬁer provided access generative model implementing nearest neighbour strategy classiﬁer close in-distribution out-distribution robustness hence achieve better robustness. unconstrained robustness upper bounds obtained empirical evaluation results currently large i.e. order magnitude larger unconstrained robustness state-of-the-art classiﬁers. possibly datasets svhn. moving complex datasets likely require larger latent space dimensions hence lead smaller bounds robustness classiﬁers perturbations. bounds intriguingly applicable classiﬁcation function including human visual system. however unclear interpret result cautious concluding humans robust adversarial perturbations. list alternative explanations recall write cumulative distribution function standard gaussian distribution technical tool used prove results paper. theorem gaussian measure s.t. used concavity along jensen’s inequality denotes perturbation norm causes misclassiﬁcation. statement follows using theorem assume denotes wasserstein distance coupling construct random variable almost surely classiﬁed diﬀerently. deﬁne classiﬁed diﬀerently otherwise deﬁned vector minimum norm classiﬁed diﬀerently. r∗)))", "year": 2018}