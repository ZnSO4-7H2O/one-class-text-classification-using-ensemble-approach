{"title": "On the Theory and Practice of Privacy-Preserving Bayesian Data Analysis", "tag": ["cs.LG", "cs.AI", "cs.CR", "stat.ML"], "abstract": "Bayesian inference has great promise for the privacy-preserving analysis of sensitive data, as posterior sampling automatically preserves differential privacy, an algorithmic notion of data privacy, under certain conditions (Dimitrakakis et al., 2014; Wang et al., 2015). While this one posterior sample (OPS) approach elegantly provides privacy \"for free,\" it is data inefficient in the sense of asymptotic relative efficiency (ARE). We show that a simple alternative based on the Laplace mechanism, the workhorse of differential privacy, is as asymptotically efficient as non-private posterior inference, under general assumptions. This technique also has practical advantages including efficient use of the privacy budget for MCMC. We demonstrate the practicality of our approach on a time-series analysis of sensitive military records from the Afghanistan and Iraq wars disclosed by the Wikileaks organization.", "text": "bayesian inference great promise privacy-preserving analysis sensitive data posterior sampling automatically preserves differential privacy algorithmic notion data privacy certain conditions posterior sample approach elegantly provides privacy free data inefﬁcient sense asymptotic relative efﬁciency show simple alternative based laplace mechanism workhorse differential privacy asymptotically efﬁcient non-private posterior inference general assumptions. technique also practical advantages including efﬁcient privacy budget mcmc. demonstrate practicality approach time-series analysis sensitive military records afghanistan iraq wars disclosed wikileaks organization. probabilistic models trained bayesian inference widely successfully used application domains privacy invaluable text analysis personalization medical informatics moocs applications data scientists must carefully balance beneﬁts potential insights data analysis privacy concerns individuals whose data studied dwork placed notion privacy-preserving data analysis solid foundation introducing differential privacy algorithmic formulation privacy gold standard privacypreserving data-driven algorithms. differential privacy measures privacy cost algorithm. designing privacy-preserving methods goal achieve good trade-off privacy utility ideally improves amount available data. observed dimitrakakis wang bayesian posterior sampling behaves synergistically differential privacy automatically provides degree differential privacy certain conditions. however substantial gaps elegant theory practical reality bayesian data analysis. privacy-preserving posterior sampling hampered data inefﬁciency measured asymptotic relative efﬁciency practice generally requires artiﬁcially selected constraints spaces parameters well data points. privacy properties also typically guaranteed approximate inference. paper identiﬁes gaps theory practice begins mend extremely simple alternative technique based workhorse differential privacy laplace mechanism approach equivalent generalization zhang recently independently proposed algorithm beta-bernoulli systems. provide theoretical analysis empirical validation advantages proposed method. extend method dimitrakakis wang posterior sample method case approximate inference privacy-preserving mcmc. finally demonstrate practical applicability technique showing privacy-preserving model analyze sensitive military records iraq afghanistan wars leaked wikileaks organization. primary contributions follows property differential privacy holds composition additive accumulation. theorem -differentially private -differentially private -differentially private. allows view total procedure privacy budget spend across operations analysis. also exists advanced composition theorem provides privacy guarantees adversarial adaptive scenario called k-fold composition also allows analyst trade increased smaller scenario differential privacy also immune data-independent post-processing. suppose would like differentially private draw parameters latent variables interest posterior private dataset. accomplish interpreting posterior sampling instance exponential mechanism utility function i.e. joint probability chosen assignment dataset draw differential privacy formal notion privacy data-driven algorithms. algorithm differentially private probabilities outputs algorithms change much individual’s data point modiﬁed thereby revealing little information individual’s data. precisely randomized algorithm said -differentially private straightforward method obtaining \u0001-differential privacy known laplace mechanism adds laplace noise revealed information amount noise depends quantiﬁable notion sensitivity changes database. specifically sensitivity function deﬁned wang derived form result ﬁrst principles noting exponential mechanism used here. although explicitly state theorem implicitly show noteworthy special cases referred posterior sample procedure. state ﬁrst cases theorem maxx∈χθ∈θ releasing sample posterior distribution prior b-differentially private. exponential mechanism provides adjustable knob trading privacy ﬁdelity. procedure samples uniform distribution giving away information procedure reduces sampling posterior approaches inﬁnity procedure becomes increasingly likely sample assignment highest posterior probability. assuming goal sample rather mode would procedure order correctly sample true posterior. generally privacy budget integer draw posterior samples within budget. energy state physical system temperature system reducing corresponds increasing temperature understood altering distribution markov chain moves state space rapidly. analyzing privacy cost sampling exponential family posteriors general case recover privacy properties many standard distributions. results applied full posterior sampling feasible gibbs sampling updates discuss section section analyze privacy cost sampling exponential family posterior distributions exactly exponential mechanism following dimitrakakis wang method based laplace mechanism generalization zhang properties methods compared table choose data dataset-speciﬁc privacy cost posterior sampling approaches worst case given exponential mechanism increases causing posterior concentrate worst-case here provide simple laplace mechanism alternative exponential family posteriors becomes increasingly faithful true posterior data points increases ﬁxed privacy cost general assumptions. approach based observation exponential family posteriors equation data interacts distribution aggrei= release privatized versions statistics perform operations we’d like including drawing samples computing moments quantiles straightforwardly accomplished laplace mechanism note corresponding standard untruncated beta distribution sensitivity unbounded. makes intuitive sense datasets impossible violates differential privacy. limitation exponential mechanism approach private bayesian inference temperature approximate posterior ﬁxed willing regardless number data points posterior becomes accurate increases approximation becomes accurate proxy approximation remains factor ﬂatter posterior data points. simply limitation analysis. adversary beta distribution posterior bernoulli observations. estimator requires distribution truncated controls exponential mechanism sensitivity determines temperature distribution i.e. extent distribution ﬂattened given here contrast laplace mechanism achieves privacy adding noise sufﬁcient statistics case pseudo-counts successes failures posterior distribution. figure illustrate ﬁdelity beneﬁts posterior sampling based laplace mechanism instead exponential mechanism amount data increases. case exponential mechanism performs better laplace mechanism number data points small quickly overtaken laplace mechanism sampling procedure. increases accuracy sampling laplace mechanism’s approximate posterior converges performance samples true posterior current number observations exponential mechanism behaves similarly posterior fewer observations. show formally next subsection. figure error private approximate samples beta posterior bernoulli success parameter function number bernoulli observations averaged repeats. true parameter exponential mechanism posterior truncated differ element. note perturbing sufﬁcient statistics equivalent perturbing parameters recently independently proposed zhang beta-bernoulli models bernoulli naive bayes. comparison equations reveals sensitivity exponential mechanism sensitivities closely related. sensitivity generally easier control involve otherwise involves similar terms exponential mechanism sensitivity. example beta posterior case binary indicator vector sensitivity contrasted exponential mechanism sensitivity equation depends heavily truncation point unbounded standard untruncated beta distribution. sensitivity ﬁxed regardless number data points amount laplace noise becomes smaller relative total increases. first show laplace mechanism approximation exponential family posteriors approaches true posterior distribution evaluated data points. proofs given supplementary. lemma minimal exponential family given conjugate prior posterior takes form gn+α denotes posterior natural parameter vector exists assumptions depend number data observations ﬁnite covariance limit holds limn→∞ corollary laplace mechanism exponential family satisﬁes noise distribution requirements lemma sensitivity sufﬁcient statistics ﬁnite either exponential family minimal exponential family parameters identiﬁable. assumptions correspond data coming distribution laplace regularity assumptions hold posterior satisﬁes asymptotic normality given bernstein-von mises theorem. example beta-bernoulli setting assumptions hold long success parameter open interval relevant parameter interior result apply. setting learning normal distribution’s mean variance known assumptions lemma always hold natural parameter space open set. however corollary apply setting sensitivity inﬁnite efﬁciency result theorem follows lemma bernstein-von mises theorem. theorem assumptions lemma laplace mechanism asymptotic posterior drawing single sample asymptotic relative efﬁciency estimating fisher information above asymptotic posterior refers normal distribution whose variance depends posterior distribution approaches increases. result contrasted exponential mechanism theorem exponential mechanism applied exponential family temperature parameter asymptotic posterior i−/n single sample asymptotic relative efﬁciency estimating fisher information here represents ratio variance estimator optimal variance i−/n achieved posterior mean limit. sampling posterior stochasticity sampling laplace mechanism approach matches. theoretical results provide explanation difference behavior methods increases seen figure laplace mechanism eventually approach true posterior impact privacy accuracy diminish data size increases. however exponential mechanism ratio variances sampled posterior true posterior given data points approaches making sampled compared values sampling apples-to-apples comparison. reality laplace mechanism advantage releases full posterior privatized parameters exponential mechanism release ﬁnite number samples ﬁnite discuss remark remark assumptions lemma using full privatized posterior instead sample laplace mechanism release privatized posterior’s mean asymptotic relative efﬁciency estimating shift discussion case approximate bayesian inference. analysis dimitrakakis wang shows posterior sampling differentially private certain conditions exact sampling general tractable. directly follow approximate sampling algorithms mcmc also differentially private private privacy level. wang give results towards understanding privacy properties approximate sampling algorithms. first show approximate sampler close true distribution certain sense privacy cost close true posterior sample proposition procedure produces samples distribution \u0001-differentially private approximate sampling procedures produces samx δ)-differentially private. unfortunately general feasible verify convergence mcmc algorithm criterion generally veriﬁable practice. second result wang study privacy properties stochastic gradient mcmc algorithms including stochastic gradient langevin dynamics extensions. sgld stochastic gradient method noise injected gradient updates converges distribution target posterior. section study privacy cost mcmc allowing quantify privacy many real-world mcmc-based bayesian analyses. focus case gibbs sampling exponential mechanism laplace mechanism approaches. reinterpreting gibbs sampling instance exponential mechanism obtain privacy free cost gibbs sampling. metropolis-hastings annealed importance sampling also privacy guarantees show supplementary materials. consider privacy cost gibbs sampler data behind privacy wall current sampled values parameters latent variables publicly known gibbs update randomized algorithm queries private data order randomly select value current variable transition kernel gibbs update utility function space possible assignments holding ﬁxed. gibbs update therefore \u0001-differentially private update corresponds equation except responses exponential mechanism restricted θ¬l. note worst case computed strictly smaller outcomes. many cases parameter latent variable associated data point case privacy cost gibbs scan improved simple additive composition. case random sequence scan gibbs pass updates θl’s exactly once r-differentially private parallel composition alternatively random scan gibbs sampler updates random θl’s -differentially private privacy ampliﬁcation beneﬁt subsampling data suppose conditional posterior distribution gibbs update exponential family. privatized sufﬁcient statistics arising data likelihoods involved update equation publicly released privacy cost perform update drawing sample approximate conditional posterior i.e. equation replaced since privatized statistics made public also subsequently draw approximate posterior based prior without paying privacy cost. especially valuable gibbs sampling context prior gibbs update often consists factors particular consider bayesian model gibbs sampler interacts data conditional posteriors corresponding likelihoods exponential family distributions. privatize sufﬁcient statistics likelihood beginning mcmc algorithm laplace mechanism privacy cost approximately sample posterior running entire mcmc algorithm based privatized statistics without paying privacy cost. typically much cheaper privacy budget exponential mechanism mcmc pays privacy cost every gibbs update shall case study section mcmc algorithm need converge obtain privacy guarantees unlike method. approach applies broad class models including bayesian parameter learning fully-observed bayesian network models. course technique useful practice aggregate sufﬁcient statistics gibbs update must large relative laplace noise. latent variable models typically corresponds setting many data points latent variable model multiple emissions timestep study next section. primary goal work establish practical feasibility privacy-preserving bayesian data analysis using complex models real-world datasets. section investigate performance methods studied paper analysis sensitive military data. july october wikileaks organization disclosed collections internal u.s. military ﬁeld reports wars afghanistan iraq respectively. disclosures contained data january december entries afghanistan entries iraq. hillary clinton time rized wounded/killed/detained friendly/hostnation features combined respectively disjunction binary values. decreased number features privatize slightly increasing size counts ﬁeld protect simplifying model visualization purposes. preprocessing remove empty timesteps near-empty region codes median number entries region/timestep pair iraq afghanistan. number entries timestep highly skewed afghanistan increase density time. models trained gibbs sampling transition probabilities collapsed following goldwater grifﬁths collapse naive bayes parameters order keep conditional likelihood exponential family. details model inference algorithm given supplementary material. trained models gibbs iterations ﬁrst used burn-in. privatization methods overall computational complexity non-private sampler. laplace mechanism’s computational overhead paid up-front greatly affect runtime roughly doubled runtime. visualization purposes recovered parameter estimates posterior mean based latent variable assignments ﬁnal iteration reported frequent latent variable assignments non-burn-in iterations. trained -state model iraq data -state model afghanistan data using laplace approach total interestingly given states privacy-preserving model assigned substantial numbers data points states non-private happily -state model data. laplace noise therefore appears play role regularizer consistent noise interpreted random prior along lines noise-based regularization techniques although course correspond regularization would typically like. phenomenon potentially merits study beyond scope paper. visualized output laplace iraq figures state shows u.s. military performing well frequent outcomes feature friendly action cache found/cleared enemy casualties u.s. military performed poorly state state prevalent regions situation improved state troop surge strategy transition typically occurred troops peaked sept.–nov. results afghanistan supplementary provide critical lens military’s performance enemy casualty rates disclosed logs correspond individual event contain textual reports well ﬁelds coarse-grained types ﬁne-grained categories casualty counts different factions civilian enemy names relative u.s. military’s perspective). techniques discussed paper privately infer hidden markov model entries. non-textual ﬁelds listed above timestep month chain region code. naive bayes conditional independence assumption used emission probabilities simplicity parametercount parsimony. ﬁeld modeled discrete distribution latent state casualty counts binaanalysis applicable. competitive laplace mechanism afghanistan amount data relatively low. iraq dataset data timestep laplace mechanism outperformed particularly high-privacy regime. privacy guaranteed mcmc converged. otherwise section worst case impractical releases sample harmed coherency visualization afghanistan latent states ﬁnal sample noisy relative estimate based post burn-in samples privatizing gibbs chain privacy cost would avoid this. paper studied practical limitations using posterior sampling obtain privacy free. explored alternative based laplace mechanism analyzed theoretically empirically. illustrated beneﬁts laplace mechanism privacy-preserving bayesian inference analyze sensitive records. study privacy-preserving bayesian inference beginning. envision extensions techniques approximate inference algorithms well practical application sensitive real-world data sets. finally argued asymptotic efﬁciency important privacy context leading open question large class private methods asymptotically efﬁcient? work chaudhuri geumlek supported part work welling supported part qualcomm google facebook. also thank mijung park eric nalisnick babak shahbaba helpful discussions. also evaluated methods prediction. uniform random timestep/region pairs held train/test splits reported average test likelihoods splits. estimated test log-likelihood split averaging test likelihood burned-in samples using ﬁnal sample methods given latent states varied also considered naive bayes model equivalent -state hmm. laplace mechanism superior naive bayes model statistics corpus-wide counts corresponding high-data regime asymptotic goldwater grifﬁths fully bayesian approach unsupervised part-of-speech tagging. proceedings annual meeting association computational linguistics pages huang kannan exponential mechanism social welfare private truthful nearly optimal. foundations computer science ieee annual symposium pages ieee. salakhutdinov mnih bayesian probabilistic matrix factorization using markov chain monte carlo. proceedings international conference machine learning pages song chaudhuri sarwate stochastic gradient descent differentially private global conference signal inforupdates. mation processing ieee pages ieee. wang y.-x. fienberg smola privacy free posterior sampling stochastic gradient monte carlo. proceedings international conference machine learning pages speciﬁc privacy cost posterior sampling. datasetspeciﬁc local privacy parameter computed grid search bernoulli success parameter bernoulli outcomes case adversary adds success failure adversary selects success/failure outcome highest local adversary able make dataset-speciﬁc approach worst case manipulating partition function posterior. exponential mechanism’s worst case posterior sampling corresponds cost terms. must cost difference loglikelihood terms always draw worst-case plus another worst case difference partition-functions terms adversary alter worst case figure described formally supplementary said exponential family. breaking structure parts vector known natural parameters distribution lies space represents vector sufﬁcient statistics fully capture information needed determine likely distribution. represents log-normalizer appendix describe additional simulation experiment supplements analysis performed main manuscript. wang analysis ﬁnds privacy cost posterior sampling directly improve number data points unless analyst deliberately modiﬁes posterior changing temperature sampling. figure report experiment showing result limitation analysis exist cases dataset-speciﬁc privacy cost posterior sampling approach exponential mechanism worst case number observations increases. experiment consider beta distribution posterior symmetrically truncated bernoulli observations. simulate adversary greedily selects data points dataset increase datasetstandard results coming algebraic manipulations seen omit proof lemmas. lemma immediately leads useful corollary minimal families conjugate prior families. corollary minimal exponential family distribution conjugate prior family given equation also minimal. next result looks sufﬁcient conditions getting divergence limit adding ﬁnite perturbance vector natural parameters. limit taken later tied amount data used forming posterior. discuss posterior distributions also forming exponential families natural parameters denoted random variables lemma denote distribution exponential family natural parameter constant vector dimensionality interested learning considering algorithms generate posterior distribution exponential families always conjugate prior family exponential family. speaking prior posterior distributions becomes random variable introduce vector natural parameters space parameterize distributions. ease notation express conjugate prior exponential family simply relabelling exponential family structure. posterior conjugate prior often written equivalent form vector scalar together specify vector natural parameters distribution. interaction posterior prior acts like observations average sufﬁcient statistics already observed. parameterization many nice intuitive properties proofs center around natural parameter vector prior. forms posterior reconciled deﬁnition natural parameters sufﬁcient statistics fully specify exponential family posterior resides deﬁned appropriate log-normalizer distribution merely constant). note space full space last component function previous components. plugging expressions following form conjugate prior begin deﬁning minimal exponential families special class exponential families nice properties. minimal sufﬁcient statistics must linearly independent. later relax requirement consider minimal exponential families. prior distributions minimal exponential families. space natural parameters space furthermore assume parameterization arising natural conjugate prior following conditions hold note ﬁrst term linear minimality lemma strictly convex. implies strictly concave thus interior local maximum must also unique global maximum. remark lemma continuously differentiable. means continuous function since interior construct open neighborhood around preimage open continuous function provides heart results ||∇b|| small connecting conclude kl||p) small respect ||γ||. wish show arising observing data points approaching grows. achieve this analyze relationship norm natural parameter covariance distribution parameterizes. relationship shows posteriors plenty observed data covariance permits lemma bound divergence perturbed posteriors. reach relationship ﬁrst prove posteriors well-deﬁned mode later relationship require mode well-behaved. lemma likelihood function conjugate follows inverse vector mapping jacobian assumption guarantees non-zero determinant neighborhood. satisﬁes inverse function theorem show continuously differentiable. machinery place remains seen conditions posterior satisﬁes conditions previous lemmas along extending case random variable ﬁxed ﬁnite vector. well deﬁned interior express relationship high magnitude posterior parameters covariance distribution generate. lemma likelihood function conjugate prior distributions minimal exponential families. space natural parameters space furthermore assume parameterization arising natural conjugate prior conditions lemma hold additional assumptions result follows laplace approximation method inner details approximation show lemma show setting satisﬁes regularity assumptions approximation. first deﬁne functions dimensionality using spectral norm ||)−|| norm ||un rearrangement show following inequalities. note covariance must invertible since covariance invertible assumption lemma minimal exponential family given conjugate prior posterior takes form gn+α denotes posterior natural parameter vector exists assumptions mechanism generating perturbed posterior noiseless posterior comes distribution depend number data observations ﬁnite covariance limit holds limn→∞ random variable depending data observations analyze behaves couple related random variables deﬁned implicitly conditioned constant denote random variable matching distribution single observation cov). exleft hand side shown zero equations right hand side bounded since divergences never negative. thus inequality sufﬁces show limit zero prove desired result. orollary laplace mechanism exponential family satisﬁes noise distribution requirements lemma sensitivity sufﬁcient statistics ﬁnite either exponential family minimal exponential family parameters identiﬁable. proof exponential family already minimal result trivial. minimal exists minimal parameterization. wish show adding noise non-minimal parameters equivalent adding differently distributed noise minimal parameterization noise distribution also satisﬁes noise distribution requirements lemma noise distribution depend ﬁnite covariance. explicitly construct minimal parameterization family distributions. exponential family minimal means dimensions sufﬁcient statistics data fully linearly independent. component maximal number linearly independent sufﬁcient statistics without loss generality assume ﬁrst components. vector linearly independent components. ∀x∃φj ˜s+zj. wish build minimal exponential family distribution identical original parameterized sufﬁcient statistics natural parameters. distributions equivalent sufﬁces equality exponents. high probability neighborhood further neighborhood modes region assumptions tell well-behaved. assignment satisﬁes conditions lemma assumptions serving round rest regularity assumptions lemma trivial translations. theorem assumptions lemma laplace mechanism asymptotic posterior drawing single sample asymptotic relative efﬁciency estimating fisher information assumptions lemma match laplace regularity assumptions asymptotic normality holds know unperturbed posterior converges bernstein-von mises theorem posterior laplace mechanism ﬁxed randomness limn→∞ must converge distribution clear samples asymptotic relative efﬁciency argue asymptotic behavior holds ﬁxed randomness laplace mechanism also holds laplace mechanism whole. show previous results relied mathematical results involving covariances posteriors observing large amount data. still need show bounds covariances accomplished adapting existing laplace approximation methods. there need quick result convex functions positive deﬁnite hessian order perform approximation lemma strictly convex function minimum positive definite exists everywhere exists implies y∗|| proof existence thus continuity know exists positive implies positive semi-deﬁnite identity matrix. global minimum know gradient thus leads taylor expansion form since merely ﬁrst components ﬁrst sums simply products combined vector ﬁrst components force equation hold choosing appropriately equation zero. note requires truly function depending written terms instead. justiﬁable assumption natural parameters identiﬁable distribution associated means bijection ensures welldeﬁned function. sufﬁces characterize additional natural parameters affect parameters equivalent minimal system. additive noise component translates linearly additive noise components meaning laplace mechanism’s noise distribution nonminimal parameter space still corresponds noise distribution minimal parameters depend data size still ﬁnite covariance. minimal exponential family tends towards divergence zero equivalent non-minimal exponential family must well. using standard laplace approximation methods seen explore begin must show assumptions satisfy regularity assumptions approximation. ﬁxed condition know exists neighborhood around positive deﬁnite. using lemma verify s.t. ||φ−φ∗ following expression note right hand side depend lemma guarantees non-zero bound right hand side equation exactly matches condition kass intuitive meaning exists sufﬁciently large integral negligible outside region conditions also match directly conditions given kass though note require even higher derivatives bounded present. extra derivatives used later extend argument given kass suit purposes give uniform bound across neighborhood. emma used demonstrate regularity assumption required next lemma performs heavy lifting using laplace approximation. lemma adapts previous argument laplace approximations posterior. adapted laplace approximation argument forms core lemma allows covariance posteriors shrink data observed. lemma function space space previously remarked continuously differentiable compact condition informs bounded away singular matrix matrix inversion also uniformly continuous compact set. means ﬁnite supremum thus term uniformly neighborhood. next consider second term dimensionality sixth fourth central moments multivariate gaussian covariance matrix sums written einstein summation notation. remark error term approximation also depends assumptions know functions continuously differentiable composition continuously differentiable functions compact next look ﬁrst derivative remark partial derivatives given recall local error term given wish bound derivatives local bound error term given kass bound derivatives. however slight modiﬁcation argument shows added assumptions higher order derivatives sufﬁcient control behavior error term. following expression equation translated setting main manuscript showed privacy cost gibbs sampling interpreting instance exponential mechanism. here show privacy cost widely used mcmc algorithms metropolishastings annealed importance sampling. since gibbs updates special case metropolishastings updates might conjecture general metropolis-hastings updates differentially private well. however accept/reject decision contains subtle non-determinacy violates pure-\u0001 differential privacy. consider metropolis-hastings update symmetric proposal temperature markov chain. updates uphill moves never rejected. since move uphill database downhill neighbor cannot bound ratio reject decisions violates differential privacy. turns metropolis updates weaker privacy guarantee resorting -differential privacy theorem private data public current value variables wish infer. metropolis update invariant posterior temperature symmetric proposal ))dθ -differentially private. proof theorem provided appendix essentially bound ratio probabilities accept decisions neighboring databases reject decisions. rejections rare privacy-violating outcomes rare sufﬁcient -privacy. hand must small meaningful level privacy e.g. less inverse polyomial ﬁfth-order taylor expansion error term continue taylor expansion another degree bound variation consider three separate functions permitting higher order taylor expansion. respective error term depending seventh-order partial derivatives note necessarily them. argument already shows terms composing error term bounded terms trivial show analogous result higher order approximations. allows extend approximation derivatives uniformly neighborhood newly introduced extra approximation terms uniform bounds still simply though larger constant now. sufﬁciently large positive constants satisfying ||z|| k||∇ψz|| k||∇ ψz|| {ψ|ψ/k remark exists virtue similarly exist ||∇ψz|| ψz|| constant term front. need release private copies importance weights procedure used algorithm. interested computing normalization constants release normalized verωi. discrete distribution sums lives simplex. sensitivity protected laplace mechanism. another possible alternative perform resampling according distribution approximated protected exponential mechanism. here prove differential privacy result above. metropolis-hastings given theorem proof neighboring databases. deﬁnition differential privacy need bound ratios probability outcome databases. consider accept reject outcomes separately. must bound probability ratio outcome under neighboring datasets. consider ﬁrst slightly simpler question ratio probabilities accept decision already selected proposal privacy results gibbs sampling metropolishastings updates reveal close connection privacy temperature markov chain. low-temperature chains high-ﬁdelity privacy-expensive hightemperature chains low-ﬁdelity privacy-cheap also rapidly. suggests annealing methods annealed importance sampling effective context allowing savings privacy budget early iterations mcmc also traversing state space rapidly. monte carlo method anneals high-temperature distribution target distribution mcmc updates sequence temperatures producing importance weights sample correct annealing. takes input annealing path sequence unnormalized distributions different temperatures. obtain privacy-preserving annealing path varying intermediate distribution instance equation privacy cost exact sample sample temperature using private gibbs transition operator equation privacy cost sample computed composition theorem could also similarly bound difference probabilities neighboring databases outcome metropolis update would \u0001-differentially private. consider ﬁrst reject probabilities proposal selected r|x) probability reject decision possible construct scenarios probability reject decision proposals e.g. global minimum cannot general lower bound overall probability reject occurs database ratio probabilities outcome inﬁnite division violating \u0001-differential privacy. assumptions additional guarantee i.e. probability rejection outcome therefore probability outcome violates \u0001-differential privacy less demonstrate privacy complete proof observe condition implies -criterion holds reject outcome transition counts excluding current updated transition probabilities implicitly conditioned depend transition counts. indicator functions arise bookkeeping counts modiﬁed changing current state. conjugacy simple update here correspond concentration parameters appropriately dimensioned dirichlet distributions. table summary notation. generative model corresponds joint probability discrete-valued feature entry region timestep latent state region timestep transition probability state discrete emission probability cluster d’th feature outcome dirichlet concentration parameters. number entries region timestep number features observations. number latent clusters. figure log-likelihood held-out data naive bayes model equivalent timestep full left afghanistan. right iraq. truncation point truncated dirichlet distributions simulate truncated dirichlet distributions gibbs updates method used approach fang involves sequentially drawing component based truncated beta distribution. full visualization results shown figures log-likelihood results held-out data given figure experiment randomly held-out region/timestep pairs testing train/test splits reported average log-likelihood repeats. sufﬁcient statistics counts nrtdj privatize laplace mechanism resulting private counts ˆnrtdj. indicator vectors count vector nrtd sensitivity perform gibbs updates privacy-preserving manner substituting private counts counts equation preserve privacy updating equation estimated based privacy-preserving counts nrtd. importantly need compute private counts ˆnrtdj once beginning algorithm privatized counts reused gibbs updates. performed simple preprocessing steps experiment. casualty count ﬁelds entry binarized wounded wounded/killed/detained ﬁelds merged disjunctively casualty indicator ﬁeld. friendly hostnation casualty indicators combined ﬁeld disjunction. iraq dataset missing data issues addressed. data available figure state assignments parameters privacy-preserving iraq. left estimate last samples. right estimate last sample. middle state bottom state figure state assignments parameters privacy-preserving afghanistan. left estimate last samples. right estimate last sample. parameters point states ordered bottom.", "year": 2016}