{"title": "Hierarchical Deep Learning Architecture For 10K Objects Classification", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Evolution of visual object recognition architectures based on Convolutional Neural Networks & Convolutional Deep Belief Networks paradigms has revolutionized artificial Vision Science. These architectures extract & learn the real world hierarchical visual features utilizing supervised & unsupervised learning approaches respectively. Both the approaches yet cannot scale up realistically to provide recognition for a very large number of objects as high as 10K. We propose a two level hierarchical deep learning architecture inspired by divide & conquer principle that decomposes the large scale recognition architecture into root & leaf level model architectures. Each of the root & leaf level models is trained exclusively to provide superior results than possible by any 1-level deep learning architecture prevalent today. The proposed architecture classifies objects in two steps. In the first step the root level model classifies the object in a high level category. In the second step, the leaf level recognition model for the recognized high level category is selected among all the leaf models. This leaf level model is presented with the same input object image which classifies it in a specific category. Also we propose a blend of leaf level models trained with either supervised or unsupervised learning approaches. Unsupervised learning is suitable whenever labelled data is scarce for the specific leaf level models. Currently the training of leaf level models is in progress; where we have trained 25 out of the total 47 leaf level models as of now. We have trained the leaf models with the best case top-5 error rate of 3.2% on the validation data set for the particular leaf models. Also we demonstrate that the validation error of the leaf level models saturates towards the above mentioned accuracy as the number of epochs are increased to more than sixty.", "text": "evolution visual object recognition architectures based convolutional neural networks convolutional deep belief networks paradigms revolutionized artificial vision science. architectures extract learn real world hierarchical visual features utilizing supervised unsupervised learning approaches respectively. approaches cannot scale realistically provide recognition large number objects high propose level hierarchical deep learning architecture inspired divide conquer principle decomposes large scale recognition architecture root leaf level model architectures. root leaf level models trained exclusively provide superior results possible -level deep learning architecture prevalent today. proposed architecture classifies objects steps. first step root level model classifies object high level category. second step leaf level recognition model recognized high level category selected among leaf models. leaf level model presented input object image classifies specific category. also propose blend leaf level models trained either supervised unsupervised learning approaches. unsupervised learning suitable whenever labelled data scarce specific leaf level models. currently training leaf level models progress; trained total leaf level models now. trained leaf models best case top- error rate validation data particular leaf models. also demonstrate validation error leaf level models saturates towards mentioned accuracy number epochs increased sixty. top- error rate entire two-level architecture needs computed conjunction error rates root leaf models. realization level visual recognition architecture greatly enhance accuracy large scale object recognition scenarios demanded cases diverse drone vision augmented reality retail image search retrieval robotic navigation targeted advertisements etc. deep learning based vision architectures learn extract represent visual features model architectures composed layers non-linear transformations stacked learn high level abstractions level features extracted images utilizing supervised unsupervised learning algorithms. recent advances training cnns gradient descent based backpropagation algorithm shown accurate results inclusion rectified linear units nonlinear transformation also extension unsupervised learning algorithms train deep belief networks towards training convolutional networks exhibited promise scale realistic image sizes supervised unsupervised learning approaches matured provided architectures need large scale object recognition ever relevant today explosion number individual objects supposed comprehended artificial vision based solutions. requirement pronounced case scenarios drone vision augmented reality retail image search retrieval industrial robotic navigation targeted advertisements etc. large scale object recognition enable recognition engines cater wider spectrum object categories. also mission critical cases demand higher level accuracy simultaneously large scale objects recognized. paper propose level hierarchical deep learning architecture achieves compelling results classify objects categories. best knowledge proposed method first attempt classify objects utilizing level hierarchical deep learning architecture. also blend supervised unsupervised learning based leaf level models proposed overcome labelled data scarcity problem. proposed architecture provides dual benefit form providing solution large scale object recognition time achieving challenge greater accuracy possible -level deep learning architecture. come across work uses -level hierarchical deep learning architecture classify objects images. object recognition large scale using shallow architectures utilizing svms discussed effort presents study large scale categorization image classes using multi-scale spatial pyramids visual words feature extraction support vector machines classification. work creates different datasets derived imagenet categories. based datasets outlines influence classification accuracy different factors like number labels dataset density dataset hierarchy labels dataset. methods proposed provide extended information classifier relationship different labels defining hierarchical cost. cost calculated height lowest common ancestor wordnet. classifiers trained loss function using hierarchical cost learn differentiate predict similar categories compared trained loss. error rate classification entire categories conclusively stated work. supervised learning based deep visual recognition architectures composed multiple convolutional stages stacked learn hierarchical visual features captured figure regularization approaches stochastic pooling dropout data augmentation utilized enhance recognition accuracy. recently faster convergence architectures attributed inclusion rectified linear units nonlinearity layer weights. state error rate reported classification categories utilizes mentioned generic architectural elements layers weights. nsupervised learning based architecture model convolutional learns visual feature hierarchy greedily training layer layer. architectures reported accuracy classifying objects architectures scaled classification objects. conjecture scaling single architecture realistic computations intractable utilize deeper architectures. employ divide conquer principle decompose classification root leaf level distinct challenges. proposed architecture classifies objects steps captured below root level model architecture first step root i.e. first level architectural hierarchy recognizes high level categories. deep vision architectural model weight layers trained using stochastic gradient descent architectural elements captured table leaf level model architecture second step leaf level recognition model recognized high level category selected among leaf models. leaf level model presented input object image classifies specific category. leaf level architecture architectural hierarchy recognizes specific objects finer categories. model trained using stochastic gradient descent architectural elements captured table based leaf level models trained unsupervised learning approach case scarce labelled images deliver blend leaf models trained supervised unsupervised approaches. root level model leaf level models need trained. imagenetk dataset compiled synsets fall- release imagenet. leaf node least labelled images amount images total. vision level features assembled form high level abstractions higher level abstractions turn assembled form higher level abstractions substantial number recognition models two-level hierarchical architecture trained utilizing supervised training. algorithms utilized method referred error-back propagation algorithm. algorithms require significantly high number labelled training images object category data set. biologically inspired architecture multiple trainable convolutional stages stacked other. layer learns feature extractors visual feature hierarchy attempts mimic human visual system feature hierarchy manifested different areas human brain eventually fully connected layers feature classifier learn classify extracted features layers different categories objects. fully connected layers likened area brain classifies hierarchical features generated area root level leaf level models architecture trained supervised gradient descent based backpropagation method. learning method cross entropy objective function minimized error correction learning rule/mechanism. mechanism computes gradients weight updates hidden layers recursively computing local error gradient terms error gradients next connected layer neurons. correcting synaptic weights free parameters network eventually actual response network moved closer desired response statistical sense. convolutional pool convolutional convolutional pool convolutional convolutional pool convolutional convolutional convolutional pool convolutional convolutional convolutional pool convolutional full-connect full-connect full-connect -softmax enhanced discriminative function chosen deeper architectures smaller kernels root leaf models make objective function discriminative. interpreted making training procedure difficult making choose feature extractors higher dimensional feature space. relu non-linearity utilized relu nonlinearities sigmoidal i.e. non-saturating nonlinearities layer reduces training time converging upon weights faster pooling output convolutional-relu combination pooling layer alternative convolutional layers. output pooling layer invariant small changes location features object. pooling method used either maxpooling stochastic pooling. pooling method averages output neighborhood neurons where-in poling neighborhoods overlapping non-overlapping. majority leaf models used pooling approach overlapping neighborhoods. alternatively also used stochastic pooling method training models. stochastic pooling output activations pooling region randomly picked activations within pooling region following multinomial distribution. distribution computed neuron activation within given region approach hyper parameter free. architecture stochastic pooling technique captured table dropout method output neuron fully connected layer zero probability ensures network samples different architecture training example presented besides method enforces neurons learn robust features cannot rely existence neighboring neurons. convolutional pool convolutional convolutional pool convolutional convolutional pool convolutional convolutional pool convolutional full-connect full-connect full-connect -softmax convolutional stochastic pool convolutional convolutional stochastic pool convolutional convolutional stochastic pool convolutional convolutional stochastic pool convolutional full-connect full-connect full-connect -softmax modified libccv open source implementation realize proposed architecture trained nvidia gtx™ titan gpus. root leaf level models trained using stochastic gradient descent leaf models trained batches models systems simultaneously. first leaf models initialized trained scratch epochs learning rate momentum rest leaf models initialized trained leaf models trained learning rate root model trained epochs learning rate initialized similar model trained imagenet dataset. takes days batch leaf models train epochs. currently root model leaf models trained weeks. full realization architecture progress estimated conclude second week september’. statistical mechanics inspired concept unsupervised training fundamentally. specifically statistical mechanics forms study macroscopic equilibrium properties large system elements starting motion atoms electrons. enormous degree freedom necessitated statistical mechanics foundation makes probabilistic methods suitable candidate modelling features compose training data sets networks trained statistical mechanics fundamentals model underlying training dataset utilizing boltzmann distribution. obviate painfully slow training time required train boltzmann machines multiple variants proposed restricted boltzmann machine provided best possible modelling capabilities minimal time. resulting stacks layers greedily trained layer layer resulting deep belief networks successfully provides solution image speech recognition document retrieval problem domains. described multilayer generative models learn hierarchy non-linear feature detectors. lower layer learns lower level features feeds higher level help learn complex features. resulting network maximizes probability training data generated network. limitations scaling realistic image sizes first difficulty computationally tractable increasing image sizes. second difficulty faced lack translational invariance modelling images. scale modelling realistic size images powerful concept convolutional introduced. cdbn learns feature detectors translation invariant i.e. feature detectors detect features located location image. perform block gibbs sampling using conditional distribution suggested learn convolutional weights connecting visible hidden layers activations neurons visible hidden layers respectively. also hidden unit biases visible unit biases. forms weight matrix connecting visible hidden layer. gibbs sampling conducted utilizing weights learnt give layers convolutional rbms crbms stacked form cdbn. probabilistic pooling layers convolutional layers introduced complete architecture. concept captured figure train first layers leaf architecture unsupervised learning. later abandon unsupervised learning learnt weights first layers initialize weights architecture. architecture weights fine-tuned using backpropagation method. architecture used training unsupervised learning mechanism captured table also two-level hierarchical deep learning architecture constructed entirely cdbn depicted figure used contrastive divergence mechanism train first layers architecture specified unsupervised learning. updates hidden units positive phase step done sampling rather using real valued probabilities. also used mini-batch size training. reconstruction error refers squared error original data reconstructed data. guarantee accurate training course training generally decrease. also large amount increase suggests training going wrong. printing learned weights learned weighs needs eventually visualized oriented localized edge filters. printing weights training helps identify whether weights approaching filter-like shape. ratio variance reconstructed data variance input image exceeds decrease learning rate factor reset values weights hidden bias updates ensure weights don’t explode. initial momentum chosen increased finally initial learning rate chosen decide parameters first build hierarchy tree synsets imagenetk dataset using thumb-rules split organize classes leaf models holding maximum classes. using wordnet relationship synsets imagenetk dataset organized hierarchical tree. wordnet relationship file lists parent-child relationships synsets imagenet. example line relationship file refers parent synset child synset however file relate single child multiple parents i.e. heifer also child another category depth synset imagenet hierarchy relationship semantic label focused building deepest branch synset. utilized simplified method exploits relationship algorithm htree depicted figure used generate hierarchy tree. paper results ninth iteration used base tree. sample illustration captured figure hierarchy tree evident dataset skewed towards categories like flora animal fungus natural objects instruments etc. levels closer tree root i.e. synsets fall branches. formulate root leaf models -level hierarchy. root level model leaf level models need trained. leaf level model recognizes categories range numbers. imagenetk dataset compiled synsets fall- release imagenet. leaf node least labelled images amount images total. error rates leaf level models computed. graph fig. plots errors leaf models vis-à-vis training epochs. observe leaf models trained higher number training epochs error decreases. error rate complete objects classification computed upon training models required -level hierarchical deep learning architecture. tilizing unsupervised learning trained leaf model leaf- consists man-made artifacts categories. model leaf- cdbn based architecture described section trained first layer cdbn architecture contrastive divergence algorithm. later first layer weights utilized initialize weights leaf- model supervised setting. fine-tuned back-propagation utilizing supervised learning. feature extractors kernels learnt supervised unsupervised learning captured fig. intend compute error rates categories using algorithm captured fig. figures depict top- classification results using -level hierarchical deep learning architecture. top-left first image original image used testing. remaining images represent top- categories predicted hierarchical model descending order confidence. top- error rate entire two-level architecture required computed conjunction error rates root leaf models. realizations level visual recognition architecture greatly simply object recognition scenario large scale object recognition problem. time proposed level hierarchical deep learning architecture help enhance accuracy complex object recognition scenarios significantly otherwise would possible -level architecture. trade proposed -level architecture size hierarchical recognition model. total size -level recognition models including root leaf models amounts approximately size might constraints towards executing entire architecture low-end devices. size constraint executed high device cloud based recognition size higher. besides always split level hierarchical model device cloud paves object recognition utilizing novel device-cloud collaboration architectures. proposed architecture soon available classifying large scale objects. breakthrough help enable applications diverse drone vision industrial robotic vision targeted advertisements augmented reality retail robotic navigation video surveillance search information retrieval multimedia content etc. hierarchical recognition models deployed commercialized various devices like smart phones home gateways head various cases. take opportunity express gratitude deep regards mentor shankar venkatesan guidance constant encouragement. without support would possible materialize paper. yann lecun koray kavukvuoglu clément farabet convolutional networks applications vision proc. international symposium circuits systems ieee krizhevsky ilya sutskever hinton. \"imagenet classification deep convolutional neural networks\" nips simonyan zisserman very deep convolutional networks large-scale image recognition arxiv technical report grosse ranganath \"convolutional deep belief networks scalable unsupervised learning hierarchical representations\" proceedings international conference machine learning montreal canada deng alexander berg fei-fei what classifying image categories tell computer vision–eccv sergey ioffe christian szegedy \"batch normalization accelerating deep network training reducing internal covariate shift\" arxiv.v deng dong socher l.-j. fei-fei. imagenet large-scale hierarchical image database cvpr george dahl dong deng alex acero context-dependent pre-trained deep neural networks large-vocabulary speech recognition ieee transactions audio speech language processing haykin \"neural networks comprehensive foundation\" jersey prentice hall dance willamowski bray csurka \"visual categorization bags keypoints\" eccv international workshop statistical learning computer vision lazebnik schmid ponce \"beyond bags features spatial pyramid matching recognizing natural scene categories\" ieee computer society conf. computer vision pattern recognition laxman katole completed signal processing indian institute science bangalore currently working samsung institute india-bangalore. technical areas interest include artificial intelligence deep learning object recognition speech recognition application development algorithms. seeded established teams domain deep learning applications image speech recognition. rishna prasad yellapragada currently working technical lead samsung institute india-bangalore. interests include deep learning machine learning content centric networks. mish kumar bedi completed b.tech computer science roorkee batch. working samsung institute indiabangalore' since july technical areas interest include deep learning/machine learning artificial intelligence. sehaj sing kalra completed b.tech computer science delhi batch working samsung institute india bangalore since june interest lies machine learning applications various domains specifically speech image. ynepalli siva chaitanya completed b.tech electrical engineering bombay batch working samsung institute india bangalore since june area interest includes neural networks artificial intelligence.", "year": 2015}