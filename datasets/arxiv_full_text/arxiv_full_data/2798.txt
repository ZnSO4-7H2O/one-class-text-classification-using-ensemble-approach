{"title": "Combining Symbolic and Function Evaluation Expressions In Neural  Programs", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Neural programming involves training neural networks to learn programs from data. Previous works have failed to achieve good generalization performance, especially on programs with high complexity or on large domains. This is because they mostly rely either on black-box function evaluations that do not capture the structure of the program, or on detailed execution traces that are expensive to obtain, and hence the training data has poor coverage of the domain under consideration. We present a novel framework that utilizes black-box function evaluations, in conjunction with symbolic expressions that integrate relationships between the given functions. We employ tree LSTMs to incorporate the structure of the symbolic expression trees. We use tree encoding for numbers present in function evaluation data, based on their decimal representation. We present an evaluation benchmark for this task to demonstrate our proposed model combines symbolic reasoning and function evaluation in a fruitful manner, obtaining high accuracies in our experiments. Our framework generalizes significantly better to expressions of higher depth and is able to fill partial equations with valid completions.", "text": "neural programming involves training neural networks learn programs data. previous works failed achieve good generalization performance especially programs high complexity large domains. mostly rely either black-box function evaluations capture structure program detailed execution traces expensive obtain hence training data poor coverage domain consideration. present novel framework utilizes black-box function evaluations conjunction symbolic expressions integrate relationships given functions. employ tree lstms incorporate structure symbolic expression trees. tree encoding numbers present function evaluation data based decimal representation. present evaluation benchmark task demonstrate proposed model combines symbolic reasoning function evaluation fruitful manner obtaining high accuracies experiments. framework generalizes signiﬁcantly better expressions higher depth able partial equations valid completions. human beings possess impressive abilities abstract mathematical logical thinking. long dream computer scientists design machines capabilities machines automatically learn reason thereby removing need manually program them. neural programming neural networks used learn programs recently shown promise towards goal. examples neural programming include neural theorem provers neural turing machines neural program inducers e.g. loos graves neelakantan boˇsnjak solve tasks learning mathematical functions prove theorems synthesize programs. works neural programming either rely black-box function evaluations availability detailed program execution traces entire programs runs recorded different input conditions black-box function evaluations easy obtain since need generate inputs outputs various functions domain. however themselves result powerful generalizable models since sufﬁcient information underlying structure domain. hand execution traces capture underlying structure generally hard obtain large number input conditions; even available computational complexity incorporating enormous. lack good coverage approaches fail generalize programs higher complexity domains large number functions. moreover performance frameworks severely dependent nature execution traces efﬁcient programs lead drastic improvement performance programs readily available. many problem domains addition function evaluations typically access information symbolic representations encode relationships given variables functions succinct manner. instance physical systems ﬂuid dynamics robotics physical model world imposes constraints values different variables take. domain programming languages declarative languages explicitly declare variables program. examples include database query languages regular expressions functional programming etc. declarative programs greatly simplify parallel programs generation symbolic computation graphs thus used modern deep learning packages e.g. theano theano development team tensorﬂow abadi mxnet chen therefore availability rich symbolic expression data many domains. summary results introduce ﬂexible scalable neural programming framework combines knowledge symbolic expressions black-box function evaluations. knowledge ﬁrst consider combined framework. demonstrate approach outperforms existing methods signiﬁcant margin using small amount training data. paper three main contributions. design neural architecture incorporate symbolic expressions black-box function evaluation data. evaluate tasks equation veriﬁcation completion domain mathematical equation modeling. propose data generation strategy symbolic expressions numerical evaluations results good balance coverage. consider mathematical equations case study since used extensively previous neural programming works e.g. zaremba allamanis loos employ tree lstms incorporate symbolic expression tree lstm cell mathematical function. parameters lstm cells shared across different expressions wherever function used. weight sharing allows learn large number mathematical functions simultaneously whereas previous works learning mathematical functions. extend tree lstms accept symbolic expression input also numerical data black-box function evaluations. employ tree encoding numbers appear function evaluations based decimal representation allows model generalize unseen numbers struggle neural programing researchers far. show recursive neural architecture able generalize unseen numbers well unseen symbolic expressions. evaluate framework tasks equation veriﬁcation completion. equation veriﬁcation consider sub-categories verifying correctness given symbolic identity whole verifying evaluations symbolic expressions given numerical inputs. equation completion involves predicting missing entry mathematical equation. employed applications mathematical question answering establish framework outperforms existing approaches tasks signiﬁcant margin especially terms generalization equations higher depth domains large number functions. propose novel dataset generation strategy obtain balanced dataset correct incorrect symbolic mathematical expressions numerical evaluations. previous methods exhaustive search possible parse trees therefore limited symbolic trees small depth dataset generation strategy relies dictionary look-up sub-tree matching applied domain providing basic axioms inputs. generated dataset good coverage domain obtaining superior generalization performance. also able scale coverage include mathematical functions compared previous works related work early work automated programming used ﬁrst order logic computer algebra systems wolfram mathematica sympy. however rule-based systems required extensive manual input cannot generalized programs. graves introduced using memory neural networks learning functions grade-school addition sorting. since then many works extended tasks program synthesis program induction automatic differentiation based type data used train models frameworks neural programming categorized different classes. models black-box function evaluation data models program execution traces models combination black-box input-output data weak supervision program sketches ﬁnally models symbolic data work extension models category uses symbolic data instead weak supervision. therefore obtain generalizable models capable function evaluation. moreover combination allows scale domain model functions well learn complex structures. extensively studied applications neural programming reasoning mathematical equations. works include automated theorem provers computer algebra-like systems work closer latter categorization however problem solve nature different. allamanis zaremba simplifying mathematical equations deﬁning equivalence classes symbolic expressions used symbolic solver. problem mathematical equation veriﬁcation completion broader applicability. recent advances symbolic reasoning natural language processing indicated signiﬁcance applying domain structure models capture compositionality semantics. socher proposed tree-structured neural networks natural language parsing neural image parsing. proposed using recursion capturing compositionality computer programs. zaremba allamanis used tree-structured neural networks modeling mathematical equations. introduced tree-structured lstm semantic relatedness natural language processing. show powerful model outperforms tree-structured neural networks validating mathematical equations. address problem modeling symbolic mathematical equations. goal validate correctness symbolic mathematical equation. enables perform equation completion. limit domain trigonometry elementary algebra paper. section ﬁrst discuss grammar explains domain study. later describe generate dataset correct incorrect symbolic equations within grammar. talk combine data input-output examples enable function evaluation. dataset allows learn representations functions capture semantic properties i.e. relate other transform input applied. interchangeably word identity referring mathematical equations input-output data refer function evaluations. start deﬁning domain mathematical identities using context-free grammar notation. identities deﬁnition consist expressions trying validate mathematical expression represented composed either terminal constant variable unary function applied expression binary function applied expression arguments functions take arguments i.e. n-ary functions omitted task description however noted grammar covers entire space trigonometric elementary algebraic identities since n-ary functions like addition represented composition multiple binary addition functions. trigonometry grammar rules thus follows table presents complete list functions symbols constants grammar. note exclude subtraction division represented addition multiplication power respectively. furthermore equations many variables needed. formulation provides parse tree symbolic function evaluation expression crucial component representing equations model. figure illustrates examples identity grammar terms expression tree. worth noting implicit notion depth identity expression tree. since deeper equations compositions multiple simpler equations validating higher-depth identities requires reasoning beyond required identities lower depths thus depth equation somewhat indicative complexity mathematical expression. however depth sufﬁcient; higher depth identities much easier validate acosθ. symbolic function evaluation expressions differentiated type terminals. symbolic expressions terminals type constant variable whereas function evaluation expressions constants numbers terminals. come back distinction section deﬁne model. shown table domain includes functions. scales domain comparison state-of-the-art methods mathematical functions also show expressions higher complexity consider equalities depth resulting trees size compared state-of-the-art methods trees size axioms refer small basic trigonometric algebraic identities axioms. axioms gathered wikipedia page trigonometric identities well manually speciﬁed ones covering elementary algebra. consists identities varying depth examples axioms sin. axioms represent basic properties mathematical functions trigonometry algebra directly specify input/output behavior. axioms consisting positive identities serve starting generating dataset mathematical identities. order provide challenging accurate benchmark task need create large varied collection correct incorrect identities manner extended domains mathematics easily. approach based generating mathematical identities performing local random changes known identities starting axioms described above. changes result identities similar higher complexity correct incorrect valid expressions within grammar. generating possible identities generate identity select equation random known equations make local changes order this ﬁrst randomly select node expression tree followed randomly selecting following actions make local change equation selected node shrinknode replace node it’s leaf children chosen randomly. replacenode replace symbol node another grownode provide node input another randomly drawn function replaces node. takes inputs second input generated randomly terminals. procedure symbolic solver sympy separate correct equations incorrect ones. since performing changes randomly number generated incorrect equations overwhelmingly larger number correct identities. makes training data highly unbalanced desired. therefore propose method based sub-tree matching generate correct identities. generating additional correct identities order generate correct identities follow intuition above replace structure others equal. particular maintain dictionary valid statements maps mathematical statement another. example dictionary value dictionary correct equation generation process look patterns dictionary. speciﬁcally look keys match subtree equation replace subtree pattern value key. e.g. given input equation subtree matching might produce equality ﬁnding key-value pair initial mathdictionaty constructed input list axioms. step equation generation choose equation random list correct equations choose random node equation tree changing. look subtree rooted matches several dictionary keys. randomly choose matches replace subtree value looking mathdictionary. generate possible equations particular depth proceeding higher depth. order ensure this limit depth ﬁnal equation increase limit equations added correct equations number repeats. examples correct incorrect identities generated dataset generation method given table generating function evaluation data generate input-output examples speciﬁc range numbers functions domain. unary functions randomly draw ﬂoating point numbers ﬁxed precision range evaluate functions randomly drawn number. binary functions repeat randomly generated numbers. make collection balanced well step generate outputs input conﬁguration correct incorrect one. note function evaluation results identities depths important dataset also representation numbers used evaluation. represent ﬂoating point numbers decimal expansion representable grammar. order make clear consider number order represent number expand decimal representation feed expressions training therefore represent ﬂoating point numbers ﬁnite precision using integers range analogous humans learn trigonometry elementary algebra propose using basic axioms learn properties mathematical functions. moreover leverage underlying structure mathematical identity make predictions validity. zaremba allamanis validate effectiveness using tree-structured neural networks modeling equations. show tree lstms powerful models capturing semantics data. therefore tree lstm model capture compositionality equation show improves performance simpler tree-structured a.k.a recursive neural networks. describe details model training setup section. tree lstm model symbolic expressions function evaluations structure tree lstm mirrors parse-tree input equation. shown figure input equation’s parse tree inherent equation. described section equation consists terminals binary unary functions. terminals input tree lstm leaves embeds representation using vectors. function associated lstm block weights weights shared among appearances function different equations. predict validity equation root tree. architecture neural network slightly different symbolic expressions compared function evaluation expressions. recall section distinguished terminal types. directly reﬂects structure network used leaves embedding. moreover different loss functions type expressions described below. symbolic expressions expressions consist constants symbols. terminals represented one-hot encoding passed symbol single layer neural network block. validity symbolic expression veriﬁed computing product left-hand-side right-hand-side vector embeddings applying logistic function. function evaluation expressions order encode terminals function evaluation expressions train autoencoder. encoder side embeds ﬂoating point numbers high-dimensional vector space. call number block. decoder auto-encoder trained predicting ﬂoating point number given input embedding. call decoder block. pass output vector embedding left-hand-side right-hand-side decoder block. validity function evaluation computed minimizing loss decoder outputs side. baseline models compare proposed model chain-structured neural networks sequential recurrent neural networks lstms’s well tree-structured neural networks consisting fully connected layers noted papers discover equivalence classes dataset since data consists many equivalence classes especially function evaluation data eqnet model proposed baseline. another baseline used sympy. given equality sympy either returns true false returns input equality original form let’s call unsure class. reported sympy accuracies treated unsure class miss-classiﬁcation. noted however since sympy used time data generation validate correctness generated equations accuracy predicting correct equations dataset always therefore degradation sympy’s performance table incorrect equations. interesting sympy’s performance another oracle used validating correct equalities. show experiments structure network crucial equation veriﬁcation equation completion. moreover adding function evaluation data tree-structured models show using type data broadens applicability model enable function evaluation also enhances ﬁnal accuracy symbolic equations compared function evaluation data used. demonstrate tree lstms outperform tree nn’s large margin without function evaluation data experiments. attribute fact lstm cells ameliorate vanishing exploding gradients along paths tree compared fully-connected blocks used tree nns. enables model capable reasoning equations higher depth reasoning difﬁcult task compared equation lower depth. therefore important tree cell memory lstm cell modeling properties mathematical functions. implementation details neural networks developed using mxnet experiments models tuned search space reported results best achievable prediction accuracy method. l-regularization well dropout avoid overﬁtting train models epochs. tuned hidden dimension optimizers {sgd rmsprop adam adagrad adadelta dcasgd sgld dropout rate {..} learning rate regularization ratio momentum {..}. networks achieved best performance using adam optimizer kingma learning rate regularization ratio hidden dimension dropout varies scenarios. indicate complexity identity depth. setup following experiments evaluate performance generalization capability proposed framework. investigate behavior learned model different tasks equation veriﬁcation equation completion. tasks assess results method symbolic well function evaluation expressions. compare models sequential recurrent neural networks lstms recursive tree neural networks also knows tree nn’s. moreover show effect adding function evaluation data ﬁnal accuracy symbolic expressions. models train dataset symbolic expressions. models tree lstm data tree data function evaluation data symbolic data. dataset consists symbolic equations correct. data includes equations depth equations depth equations depth equations depth noted equations depth maxed data generation. also function evaluation equations decimal expansion trees numbers includes correct samples. equations depth equations depth equations depth function evaluation dataset. numerical data includes numbers precision range chosen random. table generalization results train test contain equations depth results unseen equations. eval refer accuracy symbolic function evaluation expressions respectively. test sizes shown counts data. equation veriﬁcation generalization unseen identities experiment randomly split generated data includes equations depths train test partitions split ratio. evaluate accuracy predictions held-out data. results experiment presented table seen tree structured networks able make better predictions compared chain-structured networks. therefore leveraging structure identities capture information validity. moreover superiority tree lstm tree shows important incorporate cells memory. detailed prediction accuracy broken terms depth also terms symbolic function evaluation expressions also given table equation veriﬁcation extrapolation unseen depths evaluate generalization learned model equations higher lower complexity. generalization equations higher depth indicates network able learn properties mathematical function able complex equations validate correctness. ability generalize lower complexity indicates whether model infer properties simpler mathematical functions observing behavior complex equations. setup hold symbolic expressions certain depth train remaining depths. table presents results setups suggest tree lstm trained combination symbolic function evaluation data outperforms methods across metrics. comparing symbolic accuracy tree models without function evaluation data conclude models able utilize patterns function evaluations improve better model symbolic expressions well. equation completion experiment evaluate capability model completing equations ﬁlling blank unseen identities. experiment models reported table take test equations randomly choose node depth either equation replace possible conﬁgurations depth expressions grammar. give equations models look predictions blank. perform equation completion symbolic numeric expressions. figures show average accuracy top-k predictions deﬁne top-k accuracy percentage samples least correct match blank predictions. indicates hardest task high accuracy further function evaluation indicates prediction exactly hence lower accuracies figure therefore differences models function evaluation data models indicates importance combining symbolic function evaluation data task equation completion. also tree-structured models substantially better sequential models indicating important capture compositionality data structure model. finally tree lstm shows superior performance compared tree scenarios. present examples equations generated candidates figures tree lstm data model. figure presents results symbolic equation correct prediction value tree lstm able generate many candidates high conﬁdence correct. hand figure show function evaluation example prediction correct value precision even predictions quite close. also present example predictions model function evaluations plotting predicted values samples test data figure paper proposed combining black-box function evaluation data symbolic expressions improve accuracy broaden applicability previously proposed models domain. apply novel task validating completing mathematical equations. studied space trigonometry elementary algebra case study validate proposed model. also proposed novel approach generating dataset mathematical identities generated identities trigonometry elementary algebra. noted data generation technique limited trigonometry elementary algebra. show various experimental setups tree lstm’s trained combination symbolic expressions black-box function evaluation achieves superior results compared state-of-the-art models. future work expand testbed include mathematical domains inequalities systems equations. interesting multiple domains investigate learned representations domain transfered domains whether embedding domain clustered close similar word embedding vectors behave. also interested exploring recent neural models addressable differentiable memory order evaluate whether handle equations much higher complexity. authors would like thank amazon credits. arabshahi supported darpa award dap. anandkumar supported microsoft faculty fellowship career award ccf- darpa award force award fa---. mart´ın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg man´e rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda vi´egas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng. tensorflow large-scale machine learning heterogeneous systems https//www.tensorflow.org/. software available tensorﬂow.org. alexander alemi franc¸ois chollet geoffrey irving christian szegedy josef urban. deepmath-deep sequence models premise selection. arxiv preprint arxiv. miltiadis allamanis pankajan chanthirasegaran pushmeet kohli charles sutton. learning continuous semantic representations symbolic expressions. arxiv preprint arxiv. tianqi chen yutian naiyan wang minjie wang tianjun xiao bing chiyuan zhang zheng zhang. mxnet ﬂexible efﬁcient machine learning library heterogeneous distributed systems. aaron meurer christopher smith mateusz paprocki ondˇrej ˇcert´ık sergey kirpichev matthew rocklin amit kumar sergiu ivanov jason moore sartaj singh sympy symbolic computing python. peerj computer science chris piech jonathan huang andy nguyen mike phulsuksombati mehran sahami leonidas guibas. learning program embeddings propagate feedback student code. arxiv preprint arxiv. richard socher cliff chris manning andrew parsing natural scenes natural language recursive neural networks. proceedings international conference machine learning richard socher brody huval christopher manning andrew semantic compositionality recursive matrix-vector spaces. proceedings joint conference empirical methods natural language processing computational natural language learning association computational linguistics sheng richard socher christopher manning. improved semantic representations tree-structured long short-term memory networks. arxiv preprint arxiv.", "year": 2018}