{"title": "Recurrent Neural Network Training with Dark Knowledge Transfer", "tag": ["stat.ML", "cs.CL", "cs.LG", "cs.NE"], "abstract": "Recurrent neural networks (RNNs), particularly long short-term memory (LSTM), have gained much attention in automatic speech recognition (ASR). Although some successful stories have been reported, training RNNs remains highly challenging, especially with limited training data. Recent research found that a well-trained model can be used as a teacher to train other child models, by using the predictions generated by the teacher model as supervision. This knowledge transfer learning has been employed to train simple neural nets with a complex one, so that the final performance can reach a level that is infeasible to obtain by regular training. In this paper, we employ the knowledge transfer learning approach to train RNNs (precisely LSTM) using a deep neural network (DNN) model as the teacher. This is different from most of the existing research on knowledge transfer learning, since the teacher (DNN) is assumed to be weaker than the child (RNN); however, our experiments on an ASR task showed that it works fairly well: without applying any tricks on the learning scheme, this approach can train RNNs successfully even with limited training data.", "text": "recurrent neural networks particularly long short-term memory gained much attention automatic speech recognition although successful stories reported training rnns remains highly challenging especially limited training data. recent research found well-trained model used teacher train child models using predictions generated teacher model supervision. knowledge transfer learning employed train simple neural nets complex ﬁnal performance reach level infeasible obtain regular training. paper employ knowledge transfer learning approach train rnns using deep neural network model teacher. different existing research knowledge transfer learning since teacher assumed weaker child however experiments task showed works fairly well without applying tricks learning scheme approach train rnns successfully even limited training data. deep learning gained signiﬁcant success wide range applications example automatic speech recognition powerful deep learning model reported effective recurrent neural network e.g. obvious advantage rnns compared conventional deep neural networks rnns model long-term temporal properties thus suitable modeling speech signals. however rather inefﬁcient main reasons twists objective function caused high nonlinearity; vanishing explosion gradients backpropagation order address difﬁculties modiﬁed architecture called long short-term memory proposed successfully applied echo state network architecture proposed hidden-to-hidden weights learned training problem gradients exist. recently special variant hessian-free optimization approach successfully applied learn rnns random initialization particular problem approach computation demanding. another recent study shows carefully designed momentum setting signiﬁcantly improve training limited computation reach performance method although methods address difﬁculties training extent either tricky less optimal particularly limited data training remains difﬁcult. paper focuses lstm structure presents simple powerful training algorithm based knowledge transfer. algorithm largely motivated recently proposed logit matching dark knowledge distiller basic idea knowledge transfer approach well-trained model involves rich knowledge target task used guide training models. current research focuses learning simple models powerful complex model ensemble models based idea model compression idea employed train small models large complex paper conduct opposite study employs simple model train complex rnn. different existing research tries distill knowledge teacher model treat teacher model regularization training process child model smoothed pre-training step supervised training located good starting point. fact leads training approach easy perform extended model architecture. employ idea reset paper organized follows. section brieﬂy discusses related works section presents method. section presents experiments paper concluded section study directly motivated work dark knowledge distillation important aspect distinguishes work others existing methods focus distilling knowledge complex model improve simple models whereas study uses simple models teach complex models. teacher model work fact knows much sufﬁcient provide rough guide important train complex models rnns present study. another related work knowledge transfer dnns rnns proposed however employs knowledge transfer train dnns rnns. still follows conventional idea described above different ours. idea well-trained model used teacher guide training models proposed several authors almost time basic assumption teacher model encodes rich knowledge task hand knowledge distilled boost child model often simpler learn many details without teacher’s guide. ways distill knowledge. logit matching approach proposed teaches child model encouraging logits close teacher model terms norm dark knowledge distiller model proposed encourages posterior probabilities child model close teacher model terms cross entropy. transfer learning applied learn simple models approach performance complex model large model ensemble example learning small large complex focus dark knowledge distiller approach showed better performance experiments. basically well-trained model plays role teacher generates posterior probabilities training samples targets training models. posterior probabilities called ‘soft targets’ since class identities deterministic original one-hot ‘hard targets’. make targets softer temperature applied scale logits softmax formulated index output units. introduction allows information non-targets distilled. example training sample hard target involve rank information second third class; soft targets e.g. rank information second third class reﬂected. additionally large applied target even softer allows non-target classes prominent training. note additional rank information non-target classes available original target distilled teacher model. additionally larger boosts information non-target classes time reduces information target classes. large soft target falls back uniform distribution informative more. therefore controls knowledge distilled teacher model hence needs appropriately according task hand. dark knowledge form soft targets used boosting simple models also training complex models. argue training soft targets offers least advantages provides information model training makes training reliable. advantages particularly important training complex models especially training data limited. firstly soft targets offer probabilistic class labels ‘deﬁnite’ hard targets. hand matches real situation uncertainty always exists classiﬁcation tasks. example speech recognition often difﬁcult identify phone class frame effect co-articulation. hand uncertainty involves rich information within single example. example uncertainty phone classes indicates phones similar easy confused. making information form soft targets helps improve statistical strength phones collaborative therefore particularly helpful phones little training data. targets blur decision boundary classes offers smooth training. smoothness associated soft targets noticed states soft targets result less variance gradient training samples. easily veriﬁed looking gradients backpropagated logit layer i-th logit target output child model training. accumulated this argument confused conclusion found also applied child large equal logit matching. assumption equivalence large compared magnitude logit values inﬁnitely large. fact large gradient approach zero knowledge distilled teacher model. rationale soft targets results reliable training used conduct model initialization. however since information involved soft targets less discriminative reﬁnement hard targets tends helpful. informally interpreted teaching model less important discriminative information ﬁrstly model strong enough discriminative information learned. leads pre-training strategy based dark conventional pre-training apknowledge transfer. proaches based either restricted boltzmann machine auto-encoder simple models trained stacked construct complex models. dark knowledge pre-training functions different makes complex model trainable using less discriminative information model structure change. approach possesses several advantages totally supervised task-oriented; pre-trains model whole instead layer layer tends fast; used pre-train complex models layer structure clear model focus paper. pre-training view related curriculum training method discussed training samples easy learn ﬁrstly selected train model difﬁcult ones selected later model fairly strong. dark knowledge pre-training soft targets regarded easy samples pre-training hard targets difﬁcult samples ﬁne-tuning. interestingly regularization view pre-training view closely related. pre-training essentially regularization places model location parameter space good local minima easily reached. relationship regularization pre-training discussed context training verify proposed method train acoustic models task known difﬁcult. note rnns mention section indeed lstms. experiments conducted aurora database noisy conditions data proﬁle largely standard utterances model training utterances development utterances testing. kaldi toolkit used conduct model training performance evaluation process largely follows aurora recipe gpu-based training. specifically training starts constructing system based gaussian mixture models standard dimensional mfcc features plus ﬁrst second order derivatives. system trained alignment provided system. feature used system -dimensional fbanks. symmetric frame window applied concatenate neighboring frames transform used reduce feature dimension forms input. architecture const constant term. assume child model well learn teacher model gradient variance approaches zero soft targets impossible hard targets even training converged. reduced gradient variance highly desirable training deep complex models rnns. argue mitigate risk gradient vanishing explosion well known hinder training leading reliable training. known including soft hard targets improves performance appropriate setting weight factor balance relative contributions formulated regularized training problem objective function given represents parameters model cost associated hard soft targets respectively weight factor. additionally hard soft targets i-th sample j-th class respectively. note objective function conventional supervised training plays role regularization. effect regularization term force model training mimic teacher model knowledge transfer. study model used teacher model regularize training rnn. regularization training looks optima produce similar targets does risk over-ﬁtting under-ﬁtting largely reduced. instead training model soft hard targets altogether ﬁrst train reasonable model soft targets reﬁne model hard targets. transfer learning plays role pre-training conventional supervised training plays role ﬁne-tuning. involves hidden layers layer consists units. output layer composed units equal total number gaussian mixtures system. cross entropy used training criterion stochastic gradient descendent algorithm employed perform training. dark knowledge transfer learning trained model used teacher model generate soft targets training. architecture involves layers lstms cells layer. unidirectional lstm recurrent projection layer non-recurrent discarded. input features dimensional fbanks output units correspond gaussian mixtures dnn. trained streams stream contains continuous frames. momentum empirically starting learning rate default. experimental results reported table performance evaluated terms criteria frame accuracy word error rate related training criterion important speech recognition. table reported training cross validation reported test set. table rnn- baseline trained hard targets. rnn-t rnn-t trained dark knowledge transfer temperature respectively. dark knowledge transfer model soft targets employed three ways ‘soft’ soft targets used training; ‘reg.’ soft hard targets used together soft targets play role regularization gradients soft’s scaled ‘pretrain’ soft targets hard targets used sequentially soft targets play role pre-training. weight factor regularization approach empirically observed baseline beat baseline terms although much effort devoted calibrate training process including various trials different learning rates momentum values. consistent results published kaldi recipe. note mean rnns inferior dnns. results clear model leads better quality terms training objective. unfortunately advantage propagated test set. additionally results shown interpreted rnns suitable fact several researchers reported better wers rnns e.g. results aurora database basic training method generalize well terms although works well terms training criterion. problem largely solved dark knowledge transfer learning demonstrated results rnn-t rnn-t systems. seen soft targets only system obtains equal even better performance comparison baseline means knowledge embedded model transferred model knowledge arranged better form within structure. paying attention results seen knowledge transfer learning improve accuracy training leads better close compared baseline. indicates transfer learning soft targets sacriﬁces performance training little leads better generalization set. additionally advantage indicates generalization improved sense data sets also sense evaluation metrics. combining soft hard targets either regularization pre-training performance terms improved. conﬁrms hypothesis knowledge transfer learning play roles regularization pre-training. note cases results training lower baseline conﬁrms advantage knowledge transform learning resides improving generalizability resultant model. comparing dark knowledge systems different temperatures leads little worse training slightly better wers. conﬁrms higher temperature generates smoother direction leads better generalization. proposed novel training method based dark knowledge transfer learning. experimental results task demonstrated knowledge learned simple models effectively used guide training complex models. knowledge used either regularization pre-training approaches lead models generalizable desired property complex models. future work involves applying technique complex models difﬁcult train conventional approaches example deep rnns. knowledge transfer heterogeneous models investigation well e.g. probabilistic models neural models. graves a.-r. mohamed hinton speech recognition deep recurrent neural networks proceedings ieee international conference acoustics speech signal processing ieee senior beaufays long short-term memory recurrent neural network architectures large scale acoustic modeling proceedings annual conference international speech communication association zhao j.-t. huang gong learning small-size output-distribution-based criteria proceedings annual conference international speech communication association povey ghoshal boulianne burget glembek goel hannemann motlicek qian schwarz silovsky stemmer vesely kaldi speech recognition toolkit ieee workshop automatic speech recognition understanding. ieee signal processing society dec. ieee catalog cfpsrw-usb.", "year": 2015}