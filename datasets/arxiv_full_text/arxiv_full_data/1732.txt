{"title": "Deep Belief Nets for Topic Modeling", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Applying traditional collaborative filtering to digital publishing is challenging because user data is very sparse due to the high volume of documents relative to the number of users. Content based approaches, on the other hand, is attractive because textual content is often very informative. In this paper we describe large-scale content based collaborative filtering for digital publishing. To solve the digital publishing recommender problem we compare two approaches: latent Dirichlet allocation (LDA) and deep belief nets (DBN) that both find low-dimensional latent representations for documents. Efficient retrieval can be carried out in the latent representation. We work both on public benchmarks and digital media content provided by Issuu, an online publishing platform. This article also comes with a newly developed deep belief nets toolbox for topic modeling tailored towards performance evaluation of the DBN model and comparisons to the LDA model.", "text": "applying traditional collaborative ﬁltering digital publishing challenging user data sparse high volume documents relative number users. content based approaches hand attractive textual content often informative. paper describe large-scale content based collaborative ﬁltering digital publishing. solve digital publishing recommender problem compare approaches latent dirichlet allocation deep belief nets low-dimensional latent representations documents. efﬁcient retrieval carried latent representation. work public benchmarks digital media content provided issuu online publishing platform. article also comes newly developed deep belief nets toolbox topic modeling tailored towards performance evaluation model comparisons model. article concerns comparison deep belief nets latent dirichlet allocation ﬁnding low-dimensional latent representation documents. generative bag-of-words models represent conceptual meanings documents. similar documents query document retrieved lowdimensional output space distance measurement. deep belief toolbox developed implement evaluate comparisons. advantage ability highly nonlinear dimensionality reduction deep architecture low-dimensional representation output space results fast retrieval similar documents query document. model mixture model seeking posterior distribution visible hidden variables number topics must given model deﬁning dimensionality dirichletdistributed output space. latent representation document probability document topic comprising vector size simulations model used gensim package python article conducted collaboration issuu digital publishing platform delivering reading experiences magazines books catalogs newspapers. direct acyclic graph except layers form undirected bipartite graph. layers gives ability unroll deep autoencoder perform reconstructions input data consist visible layer output layer number hidden layers. training process deﬁned result gibbs chain running single gibbs step. epdata expectation respect joint distribution real data pdata pdatapdata. eprecon denotes expectation respect reconstructions. optimize training weight decay momentum parameter update model parameters pretraining passed ﬁne-tuning. network transformed replicating mirroring input hidden layers attaching output dbn. backpropagation unlabeled data performed computing probability input data instead computing probability label provided input data possible generate error estimation comparing normalized input data output probability. stochastic binary units pre-training replaced sigmoid units deterministic real-valued probabilities. since input data multinomial distribution cross-entropy applied error function. conjugate gradient optimization framework used produce values model parameters ensure convergence. output binary real output values binary output values computed adding deterministic gaussian noise input output layer ﬁne-tuning. output logistic sigmoid function output units close output values trained compared threshold order decide binary value. distance metrics using binary output vectors much faster performed model evaluations newsgroups dataset. dataset based wikipedia corpus used compare model since contains labeled data. issuu corpus labeled test compare labels deﬁned human perception topic distributions issuu’s model. models evaluated retrieving number similar documents query document test average possible queries. provides fraction number documents test similar documents proximity output space. number neighbors evaluated evaluation denoted accuracy measurement. steps pre-training ﬁne-tuning. pre-training layers separated pairwise form restricted boltzmann machines trained independently output lower provided input next higher-level forth. layers trained partly independent systems. goal pre-training process achieve approximations model parameters. document modeled word count vector. model word-count vectors bottom replicated softmax model hidden layers rbms consist stochastic binary units. training executed gibbs sampling using contrastive divergence approximation gradient rbms applies batch learning model performs single gibbs step updating weights given visible input vector probability hidden unit given denotes logistic sigmoid function bias hidden unit state visible unit weight visible unit hidden unit denotes number visible units. except visible units binary probability given denotes bias visible unit number hidden units. assumes multinomial distribution units visible layer softmax units. number softmax units identical weights equivalent multinomial unit sampled number times probability taking value learning rate momentum weight decay weights initialized -mean normal distribution variance biases initialized number epochs pre-training procedure applies batchlearning batch represents documents. ﬁne-tuning larger batches documents generated. perform three line searches conjugate gradient algorithm number epochs gaussian noise binary output deﬁned deterministic noise mean variance fig. dbnt performs comparison model hinton salakhutdinov comparing real valued output binary output observed accuracy measurements similar higher dimensional output vector following simulations considered real valued output vectors though. fig. shows manages internal representation documents better high dimensional input vectors. figure accuracy measurements ---dbn binary output units ----dbn binary output units dbnt. models trained newsgroups dataset. results read directly graph. tain articles subcategories business category. categories large pool articles strong connectivity remaining categories wikipedia corpus. categories administration commerce companies ﬁnance globalization industry labor management marketing occupations sales sports business. wikipedia business dataset consists documents split training documents test documents. wikipedia business provide indication well model captures granularity data within subcategories wikipedia corpus. order compare model model computed accuracy measurements -----dbn real numbered linear output units accuracy measurements models topics another topics. accuracy measurement -----dbn outperforming models model topics perform much worse dbn. model topics perform well evaluating neighbor deteriorates quickly throughout evaluation points. superior model dimensionality reduction wikipedia business dataset. accuracy measurements higher output -dimensional compared -dimensional topic distribution model lowest error. fig. evident -dimensional output scores much lower accuracy measurement inability differentiate documents. increasing number output units modeling -----dbn ----dbn outperform original -----dbn. even though output vector twice size other evaluations almost identical indicates saturation. fig. racy measurements evident similar results models are. difference generates -dimensional latent representation opposed -dimensional latent representation model. plotting test dataset output vectors ----dbn principal component evident input data cluttered manages spread documents output space according labels performing analysis output space categories business cars close proximity category like food cooking. test issuu dataset extracted dataset across categories deﬁned issuu’s model. documents dataset belong categories business cars food cooking individual team sports travel. training contains documents test contains documents. mentioned issuu applied labels dataset results model -dimensional latent representation. order compare models performed accuracy measurements ----dbn labels accuexploratory data analysis issuu corpus show -----dbn maps documents output space. chosen random query documents different categories retrieved documents within nearest proximity. query publication documents retrieved output space cars. publications promoting published manufacturer. related publications concern type car. comparing query output space query highdimensional input space similar documents accurate output space human perception. wikipedia issuu corporas shown superior compared proposed models. manages better internal representation documents output space lower dimensionality. dimensionality output space results fast retrieval similar documents. binary output vector larger dimensionality performs almost good real valued output vector equivalent dimensionality. finding similar documents binary latent representations even faster. hinton g.e. practical guide training restricted boltzmann machines. neural networks tricks trade volume lecture notes computer science springer reh˚uˇrek radim sojka petr. software framework proceedtopic modelling large corpora. ings lrec workshop challenges frameworks valletta malta elra. http//is.muni.cz/publication//en.", "year": 2015}