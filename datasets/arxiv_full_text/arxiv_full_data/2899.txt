{"title": "Understanding symmetries in deep networks", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "Recent works have highlighted scale invariance or symmetry present in the weight space of a typical deep network and the adverse effect it has on the Euclidean gradient based stochastic gradient descent optimization. In this work, we show that a commonly used deep network, which uses convolution, batch normalization, reLU, max-pooling, and sub-sampling pipeline, possess more complex forms of symmetry arising from scaling-based reparameterization of the network weights. We propose to tackle the issue of the weight space symmetry by constraining the filters to lie on the unit-norm manifold. Consequently, training the network boils down to using stochastic gradient descent updates on the unit-norm manifold. Our empirical evidence based on the MNIST dataset shows that the proposed updates improve the test performance beyond what is achieved with batch normalization and without sacrificing the computational efficiency of the weight updates.", "text": "recent works highlighted scale invariance symmetry present weight space typical deep network adverse effect euclidean gradient based stochastic gradient descent optimization. work show commonly used deep network uses convolution batch normalization relu max-pooling sub-sampling pipeline possess complex forms symmetry arising scaling-based reparameterization network weights. propose tackle issue weight space symmetry constraining ﬁlters unit-norm manifold. consequently training network boils using stochastic gradient descent updates unit-norm manifold. empirical evidence based mnist dataset shows proposed updates improve test performance beyond achieved batch normalization without sacriﬁcing computational efﬁciency weight updates. stochastic gradient descent workhorse optimization deep networks well-known form uses euclidean gradients varying learning rate optimize weights. regard recent work brought light scale invariance properties weight space commonly used deep networks possess. symmetries invariance reparameterizations weights imply even though loss function remains unchanged euclidean gradient varies based chosen parameterization. consequently optimization trajectories vary signiﬁcantly different reparameterizations although issues raised recently precursor methods early work amari proposed natural gradients tackle weight space symmetries neural networks. idea compute steepest descent direction weight update manifold deﬁned symmetries direction update weights theses proposals either computationally expensive implement need modiﬁcations architecture. hand optimization manifold symmetries invariances topic much research provides guidance simpler metric constructions section analysis commonly used network shows exists complex forms symmetries affect optimization hence need deﬁne simpler weight updates take account invariances. accordingly section look ∗this work initiated author department electrical engineering computer science university li`ege li`ege belgium visiting department engineering university cambridge cambridge particular resolving symmetries constraining ﬁlters unit-norm manifold. results geometric viewpoint manifold search space. proposed updates shown table symmetry-invariant numerically efﬁcient implement. stochastic gradient descent algorithms proposed updates implemented matlab manopt codes available http//bamdevmishra.com/codes/ deepnetworks. layer deep architecture archbn shown figure layer archbn typical components commonly found convolutional neural networks multiplication trainable weight matrix batch normalization layer element-wise rectiﬁcation relu max-pooling stride sub-sampling. ﬁnal layer k-way soft-max classiﬁer network trained cross-entropy loss. rows weight matrices correspond ﬁlters layers respectively. dimension corresponds input dimension layer. mnist digits dataset input dimensional vector. ﬁlters layers dimensionality dimension corresponds trainable class vector. batch normalization layer normalizes feature layers zero-mean unit variance mini-batch. separate trainable scale shift applied resulting features obtain respectively. effectively models distribution features gaussian whose mean variance learnt training. empirical results show signiﬁcantly improves convergence experiments also support claim. observation normalization allows complex symmetries exist network. consider reparameterizations diag diag elements real number. diag operator creates diagonal matrix argument placed along diagonal. batch normalization makes unit-variance unchanged hence loss invariant reparameterizations weights. equivalently exists continuous symmetries reparameterizations leave loss function unchanged. stressed analysis differs authors deal simpler case unfortunately euclidean gradient weights invariant reparameterizations weights consequently optimization trajectories vary signiﬁcantly based chosen parameterizations. issue resolved either deﬁning suitable noneuclidean gradient invariant reparameterizations placing appropriate constraints ﬁlter weights show following section. table proposed symmetry-invariant updates loss function archbn. θt+) updated weight learning rate partial derivatives loss θt). operator orth normalizes rows respect respectively linear projection operation projects arbitrary matrix onto tangent space oblique manifold element deﬁned diag)w efﬁcient resolve symmetries exist archbn constrain weight vectors oblique manifold i.e. ﬁlter fully connected layers constrained unit norm equivalently impose constraints diag operator extracts diagonal diag steepest descent direction loss unit-norm manifold computed euclidean gradient riemannian gradient unit-norm manifold effectively normal component euclidean gradient i.e. subtracted result tangential component. following tangential direction takes update manifold pulled back manifold retraction operation finally update unit-norm manifold form proposed weight update used stochastic gradient descent setting experiments described following section. emphasized proposed update numerically efﬁcient implement. formulas shown table convergence analysis manifolds follows developments train four layer deep archbn perform digit classiﬁcation mnist dataset features layer. digit images rasterized dimensional vector input network. input pre-processing performed. weights layer drawn standard gaussian ﬁlter unit-normalized. class vectors also drawn standard gaussian unit-normalized. sgd-based optimization choose base learning rate training run. ﬁnding base learning rate create validation images training set. train network ﬁxed learning rate using randomly chosen images epochs. start epoch training randomly permuted mini-batches sampled sequence ensuring training sample used within epoch. record validation error measured error training sample candidate base learning rate. choose candidate rate corresponds lowest validation error training network full training set. repeat whole process training runs network measure mean variance test error. ignore runs validation error diverged. full dataset training bold-driver protocol anneal learning rate. choose randomly chosen samples training remaining samples validation. train minimum epochs maximum epochs. training terminated either training error less validation error increases respect measured epochs successive validation error measurements differ less table four layer deep networks mean test error lower compared balanced simply euclidean update starting values ﬁlters class vectors unit-normalized. lowest mean variance test error obtained weight update used training four layer deep network. difference b-sgd updates signiﬁcant four layer deep network thereby highlighting performance improvement achieved standard batch normalization deeper networks. also seen regularize weights network training without introducing hyper-parameters e.g. weight decay term. also noted performance difference four layer networks large. raises question future research whether deep networks necessarily deep made shallower better optimization apply proposed weight updates table training segnet deep convolutional network proposed road scene image segmentation multiple classes network although convolutional possesses symmetries analyzed archbn network trained epochs camvid training images. predictions sample test images camvid shown figure qualitative results indicate usefulness symmetry-invariant weight updates larger networks arise practice. highlighted symmetries exist weight space deep neural network architectures currently popular. symmetries absorbed gradient descent applying unit-norm constraint ﬁlter weights. takes account manifold structure weights network reside. empirical results show test performance improved using proposed weight update technique modern architecture. future research direction would like explore efﬁcient symmetry-invariant weight update techniques exploit deep convolutional neural network used practical applications.", "year": 2015}