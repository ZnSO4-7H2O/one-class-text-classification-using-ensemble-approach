{"title": "Latent Intention Dialogue Models", "tag": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Developing a dialogue agent that is capable of making autonomous decisions and communicating by natural language is one of the long-term goals of machine learning research. Traditional approaches either rely on hand-crafting a small state-action set for applying reinforcement learning that is not scalable or constructing deterministic models for learning dialogue sentences that fail to capture natural conversational variability. In this paper, we propose a Latent Intention Dialogue Model (LIDM) that employs a discrete latent variable to learn underlying dialogue intentions in the framework of neural variational inference. In a goal-oriented dialogue scenario, these latent intentions can be interpreted as actions guiding the generation of machine responses, which can be further refined autonomously by reinforcement learning. The experimental evaluation of LIDM shows that the model out-performs published benchmarks for both corpus-based and human evaluation, demonstrating the effectiveness of discrete latent variable models for learning goal-oriented dialogues.", "text": "developing dialogue agent capable making autonomous decisions communicating natural language long-term goals machine learning research. traditional approaches either rely hand-crafting small state-action applying reinforcement learning scalable constructing deterministic models learning dialogue sentences fail capture natural conversational variability. paper propose latent intention dialogue model employs discrete latent variable learn underlying dialogue intentions framework neural variational inference. goal-oriented dialogue scenario latent intentions interpreted actions guiding generation machine responses reﬁned autonomously reinforcement learning. experimental evaluation lidm shows model out-performs published benchmarks corpus-based human evaluation demonstrating effectiveness discrete latent variable models learning goal-oriented dialogues. recurrent neural networks shown impressive results modeling generation tasks sequential structured output form machine translation caption generation natural language generation discriminative models trained learn conditional output distribution strings despite sophisticated architectures condition*equal contribution department engineering university cambridge cambridge united kingdom department computer science university oxford oxford united kingdom. correspondence tsung-hsien <thwcam.ac.uk> yishu miao <yishu.miaocs.ox.ac.uk>. mechanisms used ensure salience able model underlying actions needed generate natural dialogues. consequence sequence-to-sequence models limited ability exhibit intrinsic variability stochasticity natural dialogue. example goal-oriented dialogue systems sequence-to-sequence learning chatbots struggle generate diverse causal responses addition often insufﬁcient training data goal-oriented dialogues results over-ﬁtting prevents deterministic models learning effective scalable interactions. paper propose latent variable model latent intention dialogue model learning complex distribution communicative intentions goaloriented dialogues. here latent variable representing dialogue intention considered autonomous decision-making center dialogue agent composing appropriate machine responses. recent advances neural variational inference sparked series latent variable models applied models continuous latent variables reparameterisation trick commonly used build unbiased low-variance gradient estimator updating models. however since continuous latent space hard interpret major beneﬁts models stochasticity regularisation brought latent variable. contrast models discrete latent variables able produce interpretable latent distributions also provide principled framework semi-supervised learning critical tasks especially additional supervision external knowledge utilized bootstrapping however variational inference discrete latent variables relatively difﬁcult problem high variance sampling. hence introduce baselines reinforce algorithm mitigate high variance problem carry efﬁcient neural variational inference latent variable model. parse input actionable commands access search useful information order answer query. based search result model needs summarise ﬁndings reply appropriate response natural language. lidm based end-to-end system architecture described comprises three components representation construction; policy network; generator shown figure capture user’s intent match system’s knowledge dialogue state vector derived user input knowledge base distributed utterance representation formed encoding user utterance bidirectional lstm concatenating ﬁnal stage hidden states together belief vector concatenation probability distributions domain speciﬁc slot-value pairs extracted pre-trained rnn-cnn belief trackers processed different cnns shown figure preceding machine response preceding belief vector. included model current turn discourse long-term dialogue context respectively. based belief vector query formed taking union maximum values slot. used search internal return vector representing degree matching produced counting matching venues re-structuring six-bin one-hot vector. among three vectors comprise dialogue state completely trainable data pre-trained using separate objective function produced discrete database accessing operation. details belief trackers database operation refer conditioning state policy network parameterises latent intention single layer softmax discrete conditional probability distribution sentences pre-processed delexicalisation slot-value speciﬁc words replaced corresponding generic tokens based ontology. lidm latent intention inferred user input utterances. based dialogue context agent draws sample intention guides natural language response generation. firstly framework neural variational inference construct inference network approximate posterior distribution latent intention. then sampling intentions response able directly learn basic intention distribution human-human dialogue corpus optimising variational lower bound. further reduce variance utilize labeled subset corpus labels intentions automatically generated clustering. then latent intention distribution learned semi-supervised fashion learning signals either direct supervision variational lower bound perspective reinforcement learning latent intention distribution interpreted intrinsic policy reﬂects human decision-making particular conversational scenario. based initial policy learnt semi-supervised variational inference framework model reﬁne strategy easily alternative objectives using policy gradient-based reinforcement learning. somewhat analagous training process used alphago game based lidm show different learning paradigms brought together framework bootstrap development dialogue agent summary contribution paper two-fold ﬁrstly show neural variational inference framework able discover discrete interpretable intentions data form decision-making basis dialogue agent; secondly agent capable revising conversational strategy based external reward within framework. important provides stepping stone towards building autonomous dialogue agent continuously improve interaction users. experimental results demonstrate effectiveness latent intention model achieves state-of-the-art performance automatic corpus-based evaluation human evaluation. goal-oriented dialogue aims building models help users complete certain tasks natural language interaction. given user input utterance turn knowledge base model needs carry inference lidm introduce inference network approximate posterior distribution optimise variational lower bound joint probability neural variational inference framework derive variational lower bound eqφ] λdkl||πθ) based dialogue state also interpret policy network latent dialogue management component traditional pomdp-based framework latent intention sampled conditional distribution parameters -hot representation last output token decoder’s last hidden state. note equation degree information state vector controlled sigmoid gate whose input signal sampled intention prevents decoder overﬁtting deterministic state information forces take sampled stochastic intention account. lidm formally written paramedespite steps described reducing variance remain major difﬁculties learning latent intentions completely unsupervised manner high variance inference network prevents generating sensible intention samples early stages training overly strong discriminative power lstm language model prone disconnection phenomenon lstm decoder rest components whereby decoder learns ignore samples focuses solely optimising language model. ensure stable training prevent disconnection semi-supervised learning technique introduced. inferring latent intentions underlying utterances similar unsupervised clustering task. standard clustering algorithms therefore used pre-process corpus generate automatic labels part training examples model trained unlabeled examples optimise modiﬁed variational lower bound given equation main purposes learning interpretable discrete latent intention inside dialogue system able control reﬁne model’s behaviour operational experience. learnt generative network encodes policy discovered underlying data distribution necessarily optimal speciﬁc task. since parameterised policy network itself policy gradient-based reinforcement learning algorithm modelled parameterised multinomial distributions approximation functions inference producing samples compute stochastic gradients generative distribution generates required samples composing machine response. difbased samples ferent strategies alternately optimise parameters variational lower bound this divide sets parameters decoder side directly updated back-propagating gradients however gradient estimator large variance because learning signal relies samples proposal distribution reduce variance inference follow reinforce algorithm introduce baselines centered learning signal input dependent baseline respectively help reduce variance. learnable constant mlp. training baselines updated minimising distance used ﬁne-tune initial policy objective functions interested based initial policy revisit training dialogues update parameters based following strategy encountering unlabeled examples turn system samples action learnt policy receives reward conditioning these directly ﬁne-tune subset model parameters policy gradient method parameterises policy network however labeled example encountered force model take labeled action update parameters equation well. unlike whole model reﬁned end-to-end using updating effectively allows reﬁne decision-making system avoid problem over-ﬁtting. explored properties lidm model using camrest corpus collected task system assist users restaurant cambridge area. corpus collected based modiﬁed wizard online data collection. workers recruited amazon mechanical turk asked complete task carrying conversation alternating roles user wizard. three informable slots users constrain search requestable slots user value restaurant offered. dialogues dataset approximately conversational turns total. database contains unique restaurants. make direct comparison prior work follow experimental setup corpus partitioned training validation test sets ratio lstm hidden layer sizes vocabulary size around pre-processing remove rare words words delexicalised. system components trained jointly ﬁxing pre-trained belief trackers discrete database operator model’s latent intention size respectively. trade-off constants produce selflabeled response clusters semi-supervised learning intentions ﬁrstly removed function words responses clustered according content words. assigned responses i-th frequent cluster i-th latent dimension supervised set. results labeled responses across whole dataset. example resulting seed shown table inference carried stochastic estimation taking sample estimating stochastic gradients. model trained adam tuned held-out validation set. alternately optimised generative model inference network ﬁxing parameters updating parameters other. reinforcement ﬁne-tuning generated sentence model replace ground truth turn deﬁne immediate reward whether improve dialogue success relative plus sentence bleu score constant ﬁne-tuned model parameters using epochs. testing greedily selected probable intention applied beam search beamwidth decoding response. decoding criterion average log-probability tokens response. evaluated model task success rate bleu score model used predict system response held-out test set. table presents results corpus-based evaluation. ground truth block shows metrics compute human-authored responses. sets gold standard task. published models block results three baseline models borrowed vanilla neural dialogue model plus attention mechanism belief trackers attentive self-supervised sub-task neurons. results lidm model without ﬁne-tuning shown lidm models lidm models blocks respectively. seen initial policy learned ﬁtting latent intention underlying data distribution yielded reasonably good results bleu perform well task success compared deterministic counterparts fact variational lower bound dataset optimised rather task success variational inference. however applied optimise success rate part reward function during ﬁne-tuning phase resulting lidm+rl models outperformed three baselines terms task success without signiﬁcantly sacriﬁcing bleu order assess human perceived performance evaluated three models lidm lidm+rl recruiting paid subjects amazon mechanical turk. judge asked follow task carried conversation machine. conversation judges asked rate compare model’s performance. assessed subjective success rate perceived comprehension ability naturalness responses scale model collected dialogues averaged scores. human evaluation sampled top- intentions lidm models decoded response based sample. result shown table interesting fact note although lidm perform well corpus-based task success metric human judges rated subjective success almost indistinguishably others. discrepancy experiments arises mainly corpus-based success metric favors greedy policies user side behaviours ﬁxed rather interactional. despite fact lidms considered marginally better subjective success lidms outperform comprehension naturalness scores. proposed lidm models better capture multiple modes communicative intention thereby respond naturally sampling latent intention variable. three example conversations shown human judge machine lidm table lidm+rl table respectively. results displayed exchange block. induced latent intention shown tuple followed decoded response sample dialogues produced following responses highlighted bold. seen lidm shown table clearly multiple modes distribution learned intention latent variable represents easily interpreted response generated. however intentions result different responses different dialogue states even though supervised small response shown table mainly variance introduced variational inference. finally comparing table table observe difference dialogue strategies lidm inferring policy supervised dataset reﬂects diverse modes underlying distribution; whereas lidm+rl curry prince indian restaurant east side town would like telephone number curry prince indian restaurant east town number curry prince serves indian type cuisine located east part town address newmarket road ditton phone address curry prince newmarket road ditton phone number curry prince located newmarket road ditton phone anything else help table sample dialogue lidm model exchange block. latent intention shown tuple followed decoded response. sample dialogue produced following responses highlighted bold. reﬁned strategy using exhibits much greedier behavior achieving task success also supported human evaluation table lidm+rl much shorter dialogues average compared models. learning end-to-end dialogue system appealing challenging credit assignment problem. discrete latent variable dialogue models lidm attractive latent variable serve interface decomposing learning language internal dialogue decision-making. decomposition effectively help resolve credit assignment problem different learning signals applied different sub-modules update parameters. variational inference discrete latent variables latent distribution basically updated reward variational lower bound. reinforcement learning latent distribution updated rewards dialogue success sentence bleu score. hence latent variable bridges different learning paradigms bayesian learning reinforcement learning brings together framework. framework provides robust neural network-based approach previous approaches depend solely sequence-to-sequence learning instead modeling chat-based dialogues sequence-to-sequence learning problem common theme deep learning community. vinyals demonstrated seqseq-based model trained huge amount conversation corpora learns interesting replies conditioned different user queries. however inability model dialogue context models generally suffer generic response problem several approaches proposed mitigate issue modeling persona reinforcement learning introducing continuous latent variables case make latent variable inject stochasticity generating natural diverse machine responses also model hidden dialogue intentions explicitly. combines merits reinforcement learning generative models. spectrum goal-oriented dialogue systems typically adopt pomdp framework break development dialogue systems pipeline modules natural language understanding dialogue managegandhi located regent street city centre telephone number gandhi serves indian kind food centre town cheap price range gandhi indian restaurant centre location cheap price range golden house located lensﬁeld road city centre number golden house located lensﬁeld road city centre number golden house located lensﬁeld road city centre phone table sample dialogues lidm+rl model exchange block. comparing table agent demonstrates much greedier behavior toward task success. seen block block agent provides address phone number even user asks. ment natural language generation system modules communicate dialogue formalism effect constitute ﬁxed handcrafted intentions. limits ability systems scale complex tasks. contrast lidm directly infers underlying dialogue intentions data handle intention distributions long tails measuring similarities existing ones variational inference. modeling end-to-end goal-oriented dialogue systems also studied recently however models typically deterministic rely decoder supervision signals ﬁnetune large model parameters. much research focused combining different learning paradigms signals bootstrap performance. example semi-supervised learning applied sample-based neural variational inference framework reduce sample variance. practice relies discrete latent variable vehicle supervision labels. reinforcement learning common learning paradigm dialogue systems policy also parameterised discrete actions. consequence lidm parameterises intention space discrete latent variable automatically enjoy beneﬁt bootstrapping signals coming different learning paradigms. addition selfsupervised learning simple generate automatic labels heuristics popular many tasks applied memory networks neural dialogue systems recently. since additional effort required labeling also viewed method bootstrapping. paper proposed framework learning dialogue intentions discrete latent variable models introduced latent intention dialogue model goal-oriented dialogue modeling. shown lidm discover effective initial policy underlying data distribution capable revising strategy based external reward using reinforcement learning. believe promising step forward building autonomous dialogue agents since learnt discrete latent variable interface enables agent perform learning using several differing paradigms. experiments showed proposed lidm able communicate human subjects outperforms previous published results. tsung-hsien supported toshiba research europe cambridge research laboratory. authors would like thank members cambridge dialogue systems group valuable comments. references auli michael jianfeng. decoder integration expected bleu training recurrent neural network language models. acl. association computational linguistics faruqui manaal dodge jesse jauhar sujay kumar dyer chris hovy eduard smith noah retroﬁtting word vectors semantic lexicons. naacl-hlt denver colorado may– june association computational linguistics. gaˇsi´c milica breslin catherine henderson matthew dongho szummer martin thomson blaise tsiakoulis pirros young steve. on-line policy optimisation bayesian spoken dialogue systems human interaction. icassp henderson matthew thomson blaise young steve. word-based dialog state tracking recurrent neural sigdial philadelphia networks. u.s.a. june association computational linguistics. higgins irina matthey loic arka burgess christopher glorot xavier botvinick matthew mohamed shakir lerchner alexander. beta-vae learning basic visual concepts constrained variational framework. iclr kiddon chlo´e zettlemoyer luke choi yejin. globally coherent text generation neural checklist models. emnlp austin texas november association computational linguistics. koˇcisk´y tom´aˇs melis g´abor grefenstette edward dyer chris ling wang blunsom phil hermann karl moritz. semantic parsing semi-supervised emnlp sequential autoencoders. austin texas november association computational linguistics. jiwei galley michel brockett chris jianfeng dolan bill. diversity-promoting objective funcnaacl-hlt tion neural conversation models. diego california june association computational linguistics. jiwei galley michel brockett chris spithourakis georgios jianfeng dolan bill. personabased neural conversation model. berlin germany august association computational linguistics. jiwei monroe will ritter alan jurafsky galley michel jianfeng. deep reinforcement learning dialogue generation. emnlp austin texas november association computational linguistics. miao yishu blunsom phil. language latent variable discrete generative models sentence compression. emnlp austin texas november association computational linguistics. s´eaghdha diarmuid tsunghsien thomson blaise young steve. neural belief tracker data-driven dialogue state tracking. vancouver canada august association computational linguistics. papineni kishore roukos salim ward todd wei-jing. bleu method automatic evaluation machine translation. philadelphia pennsylvania july association computational linguistics. serban iulian sordoni alessandro lowe ryan charlin laurent pineau joelle courville aaron bengio yoshua. hierarchical latent variable encoderdecoder model generating dialogues. arxiv preprint serban iulian vlad sordoni alessandro bengio yoshua courville aaron pineau joelle. hierarchical neural network generative models movie dialogues. aaai silver david huang maddison chris guez arthur sifre laurent driessche george schrittwieser julian antonoglou ioannis panneershelvam veda lanctot marc mastering game deep neural networks tree search. nature pei-hao vandyke david gasic milica dongho mrksic nikola tsung-hsien young steve. learning real users rating dialogue success neural networks reinforcement learning spoken dialogue systems. interspeech pei-hao gasic milica mrkˇsi´c nikola rojas barahona lina ultes stefan vandyke david tsung-hsien young steve. on-line active reward learning policy optimisation spoken dialogue systems. berlin germany august association computational linguistics. tsung-hsien gasic milica mrkˇsi´c nikola pei-hao vandyke david young steve. semantically conditioned lstm-based natural language generation spoken dialogue systems. emnlp lisbon portugal september association computational linguistics. tsung-hsien gasic milica mrkˇsi´c nikola rojas barahona lina pei-hao ultes stefan vandyke david young steve. conditional generation snapshot learning neural dialogue systems. emnlp austin texas november association computational linguistics. tsung-hsien vandyke david mrkˇsi´c nikola gaˇsi´c milica rojas-barahona lina pei-hao ultes stefan young steve. network-based endto-end trainable task-oriented dialogue system. eacl kelvin jimmy kiros ryan kyunghyun courville aaron salakhutdinov ruslan zemel richard bengio yoshua. show attend tell neural image caption generation visual attention. icml", "year": 2017}