{"title": "Subspace Network: Deep Multi-Task Censored Regression for Modeling  Neurodegenerative Diseases", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Over the past decade a wide spectrum of machine learning models have been developed to model the neurodegenerative diseases, associating biomarkers, especially non-intrusive neuroimaging markers, with key clinical scores measuring the cognitive status of patients. Multi-task learning (MTL) has been commonly utilized by these studies to address high dimensionality and small cohort size challenges. However, most existing MTL approaches are based on linear models and suffer from two major limitations: 1) they cannot explicitly consider upper/lower bounds in these clinical scores; 2) they lack the capability to capture complicated non-linear interactions among the variables. In this paper, we propose Subspace Network, an efficient deep modeling approach for non-linear multi-task censored regression. Each layer of the subspace network performs a multi-task censored regression to improve upon the predictions from the last layer via sketching a low-dimensional subspace to perform knowledge transfer among learning tasks. Under mild assumptions, for each layer the parametric subspace can be recovered using only one pass of training data. Empirical results demonstrate that the proposed subspace network quickly picks up the correct parameter subspaces, and outperforms state-of-the-arts in predicting neurodegenerative clinical scores using information in brain imaging.", "text": "introduction recent years witnessed increasing interests applying machine learning techniques analyze biomedical data. data-driven approaches deliver promising performance improvements many challenging predictive problems. example field neurodegenerative diseases alzheimer’s disease parkinson’s disease researchers exploited algorithms predict cognitive functionality patients brain imaging scans e.g. using magnetic resonance imaging finding points typically various types prediction targets jointly learned using multi-task learning e.g. predictive information shared transferred among related models reinforce generalization performance. challenges persist despite progress applying disease modeling problems. first important notice clinical targets different typical regression targets often naturally bounded. example output mini-mental state examination test reference deciding cognitive impairments ranges smaller score indicates higher level cognitive dysfunction cognitive scores clinical dementia rating scale alzheimer’s disease assessment scale-cog also specific upper lower bounds. existing approaches e.g. relied linear regression without considering range constraint partially fact mainstream models regression e.g. developed using least squares loss cannot directly extended censored regressions. second challenge majority research focused linear models computational efficiency theoretical guarantees. however linear models cannot capture complicated non-linear relationship features clinical targets. example showed early onset alzheimer’s disease related single-gene mutations chromosomes effects mutations cognitive impairment hardly linear recent advances multi-task deep neural networks provide promising direction model complexity demands huge number training samples prohibit broader usages clinical cohort studies. address aforementioned challenges propose novel efficient deep modeling approach non-linear multi-task censored regression called subspace network highlighting following multi-fold technical innovations abstract past decade wide spectrum machine learning models developed model neurodegenerative diseases associating biomarkers especially non-intrusive neuroimaging markers clinical scores measuring cognitive status patients. multi-task learning commonly utilized studies address high dimensionality small cohort size challenges. however existing approaches based linear models suffer major limitations cannot explicitly consider upper/lower bounds clinical scores; lack capability capture complicated non-linear interactions among variables. paper propose subspace network efficient deep modeling approach non-linear multi-task censored regression. layer subspace network performs multi-task censored regression improve upon predictions last layer sketching low-dimensional subspace perform knowledge transfer among learning tasks. mild assumptions layer parametric subspace recovered using pass training data. empirical results demonstrate proposed subspace network quickly picks correct parameter subspaces outperforms state-of-the-arts predicting neurodegenerative clinical scores using information brain imaging. reference format mengying inci baytas liang zhan zhangyang wang jiayu zhou. subspace network deep multi-task censored regression modeling neurodegenerative diseases. proceedings conference york pages. https//doi.org/./ permission make digital hard copies part work personal classroom granted without provided copies made distributed profit commercial advantage copies bear notice full citation first page. copyrights components work owned others must honored. abstracting credit permitted. copy otherwise republish post servers redistribute lists requires prior specific permission and/or fee. request permissions permissionsacm.org. conference’ july washington association computing machinery. isbn ---//.... https//doi.org/./_ transformation matrix rt×d belongs linear low-rank subspace subspace allows represent product matrices columns rt×r span linear subspace rr×d embedding coefficient. note output entry-wise decoupled component relu. assuming gaussian noise derive following likelihood function probabilistic density function standardized gaussian standard gaussian tail. controls accurately low-rank subspace assumption data. note noise models assumed well. likelihood pair thus given likelihood function allows estimate subspace coefficient data enforce low-rank subspace common approach impose trace norm trace norm value since minu e.g. objective function multi-task censored regression problem given efficiently builds deep network layer-by-layer feedforward fashion layer considers censored regression problem. layer-wise training allows grow deep model efficiently. explores low-rank subspace structure captures task relatedness better predictions. critical difference subspace decoupling previous studies method lies assumption low-rank structure parameter space among tasks rather original feature space. leveraging recent advances online subspace sensing show parametric subspace recovered layer feeding pass training data allows efficient layer-wise training. synthetic experiments verify technical claims proposed outperforms various state-of-the-arts methods modeling neurodegenerative diseases real datasets. censored regression given observations dimensional feature vectors corresponding outcomes outcome cognitive scores biomarkers interest proteomics. outcome censored regression assumes nonlinear relationship features outcome rectified linear without loss generality paper assume outcomes lower censored using variants tobit models e.g. proposed algorithms analysis extended censored models minor changes loss function. theoretical results establish asymptotic non-asymptotic convergence properties algorithm proof scheme inspired series previous works briefly present proof sketch proof details found appendix. iteration sample denote intermediate differentiated t-th columns proof feasibility assume sampled i.i.d. subspace sequence lies compact set. asymptotic case estimate stochastic gradient descent iterations seen minimizing approximate tight quadratic surrocost gate based second-order taylor approximation around furthermore shown smooth bounding first-order second-order gradients w.r.t. according analysis section pass training data warrant subspace learning problem. outline solver subproblem follows problem sketches parameters current space. solve using gradient descent. parameter sketching couples subspace dimensions thus need solve collectively. update refines subspace solve using stochastic gradient descent note problem decoupled different subspace dimensions careful parallel design procedure done efficiently. given training data point algorithm network expansion hierarchical parameter subspace sketching refinement. require training data target network depth ensure deep subspace network following established that subspace sequence asymptotically converges stationarypoint batch estimator mild conditions. sei= asymptotically converges according quasi-martingale property almost sure sense owing tightness first point implies convergence associated gradient sequence regularity bi-convex block variables non-asymptotic case finite asserts distance successive subspace estimates vanish fast i−∥f constant independent following leverage unsupervised formulation regret analysis similarly obtain tight regret bound vanish single layer model limited capability capture highly nonlinear regression relationships parameters linearly linked subspace except relu operation. however single-layer procedure algorithm provided building block based develop efficient algorithm train deep subspace network greedy fashion. thus propose network expansion procedure overcome limitation. obtain parameter subspace sketch single-layer case project data points relu. straightforward idea expansion samples train another layer. denote network structure obtained k-th expansion starts expansion recursively stack relu layers however observe simply stacking layers repeating many times cause substantial information loss degrade generalization performance especially since training layer-by-layer without looking back inspired deep residual networks exploit skip connections pass lower-level data features higher levels concatenate original samples newly transformed censored outputs time expansion i.e. reformulating formulation expansion given below summarize network expansion process alg. architecture resulting illustrated fig. compared single layer model gradually refines parameter subspaces multiple stacked nonlinear projections. expected achieve superior performance higher learning capacity proposed also viewed gradient boosting method. meanwhile layer-wise low-rank subspace structural prior would improve generalization compared naive multi-layer networks. simulations synthetic data subspace recovery single layer model. first evaluate subspace recovered proposed algorithm using synthetic data. generated rn×d rt×r rr×d i.i.d. random gaussian matrices. target matrix rn×t synthesized using random noise figure shows plot subspace difference ground-truth learned subspace throughout iterations i.e. w.r.t. result verifies algorithm able correctly find smoothly converge underlying low-rank subspace synthetic data. objective values throughout online training process algorithm plotted figure show plot iteration-wise subspace differences defined ui−∥f figure complies result non-asymptotic analysis. moreover distribution correlation recovered weights true weights tasks given figure predicted weights correlations ground truth subspace recovery multi-layer subspace network. regenerated synthetic data repeatedly applying three times time following setting single-layer model. three-layer learned using algorithm simple baseline multi-layer perceptron trained whose three hidden layers dimensions three relu layers inspired applied low-rank matrix factorization layer desired rank creating factorized baseline identical architecture re-trained f-mlp data leading another baseline named retrained factorized figure experimental results subspace convergence. subspace differences w.r.t. index convergence algorithm w.r.t. index iteration-wise subspace differences w.r.t. index table evaluates subspace recovery fidelity three layers using three different metrics maximum mutual coherence column pairs matrices defined classical measurement correlated matrices’ column subspaces are; mean mutual coherence column pairs matrices; subspace difference defined single-layer case. note mutual coherencebased metrics immune linear transformations subspace coordinates ℓ-based subspace difference might become fragile. achieves clear overall advantages three measurements f-mlp rf-mlp. notably performance margin subspace difference seems small much sharper margins mutual coherencebased measurements suggest recovered subspaces significantly better aligned groundtruth. benefits going deep. re-generate synthetic data first single-layer experiment; differently show deep boost performance singlelayer subspace recovery even data generation follow known multi-layer model. compare carefully chosen sets state-of-art approaches single multi-task shallow models; deep models. first least squares treated naive baseline ridge lasso regressions considered shrinkage variables selection purpose; censor regression also known tobit model non-linear method predict bounded targets e.g. multi-task models regularizations trace norm norm demonstrated successful simultaneous structured/sparse learning e.g. also verify benefits accounting boundedness targets single-task multitask settings best performance reported scenario deep model baselines construct three dnns fair comparison -layer fully connected architecture plain loss; -layer fully connected relu added output layer feeding loss naturally implements non-negativity censored training evaluation; iii) factorized re-trained following procedure rf-mlp multi-layer synthetic experiment. apparently iii) constructed verify also performed -fold random-sampling validation dataset i.e. randomly splitting training validation data times. split fitted model training data evaluated performance validation data. average normalized mean square error across tasks obtained overall performance split. methods without hyper parameters average anmse splits regarded final performance; methods tunable parameters e.g. lasso performed grid search values chose optimal anmse result. considered different splitting sizes training samples containing samples. table compares performance approaches. standard deviation trials given parenthesis observe that censored models significantly outperform uncensored counterparts verifying necessity adding censoring targets regression. therefore censored baselines hereinafter unless otherwise specified; structured models tend outperform single task models capturing task relatedness. also evidenced performance margin nonlinear models undoubtedly favorable even single-task tobit model outperform models; nonlinear censored model combines best accounting superior performance competitors. particular even -layer already produces comparable performance -layer thanks sn’s theoretically solid online algorithm sketching subspaces. layers reaching plateau layers observation consistent among splits. computation speed. experiments machine logic cores ram). accelerations enabled baselines exploited accelerations yet. running time single round training synthetic data given table training layer cost seconds average. improves generalization performance without significant computation time burden. furthermore accelerate further reading data batch mode performing parallel updates. experiments real data evaluated real clinical setting build models prediction important clinical scores representing subject’s cognitive status signaling progression alzheimer’s disease structural magnetic resonance imaging data. major neurodegenerative disease accounts percent dementia. national institutes health thus focused studies investigating brain fluid biomarkes disease supported long running project alzheimer’s disease neuroimaging initiative used adni- cohort experiments used tesla structural collected baseline performed cortical reconstruction volumetric segmentations freesurfer following procotol image extracted features representing cortical thickness surface areas region-of-interests using desikan-killiany cortical atlas preprocessing obtained dataset containing samples features. imaging features used predict clinical scores including adas scores baseline future baseline logical different rank assumptions. table compares performances non-calibrated versus calibrated models. observe across tasks. table clear improvement assuming different shows results comparison methods outperforming else. table shows performance growth increasing number layers. table reveals performance dnns using varying rank estimations real data. expected u-shape curve suggests overly rank informative enough recover original weight space high-rank structure cannot enforce strong structural prior. however overall robustness rank assumptions fairly remarkable performance ranks competitive consistently outperforming dnns rank assumptions baselines. qualitative assessment. multi-task learning perspective subspaces serve shared component transferring predictive knowledge among censored learning tasks. subspaces thus capture important predictive information predicting cognitive changes. normalized magnitude subspace range visualized subspace brain mappings. lowest level subspaces important five subspaces illustrated figure memory wechsler memory scale neurobattery scores neuropsychiatric inventory baseline future. calibration. formulations typically assume noise across tasks true variance among tasks design many cases. deal heterogeneous calibration step optimization process estimate task-specific relu input next layer repeat layer-wise. compare performance non-calibrated calibrated methods. performance. adopted sets baselines used last synthetic experiment real world data. different synthetic data low-rank structure predefined real data groundtruth rank available recognized locally tight upper-bound surrogate locally tight gradients. following appendix show smooth first-order second-order gradients bounded w.r.t. results convergence subspace iterates proven regime developed whose main inspirations came established convergence online dictionary learning algorithm using martingale sequence theory. nutshell proof procedure proceeds first showi= asymptotically according quasi-martingale property almost sure sense owing tightness implies convergence associated gradient sequence regularity meanwhile notice bi-convex block variables therefore convexity w.r.t. fixed parameter sketches also updated exactly iteration. combined claim asymptotic convergence iterations algorithm subspace sequence asymptotically converges stationary-point batch estimator mild conditions. proof non-asymptotic properties finite data streams rely unsupervised formulation regret analysis assess performance online iterates. specifically iteration previous span partial data prompted alternating nature iterations adopt variant unsupervised regret assess goodness online subspace estimates representing partially available data. loss incurred estimate predicting t-th datum find subspace captures different information. first subspace volumes right banks superior temporal sulcus found involve prodromal rostral middle frontal gyrus highest loads pathology volume inferior parietal lobule found increased s-glutathionylated proteins proteomics study significant magnitude. also find evidence strong association pathology brain regions large magnitude subspaces. subspaces remaining levels detailed clinical analysis available journal extension paper. conclusions future work paper proposed subspace network efficient deep modeling approach non-linear multi-task censored regression layer subspace network performs multi-task censored regression improve upon predictions last layer sketching low-dimensional subspace perform knowledge transfer among learning tasks. show mild assumptions layer recover parametric subspace using pass training data. demonstrate empirically subspace network quickly capture correct parameter subspaces outperforms state-of-the-arts predicting neurodegenerative clinical scores brain imaging. based similar formulations proposed method easily extended cases targets nonzero bounds lower upper bounds. appendix hereby give details proofs asymptotic non-asymptotic convergence properties algorithm recover latent subspace proofs heavily rely series previous results many results directly referred hereinafter conciseness. include proofs manuscript self-contained. ronald killiany teresa gomez-isla mark moss kikinis tamas sandor ferenc jolesz rudolph tanzi kenneth jones bradley hyman marilyn albert. structural magnetic resonance imaging predict alzheimer’s disease. annals neurology julien mairal francis bach jean ponce guillermo sapiro. online learning matrix factorization sparse coding. jmlr morteza mardani gonzalo mateos georgios giannakis. dynamic anomalography tracking network anomalies sparsity rank. sel. sig. proc. shelley newman rukhsana sultana marzia perluigi rafella coccia jian william pierce klein delano turner allan butterfield. increase s-glutathionylated proteins alzheimer’s disease inferior parietal lobule proteomics approach. journal neuroscience research james nicoll david wilkinson clive holmes phil steart hannah markham weller. neuropathology human alzheimer disease immunization amyloid-β peptide case report. nature medicine tara sainath brian kingsbury vikas sindhwani low-rank matrix factorization deep neural network training high-dimensional output targets. icassp. ieee robert sweet howard seltman james emanuel effect alzheimer’s disease risk genes trajectories cognitive function cardiovascular health study. ame. psyc. zhizheng cassia valentini-botinhao oliver watts simon king. deep neural networks employing multi-task learning stacked bottleneck features speech synthesis. icassp. ieee jian jinyu yifan gong. restructuring deep neural network acoustic models singular value decomposition.. interspeech. haiqin yang irwin king michael lyu. online learning multi-task daoqiang zhang dinggang shen alzheimer’s disease neuroimaging initiative multi-modal multi-task learning joint prediction multiple regression classification variables alzheimer’s disease. neuroimage investigate convergence rate sequence zero grows. nonconvexity online subspace iterates challenging directly analyze fast online cumulative loss approaches optimal batch cost advocates instead investigate whether converges established first referring lemma distance successive subspace estimates vanish fast t−∥f constant independent following proof proposition similarly show that uniformly bounded i.e. constants choosing constant step size bounded regret references ehsan adeli-mosabbeb kim-han thung feng dinggang shen. robust feature-sample linear discriminant analysis brain disorders diagnosis. nips. rahul desikan florent ségonne bruce fischl automated labeling system subdividing human cerebral cortex scans gyral based regions interest. neuroimage", "year": 2018}