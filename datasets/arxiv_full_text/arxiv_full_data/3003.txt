{"title": "DeepSetNet: Predicting Sets with Deep Neural Networks", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "This paper addresses the task of set prediction using deep learning. This is important because the output of many computer vision tasks, including image tagging and object detection, are naturally expressed as sets of entities rather than vectors. As opposed to a vector, the size of a set is not fixed in advance, and it is invariant to the ordering of entities within it. We define a likelihood for a set distribution and learn its parameters using a deep neural network. We also derive a loss for predicting a discrete distribution corresponding to set cardinality. Set prediction is demonstrated on the problem of multi-class image classification. Moreover, we show that the proposed cardinality loss can also trivially be applied to the tasks of object counting and pedestrian detection. Our approach outperforms existing methods in all three cases on standard datasets.", "text": "figure left traditional cnns learn parameters predict ﬁxed vector right contrast propose train separate learn parameter vector used predict cardinality particular output. exactly least number categories across images. however natural images typically show multiple entities perhaps important number differs image image. evaluation property taken account. imagenet large scale visual recognition challenge counts error true label among top- candidates. another strategy account multiple classes number certain value test instances report precision recall counting false positive false negative predictions done arguably methods suboptimal real-world scenario correct labelling unknown prediction fact rank labels according likelihood present also report many objects actually present particular image. deciding many objects present image crucial part human perception scene understanding missing current evaluation automated image understanding methods. second example consider pedestrian detecpaper addresses task prediction using deep learning. important output many computer vision tasks including image tagging object detection naturally expressed sets entities rather vectors. opposed vector size ﬁxed advance invariant ordering entities within deﬁne likelihood distribution learn parameters using deep neural network. also derive loss predicting discrete distribution corresponding cardinality. prediction demonstrated problem multi-class image classiﬁcation. moreover show proposed cardinality loss also trivially applied tasks object counting pedestrian detection. approach outperforms existing methods three cases standard datasets. deep neural networks shown state-of-the-art performance many computer vision problems including semantic segmentation visual tracking image captioning scene classiﬁcation object detection however traditional convolutional architectures require problem formulated certain particular designed predict vector either ﬁxed length whose size depends input example consider task scene classiﬁcation goal predict label given image. modern approaches typically address series convolutional layers followed number fully connected layers ﬁnally mapped predict ﬁxedsized vector length predicted vector corresponds number candidate categories e.g. imagenet challenge element score probability particular category ﬁnal prediction probability distribution categories. strategy perfectly admissible expects tion. parallel scene classiﬁcation motivated that again real scenarios number people particular scene known beforehand. common approach assign conﬁdence score number region candidates typically selected heuristically thresholding nonmaxima suppression. argue important simply discard information actual number objects test time exploit selecting subset region proposals. examples motivate underline importance prediction certain applications. important note that contrast vectors collection elements invariant permutation size ﬁxed advance. principled deﬁnition union cardinality distribution family joint distributions cardinality value. summary main contributions follows starting mathematical deﬁnition distribution derive loss enables employ existing machine learning methodology learn distribution data. integrate loss deep learning framework exploit power multi-layer architecture. present state-of-the-art results multi-label image classiﬁcation pedestrian detection standard datasets competitive results task object counting. sudden success multiple applications including voice recognition machine translation image classiﬁcation sparked deployment deep learning methods throughout numerous research areas. deep convolutional recurrent neural networks outperform traditional approaches tasks like semantic segmentation image captioning object detection here brieﬂy review recent approaches image classiﬁcation object detections point limitations. image scene classiﬁcation fundamental task understanding photographs. goal predict scene label given image. early datasets caltech- mostly contained single object could easily described category. consequently large body literature focused single-class prediction however real-world photographs typically contain collection multiple objects therefore captioned multiple tags. surprisingly exists rather little work multi-class image classiﬁcation makes deep architectures. gong combine deep cnns top-k approximate ranking loss predict multiple labels. propose hypotheses-pooling architecture speciﬁcally designed handle multi-label output. methods open promising direction underlying architectures largely ignore correlation multiple labels. address limitation recently wang combined cnns rnns predict number classes sequential manner. rnns however suitable prediction mainly reasons. first output represents sequence rather thus highly dependent prediction order shown recently vinyals second ﬁnal prediction result feasible solution post-processing heuristics beam search must employed show approach guarantees always predict valid also outperforms previous methods. pedestrian detection also viewed classiﬁcation problem. traditional approaches follow slidingwindow paradigm possible image region scored independently contain person not. recent methods like fast rcnn single-shot multi-box detector learn relevant image features rather manually engineering them retain sliding window approach. approaches require form postprocessing suppress spurious detection responses originate person. typically addressed non-maximum suppression greedy optimisation strategy ﬁxed overlap threshold. recently several alternatives proposed replace greedy procedure including sequential head detection lstms global optimisation approach even learning end-to-end using cnns none methods however explicitly consider number objects selecting ﬁnal boxes. contrary existing pedestrian detection approaches incorporate cardinality algorithm show improvement state benchmarks. note idea considering number pedestrians applied combination aforementioned detection techniques improve performances. important bear mind unlike many existing approaches learn count main goal object counting. rather derive formulation prediction using deep learning. estimating cardinality objects thereby count byproduct approach. demonstrate effectiveness formulation also conduct experiments task object counting outperforming many recent methods widely used uscd dataset. continuous random vector deﬁned stacking several continuous random variables ﬁxed length vector mathematical function describing possible values continuous random vector associated joint probabilities known probability dencontrast random ﬁnite ﬁnite-set valued random variable {y··· ym}. main difference random vector former number constituent variables random variables random unordered. throughout paper {y··· unknown cardinality {y··· ym}|| known cardinality vector known dimension. statistical function describing ﬁnite-set variable combinatorial probability density function consists discrete probability distribution so-called cardinality distribution family joint probability densities number values constituent variables. similar deﬁnition random variable must unity possible cardinality values possible element values permutations mdimensional random vector deﬁned terms conventional machine learning approaches bayesian learning convolutional neural networks disproposed learn optimal parameters tribution maps input vector output vector paper instead propose parameter vectors proach learn pair additional parameters deﬁne cardinality explain next section. begin deﬁning training training sample pair consisting input feature output ymi} following drop instance index better readability. note denotes cardinality respectively cardinality distribution symmetric joint probability density distribution elements. unit hypervolume makes joint distribution unitless denotes parameters estimate joint distribution element values ﬁxed cardinality represents collection parameters estimate cardinality distribution elements. formulation represents probability density general completely independent choices cardinality spatial distributions. thus straightforward transfer many applications require output set. however make problem amenable mathematical derivation implementation adopt assumptions outputs independent identically distributed cardinality follows poisson distribution parameter thus write distribution regularisation parameters proportional predeﬁned covariance parameters parameters also known weight decay parameters commonly used training neural networks. used input feature vector output vector example used predict distribution image classiﬁcation categories given input image note generally learned using number existing paper rely deep chine learning techniques. cnns perform task. learn highly complex function input feature parameters used estimating output cardinality distribution train second deep neural network. using neural networks predict discrete value seem counterintuitive because methods core rely backpropagation algorithm assumes differentiable loss. note achieve describing discrete distribution continuous parameters easily draw discrete samples distribution. formally estimate compute partial derivatives objective function w.r.t. standard backpropagation learn parameters deep neural network. refer reader supplementary material complete derivation partial derivatives detailed derivation posterior eqs. proof decomposition estimation inference w∗|d). since unit sequential inference explained ﬁrst calculate mode cardinality distribution maxm then calculate mode joint distribution given cardinality assume cardinality follows poisson distribution whose mean follows gamma distribution parameters estimated input data note cardinality distribution replaced discrete distribution. example valid assumption model number objects natural images poisson distribution thus could directly predict model distribution formulating cardinality however would limit model’s expressive power visually entirely different images number objects would mapped instead allow uncertainty mean model another distribution choose gamma mathematical convenience. consequently integrals simpliﬁed form negative binomial distribution .abm since samples i.i.d. joint probability maximised probability element maximised. therefore likely cardinality obtained ordering probabilities elements y··· output ﬁrst choosing elements highest probability values. note assumptions sec. necessary make learning inference computationally tractable amenable elegant mathematical formulation. major advantage approach state-of-the-art classiﬁer/detector ﬁrst left future work. proposed method best suited applications expect solution form i.e. permutation invariant unknown cardinality. perform experiment multi-label image classiﬁcation sec. addition explore cardinality estimation loss object counting problem sec. show sec. incorporating cardinality state-of-the pedestrian detector formulating problem boost performance. opposed common studied task problem image classiﬁcation rather label photograph arbitrary apriori unknown number tags. perform experiments standard benchmarks pascal dataset microsoft common objects context dataset implementation details. experiment similar build -layers network pretrained imagenet dataset. adapt purpose modifying last fully connected prediction layer predict classes pascal classes coco. ﬁne-tune entire network datasets using commonly used losses multi-label classiﬁcation softmax binary cross-entropy learn classiﬁers weight decay momentum dropout rate learning rate adjusted gradually decrease epoch starting softmax binary cross-entropy. learned parameters classiﬁers correspond proposed deep network fig. weighted approximate ranking objective another commonly used loss multi-label classiﬁcation. however perform well softmax binary cross-entropy used datasets learn cardinality distribution vgg- network modify ﬁnal fully connected layer predict values followed weighted sigmoid activation functions important note outputs must positive describe valid gamma distribution. therefore also append weighted sigmoid transfer functions weights ensure values predicted valid range. model sensitive parameters values large enough guarantee mode distribution accommodate largest cardinality existing dataset. ﬁne-tune network cardinality distribution using objective loss deﬁned train cardinality constant learning rate weight decay momentum rate dropout evaluation protocol. evaluate performance classiﬁers deep network employ commonly used evaluation metrics multi-label image classiﬁcation precision recall generated labels per-class overall precision deﬁned ratio correctly predicted labels total predicted labels recall ratio correctly predicted labels ground-truth labels. case predictions labels exist i.e. denominator becomes zero precision deﬁned generate predicted labels particular image perform forward pass choose top-k labels according scores similar since classiﬁer always predicts ﬁxed-sized prediction categories sweep maximum number classes generate precision/recall curve common practice multi-label image classiﬁcation. however proposed deepset network number labels instance predicted cardinality network. therefore prediction/recall dependent value single precision/recall value computed. calculate per-class overall precision/recall average values classes examples computed respectively. addition also report score averaged classes instances classes pascal pascal visual object classes benchmark widely used datasets detection classiﬁcation. consists images split training test objects pre-deﬁned categories annotated bounding boxes. image contain unique objects. compare results stateof-the-art classiﬁer described above. resulting precision/recall plots shown fig. together proposed approach using estimated cardinality. note figure qualitative results multi-class image labelling approach. image ground truth tags predictions denoted below. note show exact output network without heuristics post-processing. figure interesting failure cases method. spurious class predicted left artifact annotation many examples computer monitors actually labelled cases network correctly reason number objects concepts scene constrained ﬁxed list categories deﬁned dataset. enforcing correct cardinality image able clearly outperform baseline w.r.t. measures. note also prediction nearly replicate oracle ground truth cardinality known. mean absolute cardinality error prediction pascal microsoft coco. another popular benchmark image captioning recognition segmentation recent microsoft common objects context dataset consists thousand images labelled instance segmentation masks classes. number unique objects image vary around images training dataset contain classes handful images tags. majority images contain three labels. images training validation split remaining images test data. predict cardinality objects scene mean absolute error standard deviation figure experimental results multi-label image classiﬁcation. baselines represent state-of-theart classiﬁers ﬁne-tuned dataset using different loss functions. methods evaluated choosing top-k predictions across entire dataset different approach predicts thus evaluated single point outperforms classiﬁers signiﬁcantly terms precision recall comes close performance true cardinality known network compared softmax binary cross-entropy classiﬁers ranking values also outperform state-of-the multi-label classiﬁer cnn-rnn reported value results listed tab. show around percentage points improvement score baseline classiﬁers percentage points improvement compared state dataset. examples perfect label prediction using proposed approach shown fig. deep network properly recognise images labels well images many tags. also investigated failure cases either cardinality classiﬁer fails make correct prediction. showcase cases argue failure cases simply missed ground truth annotation left-most example actually semantically correct w.r.t. cardinality prediction penalized evaluation particular object category available dataset. best illustrated second example fig. here network correctly predicts number objects scene however belong categories dataset thus annotated. similar situations also appear images right. show robustness cardinality loss ﬁrst evaluate cardinality estimation common crowd counting application. apply approach widely used ucsd dataset compare results four state-of-the approaches uscd dataset includes -frames long video sequence captured ﬁxed outdoor surveillance camera. addition video region interest perspective scene location annotations pedestrians frame also provided. implementation details. build cardinality network structure well-known alexnet architecture. however replace ﬁrst convolutional layer single channel ﬁlter accept grayscale images input last fully connected layer layers outputs similar case estimate counts calculate mode negative binomial distribution. input grayscale image constructed superimposing region proposals scores generated off-the-shelf pedestrian detector multi-scale deep approach trained kitti dataset purpose. found input provides stronger signal images yielding better results. note process input images pedestrian detector however location perspective information available dataset. learning rely object count image region. follow exactly data split used conducting four different separate experiments maximal downscale upscale minimal subsets ucsd dataset. order train network similar data augmentation experiment extracting random patches training image corresponding ground truth counts. also randomly patch training. ensure count pedestrians entire image test time choose patch sizes exactly half image size perform inference resulting nonoverlapping regions. weights initialised randomly network trained epochs. hyperparameters sec. results. tab. shows mean absolute error predicted ground truth counts. show competitive superior performance experiment except ‘minimal’ subset. main reason training size small particular split even data augmentation cannot generalize cardinality model test images. moreover unlike methods utilize location information provide object count ground truth. considering overall performance approach outperforms state-ofthe-art counting approaches perspective performs favourably compared many existing methods exploit localisation perspective information. discussion. obvious alternative proposed cardinality loss seem directly regress alternative however main drawbacks. first cannot formulated within bayesian framework model uncertainty second regression loss yield discrete distribution hence underlying mathematical foundation paper. nevertheless experiments using standard regression loss reach performance. using regression loss achieve mean cardinality error quantify detection performance adapt evaluation metrics follow protocols used caltech detection benchmark evaluation metrics used log-average miss rate false positive image. additionally compute score score computed detections predicted deepset network compared highest score along ms-cnn precision-recall curve. calculate concatenate boxes resulted adaptive approach change threshold scores predicted sets. quantitative detection results shown tab. note retrain detector still able improve performance predicting number pedestrians frame dataset. proposed deep learning approach predicting sets. achieve goal derived loss learning discrete distribution cardinality. allowed standard backpropagation training deep network prediction. demonstrated effectiveness approach crowd counting pedestrian detection multi-class image classiﬁcation achieving competitive superior results three applications. network trained independently trivially applied existing classiﬁer detector improve performance. note decoupling direct consequence underlying mathematical derivation i.i.d. assumptions renders approach general applicable wide range models. future plan extend method multi-class cardinality estimation investigate models make i.i.d. assumptions. another potential avenue could exploit bayesian nature model include uncertainty opposed relying estimation alone. acknowledgments. research supported australian research council centre excellence robotic vision laureate fellowship idr. figure example pedestrian detection result approach. select relevant detection candidates overcomplete proposals state-of-the-art methods rely non-maximum suppression ﬁxed setting show better result achieved adjusting threshold adaptively depending number instances image section cast task pedestrian detection prediction problem demonstrate incorporating cardinality prediction beneﬁcial improve performance. perform experiments widely used datasets caltech pedestrians motchallenge benchmark recalling eqs. need networks parameters cardinality estimation detection proposw∗ respectively. cardinality network exact architecture setup sec. train training sets datasets. note intention engineer completely novel pedestrian detector take off-the-shelf state-of-thehere. rather system show improved taking cardinality prediction account. generate ﬁnal detection outputs detectors often rely non-maximum suppression greedily picks boxes highest scores suppresses boxes overlap pre-deﬁned threshold heuristic makes solution ad-hoc expressed formulation however still able improve detector performance adjusting threshold frame separately. obtain ﬁnal detection output prediction number people scene choose adaptive threshold image. particular start default value increase gradually number boxes reaches case number ﬁnal boxes larger pick boxes highest scores corresponds prediction discussed sec. ensure fair comparison also determine best value", "year": 2016}