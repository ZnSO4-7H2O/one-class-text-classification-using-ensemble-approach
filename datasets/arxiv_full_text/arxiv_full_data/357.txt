{"title": "Variational Inference of Disentangled Latent Concepts from Unlabeled  Observations", "tag": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "abstract": "Disentangled representations, where the higher level data generative factors are reflected in disjoint latent dimensions, offer several benefits such as ease of deriving invariant representations, transferability to other tasks, interpretability, etc. We consider the problem of unsupervised learning of disentangled representations from large pool of unlabeled observations, and propose a variational inference based approach to infer disentangled latent factors. We introduce a regularizer on the expectation of the approximate posterior over observed data that encourages the disentanglement. We evaluate the proposed approach using several quantitative metrics and empirically observe significant gains over existing methods in terms of both disentanglement and data likelihood (reconstruction quality).", "text": "disentangled representations higher level data generative factors reﬂected disjoint latent dimensions oﬀer several beneﬁts ease deriving invariant representations transferability tasks interpretability etc. consider problem unsupervised learning disentangled representations large pool unlabeled observations propose variational inference based approach infer disentangled latent factors. introduce regularizer expectation approximate posterior observed data encourages disentanglement. evaluate proposed approach using several quantitative metrics empirically observe signiﬁcant gains existing methods terms disentanglement data likelihood feature representations observed data play crucial role success machine learning algorithms. eﬀective representations able capture underlying latent generative factors relevant task ignoring inconsequential nuisance factors. disentangled feature representations property generative factors revealed disjoint subsets feature dimensions change single generative factor causes highly sparse change representation. disentangled representations oﬀer several advantages invariance easier derive representations invariant nuisance factors simply marginalizing corresponding dimensions transferability arguably suitable transfer learning underlying generative factors appear segregated along feature dimensions interpretability human expert able assign meanings dimensions conditioning intervention allow interpretable conditioning and/or intervention subset latents observe eﬀects nodes graph. indeed importance learning disentangled representations argued several recent works recognizing signiﬁcance disentangled representations several attempts made direction past much earlier work assumes sort supervision terms partial full access generative factors instance knowledge nature generative factors knowledge changes generative factors across observations knowledge complementary signal infer representations conditionally independent however real scenarios access observations without supervision generative factors. challenging problem many earlier attempts able scale well realistic settings recently chen proposed approach learn generative model disentangled factors based generative adversarial networks however implicit generative ∗ibm research watson research center yorktown heights {abhishkpsattigavinash.bala}us.ibm.com representation still entangled rest generative factors. models like gans lack eﬀective inference mechanism hinders applicability problem learning disentangled representations. recently higgins proposed approach based variational autoencoder kingma welling inferring disentangled factors. inferred latents using method empirically shown better disentangling properties however method deviates basic principles variational inference creating increased tension observed data likelihood disentanglement. turn leads poor quality generated samples observed work propose principled approach inference disentangled latent factors based popular scalable framework amortized variational inference powered stochastic optimization disentanglement encouraged introducing regularizer induced inferred prior. unlike β-vae approach introduce extra conﬂict disentanglement latents observed data likelihood reﬂected overall quality generated samples matches much better β-vae. come cost higher entanglement approach also outperforms β-vae disentangling latents measured various quantitative metrics. formulation start generative model observed data ﬁrst samples latent variable observation generated sampling joint density latents observations denoted ppθ. problem inference compute posterior latents conditioned pθdz assume given ﬁnite samples observations i.e. true data distribution practical scenarios involving high dimensional complex data computation intractable calls approximate inference. variational inference takes reducing approximate inference problem ﬁnding member density minimizes kullback-leibler divergence true posterior i.e. inference explicitly share information across inferences made observation. successful achieving variational inference so-called recognition model parameterized encodes inverse observations approximate posteriors recognition model parameters there recent attempts direction visual data often reconstructed samples semantically quite input samples sometimes even changing object classes. although generative model starts disentangled prior main objective infer disentangled latents potentially conducive various goals mentioned sec. consider density inferred latents induced approximate posterior inference mechanism variables disentangled prior naturally encourages inferring factors close disentangled. think reason original also observed exhibit disentangling behavior simple datasets mnist however behavior carry complex datasets unless extra supervision generative factors provided apart turn causes apart non-convexity elbo objective prevents achieving global minimum exklpθ) words maximizing elbo might also result reducing value klp) however aforementioned reasons klp) klpθ) could large stationary point convergence. hence minimizing klp) explicitly give better control disentanglement. motivates klp) part objective encourage disentanglement estimator would also change optimization saddle-point problem challenges might alleviate issues still involve complicating optimization saddle point problem three parameters. also noted using variational forms distances still leave approximation actual distance. outputs deep neural parameterized case reduces covqφ epσφ covp) want close identity matrix. simplicity choose entry-wise squared -norm measure proximity. however entanglement mainly reﬂected oﬀ-diagonal entries matrix separate hyperparameters controlling relative importance loss diagonal oﬀ-diagonal entries. gives rise following optimization problem inference regularization terms involving covp) objective eﬃciently optimized using sgd. maintain running estimate covp) updated every minibatch gradient current minibatch computed treating previous estimate covp) constant. taken great higher argued encourage disentanglement cost reconstruction error authors report empirical results ranging depending dataset. already mentioned models proposed literature including β-vae work prior diagonal figure disentanglement metric score function average reconstruction error test β-vae proposed dip-vae plots generated varying β-vae dip-vae λod. number next point plots value observations. also reﬂected quality reconstructed samples worse particularly high values proposed method increased tension likelihood term disentanglement objective sample quality method vae. finally note β-vae proposed method encourage disentanglement inferred factors pulling covqφ towards identity matrix β-vae attempts making covqφ close close individually observations proposed method directly works covqφ retains sensitivity conditioned-upon observation. chairs consists chair models model rendered azimuth angles elevation angles. following earlier work ignores near-duplicates subset chair models experiments. binary masks chairs observed data experiments following first models used training rest used test. shapes synthetic dataset binary shapes generated cartesian product shape x-position y-position scale rotation consider baselines task unsupervised inference disentangled factors recently proposed β-vae consistent evaluations network architectures latent dimensions used celeba chairs shapes datasets. best performing models terms disentanglement metric score introduced chairs data metric almost models pick models based subjective evaluation reconstruction quality disentanglement. disentanglement metric score reconstruction error. higgins propose metric evaluate disentanglement performance inference mechanism. assumes ground truth generative factors available. works ﬁrst sampling generative factor followed sampling pairs examples pair sampled generative factor takes value. given inferred example compute absolute diﬀerence vectors pair followed averaging diﬀerence vectors. average diﬀerence vector assigned label sampling minibatches pairs averaged diﬀerence vectors factor process repeated generative factors. capacity multiclass classiﬁer trained vectors predict identity corresponding generative factor. experiments one-vs-rest linear weight hinge loss weight regularizer higgins argue metric captures disentangled property inferred latents reasonably well. table shows disentanglement metric scores along reconstruction error test sets celeba shapes data. evident proposed dip-vae outperforms β-vae terms disentanglement metric score reconstruction error. also show plot disentanglement metric changes reconstruction error vary hyperparameter methods fig. clear proposed method gives much higher disentanglement metric score little cost reconstruction error compared reconstruction error β-vae gets much worse increased. training project along vectors. bias learned scalars used classifying test examples. table shows results attribute show highest change across various methods proposed dip-vae outperforms β-vae attributes. performance β-vae gets worse increased further. correlations inferred latent space. visualize pearson’s correlations dimensions inferred latents also visualize correlations inferred latent dimensions ground truth attributes tables show correlations β-vae proposed dip-vae. observe less correlations inferred latents dip-vae. latent correlations β-vae even higher seems going objective disentanglement. fact β-vae gives less weight figure attributes similar bangs rosy cheeks attribute celeba dataset based correlations inferred latents β-vae proposed dip-vae respectively. correlations latent dimension rest attributes indicate entanglement latent dimension ground truth attributes. noted many attributes celeba naturally entangled quite diﬃcult disentangle. plots attributes shown fig. rest plots found appendix. equivariance implies primitive transformation input results corresponding transformation feature i.e. disentanglement requires acts small subset dimensions sense equivariance general notion encompassing disentanglement special case however special case carries additional beneﬁts interpretability ease transferrability etc. invariance also special case equivariance requires identity invariant action input observations. however invariance obtained easily disentangled representations equivariant representations simply marginalizing appropriate subset dimensions. exists prior work literature equivariant invariant feature learning mostly supervised setting assumes knowledge nature input transformations proposed principled variational framework infer disentangled latents unlabeled observations. unlike β-vae variational objective conﬂict data log-likelihood disentanglement inferred latents reﬂected empirical results. interesting direction future work take account sampling biases generative process natural well artiﬁcial (e.g. collection face images contain much smiling faces males females misleading", "year": 2017}