{"title": "An Artificial Neural Network Architecture Based on Context  Transformations in Cortical Minicolumns", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Cortical minicolumns are considered a model of cortical organization. Their function is still a source of research and not reflected properly in modern architecture of nets in algorithms of Artificial Intelligence. We assume its function and describe it in this article. Furthermore, we show how this proposal allows to construct a new architecture, that is not based on convolutional neural networks, test it on MNIST data and receive close to Convolutional Neural Network accuracy. We also show that the proposed architecture possesses an ability to train on a small quantity of samples. To achieve these results, we enable the minicolumns to remember context transformations.", "text": "cortical minicolumns considered model cortical organization. function still source research reflected properly modern architecture nets algorithms artificial intelligence. assume function describe article. furthermore show proposal allows construct architecture based convolutional neural networks test mnist data receive close convolutional neural network accuracy. also show proposed architecture possesses ability train small quantity samples. achieve context transformations. uman perception always deals ambiguity brain. receptors provide clear input. input data receptors start analyzed explicit treatment disappears. almost every frame data different sense different context. reflected types perception. many illusions visual perception caused presence different geometric contexts example inverted face. language seems exclude ambiguity doesn't. humor based unexpected changes contexts. even delusions possible ability find context sense. specificities human perception fundamentally reflected modern machine-learning approaches. rtificial neural networks presented different architectures today. recurrent anns convolution anns much common based different concepts. meanwhile neocortex homogeneous surface. means concept applied different types data images video audio natural language. even manipulators' controlling explained approach. tasks completely different algorithms theories. fundamental problem modern anns need samples training humans. convolution anns take account translation invariants recurrent anns time invariants. impossible know rotate objects described without showing particular object different rotations. can't extract twice slower process time presented training data. breakthroughs could extract features different positions time. position time contexts too. also deal contexts time orientation size frequency tempo shape others. awareness data would changed contexts. dramatically reduce amount samples need extracting features input data have. widely accepted modern implementations convolutional neural networks claimed brain zones neocortex organized minicolumns contain neuron-detectors similar functions. found visual zone minicolumns orientational selectivity experiments. research shows different types selectivity different brain zones. according experiments approaches computer vision appeared self-organizing maps neocognitron. neocognitron architecture found realization convolutional neural networks still provide best accuracy recognition tasks computer vision. self-organizing maps also well-known tasks unsupervised learning. propose another interpretation allows creation original approach data generalization recognition tasks inspired relevant information todayâ€™s research. roots interpretation described detail lmost data processed brain presented unambiguously. objects always projected retina different ways words pronounced different tones even words language different senses different contexts. assumed minicolumn neocortex presents input information context. second task minicolumn solve pattern recognition. activity minicolumns proportional certainty recognition context minicolumn. close minicolumns represent similar context transformation. single local maximums activity field minicolumns send following output next level minicolumn recognized context patterns recognition shared among minicolumns. thus neurons types memory memory transformation memory generalization. modern vision neurons' structure gone vision times hubel wiesel. possible mechanisms types memory described sample architecture article based tensorflow framework classic perceptron conception. created model contain features approach illustrate proposed functions brain zone. sample model based well-developed frameworks deep learning thus architecture source codes transparent deep learning specialists. zone receives compressed vector describes relevant information zone. trained autoencoder bottleneck obtain input vector. additionally able decoder part visualization codes minicolumns. green layers figure mark encoder decoder parts autoencoder. fter input codes transferred different contexts codes minicolumn. thus minicolumns contained codes images geometrically transformed. codes decoded decoder part autoencoder visualization next step feature extracting minicolumns. best using convolution layer. convolution layers kernels weights detecting features applied position image. minicolumn codes might presented image allow application weight vector minicolumn. could extract features despite context. allows decrease number weight coefficients cnns increase quality prediction. following function brain zone output equals local maximums last convolution layer. sample left maximum channel applied max-pooling layer. output max-pooling layer connected fully-connected layer outputs softmax activation training recognition mnist. squared error chosen training. mean squared average error epochs. results worse error convolutional autoencoder experiments important exclude purity experiments. next stage training context transformations. article term context transformation training used back-propagation approach however process closer remembering. cases possible define transformation. example mnist dataset could define geometric transformations translation scale rotation. minicolumns geometric transformations. trained separate model context resulting nets trained one. training entered codes model transformation. sign resembling digits presented training set. meant nets could predict transformations code never seen. offers hope finite training remembering transformations full variety input codes different contexts. ifferent approaches applied detecting patterns different contexts could remembering samples. thing defined context presentation choice remembering. reason decided back-propagation mechanism training detectors' weights. shortest sharing detectors different contexts convolution layer. instead sharing weights among detectors different positions convolution layer provided weight sharing different contexts. feature especially important allows recognition patterns even samples representations therefore shot learning becomes possible. then channel max-pooling layer applied actually chooses best matching context. idea image transformation computer vision tasks well known described least spatial types transformer networks approach authors proposal used across different transformations types data. main difference demands differentiable transformation functions unattainable case discrete data. additionally transformation cannot trained spatial transformer networks. hinton writes capsules inspired cortical minicolumns proposed approach connects capsulenet visual stream perception. objects contexts recognition transformations capsulenet minicolumns remember geometric transformation changes different contexts two-stream hypothesis well-known accepted model visual auditory perception ventral called what pathway dorsal stream where pathway. minicolumns first selective object types shapes colors. second stream minicolumns position orientation selectivity. interpretations propose change perception visual stream brain significantly. minicolumn activity results best context transformation input data. meanwhile minicolumns where pathway contain sufficient information objects inside help recognize context particular minicolumn. quite intuitive case where pathway. contexts geometric transformations implemented mnist sample implementation. hard imagine transformations mentioned upper part figure almost changing coordinate system time space. however counterintuitive speak context object. example hinton shown model minicolumns' activation connected recognition particular mnist. problem regulated assumption codes minicolumns digit characterize objects' properties. properties different objects example font style digits mnist. thus codes minicolumns context transformation training designate style. close ideas capsulenet styles' code shared among minicolumns. two-stream hypothesis provides idea context-transformation described translate input codes space input vectors defined space describes something else. paper introduced architecture artificial neural networks based original ideas cortical minicolumns' functionality. transformation input data context specific minicolumn allows choose best matching pattern recognition. also gives opportunity separation patterns contexts presented. moreover restriction input data type transformation. discussed chapter two-stream hypothesis detected patterns zone contexts zones give clues abstract thinking artificial intelligence.", "year": 2017}