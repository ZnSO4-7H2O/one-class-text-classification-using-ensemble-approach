{"title": "Detecting Events and Patterns in Large-Scale User Generated Textual  Streams with Statistical Learning Methods", "tag": ["cs.LG", "cs.CL", "cs.IR", "cs.SI", "stat.AP", "stat.ML"], "abstract": "A vast amount of textual web streams is influenced by events or phenomena emerging in the real world. The social web forms an excellent modern paradigm, where unstructured user generated content is published on a regular basis and in most occasions is freely distributed. The present Ph.D. Thesis deals with the problem of inferring information - or patterns in general - about events emerging in real life based on the contents of this textual stream. We show that it is possible to extract valuable information about social phenomena, such as an epidemic or even rainfall rates, by automatic analysis of the content published in Social Media, and in particular Twitter, using Statistical Machine Learning methods. An important intermediate task regards the formation and identification of features which characterise a target event; we select and use those textual features in several linear, non-linear and hybrid inference approaches achieving a significantly good performance in terms of the applied loss function. By examining further this rich data set, we also propose methods for extracting various types of mood signals revealing how affective norms - at least within the social web's population - evolve during the day and how significant events emerging in the real world are influencing them. Lastly, we present some preliminary findings showing several spatiotemporal characteristics of this textual information as well as the potential of using it to tackle tasks such as the prediction of voting intentions.", "text": "declare work thesis carried accordance requirements university’s regulations code practice research degree programmes submitted academic award. vast amount textual streams inﬂuenced events phenomena emerging real world. social forms excellent modern paradigm unstructured user generated content published regular basis occasions freely distributed. present ph.d. thesis deals problem inferring information patterns general events emerging real life based contents textual stream. show possible extract valuable information social phenomena epidemic even rainfall rates automatic analysis content published social media particular twitter using statistical machine learning methods. important intermediate task regards formation identiﬁcation features characterise target event; select textual features several linear non-linear hybrid inference approaches achieving signiﬁcantly good performance terms applied loss function. examining rich data also propose methods extracting various types mood signals revealing affective norms least within social web’s population evolve signiﬁcant events emerging real world inﬂuencing them. lastly present preliminary ﬁndings showing several spatiotemporal characteristics textual information well potential using tackle tasks prediction voting intentions. grateful parents ioannis lampos soﬁa kousidou brother spyros lampos. every person world bound ‘family’ healthy limitless way. well ‘limitless’ might sometimes deﬁnition unhealthy diving properly primitive relationship need another ph.d. thesis surely ‘funding’ exists adventure. next course signiﬁcant importance measured however relatively different scale would like thank prof. nello cristianini ph.d. advisor. keeping good moments mind working him; really working. time university came across many interesting people porters merchant venturers building. thank polite positive every time building. might sound strange positive energy people default negative extremely important. would like thank omar syed rahman stefanos vatsikas honest helpful many occasions. also grateful christopher mims journalist wrote ﬁrst article work mit’s technology review. special ‘thank you’ philip naylor maintaining servers databases; person literally saved project. ph.d. project would possible epsrc grant scholarship cover tuition fees without nokia research’s ﬁnancial support. ground would also like thank computer science department university bristol head time prof. nishan canagarajah covering parts funding third year ph.d.; thank family covering rest. friends amongst important things also part ph.d. studies ‘have’ thank them. therefore charis fanourakis chloé vastesaeger costas mimis leonidas kapsokalivas nick mavrogiannakis takis tsiatsis thanos ntzoumanis names ‘book’. finally would like thank twitter people interacting indirect help making understand several aspects platform better course invaluable company endless nights spent writing wish ‘success’ life quests deﬁning general ﬁeld research regression linear least squares regularised regression least absolute shrinkage selection operator classiﬁcation bootstrap bagging feature extraction selection foundations information retrieval vector space model term frequency inverse document frequency text preprocessing stemming stop words summary chapter twitter pool information atom feeds collecting storing tweets software libraries processing textual numerical data ground truth summary chapter introduction important observation computing term-based score twitter corpus correlations twitter ﬂu-scores rates missing? missing approach? harry potter effect error bounds l-regularised regression affect approach summary chapter recap previous method’s limitations nowcasting events related work methodology forming vector space representations text streams feature selection inference consensus threshold soft-bolasso consensus threshold validation extracting candidate features forming feature classes comparing methodologies case study nowcasting rainfall rates twitter experimental settings results case study nowcasting rates twitter experimental settings results discussion applying different learning methods nowcasting rainfall rates cart nowcasting rates cart discussion properties target events summary chapter mean frequency mood scoring mean standardised frequency mood scoring experimental results testing stability extracted patterns circadian patterns applying mfms circadian patterns applying msfms correlations across scoring schemes mood types peak moments mood signals periodicity mood scores comparison positive negative affectivity discussion detecting daily mood patterns twitter content data methods experimental results discussion results summary chapter data content correlation geographical distance forming networks content similarity network stability analysing posting time patterns social media preliminary method inferring voting intentions twitter content extracting positive negative sentiment tweets inferring voting intentions experimental process results related work discussion results summary chapter arithmetic mean variance standard deviation z-score variance bias predictor cosine similarity moving average null hypothesis p-value statistical signiﬁcance principal component analysis urban centres region important observation markers detecting temporal mood patterns -grams wordnet affect inferring voting intentions twitter content search terms published tweets twitter account ﬂudetector look like. sample atom feed retrieved querying twitter volume cumulative volume plots tweets collected june rates health protection agency regions original weekly hpa’s rates expanded smoothed order match daily data stream twitter. twitter’s ﬂu-scores based choice markers regions smoothing -point moving average applied. comparison unweighted twitter’s score respective rates region using z-scores. linear correlation equal comparison inferred versus hpa’s rates using lasso method learning. figures present results regional experiment whereas shows results aggregated sets note subﬁgure ‘days’ axis denote yearly numbers. harry potter effect might exist signals emerging real-life events correlated target topic. abstract graphical representation describes generic hypothesis behind model. state world time interval affects occurrence turn event might affect entities including content observing subset content table word cloud font size proportional regression’s weight ﬂipped words negative weights feature class inference case study feature class inference case study feature class inference case study inference results continuous training validating testing sets south england testing performed data september october comparing smoothness ground truth case studies cart based data geolocated bristol using entire data inferring rainfall rates. ﬁgure shows small fraction tree derived pruning. numbers node denote comparison target node’s frequency. frequency word larger next node visit lies right else follow edge left side. plots represent variation -hour period emotional valence fear sadness anger obtained applying mfms. line represents days winter green represents days summer. average circadian pattern extracted aggregating seasonal data sets. faded colourings represent sample mean. displayed patterns corresponding stability p-values reported. plots represent variation -hour period emotional valence fear sadness anger obtained applying msfms. line represents days winter green represents days summer. average circadian pattern extracted aggregating seasonal data sets. faded colourings represent sample mean. displayed patterns corresponding stability p-values reported. plots represent variation -hour period emotional valence positive versus negative affectivity winter summer aggregated data applying msfms scheme. faded colourings represent cis. stability p-values extracted patterns average pairwise cosine similarity location pairs plotted respective geographical distance. denote points whereas black line smoothed equivalent july december geolocated percentage tweets posted hourly interval week days weekends clustering days applying multidimensional scaling based posting linear correlation coefﬁcients ﬂu-scores derived twitter content hpa’s rates based choice markers using -point moving average. average linear correlation coefﬁcients across considered regions ﬂuscores derived twitter content hpa’s rates smoothing functions various smoothing window sizes. p-value correlations .e-. linear regression using markers choice element denotes correlation coefﬁcient weighted ﬂu-scores time series hpa’s rates region training weights region p-value correlations average linear correlation coefﬁcients across considered regression scenarios inferred ﬂu-scores derived twitter content hpa’s rates smoothing functions various smoothing window sizes. p-value correlations .e-. stemmed markers extracted applying lasso regionally. markers sorted descending order based weights linear correlations test sets performing lasso element denotes average correlation coefﬁcient three remaining regions performing lasso region order learn weights validating shrinkage parameter region stemmed markers extracted applying lasso aggregated data regions a-e. markers sorted descending order based weights also occurred table order different. top- linearly correlated terms rate england wales top- selected terms lasso nowcasting rainfall rates derived consensus thresholds numbers selected features feature classes rounds -fold cross validation fold denotes validation/testing fold round nowcasting rainfall rates rmses feature classes locations rounds -fold cross validation fold denotes validation/testing fold round last column holds rmses baseline method. nowcasting rates derived consensus thresholds numbers selected features feature classes rounds -fold cross validation fold denotes validation/testing fold round nowcasting rates rmses feature classes locations rounds -fold cross validation fold denotes validation/testing fold round last column holds rmses baseline method. mfms msfms circadian pattern linear correlations across considered mood types winter summer aggregated data set. correlations asterisk statistically signiﬁcant. mae’s standard deviation mean thresholded sentiment performing leave-one-out cross validation. denotes minimum distance between positive negative sentiment score tweet order considered. hand-picked markers list stemmed words used detect anger terms list stemmed words used detect fear terms list stemmed words used detect terms list stemmed words used detect sadness terms terms conservative party terms terms labour party terms terms liberal democrat party terms application programming interface bootstrap aggregating bootstrap least absolute shrinkage selection operator classiﬁcation regression tree centers disease control cumulative density function conﬁdence interval consensus threshold dominant sentiment class general practitioner independent identically distributed inﬂuenza-like illness information retrieval impact score jaccard distance least angle regression least absolute shrinkage selection operator locally weighted scatterplot smoothing mean absolute error multidimensional scaling mean frequency mood scoring mean ranking error mean square error mean standardised frequency mood scoring mean thresholded sentiment negative affectivity natural language processing ordinary least squares positive affectivity principal component analysis probability density function part speech royal college general practitioners root mean square error residual squares standard error sentiment part speech sentiment part speech sentiment part speech wordnet’s core terms similarity score term frequency inverse document frequency vector space model vector space representation extensive markup language july google reported world wide formed least trillion unique pages approximately hundred times estimated number seemed obvious everybody proven numbers; world wide become immense. also known ‘the internet’ deﬁnitely important tools everyday life. numerous activities including education work socialising entertainment assisted transferred entirely ‘virtual’ online space handled numerous pieces hardware software. together internet’s evolution users interact evolving well. example past would considered abnormal behaviour give privacy socialising online medium nowadays becoming mainstream conducting life social networks twitter facebook apart boosting usage individuals also increased signiﬁcantly overall amount information stored linked web. fast expansion social still means large numbers people publish thoughts cost. current estimates number facebook users million twitter active users million. result massive stream digital text attracted attention marketers politicians social scientists analysing stream communications unmediated without relying questionnaires interviews many scientists direct access people’s opinions observations ﬁrst time. perhaps equally important access although indirectly situations ground affect users spreading infectious disease extreme weather conditions long mentioned messages published. reader might noticed references previous paragraph cite fairly recent work. started project september research groups desire explore rich amount information aforementioned pieces work also development phase. ﬁrst preliminary ﬁndings presented weather talk pointing conclusion even resources blogs frequently updated usually reﬂect real time events enclosed information right modelling could used make inferences moments real life; particular case making predictions weather state several locations based blog posts news articles. could argue information might unreliable always ofﬁcially veriﬁed badly structured great variety ways organising publishing content thus task automatic interpretation deﬁnition hard. considering also dealing large-scale amounts information interpretation becomes human-feasible process. project forming ways understanding explaining textual content lies web. understanding covers notion identifying properties text might turn express domains knowledge emerging real world. explaining deﬁnes functions position properties proper inferences relate events occurring domains. encapsulated weather talk title author’s m.sc. thesis written right beginning ph.d. project. work provided interesting insights developed methods described data used m.sc. project considered project presented thesis. research project applied overlapping scientiﬁc ﬁelds artiﬁcial intelligence machine learning data mining pattern recognition information retrieval. ﬁelds offer plethora well developed techniques; using state-of-the-start also achieve adaptations improvements methods algorithms frameworks relevant context work wherever needed. section made effort break main purpose project already expressed previous section subquestions tasks ending speciﬁc constrained list aims goals. useful information exist ‘open’ text streams? question might seem simple address every answer must followed proper justiﬁcation. simplistic hypothesis would course favour case large amounts textual information probability encountering subsets decreased uncertainty hence high informational value small. indeed several works including proved statement practice early stages ph.d. project. ﬁndings reshaped initial question following one. supposing information exist successfully extracted text stream? answer question essentially main point interest ph.d. project substantial part dedicated developing methods deriving conclusions conducting inferences processing text. methodologies formed combining extending models ﬁelds machine learning information retrieval natural language processing fact handling massive amounts data makes almost certain application techniques reduce dimensionality problem. ‘useful’ information might consider speciﬁc events general patterns arising population. performed deﬁning loss functions. case though necessary numerically quantify fraction real life; might consider event measured variety units even abstract notion public reaction emotion opinion real-time happenings. supervised learning framework existence ground truth essential learning parameters model also testing inferred learning function. hand abstract scenarios enclosed pattern discovery tasks usually seek conﬁrmation either proving statistical signiﬁcance querying ‘common sense’. formulate generic methodologies. project discover generalised methodologies solving speciﬁc problem applicable class problems. could achieved experimental process investigates case studies different characteristics. general method applicable class problems thus class must carefully deﬁned. improve various subtasks. depending success developing methods able address aforementioned questions aims also consider latter improved. methods multilayered since consider text processing also learning inference schemes. extraction selection textual features algorithms learning inference might play important role process hence improvements always part research. several parts work presented thesis attracted interest mainstream technology health news media. section refer signiﬁcant articles disseminated research general public provide reader well-shaped introductory information well perspective different academic one. twitter could better predict disease outbreaks technology review christopher mims link http//www.technologyreview.com/blog/ mimssbits// article featuring paper could social media used detect disease outbreaks? science daily. press release university bristol link http//www.sciencedaily. com/releases///.htm article featuring paper brits study forecasting outbreaks using twitter examiner linda chalmer zemel link http//goo.gl/auob article featuring paper diagnosing symptoms social media natural hazards observer vasileios lampos nello cristianini link http//www.colorado.edu/hazards/o/archives//mar_observerweb.pdf. section list relevant peer-reviewed publications chronological order coauthored vasileios lampos ph.d. project. deﬁned three levels contribution tagged listed publications according them written paper contributed ideas methods algorithms provided data detector tracking epidemics twitter. vasileios lampos tijl nello cristianini. european conference machine learning principles practice knowledge discovery databases barcelona spain. springer paper presented detector tool detecting rates several regions exploiting twitter content. method described paper improvement presented previous publication nowcasting events social statistical learning. vasileios lampos nello cristianini. transactions intelligent systems technology vol. paper present general methodology nowcasting occurrence magnitude event phenomenon exploring rich amount unstructured textual information social web. paper supports ﬁndings case studies inference rates several regions inference rainfall rates several locations effects recession public mood thomas lansdall-welfare vasileios lampos nello cristianini. mining social network dynamics session social media applications news entertainment lyon france february paper associate mood scores real-life events amongst interesting ﬁndings show exists evidence connects recession increased levels negative mood social media detecting temporal mood patterns analysis twitter content. vasileios lampos thomas lansdall-welfare ricardo araya nello cristianini. submission process british journal psychiatry paper analyse seasonal circadian affective mood patterns based twitter content introductory section chapter deﬁne theoretical background work based includes basic notions machine learning information retrieval; even fundamental notions usually considered common knowledge included appendix appendix contains tables input data used several experimental processes ours. chapter refer data collection methods used crawl social network twitter well provide reader characterisation social network. chapter describes ﬁrst steps work developed supervised learning method able infer rates using content twitter input information chapter builds previously presented methodology trying resolve limitations also prove generalised capacity applying another task inference rainfall rates content social media chapter aiming extract affective mood patterns social web. focus tasks extraction circadian daily mood patterns look ways able interpret chapter presents preliminary unpublished work regarding three pattern discovery tasks twitter content; ﬁrst investigates content-based relations among locations proposes method forming geo-temporal content similarity networks second analyses users’ posting time patterns third presents methodologies model infer voting intention polls chapter presents tools developed showcased work general public academic community detector mood nation. conclude thesis chapter referring also possible future work. chapter fundamental theoretical notions serve basis work. start deﬁning general ﬁeld research project machine learning. regression classiﬁcation common supervised learning tasks discussed short; draw interest regularised regression well classiﬁcation regression trees. bootstrap bagging methods enhancing learning process also presented discussed. move presenting basic notions methods ﬁeld information retrieval vector space model stemming. finally deﬁne feature extraction selection important tasks performed quite often work. learning remembering says socratis platonas makes inverse statement remembering learning. aristotelis expands notion learning claiming each form teaching mental learning begins knowledge already exists. learning therefore could conceived process acquiring knowledge characteristics speciﬁc skill fusing input information experience previous knowledge. greek philosophers probably making statements primarily referring humans animals nowadays notion learning could also refer machine computer program. several deﬁnitions given machine learning past years. arthur samuel broadly known pioneering work developing computer programs able play game checkers deﬁned branch artiﬁcial intelligence focused task making computers learn without explicitly programmed. mitchell pointed machine learning discipline addresses following general question build computer systems automatically improve experience fundamental laws govern learning processes? previous work already formalised main concept machine learning following statement computer program said learn experience respect class tasks performance measure performance tasks measured improves experience consequently contrast animals level learning machines quantiﬁed abstract manner; actually improving based performance metric serves main proof learning. several machine learning tasks tries identify exploit patterns data using algorithms primarily driven statistical approach. roots machine learning come general ﬁeld computer science whereas popular theoretical tools derive statistics therefore related even quite often also referred statistical learning past years three basic categories learning deﬁned supervised unsupervised reinforcement learning. also commonly known socrates pluto aristotle respectively. modiﬁed version deﬁnition given wikipedia. link http//en.wikipedia.org/wiki/learning. supervised learning scenario provided inputs corresponding targets responses main task learn function maps inputs targets unsupervised learning exist speciﬁc targets; main task could generally described trying discover pattern enclosed input data exploiting content. finally reinforcement learning aims learn interaction instance discovering actions yield higher rewards trying work apply derive methodologies mainly belong supervised learning class. also apply basic methods unsupervised learning generic statistical algorithms effort extract patterns data. next sections give short description theory methods used basis throughout project. regression supervised learning task unknown variable also known target variable real valued cases target variable ndimensional though loss important information break task one-dimensional independent regression scenarios. deﬁned coefﬁcients weights dimension global intercept bias denoted target variable linear combination observations elements mapping function. suppose n-dimensional observations held matrix j-th element i-th corresponding targets contents vector common learn mapping function known ordinary least squares regression trying minimise residual squares closed-form solution reveals obvious limitation regression singular thus difﬁcult invert resulting inconsistent solutions analysis interpretation indicates least squares estimates often bias large variance affects negatively prediction accuracy additionally input highdimensional generates large number predictors something complicates interpretation known regularisation parameter decides norm used. parameter interpreted coefﬁcient controls relative importance data dependent error regularised one. amount regularisation increased increases. literature regularisation also referred shrinkage since minimising also aims ‘shrink’ regression coefﬁcients common regularise bias term would remove main purpose provide prior knowledge target variable. high-dimensional linear regression model many variables might correlated. consequently large positive coefﬁcient variable could cancelled large negative coefﬁcient given correlated. shrinking regression’s coefﬁcients therefore imposing size constraint them restrict effect setting performing variable subset selection ﬁnding subset predictors gives smallest since penalty’s count number nonzero coefﬁcients. approach provides sparse solutions computationally feasible problems high dimensionality. still exist algorithms handle small number predictors hybrid approaches forward backward stepwise selection common disadvantage learning models discrete processes exhibit high variance quite often reduce prediction error full regression model. equation deﬁnes minimisation target ridge regression also referred l-norm regularisation given l-norm minimised similarly ridge regression closed form solution given observe positive constant added diagonal deal singularity problems original matrix process also known tikhonov regularisation however regularising l-norm encourage sparsity i.e. regression’s coefﬁcients nonzero therefore ridge regression offer easily interpretable model. hand i.e. regularising l-norm advantages regularisation maintained time sparse solutions retrieved. regularisation known least absolute shrinkage selection operator lasso original formulation lasso following optimisation problem previous cases controls amount regularisation. setting transforms lasso whereas using large completely shrink zero leading null model. moderate choice result cancellation variables initial model setting corresponding regression coefﬁcients zero. contrast ridge regression lasso closed form solution. computing solution lasso quadratic programming problem estimations solution derived within framework efﬁcient compute lasso achieved enforcing modiﬁcation least angle regression algorithm current residual predictor much correlation current residual. nonzero coefﬁcient becomes zero drop corresponding variable recompute current joint least squares direction. ﬁrstly sets regression coefﬁcients zero searches predictor highest correlation residual denotes mean explores direction predictor predictor much correlation current residual. proceeds direction equiangular predictors third variable enters active correlated predictors. continues along least angle direction fourth variable enters correlated lasso modiﬁcation proposes dropping variables corresponding coefﬁcients arrive zero nonzero previous value process lasso unique solution assuming exists pair predictors perfectly collinear recent work also shown lasso unique solution predictor variables drawn continuous probability distribution lasso shown sign-consistent meaning sign well zeros inferred consistent correlations relevant irrelevant variables however model-consistent many settings recover true model i.e. errors made assignment nonzero weights variables therefore inferred sparsity pattern differs true exist classiﬁcation supervised learning task model known classiﬁer constructed predict categorical labels. recall prediction target regression numeric value. classiﬁcation infer label class example category category categories represented discrete numerical values i.e. categories respectively using notation section target variable labels mapping labels different integers also take numeric representation aiming learn function denotes observations. work directly solve classiﬁcation problems; however using structures classiﬁcation regression trees primarily built solve problems later extended include regression problems well. decision tree structure classiﬁes instances observable data sorting tree using top-down approach starting root ﬁnishing leaf node. leaf nodes labels indicating result classiﬁcation instance. node decision tree targets speciﬁc attribute instance depending value attribute indicates descending branch decision making process follow figure shows example basic decision tree. supposing function able compute ﬂu-score social media content higher ﬂu-score bigger risk epidemic time ofﬁcial information health authorities exists actual rates population purpose decision tree ﬁgure whether warning spreading epidemic issued not. course simpliﬁed example help reader understand decision trees function. decision trees found application text mining computational linguistics failure diagnosis medical domain many scientiﬁc ﬁelds statistics artiﬁcial intelligence decision theory developed methodologies learning/constructing automatically decisions trees data past years several approaches developed decision tree learning chaid well-known descendant quest many more. next section focus brieﬂy approach classiﬁcation regression tree chosen work main advantage decision trees interpretability. size reasonable create self-explanatory models easily understood nonexperts. furthermore decision tree algorithms developed able handle discrete continuous variables well missing values perform classiﬁcation regression. addition decision trees non-parametric method means rely assumptions made probability distribution creates data however construction optimal decision trees np-complete problem therefore approximate result learning might always optimal. apart non-optimality decision trees also suffer instability; minor changes training might result construction different model section explain brieﬂy main operations classiﬁcation regression tree sophisticated program constructing trees data developed breiman presented addition original script section also based review papers cart cart non-parametric method capable solving classiﬁcation regression problems signiﬁcant coverage literature also great number applications many more). since well established method also exist off-the-shelf software packages implementing variations matlab figure shows simple example cart four terminal nodes. denotes entire space input variables; speciﬁc variables used decision making process similarly previous section’s decision tree example. given terminal nodes tree real-valued numbers tree used perform regression; similar tree could perform classiﬁcation terminal nodes class labels. cart constructed splitting subsets recursively splitting subsets forming splits subset cart constructs binary tree. considered limitation fact binary trees used represent type tree structures. tree construction process example divided subsets exists rule forcing divide subsets using variable. made clear observing figure left subset divided right also limitation using variable creating subsets different branches tree well branch time. core question construction cart best split subsets supposing response values training split subset left right sub-subset decided solving following optimisation problem denotes size regression tree squared errors parameter controls ratio size tree loss learning sample. similarly lasso seen regularisation parameter penalises overﬁtting. using independent validation loss function usually mean square error prune tree decide optimal ﬁnal version. bootstrap introduced method assessing accuracy prediction also found applications improving prediction suppose sample observations probability based well predictor function able calculate estimate actual response primary bootstrap estimate accurate prediction bootstrap sample random sample size drawn replacement notation denotes different original bootstrap data consists members appearing zero times appearing once appearing algorithm shows bootstrap used estimate standard error predictor primary applications. performance predictor. versions result carrying learning bootstrap replicates original learning set. predicting numerical value bagging averages versions predictor whereas prediction target categorical uses plurality vote decide outcome prediction main operations bagging also also described algorithm bagging improves unstable predictors signiﬁcant impact stable schemes; predictor unstable small changes data cause large changes predicted values bagging might also improve sufﬁciently large sample sizes increases squared bias reduces variance higher order terms leaving linear term unaffected properties reducing variance important effect hard decision problems e.g. estimation regression function testing regression classiﬁcation trees; bagging shown smooth hard decisions interestingly properties necessarily depend dimensionality problem feature extraction aims create features based transformations combinations original attributes. given feature space dimensionality feature extraction methods determine appropriate subspace dimensionality linear non-linear manner feature extraction also special form dimensionality reduction ﬁnds many applications areas bioinformatics text processing speech processing feature selection hand dimensionality reduction method aims select subset features dimensions effort overcome high dimensionality overﬁtting improve interpretability inferred model learning scenarios speed learning inference process. feature selection follow feature extraction; feature extraction used deﬁne original dimensions attributes feature selection speciﬁes ones important. basic form feature selection achieved correlation analysis. consider feature candidates vector corresponding target vector size primitive feature selection techniques measure correlation coefﬁcient deﬁned covariance divided product standard deviations ﬁrst moments respectively. lower upper bounds indicating perfect anti-correlation correlation respectively whereas indicates absence linear correlation. correlation coefﬁcient used rank candidate features separately; univariate method. important part work included optimal selection textual features representing target concept. recent research shown bootstrapped version lasso conventionally named bolasso intersecting supports bootstrap estimates addresses lasso’s model selection inconsistency problems applied modiﬁcation bolasso effort select consistent subset textual features. addition bagged version cart applied alternative nonlinear nonparametric method deals instability problems cart time able indirectly select subset important features text stream solve regression problem information retrieval scientiﬁc ﬁeld main goal provide methods retrieving useful information large collections unstructured material context automated process based well deﬁned algorithms performed computer programs. following sections basic notions used applied throughout work. corpus collection documents turn collections words n-grams convert information something structured receive queries algebraic representation needed. vector space model deﬁnes representation. m-sized documents formed n-sized n-grams vocabulary weight n-gram document vsms presented treat document n-grams; exploit possible relationships semantic connections n-grams consider separate independent random variables. reason usually referred words models. term frequency inverse document frequency document representation enables approximate actual importance word phrase document several documents similarly previously presented vsms tf-idf also words instead simply counting terms document assigning frequencies them calculates normalised frequency documents order deﬁne weights importance. normalised frequency depends maximum frequency term speciﬁc document. term document term frequency equal zero respective normalised frequency holds also tfij stage retrieved includes normalised term frequencies documents tfij tfnm}. calculate total number documents appears least once. denoted document frequency dfi. document frequencies represented {dfi dfn}. order retrieve inverse document frequencies following formula quite common principle preprocess text forming vector space model. well-known method text preprocessing known stemming process reduces word stem basis word. many variants word merged variable something helps reducing signiﬁcantly size vocabulary index therefore dimensionality text mining problem. several approaches stemming proposed popular using sufﬁx stripping sufﬁx stripping could deﬁned removal homogeneous calibration word’s sufﬁx. work applying approaches porter stemmer developed later made available stemming-speciﬁc language software package examples stemming using porter algorithm listed below ﬁrst group examples stemmer removes sufﬁx keeps core stem ‘research’. second series converts last character ‘happy’ therefore creates stem ‘happy’ ‘happiness’ keeps words ‘happier’ ‘happiest’ i.e. stemmer makes distinction comparative superlative forms adjective. hand since stemming automated method based rules might occasions generates signiﬁcant information loss. example ‘singular’ ‘singularity’ stemmed ‘singular’ ‘author’ ‘authority’ stemmed ‘author’. addition stemming desirable remove words particular semantic notion mostly used quite often. words commonly referred stop words examples articles e.g. ‘an’ ‘the’ propositions e.g. ‘about’ ‘after’ ‘on’ english text exist several off-the-shelf lists stop words proposed popular; particular list generated combining frequent english words manual rules encapsulated deterministic ﬁnite automaton tasks stop words automatically identiﬁed setting threshold maximum allowed frequency word similarly spam stop words decided setting threshold minimum frequency word work depending task hand either off-the-shelf stop word list thresholds minimum maximum frequency ‘legitimate’ word combination practices. main motivation behind chapter gave summary theoretical background behind research; appendix covers basic notions. core scientiﬁc ﬁeld work deﬁned artiﬁcial intelligence speciﬁcally branches machine learning. supervised learning formulates scenario given input data corresponding responses ﬁgure function mapping gathers main focus work apply regularised regression methods throughout lasso regularised regressor providing sparse solutions however shown inconsistent. carts formulate method able provide sparse regression solutions nonlinear nonparametric also unstable. bolasso bootstrap version lasso resolves model inconsistencies lasso; likewise bagging addresses instabilities carts. project also incorporated basic tools ﬁeld information retrieval. feature extraction approaches create attributes unstructured information whereas vector space models deﬁne ways giving attributes consistent algebraic form. processing textual information keeping stem instead entire word addition removing stop words helps reducing initial dimensionality problem; feature selection methods statistical approaches reduce original dimensions learning. mission chapter two-fold. first give short description social network platform used input data space research project also explain value information source. then refer methods tools implemented incorporated data collection processing routines. course project collecting data; however research would impossible without blog term derived synthesising words occasionally part website supposed updated content time time. blog’s content based posts identiﬁed primarily unique uniform resource locators posts vary random opinionated articles embedding songs videos multimedia types. microblogging compact form blogging seen ‘technological evolution’ blogs surely best complementary tools blogosphere. became popular services like tumblr twitter kicked off. basic characteristic microblogs authors restricted speciﬁc number characters post. experimental derivations work presented thesis collect twitter content. twitter created march number users reached million worldwide reported accounts ratio equal february registered users twitter allowed post messages commonly known tweets reach maximum characters. default account public private i.e. everybody tweets published account without authorisation owner. users also option follow users therefore tweets time-line main part twitter’s interface. public accounts becoming follower need type approval followee; private accounts able authorise followers. sense user public private account people follows people follow distinction connections public accounts seen one-sided relationships whereas connected private accounts form -sided hidden relationship. users mention reply tweets using character followed username user mentioned therefore conduct electronic conversations. also reproduce tweet another user action known retweet. deﬁnition taken wikipedia http//en.wikipedia.org/wiki/blog. tumblr founded http//tumblr.com. twitter launched http//twitter.com. twitter blog numbers http//blog.twitter.com///numbers.html think society organism another tool look inside says noah smith twitter section refer several works justify statement practice highlight importance twitter content. twitter’s social network comprised underlying sub-networks dense made followers followees sparser close probably real friends participate. latter inﬂuential expected former reveals great degree connectivity twitter space data three distinct categories users identiﬁed broadcasters acquaintances miscreants interestingly kwak show twitter deviates known human social networks non-power follower distribution short effective diameter reciprocity twitter users also highly non-uniform sample early work java shown people microblogging talk daily activities seek share information. additional derivation presented paper users similar intentions likely connected result environment conversationality also promoted. twitter means public interaction; ﬁnds collaborative scenarios well posting tweets conducting conversations twitter feeling connectedness sustained working environments several aspects educational schemes improved twitter encourages free-ﬂowing just-in-time interaction among students faculty ﬁnds great applicability conferences great tool promoting second language active learning methods majority topics twitter headline news persistent news nature methods developed enabling detection breaking news twitter also facilitates communication communities different political orientations. similarly real-life extremely limited connectivity leftright-leaning users reported furthermore twitter rich source opinion mining sentiment analysis consumer opinions public companies track adapt overall branding strategies based intelligence source polls easily conducted regarding consumer conﬁdence political opinion stations started combine broadcasts social networks improve interaction. particular twitter provide better understanding sentiment’s temporal dynamics reaction political debate predicting result elections based twitter content much harder problem solve still preliminary approaches already proposed topic fact microblogging usually expresses real-time state author tweets considered valuable media connecting information personal experiences situations users exploiting fact twitter’s deviating behaviour mass convergence emergency events build applications improve situational awareness events alternatively deploy methodologies turn tweets predictions signals emerging real life above naturally come conclusion high degree importance twitter content has. interesting one-sided nature relationships retweet mechanism allows rapid information spread ability conduct online conversations that open twitter making source information easy crawl offer opportunity study human behaviour early stages work twitter data work models. main reason forced create pipeline able collect store tweets. hindsight essential step. short section describe data collection process references external libraries tools used. really simple syndication simply feed family formats based extensive markup language used publishing distributing content standardised manner. recent development atom syndication format effort limitations supporting example namespaces. twitter uses atom feeds deliver content structured format; option submit query twitter’s retrieve atom feeds contain sets tweets matching query. example atom feed retrieved twitter shown figure main content actual tweet feed lies inside tags <entry> </entry>. purpose encapsulated sub-tags self explanatory example <published> holds posting date time tweet. recent past twitter data sets used academic studies become publicly available. however soon removed possibly open distribution content company’s policy. data collection basic crawler implemented java programming language; java preferred object-oriented programming frameworks mainly well-established programming interface many useful methods already made available third-parties. tweets collected querying twitter’s search api. based fact information geolocation concept studies considering geolocated tweets i.e. tweets author’s location known twitter. query able retrieve recently published tweets geolocated radius location latitude longitude written english language twitter’s response produce atom feed follows format shown figure collect parse content feed using java libraries rome store contents crawled mysql database. therefore request make retrieve entries stored database. above becomes apparent sampling strategy followed data collection. every minutes form search query locations retrieve latest tweets. value varied minutes project mostly dependent total number tweets aimed collect daily basis. obviously shortest collection period tweets collected; number twitter users reached high ﬁgures able reduce frequency still collect large amount tweets day. nevertheless kept urban centres therefore possible sampling biases type reduced. cities considered alphabetical order following aberdeen basildon belfast birmingham blackburn blackpool bolton bradford brighton bristol bournemouth cardiff coventry derby dundee edinburgh glasgow gloucester huddersﬁeld hull ipswich leeds leicester liverpool london luton manchester middlesbrough newcastle newport northampton norwich nottingham oldham oxford peterborough plymouth poole portsmouth preston reading rotherham shefﬁeld slough southampton southend stockport stoke sunderland swansea swindon watford wolverhampton york. information also available appendix tweets published unique users; latter number approximation track users might continued post twitter account changed username. within dates rough average number tweets user approx. tweets collected average. however good representation daily data collection ﬁgures number registered twitter users constantly increasing together published volume content. provide reader better picture plotted daily volume collected tweets figure well cumulative equivalent figure ﬁgures observe half-way considered period number collected tweets starts increase higher acceleration point start collecting million tweets day. reduce possible side-effects induced increasing volume tweets maintain equal representation twitter corpus different dates tend introduce normalisation factor vector space models usually equal number tweets considered several software libraries implemented applying basic methods algorithms driven ﬁelds data mining machine learning. common software package used projects deal analysis textual content apache lucene libraries initially programmed java able implement text search engine. lucene index tweets i.e. tokenise identify unique words textual stream also counting word frequencies documents optionally lucene also create indices stemmed text applying example porter’s algorithm based fact already java connector query database almost straightforward embed lucene’s libraries source code create several types indices. another useful library project apache software foundation mahout. mahout capable handling large scale data sets implements common machine learning algorithms clustering classiﬁcation linear algebra operations singular value decomposition. mahout methods compatible apache hadoop also using map-reduce paradigm therefore perform operations using cluster computers; cluster available mahout also operate using single node. mahout retrieve vector space representations directly lucene index exist already implemented interfaces purpose. comprised term tf-idf weights well neat options allow remove stop words rarely used words. occasions mahout also used simple operations computation cosine similarities pairs large vectors. given fact sometimes needed customised operations data least explicitly implemented lucene mahout also created software libraries used create text index perform stemming remove stop words compute vector space representations ﬁnally perform algorithms. methods matlab’s native toolboxes well pmtk also used work. ground truth necessary data work since many proposed methods based supervised learning. hence ground truth needed validation importantly learning parameters model training process. used three types ground truth apache hadoop http//hadoop.apache.org/. reduce http//en.wikipedia.org/wiki/mapreduce. probabilistic modelling toolkit http//code.google.com/p/pmtk/. health protection agency http//www.hpa.org.uk/. royal college general practitioners http//www.rcgp.org.uk/. qsurveillance university nottingham egton medical information systems http//www. chapter started deﬁning terms blog microblog. moved describing main operations characteristics twitter microblogging service serves main source information project. twitter attracted focus academic community last years allows researchers study human behaviour using massive amounts data. platform found also many applications opinion mining event detection education. next described detail twitter’s search order retrieve tweets geolocated urban centres also provided reader ﬁgures reporting average number collected tweets cumulative distribution. then referred important software tools used process text form vector space representations carry statistical operations. finally gave short description various versions ground truth used throughout project. tracking spread epidemic disease like seasonal pandemic inﬂuenza important task reduce impact help authorities plan response. particular early detection geolocation outbreak important aspects monitoring activity. various methods routinely employed monitoring counting consultation rates general practitioners. report monitoring methodology capable measuring prevalence disease population analysing contents social networking tools twitter. method based analysis hundreds thousands tweets searching symptom-related statements automatically identifying potentially illness-related terms turning statistical information ﬂuscore. tested weeks pandemic. compare ﬂu-score data health protection agency obtaining average statistically signiﬁcant linear correlation. preliminary method uses completely independent data commonly used purposes used close time intervals hence providing inexpensive timely information state epidemic. chapter extended version paper tracking pandemic monitoring social knowledge ﬁrst showing content social media used detect infer rate illness population. monitoring diffusion epidemic population important challenging task. information gathered general population provide valuable insight health authorities location timing intensity epidemic even alert authorities existence health threat. gathering information however difﬁcult well resource-demanding procedure. various methods used estimate actual number patients affected given illness school workforce absenteeism ﬁgures phone calls visits doctors hospitals methods include randomised telephone polls even sensor networks detect pathogens atmosphere sewage methodologies require investment infrastructure various drawbacks delay information aggregation processing times recent research pointed search engine data could detect geographic clusters heightened proportion health-related queries particularly case demonstrated timely reliable information diffusion illness obtained examining content search engine queries. work presented chapter extends concept monitoring content social-web tools twitter micro-blogging website users option updating status mobile phone device. updates commonly known tweets limited characters only similarly various schemes mobile text messaging. analyse stream data generated twitter extract score quantiﬁes diffusion various regions country. score generated applying machine learning technology compared ofﬁcial data statistically signiﬁcant linear correlation coefﬁcient greater average. advantage using twitter monitor diffusion reveal situation ground utilising stream data created within hours whereas releases data delay weeks. furthermore since source data entirely independent search engine query-logs standard approaches method also used combination them improve accuracy. early stages work important observation made. idea simple what used frequencies words twitter pick signals emerging real life? following sections describe ﬁrst experiment provided evidence existence signals social web. compile textual markers {mi} look twitter corpus day. textual marker single word words forming phrase markers point target topic chosen illness particular choice random; picking rates social important task could help early detection epidemics also ground truth provided ofﬁcial health authorities i.e. data could validate performance methodologies. daily tweets denoted {tj} marker appears tweet otherwise score tweet equal number markers contains divided total number markers used denotes total number tweets day. number tweets every thus necessary choice normalise score dividing daily number tweets order retrieve comparable time series scores. ground truth basis weekly reports related ﬁgures provides weekly regional statistics based rates gathered rcgp general qsur scheme. rcgp qsur metrics express number consultations citizens result diagnosis ili. experiment using rcgp data four regions namely central england wales south england north england england wales whereas qsur covers england wales northern ireland order retrieve equal representation weekly rates daily twitter ﬂu-scores expand point former -day period; fact weekly point original rate assigned every respective week. expanding rates perform smoothing -point moving average. figure shows expanded smoothed hpa’s rates regions weeks early peak time series reﬂects swine epidemic emerged june july using small textual markers expressing illness symptoms relevant terminology e.g. ‘fever’ ‘temperature’ ‘sore throat’ ‘infection’ ‘headache’ compute twitter ﬂu-score time series regions a–e. smooth time series -point moving average order express weekly tendency data. figure shows time series twitter’s ﬂu-scores days regions compute linear correlation coefﬁcients twitter’s hpa’s ﬂu-score time series. correlations respective p-values presented table average correlation equal largest correlation found region whereas smallest reported correlation region investigating linear correlations further different window spans moving average smoothing function well different approach known locally weighted scatterplot smoothing smoothed value produced lowess essentially given performing weighted regression across points smoothing window. table average linear correlation coefﬁcients across considered regions ﬂu-scores derived twitter content hpa’s rates smoothing functions various smoothing window sizes. p-value correlations .e-. results presented table increasing span smoothing window produces improved linear correlations smoothing functions. overall moving average smoothing yields better correlations lowess. figure plotted z-scores twitter score corresponding ofﬁcial rates region high correlation signals directly noticed without need statistical measure. ﬁrst important observation. following sections explain detail steps took order actually build observation. therefore daily ﬂu-score derived twitter content represented vector elements corresponding twitter’s ﬂu-subscore marker initially retrieve twitter corpus unweighted ﬂu-score vector unweighted time series term’s ﬂu-subscores days) smoothed -point moving average. perform regression time series smoothed version expanded smoothed hpa’s rates order learn weights terms training data correspond region test predictability inferred weights remaining four regions. perform training/testing method possible training choices. linear correlation coefﬁcient inferred ofﬁcial time series rates used performance indicator. results presented table correlation coefﬁcients retrieved training region presented respectively. average performance possible training testing choices equal p-values presented correlations indicate strong statistical signiﬁcance achieve maximum average performance tweets region used training; linear correlation ﬂu-scores’ time series rates region applying weights learnt region equal similarly previous section also report results various smoothing window spans well lowess smoothing function interestingly that smoothing methods correlations reach maximum window points smoothing time span increases especially lowess function performance shows decreasing behaviour. overall lowess perform better moving average; general smoothing functions produce results similar performance. based facts that linear correlations spans differ much using -point moving average produces interpretable result unveils weekly tendency data moving average performs slightly better lowess results presented following sections perform smoothing using -point moving average leaving investigation smoothing spans methodologies future work. assess predictive power former result differently perform linear regression aggregated time series ﬂu-scores hpa’s rates using data regions. data belonging weeks form test set; remaining data used training table linear regression using markers choice element denotes correlation coefﬁcient weighted ﬂu-scores time series hpa’s rates region training weights region p-value correlations table average linear correlation coefﬁcients across considered regression scenarios inferred ﬂu-scores derived twitter content hpa’s rates smoothing functions various smoothing window sizes. p-value correlations .e-. weights. results linear correlation p-value test set. additionally perform -fold cross validation using linear regression learning. average obtain linear correlation standard deviation equal cases score tested unseen data ﬁrst experiments trained data gathered region tested remaining regions period time; last experiments using aggregation data sets carried training testing different times. together sets experiments provide strong support predictive power ﬂu-score developed. choice; know optimal markers used especially deals vague concept? following sections show ways select features automatically enabling approach general independent human effort domain knowledge. previous sections made hand crafted related textual markers. section present method extracting weighted markers automatically. method selects subset keywords weights maximise correlation rates also minimising size keyword set. formed parts creating candidate features selecting informative ones. ﬁrst create pool candidate markers articles related inﬂuenza. encyclopedic reference well informal reference potential patients discuss personal experiences. preprocessing extract stemmed candidate markers latter denoted {mci} contains words form good description topic well many irrelevant ones. formation choice candidate markers discussed justiﬁed sections forming candidate features compute daily regional unweighted ﬂusubscores given denotes twitter corpus region {a-e}. score twitter represented vector consequently region period days form array time series ﬂu-subscores candidate features denotes total number days considered. columns i.e. time series ﬂu-subscores candidate feature smoothed using -point moving average resulting array denoted coefﬁcient column linear regression methods order rank learn weights candidate features. purpose lasso method chosen advantage producing sparse solutions i.e. discard candidate features proven redundant terms predictability lasso established method estimating least squares parameters subject penalty. considered constrained optimisation task case formulated denotes least squares estimates regression problem shrinkage percentage. time series region {a-e} training time series region {{a-e} validation deciding optimal shrinkage percentage test data remaining three regions. repeat procedure possible training choices. lars algorithm applied compute lasso’s estimates results method captured table possible training/validating choices lead high linear correlations. average linear correlation possible settings indicating robustness method. experiments showed optimal scenario train region region validating leading average correlation remaining three regions figures show comparison inferred hpa’s rates time series regions respectively weight vector non-zero values i.e. able extract markers which turn presented table majority markers pointing directly indirectly illness related vocabulary. also assess performance method differently following principle previous section. aggregate regional data sets before form test using data weeks validation present results regional experiment whereas shows results aggregated sets note subﬁgure ‘days’ axis denote yearly numbers. table linear correlations test sets performing lasso element denotes average correlation coefﬁcient three remaining regions performing lasso region order learn weights validating shrinkage parameter region table stemmed markers extracted applying lasso aggregated data regions a-e. markers sorted descending order based weights also occurred table order different. value value retrieve linear correlation test set. corresponding vector weights non-zero features shown table included previously extracted features. figure presents comparison inferred versus hpa’s rates test points. again demonstrated list markers automatically inferred large candidates using supervised learning algorithm hpa’s index target signal; approach delivers correlation greater target signal unseen data. showcased sparse learning method applied order automatically select ﬂu-related markers track levels several regions. experiment chosen smooth input data -point moving average express weekly tendency however applying method unseen data data cannot always smoothed. furthermore single words might favour overﬁtting; single words unrelated target topic larger probability correlate chance larger chunks text moreover lasso known disadvantages especially inconsistent selecting features. finally approach general? methodology work applied topics well? address issues well others next chapter. mainstream technique proposes formation index vocabulary using n-grams target corpus approach however form index several encyclopedic topic related references choice partly justiﬁed well next section. following basic principle weeks index -gram applying porter stemming removing stop words appear times twitter corpus england wales. retrieved index contains terms. time series -grams smoothed applying -point moving average; weekly ground truth expanded smoothed. ﬁrst order quick characterisation data compute linear correlation -gram ground truth. table shows top- correlated markers together correlation coefﬁcients exist related words ‘ﬂu’ ‘swine’ majority words unrelated topic seems derive well known harry potter fantasy novel/ﬁlm series. correlated word ‘latitud’ something ﬁrst sight might make much sense. knowing region england wales epidemic peaked july basing search correlated terms tracked events emerged date might topic conversation twitter. indeed harry potter movie aired july moreover -day music festivals kicked july annual festival ‘latitude’ featuring several popular artists annual international festival spain possibly popular among citizens interrupted large ﬁre. this conventionally named harry potter effect something expected happen signals emerging real-life events show similar behaviour compared target topic possibility events correlation according relevant article wikipedia latitude festival http//goo.gl/oqmf. according relevant article wikipedia festival internacional benicàssim http//goo.gl/ also applied lasso data knowing might select correlated terms different variables conforming constraints. permuting data based random selection day-index perform learning applying -fold cross validation scheme. performance selected features weights disjoint test excellent i.e. level section however observing selected features weights every iteration process proof case overﬁtting. table shows top- selected terms steps cross validation process. selected features slightly different every iteration something expected based training sets different another. again basic ﬂu-related features selected ‘ﬂu’ ‘swine’ well word ‘harri’. however terms seem describe events lottery graduation ceremony. minority features related target topic illness safe conclude signiﬁcant performance test result overﬁtting. bearing mind ‘harry potter effect’ chose form candidate markers focused described previous section. important notice statistical bounds exist linking lasso’s expected performance derived training number dimensions number training samples l-norm sparsity number nonzero elements equation makes clear error proportional dimensionality problem disproportional number samples achieved sparsity shown lasso’s expected loss poly-logarithmic factors bounded denotes empirical loss upper bound l-norm i.e. error bound sharp predictive. therefore minimise prediction error using ﬁxed training samples given empirical error relatively small either reduce dimensionality problem increase shrinkage level ˆw’s l-norm aforementioned error bounds well many others literature main conclusion similar dimensionality problem increases error increases well. relatively small i.e. number variables less number samples error rate reduced. which turn means number samples relatively small order achieve satisfactory inference performance constrain number dimensions; serves additional reasoning feature extraction modelling choice. finally sparser solutions result lower error rates reasons using lasso ﬁrst place. chapter presented preliminary method tracking epidemic using contents twitter; approach could give early warning various situations mostly give timely free information health agencies plan health care. method based textual analysis micro-blog contents calibrated ground truth provided could also extended contents text messages sent mobile devices latter format tweets ﬁrst formed keywords phrases represented illness symptoms. deﬁning basic function combined normalised frequencies manually selected terms come ﬂu-score discovered time series scores linearly correlated actual rates. adding layer approach applied regression learn weight term; expected retrieved correlations predictions ground truth greater original ones. next step included automatic selection features textual stream. achieve applied sparse regressor lasso able select small weighted subset initial feature space; selected features great majority drawn illness topic. automatic method improved inference performance further. however original candidate features formed using entirety twitter corpus something usually mainstream approach chosen form candidate features text related target topic wikipedia pages discussions health-oriented websites; nevertheless majority candidate features irrelevant illness topic. choice justiﬁed ﬁrstly presenting explaining ‘harry potter effect’ event detection secondly understanding error bounds lasso affected speciﬁc characteristics learning task. approach presented chapter automatically learn inputs scoring function correlates highly score. proposed method requires access time series geolocated blog contents ground truth. however knowing lasso inconsistent learning function terms model selection also presented method topic-speciﬁc next chapter focus improving several aspects well making extent generic. present generic methodology builds methodology presented chapter resolving limitations inferring occurrence magnitude event phenomenon exploring rich amount unstructured textual information social part web. geo-tagged user posts microblogging service twitter input data investigate case studies. ﬁrst consists benchmark problem actual levels rainfall given location time inferred content tweets. second real-life task infer regional rates effort detecting timely emerging epidemic disease. analysis builds statistical learning framework performs sparse learning bootstrapped version lasso select consistent subset textual features large amount candidates. case studies selected features indicate close semantic correlation target topics inference conducted regression signiﬁcant performance; performance method also compared another approach proposed address similar task showing signiﬁcant improvement. also apply different nonlinear non-parametric learner ensemble carts core approach investigate possible differences feature selection inference process. chapter extended version paper nowcasting events social statistical learning already described chapter preliminary method enabled detect ﬂu-like illness several regions observing twitter content. however core theoretical tool used approach sparse l-regulariser known lasso proven model-inconsistent learner many settings selects variables necessary taking closer look proposed method could also argue -grams could also lead choice features consequently overﬁtting. based simple notion entropy uncertainty english language decreases number characters increases considering experimental proof learning improvements achieved -grams chapter present hybrid scheme combines unigrams bigrams. furthermore previous experimental process focused short time span comparison methods proposed similar tasks. critical point smoothing input data something improved performance approach. however real-time applications inferences made running state hidden variable type smoothing might cause loss important information. lastly previous experiments focused single speciﬁc event therefore experimental proof existed allowing generalise ﬁndings. term ‘nowcasting’ commonly used economics well meteorology expresses fact making inferences regarding current magnitude event special case ‘current’ refer near future also recent past. event. using introduced notation want infer directly nowcasting ﬁrst learn conditional probability distribution observable information following bayesian logic infer prm)|s important assumption time interval enough time depict real-life situation content. relatively small value might true majority sites shown hold social networks twitter obviously even fundamental assumption made identiﬁable possibly causal link shape content real-life events exist; indeed already presented results support assumption previous chapter. recent work concentrated exploiting user generated content conducting several types inference. signiﬁcant subset papers examples given paragraph focuses methodologies based either manually selected textual features related latent event e.g. related keywords application sentiment mood analysis turn implies predeﬁned vocabularies words phrases mapped sentiment mood scores corley reported correlation ofﬁcial rates frequency certain hand-picked inﬂuenza related words blog posts whereas similar correlations shown user search queries included illness related words rates furthermore sentiment analysis applied effort extracting voting intentions box-ofﬁce revenues twitter content. similarly mood analysis combined linear regression model derived correlation daily changes jones industrial average closing values finally sakaki presented method exploited content time stamp location tweet detect existence earthquake however approaches feature selection performed automatically applying statistical learning methods. apart obvious advantage reducing human involvement minimum methods tend improved inference performance enabled explore entire feature space general greater amount candidate features google researchers proposed model able automatically select related user search queries later used process tracking rates. method core component google trends achieved average correlation data much higher previously reported method. extension approach applied twitter data achieving correlation rates works features selected based individual correlation rates; subset candidate features appearing independently highest linear correlations target values formed result feature selection. another technique part preliminary results presented previous chapter applied sparse regression twitter content automatic feature selection resulted greater correlation hpa’s rates several besides important differences regarding information extraction retrieval techniques data sets considered fundamental distinction lies feature selection principle sparse regressor lasso handle candidate feature independently searches subset features satisﬁes constraints candidate feature extraction. vocabulary candidate features formed using n-grams i.e. phrases tokens. also refer n-grams markers. markers extracted text expected contain topic-related words e.g. encyclopedias well informal references. construction extracted candidates contains many features relevant target topic much direct semantic connection. considering schemes features -grams -grams. then investigate ways combine them. deﬁne text stream superset documents. word stream denotes continuous documents given static behaviour. documents tweets status updates facebook blog posts well combinations aforementioned examples. already mentioned comprised sets words known n-grams indicates number words set. effort extract useful signals textual streams n-grams informative others depending always task hand. ﬁrst step towards achieving goal detect quantify signals spot informative n-grams least subset guarantees signiﬁcant performance inference. n-grams take part ‘selection’ process denoted candidate n-grams features markers. suppose extracted candidate features {ci} ...|c|}. user generated textual stream time interval denoted posts {pj} ...|p given limitation tweet’s length deal twitter content choose restrict function boolean special case setting maximum value compute score candidate marker user posts follows length time interval matter choice vary depending inference task hand. work frequently equal duration day. time intervals {uk} ...|u|} given compute scores candidate markers held array used input bolasso bootstrapped version lasso bootstrap lasso selects subset candidates bolasso intersecting bootstrap outcomes attempts make selection consistent. lasso formulated follows wols regression solution denotes desired shrinkage percentage wols’s l-norm. bolasso’s implementation applies lars able explore entire regularisation path cost matrix inversion decides optimal value regularisation parameter either using largest consistent region i.e. largest continuous range regularisation path selected variables remains same cross-validating explain bolasso detail section looking entire algorithm encapsulates method. strict application bolasso implies features zero weight bootstraps going considered. methodology soft version bolasso applied features considered acquire zero weight fraction bootstraps referred consensus threshold ranges obviously equal strict application bolasso. value expressed percentage decided using validation set. constrain computational complexity learning phase consider discrete step overall experimental process includes three steps training retrieve selected features bolasso weights regression validating select optimal value based validation testing performance previous choices computed. training validation testing sets deﬁnition disjoint other. loss function yi). decision comes naturally deﬁnition lasso essentially tries minimise squared loss plus regularisation factor. although squared loss tends penalise outliers loss functions investigated thoroughly relevant literature research. root mean square error used comprehensive metric presenting results next sections. summarise ct’s validation suppose considered consensus thresholds training yields sets selected features respectively whose losses validation denoted then denotes index selected features given taking consideration target values zero positive threshold negative inferred values zero testing i.e. max{xi perform ﬁltering testing phase; ct’s validation want keep track deviations negative space well. section algorithm describes core approach. algorithm takes input candidate features training validation sets observations ground truth consensus thresholds going explored number bootstraps optional numeric parameter maxfeaturesnum causes early stop every bootstrap lasso another numeric parameter percentlcr explained later observations assumed standardised responses centred. outputs algorithm selected candidate features optimal consensus threshold. first formed performing lars entire training basis re-applying lars random samples training set. every bootstrap training sampled uniformly replacement used input lars. executing bootstraps weights bootstrap value identifying nonzero weights bootstraps selected features pair cti. next step decide optimal value cti. methods task either largest consistent region regularisation path selected values change less percentlcr% speciﬁed -fold cross-validation. experiments percentlcr general preferable choice creation consistent region selected features main bolasso. hence region exists evidence bolasso’s solution consistent characteristics therefore shall used finally computing optimal value therefore retrieving corresponding optimal selected features validation used deciding optimal therefore selected features. three classes candidate features investigated unigrams -grams bigrams -grams hybrid combination -grams -grams. grams single words cannot characterised consistent semantic interpretation topics. take different meanings express distinct outcomes based surrounding textual context. -grams hand focused semantically. however frequency corpus expected lower -grams. particularly twitter corpus consists short pieces text daily frequency sometimes close zero. hybrid class features exploits advantages classes reduces impact disadvantages. formed combining training results cts. tried approaches forming hybrid features described algorithms denoted respectively. suppose considered consensus thresholds ...|ct|} -grams -grams selected bolasso denoted respectively. then ﬁrst hybrid approach pseudo-selected n-grams hybrid class second hybrid approach pseudo-selected n-grams formed exploring possible |ct| pairs selected -grams -grams. thus case ...|ct|} hybrid combination method proceeds manner previous one. note compiling optimal hybrid scheme main focus here; investigate whether simple combination -grams -grams able deliver better results. experimental results following sections indeed indicate feature class performs average better something always hold hii. uniﬁed data set. method conventionally named expected perform worse hybrid combinations fact increases dimensionality problem hand without providing training points. indeed next sections ﬁrst hybrid approach performs better class features. part evaluation process compare results baseline approach encapsulates methodologies approaches explained section mainly differ feature selection process performed correlation analysis; algorithm describes process detail. brieﬂy given candidate features computed vsrs training validation corresponding response values feature selection process computes pearson correlation coefﬁcients candidate feature response values training ranks retrieved correlation coefﬁcients descending order computes ols-ﬁt loss incremental subsets top-k correlated terms validation selects subset candidate features minimum loss. inference performance selected features evaluated test set. ﬁrst case study exploit content twitter infer daily rainfall rates cities namely bristol london middlesbrough reading stoke-on-trent. choice locations based availability ground truth i.e. daily rainfall measurements weather stations installed vicinity. consider inference precipitation levels given time place good benchmark problem many properties useful scenarios still allows verify performance system since rainfall measurable variable. event rain piece information available signiﬁcant majority twitter users affects various activities could form discussion topic tweets. furthermore predictions always easy smooth sometimes unpredictable behaviour candidate markers case study extracted weather related references wikipedia’s page rainfall english language course weather vocabulary page formal weather terminology several others. majority extracted candidate features directly related target topic exists subset markers could probably offer good semantic interpretation. markers count twitter corpus used case study removed. hence extracted -grams kept candidates; likewise extracted -grams reduced year twitter data rainfall observations formed input data experiment. time period considered locations million tweets collected. bolasso number bootstraps proportional size training sample every bootstrap select features performing iterations. bootstrap completed soon stopping criteria met. essential trade-off guarantees quicker execution learning phase especially dealing large amounts data. performance feature class computed applying -fold cross validation. fold based months data starting month pair julyaugust ending may-june every step cross validation folds used training ﬁrst half remaining fold validating second half testing performance selected markers table nowcasting rainfall rates derived consensus thresholds numbers selected features feature classes rounds -fold cross validation fold denotes validation/testing fold round weights. training performed using vsrs locations batch data ct’s validation carried principle ﬁnally testing done batch data well location separately. finally also compute inference performance baseline approach feature selection training validation testing sets considering correlated terms. derived well numbers selected features rounds -fold cross validation presented table rounds values high result number selected features relatively small. turn could mean occasions markers able capture rainfall rates signal. last round validation data based july exception derive lower feature classes well larger numbers selected features. interpreted fact july rainy month data also summer month hence tweets rain could followed preceded tweets discussing sunny creating instabilities validation process. addition data restricted year weather observations therefore seasonal patterns like expected entirely captured. detailed performance evaluation results presented table better interpretation numerical values consider average rainfall rate data equal standard deviation range method outperforms baseline approach all-but-one intermediate rmse indications well total feature classes table nowcasting rainfall rates rmses feature classes locations rounds -fold cross validation fold denotes validation/testing fold round last column holds rmses baseline method. achieving improvement overall performance method indicates feature class performs better others interestingly unigrams outperform bigrams well remaining hybrid feature classes. presenting intermediate results round cross validation would intractable. remaining part section present results learning testing cross-validation’s round only month testing october tables list selected features alphabetical order together weights feature classes respectively. class also compiled word cloud selected features comprehensive representation selection outcome majority selected -grams close semantic connection underlying topic stem ‘puddl’ holds largest weight whereas stem ‘sunni’ taken negative weight interestingly word ‘rain’ relatively small weight. also exist words without direct semantic connection majority negative weights might acting mitigators weather related uses remaining rainy weather oriented positively weighted features. selected -grams clearer semantic connection topic; ‘pour rain’ acquires highest weight. particular case features class formed exact union ones classes take different weights inference results location cross validation’s round presented figures feature classes respectively. overall inferences follow pattern actual rain; feature class inferences occasions appear positive lower bound actually positive bias term regression appearing selected markers zero frequencies daily twitter corpus location. mentioned before problem resolved since unlikely -grams also zero frequency. results class depicted figures demonstrate good target signal. second case study content twitter infer regional rates base inferences three regions namely central england wales north england south england. ground truth i.e. ofﬁcial rate measurements derived hpa. hpa’s weekly reports based information collected rcgp express number consultations citizens result diagnosis ili. according rate less equal considered baseline rates normal average characterised exceptional. create daily representation hpa’s weekly reports linear interpolation weekly rates. given rates consecutive weeks compute step factor candidate features extracted several references sites national health service wikipedia following general principle i.e. including encyclopedic scientiﬁc informal input. similarly previous case study extracted -grams reduced extracted -grams here removed n-grams count since number tweets involved approximately times larger compared rainfall case study. case study enough peaks present ground truth time series collected twitter data cover period average rates performance evaluation results training mostly non-ﬂu periods strong signal hence feature selection conditions optimal. overcome assess properly proposed methodology perform random permutation data points based index. randomly permuted result south england’s rate shown figure apply randomised index regions performance evaluation. table nowcasting rates derived consensus thresholds numbers selected features feature classes rounds -fold cross validation fold denotes validation/testing fold round experiment considered tweets ground truth time period june april total number tweets used reaches approximately million. similarly previous case study applying -fold cross validation using data days fold. round crossvalidation folds used training. remaining fold days data used validating rest testing. bolasso settings identical ones used rainfall case. notice following experiments data points contiguous terms time since permuted randomly based index however included example next section contiguous training validating testing data points used. derived numbers selected features rounds -fold cross validation presented table case study value closer lower bound validation average features selected. either existence signiﬁcant period ground truth data general inadequacy grams describe underlying topic effectively previous case study. table holds performance results rounds cross validation. comprehensive interpretation numerical values consider average rate across regions used experiments equal standard deviation table nowcasting rates rmses feature classes locations rounds -fold cross validation fold denotes validation/testing fold round last column holds rmses baseline method. ranges again feature class performs better whereas outperforms others. however case study feature class second best terms performance. similarly previous case study method improves performance baseline approach factor case study present intermediate results cross validation’s round tables show selected features feature classes respectively. selected -grams stem ‘irrig’ largest weight. many illness related markers selected ‘cough’ ‘health’ ‘medic’ ‘nurs’ ‘throat’ exist also words clear semantic relation. surprisingly stem ‘ﬂu’ selected feature round contrary almost selected -grams considered related ‘conﬁrm swine’ largest weight feature classes suspect swine symptom swine symptom physic emotion healthcar worker underli health viru epidem visit doctor weight loss wikipedia woke sweat wonder swine regional inference results presented figures classes respectively. clear indication inferred signal strong correlation actual one; e.g. feature class linear correlation coefﬁcients inferred actual rate central england wales north england south england equal respectively. using folds cross validation average linear correlation classes equal finally present additional experimental results training validating testing carried contiguous time-wise manner. days data used days validating testing remaining days used training. formation setting train data swine epidemic period test period inﬂuenza existed rate within normal range. figure show inference outcome south england three distinctive feature classes also included smoothed representation inferences induce weekly trend. class best performance; example class performs better experimental results provided practical proof effectiveness method case studies rainfall rates inference. rain observable pieces information available general public therefore expected parts discussions social media. samples rainfall rates described exponentially distributed random variables phenomena distinctive property. precipitation especially rather unstable i.e. prone daily changes whereas rate evolves much smoothly. figures provide clear picture this. consequently rainfall rates inference much harder problem; users discussing weather preceding forecast next especially current weather conditions contradict affect inference process learning well. example round cross validation derived worst inference performance vsrs test -gram ‘ﬂood’ exact average frequency rainy rainy days furthermore average frequency stem ‘rain’ days rain equal rainy days. similar statistics also observed training -grams e.g. average frequencies ‘rain hard’ ‘pour rain’ training proposed method able overcome tendencies selecting features stable behaviour extent possible. however ﬁgures previous sections make clear inferences higher correlation ground truth case study even deploying randomly permuted version data turn encapsulates major period therefore worse quality compared rainfall data. based experimental results properties target events reach several extremes argue proposed method applicable another important point methodology regards feature extraction approach. mainstream technique implies formation vocabulary index entire corpus instead chosen form focused restricted numbers candidate features online references related target event choice justiﬁed lasso’s risk bound ‘harry potter effect’ short time span data limits amount training samples therefore directs choice reducing number candidate features minimise risk error avoid overﬁtting. fewer slightly focused target event’s domain candidates constrain dimensionality training samples ratio issues resolved. nevertheless size -gram vocabularies case studies small daily tweets location region contained least candidate feature. however -grams proportion reduced rainfall rates case studies respectively meaning class features required much higher number tweets order properly contribute. experimental process made also clear manual selection obvious keywords logically describe topic ‘ﬂu’ ‘rain’ might optimal especially using -grams; rare words exhibited stable indications target events’ magnitude. finally important note operates additional layer feature selection process facilitating adaptation special characteristics data set. ct’s validation showed blind application strict bolasso would performed good relaxed version applied; times validation sets used experiments optimal value equal previous sections presented methodology used soft-bolasso validation nowcast events emerging real life using twitter content. approach linear since predictors corresponding weights order make inference compute inner product observed values weights. words model makes initial assumption relationship observations response targets linear parameters functional form chosen moreover lasso linear regression techniques based assumption either observations independent responses conditionally independent given observations section apply class learning functions problem. motivation behind two-fold ﬁrstly want acquire comparison metric based different theoretical tools secondly investigate whether problem affected nonlinearities data. thus task chosen cart decision tree method nonlinear nonparametric i.e. make assumption probability distribution data. total applied three cart methods basic cart pruned version ﬁnally ensemble ‘bagged’ carts. attention series experiments data section i.e. tweets candidate features locations rainfall observations order able compare performance different learners. performance method computed applying -fold cross validation; process identical used measuring performance soft-bolasso validation. fold based figure cart based data geolocated bristol using entire data inferring rainfall rates. ﬁgure shows small fraction tree derived pruning. numbers node denote comparison target node’s frequency. frequency word larger next node visit lies right else follow edge left side. months data starting month pair july-august ending mayjune every step cross validation folds used training ﬁrst half remaining fold either deciding optimal pruning level optimal number trees second half testing performance derived learning function. total three feature classes considered -grams -grams concatenation grams -grams. experiments lower threshold percentage tree-levels present pruning. equal i.e. pruned cart must retain least levels full tree. provide reader combining derivations bootstraps form ensemble carts. bootstrap training sampled uniformly replacement; number trees going used ensemble decided using validation set. ensemble makes inference averaging across predictions trees participating inference performance methods feature class terms rmse presented tables pruning improves performance cart cases expected cart full pruned cannot compete elaborate bootstrap methods. ensemble carts lower rmse bolasso -grams -grams feature classes combined cannot improve bolasso; case error equal close bolasso’s folds optimal number trees used ensemble kept also performance ensemble carts improves features. feature selection ensemble trees performed ﬁrst computing importance factor variable every subtree averaging trees ensemble importance variable tree given yi’s denote observations node denotes mean number observations node expresses decrease enclosed summation move tree. compute overall importance variable average equation trees ensemble tables show important features based round -fold cross validation investigated feature classes based ensemble carts. results directly comparable ones tables respectively; features selected bolasso typed bold. feature classes distinctively important features related topic weather; n-grams ‘rain’ ‘rain rain’ shown largest however features identical ones selected bolasso occasions series experiments used data section i.e. tweets candidate features regions ofﬁcial rates order able compare performance different learners. performance method computed applying -fold cross validation; folds using data days used training remaining fold days data used validation inference performance methods feature class terms rmse presented tables again pruning improved performance cart cases enough competing ensemble carts bolasso. bolasso performs better ensemble carts case -grams best overall performance derived ensemble combining -grams tables show important features based round -fold cross validation investigated feature classes ensemble carts applied. results directly comparable ones tables respectively; similarly previous section features also selected bolasso typed bold. again markers selected ensemble carts differences compared selected bolasso; still selected features mostly related topic illness. important words directly related swine epidemic addition -gram ‘irrig’ highest weight bolasso applied within important features ensemble. overall seen subcases ensemble carts outperforms bolasso. still rainfall rates inference task performance learners quite similar best achieved bolasso’s hybrid class; rates inference problem ensemble carts achieves signiﬁcantly better performance bolasso. terms feature selection conclude learning functions select sets features; features selected learners weighting and/or ranking might also different high probability. semantic correlation selected features target topic concerned bolasso tends select ‘reliable’ features. particularly task rainfall rates inference even hybrid class ensemble carts selected irrelevant words ‘sentenc’ ‘archiv’ ‘blind’ ‘manag’ might reason bolasso outperformed case. nevertheless selected -grams learning methods seem closer semantic correlation topics reconﬁrming initial assumptions based fact ensemble carts gives importance ranking large amount features hence feature importance threshold must deﬁned manually might argue bolasso offers preferable perform feature selection. hand ensemble seems outperform bolasso inference process could perhaps assume problem nonlinear characteristics cannot incorporated linear learning function. another statistical fact might also affect inference performance schemes underlying distribution ground truth data; next section examining aspect show rainfall rates might generated different probability density functions finally general limitation inference methods used proposed models produce single inferred values without associated spread. spreads inferences usually indicated conﬁdence intervals assist identifying locations regions models operate high conﬁdence vice versa. approach tackling task could approximate inferences bootstrap samples ensemble carts case equal standard deviation inferred values cart divided square root number carts ensemble. computed multiplying quantile normal distribution. figure shows inferences error bars round -fold cross validation case study. majority samples tend quite tight; also observe across three considered regions behave similarly daily time intervals however possibly better address task could model based gaussian processes. gaussian process seen collection random variables hypothesised ﬁnite number random variables described joint gaussian distribution; completely deﬁned mean covariance function within advantages framework lies natural acquiring spreads inferences regression problems also ability blending various parameters information inputs model quite structured manner therefore temporal well spatial characteristics event detection process could also explored resulting probably better deﬁnitely general solutions. chapter proposed methodology capable using twitter content nowcast rainfall rates purpose section identify properties target events. assume proposed theoretical framework least also applicable inferring magnitude events similar characteristics. section already given initial insight main similarities differences random variables rainfall rates; shown figure holds plots entire sets ground truth points rainfall rates. data concatenated scores different locations regions chronological order. rainfall’s mean equal standard deviation minimum maximum values respectively median equal likewise mean rate standard deviation minimum maximum respectively median equal figure reconﬁrms argument previous paragraph; rainfall rates general much unstable rates many times equal whereas rates never become data set. figure shows empirical cumulative density functions random variables. point corresponding value plot y-axis denotes ratio observations smaller total number observations. figure made quite obvious approx. rainfall rates equal likewise figure sample lies therefore events seem inactive majority time tend acquire notable values short periods only. finally plot histograms variables commonly applied pdfs histogram derived rainfall rates points data likely follow exponential distribution; best occurred exponential however rates based fact zero small values dominant class show best exponential density function. figure shows histogram well three pdfs exponential log-normal gamma log-normal offers best data meaning logarithm samples approximated normal distribution. summarising results above seen method able track events approximated exponential log-normal distribution. common ground events tend oscillate different frequencies though occasions metric schemes tend assigned small values. therefore targets events inactive considered inactive majority time become active precisely take signiﬁcant values shorter periods time. number distinguishable peaks increases expect event-tracking process feasible also comparing semantic correlation target topic selected features rainfall peaks present ones could argue favour hypothesis. chapter presented general methodology inferring occurrence magnitude event phenomenon exploring rich amount unstructured textual information social part particular twitter; improve several aspects methodology chapter speciﬁc procedures followed namely soft-bolasso consensus threshold validation feature selection large candidates ensemble carts proven work signiﬁcantly better compared another relevant state-of-the-art approach general claim statistical learning techniques deployed selection features time inference useful statistical estimator. several feature classes applied -grams -grams hybrid combinations them; combination grams -grams gave best inference performance whereas selected -grams formed better semantic representation target event. overall inference performance ensemble carts nonlinear learning function better derived bolasso; however feature selection alone naturally performed bolasso. comparisons even variants machine learning methods surely interest though would change main message learn estimator data means supervised learning. case methods propose simply count frequency disease name. work well people diagnose disease confounding factors exist. however experimental results conclude optimal choice. furthermore obvious function twitter content correlate actual health state population rainfall rates. various possible sampling biases prevent signal emerging. important result study possible make bias calibrating estimator large data twitter posts actual readings precipitation measurements. true twitter users represent general population twitter content might represent particular state theirs actual states general population inferred linear nonlinear function signal twitter. lastly insight characteristics target events given. rates better described log-normal whereas rainfall rates show exponential behaviour. hypothesise methods generic therefore able nowcast events similar characteristics. chapter look methods able extract affective norms content twitter. particular focusing four mood types namely anger fear sadness compute seasonal well average circadian daily mood scores. circadian mood patterns used explain behavioural patterns throughout day; seasonal characteristics also identiﬁed comparing patterns winter summer. also show daily mood patterns correlated signiﬁcant events annual celebrations emerging real-life. chapter partially based content ideas papers detecting temporal mood patterns analysis twitter content effects recession public mood social media particularly micro-blogging website ‘twitter’ provide novel gather real time data large quantities directly users. data also time-stamped geolocated analysed various ways examine patterns wide range subjects. several methods already proposed exploiting rich information order detect events emerging population track possible epidemic disease even infer result election mood seems change throughout spontaneously response life’s vicissitudes. nonetheless exploring variations real time large samples difﬁcult. information gathered twitter messaging permits overcoming obstacles. better knowledge mood variations important conﬁrm widely accepted notions linking certain psychiatric phenomena diurnal even seasonal patterns recent studies using information captured twitter messaging found circadian seasonal patterns content emotionally loaded wording. circadian variation volume words representing negative affect reached lowest point mornings improved steadily throughout day. ﬁnding contrary clinical concept diurnal variation mood among depressed patients highest load depressive symptoms early mornings. seasonal patterns particular increase depressive symptoms winter months conﬁrmed either however study showed seasonal changes positive affectivity speculated winter blues associated diminished rather increased negative affectivity assessed approximately million tweets million winter summer periods respectively. populated urban centres periodically retrieved recent tweets geolocated within range urban centre. attempted reduce potential sampling bias applying daily sampling frequency urban centre. data collected weeks season used stemmed version text tweets applying porter’s algorithm tweets divided bins hour emotional valence hour assessed text analysis tool. estimated level activity four emotions namely fear sadness anger based contents twitter messages emotion-related words text. emotion-related word-lists retrieved priori wordnet affect tool builds wordnet selecting labelling synsets represent affective concepts. word-lists processed stemming ﬁltering single words kept end. preprocessing formed stemmed words respectively wornet affect well stemming regularly applied emotiondetection text mining applications thus considered standard ways type information retrieval extract circadian pattern considered mood types approaches based different assumptions applied. suppose terms used track emotion hourly time intervals total number days considered time interval denoted collected number tweets time interval denoted every count appearances term twitter corpus ‘hourly’ term counts normalised divided respective number tweets collected time interval therefore instead working counts method based assumption frequency word indicates importance. words higher frequencies larger impact computed means therefore ﬁnal mood score. refer approach mean frequency mood scoring second mood scoring approach computing hourly term frequencies form vectors size hold frequencies word time intervals i.e. next standardise wi’s ﬁrst subtracting mean entry dividing standard deviation type computed circadian pattern winter summer well aggregated data set. ﬁgures also included using faded colour. approximated multiplying sample mean quantile normal distribution assumed represent mood scores time interval across considered dates. table shows linear correlation coefﬁcients inferred circadian patterns between winter summer scoring schemes. correlated emotion metrics correlations higher high positive correlation remaining emotions well pointing overall mood trend change signiﬁcantly seasons might speciﬁc deviations smaller impact pointed discussed following sections. statistical signiﬁcance test performed inferred average circadian rhythms testing stability across daily samples. consider emotion type average circadian mood pattern based relevant data denoted vector every data compute circadian mood pattern then compute di’s linear correlation form random permutations compute linear correlation count number times pseudo-random correlation higher equal p-value statistical test therefore given based statistical signiﬁcance test circadian patterns derived applying mfms anger fear sadness slightly threshold hence might stable representations; patterns derived applying msfms statistically signiﬁcant. proof instability strong considering fact scoring schemes making different assumptions weighting mood terms present extracted signals section describe circadian rhythms based mfms depicted figure fear ﬁrst peak morning drops p.m. starts increasing reaching maximum –p.m.; seasonal patterns highly correlated summer rates slightly higher hourly intervals sadness minimum –a.m. intermediate peak –a.m. steadily increases reaching maximum midnight; noticeable difference seasons winter’s morning peak happens hour earlier summer’s much stronger. peaks morning decreasing afternoon increases –a.m. starts drop reaching minimum –a.m.; circadian pattern similar among seasons winter slightly increased levels joy. anger reaches minimum interval –a.m. increases midnight peaks; summer observe increased rates late night winter anger rates higher mornings afternoon msfms produces circadian patterns noticeable differences compared previously presented ones. figure fear ﬁrst peak –a.m. drops p.m. starts increasing reaching maximum midnight; observe higher levels fear early mornings daytime winter something reversed summer p.m. a.m. sadness peaks mornings decreases p.m. becomes steady. midnight sadness score decreasing reaching minimum –a.m.; winter higher early morning hours noon whereas p.m. midnight higher rates summer. also peaks morning decreases p.m. slightly starts increase again reaching signiﬁcantly smaller peak midnight; minimum scores figure plots represent variation -hour period emotional valence fear sadness anger obtained applying mfms. line represents days winter green represents days summer. average circadian pattern extracted aggregating seasonal data sets. faded colourings represent sample mean. displayed patterns corresponding stability p-values reported. figure plots represent variation -hour period emotional valence fear sadness anger obtained applying msfms. line represents days winter green represents days summer. average circadian pattern extracted aggregating seasonal data sets. faded colourings represent sample mean. displayed patterns corresponding stability p-values reported. happen a.m. levels night hours midnight tend higher summer however midnight ‘winter tweets’ seem joyful. anger minimum –a.m. ﬁrst peak –a.m. second peak –p.m. decreases p.m. starts increasing reaches maximum midnight; winter core daytime higher levels anger whereas summer anger higher p.m. late night hours. table mfms msfms circadian pattern linear correlations across considered mood types winter summer aggregated data set. correlations asterisk statistically signiﬁcant. table shows linear correlations circadian patterns derived mfms msfms winter summer aggregated data set. circadian patterns positively correlated across different scorings schemes; results drawn seasonal patterns however statistically signiﬁcant. fear show highest correlations thus expect derive similar patterns independently applied scoring approach something fact displayed figures table shows linear correlation coefﬁcients circadian patterns mood pairs scoring schemes. mood pairs containing emotion average lowest correlations. sadness fear correlated mood types; also exists correlation anger fear sadness. clearly uncorrelated emotions anger. section investigate peak moments diurnal pattern extracted applying msfms scheme. patterns presented previous sections show average behaviour mood type. emotions fear anger clearly distinctive peak average patterns result stability importance several occurring peaks unclear. extracting peak moments daily mood signals data forming histogram observe distribution peak moments across days; depicted figure black coloured rectangles hourly interval. since difference highest mood score second-highest value might small samples also visualised frequency second-highest mood scores peak frequency. retrieves highest values mornings especially a.m. sadness similar peaking behaviour; usually peaks a.m. emotion fear peaking time periods; occurs late evenings midnights mornings anger unstable behaviour amongst investigated emotional types. still derive rarely peaks evenings reaches high levels quite often midnight late night hours right lunch time additional step experimental process compute autocorrelation ﬁgures four mood types based msfms scheme using lags ranging hours i.e. total number hours week. consecutive time intervals highly correlated emotional types; common logic something expected high probability mood population time instance affected mood previous time instance autocorrelation plots also becomes evident emotional type exists level daily well weekly periodicity autocorrelations lower observed indicating weekly pattern mood signal stronger intermediate ones. test statistical signiﬁcance observed levels periodicity using autocorrelation test statistic. suppose vector length holds scores mood type. compute autocorrelations compute autocorrelations randomly permuted versions count many times levels autocorrelation randomised signal greater equal ones computed p-value test therefore equal tests mood types considered lags computed p-values equal indicating observed levels periodicity statistically signiﬁcant. overall emotion highest levels autocorrelation shows strong periodic behaviour whereas periodicity seems less strong mood type anger periodicities affect apart generally interesting result also justify initial assumption extracted mood scores able overcome several biases might created example speciﬁc possibly unfortunate signiﬁcant events emerge real-life temporarily inﬂuence content tweets. finally merged mood types expressing compared expressing using msfms. figure shows extracted versus patterns winter summer well entire data set. plots derive higher daytime stronger evenings nights. comparing winter summer plots counting seasonal differences hours night also winter begins dominate slightly earlier afternoons. winter’s negatively affected hour –a.m.; summer observe increased levels negativity midnight well. previous sections performed mood analysis twitter content geolocated trying extract diurnal seasonal average patterns emotions anger fear sadness. aware ﬁrst study real-time mood variation population level using social media information developed methods computing mood scores mfms msfms; main difference based fact second standardises frequencies mood term considered amount time thus reduces impacts highly frequent terms might have. statistical analysis indicates msfms scheme produces stable results; hence show elaborate ﬁndings considering latter. still extracted patterns scorings schemes positively correlated minimum linear correlation equal msfms mood types fear sadness clear peaks mornings. fear also peaks midnight; side anger ambiguous behaviour peaks emerging afternoons midnights late night/very early morning hours. combined circadian rhythm shows peaks mornings another midnights whereas also peaks mornings hour na’s peak; comparison indicates takes higher values mornings increases evolves takes evenings figure plots represent variation -hour period emotional valence positive versus negative affectivity winter summer aggregated data applying msfms scheme. faded colourings represent cis. stability p-values extracted patterns early morning hours. mood signals highly correlated across different seasons still exist noticeable ﬂuctuations example emotion levels higher late night/early morning hours winter exist stronger emotional indicators evenings midnights summer. comparison ﬁndings results widely reported psychiatric studies reveals similarities also several differences. according increases progresses peak evenings; something reported also results summer’s rhythm. similarly shows afternoon late night/early morning hours average stronger terms ‘laughing’ ‘socialising’ symmetric peak mornings however circadian patterns anger expressed action ‘arguing’ sadness expressed ‘sighing’ correlated results since show differences peak moments well monotonicity. furthermore observe emotional state ‘angry’ reaching maximum time period emotion anger ﬁndings; holds emotional states ‘depressed/blue’ ‘worry’ emotion sadness. contradicting point study oriented emotional states reported increasing behaviour day. recent similar study presented circadian patterns across different days week daily averages among different cultures ﬁndings regions australia contrast ones derived alone. shown retrieve peak value midnight; work scoring schemes happening mornings. moreover peaks late night hours reported reduced levels time period importantly results vary signiﬁcantly across different cultures; example circadian rhythms india africa indicate much better ones extracted. limitations conducting studies. among these obvious people cannot send messages whilst asleep therefore drop signalling night. however adjusted results normalising normalising standardising reduce impact effect. work consider inﬂuence signiﬁcant events natural phenomena emerging real-life might extracted signals method collecting twitter messages excluded content geolocated rural areas. however argue biases usually resolved signiﬁcant level working large-scale samples data; periodical characteristics statistical signiﬁcance provide proof stability extracted signals. hand limitations arising attributes general population might present twitter users behavioural motives elderly create unresolved biases; therefore present script make claims population twitter users general population. finally validation section investigating daily mood patterns twitter. similarly previous section base affective analysis wordnet affect focus three negative affective norms positive methods data collection processing already described section collected tweets geolocated therefore results partly affected this. considered time period study year total number tweets used experimental process approx. million. mfms msfms applied again difference hourly time intervals replaced daily time intervals i.e. extracting mood patterns daily basis. therefore using notation section mood score given figures show extracted daily mood scores mfms msfms anger fear sadness respectively. plot apart exact scores also included smoothed version exact values allow retrieve dates peaks emotional signal whereas smoothed time series reveal weekly trend emotional level thus used extract periods alleviated aggravated affect. section explain time series based assumption affective norms twitter might emerging events real-life. turns signiﬁcant events perceived common sense tracked identiﬁed within time series. anger. figure distinctive periods anger emerging october july respectively. anger peaking october something could explained series events happened including death many civilians clashes egypt announcement number jobless people reached record high vettel’s liam fox’s security scandal. peak july could emerged news international phone hacking scandal. additional distinctive moments periods terms anger january april royal wedding. however signal derived applying msfms quite different; indeed linear correlation signals almost existent uncorrelated signal uncovers even interesting moments ‘angry’ period august peaks month; fear. fear emotion scoring schemes produce equivalent outputs; linear correlation mfms msfms time series equal distinctive fearful moments signals happen riots earthquake japan followed tsunami. fear also high winehouse’s death co-occurred ander breivik’s attacks norway. side effect wordings used describe halloween’s traditions period halloween also characterised possibly artiﬁcial fear. joy. similarly anger mfms msfms produce totally different scores therefore uncorrelated time series mfms interesting attribute identifying signiﬁcant least population twitter users events throughout year; peak happens christmas rest signiﬁcant days listed descending score order year’s christmas year’s valentine’s easter sunday father’s andrew’s mother’s day. since mfms based assumption relative importance term ﬁnal mood score deﬁned frequency corpus made clear traditional celebrations ‘joyful words’ overused simplifying identiﬁcation. side msfms shows peaks different moments much difﬁcult explain using calendar events news outlets. apart peak clearly correlates riots rest could explained major sport events events international scale egyptian revolution death libyan warlord muammar gaddaﬁ possibly weather conditions. series also relate unexpected deaths also short peak riots august. msfms time series correlation result derived mfms time series sadness peaks july followed riots period gary speed’s death date also less strong peaking moments could matched sudden tragic deaths. overall extracted signals main characteristic signiﬁcant moments usually matched tragic and/or sudden deaths. minus figure shows time series average minus score mfms msfms. detect days year triggered negative emotions twitter users. osama laden’s winehouse’s deaths together riots distinguishable events based score peaks applying mfms; also high levels negative emotion earthquake japan ‘occupy london stock exchange’ movement gary speed’s death negative hence positive christmas day. msfms time series correlation ones mfms; distinctive dates characterised riots winehouse’s death earthquake japan; positive year christmas. linear correlations amongst mood types. table shows linear correlations across different mood types scoring schemes observe mfms correlations follow common logic i.e. negatively correlated uncorrelated emotions; positive correlations exist among anger fear sadness highest occurs fear sadness contrary msfms shows high correlation anger; still stronger correlation negative emotions happens fear sadness. looking correlations smoothed time series emotion reveal similarities weekly tendencies signal mfms negative correlations fear sadness increasing whereas correlated negative emotions anger fear. msfms shows anger highly correlated well sadness ﬁrst sight might seem contradict common sense however emotional levels express affective norms individual possibly uniform autocorrelation mood types. figures show autocorrelation ﬁgures -day mfms msfms respectively. correlations inside region deﬁned positive negative conﬁdence bounds statistically signiﬁcant. autocorrelation used experimental indication existence periodic patterns data emotion types scoring schemes show highest autocorrelations interpreted fact emotion usually change rapidly consecutive days. mfms gives highest -day autocorrelation emotion fear whereas msfms emotion interesting result considers autocorrelations reveal mood periodic weekly behaviour. indeed mfms anger fear sadness autocorrelations second largest value extracted ones; strong weekly pattern belongs emotion anger exception joy’s strongest autocorrelation happens might result ﬁxed annual celebration dates emotion clusters weekdays days average four mood scores weekday data obtain matrix applied. data projected ﬁrst principal components result scoring schemes visualised figure mfms clear clusters consecutive days monday-tuesday wednesday-thursday saturdaysunday. friday seems ‘special day’ separated days. pattern evident msfms difference tuesday closer wednesday monday; gives indication msfms mondays might figure autocorrelation ﬁgures mood types mean standardised frequency mood scoring correlations outside conﬁdence bounds statistically signiﬁcant. also represented days distinctive emotional patterns. second task cluster single days therefore applied matrix. derived clusters mfms msfms expected entirely similar. figure characteristic moments clustered together. christmas entirely separated rest popular days ﬁxed celebrations also grouped together. also dates gary speed’s death japan’s earthquake halloween also separated main cluster days. msfms observe christmas probably distinctive riots entirely separated rest also placed opposite points considered dimensional space. distinctive cluster close point formed another date within riots together dates earthquake japan well winehouse’s death combined breivik’s attacks remaining clusters deviate much main space majority points lies. general conclusion seen emotional affect extracted twitter’s textual stream shows correlation real-life events emerging well international level. something occurs scoring schemes; recall mfms based assumption frequency emotional term deﬁnes importance whereas msfms applies standardised weighting scheme. emotional peaks also present annual calendar celebrations christmas valentine’s halloween signiﬁcant events concerned terms correlation elevated negative affect twitter scoring schemes riots august winehouse’s death severe earthquake japan. scoring schemes gave rise quite different events especially emotions anger joy; fear sadness much consistent across proposed schemes. autocorrelation ﬁgures scoring schemes indicate emotions correlate between consecutive days increased autocorrelations decrease signiﬁcantly. indicator pointing state mood might relative -day consistency usually changes days passed. perhaps ‘sudden’ events force emotional reconﬁgurations. interestingly autocorrelations mood types increase again equal revealing weekly periodic pattern. result statistical proof mood also affected week i.e. common emotional ground weekday time. scoring schemes indicate anger might strongest weekly autocorrelation based solely msfms only highest weekly period. clustering weekdays based principal components -dimensional daily affective norm discovered scoring schemes consecutive days especially weekends tend group together common cluster apart ‘fridays’ seem unique days displaying distinctive emotional behaviour. finally extracting clusters single dates observe important events positioned identiﬁable areas -dimensional space sometimes clustered together. similarly previous section results face limitations. importantly acknowledge fact users twitter might biased sample population; using large data overcome smooth biases. contrast previous section actual ground truth indications existed show evidence coming primarily events reported news partly justify moments mood signals. chapter investigated seasonal circadian daily patterns emotional affect twitter content. considered four mood types three expressing i.e. anger fear sadness expressing i.e. joy. methods applied order extract mood scores; ﬁrst weighs emotional term proportionally frequency corpus whereas second removes factor considering standardised version term’s frequency. firstly extracted seasonal diurnal mood rhythms based twitter content published within geographical region derived results partly agreement psychiatric studies contradict ones presented recent similar work among interesting ﬁndings acquired clear indication within data considering inevitable biases stronger daytime vice versa night hours. furthermore mood patterns show level periodicity; common values period hours less periodic emotion anger. analysing daily mood patterns gained evidence supporting real-life events affect mood twitter users. reversing relationship mood ﬁgures could used track signiﬁcant events emerging real life; example riots winehouse’s death combined norwegian massacre well japanese earthquake osama laden’s death affected emotional output twitter users. among relevant discoveries could also track annual celebrations calendar events christmas year’s valentine’s halloween father’s different scoring schemes give different uncorrelated results emotions anger joy; however fear sadness emotions also correlated scoring schemes produced quite similar results. nevertheless analysis daily mood patterns show output scoring schemes combined events identiﬁed scheme missed one. finally showed investigated affective types periodic behaviour lags equal days meaning ﬁrstly consecutive days likely emotionally correlated secondly mood might also dependency speciﬁc week. direction clustered weekdays based mood scores showing consecutive days positioned closer -dimensional space exception ‘fridays’; also showed dates exceptionally negative positive mood also identiﬁable space. chapter present additional preliminary work mainly based pattern discovery methodologies proposes ways explore rich information social particular twitter. ﬁrst investigate spatiotemporal content relationships locations forming networks based similarities published tweets. show networks stable time therefore form trustful description content shared across uk’s microblogging space. then brieﬂy examine posting time patterns twitter. show patterns slightly different weekdays weekends something explained different characteristics time periods posting time data features order form clusters days. finally preliminary method presented address interesting problem voting intention inference based inputs social media; general election used case study. point would like remind reader work presented chapter considered work-in-progress; still results interesting could serve basis future research directions. following sections investigate spatiotemporal relationships content published social networks twitter. aims study geographical distance locations inﬂuences respective similarity tweets geolocated vicinity. additionally look whether content originated different locations time window signiﬁcant similarities similarities exist affected length time window. latter question answered formation networks show content shared among locations. following experiments using million tweets published twitter second semester geolocated urban centres process data collection similar carried previous experiments already described section reminder radius urban centre location equal turn means tweets geolocated within area mapped corresponding location. conducted experimental process conﬁgurations settings. ﬁrst experiment document formed tweets geolocated urban centres day. since data comprised days total number documents considered days locations second experiment time span document reduced minutes also reduce total number days month therefore total number documents becomes days documents locations location proximity proportional content similarities twitter? answer question investigate relationship quantities i.e. pairwise geographical distance locations respective similarity textual content. pairwise geographical distance approximated using longitude latitude urban centres content correlation computed using cosine similarity metric denote urban centres {ui} considered time intervals ﬁrst experiment second. document geolocated denoted retrieved applying tf-idf entire collection documents removing stop words stemming denoted dij. average cosine similarity experimental settings depicted figures looking smoothed equivalents obvious relation exists geographical distance content similarity; ﬁrst experiment slight decrease similarity distance increases relatively small signiﬁcant whereas second experiment similarity seems unaffected distance. ﬁgures observe high content correlations urban centres located other. addition linear correlation results equal p-value showing time windows used affect much ﬁnal pattern. however also becomes apparent decreased time window produces lower cosine similarities. figure average pairwise cosine similarity location pairs plotted respective geographical distance. denote points whereas black line smoothed equivalent twitter data global level pattern likely change. cannot generalise result fact using twitter content geolocated only. hand could also main characteristics twitter microblogging general ability quickly spread share information places regardless actual geographical proximity least within well deﬁned space country unitary state strengthen argument visualise result better apply multidimensional scaling data sets aiming plot considered locations -dimensional space based pairwise cosine similarities. well-established method visualising patterns proximities among variables; performing several trials data best solution retrieved performing nonmetric scaling form computed solution represents ordinal properties data figures shepard plots showing pairwise similarities well monotonic transformations induced nonmetric known disparities. ﬁgures disparities fairly good nonlinear approximation pairwise-cosine similarities. hence plot conﬁguration points retrieved applying -dimensional space expecting representation reﬂects original similarities satisfactory level time spans spatial proximities seem inﬂuence clusters locations; exist nearby locations clustered together time distant locations might members cluster well. additional observation visualised ﬁgures major cities situated outer space -dimensional whereas locations smaller population ﬁgures gathered centre map. next section step further trying understand exactly content shared among locations. based previous experimental results revealed proximity signiﬁcant inﬂuence content similarity within country taking step concentrating efforts explaining spatiotemporal content relations. particular want examine twitter content shared among different urban centres proposing preliminary method forming networks capture describe relationships. pre-speciﬁed time intervals {tj} compute average cosine similarity location pairs manner already described previous section. length arbitrary choice deﬁnes overall time period encapsulated network. every location retrieves impact score quantity expressing degree shared content deﬁned average cosine similarity location pairs participates computed using number times occurs location pair average pairwise cosine similarities average-similarity location pairs ranked decreasing order ones selected. location pairs form edges inferred spatiotemporal network. directionality edge decided impact scores participants else numerical value arbitrary choice; experiments therefore expect infer networks comprised edges. first equal entire -month time span data equal day. give average picture network -month period based one-day long content similarities considered locations. result depicted figures network figure starts node highest out-degree clockwise manner lists nodes decreasing out-degree order. london shown central location topic space followed manchester liverpool. apparent locations highest out-degrees ones highest populations; general something expected happen average day. figure depicts network using spring-embedded layout makes edges network visible; directly observe london shares content nearby cities luton reading well places away liverpool manchester. likewise connection cardiff swansea also cardiff glasgow. second experiment reduce month minutes. similarly previous experiment figure shows average picture inferred network december based -minute long content similarities considered locations. network directly comparable previous since encapsulates data december entire -month period; still spot major differences fact manchester central location topic space followed glasgow london. figures show comparable networks. figure reduced version network ﬁrst experiment month whereas figure identical network inferred second experiment time displayed using spring-embedded layout. inferred networks different values quite dissimilar. examining closely observe based short-term content similarities locations different major ‘player’ also distributes out-degrees balanced manner among locations; london’s nearby locations included network also glasgow signiﬁcantly increased level shared content. additionally also reveals ‘strange’ connection poole blackpool connection appear former network day. applying network similarity scoring function networks similarity previous section presented method forming content similarity networks among locations different time intervals. section show inferred networks stable time therefore good well deﬁned descriptions different locations share content time twitter. similar approach introduced followed; prove network stability time ﬁrst form time series networks apply similarity metric compare consecutive networks time series ﬁnally using null hypothesis show similarities statistically signiﬁcant. given maximum number nodes networks known using simple similarity measure compares directed edges networks known jaccard distance sets edges deﬁned following similar approaches presented base null hypothesis model randomisation strategy able generate networks degree distribution original ones randomised topology. randomisation strategy starts given graph randomly switches edges times. example edges present original network replaced experiments similarity consecutive time versions inferred network tested statistical signiﬁcance randomised switching performed random swaps randomised network compared original counting times higher equal compute p-value null hypothesis p-values lower equal justify statistical signiﬁcance hence network stability. study stability network cases. ﬁrst equal month day. sense examining whether networks based long content similarities stable monthly basis. second setting month minutes investigating whether networks based -minute long content similarities stable day-to-day basis. results plotted figures respectively. ﬁgures becomes apparent networks stable monthly basis ﬁrst case daily basis second similarity score consecutive instances always higher derived randomised switching technique might expected common logic network ﬂuctuates monthly basis thus similarity scores ﬁrst case lower ones observed second. last using data second experiment investigate inferred networks evolve minute basis. following notation section achieved setting minutes. figure shows derived result. time series similarity scores drawn -day period mean similarity score equal standard deviation lower expected compared ones derived comparing networks larger time spans. smoothing -moving point average extract hour trend interesting pattern derived. similarity scores reach minimum every midnight long night hours rise back again indicating possible periodic bias. course could reduced twitter night individual users importantly professional news media agencies. latter observation served motivation next section study posting time patterns users twitter. section investigating posting time patterns twitter. purpose approx. million tweets posted july december geolocated initially show relative volume tweets hour aggregating days data three distinctive time intervals terms posting volume –p.m. –p.m. –p.m. hour highest number tweets. also twitter volume goes signiﬁcantly late night hours twitter becoming ‘alive’ approx. a.m. therefore simple statistical analysis conﬁrms common sense indicates twitter activity might highly correlated people’s real life activities. test stability pattern performing following basic statistical signiﬁcance test. every data compute percentages twitter volume hourly interval obtain linear correlation coefﬁcient general volume pattern. also compute linear correlation randomly permuted versions count many times correlation pseudo-randomised pair higher equal perform operation days data set; hence p-value equal p-value justiﬁes statistical signiﬁcance shows extracted pattern stable. p-value posting times pattern presented previous paragraph equal therefore assume stable representation hourly volume percentages. moving basic statistical analysis divide data subsets containing twitter volume week days weekends. twitter follows reallife patterns expect altered behaviour posting trends weekends. indeed weekends twitter volume peaks slightly different time interval however initial pattern still signals similar linear correlation equal interesting deviations occurring late night hours showing twitter users tend post weekends possibly sleep later days week well ones early morning hours higher volume tweeting observed weekdays. repeating previous statistical test show patterns stable; p-values equal weekdays weekends respectively. shown exist least slightly different posting time patterns weekdays another weekends. investigate whether posting patterns present visualise result performed simple clustering method. represented vector features i.e. twitter volume percentage hourly interval. similarly section apply mds; shepard plot indicates solution presented figure good ﬁgure derive monday distinctive week also observe consecutive days placed nearby positions -dimensional space. interestingly friday close saturday thursday saturday close friday sunday. general comment simple also probably interesting statistical analysis offers additional proof common assumption social media might follow patterns emerging real life; since important tool majority people perhaps easy differentiate real-life trends. section present preliminary method extracting voting intention ﬁgures twitter. case study used verify ﬁndings general election united kingdom recent past papers published topic offering preliminary solutions discussing limitations several approaches might have; refer discuss section. consider three major parties namely conservative party labour party liberal democrat party overall using three techniques extracting positive negative sentiment tweets different methods sentiment voting intention percentages. ground truth acquired yougov’s published results consists voting intention polls dated january polls usually refer pair days indicate expectation voting intention percentage political party. move closer election become dense; poll published every day. tweets drawn period time i.e. january may; total number greater million used become apparent following sections. applying ﬁltering keep tweets regarding politics tweets i.e. approximately tweets political party. common characteristic three approaches ﬁrst retrieve tweets regarding political party using handmade keywords keywords different party many simple possible; names politicians party name party search case sensitive keyword -gram case insensitive n-gram latter case looking exact match either; searching tweets contain -grams target n-gram. character front -gram denotes searching twitter topic. since keywords based mainly names entities could argue could also created automatic manner extracted repository hence human involvement could become insigniﬁcant. three approaches build other; elaborate version precedent. ﬁrst approach using stemmed version sentiwordnet extract positive negative sentiment tweets without taking consideration different parts speech sentiwordnet result automatically annotating wordnet synsets according degrees positivity negativity neutrality stemming performed applying porter’s algorithm part-of-speech tagging ‘skipped’ computing average positive negative sentiment weight stem possible might appear stems equal positive negative sentiment considered. positive negative sentiment scores tweet retrieved computing positive negative sentiment weights words contains. noted might exist tweets words listed sentiwordnet; tweets zero positive negative sentiment therefore ignored. acronym snpos used denote approach. motivation behind removal tagging assumption twitter language might follow norms formal scripts therefore taggers trained ‘mainstream’ types text might create inaccurate results. however practice following sections always case probably tweets refer politics much better structure casual ones. happen stemming) average positive negative sentiment weights assigned tagging tweets carried using stanford tagger java implementation log-linear taggers described summing sentiment weights tweet’s terms retrieve sentiment score. method denoted spos finally extend spos incorporating core word senses wordnet semiautomatically compiled list terms. wordnet gives synonyms term list synonyms extend content tweet. tweet contains word listed wordnet’s core terms tweet extended attaching synonyms word again sentiment score tweet computed summing sentiment weights tagged terms. method identiﬁed acronym sposw main motivation behind extending content tweet synonyms fact short length tweet might reduce expressiveness therefore adding words could enhance semantic orientation. furthermore sentiwordnet include english words attaching synonyms achieve compute pseudo-sentiment greater number tweets. applying method ones described previous section compute positive negative sentiment scores tweets turn originally extracted using keywords political party. next task turn scores percentage represent voting intention particular party. section describe methods address task. optionally could ﬁrst remove tweets almost equal positive negative sentiments. semantic orientation tweets unclear therefore might always help. later present experimental ﬁndings every experimental setting test also replicating experiment introducing threshold removes remaining tweets compute mean positive negative sentiment scores µposm µnegm respectively. sentiment score assigned tweets derived simply subtracting quantities suppose computed sentiment scores time instances three parties senti lib. calibrate relation sentiment scores voting intention percentages regress vectors corresponding ground truth using compute three weights wcon wlab wlib. example poll denotes ofﬁcial voting intention poll percentage time instance introduce bias terms regression experimental process became evident receive large values reducing signiﬁcantly freedom model causing overﬁtting. case study considering percentages three major parties. inference process results normalised order represent valid percentages special case inferences lower negative results thresholded normalisation take place unless nonzero percentages value greater ofﬁcial voting intentions however take consideration existence political ‘forces’ well people’s desire vote create equivalent comparable representation normalised ofﬁcial voting intention percentages well based fact voting intention percentages three major parties average equal approx. normalisation change picture much voting intention inferences made unseen data. again time instances overlap time instances used learning calibration weights ﬁrst compute sentiment scores party time instance. then sentiment scores multiplied corresponding calibration weight; example inferred score conservative party equal second method identical uses different function compute senti retrieving tweets satisfy threshold count many positive sentiment score higher negative vice versa. overall sentiment score tweets computed {pos neg} number tweets positive sentiment greater negative {neg pos} number tweets negative sentiment greater positive. refer method dominant sentiment class measure performance methods using loss functions. primarily mean absolute error inferred target voting intention percentages. metric allows easier interpretation read percentage i.e. units inferred values. addition aiming assess good ranking political parties based inferred voting intention percentages measuring mean ranking error triplet voting intention percentages ranking error deﬁned three parties distance correct inferred ranking. example exists incorrect ranking size since dealing variables either exists incorrect ranking size therefore triplet’s total error equal ranking errors size make triplet’s total error equal obviously triplet correct rankings error equal maximum error triplet equal ranges computed total combining three methods extracting positive negative sentiment tweets methods converting sentiment scores voting intentions come experimental setups. experiments i.e. tweets sentiment score also value removes ‘semantically unclear’ tweets. retrieve ﬁrst performance ﬁgures performing leave-one-out cross validation aforementioned experimental setups. performance results presented tables respectively. methods thresholding tends occasions improve inference performance terms mre. snpos performs signiﬁcantly better spos also fails rank voting intentions properly mts. using results better performance average deliver overall best performance derived sposw using thresholded data set. figure depicts best performing inferences corresponding ground truth; case across parties equal standard deviation since main data small size based fact oscillations actual voting intention signals major also perform focused testing. polls training perform testing remaining ones number tweets retrieve using search terms increasing nearer election day. training sliced parts table mae’s standard deviation mean thresholded sentiment performing leave-one-out cross validation. denotes minimum distance between positive negative sentiment score tweet order considered. ‘way’ election another much closer experiment additionally retrieve statistical signiﬁcance indication inferences. that randomly permute outcomes come randomised training test set; repeat process times count many times model based randomly permuted data delivers better inference performance actual fraction gives p-value. consider p-value lower indicates statistical signiﬁcance. becomes apparent combining sentiwordnet tagging extending tweets wordnet’s core senses gives best inference performance well performs average better mts. snpos spos show fairly poor performance terms also deliver statistically signiﬁcant results. contrary sposw’s inferences shown statistical signiﬁcant; best performance achieved performing thresholding reaching topic voting intention electoral result inference content twitter quite scientiﬁc literature. tumasjan published paper providing proof twitter platform political discussions conducted proposes method predicting result german elections method uses linguistic inquiry word count semantic analysis tool produces dimensions positive negative sentiment therefore method introduce averaging technique order acquire -dimensional representations. method like uses sets keywords select politically oriented tweets proposes model matching twitter trafﬁc party ﬁnal result elections. another paper presents different method tracking voting intention polls based ratio positive versus negative sentiment tweet; learning approach similarities deals bivariate problems i.e. polls outcomes. nevertheless paper published indicated methods problem speciﬁc generic application proven better chance predicting result congressional elections another paper showed popular tool google trends limited capacity predicting election results moreover interesting paper conducted analysis aforementioned methodologies proposed triplet necessary standards theory aiming provide consistent prediction elections using content social media follow authors recommend ﬁrstly prediction theory formed well deﬁned algorithm secondly analysis aware different characteristics arising social thirdly method must include experimental justiﬁcation works. based propositions tried formalise steps methodology provided statistical signiﬁcance ﬁgures results ﬁnally forming three different schemes extracting sentiment tweets tried encounter special characteristics twitter’s textual stream. particularly sposw enriched tweets adding synonymous words order enhance semantic interpretation observed signiﬁcant improvement inference performance. similarly aforementioned works ﬁrst sentiment extraction methods tried showed poor performance statistically signiﬁcant. average modelling sentiment performed better mts. quite interesting deﬁnition compresses information mts; might argue compression results much clearer signal i.e. removes noisy observations especially applied extended tweets. contribution thresholding experiments ambiguous; sometimes improves inference performance occasions reduces mre. mentioned beginning optimal value well contribution thresholding investigated further. remind reader presented methods modelling voting intention polls section preliminary several aspects matter future research. primarily methods applied data sets well come experimental proof capability generalise. another important factor choice keywords used select tweets relevant task hand. argued keywords selected topic-related repositories; still inﬂuence keyword modelled. ideally automatic mechanism algorithm compiled order select optimal subset keywords also quantify contribution term. moreover possible biases introduced different sentiment analysis tools also considered; tools incorporate special emotional expressions used instant messaging twitter ‘;-)’ might achieve better performance. general conclusion could extracted reconﬁrmed presented work social media encapsulate content related public’s political opinion; extending tweets synonymous terms probably assists ampliﬁcation signal. however important contemplate validity voting intention polls questionable. polls good representation actually happening? usually polls different agencies tend signiﬁcant differences percentages quite often differences exceed example lowest total derived experimental process therefore formation consistently good ground truth another issue approaches like must resolve. chapter presented additional preliminary work trying exploit rich content twitter. firstly investigated spatiotemporal relationships based textual stream. showed content correlation necessarily depend geographical distance within space country. proposal preliminary method capable forming networks content similarity among considered locations. networks proven stable time therefore good description content shared across secondly focused extracting posting time patterns twitter. main result showed twitter users tend tweet evolves peak tweeting p.m.; expected lowest volume rates occur late night hours. interestingly temporal pattern changes weekends special characteristics days explain observed deviations weekday pattern. forming clusters weekdays based data also showed consecutive days clustered together; exception monday seems ‘independent’ behaviour. finally presented preliminary approach addressing task inferring voting intention polls content twitter something could lead approximate inference election result. approach similarly approaches topic selects tweets might refer politics using lists relevant keywords. formed three ways converting tweets positive negative sentiment scores; ﬁrst method applies averaged version sentiwordnet disregarding different parts speech second considers different parts speech incorporating stanford tagger third builds second extending selected tweets synonyms wordnet’s core terms proposed ways converting sentiment scores tweets voting intention percentage experimental results indicated sposw achieves best performance. chapter present applications created effort showcase ﬁndings academic community general public. detector uses methodology presented chapter infer inﬂueza-like illness rates several regions. mood nation application mood scoring method presented chapter displays levels four types emotion regions. parts chapter based publication detector tracking epidemics twitter monitoring diffusion epidemic disease seasonal inﬂuenza important task. various methods deployed health sector order detect constrain epidemics counting consultation rates school workforce absenteeism ﬁgures need proper infrastructure time delays necessary data processing main drawbacks methodologies. argue information available provide additional means tackling problem. demonstrated user queries search engines used provide early warning epidemic. furthermore recent work shown social media predictive power different domains. particular article presented method inferring rates several regions using data twitter core method performed feature selection applying soft bolasso consensus threshold validation. section present complete pipelined application implementing theoretical ﬁndings titled detector detector automated tool interface used tracking prevalence several regions using contents twitter’s microblogging service. data used train validate models behind tool similar ones described section particular approx. million tweets collected period days geolocated within range populated urban centres ofﬁcial rates form ground truth. applied methodology already described detail section give short summary process. considered candidate features formed -grams -grams; feature extraction process described section algorithm applied data comprised -grams well -grams features selected cts. then applying algorithm decide optimal select combined -grams -grams learn weight n-gram applying regression. finally combining selected features weights daily twitter content geolocated within region able compute daily score region. since inferred score sometimes negative number something realistic value given rates always larger equal zero threshold negative inferences zero. additionally based fact rates show smooth behaviour therefore sudden peaks expected also smooth inference smoothed version inferences past days. also maintain weekly trend displayed scores. section already presented data collection process. short focus data collection tweets geolocated within radius populated urban centres query twitter’s search periodically order recent tweets urban centre. posts retrieved atom format parsed using rome java stored mysql database. learning process described previous section performed ofﬂine. automated procedure updates ﬂu-score inferences daily basis uploads website. apart main three regions training procedure based also display score projections northern ireland scotland wales england wales entire detector’s website running apache server using common technologies charts flash applications based open flash chart api. figure shows page website inferred scores last months displayed aforementioned regions inferred rate previous i.e. current score also displayed separate left. recall score denotes number ili-diagnosed patients population citizens; rates considered baseline normal average exceed characterised exceptional. finally figure shows page website entire rate time series displayed social media perceived another look society particular work shown ‘mood’ tweets able predict stock market well detect signiﬁcant events situations emerging real-life chapter section present complete pipelined application implementing theoretical ﬁndings titled mood nation method applied order compute displayed mood scores already described sections detail. mood nation displays scores four emotions namely anger fear sadness based affective terms included wordnet affect particular applied display mood scores z-scores i.e. score subtract mean divide remainder standard deviation mood score. mood scores comparable plotted single ﬁgure. positive score indicates mood type standard deviations larger mean value; opposite interpretation stands negative mood score. chosen multiply standardised mood scores constant number equal allow better presentation ﬁnal results user. another choice maintain dynamic mean standard deviation mood type means every data comes values automatically adapted. ﬁnal note contrast detector’s rates smoothing take place might ‘hide’ sudden emotional peaks interest. data collection back-end operations carried similar manner detector. kept skeleton website small amount changes also maintained considered regions. course might using twitter data core computation different. therefore mfms scheme implemented java updates scores four mood types daily basis. every page mood nation shows chart includes semi-transparent plots four mood types figure shows main page website mood scores entire displayed whereas figure page mood scores last days displayed region. another interesting page displays historical mood data example observe among interesting observations discussed chapter peaks ‘joy’ christmas chapter summarises closes thesis. start providing reader chapter-by-chapter synopsis script. then discuss outcomes conclusions work general perspective referring also impact academic community. chapter propose research directions might interesting explore future. chapter summarised theoretical background behind research. core scientiﬁc ﬁeld work primarily deﬁned statistical machine learning also apply methods discipline amongst theoretical notions chapter also introduced reader regularised regression explained carts operate introduced various vector space models. chapter gave characterisation twitter social network used primary source information project ﬁrstly describing main attributes referring several ‘twitter-driven’ studies proving signiﬁcant value content practice. second part chapter explained crawl store process twitter data also described information sources served ground truth experiments. chapter presented ﬁrst preliminary method tracking epidemic general diffusion population exploiting content twitter starting manually formed illness related keywords proven well correlated frequencies ofﬁcial rates moving proposing fully automated methodology used sparse regressor lasso able extract textual features close semantic connection underlying topic also infer accurately rates several regions epidemic however approach faced limitations mainly lasso model-consistent learner -grams always encapsulate valid semantic description topic proof provided possible generalisation experimental results. chapter presented generic methodology inferring occurrence magnitude event phenomenon exploring rich amount unstructured textual information social part particular twitter improving methods presented previous chapter. linear nonlinear learning functions applied core methodology inference performance compared techniques used address similar problems study used types features -grams -grams also investigated several hybrid combinations feature classes effort nowcast phenomena emerging real life rainfall rates. whereas importance usefulness inferring rates clear fact rainfall rates inference mainly served benchmark much harder problem tested capacity proposed inference schemes. chapter explored data source different problem extraction seasonal circadian daily mood patterns analysis based methodologies assigns emotion-triggering words importance weight analogous frequency text stream another standardises term frequencies therefore removes biases induced frequent emotional markers. investigated four mood types namely anger fear sadness. inferred circadian patterns partly conﬁrmed research results ﬁeld psychiatry contradicted ones presented similar recent work general shown stronger daytime vice versa night hours. looking daily mood patterns gained evidence supporting real-life events affect mood twitter users; example riots winehouse’s death earthquake japan caused observable increase twitter users geolocated circadian daily mood patterns periodical features; common period mood signals length week. chapter presented three additional pattern discovery studies twitter content could serve basis future research directions. ﬁrst investigated spatiotemporal relationships textual stream. within limits country showed similarities twitter content depend distance locations published content authored; then proposed preliminary method forming networks content similarity among considered locations demonstrated inferred networks stable time. secondly focused extracting posting time patterns twitter distinguishing weekdays weekends. interestingly patterns seem conﬁrm common sense match behavioural norms drawn real-life. finally reported preliminary methods addressing problem voting intention inference twitter content using general election case study. lastly chapter presented online tools showcase ﬁndings academic community also serve dynamic applications theoretical deriva‘flu detector’ detects inﬂuenza-like illness several regions whereas tions. ‘mood nation’ displays mood scores regions; tools twitter content input information. work presented thesis additional consistent proof information high value exist unstructured user-generated text streams. information reﬂects various aspects real-life already number people using conducting parts life social media constantly increases logically expected reﬂect even more. throughout thesis demonstrated several ways extracting types information social especially twitter’s microblogging service. speciﬁcally focused extracting identiﬁable moments emerging real-life commonly referred events well ambiguous trends encapsulated general task pattern discovery. case studies rainfall rates inference showed events tend oscillate active inactive moments time topic discussion social media could successfully detected quantiﬁed. additional work showed textual stream encloses affective norms general population delivered common life patterns weekly periodic behaviours distinctive emotional norms weekends. content similarities time used form stable spatiotemporal networks urban centres reveal information shared among them. combining statistical learning position produce function turns twitter content voting intention ﬁgures therefore propose preliminary method interesting task. important crucial part work veriﬁcation methodologies. supervised learning scenario usually part problem’s formulation standard loss functions similarity metrics applied based fact ground truth available compare with. unsupervised scenario verify acquired results facts numerical statistical abstract might directly comparable them. based performance supervised methods nowcasting events inferring voting intention polls strongly argue information social good representation real life. conclusion becoming stronger observing well results mood analysis real-life happenings sudden deaths famous personas celebrations public unrest. made effort develop generic generalisable methods algorithms possible. side entirety proposed methodologies directly applicable textual content streams. importantly investigating distinct case studies also acquired experimental proof terms semantic relativity measurable distance target topic variable techniques would able extract several types signals; speciﬁc ones position test. experiments might based social platform only this course considered signiﬁcant limitation avoided speciﬁc structural features twitter. wherever ‘early’ success achieved also tried extend improve methodologies. -grams example increased semantic connection automatically selected terms target topic hybrid combinations -grams -grams improved inference performance. lasso’s model selection inconsistencies resolved bootstrapping nonlinearities explored using carts. lastly experimental proof indicated enriching content tweets synonyms might improve performance sentiment analysis them. publications well-received academic community several papers referred built basic ideas; short paragraph refer them. detection infectious diseases using contents social streams together geolocation properties occasions became research subject many conference journal publications well main theme recent book serving newly introduced topic ‘infoveillance’ another research subject increasing interest extraction voting intentions applying sentiment analysis tweets finally detection environmental incidents possible hazards observing social networks especially twitter another important topic work could extended several directions glad report many already explored growing academic community something easily proven increasing number eu-funded projects scientiﬁc publications social media analysis. research direction always improving core theory proposed methods algorithms techniques. speciﬁcally could investigate application even sophisticated linear nonlinear generative learners. experimentation additional case studies might also give information properties learning function hold. addition mixture linear nonlinear learning functions could also interest. feature space concerned based fact -grams showed much better semantic correlation target topics could also investigate using n-grams formalised regular expressions aimed look identify match speciﬁc textual characteristics. another interesting aspect future exploration incorporation mood sentiment analysis event detection methodology. rational consider events emerging real life affect mood social media already proof this. therefore relationship needs formalised affective norms could assist detection quantiﬁcation least class events effort detecting preventing potential social dangers. methods concentrated textual content written english language only hence porting methods languages applying machine translation frameworks could interest. going step further exciting challenge would successfully fuse different types information; work used textual information offers much more. information images podcasts videos explored combination signals textual stream. important challenges also emerge combination scientiﬁc ﬁelds. vast amount information provides example social scientists psychiatrists opportunity answer questions considered experimentally infeasible. hand results derived statistical analysis usually need interpretation experts disciplines. therefore interdisciplinary research paths naturally explored. ﬁnal equally important note warn academic community privacy issues imposed developments. issues taken carefully consideration proposed methods could indirectly expose stigmatise users therefore people. computer scientists must work closely legislators reassure automation improve quality life restrict society’s freedom speech right privacy. z-score method used standardising samples variable using sample mean standard deviation. numerical result standardisation shows many standard deviations away mean value sample. example transformed broken main components variance prediction squared bias showing amount average estimate differs target value. third component variance target around true mean something cannot avoided unless equal process known bias-variance decomposition. depending properties learning models face biasvariance tradeoffs. commonly increasing complexity model results lower bias higher variance. extended reference bias-variance decomposition. work applying moving average method smoothing technique. simple example follows. suppose adding random noise function ﬁgure plotted -point sample noise signal well -point moving average. smoothing random noise removed recovering original signal. null hypothesis assertion made expectation rejected lack statistical proof data support rejecting null hypothesis aiming support hypothesis opposite nature hypothesis deﬁned. p-value threshold occasions null hypothesis rejected fact statistics indicate unlikely hold. usually serves indication hypothesis opposite nature related assumption contradicts null hypothesis might statistically signiﬁcant. widely used technique clear application feature extraction principal component analysis given d-dimensional observations deﬁned orthogonal projection onto m-dimensional space respect maximising variance projected data generally twitter data collection focused urban centres. collect tweets geolocated within range centre. inferring rates using sets groups locations belong geographic region. list considered geographic regions well locations belonging them. central england basildon birmingham coventry derby ipswich leicester luton northampton norwich nottingham peterborough southend stoke watford wolverhampton list terms used ﬁrst experiments aiming extract rates twitter content manually formed list textual markers thought might able describe illness symptoms. abhor annoi discourag disdain brood exacerb displeas displeasur dudgeon greedi exasper hufﬁli grievanc incens loath madden maleﬁc malevol malic malici malign misanthrop misanthropi misogyn misogyni mison pester terms used select tweets related major political parties character denotes empty space denotes twitter topic. tables hold terms conservative labour liberal democrat party respectively. sentiment sentiment analysis shepard plot similarity score social media spatiotemporal standard error statistical learning statistical signiﬁcance stemming", "year": 2012}