{"title": "Learning to Compose Neural Networks for Question Answering", "tag": ["cs.CL", "cs.CV", "cs.NE"], "abstract": "We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural model network, achieves state-of-the-art results on benchmark datasets in both visual and structured domains.", "text": "training data consists triples approach requires supervision network layouts. achieve state-of-the-art performance markedly different question answering tasks questions natural images another compositional questions united states geography. describe question answering model applies images structured knowledge bases. model uses natural language strings automatically assemble neural networks collection composable modules. parameters modules learned jointly network-assembly parameters reinforcement learning triples supervision. approach term dynamic neural module network achieves state-of-theart results benchmark datasets visual structured domains. paper presents compositional attentional model answering questions variety world representations including images structured knowledge bases. model translates questions dynamically assembled neural networks applies networks world representations produce answers. take advantage largely independent lines work hand extensive literature answering questions mapping strings logical representations meaning; other series recent successes deep neural models image recognition captioning. constructing neural networks instead logical forms model leverages best aspects linguistic compositionality continuous representations. model components trained jointly ﬁrst collection neural modules freely composed second network laypredictor assembles modules complete deep networks tailored question andreas describe heuristic approach decomposing visual question answering tasks sequence modular sub-problems. example question color bird? might answered steps ﬁrst where bird? second what color part image? ﬁrst step generic module called find expressed fragment neural network maps image features lexical item distribution pixels. operation commonly referred attention mechanism standard tool manipulating images text representations ﬁrst contribution paper extension generalization mechanism enable fully-differentiable reasoning structured semantic representations. figure shows module used focus entity georgia non-visual grounding domain; generally representing every entity universe discourse feature vector obtain distribution entities corresponds roughly logical set-valued denotation. obtained distribution existing neural approaches immediately compute weighted average image features project back labeling decision—a describe module logical perspective suggests number novel modules might operate attentions e.g. combining inspecting directly withreturn feature space modules discussed detail section unlike formal counterparts differentiable end-to-end facilitating integration learned models. building previous work learn behavior collection heterogeneous modules triples. second contribution paper model learning assemble modules compositionally. isolated modules limited use—to obtain expressive power comparable either formal approaches monolithic deep networks must composed larger structures. figure shows simple examples composed structures realistic question-answering tasks even larger netfigure simple neural module networks corresponding questions color bird? states? neural find module computing attention pixels. operation applied knowledge base. using attention produced lower module identify color region image attended performing quantiﬁcation evaluating attention directly. works required. thus goal automatically induce variable-free tree-structured computation descriptors. familiar functional notation formal semantics represent computations. write examples figure respectively. network layouts specify structure arranging modules complete network. andreas hand-written rules deterministically transform dependency trees layouts restricted producing simple structures like non-synthetic data. full generality need solve harder problems like transforming cities georgia? extensive literature database question answering strings mapped logical forms evaluated black-box execution model produce answers. supervision provided either annotated logical forms triples alone general primitive functions logical forms assembled ﬁxed recent line work focuses inducing predicates functions automatically either perceptual features underlying schema model describe paper uniﬁed framework handling perceptual schema cases differs existing work primarily learning differentiable execution model continuous evaluation results. neural models question answering also subject current interest. include approaches model task directly multiclass classiﬁcation problem models attempt embed questions answers shared vector space attentional models select words documents sources approaches generally require answers retrieved directly based surface linguistic features without requiring intermediate computation. structured approach described learns query execution model database tables without natural language component. previous efforts toward unifying formal logic representation learning include grefenstette krishnamurthy mitchell lewis steedman beltagy visually-grounded component work relies recent advances convolutional networks computer vision particular fact late convolutional layers networks trained image recognition contain rich features useful vision tasks preserving spatial information. features used image captioning visual previous approaches visual question answering either apply recurrent model deep representations image question question compute attention input image answer based question image features attended approaches include simple classiﬁcation model described zhou dynamic parameter prediction network described models assume ﬁxed computation performed image question compute answer rather adapting structure computation question. noted andreas previously considered simple generalization attentional approaches small variations network structure per-question permitted structure chosen syntactic processing questions. approaches general family include universal parser sketched bottou graph transformer networks bottou knowledge-based neural networks towell shavlik recursive neural networks socher ﬁxed tree structure perform linguistic analysis without external world representation. unaware previous work simultaneously learns parameters structures instance-speciﬁc networks. observed section describe evaluate learn modules parameterized within ﬁxed structures. section move real scenario unknown. describe predict layouts questions learn jointly without layout supervision. evaluating modules given layout assemble corresponding modules full neural network apply knowledge representation. intermediate results modules answer produced root. denote output possible layouts restricted module type constraints modules operate directly input representation others also depend input speciﬁc earlier modules. base types considered paper attention labels parameters tied across multiple instances module different instantiated networks share parameters others. modules parameter arguments ordinary inputs parameter arguments like running bird example section provided layout used specialize module behavior particular lexical items. ordinary inputs result computation lower network. addition parameter-speciﬁc weights modules global weights shared across instances module write global weights weights associated parameter argument denote elementwise addition multiplication respectively. complete global weights parameter-speciﬁc weights constitutes every module access modules used paper shown below names type constraints ﬁrst description module’s computation following. lookup lookup produces attention focused entirely index relationship words positions input known ahead time mapped onto semantic primitives. second semantic primitives must combined structure closely exactly parallels structure provided syntax. example state province might need identiﬁed ﬁeld database schema states capital might need identiﬁed correct quantiﬁer scope. cannot avoid structure selection problem continuous representations simplify lexical selection problem. modules accept vector parameter associate parameters words rather semantic tokens thus turn combinatorial optimization problem associated lexicon induction continuous one. order learn province state denotation sufﬁcient learn associated parameters close embedding space—a task amenable gradient descent. remaining combinatorial problem arrange provided lexical items right computational structure. respect layout prediction like syntactic parsing ordinary semantic parsing rely off-the-shelf syntactic parser there. work syntactic structure provided stanford dependency parser associate layout fragment ordinary nouns verbs mapped single find module. proper nouns single lookup module. prepositional phrases mapped depth- fragment relate module preposition find module enclosed head noun. figure generation layout candidates. input sentence represented dependency parse fragments dependency parse associated appropriate modules fragments assembled full layouts observed model described corresponds largely andreas though module inventory different— particular exists relate modules depend two-dimensional spatial structure input. enables generalization nonvisual world representations. learning simpliﬁed setting straightforward. assuming top-level module layout describe exists module fullyinstantiated network corresponds distribution labels conditioned layouts. train maximize directly. understood parameter-tying scheme decisions parameters governed observed layouts assembling networks next describe layout model ﬁrst ﬁxed syntactic parse generate small candidate layouts analogously semantic grammar generates candidate semantic parses previous work layouts resulting process feature relatively tree structure conjunction quantiﬁer. strong simplifying assumption appears sufﬁcient cover examples appear tasks. approach includes categories relations simple quantiﬁcation range phenomena considered generally broader previous perceptually-grounded work generated candidate parses need score them. ranking problem; rest approach solve using standard neural machinery. particular produce lstm representation question feature-based representation query pass representations multilayer perceptron query feature vector includes indicators number modules type present well associated parameter arguments. easily imagine sophisticated parsescoring model simple approach works well tasks. expensive application deep network large input representation) tractably evaluate opposite situation usually encountered semantic parsing calls query execution model fast candidate parses large score exhaustively. fact problem closely resembles scenario faced agents reinforcement learning setting adopt common approach literature express model stochastic policy. policy ﬁrst sample layout distribution apply knowledge source obtain distribution answers chosen train execution model directly maximizing respect hard selection nondifferentiable optimize using policy gradient method. gradient reward surface respect parameters policy expectation taken respect rollouts policy reward. goal select network makes accurate predictions take reward identically negative log-probability execution phase i.e. thus update layout-scoring model timestep simply gradient log-probability chosen layout scaled accuracy layout’s predictions. training time approximate expectation single rollout step update direction single optimized using adadelta gradient clipping norm weakness basic framework difﬁculty modeling prior knowledge answers kinds linguistic prior essential task easily incorporated. simply introduce extra hidden layer recombining ﬁnal module network output input sentence representation replacing equation with results shown table dynamic networks provides small gain noticeably other questions. achieve state-of-the-art results task outperforming highly effective visual bag-of-words model model dynamic network parameter prediction conventional attentional model previous approach using neural module networks structure prediction examples shown figure general model learns focus correct region image tends consider broad window around region. facilitates answering questions like cat? requires knowledge surroundings well object question. figure sample outputs visual question answering task. second shows ﬁnal attention provided input top-level describe module. ﬁrst examples model produces reasonable parses attends correct region images generates correct answer. third image verb discarded wrong answer produced. framework described paper general interested well performs datasets varying domain size linguistic complexity. evaluate model tasks opposite extremes criteria large visual question answering dataset small collection structured geography questions. release employing development model selection hyperparameter tuning reporting ﬁnal results evaluation server test-standard set. experiments described section input feature representations computed ﬁfth convolutional layer -layer vggnet pooling input images scaled computing representations. found performance task table results geoqa dataset geoqa dataset quantiﬁcation. approach outperforms purely logical model model learned perceptual predicates original dataset ﬁxedstructure evaluation conditions. next experiments consider focuses geoqa geographical question-answering task ﬁrst introduced krishnamurthy kollar task originally paired visual question answering task much simpler discussed appealing number reasons. contrast dataset geoqa quite small containing examples. baselines available using classical semantic parser backed database another induces logical predicates using linear classiﬁers spatial distributional features. allows evaluate quality model relative perceptually grounded logical semantics well strictly logical approaches. geoqa domain consists entities participate various relations take world representation consist pieces category features different relational features experiments subset features originally used krishnamurthy original dataset includes quantiﬁers treats questions cities texas? cities texas? identically. because interested testing parser’s ability predict variety different structures introduce version dataset geoqa+q distinguishes cases expects boolean answer questions second kind. vironments. dynamic model outperforms logical perceptual models described well ﬁxed-structure neural module improvement particularly notable dataset quantiﬁers dynamic structure prediction produces relative improvement ﬁxed baseline. variety predicted layouts shown figure introduced model dynamic neural module network answering queries structured unstructured sources information. given triples training data model learns assemble neural networks inventory neural models simultaneously learns weights modules composed novel structures. approach achieves state-of-the-art results tasks. believe success work derives factors continuous representations improve expressiveness learnability semantic parsers replacing discrete predicates differentiable neural network fragments bypass challenging combinatorial optimization problem associated induction semantic lexicon. structured world representations neural predicate representations allow model invent reusable attributes relations expressed schema. perhaps importantly extend compositional questionanswering machinery complex continuous world representations like images. semantic structure prediction improves generalization deep networks replacing ﬁxed network topology dynamic tailor computation performed problem instance using deeper networks complex questions representing combinatorially many queries comparatively parameters. practice results considerable gains speed sample efﬁciency even little training data. supported national science foundation graduate fellowship. supported fellowship within weltweit-program german academic exchange service work additionally supported darpa afrl muri award awards iis- berkeley vision learning center.", "year": 2016}