{"title": "Formal Guarantees on the Robustness of a Classifier against Adversarial  Manipulation", "tag": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "abstract": "Recent work has shown that state-of-the-art classifiers are quite brittle, in the sense that a small adversarial change of an originally with high confidence correctly classified input leads to a wrong classification again with high confidence. This raises concerns that such classifiers are vulnerable to attacks and calls into question their usage in safety-critical systems. We show in this paper for the first time formal guarantees on the robustness of a classifier by giving instance-specific lower bounds on the norm of the input manipulation required to change the classifier decision. Based on this analysis we propose the Cross-Lipschitz regularization functional. We show that using this form of regularization in kernel methods resp. neural networks improves the robustness of the classifier without any loss in prediction performance.", "text": "recent work shown state-of-the-art classiﬁers quite brittle sense small adversarial change originally high conﬁdence correctly classiﬁed input leads wrong classiﬁcation high conﬁdence. raises concerns classiﬁers vulnerable attacks calls question usage safety-critical systems. show paper ﬁrst time formal guarantees robustness classiﬁer giving instance-speciﬁc lower bounds norm input manipulation required change classiﬁer decision. based analysis propose cross-lipschitz regularization functional. show using form regularization kernel methods resp. neural networks improves robustness classiﬁer small loss prediction performance. problem adversarial manipulation classiﬁers addressed initially area spam email detection e.g. goal spammer manipulate spam email detected classiﬁer. deep learning problem brought seminal paper showed state-ofthe-art deep neural networks manipulate originally correctly classiﬁed input image non-perceivable small transformation classiﬁer misclassiﬁes image high conﬁdence figure illustration. property calls question usage neural networks classiﬁers showing behavior safety critical systems vulnerable attacks. hand also shows concepts learned classiﬁer still quite away visual perception humans. subsequent research found fast ways generate adversarial samples high probability suggested training form data augmentation gain robustness. however turns so-called adversarial training settle problem construct adversarial examples ﬁnal classiﬁer. interestingly recently shown exist universal adversarial changes applied lead every image wrong classiﬁcation high probability needs access neural network model generation adversarial changes shown adversarial manipulations generalize across neural networks means neural network classiﬁers attacked even black-box method. extreme case shown recently attack commercial system clarifai black-box system neither underlying classiﬁer training data known. nevertheless could successfully generate adversarial images existing network fool commercial system. emphasizes indeed severe security issues modern neural networks. countermeasures proposed none provides guarantee preventing behavior might think generative adversarial neural networks resistant problem recently shown also attacked adversarial manipulation input images. paper show ﬁrst time instance-speciﬁc formal guarantees robustness classiﬁer adversarial manipulation. means provide lower bounds norm change input required alter classiﬁer decision said otherwise provide guarantee classiﬁer decision change certain ball around considered instance. exemplify technique widely used family classiﬁers kernel methods neural networks. based analysis propose regularization functional call cross-lipschitz regularization. regularization functional used kernel methods neural networks. show using cross-lipschitz regularization improves formal guarantees resulting classiﬁer well change required adversarial manipulation maintaining similar prediction performance achievable forms regularization. exist fast ways generate adversarial samples without constraints provide algorithms based ﬁrst order approximation classiﬁer generate adversarial samples satisfying constraints input dimension. following consider multi-class setting classes features classiﬁer point classiﬁed call classiﬁer robust small changes input alter decision. formally problem described follows suppose classiﬁer outputs class input problem generating input classiﬁer decision changes formulated constraint specifying certain requirements generated input e.g. image typically optimization problem non-convex thus intractable. generated points called adversarial samples. depending p-norm perturbations diﬀerent characteristics perturbations small aﬀect features whereas gets sparse solutions extreme case single feature changed. used leads spread still localized perturbations. striking result instances computer vision datasets change necessary alter decision astonishingly small thus clearly label change. however later regularizer leads robust classiﬁers sense required adversarial change large also class label changes already suggested generated adversarial samples form data augmentation training neural networks order achieve robustness. denoted adversarial training. later fast ways approximately solve proposed order speed adversarial training process however gets given approximation successful upper bounds perturbation necessary change classiﬁer decision. also noted early ﬁnal classiﬁer achieved adversarial training vulnerable adversarial samples robust optimization suggested measure adversarial manipulation eﬀectively boils adversarial training practice. thus fair date mechanism exists prevents generation adversarial samples defend paper focus instead robustness guarantees show classiﬁer decision change small ball around instance. thus guarantees hold method generate adversarial samples input transformations noise sensor failure etc. formal guarantees point view absolutely necessary classiﬁer becomes part safety-critical technical system autonomous driving. following ﬁrst show achieve guarantee explicitly derive bounds kernel methods neural networks. think formal guarantees robustness investigated become standard report diﬀerent classiﬁers alongside usual performance measures. following guarantee holds classiﬁer continuously diﬀerentiable respect input output component. instance-speciﬁc depends extent conﬁdence decision least measure conﬁdence relative diﬀerence maxj=c typical cross-entropy loss multi-class losses. following notation theorem multi-class classiﬁer continuously diﬀerentiable components class predicts j=...k deﬁned ﬁrst inequality holds last step used hölder inequality together fact q-norm dual p-norm deﬁned thus minimal norm change required change classiﬁer decision satisﬁes note bound requires denominator bound local lipschitz constant cross terms call local cross-lipschitz constant following. however require global bound. problem global bound ideal robust classiﬁer basically piecewise constant larger regions sharp transitions classes. however global lipschitz constant would inﬂuenced sharp transition zones would yield good bound whereas local bound adapt regions classiﬁer approximately constant yields good guarantees. suggest study global lipschitz constant small global lipschitz constant implies good bound converse hold. discussed turns local estimates signiﬁcantly better suggested global estimates implies also better robustness guarantees. turn want emphasize bound tight bound attained linear classiﬁers holds section reﬁne result case input constrained general possible integrate constraints input simply maximum intersection constraint e.g. gray-scale images. next discuss bound evaluated diﬀerent classiﬁer models. simplicity restrict case leave cases future work. consider class kernel methods classiﬁer form training points positive deﬁnite kernel function rk×n trained parameters e.g. svm. goal upper bound term maxy∈b k∇fj ∇fck classiﬁer model. simple calculation shows second step separated direction norm vector optimization direction yields cauchy-schwarz result. finally constrained convex one-dimensional optimization problem solved explicitly min{kx−xr−xsk proof results follows analogously noting proof. bound term equation separately using correspond exponential terms upper bounds inner product. similarly individual upper lower bounds taken lemma bound leads non-trivial estimates seen section bound tight. reason bounded elementwise quite pessimistic. think better bounds possible postpone future work. derive bound neural network hidden layer. principle technique apply used arbitrary layers computational complexity increases rapidly. problem directed network topology consider almost path separately derive bound. number hidden units weight matrices output resp. input layer. assume activation function continuously diﬀerentiable assume derivative monotonically increasing. prototype activation function mind later experiment diﬀerentiable approximation relu activation function σrelu max{ note limα→∞ σrelu +e−αx output neural network written discussed global lipschitz bounds individual classiﬁer outputs lead upper bound desired local cross-lipschitz constant. experiments local bounds lipschitz constant times smaller would achieve global lipschitz bounds shows global approach much rough meaningful robustness guarantees. latter formulation becomes apparent loss tries make diﬀerence large possible goal good robustness guarantees natural consider proxy quantity regularization. deﬁne cross-lipschitz regularization functional function class thus maximize time keep k∇fl ∇fmk small uniformly classes. automatically enforces robustness resulting classiﬁer. important note regularization functional coherent loss shares degrees freedom adding function outputs leaves loss regularization functional invariant. main diﬀerence enforce global lipschitz constant smaller one. kernel methods uses typically regularization functional induced kernel corresponding reproducing kernel hilbert space αiαjk. particular translation invariant kernels make directly connection penalization derivatives function fourier transform however penalizing higher-order derivatives irrelevant achieving robustness. given kernel expansion write cross-lipschitz regularization function standard regularize neural networks weight decay; squared euclidean norm weights added objective. recently dropout seen form stochastic regularization introduced. dropout also interpreted form regularization weights interesting note classical regularization functionals penalize derivatives resulting classiﬁer function typically used deep learning noted restrict hidden layer neural networks simplify notation write cross-lipschitz main emphasis paper robustness guarantees without resorting particular ways generate adversarial samples. hand theorem gives lower bounds required input transformation eﬃcient ways approximately solve adversarial sample generation helpful upper bounds required change. upper bounds allow check tight derived lower bounds are. experiments concerned images reasonable adversarial samples also images. however knowledge current main techniques generate adversarial samples integrate constraints clipping results provide following fast algorithms generate adversarial samples strategy similar linear approximation classiﬁer derive adversarial samples respect diﬀerent norms. formally order minimal adversarial sample solve take minimal kδkp. yields minimal adversarial change linear classiifers. note convex optimization problem reduced one-parameter problem dual. allows derive following result proposition solved time. start problem given note thus −λvr implies thus similarly −λvr implies thus also −vrxr note term monotonically decreasing increasing. thus sort max{ increasing order represent thresholds summation changes. compute thresholds determine largest threshold ﬁxes index sets sums. determine lower bound attained min{t −xr} max{−t−xr} note terms monotonically decreasing algorithm complexity initial sorting step followed steps complexity goal experiments evaluation robustness resulting classiﬁers necessarily state-of-the-art results terms test error. cases compute robustness guarantees theorem optimize using binary search adversarial samples algorithm -norm section binary search classiﬁer output diﬀerence order point decision boundary. additional experiments found supplementary material. kernel methods optimize cross-entropy loss standard regularization cross-lipschitz regularization convex optimization problems l-bfgs solve them. gaussian kernel e−γkx−yk ρknn mean nearest neighbor distances training show results mnist however checked parameter selection using subset images training evaluating rest yields indeed parameters give best test errors trained full set. regularization parameter chosen {−k|k kernel-svm kernel-cl. results optimal parameters given following table performance parameters shown figure note high computational complexity could evaluate robustness guarantees optimal parameters. figure kernel methods cross-lipschitz regularization achieves better test error robustness adversarial samples compared standard regularization. robustness guarantee weaker neural networks likely relatively loose bound. neural networks demonstrate upper lower bounds improve using cross-lipschitz regularization ﬁrst want highlight importance usage local cross-lipschitz constant theorem robustness guarantee. local versus global cross-lipschitz constant robustness guarantee proven before discussed penalization global lipschitz constant improve robustness also purpose derive lipschitz constants several diﬀerent layers fact lipschitz constant composition functions upper bounded product lipschitz constants functions. analogy would mean term supy∈b k∇fc ∇fjk upper bounded proposition denominator theorem could replaced global lipschitz constant given largest singular value supy∈rd k∇gk supx=y table show average ratio αglobal robustness guarantees αglobal αlocal theorem αlocal test data mnist cifar diﬀerent regularizers. guarantees using local cross-lipschitz constant eight times better global one. advantage clearly global cross-lipschitz constant computed using theorem evaluate guarantees quickly. however turns gets signiﬁcantly better robustness guarantees using local cross-lipschitz constant terms bound derived proposition instead derived global lipschitz constant. note optimization theorem done using binary search noting bound local lipschitz constant proposition monotonically decreasing following comparison table want highlight robustness guarantee global cross-lipschitz constant always worse using local cross-lipschitz constant across regularizers data sets. table shows guarantees using local cross-lipschitz eight times better global one. hidden layer networks obvious robustness guarantees deep neural networks based global lipschitz constants coarse useful. experiments hidden layer network hidden units softplus activation function thus resulting classiﬁer continuously diﬀerentiable. compare three diﬀerent regularization techniques weight decay dropout crosslipschitz regularization. training done sgd. method adapted learning rate regularization parameters methods achieve good performance. experiments mnist cifar three settings plain data augmentation adversarial training. exact settings parameters augmentation techniques described below.the results mnist shown figure results cifar figure .for mnist clear trend cross-lipschitz regularization improves robustness resulting classiﬁer competitive resp. better test error. surprising data augmentation lead robust models. however adversarial training improves guarantees well adversarial resistance. cifar picture mixed cl-regularization performs well augmented task test error upper bounds signiﬁcantly better robustness guarantees. problem might overall performance simple model preventing better behavior. data augmentation leads better test error robustness properties basically unchanged. adversarial training slightly improves performance compared plain setting improves upper lower bounds terms robustness. want highlight guarantees upper bounds adversarial samples away. mnist learning rate methods chosen regularization parameters weight decay chosen cross-lipschitz dropout probabilities taken cifar learning rate methods chosen regularization parameters weight decay cross-lipschitz dropout probabilities taken cifar data augmentation choose learning rate methods regularization parameters weight decay figure neural networks left adversarial resistance l-norm mnist. right average robustness guarantee l-norm mnist diﬀerent neural networks hyperparameters. cross-lipschitz regularization leads better robustness similar better prediction performance. plain mnist middle data augmentation bottom adv. training cross-lipschitz dropout probabilities taken data augmentation mnist means apply random rotations angle random crop cifar- apply additionally mirror image probability apply random brightness random contrast change substep ensure image clipping. implemented adversarial training generating adversarial samples inﬁnity norm code section replaced batch adversarial samples. finally batchsize experiments. illustration adversarial samples take test image mnist apply adversarial generation section -norm generate adversarial samples diﬀerent kernel methods neural networks method parameters leading best test performance. classiﬁers change originally correct decision wrong one. interesting note cross-lipschitz regularization adversarial sample really decision boundary thus decision actually correct. eﬀect strongest kernel-cl also requires strongest modiﬁcation generate adversarial sample. situation diﬀerent neural networks classiﬁers obtained standard regularization techniques still vulnerable adversarial sample still clearly dropout weight decay. show examples below. figure left adversarial resistance l-norm test cifar. right average robustness guarantee test l-norm test cifar diﬀerent neural networks hyperparameters. cross-lipschitz regularization yields good test errors guarantees necessarily stronger. cifar middle cifar trained data augmentation bottom adversarial training. figure left original test image classiﬁer generate corresponding adversarial sample changes classiﬁer decision note cross-lipschitz regularization decision makes sense whereas neural network models change small decision clearly wrong. figure left original test image classiﬁer generate corresponding adversarial sample changes classiﬁer decision note kernel methods decision makes sense whereas neural network models change small decision clearly wrong. figure left original test image classiﬁer generate corresponding adversarial sample changes classiﬁer decision note kernel methods decision makes sense whereas neural network models change small decision clearly wrong. figure left original test image classiﬁer generate corresponding adversarial sample changes classiﬁer decision note kernel methods decision makes sense whereas neural network models change small decision clearly wrong. figure left original test image classiﬁer generate corresponding adversarial sample changes classiﬁer decision note kernel methods decision makes sense whereas neural network models change small decision clearly wrong. figure left original test image classiﬁer generate corresponding adversarial sample changes classiﬁer decision note kernel methods decision makes sense whereas neural network models change small decision clearly wrong. figure left original test image classiﬁer generate corresponding adversarial sample changes classiﬁer decision note kernel methods decision makes sense whereas neural network models change small decision clearly wrong. figure left original test image classiﬁer generate corresponding adversarial sample changes classiﬁer decision note kernel methods decision makes sense whereas neural network models change small decision clearly wrong. figure left adversarial resistance l-norm test german traﬃc sign benchmark plain setting. right average robustness guarantee test l-norm test gtsb diﬀerent neural networks hyperparameters. dropout performs well terms performance robustness. figure left original test image classiﬁer generate corresponding adversarial sample changes classiﬁer decision note kernel methods decision makes sense whereas neural network models change small decision clearly wrong. german traﬃc sign benchmark third dataset used german traﬃc sign benchmark consists images german traﬃc signs classes training test samples. results shown figure dataset cross-lipschitz regularization improves upper bounds compared weight decay dropout achieves signiﬁcantly better prediction performance similar upper bounds. robustness guarantees weight decay cross-lipschitz slightly better dropout. residual networks experiments done hidden layer neural networks evaluate lower upper bounds. want demonstrate cross-lipschitz regularization also successfully used deep networks. residual networks proposed parameter layers non-bottleneck residual blocks. follow basically setting apart subtract per-pixel mean images random crop without padding similar train epochs learning rate divided epochs. experiments dropout followed recommendation inserting dropout layer convolutional layers inside residual block. cross-lipschitz regularization automatic diﬀerentiation tensorflow calculate derivative respect input slows done training factor plain setting learning rate methods chosen except runs without regularization weight decay regularization parameter chosen cross-lipschitz dropout probabilities data augmentation setting diﬀerence higher learning rates figure results cifar residual network diﬀerent regularizers. lower bounds hidden layer networks show upper bounds adversarial resistance. left data augmentation similar right plain setting regularization weight decay cross-lipschitz results shown figure cross-lipschitz regularization improves upper bounds robustness adversarial manipulation compare weight decay dropout factor plain setting data augmentation comes price slightly worse test performance. however shows cross-lipschitz regularization also eﬀective deep neural networks. remains interesting future work come also interesting instance-speciﬁc lower bounds deep neural networks. outlook formal guarantees machine learning systems becoming increasingly important used safety-critical systems. think research robustness guarantees whereas current research focused attacks argued instance-speciﬁc guarantees using local cross-lipschitz constant eﬀective using global leads lower bounds times better. major open problem come tight lower bounds deep networks. references abadi agarwal barham brevdo chen citro corrado davis dean devin ghemawat goodfellow harp irving isard józefowicz kaiser kudlur levenberg mané monga moore murray olah schuster shlens steiner sutskever talwar tucker vanhoucke vasudevan viégas vinyals warden wattenberg wicke zheng. tensorﬂow large-scale machine learning heterogeneous distributed systems", "year": 2017}