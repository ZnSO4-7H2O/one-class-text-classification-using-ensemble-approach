{"title": "Variants of RMSProp and Adagrad with Logarithmic Regret Bounds", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "stat.ML"], "abstract": "Adaptive gradient methods have become recently very popular, in particular as they have been shown to be useful in the training of deep neural networks. In this paper we have analyzed RMSProp, originally proposed for the training of deep neural networks, in the context of online convex optimization and show $\\sqrt{T}$-type regret bounds. Moreover, we propose two variants SC-Adagrad and SC-RMSProp for which we show logarithmic regret bounds for strongly convex functions. Finally, we demonstrate in the experiments that these new variants outperform other adaptive gradient techniques or stochastic gradient descent in the optimization of strongly convex functions as well as in training of deep neural networks.", "text": "goal paper twofold. first propose scadagrad variant adagrad adapted strongly convex case. show sc-adagrad achieves logarithmic regret bound case strongly convex functions data-dependent. known bounds much better practice data independent bounds second analyze rmsprop become standard methods train neural networks beyond stochastic gradient descent. show conditions weighting scheme rmsprop algorithm regret bound. fact achieves data-dependent said work. reason sc-adagrad comes along damping factor prevents potentially large steps beginning iterations. however analysis shows damping factor rather large initially prevent large steps monotonically decreasing function iterations order stay adaptive. finally show experiments three datasets methods competitive outperform adaptive gradient techniques well stochastic gradient descent strongly convex optimization problem terms regret training objective also perform well training deep neural networks show results different networks datasets. adaptive gradient methods become recently popular particular shown useful training deep neural networks. paper analyzed rmsprop originally proposed training deep neural networks context on-type reline convex optimization show gret bounds. moreover propose variants sc-adagrad sc-rmsprop show logarithmic regret bounds strongly convex functions. finally demonstrate experiments variants outperform adaptive gradient techniques stochastic gradient descent optimization strongly convex functions well training deep neural networks. recently work adaptive gradient algorithms adagrad rmsprop adadelta adam original idea adagrad parameter speciﬁc learning rate analyzing gradients observed optimization turned useful online convex optimization also training deep neural networks. original analysis adagrad limited case convex functions obtained datadependent regret bound order known optimal class. however learning problems structure sense optimizes restricted class strongly convex functions. shown achieve much better logarithmic regret bounds class strongly convex functions. department mathematics computer science saarland university germany imprs-cs planck institute informatics saarbr¨ucken germany correspondence mahesh chandra mukkamala <mmahesh.chandragmail.com>. paper analyze online convex optimization setting convex round access -gradient continuous convex function t-th iterate predict suffer loss goal perform well respect optimal decision hindsight deﬁned note standard euclidean inner product becomes xiyi general notation matrices comparison literature. positive deﬁnite matrices paper diagonal matrices computational effort computing inner products norms still linear cauchyschwarz inequality becomes introduce element-wise product vectors. aibi rd×d symmetric positive deﬁnite matrix convex set. deﬁne weighted projection thus alternative point view adagrad decaying stepsize correction term becomes running average squared derivatives plus vanishing damping term. however effective stepsize decay faster logarithmic regret bound strongly convex case. analyze next section propose sc-adagrad strongly convex functions. again denominator running average observed gradients decaying damping factor. effective stepsize order scadagrad. formal method presented algorithm derived difference adagrad scadagrad deﬁnition diagonal matrix note also deﬁned damping factor function also different standard adagrad. constant adagrad mainly introduced numerical reasons order avoid problems small components ﬁrst iterations typically chosen quite small e.g. scadagrad situation different. ﬁrst components small order update become extremely large chosen step-size scheme online projected gradigradient descent method achieves optimal regret bound strongly-convex problems consider adagrad next subsection popular adaptive alternative online projected gradient descent. section brieﬂy recall main result adagrad. algorithm adagrad given algorithm adversarial allowed choose possible convex functions adagrad achieves regret bound order shown regret bound known optimal class e.g. better comparison results rmsprop recall result notation. purpose introduce notation i-th component gradient function evaluated theorem assumptions hold sequence generated sc-adagrad algorithm arbitrary µ-strongly convex function stepsize fulﬁlls maxi=...d furthermore δt−i∀t regret scadagrad upper bounded small. would make method unstable would lead huge constants bounds. probably modiﬁcation adagrad drops square-root work. good choice initially roughly order starts grow. potential decay scheme satisﬁes properties sufﬁciently large chosen order also achieve constant decay scheme come back choice proof. following provide regret analysis sc-adagrad show optimal logarithmic regret bound achieved. however data-dependent typically signiﬁcantly better practice data-independent bounds. note initial work sc-adagrad using constant decay scheme already proposed also shown regret bounds strongly convex problems. unfortunately unaware work created sc-adagrad wasn’t mentioned journal version recently related work done constant decay scheme gives uniﬁed analysis adaptive algorithms results regarding logarithmic regret bounds closely match scadagrad contribution decay scheme vectorized need restrict constant scheme. require mild condition non-increasing element-wise order achieve logarithmic regret bounds. last step equality diagonal matrices. choose diag diag diag diag diag since diag− diag diag round difference subsequent squares sub-gradients bounded also algorithm δt−i∀t hence diag diag hence choosing diag diag diag∀t αdiag diag yields diag second inequality bounded tr). second last step lemma constant hence proving result ζ-strongly convex functions choosing obtain results µ-strongly convex functions. seen setting note ﬁrst last term regret bound upper bounded constants. second term depends note monotonically decreasing second term order thus logarithmic regret bound. bound data-dependent sense depends observed sequence gradients much tighter data-independent bound. bound includes also case non-decaying damping factor rather large constant damping factor work well noticed best results obtained decay scheme experiments. note decay scheme adaptive speciﬁc dimension thus increases adaptivity overall algorithm. completeness also give bound specialized decay scheme. rmsprop popular adaptive gradient algorithms used training deep neural networks used frequently computer vision e.g. train latest inceptionv network note rmsprop outperformed adaptive methods like adagrad order adadelta well momentum large number tests argued changes parameter update approximately gaussian distributed matrix seen preconditioner approximates diagonal hessian however fair despite huge empirical success practice ﬁrst analysis literature rigorous theoretical analysis rmsprop. analyze rmsprop given algorithm framework online convex optimization. first show rmsprop reduces adagrad certain choice parameters. second prove general convex case regret bound similar bound given theorem turns convergence analysis requires update weighted cumulative squared gradients hold contrast original suggestion choose turn later experiments constant choice leads sometimes divergence sequence whereas choice derived theoretical analysis always leads convergent scheme even applied deep neural networks. thus think analysis following interesting convex case give valuable hints parameters rmsprop chosen deep learning. unfortunately obvious regret bound decaying damping factor better constant damping factor. note however third term regret bound theorem negative. thus remains uses additionally stepsize scheme recover update scheme adagrad particular case rmsprop. aware correspondence adagrad rmsprop observed before. theorem assumptions hold sequence generated sc-rmsprop algorithm arbitrary µstrongly convex function furthermore assume δt−i∀t regret scrmsprop upper bounded used lemma last inequality note regret bound reduces scadagrad. comparison bounds straightforward terms cannot compared. interesting future research question whether possible show scheme better potentially dependent problem characteristics. idea experiments show proposed algorithms useful standard learning problems online batch settings. aware fact strongly convex case online batch conversion tight however necessarily imply algorithms behave generally suboptimal. compare algorithms strongly convex problem present relative suboptimality plots global optimum well separate regret plots compare best optimal parameter hindsight fraction training points seen far. hand rmsprop originally developed usage deep learning. discussed ﬁxed choice regret allowed wants optimal datasets three datasets easy difﬁcult difﬁcult achieve good test performance order inﬂuences performance. purpose mnist cifar cifar refer details cifar datasets. algorithms compare stochastic gradient descent decaying step-size strongly convex problems non-convex problems constant learning rate adam used step size decay strongly convex problems non-convex problems constant step-size. adagrad algorithm remains strongly convex problems non-convex problems. rmsprop proposed used strongly convex problems non-convex problems rmsprop used step-size decay order parameter range similar original methods initialized zero weights. regularization parameter chosen achieves best prediction performance test set. results shown figure also conduct experiments online setting restrict number iterations number training samples. algorithms choose stepsize resulting best regret value end. plot regret dataset proportion seen expected sc-adagrad sc-rmsprop outperform methods across considered datasets. also rmsprop lower regret values original rmsprop shown figure convolutional neural networks test -layer convolutional fully connected layer activation function relu last convolutional layer max-pooling window dropout. ﬁnal layer softmax layer ﬁnal objective cross-entropy loss. pretty rmsprop experiment equivalent adagrad) sc-rmsprop used stepsize rmsprop sc-adagrad used constant stepsize decaying damping factor sc-adagrad sc-rmsprop used convex problems non-convex deep learning problems. finally numerical stability parameter used adagrad adam rmsprop typically recommended algorithms. setup note methods varying parameter stepsize choose experiments. setup method advantage hyperparameters optimize. optimal rate always chosen algorithm separately achieves either best training objective best test performance ﬁxed number epochs. strongly convex case softmax regression given training data linear model cross entropy loss regularization squared euclidean norm weight parameters. simple standard architecture datasets. results shown figures sc-rmsprop competitive terms training objective datasets though achieves best performance. sc-adagrad competitive reason seems numerical stability parameter small. rmsprop diverges cifar dataset whereas rmsprop converges datasets similar performance adagrad terms training objective. rmsprop scadagrad perform better methods terms test accuracy cifar dataset. cifar mnist datasets sc-rmsprop competitive. multi-layer perceptron also conduct experiments -layer multi-layer perceptron fully connected hidden layers softmax layer according number classes dataset. ﬁrst hidden layers units layer relu activation function dropout. ﬁnal layer softmax layer. report results figures datasets scadagrad sc-rmsprop perform better terms test accuracy also best training objective performance cifar dataset. mnist dataset adagrad rmsprop achieves best training objective performance however sc-adagrad sc-rmsprop eventually performs good adagrad. here performance competitive adagrad numerical stability decay parameter sc-adagrad sc-rmsprop prohibitive. residual network also conduct experiments resnet- network proposed residual blocks used modiﬁcations proposed cifar dataset. report results figures sc-adagrad sc-rmsprop rmsprop best performance terms test accuracy rmsprop best performance terms training objective along adagrad. analyzed rmsprop originally proposed deep learning community framework online convex optimization. show conditions convergence rmsprop convex case different used leads better performance practice. also propose variants sc-adagrad sc-rmsprop achieve logarithmic regret bounds strongly convex case. moreover perform well different network models datasets thus interesting alternative existing adaptive gradient schemes. future want explore algorithms perform well deep learning tasks even though designed strongly convex case.", "year": 2017}