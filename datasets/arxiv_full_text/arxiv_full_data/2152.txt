{"title": "Deep unsupervised learning through spatial contrasting", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Convolutional networks have marked their place over the last few years as the best performing model for various visual tasks. They are, however, most suited for supervised learning from large amounts of labeled data. Previous attempts have been made to use unlabeled data to improve model performance by applying unsupervised techniques. These attempts require different architectures and training methods. In this work we present a novel approach for unsupervised training of Convolutional networks that is based on contrasting between spatial regions within images. This criterion can be employed within conventional neural networks and trained using standard techniques such as SGD and back-propagation, thus complementing supervised methods.", "text": "convolutional networks marked place last years best performing model various visual tasks. however suited supervised learning large amounts labeled data. previous attempts made unlabeled data improve model performance applying unsupervised techniques. attempts require different architectures training methods. work present novel approach unsupervised training convolutional networks based contrasting spatial regions within images. criterion employed within conventional neural networks trained using standard techniques back-propagation thus complementing supervised methods. past years convolutional networks lecun proven successful model vision related tasks krizhevsky mnih pinheiro razavian convolutional network composed multiple convolutional pooling layers followed fully-connected afﬁne transformations. neural network models layer typically followed non-linearity transformation rectiﬁed-linear unit convolutional layer applied cross correlating image trainable weight ﬁlter. stems assumption stationarity natural images means features learned local region image shared regions images. deep learning models including convolutional networks usually trained supervised manner requiring large amounts labeled data almost modern applications. models optimized variant stochastic-gradient-descent batches images sampled whole training dataset ground truth-labels. gradient estimation optimized parameters done back propagating objective error ﬁnal layer towards input. commonly known backpropagation rumelhart al.. early well known usage unsupervised training deep architectures part pretraining procedure used obtaining effective initial state model. network later ﬁne-tuned supervised manner displayed hinton unsupervised pre-training procedures later abandoned since provided apparent beneﬁt initialization heuristics careful fully supervised training regimes. de-facto almost exclusive usage neural networks supervised environments. work present novel unsupervised learning criterion convolutional network based comparison features extracted regions within images. experiments indicate using unsupervised methods improve performance holy grail deep learning last couple years vast research efforts focused that. hereby give short overview popular recent methods tried tackle problem. autoencoders reconstruction loss probably popular models unsupervised learning using neural networks convnets particular. autoencoders transform inputs outputs least possible amount distortion. autoencoder constructed using encoder maps input hidden compressed representation followed decoder maps representation back input space. mathematically written following general form underlying encoder decoder contain trainable parameters tied together optimized predeﬁned criterion. encoder decoder different architectures including fully-connected neural networks convnets others. criterion used training reconstruction loss usually mean squared error original input reconstruction zeiler allows efﬁcient training procedure using aforementioned backpropagation techniques. years autoencoders gained fundamental role unsupervised learning many modiﬁcation classic architecture made. regularized latent representation sparse vincent substituted input noisy version thereof requiring model denoise reconstructing. kingma obtained promising results variational autoencoders variational autoencoder model inherits typical autoencoder architecture makes strong assumptions concerning distribution latent variables. variational approach latent representation learning results additional loss component speciﬁc training algorithm called stochastic gradient variational bayes assumes data generated directed graphical model require encoder learn approximation posterior distribution denote parameters encoder decoder. objective variational autoencoder case following form recently stacked denoising autoencoders architectures showed promising results semi-supervised unsupervised tasks. stacked what-where autoencoder zhao computes complementary variables enable reconstruction whenever layer implements many-to-one mapping. ladder networks rasmus lateral connections allow higher levels autoencoder focus invariant abstract features applying layer-wise cost function. exemplar networks unsupervised method introduced bydosovitskiy takes different approach task trains network discriminate pseudo-classes. pseudo-class formed applying multiple transformations randomly sampled image patch. number pseudo-classes size input samples. criterion ensures different input samples would distinguished providing robustness applied transformations. context prediction another method unsupervised learning context introduced doersch method uses auxiliary criterion predicting location image patch given another image. done classiﬁcation possible locations. adversarial generative models recently introduced model used unsupervised fashion goodfellow adversarial generative models uses networks trained discriminate data sampled true underlying distribution separate generative network trained adversary trying confuse ﬁrst network. propagating gradient paired networks model learns generate samples distributed similarly source data. shown radford this model create useful latent representations subsequent classiﬁcation tasks demonstrated sampling methods methods training models discriminate large number classes often noise contrasting criterion. methods roughly speaking posterior probability ground-truth target given model output input sampled true distribution maximized probability given noise measurement minimized. successfully used language domain learn unsupervised representation words. noteworthy case wordvec model introduced mikolov using setting language applications natural contrasting noise smooth approximation unigram distribution. suitable contrasting distribution less obvious data points sampled high dimensional continuous space case patches images. majority unsupervised optimization criteria currently used based variations reconstruction losses. limitation fact pixel level reconstruction non-compliant idea discriminative objective expected agnostic level information input. addition evident best suited measurement compare images example viewing possibly large square-error image single pixel shifted copy another problem recent approaches rasmus zeiler need extensively modify original convolutional network model. leads unsupervised method state-of-the-art supervised models classiﬁcation hurt future attempt reconcile uniﬁed framework also efﬁciently leverage unlabeled data otherwise supervised regimes. common train deﬁning loss function target values network output. learning comparison approaches supervised task different angle. main idea distance comparisons samples learn useful representations. example consider relative qualitative examples form closer using comparative measure neural network learn embedding space introduced siamese network framework bromley later used works chopra methods number classes large expected vary time case face veriﬁcation face contained image compared another image face. problem recently tackled schroff training convolutional network model triplets examples. there image served anchor additional pair images served positive example together negative example containing face different person. training objective embedded distance input faces distance anchor positive example adjusted smaller least constant negative distance. precisely loss function used case deﬁned max{f embedding predeﬁned margin constant. another similar model used hoffer ailon triplets comparisons classiﬁcation examples class trained lower embedded distance images distinct classes. work introduced concept distance ratio loss ﬂavor probability biased coin ﬂip. ‘pushing’ probability zero express objective pairs samples coming distinct classes less similar other compared pairs samples coming class. shown empirical balntas provide better feature embeddings margin based distance loss implicit assumption convolutional networks features gradually learned hierarchically level hierarchy corresponding layer network. spatial location within layer corresponds region original image. empirically observed deeper layers tend contain ‘abstract’ information image. intuitively features describing different regions within image likely semantically similar indeed corresponding deep representations tend similar. conversely regions probably unrelated images tend deep representation. logic commonly used modern deep networks szegedy global average pooling used aggregate spatial features ﬁnal layer used classiﬁcation. suggestion property often observed side effect supervised applications used desired objective learning deep representations unsupervised task. later resulting representation used typically done starting point supervised learning task. call idea formalize spatial contrasting. spatial contrasting criterion similar noise contrasting estimation gutmann hyv¨arinen mnih kavukcuoglu trying train model maximizing expected probability desired inputs minimizing contrasting sampled measurements. concern samples images patches taken image convolutional network model denoted extracts spatial features image patch wish optimize model features representing patches taken image conditional probability means features patch taken speciﬁc image effectively predict model features extracted patches image. conversely want model minimize patches taken distinct images. following logic presented before need sample contrasting patch different image order obtain contrasting samples regions random images training set. distance ratio described earlier supervised case represent probability feature vectors taken image. resulting training loss pair images deﬁned effectively minimizing log-probability softmax measure. formulation portrayed ﬁgure since sample contrasting sample underlying distribution evaluate loss considering image patch patch compared contrast symmetrically. ﬁnal loss average estimations since training convolutional network done batches images multiple samples batch train model. image serves source anchor positive patches corresponding features closer also source contrasting samples images batch. batch images samples image taken different distance comparisons made. ﬁnal loss average distance ratio images batch since criterion differentiable respect inputs fully compliant standard methods training convolutional network speciﬁcally using backpropagation gradient descent. furthermore applied layer network hierarchy. fact used multiple layers within convolutional network. spatial properties features means also sample feature space instead original image simplify implementation. complete algorithm batch training described algorithm also related batch normalization layer ioffe szegedy recent usage batch statistics neural networks. spatial contrasting also uses batch statistics sample contrasting patches. section report empirical results showing using loss unsupervised pretraining procedure improve state-of-the-art performance subsequent classiﬁcation. experimented mnist cifar- datasets. used modiﬁed versions well studied networks rasmus detailed description architecture found appendix experiments used spatial contrasting criterion train network unlabeled images. training done using initial learning rate decreased factor whenever measured loss stopped decreasing. convergence used trained model initialization supervised training complete labeled dataset. supervised training done following regime starting lower initial learning rate used mild data augmentations small translations horizontal mirroring. datasets used cifar well known cifar- image classiﬁcation benchmark dataset containing training images test images. image sizes pixels color. classes airplanes automobiles birds cats deer dogs frogs horses ships trucks. mnist mnist database handwritten digits studied dataset benchmark image classiﬁcation. dataset contains examples handwritten digits training additional examples testing. sample pixel gray level image. since comprised mostly unlabeled data suitable highlight beneﬁts spatial contrasting criterion. initial training unsupervised described earlier using entire samples representation outputted training used initialize supervised training labeled images. evaluation done separate test samples. comparing state results improvement test accuracy best model zhao setting best model test classiﬁcation accuracy. also compare network without initialization achieves lower classiﬁcation indication indeed managed leverage unlabeled examples provide better initialization point supervised model. model zero-bias convnets paine triplet network hoffer ailon exemplar convnets dosovitskiy target coding yang stacked what-where zhao spatial contrasting initialization model without initialization model convolutional k-means network coates view-invariant k-means dcgan radford exemplar convnets dosovitskiy ladder networks rasmus spatial contrasting initialization model without initialization cifar used previously used setting coates dosovitskiy test model’s ability learn unlabeled images. setting samples available used label annotation entire dataset used unsupervised learning. ﬁnal test accuracy measured entire test set. experiments trained model using criterion entire dataset used labeled samples class supervised regime initialized network. results compared previous efforts table using criterion allowed improvement non-initialized model achieved ﬁnal test accuracy competitive result current state-of-the-art model rasmus mnist dataset different nature cifar experimented earlier. biggest difference relevant work spatial regions sampled mnist images usually provide little information. fact much less suited mnist conjured little beneﬁt. still however experimented initializing model criterion continuing fully-supervised regime labeled examples. found provided beneﬁt training network without preinitialization improving results error test set. results compared previous attempts included work presented spatial contrasting novel unsupervised criterion training convolutional networks unlabeled data. based comparison spatial features sampled number images. we’ve shown empirically using spatial contrasting pretraining technique initialize convnet improve performance subsequent supervised training. cases unlabeled data available dataset translates state-of-the-art classiﬁcation accuracy ﬁnal model. model stacked what-where zhao triplet network hoffer ailon jarrett ladder networks rasmus dropconnect spatial contrasting initialization model without initialization since spatial contrasting loss differentiable estimation computed within network parallel supervised losses future work attempt embed semi-supervised model. usage allow create models leverage labeled unlabeled data compared similar semi-supervised models ladder network rasmus also apparent contrasting occur dimensions spatial straightforward temporal one. suggests similar training procedure applied segments sequences learn useful representation without explicit supervision. jane bromley james bentz l´eon bottou isabelle guyon yann lecun cliff moore eduard s¨ackinger roopak shah. signature veriﬁcation using siamese time delay neural network. international journal pattern recognition artiﬁcial intelligence sumit chopra raia hadsell yann lecun. learning similarity metric discriminatively application face veriﬁcation. computer vision pattern recognition cvpr ieee computer society conference volume ieee adam coates andrew honglak lee. analysis single-layer networks unsupervised feature learning. international conference artiﬁcial intelligence statistics carl doersch abhinav gupta alexei efros. unsupervised visual representation learning context prediction. proceedings ieee international conference computer vision goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems michael gutmann aapo hyv¨arinen. noise-contrastive estimation estimation principle unnormalized statistical models. international conference artiﬁcial intelligence statistics sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. proceedings international conference machine learning alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems tomas mikolov ilya sutskever chen greg corrado jeff dean. distributed representations words phrases compositionality. advances neural information processing systems volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature razavian hossein azizpour josephine sullivan stefan carlsson. features off-theshelf astounding baseline recognition. proceedings ieee conference computer vision pattern recognition workshops christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. proceedings ieee conference computer vision pattern recognition pascal vincent hugo larochelle yoshua bengio pierre-antoine manzagol. extracting composing robust features denoising autoencoders. proceedings international conference machine learning matthew zeiler sixin zhang yann fergus. regularization neural networks using dropconnect. proceedings international conference machine learning input conv. relu conv. relu conv. relu max-pooling stride conv. relu conv. relu conv. relu max-pooling stride conv. relu conv. relu conv. relu conv. relu max-pooling stride dropout conv. relu dropout fully-connected model cifar- input conv. leakyrelu conv. leakyrelu conv. leakyrelu max-pooling stride conv. leakyrelu conv. relu conv. leakyrelu conv. relu conv. leakyrelu max-pooling stride", "year": 2016}