{"title": "Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks", "tag": ["cs.LG", "cs.AI"], "abstract": "Deep learning classifiers are known to be inherently vulnerable to manipulation by intentionally perturbed inputs, named adversarial examples. In this work, we establish that reinforcement learning techniques based on Deep Q-Networks (DQNs) are also vulnerable to adversarial input perturbations, and verify the transferability of adversarial examples across different DQN models. Furthermore, we present a novel class of attacks based on this vulnerability that enable policy manipulation and induction in the learning process of DQNs. We propose an attack mechanism that exploits the transferability of adversarial examples to implement policy induction attacks on DQNs, and demonstrate its efficacy and impact through experimental study of a game-learning scenario.", "text": "abstract. deep learning classiﬁers known inherently vulnerable manipulation intentionally perturbed inputs named adversarial examples. work establish reinforcement learning techniques based deep q-networks also vulnerable adversarial input perturbations verify transferability adversarial examples across diﬀerent models. furthermore present novel class attacks based vulnerability enable policy manipulation induction learning process dqns. propose attack mechanism exploits transferability adversarial examples implement policy induction attacks dqns demonstrate eﬃcacy impact experimental study game-learning scenario. inspired psychological neuroscientiﬁc models natural learning reinforcement learning techniques optimize actions intelligent agents complex environments learning eﬀective controls reactions maximize long-term reward agents. applications range combinatorial search problems learning play games autonomous navigation multi-agent systems optimal control however classic techniques generally rely hand-crafted representations sensory input thus limiting performance complex high-dimensional real world environments. overcome limitation recent developments combine techniques signiﬁcant feature extraction processing capabilities deep learning models framework known deep q-network approach exploits deep neural networks feature selection q-function approximation hence enabling unprecedented performance complex settings learning eﬃcient playing strategies unlabeled video frames atari games robotic manipulation autonomous navigation aerial ground vehicles growing interest application dqns critical systems necessitate investigation framework regards resilience robustness adversarial attacks integrity reinforcement learning processes. reliance interactions environment gives rise inherent vulnerability makes process learning susceptible perturbation result changes observable environment. exploiting vulnerability provides adversaries means disrupt change control policies leading unintended potentially harmful actions. instance manipulation obstacle avoidance navigation policies learned autonomous unmanned aerial vehicles enables adversary systems kinetic weapons inducing actions lead intentional collisions. paper study eﬃcacy impact policy induction attacks deep q-learning framework. propose novel attack methodology based adversarial example attacks deep learning models experimental results verify similar classiﬁers networks also vulnerable adversarial examples conﬁrm transferability examples diﬀerent models. evaluate proposed attack methodology original architecture mnih results verify feasibility policy induction attacks incurring minimal perturbations environment sensory inputs system. also discuss insuﬃciency defensive distillation adversarial training techniques state countermeasures proposed adversarial example attacks deep learning classiﬁers present potential techniques mitigate eﬀect policy induction attacks dqns. remainder paper organized follows section presents overview q-learning deep q-networks adversarial examples. section formalizes problem deﬁnes target attacker models. section outline attack methodology algorithm followed experimental evaluation proposed methodology section high-level discussion eﬀectiveness current countermeasures presented section paper concluded section remarks future research directions. generic problem formally modeled markov decision process described tuple reachable states process available actions mapping transitions immediate reward represents transition probabilities. given time-step state agent’s choice action time causes transition state according transition probability stst+a agent receives reward choosing action state objective optimal policy maximizes cumulative reward time time denoted return function pt′=t −trt′ discount factor representing diminishing worth rewards obtained time hence ensuring bounded. approach problem estimate optimal value action deﬁned expected future rewards taking action following optimal policy thereafter. value action state given action-value function deﬁned q-learning method estimates optimal action policies using bellman equation iterative update value iteration technique. practical implementation q-learning commonly based function approximation parametrized q-function common technique approximating parametrized non-linear q-function train neural network whose weights correspond neural networks commonly referred q-networks trained every iteration minimizes loss function probability distribution states actions optimization problem typically solved using computationally eﬃcient techniques stochastic gradient descent classical q-networks present number major disadvantages q-learning process. first sequential processing consecutive observations breaks requirement training data successive samples correlated. furthermore slight changes q-values leads rapid changes policy estimated qnetwork thus enabling policy oscillations. also since scale rewards q-values unknown gradients q-networks suﬃciently large render backpropagation process unstable. deep network multi-layered q-network designed mitigate disadvantages. overcome issue correlation consecutive observations employs technique named experience replay instead training successive observations experience replay samples random batch previous observations stored replay memory train result correlation successive training samples broken setting re-established. order avoid oscillations ﬁxes parameters optimization target parameters updated regulat intervals adopting current weights q-network. issue unstability backpropagation also solved clipping reward values range thus preventing q-values becoming large. mnih demonstrate application q-network technique end-to-end learning values playing atari games based observations pixel values game environtment. neural network architecture work depicted ﬁgure capture movements game environment mnih stacks consecutive image frames input network. train network random batch sampled previous observation tuples observation processed layers convolutional neural networks learn features input images employed feed-forward layers approximate q-function. target network parameters synchronized parameters original network ﬁxed periods intervals. i.e. every iteration kept ﬁxed next synchronization. target value optimization learning thus becomes szegedy report intriguing discovery several machine learning models including deep neural networks vulnerable adversarial examples. machine learning models misclassify inputs slightly diﬀerent correctly classiﬁed samples drawn data distribution. furthermore wide variety models diﬀerent architectures trained diﬀerent subsets training data misclassify adversarial example. suggests adversarial examples expose fundamental blind spots machine learning algorithms. issue stated follows consider machine learning system benign input sample correctly classiﬁed machine learning system i.e. ytrue. according report szegedy many proceeding studies possible construct adversarial example perceptually indistinguishable classiﬁed incorrectly i.e. ytrue. adversarial examples misclassiﬁed often examples perturbed random noise even magnitude noise much larger magnitude adversarial perturbation according objective adversaries adversarial example attacks generally classiﬁed following categories generate adversarial examples several algorithms proposed fast gradient sign method goodfellow jacobian saliency algorithm approach papernot grounding assumption many crafting algorithms attacker complete knowledge target neural networks architecture weights hyperparameters. recently papernot proposed ﬁrst black-box approach generating adversarial examples. method exploits generalized nature adversarial examples adversarial example generated neural network classiﬁer applies neural network classiﬁers perform classiﬁcation task regardless architecture parameters even distribution training data. accordingly approach based generating replica target network. train replica attacker creates trains dataset mixture samples obtained observing target’s performance synthetically generated inputs label pairs. trained adversarial example crafting algorithms require knowledge target network applied replica. transferability adversarial examples perturbed samples generated replica network induce misclassiﬁcations many networks perform task. following sections describe similar approach adopted policy induction attacks dqns. consider attacker whose goal perturb optimality actions taken learner inducing arbitrary policy πadv target dqn. attacker assumed minimal priori information target type format inputs well reward function estimate frequency updating network. noteworthy even target’s reward function known estimated inverse reinforcement learning techniques knowledge target’s exact architecture considered work attacker estimate architecture based conventions applied input type model attacker direct inﬂuence target’s architecture parameters including reward function optimization mechanism. parameter attacker directly manipulate conﬁguration environment observed target. instance case video game learning attacker capable changing pixel values game’s frames score. cyber-physical scenarios perturbations implemented strategic rearrangement objects precise illumination certain areas tools laser pointers. assume attacker capable changing state observed target either predicting future states states generated environment’s dynamics. latter achieved attacker faster action speed target’s sampling rate inducing delay generation environment observation target. avoid detection minimize inﬂuence environment’s dynamics impose extra constraint attack magnitude perturbations applied conﬁguration must smaller value denoted also limit attacker’s domain perturbations discussed section framework mnih seen consisting neural networks native network performs image classiﬁcation function approximation auxiliary network whose architecture parameters copies native network sampled every iterations. training performed optimizing loss function equation stochastic gradient descent similarity process training mechanism neural network classiﬁers hypothesize function approximators also vulnerable adversarial example attacks. words possible inputs approximated functions contains elements cause approximated functions generate outputs diﬀerent output original function. furthermore hypothesize similar case classiﬁers elements cause generate incorrect values incur eﬀect dqns approximate q-function. consequently attacker manipulate dqn’s learning process crafting states identiﬁes incorrect choice optimal action st+. attacker capable crafting adversarial inputs value equation minimized speciﬁc action policy learned time-step optimized towards suggesting optimal action given state considering attacker aware target’s network architecture parameters every time step crafting adversarial states must rely black-box techniques introduced attacker exploit transferability adversarial examples obtaining state perturbations replica target’s dqn. every time step training replica attacker calculates perturbation vectors ˆδt+ next state maxa′ causes generate maximum i.e. maximum reward next state obtained optimal action taken state determined attacker’s policy. procedurally similar targeted misclassiﬁcation attacks described section minimal perturbations input sample classiﬁer assigns maximum value likelihood incorrect target class. therefore adversarial example crafting techniques developed classiﬁers fast gradient sign method jacobian saliency algorithm applied obtain perturbation vector ˆδt+. procedure attack divided phases initialization exploitation. initialization phase implements processes must performed target begins interacting environment exploitation phase implements attack processes crafting adversarial inputs. phase constitutes attack cycle depicted ﬁgure cycle initiates attacker’s ﬁrst observation environment runs tandem target’s operation. algorithm details procedural phase. study performance eﬃcacy proposed mechanism examine targeting mnih al.’s designed learn atari games setup train network game pong implemented python using pygame library game played opponent modest level heuristic artiﬁcial intelligence customized handle delays dqn’s reaction training process. game’s backened provides agent game screen sampled well game score throughout episode game. available actions stand} enables agent control movements paddle. figure illustrates game screen pong used experiments. training process implemented tensorflow executed amazon g.xlarge instance intel xeon cores nvidia cuda cores video memory. state observed stack consecutive gray-scale game frames. similar original architecture mnih input ﬁrst passed convolutional layers extract compressed feature space following feed-forward layers function estimation. discount factor initial probability taking random action annealed every actions. agent also following threat model presented section experiment considers attacker capable observing states interactions target game domain inﬂuence limited implementation minor changes environment. considering visual representation environment setup minor changes incurred attacker take form perturbing pixel values consecutive frames given state. successful implementations proposed policy induction attack mechanisms rely vulnerability dqns targeted adversarial perturbations. verify existence vulnerability networks target sampled regular intervals training game environment. next step observations comprised pair consecutive states randomly selected experience memory ensure possibility occurrence game. considering variable manipulated attacker passed along model adversarial example crafting algorithms. study extent vulnerability evaluated success rate fgsm jsma algorithms random observations inducing random game action current optimal results presented figure verify dqns indeed vulnerable adversarial example attacks. noteworthy success rate fgsm ﬁxed perturbation limit decreases percent observations number observations increases. jsma seems robust eﬀect maintains success rate percent throughout experiment. measure transferability adversarial examples models trained another q-network similar architecture experience memory game sampled instances previous experiment. noteworthy random initializations exploration mechanism stochastic nature even similar q-networks trained observations obtain diﬀerent sets weights. second q-network tested measure vulnerability adversarial examples obtained last experiment. figure shows perturbations ﬁnal experiment tests performance proposed exploitation mechanism. experiment consider adversary whose reward value exact opposite game score meaning aims devise policy maximizes number lost games. obtain policy trained adversarial game whose reward value negative value obtained target dqn’s reward function. adversarial policy hand target setup train game environment maximize original reward function. game environment modiﬁed allow perturbation pixel values game frames adversary. second also setup train target’s observations provide estimation target enable blackbox crafting adversarial example. every observation adversarial policy obtained initialization phase consulted calculate action would satisfy adversary’s goal. then jsma algorithm utilized generate adversarial example would cause output replica network action selected adversarial policy. example passed target observation. figure compares performance unperturbed attacked dqns terms reward values measured diﬀerence current game score average score. seen reward value targeted agent rapidly falls unperturbed case maintains trend losing game throughout experiment. result conﬁrms eﬃcacy proposed attack mechanism veriﬁes vulnerability deep q-networks policy induction attacks. since introduction adversarial examples szgedey various counter-measures proposed mitigate exploitation vulnerability deep neural networks. goodfellow proposed retrain deep networks minimally perturbed adversarial examples prevent misclassiﬁcation. approach suﬀers inherent short-comings firstly aims increase amount perturbations required craft adversarial example. second approach provide comprehensive counter-measure computationally ineﬃcient possible adversarial examples. furthermore papernot argue training network adversarial examples emerging network adversarial examples hence technique solve problem exploiting vulnerability critical systems. consequently papernot proposed technique named defensive distillation also based retraining network dimensionally-reduced training data. approach recently shown insuﬃcient mitigating adversarial examples hence concluded current state countering adversarial examples exploitation incapable providing concrete defense exploitations. context policy induction attacks conjecture temporal features training process utilized provide protection mechanisms. proposed attack mechanism relies assumption decreasing chance random actions target likely perform action induced adversarial inputs number iterations progress. mitigated implementing adaptive exploration-exploitation mechanisms increase decrease chance random actions according performance trained model. also possible exploit spatio-temporal pattern recognition techniques detect omit regular perturbations pre-processing phase learning process. investigating techniques priority future work. established vulnerability reinforcement learning based deep qnetworks policy induction attacks. furthermore proposed attack mechanism exploits vulnerability deep neural networks adversarial examples demonstrated eﬃcacy impact experiments game-learning dqn. preliminary work solicitates wide-range studies security deep reinforcement learning. discussed section novel countermeasures need investigated mitigate eﬀect attacks dqns deployed cyber-physical critical systems. also analytical treatment problem establish bounds relationships model parameters network architecture exploration mechanisms dqn’s vulnerability policy induction provide deeper insight guidelines designing safe secure deep reinforcement learning architectures.", "year": 2017}