{"title": "Optimistic Initialization and Greediness Lead to Polynomial Time  Learning in Factored MDPs - Extended Version", "tag": ["cs.AI", "cs.LG"], "abstract": "In this paper we propose an algorithm for polynomial-time reinforcement learning in factored Markov decision processes (FMDPs). The factored optimistic initial model (FOIM) algorithm, maintains an empirical model of the FMDP in a conventional way, and always follows a greedy policy with respect to its model. The only trick of the algorithm is that the model is initialized optimistically. We prove that with suitable initialization (i) FOIM converges to the fixed point of approximate value iteration (AVI); (ii) the number of steps when the agent makes non-near-optimal decisions (with respect to the solution of AVI) is polynomial in all relevant quantities; (iii) the per-step costs of the algorithm are also polynomial. To our best knowledge, FOIM is the first algorithm with these properties. This extended version contains the rigorous proofs of the main theorem. A version of this paper appeared in ICML'09.", "text": "abstract. paper propose algorithm polynomial-time reinforcement learning factored markov decision processes factored optimistic initial model algorithm maintains empirical model fmdp conventional always follows greedy policy respect model. trick algorithm model initialized optimistically. prove suitable initialization foim converges ﬁxed point approximate value iteration number steps agent makes non-near-optimal decisions polynomial relevant quantities; per-step costs algorithm also polynomial. best knowledge foim ﬁrst algorithm properties. extended version contains rigorous proofs main theorem. version paper appeared icml’. factored markov decision processes practical ways compactly formulate sequential decision problems—provided ways solve them. environment unknown eﬀective reinforcement learning methods apply form optimism face uncertainty principle whenever learning agent faces unknown assume high rewards order encourage exploration. factored optimistic initial model takes principle extreme model initialized overly optimistic. often visited areas state space model gradually gets realistic inspiring agent head unknown regions explore them search imaginary garden eden. working algorithm simple extreme make explicit eﬀort balance exploration exploitation always follows greedy optimal policy respect model. show paper simple trick suﬃcient eﬀective fmdp learning. algorithm extension sample-eﬃcient learning algorithm mdps. important diﬀerence however model solved. every time model updated corresponding value function needs re-calculated mdps problem various dynamic programming-based algorithms solve model required accuracy polynomial time. situation less bright generating near-optimal fmdp solutions currently known algorithms take exponential time e.g. approximate policy iteration using decision-tree representations policies solving exponential-size ﬂattened version fmdp. require polynomial running time accept sub-optimal solutions. known example polynomial-time fmdp planner factored value iteration serve base planner learning method. planner guaranteed converge error solution bounded term depending quality function approximators. analysis algorithm follow established techniques analyzing sample-eﬃcient reinforcement learning mdps fmdps). however listed proofs convergence rely critically access near-optimal planner generalized suitably. able show foim converges bounded-error solution polynomial time high probability. introduce basic concepts notations section section review existing work special emphasis immediate ancestors method. sections describe blocks foim foim algorithm respectively. ﬁnish paper short analysis discussion. dimensional vectors reals indexed states. vector corresponding denoted value state similarly deﬁne -dimensional column vector entries matrix entries assume reward function local-scope functions also assume variable exist neighborhood sets value depends action taken. write transition probabilities factored form planning known fmdps. decision trees provide represent agent’s policy compactly. present algorithms evaluate improve policies according policy iteration scheme. unfortunately size policies grow exponentially even decision tree representation constraints. objective function constraints written compact forms exploiting local-scope property appearing functions. show maximum exponentially many local-scope functions computed rephrasing task non-serial dynamic programming task eliminating variables one. therefore equations transformed equivalent compact linear program. gain exponential necessarily cases. furthermore solutions optimal function approximation; best proved bounded error optimum approximate policy iteration algorithm also uses approximate reformulation based policy-evaluation bellman equations. policyevaluation equations however linear contain maximum operator need costly transformation step. hand algorithm needs explicit decision tree representation policy. shown size decision tree representation grow exponentially. furthermore convergence properties algorithms unknown. factored value iteration also approximates value function linear combination basis functions uses variant approximate value iteration projection operator modiﬁed avoid divergence. converges polynomial number steps solution sub-optimal. error solution bounded distance optimal value function bound depends quality function approximation. integral part foim described detail section reinforcement learning fmdps. reinforcement learning setting agent interacts fmdp environment unknown parameters. model-based approach agent learn structure fmdp transition probability factors reward factors unknown transitions. approaches assume structure fmdp reward functions known transition probabilities need learnt. examples include factored versions sample-eﬃcient model-based factored factored r-max factored mbie algorithms abovementioned algorithms polynomial sample complexity require polynomially many calls fmdp-planner. note however mentioned approaches require access planner able produce ǫ-optimal solutions date algorithm exists would accomplish accuracy polynomial time. also present algorithm exploration guided uncertainties linear programming solution. approach require access near-optimal planner formal performance bounds known. unknown rewards. typically asserted rewards approximated observations analogously transition probabilities. however reward composed multiple factors observe sums unknown quantities individual quantities themselves. date know eﬃcient approximation method learning factored rewards. wkhk. matrix mapping feature weights state values entries arbitrary linear mapping projecting state values feature weights. denote weight vector basis functions. known khgk∞ approximate bellman optimistic initial model mdps. number sampleeﬃcient learning algorithms mdps e.g. rmax mbie recently oim. underlying principle methods similar maintain approximate model environment. wherever uncertainty model parameters high models optimistic. agent encouraged explore unknown areas reducing uncertainty models. here shall extend factored environments. algorithm introduce hypothetical garden eden state agent gets large reward remains indeﬁnitely. model initialized fake experience according agent experienced ues. model continuously updated collected experience agent always takes greedy optimal action respect current model. well-explored pairs optimism model vanishes thus encouraging agent explore less-known areas. reason choosing twofold optimism model ensured initialization time that extra work needed ensure optimism model encourage exploration. results several standard benchmark mdps indicate superior algorithms mentioned. optimistic initial model factored mdps. learning process maintain approximations model particular transition probability factors. extend state factors hypothetical garden eden state seeing current state action taken transition model give probabilities various next states speciﬁcally factor transition model give probabilities various values given initially agent idea start overly optimistic model inject fake experience model taking action leads state component optimistic model encourage agent explore action whenever state consistent many visits weight initial fake experience shrink optimistic belief agent fades away. however time collected experience provides accurate components model optimistic indeed non-goe states value least γre/. note possible encode re-rewards states using original reward factors state factor reward factor local scope analysis. prove foim gets good possible. good possible? clearly cannot expect better policies planner would output parameters fmdp known. polynomial-running-time constraint planner able compute near-optimal solution. however prove foim gets ǫ-close theorem suppose agent following foim unknown fmdp reward components fall interval state factors probabilityreward-factors depend factors. initial values foim satisfy proof sketch. proof uses standard techniques literature sampleeﬃcient reinforcement learning. notably proof follows structure important diﬀerences compared previous approaches assume planner able output near-optimal solution foim make unbounded number model updates cannot make standard argument encountering ﬁnitely many diﬀerent models fails negligible probability whole algorithm fails negligible probability. instead careful analysis failure probability needed. rigorous proof found appendix. visit counts model accuracy. foim algorithm builds transition probability model keeping track visit counts state-action components state-action-state transition components first show state-action component visited many times corresponding known-state fmdp. state-action component called known timestep visited least times i.e. deﬁne known-component fmdp follows state action space rewards decompositions transition probabilities identical corresponding quantities true fmdp proximity value functions. rest proof standard give rough sketch only. deﬁne cutoﬀ horizon smaller probability escape show qfoim otherwise probability. however happen mnf|a|k times polynomial second case happen polynomial number times. foim conceptually simple exploration-exploitation dilemma resolved without explicit exploration action selection always greedy. model update model solution also least simple alternatives found literature. further foim favorable theoretical properties. foim ﬁrst example algorithm polynomial per-step computational complexity fmdps. achieve this relax near-optimality fmdp planner. particular planner used runs polynomial time reach bounded error looseness bound depends quality basis functions. almost time steps foim gets ǫ-close value practical point view calling fmdp model-solver iteration could prohibitive. however model value function usually change little single model update initialize previous value function iterations might suﬃcient. work supported nest perceptual consciousness explication testing grant contract opinions errors manuscript authors responsibility necessarily reﬂect opinions project members. ﬁrst author partially supported fulbright scholarship. note general hard-to-reach states visited long time only steps near-optimal polynomial number steps. issue analyzed deﬁned analogue probably approximately correctness mdps. following lemma almost identical corollary change allow diﬀerent diﬀerent components. original proof carries modiﬁcation unchanged manner omitted here. previous lemma bounds error single state. following corollary extends results showing probability large approximation error anywhere state space low. note number visit counts second term quantity maximum expressions dominates ﬁrst one. therefore visits component really feel conﬁdent known components known approximate transition probabilities fmdp y∈x˛˛˛bp kbvπ vπk∞ proof. −bvπk∞ maxa∈a‚‚‚p abvπ‚‚‚∞ =˛˛˛˛˛xy∈x bvπ˛˛˛˛˛ a∈a‚‚‚p abvπ‚‚‚∞ ‚‚‚‚‚ π`ra avπ´ hgxa∈a hgxa∈a a∈a‚‚‚p abvπ‚‚‚∞ a∈a‚‚‚p abvπ‚‚‚∞ γ˛˛˛˛˛xy∈x bvπ˛˛˛˛˛ γ˛˛˛˛˛xy∈xˆp ˜vπ˛˛˛˛˛ γxy∈x˛˛˛p ˛˛˛v γxy∈xbp −bvπk∞ proximity value functions. following show whenever algorithm remains known region fmdp value function close avi-optimal value functions related sequence value functions. ǫ-horizon time. given point execution algorithm denote event algorithm encounter unknown transition next steps. separate cases depending whether small large. firstly assume known-state fmdp deﬁned -known components. statement qfoim proof. known states identical collected rewards identical too. algorithm encounters unknown state-action", "year": 2009}