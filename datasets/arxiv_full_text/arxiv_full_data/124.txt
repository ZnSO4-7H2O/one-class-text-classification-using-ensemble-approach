{"title": "Stacked Attention Networks for Image Question Answering", "tag": ["cs.LG", "cs.CL", "cs.CV", "cs.NE"], "abstract": "This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images. SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer. We argue that image question answering (QA) often requires multiple steps of reasoning. Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively. Experiments conducted on four image QA data sets demonstrate that the proposed SANs significantly outperform previous state-of-the-art approaches. The visualization of the attention layers illustrates the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer.", "text": "visualization learned multiple attention layers. stacked attention network ﬁrst focuses referred concepts e.g. bicycle basket objects basket ﬁrst attention layer narrows focus second layer ﬁnds answer dog. dogs. answer question sitting basket bicycle need ﬁrst locate objects concepts referred question gradually rule irrelevant objects ﬁnally pinpoint region indicative infer answer paper propose stacked attention networks allow multi-step reasoning image sans viewed extension attention mechanism successfully applied image captioning machine translation overall architecture illustrated fig. consists three major components image model uses paper presents stacked attention networks learn answer natural language questions images. sans semantic representation question query search regions image related answer. argue image question answering often requires multiple steps reasoning. thus develop multiple-layer query image multiple times infer answer progressively. experiments conducted four image data sets demonstrate proposed sans signiﬁcantly outperform previous state-of-the-art approaches. visualization attention layers illustrates progress locates relevant visual clues lead answer question layer-by-layer. recent advancement computer vision natural language processing image question answering becomes active research areas unlike pure language based systems studied extensively community image systems designed automatically answer natural language questions according content reference image. recently proposed image models based neural networks commonly used approach extract global image feature vector using convolution neural network encode corresponding question feature vector using long short-term memory network combine infer answer. though impressive results reported models often fail give precise answers answers related ﬁne-grained regions image. examining image data sets often case answering question image requires multi-step reasoning. take question image fig. example. several objects image bicycles window street baskets extract high level image representations e.g. vector region image; question model uses lstm extract semantic vector question stacked attention model locates multi-step reasoning image regions relevant question answer prediction. illustrated fig. ﬁrst uses question vector query image vectors ﬁrst visual attention layer combine question vector retrieved image vectors form reﬁned query vector query image vectors second attention layer. higher-level attention layer gives sharper attention distribution focusing regions relevant answer. finally combine image features highest attention layer last query vector predict answer. main contributions work three-fold. first propose stacked attention network image tasks. second perform comprehensive evaluations four image benchmarks demonstrating proposed multiple-layer outperforms previous state-of-the-art approaches substantial margin. third perform detailed analysis visualize outputs different attention layers demonstrate process takes multiple steps progressively focus attention relevant visual clues lead answer. related work image closely related image captioning system ﬁrst extracted high level image feature vector googlenet lstm generate captions. method proposed went step attention mechanism caption generation process. different approach proposed ﬁrst used detect words given images used maximum entropy language model generate list caption candidates ﬁnally used deep multimodal similarity model rerank candidates. instead using lstm dmsm uses model semantics captions. unlike image captioning image question given task learn relevant visual text representation infer answer. order facilitate research image several data sets constructed either automatic generation based image caption data human labeling questions answers given images. among them image data generated based coco caption data set. given sentence describes image authors ﬁrst used parser parse sentence replaced word sentence using question words word became answer. created image data human labeling. initial version chinese translated english. also created several image models proposed literature. used semantic parsers image segmentation methods predict answers based images questions. used encoder-decoder framework generate answers given images questions. ﬁrst used lstm encoder images questions used another lstm decode answers. image feature every lstm cell. proposed several neural network based models including encoderdecoder based models single direction lstms bi-direction lstms respectively. however authors found concatenation image features words features worked best. ﬁrst encoded questions lstms combined question vectors image vectors element wise multiplication. used question modeling used convolution operations combine question vectors image feature vectors. compare models sec. best knowledge attention mechanism proved successful image captioning explored image adapt attention mechanism image viewed signiﬁcant extension previous models multiple attention layers used support multi-step reasoning image task. stacked attention networks unlike previous studies features last inner product layer choose features last pooling layer retains spatial information original images. ﬁrst rescale images pixels take features last pooling layer therefore dimension shown fig. number regions image dimension feature vector region. accordingly feature vector corresponds pixel region input images. denote feature vector image region. essential structure lstm unit memory cell reserves state sequence. step lstm unit takes input vector updates memory cell output hidden state update process uses gate mechanism. forget gate controls much information past state preserved. input gate controls much current input updates memory cell. output gate controls much information memory output hidden state. detailed update process follows given question vector representation word position ﬁrst embed words vector space embedding matrix weqt. every time step feed embedding vector words question lstm study also explore similar question representation. similar lstmbased question model ﬁrst embed words vectors weqt question vector concatenating word vectors apply convolution operation word embedding vectors. three convolution ﬁlters size three respectively. t-th convolution output using window size given max-pooling vectors coordinate-wise operation. convolution feature maps different sizes concatenate form feature representation vector whole question sentence compared models simply combine question vector global image vector attention models construct informative since higher weights visual regions relevant question. however complicated questions single attention layer sufﬁcient locate correct region answer prediction. example question fig. sitting basket bicycle refers subtle relationships among multiple objects image. therefore iterate query-attention process using multiple attention layers extracting ﬁne-grained visual attention information answer prediction. formally sans take following formula k-th attention layer compute fig. illustrates reasoning process example. ﬁrst attention layer model identiﬁes roughly area relevant basket bicycle sitting second attention layer model focuses sharply region corresponds answer dogs. examples found sec. many cases answer related small region image. example fig. although multiple objects image bicycles baskets window street dogs answer question relates dogs. therefore using global image feature vector predict answer could lead suboptimal results noises introduced regions irrelevant potential answer. instead reasoning multiple attention layers progressively able gradually ﬁlter noises pinpoint regions highly relevant answer. given image feature matrix question vector ﬁrst feed single layer neural network softmax function generate attention distribution regions image rd×m image representation dimension number image regions dimensional vector. suppose rk×d dimensional vector corresponds attention probability image region given note denote addition matrix vector. since wiavi rk×m wqavq vectors addition matrix vector performed adding column matrix vector. based attention distribution calculate weighted image vectors region combine question vector form reﬁned query vector regarded reﬁned query since encodes question information visual information relevant images mainly indoor scenes. questions categorized three types including object color number. answers single words. following setting exclude data samples multiple words answers. remaining data covers original data set. reduced version daquar-all. training samples test samples. data constrained object categories uses test images. single word answers data covers original data set. coco-qa proposed based microsoft coco data authors ﬁrst parse caption image off-the-shelf parser replace components caption question words form questions. training samples test samples data set. questions based images respectively. four types questions including object number color location. type takes whole data respectively. answers data single word. created human labeling data uses images coco image caption data unlike data sets image three questions question answers labeled human annotators. training questions validation questions data set. following frequent answer possible outputs answers covers answers. ﬁrst studied performance proposed model validation set. following split validation data halves val. training train validate test locally. results reported table. also evaluated best model standard test server provided report results table. compare models baselines proposed recently image since results baselines reported different data sets different literature present experimental results different data sets different tables. four data sets formulate image classiﬁcation problem since answers single words. evaluate model using classiﬁcation accuracy reported reference models also report wu-palmer similarity measure wups measure calculates similarity words based longest common subsequence taxonomy tree. threshold wups similarity less threshold zeroed out. following reference models wups. wups. evaluation metrics besides classiﬁcation accuracy. evaluation data different three data sets since question answer labels same. follow following metric basically gives full credit answer three human labels match answer gives partial credit less matches. daquar coco-qa word embedding dimension lstm’s dimension question model. based question model unigram bigram trigram convolution ﬁlter size respectively. combination ﬁlters makes question vector size dataset since larger data sets double model size lstm accommodate large data large number classes. evaluation experiment attention layers. using three attention layers improve performance. experiments models trained using stochastic gradient descent momentum batch size ﬁxed best learning rate picked using grid search. gradient clipping technique dropout used. experimental results daquar-all daquarreduced coco-qa presented table. respectively. model names explain settings short proposed stacked attention networks value brackets refer using attention layers respectively. keyword lstm refers question model sans use. experimental results table. show two-layer gives best results across data sets kinds question models lstm give similar performance. example daquar-all proposed twolayer sans outperform best baselines imgcnn ask-your-neuron absolute accuracy respectively. similar range improvements observed metrics wups. wups.. also observe signiﬁcant improvements daquar-reduced i.e. outperforms img-cnn -vis+blstm ask-your-neurons approach multi-world absolute accuracy respectively. larger coco-qa data proposed two-layer sans signiﬁcantly outperform best baselines accuracy outperforms lstm model best baseline absolute. superior performance sans across four benchmarks demonstrate effectiveness using multiple layers attention. order study strength weakness detail report performance question-type level large data sets coco-qa table. respectively. observe cocoqa compared best baselines img+bow -vis+blstm best model improves question type color followed objects location number. observe similar trend improvements vqa. shown table. compared best baseline lstm biggest improvement type followed improvement number improvement yes/no. note type refers questions usually form what color kind type where etc. similar question types color objects location coco-qa. data special yes/no type questions. improves performance type questions slightly. could answer yes/no question question dependent better modeling visual information provide much additional gains. also conﬁrms similar observation reported e.g. using additional image information slightly improves performance yes/no shown table. question lstm lstm results demonstrate clearly positive impact using multiple attention layers. four data sets twolayer sans always perform better one-layer san. speciﬁcally coco-qa average two-layer sans outperform one-layer sans type color followed location objects categories number. aligns order improvements baselines. similar trends observed e.g. two-layer improve one-layer type question followed improvement number yes/no. section present analysis demonstrate using multiple attention layers perform multi-step reasoning leads ﬁne-grained attention layer-by-layer locating regions relevant potential answers. visualizing outputs attention layers sample images coco-qa test set. note attention probability distribution size original image up-sample attention probability distribution apply gaussian ﬁlter make size original image. fig. presents examples. examples presented appendix. cover types broad object numbers color location. example three images left right original image output ﬁrst attention layer output second attention layer respectively. bright part image detected attention. across examples ﬁrst attention layer attention scattered many objects image largely corresponds objects concepts referred question whereas second layer attention focused regions lead correct answer. example consider question color horns asks color horn woman’s head fig. output ﬁrst attention layer model ﬁrst recognizes woman image. output second attention layer attention focused head woman leads answer question color horn red. errors analysis randomly sample images coco-qa test make mistakes. group errors four categories sans focus attention wrong regions e.g. example fig. sans focus right region predict wrong answer e.g. examples fig. answer ambiguous sans give answers different labels might acceptable e.g. fig. answer label model predicts vase also visually reasonable; labels clearly wrong e.g. fig. model gives correct answer trains label cars wrong. conclusion paper propose stacked attention network image uses multiple-layer attention mechanism queries image multiple times locate relevant visual region infer answer progressively. experimental results demonstrate proposed signiﬁcantly outperforms previous state-of-theart approaches substantial margin four image", "year": 2015}