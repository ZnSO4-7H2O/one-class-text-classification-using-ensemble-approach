{"title": "Learning to Transfer", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Transfer learning borrows knowledge from a source domain to facilitate learning in a target domain. Two primary issues to be addressed in transfer learning are what and how to transfer. For a pair of domains, adopting different transfer learning algorithms results in different knowledge transferred between them. To discover the optimal transfer learning algorithm that maximally improves the learning performance in the target domain, researchers have to exhaustively explore all existing transfer learning algorithms, which is computationally intractable. As a trade-off, a sub-optimal algorithm is selected, which requires considerable expertise in an ad-hoc way. Meanwhile, it is widely accepted in educational psychology that human beings improve transfer learning skills of deciding what to transfer through meta-cognitive reflection on inductive transfer learning practices. Motivated by this, we propose a novel transfer learning framework known as Learning to Transfer (L2T) to automatically determine what and how to transfer are the best by leveraging previous transfer learning experiences. We establish the L2T framework in two stages: 1) we first learn a reflection function encrypting transfer learning skills from experiences; and 2) we infer what and how to transfer for a newly arrived pair of domains by optimizing the reflection function. Extensive experiments demonstrate the L2T's superiority over several state-of-the-art transfer learning algorithms and its effectiveness on discovering more transferable knowledge.", "text": "transfer learning borrows knowledge source domain facilitate learning target domain. primary issues addressed transfer learning transfer. pair domains adopting different transfer learning algorithms results different knowledge transferred them. discover optimal transfer learning algorithm maximally improves learning performance target domain researchers exhaustively explore existing transfer learning algorithms computationally intractable. trade-off sub-optimal algorithm selected requires considerable expertise adhoc way. meanwhile widely accepted educational psychology human beings improve transfer learning skills deciding transfer metacognitive reﬂection inductive transfer learning practices. motivated this propose novel transfer learning framework known learning transfer automatically determine transfer best leveraging previous transfer learning experiences. establish framework stages ﬁrst learn reﬂection function encrypting transfer learning skills experiences; infer transfer newly arrived pair domains optimizing reﬂection function. extensive experiments demonstrate lt’s superiority several state-of-the-art transfer learning algorithms effectiveness discovering transferable knowledge. inspired human beings’ capabilities transfer knowledge across tasks transfer learning aims leverage knowledge source domain improve learning performance minimize number labeled examples required target domain. particular signiﬁcance tackling tasks limited labeled examples. transfer learning proved wide applicability image classiﬁcation sentiment classiﬁcation dialog systems urban computing three research issues transfer learning pointed yang transfer transfer transfer. transfer learning source domain considered beneﬁt target domain algorithm discover shared knowledge transferred across domains different algorithms likely discover different parts transferable knowledge thereby lead uneven transfer learning effectiveness evaluated performance improvement transfer learning conducted. achieve optimal performance improvement pair source target domains researchers tens hundreds transfer learning algorithms covering instance parameter feature based algorithms. brute-force exploration computationally expensive practically impossible. tradeoff sub-optimal improvement usually obtained heuristically selected algorithm unfortunately requires considerable expertise ad-hoc unsystematic manner. exploring different algorithms better determine transfer thereby improve transfer learning effectiveness. previous transfer learning experiences also help widely accepted educational psychology human beings sharpen transfer learning skills deciding transfer conducting meta-cognitive reﬂection thorough diverse transfer learning experiences. example children good playing chess transfer mathematical skills reading skills visuospatial skills decision making skills learned chess solve arithmetic problems comprehend reading materials solve pattern matching puzzles play basketball respectively. later easier decide transfer mathematical decision making skills learned chess rather reading visuospatial skills market investment. unfortunately existing transfer learning algorithms transfer scratch ignore previous transfer learning experiences. motivated this propose novel transfer learning framework called learning transfer idea framework enhance transfer learning effectiveness leveraging previous transfer learning experiences automatically determine transfer best pair source target domains interest. achieve goal establish framework stages. ﬁrst stage encode transfer learning experience three components including pair source target domains transferred knowledge parameterized shared latent feature factors performance improvement. learn experiences reﬂection function maps pair domains transferred knowledge corresponding performance improvement. reﬂection function therefore believed encrypt transfer learning skills deciding transfer. second stage knowledge transferred newly arrived pair domains optimized value learned reﬂection function matching performance improvement maximized. contribution paper propose novel transfer learning framework opens door improve transfer learning effectiveness taking advantages previous transfer learning experiences. discover transferable knowledge across domains systematic automatic fashion without requiring considerable expertise also evidenced comprehensive empirical studies showing lt’s superiority state-of-the-art transfer learning algorithms. related work transfer learning successfully conduct transfer learning critical research issues identiﬁed decide transfer. parameters instances latent feature factors transferred domains. works transfer parameters source domains target domain regularizers svm-based models. basic learner target domain boosted borrowing useful instances source domain. different techniques capable extracting transferable latent feature factors domains investigated extensively. techniques include manually selected pivot features dimension reduction collective matrix factorization dictionary learning sparse coding manifold learning deep learning unlike existing studies transfer learning transfer scratch i.e. considering source domain target domain interest ignoring previous transfer learning experiences. since existing transfer learning algorithm mentioned could applied transfer learning experience even collect algorithms’ wisdom together. lifelong learning assuming learning task environment training tasks learning learn meta-learning transfers knowledge shared among training tasks task. ruvolo eaton considered lifelong learning online meta-learning. though lifelong learning continuously improve learning system leveraging histories differs task consider transfer learning task rather traditional learning task. therefore learn transfer learning skills instead task-sharing knowledge. learning transfer begin ﬁrst introducing proposed framework. detail steps involved framework i.e. learning transfer learning skills previous transfer learning experiences applying skills infer transfer future pair source target domains. agent previously conducted transfer learning several times kept record transfer learning experiences figure deﬁne transfer learning experience denote source domain represents feature matrix either domain target domain respectively. examples m-dimensional feature space superscript either denote source target domain. denotes vector labels length number target labeled examples much smaller source labeled examples i.e. pair domains. {a··· ana} focus setting denotes transfer learning algorithm randomly selected containing base algorithms. suppose transfer inferred algorithm parameterized finally transfer learning experience labeled performance improvement ratio learning performance without transfer test dataset learning performance transferring knowledge test dataset. previous transfer learning experiences input agent aims learn function approximates shown step figure call reﬂection function encrypts meta-cognitive transfer learning skills transfer maximize improvement ratio given pair source target domains. whenever pair domains sne+tne+ arrives agent identify optimal knowledge transferred pair contained maximizing figure adopting different algorithms pair source target domains brings different transfer them. learn reﬂection function transfer input uniformly parameterize what transfer algorithms base algorithm work consider contain algorithms transferring single-level latent feature factors existing parameter-based instance-based algorithms cannot address transfer learning setting focus though limited parameter-based algorithms transfer across domains heterogeneous label spaces handle binary classiﬁcation problems. deep neural network based algorithms transferring latent feature factors hierarchy left future research. consequence transfer parameterized latent feature factor matrix elaborate following. latent feature factor based algorithms transfer learning setting mentioned learn domain-invariant feature factors across domains. consider classifying pictures source domain pictures target domain. domain-invariant feature factors could include eyes mouth tails etc. re-characterized latent feature factors descriptive target domain expected achieve better performance less labeled examples. transfer case shared feature factors across domains. according different ways extracting domain-invariant feature factors existing latent feature factor based algorithms categorized groups i.e. common latent space based algorithms manifold ensemble based algorithms. common latent space based algorithms line algorithms including limited lsdt assumes domain-invariant feature factors single shared latent space. denote function mapping original feature representation latent space. linear represented embedding matrix rm×u dimensionality latent space. therefore parameterize latent space transfer focus describes latent feature factors. otherwise nonlinear latent space still parameterized although nonlinear explicitly speciﬁed cases lsdt using sparse coding transformed target instances always available. consequently infer similarity latent space metric matrix latent space i.e. rm×m according points grassmann manifold them. representation target domain domaine×nuu subspaces invariant latent factors turns manifold sampled. continuous subspaces manifold considered i.e. gong proved similarity metric matrix. computational details please refer therefore also qualiﬁed represent latent feature factors distributed series subspaces manifold. learning experiences goal learn reﬂection function approximate experiences {··· ne}. improvement ratio closely related aspects difference source target domain shared latent space discriminative ability target domain latent space. smaller difference guarantees overlap domains latent space signiﬁes transferable latent feature factors higher improvement ratios result. discriminative ability target domain latent space also vital improve performances. therefore build take aspects consideration. difference source target domain follow adopt maximum mean discrepancy measure difference source target domain. mapping domains reproducing kernel hilbert space empirically evaluates distance means instances source domain target domain maps u-dimensional latent space rkhs kernel function. using different nonlinear mappings leads different distances thereby different values therefore learning reﬂection function equivalent ﬁnding optimal distance well characterize improvement ratio pairs domains. considering difﬁculty directly deﬁning learning learn optimal alternatively. inspired multiple kernel βkkk learn combination coefﬁcients instead. result rewritten ewe) computed using k-th kernel paper consider expa b/δk) different bandwidths varying values unfortunately measuring distance means domains insufﬁcient measure difference domains. distance variance among pairs instances across domains also required fully characterize difference pair domains small high variance still small overlap. according equation actually empirical estimation ewe). element note shorthand calculated using k-th kernel. unknown data distributions either source target domains empirically estimate detailed supplementary material page limit. discriminative ability target domain considering lack labeled data target domain resort unlabeled data evaluate discriminative ability instead using labelled data usual. principles unlabeled discriminant criterion two-fold similar instances also neighbours embedded latent space; dissimilar instances away. adopt unlabeled discriminant criterion proposed j-th instance mutual r-nearest neighbours other value equals kernel function maximizing unlabeled discriminant criterion local scatter covariance matrix guarantees ﬁrst principle non-local scatter covariance matrix enforces second principle. also depends kernel used since different kernels indicate different neighbour information different degrees similarity neighboured instances. obtained k-th kernel unlabeled discriminant criterion huber regression loss constraining ˆqeβ value close possible. controls complexity parameters. minimizing difference domains including distance variance ˆqeβ meanwhile maximizing discriminant criterion target domain contribute large performance improvement ratio balance importance three terms bias term. agent learned reﬂection function take advantage function infer optimal transfer i.e. latent feature factor matrix newly arrived pair source domain sne+ target domain tne+. optimal latent feature factor matrix maximize value optimize following objective regard section empirically test performance proposed framework. datasets evaluate framework image datasets caltech- sketches caltech- collected google images contains total images categories. sketches dataset however consists unique sketches human beings evenly distributed different categories. overlap categories caltech categories sketches e.g. backpack horse. obviously caltech- images different distribution sketches regard source domain target domain. construct pair source target domains randomly sampling three categories caltech- source domain randomly sampling three categories sketches target domain give example supplementary material. consequently examples target domain pair. total generate training pairs transfer learning experiences validation pairs determine hyperparameters reﬂection function testing pairs evaluate reﬂection function. characterize image datasets using -dimensional features extracted convolutional neural network pre-trained imagenet dataset. baselines evaluation metrics compare proposed framework following nine baseline algorithms including without transfer learning eight feature-based transfer learning algorithms categories i.e. common latent space manifold ensemble. original builds model using labeled data target domain only. common latent space based algorithms listed follows. learns transferable components across domains embeddings domains minimum mmd. learns common subspace instances domains distributed similarly also discriminatively optimizing information-theoretic metric. graph co-regularized matrix factorization proposed denoted extracts common subspace geometric structures within domain across domains preserved. lsdt learns common subspace instances target domain sparsely reconstructed combined source target instances. self-taught learning learns dictionary source domain obtains enriched representations target instances based learned dictionary. learn domain-invariant projection minimize hellinger distance domains respectively. manifold ensemble based algorithm embeds source target domain onto grassmann manifold projections inﬁnite number subspaces along manifold integrated represent target domain. eight featurebased transfer learning algorithms also base transfer learning algorithms used generate experiences section based feature representations obtained different algorithms nearest-neighbor classiﬁer perform three-class classiﬁcation target domain. classiﬁcation accuracy test data target domain evaluation metric. since target domains different levels difﬁculty accuracies incomparable different pairs domains. evaluate different pairs source target domains adopt performance improvement ratio deﬁned section another evaluation metric. performance comparison experiment learn reﬂection function transfer learning experiences evaluate reﬂection function testing pairs source target domains comparing average performance improvement ratio baselines. building reﬂection function kernels bandwidth range following median trick figure shows average proposed framework outperforms baselines varying number labeled samples target domain. number labeled examples target domain used building classiﬁer increases performance improvement ratio becomes smaller classiﬁcation accuracy without transfer tends increase. baseline algorithms behave differently. transferable knowledge learned lsdt helps target domain training examples scarce performs poorly training examples become more. almost worst baseline learns dictionary source domain ignores target domain. runs high risk failure especially domains distant. minimize hellinger distance domains subject manifold constraints competent. note paired t-test baseline p-values order concluding signiﬁcantly superior. also randomly select testing pairs compare classiﬁcation accuracies different algorithms pair figure performance baselines varies pair pair. among baseline methods performs best transferring domains figure lsdt superior figure however consistently outperforms baselines settings. pairs e.g. figures three classes target domains comparably easy tell apart hence original without transfer achieve even better results transfer learning algorithms. case still improves discovering best transferable knowledge source domain especially number labeled examples small domains related e.g. source galaxy saturn target figure even ﬁnds transferable knowledge contributes signiﬁcant improvement. varying experiences investigate transfer learning experiences used learn reﬂection function inﬂuence performance experiment evaluate randomly sampled pairs testing pairs order efﬁciently investigate wide range cases following. sampled unbiased sufﬁcient characterize inﬂuence evidenced asymptotic consistency average performance improvement ratio pairs figure pairs last line table first number transfer learning experiences vary base transfer learning algorithms. results shown table even experiences generated single base algorithm e.g. still learn reﬂection function signiﬁcantly better decides transfer using directly. base algorithms involved transfer learning experiences informative cover situations source-target pairs knowledge transferred them. result learns better reﬂection function thereby achieves higher performance improvement ratios. second base algorithms include table performance improvement ratios varying different approaches used generate transfer learning experiences. example itl+lt denotes learning experiences generated only second line results itl+lt p-value compared itl. eight baselines vary number transfer learning experiences used training. shown figure average performance improvement ratio achieved tends increase number labeled examples target domain decreases importantly increases number experiences increases. lays foundation conducting online manner gradually assimilate transfer learning experiences continuously improve. figure varying number transfer learning experiences. varying reﬂection function also study inﬂuence different conﬁgurations reﬂection function performance first vary components considered building reﬂection function shown figure considering single type either variance discriminant criterion brings inferior performance even negative transfer. taking three factors consideration outperforms others demonstrating three components necessary mutually reinforcing. three components included supplementary material. second change different kernels used. plot values learned figure present results either narrowing extending range obviously kernels capable encrypting better transfer learning skills reﬂection function achieve larger performance improvement ratios. conclusion paper propose novel framework transfer learning automatically determines transfer best source target domain leveraging previous transfer learning experiences. particular learns reﬂection function mapping pair domains knowledge transferred performance improvement ratio. pair domains arrives optimizes transfer maximizing value learned reﬂection function. believe opens door improve transfer learning leveraging transfer learning experiences. many research issues e.g. incorporating hierarchical latent feature factors transfer designing online investigated. belmont butterﬁeld ferretti secure transfer training instruct self-management skills. detterman sternberg editors much intelligence increased pages ablex norwood yang zhang j.-y. yang niu. globally maximizing locally minimizing unsupervised discriminant projection applications face palm biometrics. tpami performance improvement ratio heavily depends number labeled examples target domain i.e. smaller number target labeled examples tends produce larger performance improvement ratio vice versa. since varies experience experience adopt transformed instead train reﬂection function equation expected expectation performance improvement ratio range e-th experience minimum maximum number target labeled examples. compute expectation ﬁrst assume performance improvement ratio regard number target labeled examples follows following monotonically decreasing function parameters deciding function. conditioned speciﬁc experience shared across experiences. note consequence obtain expected performance improvement ratio", "year": 2017}