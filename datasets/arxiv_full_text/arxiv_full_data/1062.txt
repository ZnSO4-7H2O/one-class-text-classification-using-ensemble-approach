{"title": "Tunable Sensitivity to Large Errors in Neural Network Training", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "When humans learn a new concept, they might ignore examples that they cannot make sense of at first, and only later focus on such examples, when they are more useful for learning. We propose incorporating this idea of tunable sensitivity for hard examples in neural network learning, using a new generalization of the cross-entropy gradient step, which can be used in place of the gradient in any gradient-based training method. The generalized gradient is parameterized by a value that controls the sensitivity of the training process to harder training examples. We tested our method on several benchmark datasets. We propose, and corroborate in our experiments, that the optimal level of sensitivity to hard example is positively correlated with the depth of the network. Moreover, the test prediction error obtained by our method is generally lower than that of the vanilla cross-entropy gradient learner. We therefore conclude that tunable sensitivity can be helpful for neural network learning.", "text": "humans learn concept might ignore examples cannot make sense ﬁrst later focus examples useful learning. propose incorporating idea tunable sensitivity hard examples neural network learning using generalization cross-entropy gradient step used place gradient gradient-based training method. generalized gradient parameterized value controls sensitivity training process harder training examples. tested method several benchmark datasets. propose corroborate experiments optimal level sensitivity hard example positively correlated depth network. moreover test prediction error obtained method generally lower vanilla cross-entropy gradient learner. therefore conclude tunable sensitivity helpful neural network learning. recent years neural networks become empirically successful wide range supervised learning applications computer vision speech recognition natural language processing computational paralinguistics standard implementations training feed-forward neural networks classiﬁcation based gradient-based stochastic optimization usually optimizing empirical cross-entropy loss however cross-entropy surrogate true objective supervised network training cases reduce probability prediction error optimizing using cross-entropy loss show below effect training examples gradient linear prediction bias difference network-predicted class probabilities target class probabilities. particular wrong conﬁdent prediction induces larger gradient similarly wrong less conﬁdent prediction. copyright association advancement artiﬁcial intelligence rights reserved. contrast humans sometimes employ different approach learning learning concepts might ignore examples feel understand focus examples useful them. improving proﬁciency regarding familiar concept might focus harder examples contain relevant information advanced learner. make ﬁrst step towards incorporating ability neural network models proposing learning algorithm tunable sensitivity easy hard training examples. intuitions human cognition often inspired successful machine learning approaches work show case also tunable sensitivity. intuitively depth model positively correlated optimal sensitivity hard examples. network relatively shallow modeling capacity limited. case might better reduce sensitivity hard examples since likely examples cannot modeled correctly network adjusting model according examples might degrade overall prediction accuracy. hand network relatively deep high modeling capacity. case might beneﬁcial allow sensitivity hard examples thereby possibly improving accuracy ﬁnal learned model. learning algorithm works generalizing crossentropy gradient function used instead gradient gradient-based optimization method neural networks. many training methods proposed including name momentum rmsprop adam proposed generalization parameterized value controls sensitivity training process hard examples replacing ﬁxed dependence cross-entropy gradient. proposed update rule exactly cross-entropy gradient. smaller values decrease sensitivity training hard examples larger values increase report experiments several benchmark datasets. experiments show matching expectations almost cases prediction error improved using large values deep networks small values shallow netconsider standard feed-forward multilayer neural network output layer softmax layer units representing class. denote neural network parameters denote value output unit network parameters applying softmax function. applying softmax function probability assigned network class ezi. label predicted network example argmaxj∈ consider task supervised learning using labeled training sample optimizing loss function popular choice cross-entropy cost function deﬁned proposed method allows controlling sensitivity training procedure examples network large errors prediction means generalizing gradient. na¨ıve alternative towards goal would using exponential version cross-entropy loss logk| probability assigned correct class hyperparameter controlling sensitivity level. however derivative function respect undesired term since monotone ﬁxed resulting lack relevant meaning small large values gradient resulting form desired form cancellation terms derivatives softmax function. another na¨ıve option would consider scaled version cross-entropy loss amounts change learning rate. general controlling loss function alone sufﬁcient controlling relative importance training procedure examples network large small errors prediction. indeed computing gradients derivative loss function multiplied derivative softmax function latter term also contains probabilities assigned model different classes. alternatively controlling parameters updates themselves describe below direct achieving desired effect. works values close default networks medium depth. show using tunable sensitivity parameter generally improves results learning. paper structured follows section related work discussed. section presents setting notation. framework generalizing loss gradient developed section section presents desired properties generalization speciﬁc choice given section experiment results presented section conclude section analysis additional experimental results deferred supplementary material lack space. related work challenge choosing best optimization objective neural network training one. past quadratic loss typically used gradient-based learning neural networks line studies demonstrated theoretically empirically cross-entropy loss preferable properties quadratic-loss better learning speed better performance suitable shape error surface cost functions also considered. instance novel cost function proposed clearly advantageous cross-entropy. authors address question different setting sequence prediction. method allows controlling sensitivity training process examples large prediction bias. sensitivity method seen form implicit outlier detection noise reduction. several previous works attempt explicitly remove outliers noise neural network training. work data preprocessed detect label noise induced overlapping classes another work authors auxiliary neural network detect noisy examples. contrast approach requires minimal modiﬁcation gradient-based training algorithms neural networks allows emphasizing examples large prediction bias instead treating noise. interplay easy hard examples neural network training addressed framework curriculum learning framework suggested training could successful network ﬁrst presented easy examples harder examples gradually added training process. anwork authors deﬁne easy hard examples based current model parameters. propose curriculum learning algorithm tunable parameter controls proportions easy hard examples presented learner phase. method simpler curriculum learning approaches examples presented random order network. addition method allows also heightened sensitivity harder examples. recent work authors indeed property consider following simple example. assume network hidden layer softmax layer inputs softmax layer outputs hidden layer input vector scalar bias weight vector input layer hidden layer. suppose point training hidden unit connected units softmax layer positive weight words suppose training process encounters training example input coordinate. training example cause? clearly need change consider case value directly affected changing deﬁnition predicted probabilities fully determined ratios /ezj equivalently differences therefore therefore conclude case equal weights unit output units reason change weight moreover preliminary experiments show cases desirable keep weight stationary otherwise cause numerical instability explosion decay weights. pseudo-gradients. therefore require case. follows given deﬁne prediction bias network example class denoted difference probability assigned network class probability assigned based true label example. otherwise. thus crossentropy loss discussed section likely many cases results training could improved effect single example gradient linear prediction bias. therefore propose generalization gradient allows non-linear dependence given deﬁne consider following generalization call vector values pseudogradient propose place gradient within gradient-based algorithm. optimization cross-entropy loss replaced different algorithm similar form. however show section necessarily gradient loss function. consider types functions reasonable instead identity. first expect monotonic non-decreasing larger prediction bias never results smaller update. reasonable requirement cannot identify outliers training examples wrong label. expect positive negative otherwise. case cross-entropy identity leading linear dependence natural generalization consider higher order polynomials. combining approach requirement following assignment parameter. illustrate relationship value effect prediction biases different sizes pseudo-gradient plot function several values note absolute values pseudo-gradient little importance since gradient-based algorithms gradient usually multiplied scalar learning rate tuned. ﬁgure shows large pseudo-gradient strongly affected large prediction biases compared small ones. follows since |\u0001|k |\u0001|k monotonic increasing hand using small positive |\u0001|k |\u0001|k tends therefore pseudo-gradient case would much less sensitive examples large prediction biases. thus choice parameterized allows tuning sensitivity training process large errors. note could reasonable choices similar desirable properties. leave investigation choices future work. example motivate choice describe simple example distribution neural network. consider neural network hidden layers input unit connected softmax units. denoting input input softmax unit network weights biases respectively. hard possible prediction functions represented network exactly threshold functions form sign −sign. convenience assume labels mapped softmax units named suppose labeled examples drawn independently random following distribution examples uniform labels examples deterministically examples. distribution prediction function smallest prediction error represented network sign. however optimizing cross-entropy loss distribution limit large training sample would result different threshold leading larger prediction error intuitively traced fact examples cannot classiﬁed correctly network threshold close still affect optimal threshold cross-entropy loss. thus simple case motivation move away optimizing cross-entropy different update rule less sensitive large errors. reduced sensitivity achieved update rule hand larger values would result higher sensitivity large errors thereby degrading classiﬁcation accuracy even more. thus expect training network using update rule prediction error resulting network monotonically increasing hence values smaller would give smaller error. tested hypothesis training simple network synthetic dataset generated according distribution described above generated examples training validation test datasets. biases initialized weights initialized uniform distribution used batch gradient descent learning rate optimization four parameters gradient replaced pseudo-gradient using function deﬁned parameterized performed experiment using values epoch computed prediction error validation training stopped epochs error changed values parameters training used compute misclassiﬁcation rate test set. table reports results experiments averaged runs value results conﬁrm hypothesis regarding behavior network different values motivate possible beneﬁts using note prediction error monotonic experiment cross-entropy demonstrating fact optimizing cross-entropy optimal case. non-existence cost function natural whether choice gradient another cost function instead cross-entropy. following lemma demonstrates case. lemma assume resulting pseudo-gradient. exists neural network gradient cost function. experiments used four classiﬁcation benchmark datasets ﬁeld computer vision mnist dataset street view house numbers dataset cifar- cifar- datasets detailed description datasets found appendix supplementary material. neural networks experimented feedforward neural networks contain three hidden layers various layer sizes. optimization used stochastic gradient descent momentum several values momentum minibatch size examples. value replaced gradient algorithm pseudo-gradient using function deﬁned multilayer experiments also used gradient-clipping threshold hidden layers biases initialized weights used initialization scheme biases weights softmax layer initialized experiment used cross-validation select best value learning rate optimized using crossvalidation value separately size pseudo-gradient signiﬁcantly different different values evident compared test error models using selected best performing learning rate. additional details experiment process found appendix supplementary material. report test error trained models mnist svhn cifar- cifar- tables networks three layers respectively. additional experiments reported appendix supplementary material. report crossentropy values using selected default several observations evident experiment results. first aligned hypothesis value selected cross-validation scheme almost always smaller shallow networks larger deep networks close networks medium depth. indeed capacity network positively correlated optimal sensitivity hard examples. second shallow networks cross-entropy loss test always worse selected implies indeed using different value optimizing cross-entropy loss improving success optimizing true prediction error. contrary experiments three layers cross entropy also improved selecting larger interesting phenomenon might explained fact examples large prediction bias high cross-entropy loss focusing training examples reduces empirical cross-entropy loss therefore also true cross-entropy loss. inspired intuition human cognition work proposed generalization cross-entropy gradient step tunable parameter controls sensitivity training process hard examples. experiments show that expected optimal level sensitivity hard examples positively correlated depth network. moreover experiments demonstrate selecting value sensitivity parameter using cross validation leads overall improved prediction error performance variety benchmark datasets. proposed approach limited feed-forward neural networks used gradient-based training algorithm network architecture. future work plan study method tool improving training architectures convolutional networks recurrent neural networks well experimenting different levels sensitivity hard examples different stages training procedure combining predictions models different levels sensitivity.", "year": 2016}