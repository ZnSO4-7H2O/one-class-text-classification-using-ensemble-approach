{"title": "Learning Gradient Descent: Better Generalization and Longer Horizons", "tag": ["cs.LG", "cs.AI"], "abstract": "Training deep neural networks is a highly nontrivial task, involving carefully selecting appropriate training algorithms, scheduling step sizes and tuning other hyperparameters. Trying different combinations can be quite labor-intensive and time consuming. Recently, researchers have tried to use deep learning algorithms to exploit the landscape of the loss function of the training problem of interest, and learn how to optimize over it in an automatic way. In this paper, we propose a new learning-to-learn model and some useful and practical tricks. Our optimizer outperforms generic, hand-crafted optimization algorithms and state-of-the-art learning-to-learn optimizers by DeepMind in many tasks. We demonstrate the effectiveness of our algorithms on a number of tasks, including deep MLPs, CNNs, and simple LSTMs.", "text": "training deep neural networks highly nontrivial task involving carefully selecting appropriate training algorithms scheduling step sizes tuning hyperparameters. trying different combinations quite labor-intensive time consuming. recently researchers tried deep learning algorithms exploit landscape loss function training problem interest learn optimize automatic way. paper propose learning-to-learn model useful practical tricks. optimizer outperforms generic hand-crafted optimization algorithms state-of-the-art learning-to-learn optimizers deepmind many tasks. demonstrate effectiveness algorithms number tasks including deep mlps cnns simple lstms. training neural network viewed solving optimization problem highly non-convex loss function. gradient-based algorithms widely used algorithms training neural networks basic adagrad rmsprop adam etc. particular neural network unclear priori best optimization algorithm hyperparameters usually takes time experienced hands identify best optimization algorithm together best hyperparameters possibly tricks necessary make network work. *equal contribution †the research supported part national basic research program china grants national nsfc grants interdisciplinary information sciences tsinghua university beijing china. correspondence kaifeng <vﬂeaking.com> shunhua jiang <linda.com> jian <lijianmail.tsinghua.edu.cn>. address issue promising approach machine learning algorithms replace hard-coded optimization algorithms hopefully learning algorithm capable learning good strategy experience explore landscape loss function adaptively choose good descent steps. high level idea categorized umbrella learning-tolearn broad area known learning community decades. using deep learning training deep neural networks initiated recent paper authors proposed optimizer using coordinatewise long short term memory takes gradients optimizee input outputs updates optimizee parameters. call optimizer dmoptimizer throughout paper term optimizee refer loss function neural network optimized. authors showed dmoptimizer outperforms traditional optimization algorithms solving task trained also generalizes well type tasks. experiments trained dmoptimizer minimize average loss -step training process -hidden-layer multilayer perceptron sigmoid activation function optimizer shown generalization ability extent also performs well hidden layer double hidden neurons. however still limitations activation function changed sigmoid relu test phase dmoptimizer performs poorly train mlp. words algorithms fail generalize different activations. even though authors showed dmoptimizer performs well train optimizee descent steps loss increases dramatically much longer horizons. words algorithms fail handle relatively large number descent steps. effective trick random scaling used training optimizer improve generalization ability randomly scaling parameters optimizee. trick combine loss function optimizee simple convex functions helps accelerate training process. help training tricks model called rnnprop achieves notable improvements upon previous work trained simple -hidden-layer generalize variety neural networks including much deeper mlps cnns simple lstms. tasks achieves better least comparable performance traditional optimization algorithms. notion learning learn meta-learning used address concept learning meta-knowledge learning process years. however agreement exact deﬁnition meta-learning various concepts developed different authors paper view training process neural network optimization problem optimizer train neural networks. usage another neural network direct training neural networks forward naik mammone early work cotter younger argued rnns used model adaptive optimization algorithms idea further developed gradient descent used train optimizer convex problems. recently shown section andrychowicz proposed general optimizer model using lstm learn gradient descent work directly follows work. another recent paper used take current position value input outputs next position works well black-box optimization simple tasks. reinforcement learning perspective optimizer viewed policy takes current state input output next action recent papers trained adaptive controllers adjust hyperparameters traditional optimization algorithms perspective. method regarded hyperparameter optimization. general methods introduced also take perspective train neural network model policy. great number optimization algorithms proposed improve performance vanilla gradient descent including momentum adagrad adadelta rmsprop adam. update rules several common optimization algorithms listed table table traditional optimization algorithms. parameters neural network represents gradient. hyperparameters optimization algorithm. vector operations coordinatewise. interested ﬁnding optimizer undertakes optimization tasks different optimizees. optimizee function minimized. case optimizee stochastic value defar local minimum. second class smaller effective step size |∆θti| coordinate mainly induced relatively smaller partial derivative comparing past partial derivatives. approaching local minimum gradient ﬂuctuate stochastic nature. algorithms second class decrease effective step size coordinate accordance ﬂuctuation amplitude coordinate i.e. coordinate larger uncentered variance yields smaller effective step size. thus algorithms second class able decrease effective step size coordinates uncertainty robust ﬁrst class. insight difference classes algorithms consider happens scale optimizee factor i.e. ideally scaling affect behaviors algorithms. however algorithms ﬁrst class since effective step size also scaled hence behaviors algorithms change completely. algorithms second class behave since scale factor canceled out. thus algorithms second class robust respect scaling. observation albeit simple inspiration model. hand training tricks model exposed functions different scales training stage. hand take relative gradients input optimizer belongs second class. following section introduce training tricks model details. optimizer operates coordinatewise parameters follows directly optimizer handles gradients coordinatewise maintains hidden states every coordinate respectively. parameters shared different coordinates. optimizer train optimizees number parameters. propose training trick called random scaling prevent overﬁtting training model. introducing ideas consider happens train optimizer minimize initial parameλ∇f optimal polter clearly since lowest point reached step. however optimizer learns follow rule exactly testing optimizer function different might produce modest even result. table summaries optimization algorithms commonly used training neural networks. optimization algorithms degree adaptivity able adjust effective step size |∆θt| training. divide algorithms classes. ﬁrst class includes momentum determine effective step size absolute size gradients. second class includes adagrad adadelta rmsprop adam. algorithms maintain moving average past gradients seen little abuse terminology second moment then algorithms produce effective step size relative size gradient namely gradient divided square root second moment coordinatewise. training process parameters gradually approach local minimum smaller effective step size required careful local optimization. obtain smaller effective step size classes algorithms different mechanisms. ﬁrst class take full gradient effective step size automatically gets smaller approaching local minimum. however since stochastic gradient descent effective step size small enough even deﬁned adam table change input three advantages. first input contains information absolute size gradients algorithm belongs second class automatically hence robust. second manipulation gradients seen kind normalization input values bounded constant somewhat easier neural network learn. lastly model outputs constant times reduces adam. similarly model outputs constant times reduces rmsprop. hence hope further optimizing parameters rnnprop capable achieving better performance adam rmsprop ﬁxed learning rate. input preprocessed fully-connected layer activation function handled rnn. central part model twolayer coordinatewise lstm dmoptimizer. outputs single vector xout increment taken method solve issue rather simple randomly pick every iteration training optimizer. notice also pick random number scale parameters achieve goal. generalize idea design training trick random scaling coordinatewise randomly scales parameters objective function training stage. details iteration training optimizer loss function initial parameter ﬁrst randomly pick vector dimension coordinate sampled independently distribution then train model optimizee initial parameter multiplication inversion operations performed coordinatewise. optimizer forced learn adaptive policy determine best effective step size rather learn best effective step size particular task. introduce another training trick. clear train optimizer optimizees implemented neural networks. however non-convex stochastic nature neural networks hard learn basic idea gradient descent. without trick optimizer wanders around aimlessly non-convex loss surface function beginning stage training. combine optimizee function since good property convexity optimizer soon learns basic knowledge gradient descent additional optimizee coordinates. knowledge shared coordinates optimizer processes input coordinatewise. accelerate training process optimizer. training continues optimizer learns better method gradient decent baseline. figure performance base mlp. left rnnprop achieves comparable performance allowed steps. right rnnprop continues decrease loss even steps performance slightly worse traditional algorithms. optimizee train optimizers crossentropy loss simple mnist dataset. convenience address base mlp. hidden layer hidden units uses sigmoid activation function. value computed using minibatch random pictures. iteration training optimizers allowed steps. optimizers trained using truncated backpropagation trough time split steps periods steps. period initialize initial parameter initial hidden state last period generate ﬁrst period. adam used minimize loss trained dmoptimizer using loss rnnprop optimizer strictly required produce loss step ﬂexible. also notice loss results slightly better performance. structure model rnnprop shown section two-layer lstm whose hidden state size avoid division zero actual experiments another term input changed training rnnprop ﬁrst apply random scaling optimizee function convex function respectively deﬁned equation combine together introduced section dimension convex function generate vectors uniformly randomly. generate coordinate vector random scaling ﬁrst generate number uniformly randomly take value coordinate natural exponential function. implementation aimed produce differ also tried transformations including using uniform distribution scaling entire function directly randomly dropping coordinates etc. version random scaling selected comprehensive comparison. experiments function function save parameters optimizers every iterations training. dmoptimizer select saved optimizer best performance validation task since rnnprop tends overﬁt training task because random scaling method simply select saved optimizer lowest average train loss moving average losses past iterations decay factor selected optimizers tested different tasks. performances compared best traditional optimization algorithms whose learning rates carefully chosen hyperparameters default values tensorﬂow initial optimizee parameters used experiments generated independently gaussian distribution ﬁgures shown section plotted running optimization process multiple times random initial values data. removed outliers exceedingly large loss value plotting loss curves. loss value rnnprop removed plotting ﬁgures. ﬁrst test optimizers task used training stage optimize base steps. dmoptimizer rnnprop outperform traditional optimization algorithms. dmoptimizer better performance possibly overﬁtting. test optimizers steps base mlp. left plot figure indicates rnnprop achieve comparable performance traditional algorithms steps dmoptimizer fails. also test optimizers much steps steps shown right plot figure clear dmoptimizer loses ability decrease loss after steps loss begins increase dramatically. rnnprop hand able decrease loss continuously though slows gradually traditional algorithms overtake main reason rnnprop trained steps step training process signiﬁcantly different -step training process. additionally traditional optimization algorithms able achieve good performance tasks explicitly adjusted learning rates adapt tasks. test optimizers base different activation functions. shown figure activation function changed sigmoid relu rnnprop still achieve better performance traditional algorithms dmoptimizer fails. activations rnnprop also generalizes well shown table deep neural networks different layers different optimal learning rates traditional algorithms global learning rate parameters. optimizer achieve better performance beneﬁted figure ﬁnal loss different algorithms base steps. colorful solid curves show ﬁnal losses traditional algorithms steps change different learning rates horizontal dash line shows ﬁnal loss rnnprop. compute ﬁnal loss freezing ﬁnal parameters optimizee compute average loss using data encountered optimization process. tested optimizers deeper mlps. hidden layers added base hidden units sigmoid activation function. shown figure rnnprop always outstrip traditional algorithms becomes deep none decrease loss steps. figure shows loss curves hidden layers example. optimizees cross-entropy losses convolutional neural networks similar structure vggnet dataset mnist dataset cifar-. convolutional layers ﬁlters window max-pooling layer size stride denote convolutional layer denote max-pooling layer denote fully-connected layer. three cnns used experiments structure c-c-p-f mnist results shown figure rnnprop outperform traditional algorithms structure c-c-p-f dataset mnist. cnns best traditional algorithm outperforms rnnprop. even though showed dmoptimizer trained cnns train cnns faster traditional algorithms experiments dmoptimizer fails train cnns training ﬁxed base mlp. optimizers also tested mean squared loss lstm hidden state size simple task given sequence additive noise lstm needs predict value sin. generating dataset uniformly randomly choose draw noise gaussian distribution table performance task different settings lstm. list ﬁnal loss rnnprop best traditional optimization algorithms task -layer lstm task smaller noise. numbers table computed running optimization processes times. even though task completely different task used training rnnprop still comparable even better performance traditional algorithms fact structure inside lstm similar base sigmoid between. recall trick combining convex function aims accelerate training optimizers. test performance rnnprop whose parameters trained different numbers iterations without trick. result shown table trick optimizer achieve good result fewer iterations training. figure performance different cnns. left convolutional layer pooling layer fully-connected layer dataset mnist. center convolutional layer pooling layer fully-connected layer dataset mnist. right convolutional layer pooling layer fully-connected layer dataset cifar-. assess contributions select trained optimizers rnnprop. figure test performances base activation replaced relu steps. ﬁgure conclude random scaling effective trick. table comparison rnnprop without combination convex functions. test base base activation replaced relu. column shows number iterations used train optimizers. column shows ﬁnal loss produced rnnprop training tricks last column shows ﬁnal loss produced rnnprop trained without combination convex functions. figure comparison rnnprop trained tricks rnnprop trained without random scaling dmoptimizer dmoptimizer trained tricks. optimizers tested base activation replaced relu steps. paper present learning-to-learn model several useful tricks. show optimizer better generalization ability state-of-art learning-to-learn optimizers. trained using simple optimizer achieves better comparable performance traditional optimization algorithms training complex neural networks training longer horizons. believe possible improve generalization ability optimizer. indeed tasks experiments optimizer outperform best traditional optimization algorithms particular training much longer horizon training neural networks different datasets. future further develop generic optimizer elaborate designing achieve better performance wider range tasks analogous optimizee used training. references abadi agarwal barham brevdo chen citro corrado g.s. davis dean devin tensorﬂow large-scale machine learning heterogeneous distributed systems. arxiv preprint arxiv. andrychowicz denil g´omez hoffman pfau schaul freitas learning learn gradient descent gradient descent. sugiyama luxburg guyon garnett advances neural information processing systems curran associates inc.", "year": 2017}