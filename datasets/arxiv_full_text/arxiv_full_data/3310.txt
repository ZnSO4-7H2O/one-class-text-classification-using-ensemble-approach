{"title": "A deep matrix factorization method for learning attribute  representations", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Semi-Non-negative Matrix Factorization is a technique that learns a low-dimensional representation of a dataset that lends itself to a clustering interpretation. It is possible that the mapping between this new representation and our original data matrix contains rather complex hierarchical information with implicit lower-level hidden attributes, that classical one level clustering methodologies can not interpret. In this work we propose a novel model, Deep Semi-NMF, that is able to learn such hidden representations that allow themselves to an interpretation of clustering according to different, unknown attributes of a given dataset. We also present a semi-supervised version of the algorithm, named Deep WSF, that allows the use of (partial) prior information for each of the known attributes of a dataset, that allows the model to be used on datasets with mixed attribute knowledge. Finally, we show that our models are able to learn low-dimensional representations that are better suited for clustering, but also classification, outperforming Semi-Non-negative Matrix Factorization, but also other state-of-the-art methodologies variants.", "text": "abstract—semi-non-negative matrix factorization technique learns low-dimensional representation dataset lends clustering interpretation. possible mapping representation original data matrix contains rather complex hierarchical information implicit lower-level hidden attributes classical level clustering methodologies interpret. work propose novel model deep semi-nmf able learn hidden representations allow interpretation clustering according different unknown attributes given dataset. also present semisupervised version algorithm named deep allows prior information known attributes dataset allows model used datasets mixed attribute knowledge. finally show models able learn low-dimensional representations better suited clustering also classiﬁcation outperforming semi-non-negative matrix factorization also state-of-the-art methodologies variants. index terms—semi-nmf deep semi-nmf unsupervised feature learning face clustering semi-supervised learning deep matrix factorization face classiﬁcation techniques data analysis. recent years signiﬁcant amount research factorization methods focus particular characteristics data matrix resulting factors. nonnegative matrix factorization example focuses decomposition non-negative multivariate data matrix factors also nonnegative application area family algorithms grown signiﬁcantly past years. shown successful dimensionality reduction technique variety areas including limited environmetrics microarray data analysis document clustering face recognition blind audio source separation more. makes algorithms particularly attractive non-negativity constraints imposed factors produce allowing better interpretability. moreover shown variants equivalent soft version k-means clustering fact variants expected perform better k-means clustering particularly data distributed spherical manner non-negativity constraints second factor allows mixed signs data matrix ﬁrst factor motivated clustering perspective represents cluster centroids represents soft membership indicators every data point allowing semi-nmf learn lower-dimensional features data convenient clustering interpretation. possible mapping representation original data matrix contains rather complex hierarchical structural information. complex dataset produced multimodal data distribution mixture several distributions constitutes attribute dataset. consider example problem mapping images faces identities face image also contains information attributes like pose expression help identify person depicted. could argue factorizing mapping factor adds extra layer abstraction could automatically learn latent attributes intermediate hidden representations implied allowing better higher-level feature representation work propose deep seminmf novel approach able factorize matrix multiple factors unsupervised fashion figure therefore able learn multiple hidden representations original data. semi-nmf close relation k-means clustering deep semi-nmf also clustering interpretation according different latent attributes dataset demonstrated figure using non-linear deep model matrix factorization also allows project data-points initially linearly separable representation another line work related multi-label learning multi-label learning techniques rely correlations exist different attributes extract better features. interested cases complete knowledge attributes dataset rather propose paradigm learning representations data partly annotated attributes. example mixture datasets label information different attributes. paradigm leverage correlations attribute labels rather rely hierarchical structure data uncover relations different dataset attributes. best knowledge ﬁrst piece work tries automatically discover representations different attributes dataset application multi-modal application face clustering. novelty work summarised follows outline novel deep framework matrix factorization suitable clustering multimodally distributed objects faces present greedy algorithm optimize factors semi-nmf problem inspired recent advances deep learning evaluate representations learned different nmf–variants terms clustering performance present deep model already known information attributes data distribution extract better features model demonstrate improve performance deep semi-nmf using existing weights trained deep model. collection data vectors columns features. matrix factorization aims ﬁnding factors satisfy certain constraints. singular value decomposition method underlies principal component analysis factorize imposing sign restrictions either data resulting factors. non-negative matrix factorization assume matrices involved contain non-negative elements approximate factorization preliminary version work appeared clear context notation state matrix contains non-negative elements. similarly clear notation state contain real number. fig. semi-nmf model results linear transformation initial input space. deep semi-nmf learns hierarchy hidden representations uncovering ﬁnal lower-dimensional representation data. might case different attributes data latent. known actually label information data would naturally want leverage learn representations would make data separable according attributes. effect also propose weakly-supervised deep semi-nmf technique able learn semisupervised manner hierarchy representations given dataset. level hierarchy corresponds speciﬁc attribute known priori show incorporating partial label information graph regularization techniques able perform better fully unsupervised deep semi-nmf task classifying dataset faces according different attributes known. also show initializing unsupervised deep seminmf weights learned deep able improve clustering performance deep semi-nmf could particularly useful have example small dataset images faces partial attribute labels larger attribute labels. initializing deep semi-nmf weights learned deep small labelled dataset leverage information allow unsupervised model uncover better representations initial data task clustering faces. relevant proposal hierarchical clustering algorithms popular gene document clustering applications. algorithms typically abstract initial data distribution form tree called dendrogram useful analysing data help identify genes used biomarkers topics collection documents. graph-regularized takes account intrinsic geometric discriminating structure data space essential realworld applications especially area clustering. accomplish this gnmf constructs nearest neighbor graph model manifold structure. preserving graph structure allows learned features discriminating power standard algorithm cases data sampled submanifold lies higher dimensional ambient space. closest proposal recent work presented nmf-variants factorize factors. speciﬁcally demonstrated concept multi-layer facial images proposed similar models used blind source separation classiﬁcation digit images documents. representations multi-layer however lend clustering interpretation representations learned model. although multi-layer promising technique learning hierarchies features data show work proposed model deep semi-nmf outperforms multi-layer fact models compared task feature learning clustering images faces. semi-supervised matrix factorization case proposed deep algorithms also evaluate method previous semi-supervised non-negative matrix factorization techniques. include constrained nonnegative matrix factorization discriminant nonnegative matrix factorization although take label information additional constraints difference between cnmf uses label information hard constrains resulting features whereas dnmf tries fisher criterion order incorporate discriminant information decomposition approaches work cases want encode prior information attribute contrast proposed deep model. deep semi-nmf semi-nmf goal construct low-dimensional representation original data serving mapping bases matrix original data lower-dimensional representation many cases data wish analyze often rather complex collection distinct often unknown attributes. work example deal datasets human faces variability data stem difference appearance subjects also attributes pose head relation camera facial expression subject. multi-attribute nature data calls hierarchical semi-nmf turn semi-nmf relaxes non-negativity constrains allows data matrix loadings matrix mixed signs restricting features matrix comprise strictly nonnegative components thus approximating following factorization fact matrix nonnegative also orthogonal every column vector would positive element making semi-nmf equivalent k-means following cost function denotes l-norm vector frobenius norm matrix. thus semi-nmf impose orthogonality constraint features matrix seen soft clustering method features matrix describes compatibility component cluster centroid base fact cost function optimize approximating semi-nmf factors indeed state-of-the-art learning features clustering based nmf-variants work compare method with among others state-of-the-art techniques learning features purpose clustering. proposed fig. deep semi-nmf model learns hierarchical structure features layer learning representation suitable clustering according different attributes data. simpliﬁed demonstration purposes example multi-pie database deep semi-nmf model able simultaneously learn features pose clustering expression clustering identity clustering images associated colour coding indicates memberships according attributes happens. case input model collection face images different subjects expressing variety facial expressions taken many angles semi-nmf model would representation would useful performing clustering according identity subjects mapping identities face images. deep semi-nmf model also ﬁnds representation data similar interpretation layer last factor however mapping identities face images analyzed product three factors corresponding mapping identities expressions corresponding mapping identities poses ﬁnally corresponding mapping identities face images. means that shown figure able decompose data different ways according different attributes above restrict implicit representations also non-negative. every layer hierarchy representations also lends clustering interpretation constitutes method radically different multilayer approaches examining figure better understand intuition algorithm suggested algorithm training deep semi-nmf model. initially approximate factors greedily using semi-nmf algorithm ﬁnetune factors reach convergence criterion. complexity pretraining stage deep semi-nmf order layers number iterations convergence maximum number components layers. complexity ﬁne-tuning stage additional iterations needed. non-linear representations linear decomposition initial data distribution fail describe efﬁciently nonlinearities exist latent attributes model. introducing non-linear functions layers enable extract features latent attributes model non-linearly separable initial input space. neurophysiology paradigms theoretical experimental evidence suggests human visual system hierarchical rather non-linear approach processing image structure neurons become selective process progressively complex features image structure. argued malo employing adaptive non-linear image representation algorithm results reduction statistical perceptual redundancy amongst representation elements. model collapse semi-nmf model. hypothesis factorizing able construct deep model able automatically learn latent hierarchy attributes representations data suitable clustering according attribute corresponds layer model; better highlevel ﬁnal-layer representation clustering according attribute lowest variability case identity face depicted. example figure would expect better features clustering according identities learning hidden representations layer suitable attributes data example clustering original images terms poses clustering face images terms expressions. order expedite approximation factors model pretrain layers initial approximation matrices greatly improves training time model. tactic employed successfully deep autoencoder networks. perform pre-training ﬁrst decompose initial data matrix k×n. following this rp×k decompose features matrix continuing rk×k pre-trained layers. afterwards ﬁne-tune weights layer employing alternating minimization factors layer order reduce total reconstruction error model according cost function equation weakly-supervised attribute learning before consider dataset faces figure dataset collection subjects number images expressing different expressions taken different angles three layer deep semi-nmf model could used automatically learn representations unsupervised manner conform latent hierarchy attributes. course features extracted without accounting available information exist attributes dataset. effect propose deep semi-nmf approach incorporate partial attribute information named weakly-supervised deep semi-nonnegative matrix factorization deep able learn semi-supervised manner hierarchy representations; level hierarchy corresponding speciﬁc attribute partial labels for. depicted figure show incorporating label information graph regularization techniques able better deep semi-nmf classifying faces according pose expression identity. also show initializing deep semi-nmf weights learned deep able improve performance deep semi-nmf task clustering faces according identity. incorporating known attribute information consider undirected graph nodes nodes corresponds data point initial dataset. node connected another node priori knowledge samples share label edge weight wij. using graph weight matrix formulate denotes graph laplacian stores prior knowledge relationship samples deﬁned diagonal matrix whose entries column implicit representations order better approximate non-linear manifolds given data matrix originally lies words using non-linear squashing function enhance expressibility model allow better reconstruction initial data. proved stone-weierstrass theorem case multilayer feedforward network structures seminmf instance arbitrary squashing functions approximate virtually function interest desired degree accuracy provided sufﬁciently many hidden units available. fig. weakly-supervised deep semi-nmf model uses prior knowledge attributes model improve ﬁnal representation data. illustration incorporate information pose expression identity attributes feature layers model hpose hexpression hidentity respectively. update rules also algorithm training model found supplementary material. incorporate available partial labelled information pose expression identity forming graph laplacian pose ﬁrst layer expression second layer identity third layer model. tune regularization parameters accordingly layers express importance parameters deep model. using modiﬁed version objective function equation derive algorithm minimizing term ensure euclidean difference ﬁnal level representations data points prior knowledge samples relationship producing similar features hand expert information even class information attribute term inﬂuence rest optimization. deriving update rules algorithm multi-layer deep model ﬁrst show simpler case layer version come pre-training model semi-nmf used pre-train purely unsupervised deep seminmf. call model weakly supervised semi-nmf wsf. learning internal model data either using purely unsupervised deep semi-nmf perform semi-supervised learning using deep model learned weights features project out-of-sample data point accomplish lower-dimensional embedding using presented methods. method basis matrix reconstruction. testing sample projected linear space deﬁned weights matrix although method used various previous works using model guarantee non-negativity algorithm proposed algorithm training deep model. initially approximate factors greedily using semi-nmf ﬁne-tune factors reach convergence criterion. using derivatives make gradient descent optimizations non-linear deep seminmf model minimize cost function respect factors model. instead linear version algorithm identity function derive multiplicative update algorithm version deep described algorithm weakly supervised factorization multiple label constraints another approach propose within framework single–layer model learns single representation based information multiple attributes. multiple-attribute extension wsf-ma accounts case multiple number attributes data matrix experiments main hypothesis deep semi-nmf able learn better high-level representations original data one-layer semi-nmf clustering according attribute lowest variability dataset. order evaluate hypothesis compared performance deep semi-nmf methods task clustering images faces distinct datasets. datasets used freely available version comprises grayscale face images subjects. person facial images different light illumination conditions. database know identity face image. extended multi modal veriﬁcation teleservices security applications contains frontal images different subjects. subject available images four different laboratory sessions total images. images eye-aligned resized order evaluate performance deep semi-nmf model compared semi-nmf also variants could useful learning representations. speciﬁcally datasets performed following experiments pixel intensities using pixel intensities images datasets course give strictly non-negative input data matrix compare reconstruction error clustering performance deep semi-nmf method semi-nmf multiplicative update rules multi-layer gnmf nenmf general trend computer vision complicated engineered features like hogs sift lbps etc. proof concept choose conduct experiments simple gradient orientations features instead pixel intensities results data matrix mixed signs expect learn better data representations clustering faces according identities. case compared deep semi-nmf one-layer semi-nmf equivalent techniques able deal mixed-sign matrices. subsection demonstrated effectiveness purely unsupervised deep semi-nmf model show next pretraining deep model auxiliary dataset using learned weights perform unsupervised deep semi-nmf lead signiﬁcant improvements terms clustering accuracy. finally three attributes multi-pie dataset test secondary hypothesis i.e. every representation layer fact suited learning according attributes corresponds layer interest. example multi-modal synthetic data previously mentioned images faces multimodal distributions composed multiple attributes pose identity. simpliﬁed example dataset figure subjects depicting poses each. example two-dimensional dataset xxor generated using samples four normal distributions previously discussed subsection semi-nmf instance single layer neural network. exist linear projection zxor maps original data distribution xxor sub-space subjects dataset linearly separable. instead employing deep factorization model using labels pose identity ﬁrst second layer respectively non-linear mapping separates identities shown figure proposed initialization semi-nmf authors instead using k-means algorithm nonetheless k-means computationally heavy number components fairly high alternative implemented approach suggests exact heuristic algorithms mean components arrange decreasing order. decided comply main assumption ﬁrst layers hierarchical model capture attributes larger variance thus model needs larger capacity encode them last layers capture attributes lower variance. fig. number layers clustering accuracy generated hundred conﬁgurations deep seminmf variant number layers evaluate ﬁnal feature layer according clustering accuracy xmvts datasets. make models comparable keep constant number components last layer generated number components rest layers according exponential distribution mean components. reconstruction error results ﬁrst experiment evaluate whether extra layers naturally introduce factors therefore difﬁcult optimize result lower quality local optimum. evaluated well matrix decomposition performed calculating reconstruction error frobenius norm difference original data reconstruction methods compared. note that order comparable results methods stopping criterion rules. maximum amount iterations convergence rule order stop process reconstruction error current previous update small enough. experiments subsection shows change reconstruction error respect selected number features methods used multi-pie dataset. results show semi-nmf manages reach much lower reconstruction error methods consistently would match expectations constrain weights non-negative. layer features fig. features extracted layers deep factorization model artiﬁcially generated dataset. second layer manages projection initial data makes classes linearly separable task infeasible simple seminmf model. similarly speed convergence rate non-negative double singular value decomposition suggested boutsidis nndsvd method based processes approximate initial data matrix approximate positive sections resulting partial factors. gnmf experimental setup chose suitable number neighbours create regularizing graph visualizing datasets using laplacian eigenmaps visually distinct clusters number layers important experimental setup selected structure multi-layered models. careful preliminary experimentation focused experiments involve hidden layer architectures deep semi-nmf multi-layer nmf. speciﬁcally experimented models ﬁrst hidden representation features second representation number features ranged allowed comparable conﬁgurations different datasets reasonable compromise speed accuracy. nonetheless figure show experiments layers datasets. latter experiment generated hundred conﬁgurations deep seminmf variant number layers evaluated ﬁnal feature layer according clustering accuracy xmvts multipie datasets. make models comparable keep constant number components last layer generated number components rest layers drawn exponential distribution fig. xmvts-pixel intensities accuracy clustering based representations learned model respect identities. deep architectures comprised representation layers representations used layer. parenthesis show scores. fig. pie–pixel intensities accuracy clustering based representations learned model respect identities. deep architectures comprised representation layers representations used layer. parenthesis show scores. clustering results achieving satisfactory reconstruction error method proceeded evaluate features learned ﬁnal representation layer using k-means clustering assess clustering quality representations produced algorithms compared take advantage fact datasets already labelled. metrics used accuracy normalized mutual information metric deﬁned cleaner presentation included experiments supplement. figures show comparison clustering accuracy using k-means feature representations produced techniques compared input matrix contained pixel intensities image. method signiﬁcantly outperforms every method compared datasets terms clustering accuracy. using igos deep semi-nmf able outperform single-layer semi-nmf shown figures making simple mixed-signed features improved clustering accuracy considerably. noted cases exception pose experiment igos deep seminmf outperformed methods difference performance statistically signiﬁcant important note deep seminmf models signiﬁcantly lower reconstruction error compared equivalent semi-nmf models even though approximation involves factors. multi-layer gnmf larger reconstruction error return uncovering meaningful features counterpart. supervised pre-training optimization process deep architectures highly non-convex initialization point process important factor obtaining good ﬁnal representation initial dataset. following trends deep learning show supervised pretraining model auxiliary dataset using learned weights initialization points unsupervised experiments models supervised pre-training outperformed ones without shown figure terms clustering accuracy. additionally validates claim pretraining exploited better representations unlabelled data. fig. supervised pre-training clustering accuracy dataset supervised training xmvts dataset using priori deep semi-nmf parenthesis show scores. learning respect different attributes finally conducted experiments classiﬁcation using three representations learned three-layered deep models input pixel intensities images larger subset multi-pie dataset. multi-pie contains around images subjects captured laboratory conditions four different sessions. work used subset images subjects different poses expressing different emotions amount samples annotations imposed illumination conditions. using annotations aligned images based common frame. that resized smaller resolution database comes labels attributes mentioned above identity illumination pose expression. used multi-pie experiment since identity labels datasets. split subset training validation images rest testing. compare classiﬁcation performance classiﬁer using data representations semi-nmf deep seminmf models attribute information. fig. xmvts-igo accuracy scores clustering based representations learned model respect identities. deep architectures comprised hidden layers representations used layer. parenthesis show scores. fig. pie-igo accuracy scores clustering based representations learned model respect identities. deep architectures comprised hidden layers representations used layer. parenthesis show scores. auxiliary dataset xmvts resize images resolution match image resolution primary dataset. splitting xmvts dataset training/validation sets learn weights zxmvts using deep model layers regularization parameters supervised task initialization point perform unsupervised ﬁne-tuning dataset. fig. three layer deep model trained multipie frontal illumination bars depict accuracy levels pose emotion identity respectively layer linear classiﬁer. technique able learn high-level ﬁnal-layer representation clustering respect attribute lowest variability case popular datasets face images outperforming considered range typical powerful nmf-based techniques. proposed deep incorporates knowledge known attributes dataset might available. deep used datasets annotated attributes even combination different data sources providing different attribute information. demonstrated abilities model multi-pie dataset using additional information provided training pose emotion identity information subject able uncover better features attributes model learning available attributes simultaneously. moreover shown deep could used pretrain models auxiliary datasets speed learning process also uncover better representations attribute interest. future avenues include experimenting applications e.g. area speech recognition especially multi-source speech recognition investigate multilinear extensions proposed framework acknowledgments george trigeorgis recipient fellowship department computing imperial college london work partially funded work konstantinos bousmalis funded partially google europe fellowship social signal processing. work stefanos zafeiriou partially funded epsrc project ep/j/ work bj¨orn schuller partially funded european cnmf dnmf models attribute labels attribute classifying wsf-ma deep learned data representations based attribute information available. table demonstrate performance accuracy methods. methods feature layer components case also compared performance deep wsf-ma whether different levels representation amount better performance classiﬁcation tasks attributes represented. cases also comparison rest state-of-the-art unsupervised semisupervised matrix factorization techniques proposed solution manages extract better features task hand seen table classiﬁcation. conclusion introduced novel deep architecture seminon-negative matrix factorization deep semi-nmf able automatically learn hierarchy attributes given dataset well representations suited clustering according attributes. furthermore presented algorithm optimizing factors deep semi-nmf evaluate performance compared single-layered semi-nmf related work problem clustering faces respect identities. shown paatero tapper positive matrix factorization nonnegative factor model optimal utilization error estimates data values environmetrics vol. j.-p. brunet tamayo golub mesirov metagenes molecular pattern discovery using matrix factorization pnas vol. berry browne email surveillance using nonnegative matrix factorization computational mathematical organization theory vol. zafeiriou tefas buciu pitas exploiting discriminant information nonnegative matrix factorization application frontal face veriﬁcation vol. kotsia zafeiriou pitas novel discriminant nonnegative matrix factorization algorithm applications facial image characterization problems. tifs vol. weninger schuller optimization parallelization monaural source separation algorithms openblissart toolkit journal signal processing systems vol. ding jordan convex semi-nonnegative matrix factorizations ieee tpami vol. cing simon equivalence nonnegative matrix factorization spectral clustering proc. siam data mining kotsia zafeiriou pitas novel discriminant nonnegative matrix factorization algorithm applications facial image characterization problems tifs vol. riesenhuber poggio hierarchical models object recognition cortex. nature neuroscience vol. messer matas kittler luettin maitre xmvtsdb extended mvts database international conference audio video-based biometric person authentication vol. citeseer gong document clustering based nonnegative matrix factorization sigir. lecun bottou ¨uller efﬁcient backprop lecture notes computer science vol. lectu konstantinos bousmalis researcher working google robotics california. stefanos zafeiriou lecturer department computing imperial college london. ¨orn schuller lecturer department computing imperial college london.", "year": 2015}