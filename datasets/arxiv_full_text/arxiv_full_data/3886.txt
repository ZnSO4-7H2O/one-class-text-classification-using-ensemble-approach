{"title": "Extremal Optimization: an Evolutionary Local-Search Algorithm", "tag": ["cs.NE", "cs.AI", "I.2.8"], "abstract": "A recently introduced general-purpose heuristic for finding high-quality solutions for many hard optimization problems is reviewed. The method is inspired by recent progress in understanding far-from-equilibrium phenomena in terms of {\\em self-organized criticality,} a concept introduced to describe emergent complexity in physical systems. This method, called {\\em extremal optimization,} successively replaces the value of extremely undesirable variables in a sub-optimal solution with new, random ones. Large, avalanche-like fluctuations in the cost function self-organize from this dynamics, effectively scaling barriers to explore local optima in distant neighborhoods of the configuration space while eliminating the need to tune parameters. Drawing upon models used to simulate the dynamics of granular media, evolution, or geology, extremal optimization complements approximation methods inspired by equilibrium statistical physics, such as {\\em simulated annealing}. It may be but one example of applying new insights into {\\em non-equilibrium phenomena} systematically to hard optimization problems. This method is widely applicable and so far has proved competitive with -- and even superior to -- more elaborate general-purpose heuristics on testbeds of constrained optimization problems with up to $10^5$ variables, such as bipartitioning, coloring, and satisfiability. Analysis of a suitable model predicts the only free parameter of the method in accordance with all experimental results.", "text": "recently introduced general-purpose heuristic ﬁnding high-quality solutions many hard optimization problems reviewed. method inspired recent progress understanding far-from-equilibrium phenomena terms self-organized criticality concept introduced describe emergent complexity physical systems. method called extremal optimization successively replaces value extremely undesirable variables sub-optimal solution random ones. large avalanche-like ﬂuctuations cost function self-organize dynamics effectively scaling barriers explore local optima distant neighborhoods conﬁguration space eliminating need tune parameters. drawing upon models used simulate dynamics granular media evolution geology extremal optimization complements approximation methods inspired equilibrium statistical physics simulated annealing. example applying insights non-equilibrium phenomena systematically hard optimization problems. method widely applicable proved competitive even superior elaborate general-purpose heuristics testbeds constrained optimization problems variables bipartitioning coloring satisﬁability. analysis suitable model predicts free parameter method accordance experimental results. extremal optimization general-purpose local search heuristic based recent progress understanding far-from-equilibrium phenomena terms self-organized criticality inspired previous attempts using physical intuition optimize simulated annealing genetic algorithms opens door systematically applying non-equilibrium processes manner applies equilibrium statistical mechanics. appears powerful addition mentioned meta-heuristics generality ability explore complicated conﬁguration spaces efﬁciently. despite original aspirations even conceptually elegant methods provide panacea optimization. incredible diversity problems resembling physics would allow that. hence need creative alternatives arises. show provides true alternative approach advantages disadvantages compared general-purpose heuristics. method choice many problems; fate shared methods. based existing studies believe prove indispensable problems general-purpose heuristics become. next section motivate terms evolutionary model sneppen sec. discuss general eo-implementation example graph bipartitioning. finally sec. describe implementations problems results obtained. heuristic motivated bak-sneppen model biological evolution model species located sites lattice associated ﬁtness value time step species smallest value selected random update ﬁtness replaced value drawn randomly distribution interval change ﬁtness species impacts ﬁtness interrelated species. therefore species connected weakest ﬁtness replaced random numbers well. sufﬁcient number steps system reaches highly correlated state known self-organized criticality state almost species reached ﬁtness certain threshold. species possess punctuated equilibrium one’s weakened neighbor undermine one’s ﬁtness. coevolutionary activity gives rise chain reactions avalanches large ﬂuctuations rearrange major parts system potentially making conﬁguration accessible. although coevolution optimization exclusive goal serves powerful paradigm follows spirit baksneppen model merely updates variables extremal arrangement current conﬁguration replacing random values without ever explicitly improving them. large ﬂuctuations allow escape local minima efﬁciently explore conﬁguration space extremal selection process enforces frequent returns near-optimal conﬁgurations. selection contrasts sharply breeding pursued gas. many practical decision-making problems modeled analyzed terms standard combinatorial optimization problems intractable ones provided class np-hard problems problems considered hard solve require computational time general grows faster power number variables instance discern optimal solution close analogy many real-world optimization problems study problems spawned development efﬁcient approximation methods called heuristics methods approximate near-optimal solutions rapidly example hard problem constraints graph bi-partitioning problem variables given vertices even. edges connect certain pairs vertices form instance graph. problem partitioning vertices subsets constrained exactly size minimal number edges subsets. size conﬁguration space grows exponentially since unordered divisions vertices equal-sized sets feasible conﬁgurations cost function counts number edges need separate subsets. typical local-search neighborhood arises -exchange vertex subset simplest update preserves global constraint. near-optimal solutions hard problem performs search single conﬁguration particular optimization problem. characteristically consists large number variables theses variables usually obtain state could boolean p-state continuous assume possesses neighborhood originating updates variables. cost assumed linear function ﬁtness assigned variable typically example satisﬁed attribute vertex local cost −bi/ number edges equally shared vertex edge. update vertex identiﬁed possesses lowest ﬁtness neighboring conﬁguration chosen -exchange swapping randomly selected vertex opposite set. algorithm operates single conﬁguration step. variable ﬁtness worst identiﬁed. ranking variables provides measure quality implying variables better current move neighboring conﬁguration typically small number variables change state connected variables need re-evaluated re-ranked note single parameter adjust selection better solutions aside ranking. fact memory encapsulated ranking directs neighborhood increasingly better solutions. hand choice move consideration given outcome move even worst variable guaranteed improve ﬁtness. accordingly large ﬂuctuations cost accumulate sequence updates. merely bias extremelybad ﬁtnesses enforces repeated returns near-optimal solutions. random geometric graphs connectivity connectivity figure optimized conﬁguration found graph barely percolates merely edge connecting round points square points. denser graph bottom reduced cutsize -exchange turn square vertex round round vertex square one. evolution cost function typical figure bipartitioning -vertex graph introduced ref. best cost ever found contrast large ﬂuctuations early stages converges much later extremal optimization quickly approaches stage broadly distributed ﬂuctuations allow probe escape many local minima. plot average costs obtained spin glass graph figure bipartitioning function size number instances generated. instance different runs performed results averaged runs instances. although problems quite distinct either case best results obtained satisfy step below sometimes neighborhood chosen problem turns deterministic process selecting always worst variable step leaves choice step like iterative improvement eo-process would stuck local minima. avoid dead ends improve results generally introduce single parameter algorithm. parameter remains ﬁxed varies problem system size parameter allows exploit memory contained ﬁtness ranking detail. permutation labels given value parameter update select rank according then modify step variable gets chosen update step example case -exchange select numbers according swap vertex vertex updating lowest-ranked variable bound reach dead extremes results typically poor. however intermediate values choice power-law distribution ensures rank gets excluded evolution still maintaining bias variables ﬁtness. show next section algorithm analyzed show asymptotic choice optimizes performance algorithm veriﬁed problems studied exempliﬁed fig. stochastic local search heuristics notoriously hard analyze. powerful results derived convergence properties dependence temperature schedule based well-developed knowledge equilibrium statistical physics markov processes. predictions particular optimization problems between. often instance analyzed simpliﬁed models ref. gain insight workings general-purpose heuristic. studied appropriately designed model problem able reproduce many properties observed realistic implementations. particular found analytical results average convergence function ref. considered model consisting a-priori independent variables. variable take three ﬁtness states respectively assigned fractions variables optimal state cost λi/n according system model dynamics local search hard problems designing interesting equations mimic complex search space energetic entropic barriers instance equations specify fraction variables transfer ﬁtness state another given variable certain state updated. update probabilities easily derived giving highly nonlinear dynamic system. local searchs studied model comparison particular design allows study generic feature local search suggested close analogy optimization problems low-temperature properties spin glasses many update steps variables freeze near-perfect local arrangement resist change ﬁnite fraction remains frustrated poor local arrangement frozen variables dislocated collectively accommodate frustrated variables system whole plot cost averaged many runs function figure ref. reaches minimum value near prediction τopt higher-order corrections] rises sharply beyond that similar empirical ﬁndings figs. a-b. improve state. highly correlated state slow variables block progression fast variables emerges. asymptotic analysis equations jammed system indeed reproduces features previously conjectured numerical data real optimization problems. especially predicts value cost minimal given runtime model provides ideal setting probe deeper properties compare local search methods. similarly analyzed terms homogeneous markov chain although little effort made direction theoretical investigations hand-in-hand experimental studies provide clearer picture capabilities part project often compare combine metaheuristics problem speciﬁc methods show provides alternative philosophy canon heuristics. distinctions imply methods fundamentally better worse. contrary differences improve chances least heuristics provide good results particular problem others fail times best results obtained hybrid heuristics apparent distinction methods need deﬁne local cost contributions variable instead global cost. eo’s capability seems derive ability access local information directly. simulated annealing emulates behavior frustrated systems thermal equilibrium couples system heat bath adjustable temperature cooling system slowly come close attaining state minimal energy accepts rejects local changes conﬁguration according metropolis algorithm given temperature enforcing equilibrium dynamics requiring carefully tuned temperature schedule contrast drives system equilibrium aside ranking applies decision criteria conﬁgurations accepted indiscriminately. instead tuning schedule parameters often requires choices. appear eo’s results resemble ineffective random search similar ﬁxed ﬁnite temperature fact persistent selection worst ﬁtnesses quickly approaches nearoptimal solutions. large ﬂuctuations remain late runtimes escape deep local minima access regions conﬁguration space. versions acceptance rates near freezing circumvented using scheme picking trials rank-ordered list possible moves derived continuous-time monte carlo methods like every move gets accepted. moves based outcome-oriented ranking favoring downhill moves permitting limited uphill moves. hand ranking variables based current future state variable allowing unlimited uphill moves. genetic algorithms although similarly motivated evolution algorithms hardly anything common. mimicking evolution genotypical level keep track entire gene pools conﬁgurations many tunable parameters select breed improved generation solutions. comparison based competition phenomenological level species operates local updates single conﬁguration improvements achieved persistent elimination variables. general-purpose heuristics local search. contrast cross-over operators perform global exchanges pair conﬁgurations. table best cutsizes testbed large graphs. results best reported results runs out-pacing results almost order magnitude large comparison data three large graphs results spectral heuristics ref. metis partitioning program based hierarchical reduction instead local search obtaining extremely fast deterministic results memory permits escapes local minima avoids recently explored conﬁgurations. similar converge moves ranked. uphill moves limited tuned parameters evaluate memory. above rankings scoring moves done basis anticipated outcome current ﬁtness individual variables. conducted whole series projects demonstrate capabilities simple implementations obtaining near-optimal solutions -coloring graphs ising spin-glass problem case studied statistically relevant number instances ensemble variables chosen where really hard problems results discussed following. table summarize early results implementation testbed graphs large here best-of- runs. graph used many update steps appeared productive reliably obtain stable results. varied particularities graph reported runtimes inﬂuenced this. extensive numerical study random geometric graphs shown outperforms signiﬁcantly near phase transitions cutsizes ﬁrst become non-zero. compared averaged best results obtained methods large number instances increasing ﬁxed parameter setting. used algorithm plot error best result relative eo’s identical instances figure random graphs geometric graphs function average connectivity critical points random graphs geometric graphs. sa’s error relative near critical point cases rises described sec. used algorithm developed johnson geometric temperature schedule temperature length equalize runtimes programs used data structure requiring small extra overhead sorting ﬁtness variables heap clearly since update leads move entails sorting individual updates take much longer trial step. fig. shows gets rapidly worse near phase transition relative equalized cpu-time. studies average rate convergence toward better-cost conﬁgurations function runtime indicate power-law convergence roughly like hcit also found ref. course easy assert graphs large runs fact converge closely optimum ﬁnite-size scaling analysis random graphs justiﬁes expectation instance graph coloring consists graph vertices connected edges like gbp. considered problem max-k-col given different colors label vertices coloring graph minimizes number monochromatic edges connect vertices identical color. max-k-col deﬁne ﬁtness −bi/ like number monochromatic edges emanating vertex since global constraints simple random reassignment color selected variable sufﬁcient local-search neighborhood. plot average cost backbone fraction function figure average connectivity random graph -coloring. data collapse according insert left predicts critical point random graphs ccrit generated value instances respectively. studied max--col problem near phase transition hardest instances reside ref. phenomena phase transition studied ﬁrst -col. here used completely enumerate optimal solutions smin near critical point -col random graphs. instances random graphs typically high ground-state degeneracy possess large number equally optimal solutions smin. ref. shown phase transition -sat fraction constrained variables found identical state almost smin discontinuously jumps non-zero value. conjectured ﬁrst-order phase transition backbone general phenomenon np-hard optimization problems. test conjecture -col generated large number random graphs explored many ground states could ﬁnd. instance measured optimal cost backbone fraction ﬁxed pairs vertices. results fig. allow estimate precisely location transition scaling behavior cost function. ﬁnite-size scaling ansatz collapse data average ground-state cost onto single scaling curve signiﬁcant physical relevance temperature properties spin glasses closely related max-cut problems originally designed applications spin glasses mind successful results obtained systems many physical classic combinatorial optimization problems cast terms spin glass spin glass consists lattice graph spin variable placed vertex every spin connected nearest neighbors ﬁxed bond variable drawn random distribution zero mean unit variance. spins coupled arbitrary external ﬁeld optimization problem consists ﬁnding minimum cost states smin hamiltonian arranging spins optimal conﬁguration hard frustration variables will individually collectively never able satisfy constraints imposed them. cost function equivalent integer quadratic programming problems implementation spin glass random nearest-neighbor bonds cubic lattice used large number realizations instance restarts random initial conditions retaining lowest energy state obtained averaging instances. inspection results convergence genetic algorithms refs. suggest computational cost least consistent performance. indeed using updates enables reproduce lowest energy states restarts results listed tables data energy spin deﬁned const/n predicts consistent table approximations average ground-state energy spin spin glass compared results refs. size studied large number instances. also shown average time needed presumed ground state pentium. gauge eo’s performance larger implementation also lattice instances toruspm-- toruspm-- considered dimacs challenge semideﬁnite problems bounds ground-state cost established larger instance clower cupper found signiﬁcant improvement upper bound already lower above. furthermore collected states roughly segregate clusters mutual hamming distance least distinct spins. smaller instance bounds given resp. ﬁnds took minutes results larger instance require hours.", "year": 2002}