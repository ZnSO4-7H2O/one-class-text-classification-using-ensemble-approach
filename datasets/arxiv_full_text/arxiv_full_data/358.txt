{"title": "Three Factors Influencing Minima in SGD", "tag": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "abstract": "We study the properties of the endpoint of stochastic gradient descent (SGD). By approximating SGD as a stochastic differential equation (SDE) we consider the Boltzmann-Gibbs equilibrium distribution of that SDE under the assumption of isotropic variance in loss gradients. Through this analysis, we find that three factors - learning rate, batch size and the variance of the loss gradients - control the trade-off between the depth and width of the minima found by SGD, with wider minima favoured by a higher ratio of learning rate to batch size. We have direct control over the learning rate and batch size, while the variance is determined by the choice of model architecture, model parameterization and dataset. In the equilibrium distribution only the ratio of learning rate to batch size appears, implying that the equilibrium distribution is invariant under a simultaneous rescaling of learning rate and batch size by the same amount. We then explore experimentally how learning rate and batch size affect SGD from two perspectives: the endpoint of SGD and the dynamics that lead up to it. For the endpoint, the experiments suggest the endpoint of SGD is invariant under simultaneous rescaling of batch size and learning rate, and also that a higher ratio leads to flatter minima, both findings are consistent with our theoretical analysis. We note experimentally that the dynamics also seem to be invariant under the same rescaling of learning rate and batch size, which we explore showing that one can exchange batch size and learning rate for cyclical learning rate schedule. Next, we illustrate how noise affects memorization, showing that high noise levels lead to better generalization. Finally, we find experimentally that the invariance under simultaneous rescaling of learning rate and batch size breaks down if the learning rate gets too large or the batch size gets too small.", "text": "study properties endpoint stochastic gradient descent approximating stochastic differential equation consider boltzmann-gibbs equilibrium distribution assumption isotropic variance loss gradients. analysis three factors learning rate batch size variance loss gradients control trade-off depth width minima found wider minima favoured higher ratio learning rate batch size. direct control learning rate batch size variance determined choice model architecture model parameterization dataset. equilibrium distribution ratio learning rate batch size appears implying equilibrium distribution invariant simultaneous rescaling learning rate batch size amount. explore experimentally learning rate batch size affect perspectives endpoint dynamics lead endpoint experiments suggest endpoint invariant simultaneous rescaling batch size learning rate also higher ratio leads ﬂatter minima ﬁndings consistent theoretical analysis. note experimentally dynamics also seem invariant rescaling learning rate batch size explore showing exchange batch size learning rate cyclical learning rate schedule. next illustrate noise affects memorization showing high noise levels lead better generalization. finally experimentally invariance simultaneous rescaling learning rate batch size breaks learning rate gets large batch size gets small. despite massively over-parameterized deep neural networks demonstrated good generalization ability achieved state-of-the-art performances many application domains image speech recognition reason success focus research recently still remains open question. work provides theoretical insights useful suggestions deep learning practitioners. standard training dnns involves minimizing loss function using variants parameters updated taking small discrete step depending learning rate direction negative loss gradient approximated based small subset training examples since loss functions dnns highly non-convex functions parameters complex structure potentially multiple minima saddle points generally converges different regions parameter space depending optimization hyper-parameters initialization. ∗first authors contributed equally †jagiellonian university ‡mila université montréal §facebook research ¶university bonn cifar senior fellow ∗∗university edinburgh recently several works investigated impacts generalization dnns. argued wide minima tend generalize better sharp minima entirely compatible bayesian viewpoint emphasizes targeting probability mass associated solution rather density value solution speciﬁcally larger batch sizes correlate sharper minima. contrast ratio learning rate batch size correlated sharpness minima batch size alone. vein discuss existence sharp minima behave similarly terms predictions compared wide minima argue naturally tends wider minima higher noise levels gradients wider minima seem correlate better generalization. order achieve goal approximate continuous stochastic differential equation assuming isotropic gradient noise derive boltzmann-gibbs equilibrium distribution stochastic process derive relative probability landing local minima compared another terms depth width. main ﬁnding ratio learning rate batch-size along gradient’s covariances inﬂuence trade-off depth sharpness ﬁnal minima found high ratio learning rate batch size favouring ﬂatter minima. addition analysis provides theoretical justiﬁcation empirical observation scaling learning rate linearly batch size leads identical performance dnns verify theoretical insights experimentally different models datasets. particular demonstrate high learning rate batch size ratio leads wider minima correlates well better validation performance. also show high learning rate batch size ratio helps prevent memorization. furthermore observe multiplying learning rate batch size scaling factor results similar training dynamics. extending observation validate experimentally exchange learning rate batch size recently proposed cyclic learning rate schedule learning rate oscillates levels. finally discuss limitations theory practice. relationship sampling posterior distribution stochastic langevin methods subject discussion number papers particular describe dynamics stochastic gradient descent stochastic process divided three distinct phases. ﬁrst phase weights diffuse move away initialization. second phase gradient magnitude dominates noise gradient estimate. ﬁnal phase weights near optimum. make related observations information theoretic point view suggest diffusion behaviour parameters last phase leads minimization mutual information input hidden representation. also relate dynamics stationary distribution stochastic differential equation. derivation bears similarity however study approximate bayesian inference method ﬁnal phase optimization locally convex setting goal analyze stationary distribution entire parameter space reached sgd. further analysis allows compare probability ending minima another novel case. work also closely related ongoing discussion role large batch size sharpness minima found terms generalization showed ends sharp minimum using large batch size. empirically observed scaling learning rate training epochs leads good generalization using large batch size. novelty explaining importance ratio learning rate batch size. particular theoretical empirical concurrent work analyzed approximated continuous time stochastic process stressed importance learning rate batch size ratio. focused training dynamics explored stationary non-equilibrium solution stochastic differential equation non-isotropic gradient noise assuming conditions covariance loss enforce stationary distribution path-independent. solution explicit solution terms loss case. contrast work strictly focus explicitly solvable case boltzmann-gibbs equilibrium distribution isotropic noise. focus allows relate noise controlled learning rate batch size ratio width endpoint. empirically verify width height minima correlates learning rate batch size ratio practice. work continues line research importance noise novelty formalizing impact batch size learning rate width depth ﬁnal minima empirical veriﬁcations this. focus section ﬁnding relative probability optimization certain minimum compared another minimum. relative probability depends local geometry loss function minimum along batch size learning rate covariance loss gradients. reach result ﬁrst derive equilibrium distribution parameter space stochastic differential equation treatment. make assumption isotropic covariance loss gradients allows write explicit expression equilibrium distribution turns boltzmann-gibbs distribution. follow theoretical setup similar approximating continuoustime stochastic process outline. consider model parameterized θq}. training examples loss function corresponding gradient deﬁned based loss values training examples. stochastic gradients arise consider batch size random indices drawn uniformly form estimate loss gradient based corresponding subset training examples langevin equation stochastic differential equation interested equilibrium distribution gives insights behavior properties points converges assuming isotropic noise langevin equation well known gibbsboltzmann distribution equilibrium distribution. equilibrium distribution derived ﬁnding stationary solution fokker-planck equation detailed balance governs evolution probability density value parameters time. fokkerplanck equation derivation found appendix equation appendix restate standard proofs stationary distribution langevin system provide resulting gibbs-boltzmann equilbirium distribution here using notation paper theorem assume gradient covariance isotropic i.e. constant. equilibrium distribution stochastic differential equation given discussion deﬁnes density parameter space. result says long enough probability parameters particular state asymptotically follows density. note measure noise system choice learning rate batch size fact loss divided emphasizes higher noise less granular loss surface appears sgd. gradient variance hand determined dataset model priors reveals important area investigation i.e. different architectures model parameterizations affect gradient’s covariance structure. note analysis above assumption gradient covariance ﬁxed isotropic parameter space unrealistic. however simpliﬁcation enables straightforward insights regarding relationship noise batch size learning rate gibbs-boltzmann equilibrium. empirically show various predictions based relationship hold practice. returning optimization method given probability density derive probability ending given minimum denote lowercase ˜pac normalization constant minima probability derived appendix given following theorem core result theory. theorem assume loss locally strictly convex hessian loss minimum unnormalized probability ending minima discussion analysis qualitatively categorize minima loss determinant hessian here also assume weak regularity condition loss includes regularization term result shows probability landing speciﬁc minimum depends three factors learning rate batch-size covariance gradients. factors directly control appear ratio given noise η/s. case highlights equilibrium favors minimum lower determinant hessian factors identical. side seen minima curvature favor minima lower loss. finally general case holds upper bound inverse noise favored case loss higher upper bound depends difference heights compared ratio widths. particular amount noise result probable words minimum higher sharper minimum never reached higher probability regardless amount noise. however lower bound noise make probable words minimum higher ﬂatter minimum favored long noise large enough deﬁned summarize presented theory shows noise level controls extent optimization favors wider deeper minima. increasing noise increasing ratio learning rate batch size increases probability wider compared deeper minima. discussion relative probabilities critical points strictly minima appendix section empirically study impact learning rate batch size local minimum ﬁnds. ﬁrst focus -layer batch normalized relu trained fashion-mnist study noise ratio leads minima different curvatures validation accuracy. measure curvature minimum compute norm hessian using ﬁnite difference method figure report norm hessian local minima obtained different experiment epochs; models reach approximately accuracy train set. grows observe norm hessian minima also decreases suggesting higher pushes optimization towards ﬂatter minima. agrees theorem higher favors ﬂatter sharper minima. figure shows results exploring impact ﬁnal validation performance conﬁrms better generalization correlates higher taken together fig. fig. imply wider minima correlate well better generalization. increases ﬁnds local minima generalize better. appendix report similar results resnet applied cifar figure layer relu network good initialization schemes figures initilization figure illustrate behavior different noise levels train three resnet models cifar using different baseline model uses comparision investigate large batch model figure interpolation parameters models trained learning rate batch-size ratio η=.×β s=×β different values determined predicted theory minima models identical noise levels qualitatively similar seen plots. figure learning rate schedule replaced equivalent batch size schedule. ratio learning rate batch size equal times blue curves plot. plots show train test accuracy experiments involving vgg- architecture cifar dataset. left cyclic batch size schedule range compared cyclic learning rate schedule range right constant batch size constant learning rate compared constant batch size constant learning rate small learning rate model ratio. follow investigating loss line interpolating parameters models. speciﬁcally ﬁnal parameters found report loss values results indicate using different models large batch size learning rate sharper minimum relative baseline model. plots consistent theoretical analysis higher gives preference wider minima sharper minima. hand ﬁgure shows models trained roughly level noise minima similar quality. following experiment explores aspect further. train vgg- models cifar- models trained noise level different values learning rate batch size. speciﬁcally η=.×β s=×β interpolate model parameters found training interpolation results indicate minima similar width depth qualitatively supporting theoretical observation noise ratio ends minima similar quality. section look experimental phenomena ﬁrstly equilibrium endpoint secondly dynamical evolution sgd. former theoretically analysed theory section latter directly addressed theory section note related endpoint result intermediate dynamics. experimentally study phenomena following four experiments involving architecture cifar dataset shown left plot compares experiments cyclic batch size schedule range compared cyclic learning rate schedule range right plot compares experiments constant learning rate batch-size ratio regarding ﬁrst phenomena endpoint test accuracy training cyclic batch size cyclic learning rate respectively emphasize similar scores. constant learning rate batch-size ratio test accuracy respectively emphasize scores similar other. experiments endpoint test accuracies figure impact memorization mnist labels training replaced random labels using momentum momentum parameter observe speciﬁc level memorization high leads better generalization. higher value similar shows exchangability learning rate batch size endpoint consistent theoretical calculation says characteristics minima found endpoint determined ratio learning rate batch-size individually learning rate batch size. regarding second phenomena dynamical evolution note similarity training test accuracy curves pair same-noise curves experiment. theoretical analysis explain phenomena determine dynamical distribution. nonetheless report interesting observation point appendix intuition occur fokker-planck equation. appendix fig. show detail loss curves. epoch-averaged loss curves match well exchanging batch size learning rate per-iteration loss invariant switching batch size learning rate. particular note smaller batch-size higher variance per-iteration loss it’s same-noise pair. expected since iteration next examples higher variance smaller batch-size. take-away message section endpoint dynamics approximately invariant batch size learning rate simultaneously rescaled amount. contrast commonly used heuristic consisting scaling learning rate square root batch size i.e. keeping ratio constant. used example keeping covariance matrix parameter update step batch size. however theory experiments suggest changing learning rate batch size keeps ratio constant instead since results equilibrium distribution. generalize well model must identify underlying pattern data instead simply perfectly memorizing training example. empirical approach test memorization analyze good training true labels partly replaced random labels experiments described section highlight sufﬁcient amount noise improves generalization given level memorization. experiments performed mnist dataset similar used hidden units. train different amounts random labels training set. level label noise evaluate impact generalization performance. speciﬁcally experiments taking values grid batch size learning rate momentum models trained epochs. fig. reports mlps performances noisy training validation set. results show larger noise leads solutions generalize better amount random labels memorized training set. thus analysis highlights noise steers endpoint optimization towards minimum generalization ability. figure cyclical schemes oscillate sharp wide regions. additionally cyclical schemes wider minima baseline level loss might explain better generalization. cyclical schedules base cycle length epochs approximate convergence cycle. plots left right discrete discrete triangle constant learning rate constant learning rate vertical axis report loss approximated norm hessian table comparison different cyclical training schedules discrete schedules perform similarly slightly better triangular. additionally discrete schedule leads much wider minima similar loss. hessian norm approximated repetitions measured minimum value reports generalization endpoint observe larger noise continuously steers away sharp solutions throughout dynamics. also reproduce observation reported memorization roughly starts reaching maximum generalization. runs momentum exclude learning rates higher lead divergence. full learning curves reported fig. included appendix observed cyclic learning rate schedule leads better generalization sec. demonstrated exchange cyclic learning rate schedule cyclic batch size approximately preserve practical beneﬁt clr. exchangeability shows generalization beneﬁt must come varying noise level rather cycling learning rate. explore helps generalization cifar using training schedules compared discrete schedules baseline schedules constant triangle track norm hessian training loss throughout training. experiment repeated times. schedule optimize cycle length validation set. cyclical schedules maximum value larger minimum value. first observe cyclical schemes oscillate sharp wide regions parameter space fig. next empirically demonstrate discrete schedule varying either performs similarly slightly better triangular schedule tab. finally observe cyclical schemes reach wider minima loss value fig. tab. suggest changing noise levels cyclical schemes reach different endpoints constant learning rate schemes ﬁnal noise level. leave exploration implications thorough comparison learning schedules future work. figure breaking point analysis theory suggests ﬁnal performance similar noise level terms large learning rate small batch size. validation accuracy different dataset sizes different values vgg- architecture trained cifar. experiment multiply learning rate batch size ratio ﬁxed. observe ratio increasing learning rate batch size yields similar performance certain value performance drops signiﬁcantly. breaking point analysis half noise level used. breaking point happens much larger using smaller noise. experiments repeated times different random seeds. graphs denote mean validation accuracies numbers brackets denote mean standard deviation maximum validation accuracy across different runs. denotes least seed lead divergence. analysis relies assumption gradient step sufﬁciently small guarantee ﬁrst order approximation taylor’s expansion good estimate loss function. case learning rate becomes high approximation longer suitable continuous limit discrete update equation longer valid. case stochastic differential equation doesn’t hold hence neither fokker-planck equation don’t expect theory valid. particular don’t expect arrive stationary distribution indicated ﬁxed ratio learning rate gets high. exempliﬁed empirical results reported fig. similar learning dynamics ﬁnal performance observed simultaneously multiplying learning rate batch size factor certain limit. done different training sizes investigate breaking point depends factor plots suggest breaking point happens smaller values dataset size smaller. also investigate inﬂuence half noise level used halving learning rate experiments strongly suggest reason behind breaking point high learning rate performance drops much higher base learning rate halved. similar experiment performed resnets highlight limitations theory appendix theoretical section work treat learning rate ﬁxed throughout training. however practical applications learning rate annealed lower value either gradually discrete jumps. viewed within framework beginning high noise favors width depth region noise decreases prioritizes depth strongly seen theorem comments follow. theoretical section made additional assumption covariance gradients isotropic order able derive closed form solution equilibrium distribution. expect assumption hold practice speculate mechanisms drive covariance towards isotropy example able tune learning rates per-parameter basis combination learning rate covariance matrix approximately isotropic lead improvements optimization. perhaps existing mechanisms batch normalization careful initialization give rise equalized covariance leave study future work. note theoretical analysis considered equilibrium distribution independent intermediate dynamics. however case practice. without isotropic covariance system partial differential equations late time limit general solution depend path optimization occurs unless restrictive assumptions made force path dependence disappear despite simplifying assumption empirical results consistent developed theory. leave study path dependence dynamics future work. experiments investigating memorization explored noise level changes preference wide minima sharp ones. argues ﬁrst learns true labels focusing random labels. insight second phase high level noise maintains generalization. illustrates trade-off width minima depth practice. noise level lower dnns likely random labels better expense generalizing less well true ones. shed light role noise optimization dnns argue three factors strongly inﬂuence properties ﬁnal minima converges. learning rate batch size viewed effective hyper-parameter acting noise factor η/s. this together gradient covariance inﬂuences trade-off loss width ﬁnal minima. speciﬁcally higher noise favors wider minima turn correlates better generalization. further experimentally verify noise determines width height minima towards converges. also show impact noise memorization phenomenon. discuss limitations theory practice exempliﬁed learning rate gets large. also experimentally verify simultaneously rescaled long noise remains same. thank jason nicolas roux mike rabbat leon bottou james grifﬁn helpful discussions. supported grant ministry science higher education poland etiuda stipend //t/st/ national science centre poland. supported part grant centre effective altruism. references martín abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg mané rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda viégas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng. tensorflow large-scale machine learning heterogeneous systems https//www.tensorflow.org/. software available tensorﬂow.org. dario amodei sundaram ananthanarayanan rishita anubhai jingliang eric battenberg carl case jared casper bryan catanzaro qiang cheng guoliang chen deep speech endinternational conference machine to-end speech recognition english mandarin. learning devansh arpit stanisław jastrz˛ebski nicolas ballas david krueger emmanuel bengio maxinder kanwal tegan maharaj asja fischer aaron courville yoshua bengio simon lacoste-julien. closer look memorization deep networks. doina precup whye proceedings international conference machine learning volume proceedings machine learning research international convention centre sydney australia pmlr. http//proceedings.mlr.press/ v/arpita.html. pratik chaudhari adam oberman stanley osher stefano soatto guillame carlier. deep relaxation partial differential equations optimizing deep neural networks. arxiv preprint arxiv. gardiner. stochastic methods handbook natural social sciences. springer series synergetics. springer berlin heidelberg isbn https //books.google.ca/books?id=euqaacaaj. kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition qianxiao cheng weinan stochastic modiﬁed equations adaptive stochastic gradient algorithms. doina precup whye proceedings international conference machine learning volume proceedings machine learning research international convention centre sydney australia pmlr. http//proceedings.mlr.press/v/lif.html. issei sato hiroshi nakagawa. approximation analysis stochastic gradient langevin dynamics using fokker-planck equation process. eric xing tony jebara proceedings international conference machine learning volume proceedings machine learning research bejing china pmlr. http//proceedings.mlr.press/v/satoa.html. xiaocheng shang zhanxing benedict leimkuhler amos storkey. covariance-controlled adaptive langevin thermostat large-scale bayesian sampling. cortes lawrence sugiyama garnett advances neural information processing systems appendix derive fokker-planck equation. stochastic differential equation since evolution noisy can’t exactly parameter space parameter values given time. talk probability parameter takes certain value certain time given started captured fokker-planck equation reads appendix derive equation stochastic differential equation interested pure mathematical rigour intend proof intuition machine learning audience. brevity sometimes write probability sometimes make tensor index notation tensor denoted components index summation convention repeated index summed over. identity integral version chain rule probability stating multiple paths getting initial position time paths. substitute ﬁrst term right hand side apply integration parts move derivatives delta functions onto terms. take ﬁrst term right hand side side insert divide take limit getting partial derivative respect time left hand side leading directly fokker-planck equation quoted text appendix give supplementary comments intuition gain fokker-planck equation learning rate batch size constant covariance proportional identity constant rewrite fokker-planck equation following form terms balance drift diffusion higher value ratio gives rise diffusive evolution lower value allows potential drift term dominate. next section ratio controls stationary distribution converges highlight terms rescaled coordinate ratio controls evolution towards stationary distribution terms rescaled time learning rate batch size interchangable sense ratio invariant transformations note time takes reach stationary distribution depends well rescaled time variable. example higher learning rate constant ratio arrives stationary distribution quicker time factor however caution necessary. ﬁrst order update equation holds small enough ﬁrst order approximation taylor expansion valid hence expect ﬁrst order approximation continuous stochastic differential equation break high thus expect learning rate batch size interchangable maximum value approximation breaks. order prove equilibrium distribution need solve fokker-planck equation left hand side equal zero require equilibrium detailed balance holds. this begin writing fokkerplanck equation slightly different form making probability current deﬁned stationary solution solution stronger demand stationary solution. equuilibrium solution particular stationary solution detailed balance occurs. detailed balance means stationary solution individual transition balances precisely time reverse resulting zero probability currents i.e. detailed balance sufﬁcient condition entropy increasing time. non-zero would correspond non-equilibrium stationary distribution don’t consider here. appendix derive discrete probabilities ending minima given essentially laplace’s method approximate integral probability density region near minimum. work locally near take following approximation loss function since near minimum hessian positive deﬁnite minimum. distribution probability density interested discrete probabilities ending given minimum denote lowercase calculate discrete probabilities minimum need integrate stationary distribution interval containing minimum. integrating region around using last line assume region large enough approximation full gaussian integral used note region can’t large otherwise would invalidate local assumption. picture minima sufﬁciently apart region taken sufﬁciently large approximation valid. note different includes normalization factors performing gaussian integral minima. note derivation used talks strict minima i.e. minima positive deﬁnite hessian. practice however deep neural networks large number parameters unrealistic expect endpoint training strict minimum. instead likely point hessian positive eigenvalues directions eigenvalues zero. cases understand minima favors consider fact equilibrium iterate follows distribution deﬁnition means time equilibrium iterate likely found region higher probability. restrictive case strict minimum model gaussian characterize probability landing minimum depending curvature depth loss around minimum. general case minima degenerate directions minimum volume probable one. figure experiments involving resnet architecture cifar dataset. curve multiply ratio given factor observe multiplying ratio factor results similar performances. however performances degrades factor superior assumptions expect theory become unreliable discrete continuous approximation fails covariance gradients non-isotropic batch size becomes comparable ﬁnite size training momentum considered. covariance gradients highly non-isotropic equilibrium solution fokkerplanck equation solution complicated partial differential equation can’t easily spot solution inspection appendix expect approximation break especially case complicated architectures different gradient directions different gradient covariances. theory involve ﬁnite size training drawback theory. especially apparent batch size becomes large compared training size expect theory break point. finally mention momentum used practical deep learning optimization algorithms. theory consider momentum drawback theory expect break models momentum important. write langevin equation case momentum momentum damping coefﬁcient complicated fokker-planck equation hard spot equilibrium distribution. leave study work. note factor taken right hand side ratio diffusion drift terms ratio ησ/. appendix look experiments exploring correlation learning rate batch-size ratio sharpness minima validation performance. figure report validation accuracy resnet models trained cifar different learning rate batchsize ratio. notice peak validation accuracy learning rate batch-size ratio around particular example emphasizes higher learning rate batch-size ratio doesn’t necessarily lead higher validation accuracy. instead acts control validation accuracy optimal learning rate batch-size ratio. figures show results variant layer relu experiment layers remove batch normalization. inspiration test predictions theory challenging setup. observe correlation hessian norm learning rate batch-size ratio similarly validation performance learning rate batch-size ratio. appendix show detail experiments section show exchangeability learning rate batch size. fig. show cross entropy loss epochaveraged cross entropy loss train validation test accuracy well batch size schedule learning rate schedule. learning rate batch-size ratio plot orange blue lines ratio learning rate batch-size throughout dynamics similar other. holds green lines constant batch size learning rate. supports theoretical result paper ratio governs stationary distribution sgd. figure show detail exchangeability batch size learning rate one-to-one ratio. blue cyclic batch size schedule size ﬁxed learning rate exchangeable orange cyclic learning rate schedule learning rates ﬁxed batch size green constant batch size constant learning rate exchangeable with constant batch size constant learning rate note stationary distribution exchangeability holds plots appears throughout training highlighted especially cyclic schedules. postulate scaling relation fokker-planck equation exchangeability holds throughout learning well long learning rate high ruin approximations fokker-planck equation holds. note similar behaviour occurs also standard learning rate annealing schedules omit brevity. figure learning curves memorization experiment momentum solid lines represent training accuracy dotted validation accuracy. warm color indicates higher report learning curves memorization experiment momentum fig. additionally conﬁrm similar results previous experiments show correlation batch size learning rate ratio norm hessian fig. without momentum momentum", "year": 2017}