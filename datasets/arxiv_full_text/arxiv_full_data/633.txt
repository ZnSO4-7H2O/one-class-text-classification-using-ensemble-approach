{"title": "Explainable Artificial Intelligence: Understanding, Visualizing and  Interpreting Deep Learning Models", "tag": ["cs.AI", "cs.CY", "cs.NE", "stat.ML"], "abstract": "With the availability of large databases and recent improvements in deep learning methodology, the performance of AI systems is reaching or even exceeding the human level on an increasing number of complex tasks. Impressive examples of this development can be found in domains such as image classification, sentiment analysis, speech understanding or strategic game playing. However, because of their nested non-linear structure, these highly successful machine learning and artificial intelligence models are usually applied in a black box manner, i.e., no information is provided about what exactly makes them arrive at their predictions. Since this lack of transparency can be a major drawback, e.g., in medical applications, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This paper summarizes recent developments in this field and makes a plea for more interpretability in artificial intelligence. Furthermore, it presents two approaches to explaining predictions of deep learning models, one method which computes the sensitivity of the prediction with respect to changes in the input and one approach which meaningfully decomposes the decision in terms of the input variables. These methods are evaluated on three classification tasks.", "text": "availability large databases recent improvements deep learning methodology performance systems reaching even exceeding human level increasing number complex tasks. impressive examples development found domains image classiﬁcation sentiment analysis speech understanding strategic game playing. however nested non-linear structure highly successful machine learning artiﬁcial intelligence models usually applied black manner i.e. information provided exactly makes arrive predictions. since lack transparency major drawback e.g. medical applications development methods visualizing explaining interpreting deep learning models recently attracted increasing attention. paper summarizes recent developments ﬁeld makes plea interpretability artiﬁcial intelligence. furthermore presents approaches explaining predictions deep learning models method computes sensitivity prediction respect changes input approach meaningfully decomposes decision terms input variables. methods evaluated three classiﬁcation tasks. ﬁeld machine learning artiﬁcial intelligence progressed last decades. driving force development earlier improvements support vector machines recent improvements deep learning methodology also availability large databases imagenet sportsm speed-up gains obtained powerful cards high ﬂexibility software frameworks caffe tensorflow crucial factors success. today’s machine learningbased systems excel number complex tasks ranging detection objects images understanding natural languages processing speech signals that recent systems even outplay professional human players difﬁcult strategic games texas hold’em poker immense successes systems especially deep learning models show revolutionary character technology large impact beyond academic world also give rise disruptive changes industries societies. however although models reach impressive prediction accuracies nested non-linear structure makes highly non-transparent i.e. clear information input data makes actually arrive decisions. therefore models typically regarded black boxes. move second game historic match sedol player alphago artiﬁcial intelligence system built deepmind demonstrates non-transparency system. alphago played move totally unexpected commented expert following although match unclear system played move deciding move alphago game. case black character alphago matter many applications impossibility understanding validating decision process system clear drawback. instance medical diagnosis would irresponsible trust predictions black system default. instead every reaching decision made accessible appropriate validation human expert. also self-driving cars single incorrect prediction costly reliance model right features must guaranteed. explainable human interpretable models prerequisite providing guarantee. discussion necessity explainable found section surprisingly development techniques opening black models recently received attention community includes development methods help better understand model learned well techniques explaining individual predictions tutorial methods categories found note explainability also important support vector machines advanced machine learning techniques beyond neural networks main goal paper foster awareness necessity explainability machine learning artiﬁcial intelligence. done section section present recent techniques namely sensitivity analysis layer-wise relevance propagation explaining individual predictions model terms input variables. question objectively evaluate quality explanations addressed section results image text video classiﬁcation experiments presented section paper concludes outlook future work section ability explain rationale behind one’s decisions people important aspect human intelligence. important social interactions e.g. person never reveals one’s intentions thoughts probably regarded strange fellow also crucial educational context students comprehend reasoning teachers. furthermore explanation one’s decisions often prerequisite establishing trust relationship people e.g. medical doctor explains therapy decision patient. although social aspects less importance technical systems many arguments favor explainability artiﬁcial intelligence. important ones veriﬁcation system mentioned many applications must trust black system default. instance health care models interpreted veriﬁed medical experts absolute necessity. authors show example domain system trained predict pneumonia risk person arrives totally wrong conclusions. application model black manner would reduce rather increase number pneumonia-related deaths. short model learns asthmatic patients heart problems much lower risk dying pneumonia healthy persons. medical doctor would immediately recognize true asthma hearth problems factors negatively affect prognosis recovery. however model know anything asthma pneumonia infers data. example data systematically biased contrast healthy persons majority asthma heart patients strict medical supervision. supervision increased sensitivity patients group signiﬁcant lower risk dying pneumonia. however correlation causal character therefore taken basis decision pneumonia therapy. improvement system ﬁrst step towards improving system understand it’s weaknesses. obviously it’s difﬁcult perform weakness analysis black models models interpretable. also detecting biases model dataset easier understands model doing arrives it’s predictions. furthermore model interpretability helpful comparing different models architectures. instance authors observed models classiﬁcation performance largely differ terms features basis decisions. works demonstrate identiﬁcation appropriate model requires explainability. even claim better understand models easier becomes improve them. learning system today’s systems trained millions examples observe patterns data accessible humans capable learning limited number examples. using explainable systems extract distilled knowledge system order acquire insights. example knowledge transfer system human mentioned quote above. system identiﬁes strategies play certainly also adapted professional human players. another domain information extraction model crucial sciences. simple physicists chemists biologists rather interested identifying hidden laws nature predicting quantity black models. thus models explainable useful domain compliance legislation systems affecting areas daily life. also legal aspects e.g. assignment responsibility systems makes wrong decision recently received increased attention. since impossible satisfactory answers legal questions relying black models future systems prediction rooster. presence yellow ﬂowers certainly indicative presence rooster image. property perform well quantitative evaluation experiments presented section discussion drawbacks sensitivity analysis found following provide general framework decomposing predictions modern systems e.g. feedforwards neural networks bag-of-words models long-short term memory networks fisher vector classiﬁers terms input variables. contrast sensitivity analysis method explains predictions relative state maximum uncertainty i.e. identiﬁes pixels pivotal prediction rooster. recent work also shows close relations taylor decomposition general function analysis tool mathematics. recent technique called layer-wise relevance propagation explains classiﬁer’s decisions decomposition. mathematically redistributes prediction backwards using local redistribution rules assigns relevance score input variable property redistribution process referred relevance conservation summarized property says every step redistribution process total amount relevance conserved. relevance artiﬁcially added removed redistribution. relevance scores input variable determines much variable contributed prediction. thus contrast sensitivity analysis truly decomposes function value following describe redistribution process feed-forward neural networks redistribution procedures also proposed popular models neuron activations layer relevance scores associated neurons layer weight connecting neuron neuron simple rule redistributes relevance layer layer following small stabilization term added prevent division zero. intuitively rule redistributes relevance proportionally layer neuron layer based criteria namely neuron activation i.e. activated neurons receive larger share relevance strength connection i.e. relevance ﬂows prominent connections. note relevance conservation holds necessarily become explainable. anexample regulations become driving force explainability artiﬁcial intelligence individual rights. persons immediately affected decisions system want know systems decided way. explainable systems provide information. concerns brought european union adapt regulations implement right explanation whereby user explanation algorithmic decision made section introduces popular techniques explaining predictions deep learning models. process explanation summarized fig. first system correctly classiﬁes input image rooster. then explanation method applied explain prediction terms input variables. result explanation process heatmap visualizing importance pixel prediction. example rooster’s comb wattle basis system’s decision. ﬁrst method known sensitivity analysis explains prediction based model’s locally evaluated gradient mathematically sensitivity analysis quantiﬁes importance input variable measure assumes relevant input features output sensitive. contrast approach presented next subsection sensitivity analysis explain function value itself rather variation following example illustrates measuring sensitivity function suboptimal explaining predictions systems. heatmap computed sensitivity analysis indicates pixels need changed make image look less like predicted class. instance example shown fig. pixels would yellow ﬂowers occlude part rooster. changing pixels speciﬁc would reconstruct occluded parts rooster probably would also increase classiﬁcation score rooster would visible image. note heatmap would indicate pixels actually pivotal fig. explaining predictions system. input image correctly classiﬁed rooster. order understand system arrived decision explanation methods applied. result explanation image heatmap visualizes importance pixel prediction. example rooster’s comb wattle basis system’s decision. heatmap verify system works intended. perturbation input variables highly important prediction leads steeper decline prediction score perturbation input dimensions lesser importance. explanation methods provide score every input variable. thus input variables sorted according relevance score. iteratively perturb input variables track prediction score every perturbation step. average decline prediction score used objective measure explanation quality large decline indicates explanation method successful identifying truly relevant input variables. denote positive negative parts respectively. conservation relevance enforced additional constraint special case authors showed redistribution rule coincides deep taylor decomposition neural network function neural network composed relu neurons. toolbox provides python matlab implementation method well integration popular frameworks caffe tensorflow. toolbox directly applied peoples’ models. toolbox code online demonstrators information found www.explain-ai.org. ﬁrst experiment googlenet model state-of-the deep neural network classify general objects ilsvrc dataset. fig. shows images dataset correctly classiﬁed volcano coffee respectively. heatmaps visualize explanations obtained lrp. heatmap coffee image shows model identiﬁed ellipsoidal shape relevant feature image category. example particular shape mountain regarded evidence presence volcano image. heatmaps much noisier ones computed large values assigned regions consisting pure background e.g. although pixels really indicative image category volcano. contrast indicate much every pixel contributes prediction rather measures sensitivity classiﬁer changes input. therefore produces subjectively better explanations model’s predictions lower part fig. displays results perturbation analysis introduced section y-axis shows relative decrease prediction score average ﬁrst images ilsvrc dataset i.e. value means original scores decreased average every perturbation step patch image replaced random values sampled uniform distribution. since prediction score decrease much faster perturbing images using heatmaps using heatmaps also objectively provides better explanations discussion image classiﬁcation experiment found experiment word-embedding based convolutional neural network trained classify text documents newsgroup dataset. fig. shows heatmaps overlayed document classiﬁed topic sci.med i.e. text assumed medical topic. explanation methods indicate words sickness body discomfort basis classiﬁcation decision. contrast sensitivity analysis distinguishes positive negative words i.e. words support classiﬁcation decision sci.med words contradiction i.e. speak another category obviously words ride astronaut shuttle strongly speak topic space necessarily topic medicine. heatmap although classiﬁer decides correct sci.med class evidence text contradicts decision. method distinguish positive negative evidence. lower part ﬁgure shows result quantitative evaluation. y-axis displays relative decrease prediction accuracy documents newsgroup dataset. every perturbation step important words deleted setting corresponding input values also result conﬁrms quantitatively provides informative heatmaps heatmaps lead larger decrease classiﬁcation accuracy compared heatmaps. discussion text document classiﬁcation experiment found last examples demonstrates explanation fisher vector classiﬁer trained predicting human actions compressed videos. order reduce computational costs classiﬁer trained blockwise motion vectors evaluation performed hmdb dataset fig. shows heatmaps overlayed onto exemplar frames video sample. video correctly classiﬁed showing action sit-up. model mainly focuses blocks surrounding upper body person. makes perfectly sense part video frame shows motion indicative action sit-up namely upward downward movements body. curve bottom fig. displays distribution relevance frames. relevance scores larger frames person performing upwards downwards movement. thus heatmaps visualizes relevant locations action within video frame also identiﬁes relevant time points within video sequence discussion experiment found paper approached problem explainability artiﬁcial intelligence. discussed black models acceptable certain applications e.g. medical domain wrong decisions system harmful. furthermore explainability presented prerequisite solving legal questions arising increased usage systems e.g. assign responsibility case system failure. since right explanation become part european fig. explaining predictions systems. shows application explainable methods image classiﬁcation. heatmaps noisy difﬁcult interpret whereas heatmaps match human intuition. shows application explainable methods text document classiﬁcation. heatmaps identify words discomfort body sickness relevant ones explaining prediction sci.med. contrast sensitivity analysis distinguishes positive negative relevances. shows explanations human action recognition classiﬁer based motion vector features. heatmaps video classiﬁed sit-up show increased relevance frames person performing upwards downwards movement. besides gateway society explainability also powerful tool detecting ﬂaws model biases data verifying predictions improving models ﬁnally gaining insights problem hand future work investigate theoretical foundations explainability particular connection post-hoc explainability i.e. trained model given goal explain it’s predictions explainability incorporated directly structure model. furthermore study ways better understand learned representation especially relation generalizability compactness explainability. finally arras horn montavon k.-r. m¨uller samek. explaining predictions non-linear classiﬁers nlp. proceedings workshop representation learning pages arras montavon k.-r. m¨uller samek. explaining recurrent neural network predictions senproceedings emnlp’ timent analysis. workshop computational approaches subjectivity sentiment social media analysis pages bach binder montavon klauschen k.-r. m¨uller samek. pixel-wise explanations non-linear classiﬁer decisions layer-wise relevance propagation. plos caruana gehrke koch sturm elhadad. intelligible models healthcare predicting pneumonia risk hospital -day readmission. proceedings sigkdd international conference knowledge discovery data mining pages merri¨enboer gulcehre bahdanau bougares schwenk bengio. learning phrase representations using encoder-decoder arxiv preprint statistical machine translation. arxiv. deng dong socher l.-j. imagenet large-scale hierarchical iml. fei-fei. database. proceedings ieee conference computer vision pattern recognition pages deng hinton kingsbury. types deep neural network learning speech recognition related applications overview. ieee international conference acoustics speech signal processing pages shelhamer donahue karayev long girshick guadarrama darrell. caffe convolutional architecture fast feature embedding. proceedings international conference multimedia pages kantorov laptev. efﬁcient feature extraction encoding classiﬁcation action recognition. proceedings ieee conference computer vision pattern recognition pages karpathy toderici shetty leung sukthankar fei-fei. large-scale video classiﬁcation convolutional neural networks. proceedings ieee conference computer vision pattern recognition pages kuehne jhuang garrote poggio serre. hmdb large video database human motion recognition. proceedings ieee international conference computer vision pages ieee landecker thomure bettencourt mitchell kenyon brumby. interpreting individual classiﬁcations hierarchical networks. proceedings ieee symposium computational intelligence data mining pages lapuschkin binder montavon k.-r. m¨uller samek. analyzing classiﬁers fisher vectors deep neural networks. proceedings ieee conference computer vision pattern recognition pages lapuschkin binder montavon k.-r. m¨uller samek. layer-wise relevance propagation toolbox artiﬁcial neural networks. journal machine learning research mahendran vedaldi. understanding deep image representations inverting them. proceedings ieee conference computer vision pattern recognition pages nguyen yosinski clune. multifaceted feature visualization uncovering different types features learned neuron deep neural networks. arxiv preprint arxiv. ribeiro singh guestrin. trust you? explaining predictions classiﬁer. proceedings sigkdd international conference knowledge discovery data mining pages samek binder montavon lapuschkin k.-r. m¨uller. evaluating visualization ieee transactions deep neural network learned. neural networks learning systems press. srinivasan lapuschkin hellge k.-r. m¨uller samek. interpretable human action recognition compressed domain. proceedings ieee international conference acoustics speech signal processing pages szegedy sermanet reed anguelov erhan vanhoucke rabinovich. going deeper convolutions. proceedings ieee conference computer vision pattern recognition pages", "year": 2017}