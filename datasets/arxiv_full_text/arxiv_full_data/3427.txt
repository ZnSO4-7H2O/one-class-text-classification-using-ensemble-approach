{"title": "Episode-Based Active Learning with Bayesian Neural Networks", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "We investigate different strategies for active learning with Bayesian deep neural networks. We focus our analysis on scenarios where new, unlabeled data is obtained episodically, such as commonly encountered in mobile robotics applications. An evaluation of different strategies for acquisition, updating, and final training on the CIFAR-10 dataset shows that incremental network updates with final training on the accumulated acquisition set are essential for best performance, while limiting the amount of required human labeling labor.", "text": "feras dayoub niko s¨underhauf peter corke australian centre robotic vision queensland university technology brisbane australia {feras.dayoub niko.suenderhauf peter.corke}qut.edu.au investigate different strategies active learning bayesian deep neural networks. focus analysis scenarios unlabeled data obtained episodically commonly encountered mobile robotics applications. evaluation different strategies acquisition updating ﬁnal training cifar- dataset shows incremental network updates ﬁnal training accumulated acquisition essential best performance limiting amount required human labeling labor. obtaining labeled training data challenge robotic scene understanding system pre-trained dataset adopt real world deployment environment. active learning cohn helps minimize necessary human labeling labor acquiring informative samples pool available images. context mobile robotics active learning happens episodically. starting initial classiﬁer robot encounters stream images performing mission. certain time acquisition function determines informative encountered images human asked provide ground truth labels. initial network updated using acquired images. process repeats previously unseen images encountered next episode. paper investigate different strategies performing active learning episode-based scenario. acquisition function using bayesian deep networks classiﬁers. goal enable mobile robot adopt perception system deployment environment little human help interaction possible. notation represent network obtained episode write express ﬁne-tuning network data obtain problem deﬁnition deﬁne problem episode-based active learning follows start initial pre-trained network episodes perform following steps present episode previously unseen unknown images network obtain classiﬁcation results based acquisition function determine informative images oracle ground truth labels; update network tuning acquired dataset stopping active learning process episodes ﬁnal training step might performed obtain ﬁnal network different strategies available updating network ﬁnal training acquisition function. table lists different combinations strategies investigated evaluated paper. strat. network update table different active learning strategies evaluated paper. accuracy test size used training relative efﬁciency reported ﬁnal network episodes. network update updating network every episode done using newly acquired episode using full data i=ai acquired far. evaluate four different methods updating network incremental ﬁne-tuning using recently acquired images {at}; incremental updating using growing acquired images i=ai}; ﬁne-tuning initial network acquisition function apply maximum entropy acquisition strategy selects images highest classiﬁcation uncertainty episode. showed strategy competitive complex acquisition schemes. every image current episode obtain class probability distribution current network image acquired denotes entropy threshold parameter. evaluation true class probability distribution approximated bayesian neural network approach proposed ghahramani enable dropout test time pass every image network times average obtained distributions. final training ﬁnal network episodes obtained either simply using network last update step using acquired images perform additional ﬁne-tuning evaluation cifar- dataset evaluate different strategies table using cifar- dataset. standard dataset consists pixel images classes split images training testing. divide training splits images each. ﬁrst split used train initial model shared strategies. rest non-overlapping splits used episodes. testing done test images. network architecture simple network consisting convolution layers followed fully connected layer -fold softmax layer. dropout used every layer max-pooling layers conv layers network trained adam kingma early stopping. evaluation protocol every strategy table evaluate test accuracy ﬁnal network would result stopping active learning every episode. experiments performed times average accuracy acquired number training images reported. compare performance different strategies furthermore deﬁne efﬁciency score fraction used training set. efﬁciency network high uses less training samples gain higher accuracy test set. furthermore relative efﬁciency deﬁned efﬁciency network trained full training set. empirically acquisition threshold also analysed inﬂuence parameter. results evaluation showed strategies incremental ﬁne-tuning update step ﬁnal training step accumulated acquired images outperform strategies. particular strategies update based initial network perform worse. illustrated plots fig. strategies learn faster outperforms variants even reaches slightly better performance baseline model trained full dataset. plots averaged trials. figure acquisition function network update strategy determine data-efﬁciency strategy acquires least number images outperforms even baseline model terms test accuracy. inﬂuence selection threshold entropy acquisition function evaluated strategy episode) efﬁcient strategies even outperform baseline model small margin. strategy data-efﬁcient using less training data sacriﬁcing test accuracy. comes increased computational cost update step instead growing update ﬁne-tuning dataset i=ai. training randomly selected training data results signiﬁcantly worse performance underlines efﬁcacy proposed active learning strategies differences strategy ﬁrst strategies illustrate importance using accumulated acquired images i=ai updating least ﬁnal training. assume prevents forgetting seems occur using recent acquired images. fig. compares number images acquired every strategy episodes. strategy data-efﬁcient acquiring least amount images reaching second highest accuracy. expressed relative efﬁciency score highest strategies. notice strategy always acquires roughly amount images. surprising given update strategy uses initial network recent acquisition thereby learning environment time strategies fig. reveals inﬂuence parameter used threshold maximum entropy acquisition function. expected higher threshold acquires fewer images ultimately leads smaller training sacriﬁces accuracy. evaluation found incremental update strategies ﬁnal training step based accumulated acquired image performed best episode-based active learning bayesian neural networks. contrast earlier work islam demonstrated active learning ideas bayesian networks pool-based setup mnist presented experiments challenging cifar- dataset. furthermore evaluated episode-based scenario", "year": 2017}