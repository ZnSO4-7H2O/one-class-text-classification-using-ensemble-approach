{"title": "Self-calibrating Neural Networks for Dimensionality Reduction", "tag": ["cs.LG", "cs.NE", "q-bio.NC", "stat.ML"], "abstract": "Recently, a novel family of biologically plausible online algorithms for reducing the dimensionality of streaming data has been derived from the similarity matching principle. In these algorithms, the number of output dimensions can be determined adaptively by thresholding the singular values of the input data matrix. However, setting such threshold requires knowing the magnitude of the desired singular values in advance. Here we propose online algorithms where the threshold is self-calibrating based on the singular values computed from the existing observations. To derive these algorithms from the similarity matching cost function we propose novel regularizers. As before, these online algorithms can be implemented by Hebbian/anti-Hebbian neural networks in which the learning rule depends on the chosen regularizer. We demonstrate both mathematically and via simulation the effectiveness of these online algorithms in various settings.", "text": "column vector corresponding output column vector concatenate input vectors input data matrix output vectors output data matrix match pairwise similarities minimizing summed squared differences pairwise similarities avoid trivial solution restrict number degrees freedom output setting solution minimization problem projection input data k-dimensional principal subspace i.e. subspace spanned eigenvectors corresponding eigenvalues input similarity matrix similarity matching objective optimized online setting input data vectors arrive sequentially time corresponding output computed prior arrival next input. remarkably derived algorithm implemented single-layer neural network components input data vectors represented activity input neurons corresponding time. algorithm proceeds alternating steps neuronal dynamics computes output synaptic weights updated according local learning rules meaning weight update synapse depends activity prepostsynaptic neurons. family similarity matching neural networks unique among dimensionality reduction networks combining biological plausibility derivation principled cost function. real-world signal-processing applications neuroscience context desired number output dimensions often unknown algorithm priori varies time input non-stationarity. number output dimensions neural circuit solution number output neurons cannot adjusted quickly. problem proposed penalize rank output adding regularizer abstract— recently novel family biologically plausible online algorithms reducing dimensionality streaming data derived similarity matching principle. algorithms number output dimensions determined adaptively thresholding singular values input data matrix. however setting threshold requires knowing magnitude desired singular values advance. propose online algorithms threshold self-calibrating based singular values computed existing observations. derive algorithms similarity matching cost function propose novel regularizers. before online algorithms implemented hebbian/anti-hebbian neural networks learning rule depends chosen regularizer. demonstrate mathematically simulation effectiveness online algorithms various settings. dimensionality reduction plays important role artiﬁcial natural signal processing systems. man-made data analysis pipelines denoises input simpliﬁes processing identiﬁes important features. dimensionality reduction algorithms developed ofﬂine setting whole dataset available algorithm outset online setting data streamed sample time brain online dimensionality reduction takes place example early processing streamed sensory inputs evidenced high ratio input output nerve ﬁber counts therefore dimensionality reduction algorithms help model neuronal circuits observations neuroscience inspire future development artiﬁcial signal processing systems. recently novel principled approach online dimensionality reduction neuronal circuits developed approach based principle similarity matching similarity outputs must match similarity inputs certain constraints. mathematically pairwise similarity quantiﬁed inner products data vectors matching enforced classical multidimensional scaling cost function. dimensionality reduced constraining number output degrees freedom either explicitly adding regularization term. formulate similarity matching mathematically represent centered input data sample received time algorithms number output dimensions given number input singular values exceeding threshold depends parameter singular values scale number time steps threshold scales however choosing regularization parameter hard requires knowing exact scale input singular values advance. furthermore scale-dependent threshold adaptive non-stationary inputs changing singular values. paper introduce self-calibrating regularizers cost function depend time explicitly designed automatically adjust variation singular values input. speciﬁcally propose αtrx)try) αy)). solve cost function regularizers ofﬂine online settings. online algorithms also onto single-layer neuronal network importantly different learning rules. section mathemathically illustrate difference among three regularizers simple data generation scenario. section propose corresponding algorithms non-stationary input scenario introducing discounting forgetting. section ﬁrst summarize previous derivation adaptive dimensionality reduction cost function scale-dependent regularizer discuss potential shortcomings. introduce selfcalibrating adaptive dimension reduction methods involving solving cost function ofﬂine regularizers ryy. optimal output matrix projection input data onto principal subspace soft-thresholding input singular values. indeed suppose eigendecomposition diagx. solution ofﬂine problem equation shows regularization coefﬁcient sets threshold eigenvalues input covariance. input modes eigenvalues included output albeit eigenvalue shrunk modes rejected setting corresponding output singular values zero. scaling regularization coefﬁcient time ensures threshold occupies relative position spectrum eigenvalues grow linearly time stationary signal. algorithm separate signal noise signal eigenvalues greater noise eigenvalues between. however setting value requires knowing variance input signal noise. ofﬂine setting computed data. however chosen priori e.g. online setting choosing value suits various inputs different signal variance difﬁcult. particular noise variance possible input exceeds signal variance another input universal value exist. possible regularize problem regularization coefﬁcient chosen applies universally inputs arbitrary variance? regularizer applies relative rather absolute threshold input singular values would able deal various input setting. rather using threshold depending time explicitly threshold value proportional eigenvalues formally leads following optimization problem inputoutput regularizer ofﬂine setting change relative previous method minor always good choice make former formulation similar later observing whole consequence ofﬂine solution similar previous method. projection input data onto principal subspace different eigenvalue cutoff based input eigenvalue sum. turn coefﬁcient sets threshold relative eigenvalues trx) table summarizes different ranges regularization coefﬁcients three methods keep track ﬁrst eigenmodes resulting output similarity matrix’s eigenvalue. illustrate difference known scaledependent regularizer newly proposed regularizers compute fraction various pairs algorithm achieves goal values fig. shows fraction pairs signal transmitted fraction noise transmitted varies along curve. curve corresponding scale-dependent regularizer reach point fig. indicating value achieves goal pairs inputoutput squared-output regularizers pass point indicating universal values exist algorithms transmit signal discarding noise. alternative apply relative threshold input singular values deploy regularization proportional eigenvalues dimension reduction eigenvalues reﬂective eigenvalues reasoning leads following optimization problem squared-output regularizer derivation closed-form ofﬂine solution found appendix intuitively amount shrinkage still depends approximately input eigenvalues computed eigenvalues. similar input-output regularizer regularization coefﬁcient sets threshold relatively input statistics. three similarity matching algorithms compare ofﬂine solutions input covariance sets degenerate eigenvalues. suppose eigenvalues normalized input similarity matrix illustrating range parameters algorithm transmits signal rejects noise. speciﬁcally fig. shows range regularization coefﬁcient different noise-to-signal ratio input. range squared-output regularizer achieves goal much larger input-output regularizer indicating robustness. section ﬁrst formulate online versions dimensionality reduction optimization problems presented section derive corresponding online algorithms onto dynamics neural networks biologically plausible local learning rules. derivations follow time algorithm minimizes cost depending previous inputs outputs time respect keeping previous ﬁxed. large-t limit last terms ignored since ﬁrst terms order dominates. remaining cost function positive quadratic form could minimum solving system linear equations unregularized formulation output dimensionality determined column dimensition online adaptive dimension reduction methods adaptively choose output dimensionality based trade-off cmds cost regularizers. algorithm implemented dynamics neural activity single-layer network. represent weights feedforward lateral synaptic connections. time step ﬁrst iterate convergence update weights fig. left hebbian/anti-hebbian neural network implementing online soft-thresholding algorithm scale-dependent regularizer. middle network input-output regularizer. right network squared-output regularizer. simpliﬁcation update similar previous input-output regularizer except learning rate depends norm current output vector since scalar summation easily implemented biology using summation extracellular space glia time step neural network dynamics iterates convergence jacobi iteration convergence synaptic weights updated online follows online algorithms proposed assume input stationary statistics. truly adaptive algorithm addition self-calibrating number dimensions transmit able adapt temporal statistics changes. address issue introduce discounting cost function reduces contribution older data samples equivalently forgets them order react changes input statistics. online optimization problem input-output regularizer similar previous one. every time step amount thresholding given cumulative input eigenvalues. slight modiﬁcations arrive following neural network algorithm. time step iterated convergence based following jacobi iteration differs scalar variable needed sums current input amplitude across input neurons. biologically hebbian/anti-hebbian neural network summation could implemented extracellular space glia evaluate performance three online algorithms synthetic dataset. ﬁrst generate dimensional colored gaussian process speciﬁed covariance matrix. covariance matrix eigenvalues remaining chose uniformly interval correlations introduced covariance matrix generating random orthonormal eigenvectors. synaptic weight matrices initialized randomly. initially choose different three algorithms thresholding approximately effect original data output keeps track three principal components discarding rest principal components. additionally three eigenvalues input similarity matrix soft-thresholded quantify performance algorithms precisely looking individual eigenvalues different metrics. ﬁrst metric eigenvalue error measures deviation output covariance eigenvalues optimal ofﬂine values derived section eigenvalue error iteration calculated summing squared differences eigenvalues second metric subspace error quantiﬁes deviation learned subspace input principal subspace. exact formula subspace error metric found shows three algorithms perform similarly terms metrics. errors algorithm decrease function iterations derive online algorithm follow steps before. keeping terms depend current output arrive quadratic function solved weighted jacobi iteration. online synaptic learning rules modiﬁed difference non-discounted learning rules gets updated. decay update prevents growing indeﬁnitely. consequently learning rate steadily decrease saturates allowing synaptic weights react changes input statistics. introduced online dimensionality reduction algortihms self-calibrating regularizers. unlike scaledependent adaptive dimensionality reduction algorithm self-calibrating algorithms designed automatically adjust variation singular values input. consequence appropriate modeling neuronal circuits related artiﬁcial signal processing systems. suppose eigen-decomposition since frobenius norm invariant multiplication unitary matrices could multiply left right obtain equivalent objective ﬁrst original data generation process iterations. chosen algorithm three principal components retained rest discarded. change input data generation multiplying eigenvalues order whether algorithms still keep track three principal components. finally iteration change back original statistics. since input reﬂective statistics changes time eigenvalues online algorithms tracking time thus eigenvalues short period data ﬁrst iterations three online algorithms keep track three principal components fourth output singular value kept zero time three singular values zero. iteration sudden change input data generation. fourth output singular value input-output regularizer squared-output regularizer remains zero however fourth output singular value scale-dependent regularizer becomes larger zero scale-dependent regularizer output effective dimension four rather three. better keeping track three principal components. general form nnls allow closed form solution. general solved active-set type optimization algorithm number iterations worse case could exponential input dimension. case almost allows closed form solution. combining support solution always ﬁrst elements. sufﬁcient different supports best feasible solution. support solution size could obtain closed form solution ofﬂine problem. given support nnls problem equivalent unconstrained quadratic problem. zhang cheng yang chen effective efﬁcient dimensionality reduction large-scale streaming data preprocessing ieee transactions knowledge data engineering vol. hyv¨arinen hurri hoyer natural image statistics springer pehlevan chklovskii hebbian/anti-hebbian neural network linear subspace learning derivation multidimensional scaling streaming data neural computation vol. pehlevan chklovskii hebbian/anti-hebbian network online sparse dictionary learning derived symmetric matrix factorization asilomar conference signals systems computers. pehlevan chklovskii hebbian/anti-hebbian network derived online non-negative matrix factorization cluster discover sparse features asilomar conference signals systems computers. carroll chang idioscal generalization indscal allowing idiosyncratic reference systems well analytic approximation indscal psychometric meeting princeton", "year": 2016}