{"title": "Anomaly Detection using One-Class Neural Networks", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "We propose a one-class neural network (OC-NN) model to detect anomalies in complex data sets. OC-NN combines the ability of deep networks to extract progressively rich representation of data with the one-class objective of creating a tight envelope around normal data. The OC-NN approach breaks new ground for the following crucial reason: data representation in the hidden layer is driven by the OC-NN objective and is thus customized for anomaly detection. This is a departure from other approaches which use a hybrid approach of learning deep features using an autoencoder and then feeding the features into a separate anomaly detection method like one-class SVM (OC-SVM). The hybrid OC-SVM approach is suboptimal because it is unable to influence representational learning in the hidden layers. A comprehensive set of experiments demonstrate that on complex data sets (like CIFAR and PFAM), OC-NN significantly outperforms existing state-of-the-art anomaly detection methods.", "text": "figure hidden layer activations tsne embeddings oc-nn model cifar- dataset. following success transfer learning obtain rich representative features hybrid models adopted pre-trained transfer learning models obtain features inputs anomaly detection methods. although using generic pre-trained networks transfer learning representations efficient learning representations scratch moderately sized dataset specific task anomaly detection shown perform better since hybrid models extract deep features using autoencoder feed separate anomaly detection method like oc-svm fail influence representational learning hidden layers. paper build theory integrate oc-svm equivalent objective neural network architecture. oc-nn combines ability deep networks extract progressively rich representation data alongwith one-class objective obtains hyperplane separate normal data points origin. oc-nn approach novel following crucial reason data representation hidden layer illustrated figure driven oc-nn objective thus customized anomaly detection. show oc-nn achieve comparable better performance existing state-of-the methods complex datasets reasonable training testing time compared existing methods. summarize main contributions follows derive class neural network model anomaly detection. oc-nn uses class like loss function drive training neural network. propose alternating minimization algorithm learning parameters oc-nn model. observe subproblem oc-nn objective equivalent solving quantile selection problem. carry extensive experiments convincingly demonstrate oc-nn outperforms state-of-the-art deep learning approaches anomaly detection complex image sequence data sets. abstract propose one-class neural network model detect anomalies complex data sets. oc-nn combines ability deep networks extract progressively rich representation data one-class objective creating tight envelope around normal data. oc-nn approach breaks ground following crucial reason data representation hidden layer driven oc-nn objective thus customized anomaly detection. departure approaches hybrid approach learning deep features using autoencoder feeding features separate anomaly detection method like one-class hybrid oc-svm approach suboptimal unable influence representational learning hidden layers. comprehensive experiments demonstrate complex data sets oc-nn significantly outperforms existing state-of-the-art anomaly detection methods. common need analysing real-world datasets determining instances stand dissimilar others. instances known anomalies goal anomaly detection determine instances data-driven fashion anomalies caused errors data sometimes indicative previously unknown underlying process; fact hawkins defines outlier observation deviates significantly observations arouse suspicion generated different mechanism. unsupervised anomaly detection techniques uncover anomalies unlabeled test data plays pivotal role variety applications fraud detection network intrusion detection fault diagnosis. one-class support vector machines widely used effective unsupervised techniques identify anomalies. however performance oc-svm sub-optimal complex high dimensional datasets recent literature unsupervised anomaly detection using deep learning proven effective deep learning methods anomaly detection broadly classified model architecture using autoencoders hybrid models models involving autoencoders utilize magnitude residual vector making anomaly assessments. hybrid models mainly autoencoder feature extractor wherein hidden layer representations used input traditional anomaly detection algorithms one-class separate data points origin reproducing kernel hilbert space maximises distance hyperplane origin. intuitively oc-svm data points considered positively labeled instances origin negative labeled instance. specifically given training data without class information rkhs function input space feature space hyper-plane linear decision function feature space constructed separate many possible mapped vectors origin. norm perpendicular hyper-plane bias hyper-plane. order obtain need solve following optimization problem present one-class neural network model unsupervised anomaly detection. method seen designing neural architecture using oc-svm equivalent loss function. using oc-nn able exploit refine features obtained unsupervised tranfer learning specifically anomaly detection. turn make possible discern anomalies complex data sets decision boundary normal anomalous highly nonlinear. one-class neural networks design simple feed forward network hiden layer linear sigmoid activation output node. generalizations deeper architectures straightforward. oc-nn objective formulated scalar output obtained hidden output layer weight matrix input hidden units. thus insight paper replace product oc-svm product change make possible leverage transfer learning features obtained using autoencoder create additional layer refine features anomaly detection. however price change objective becomes non-convex thus resulting algorithm inferring parameters model lead global optima. experiment setup evaluation metrics model configurations described section results analysis experiments focus section conclude section summary directions future work. anomaly detection well-studied topic data science unsupervised anomaly detection aims discovering rules separate normal anomalous data absence labels. one-class popular unsupervised approach detect anomalies constructs smooth boundary around majority probability mass data oc-svm described detail section recent times several approaches feature selection feature extraction methods proposed complex high-dimensional data oc-svm following unprecedented success using deep autoencoder networks feature extractors tasks diverse visual speech anomaly detection several hybrid models combine feature extraction using deep learning oc-svm appeared benefits leveraging pre-trained transfer learning representations anomaly detection hybrid models made evident results obtained using publicly available pre-trained models imagenet-matconvnet-vgg-f imagenetmatconvnet-vgg-m however hybrid oc-svm approaches decoupled sense feature learning task agnostic customized anomaly detecion. besides hybrid approaches oc-svm deep learning features another approach anomaly detection deep autoencoders. inspired rpca unsupervised anomaly detection techniques robust deep autoencoders used separate normal anomalous data robust deep autoencoder robust deep convolutional autoencoder decompose input data parts represents latent representation hidden layer autoencoder. matrix captures noise outliers hard reconstruct shown equation decomposition carried optimizing objective function shown equation optimization problem solved using combination backpropagation alternating direction method multipliers approach experiments carried detailed comparision oc-nn approaches based robust autoencoders. one-class anomaly detection one-class widely used approach discover anomalies unsupervised fashion oc-svms special case support vector machine learns hyperplane oc-nn algorithm summarize solution algorithm initialize line learn parameters neural network using standard backpropogation algorithm experiment section train model using features extracted autoencoder instead data points. however impact oc-nn algorithm. show theorem solve using υ-quantile scores ⟨yn⟩. convergence criterion satisfied data points labeled normal anomalous using decision function sдn. experimental setup section show empirical effectiveness oc-nn formulation state-of-the-art methods real-world data. although method applicable context autoencoders used feature representation e.g. speech. used keras tensorflow implementation oc-nn cae-oc-svm rcae dbn. oc-svm isolation forest used publicly available implementations. robust-pca code released respective authors used experiments. https//github.com/raghavchalapathy/oc-nn https//github.com/raghavchalapathy/rcae https//github.com/wuaalb/keras_extensions/blob/master/examples http//scikit-learn.org/stable/auto_examples/svm/plot_oneclass.html http//scikit-learn.org/stable/modules/generated/sklearn.ensemble.isolationforest. html https//github.com/zc/robustautoencoder/tree/master/lib https//github.com/dganguli/robust-pca evaluation methodology anomaly detection unsupervised learning problem model evaluation challenging. datasets follow standard protocol wherein anomalies explicitly identified training set. evaluate predictive performance method measured ground truth anomaly labels using three standard metrics auprc auroc measure ranking performance former preferred class imbalance measures classification performance fraction scored instances actually anomalous. mnist experiments dataset created mixture images digit images randomly sampled digit’s images anomalies formulated paper inline previous mnist experiments challenging task usps dataset created mixture images images cifar deep convolution autoencoder trained using images dogs obtaining unsupervised transfer learning representations randomly sampled images dogs images cats used testing; good anomaly detection method thus flag cats anomalous. finally pfam training dataset comprising instances soluble proteins length flagged normal instances used obtaining unsupervised transfer learning representations using long short-term memory autoencoder randomly selected samples soluble proteins instances insoluble proteins testing; good anomaly detection technique detect latter anomalous. model architecture adopt transfer learning methodology train proposed oc-nn model. usual transfer learning approach followed train base network copy first layers first layers target network. remaining layers target network randomly initialized trained towards specified task. oc-nn model firstly deep autoencoder trained obtain representative features input illustrated figure then encoder layers pre-trained autoencoder copied input feed-forward network hidden layer shown figure summary feed forward network architecture’s used various datasets presented table weights encoder network frozen learn feed-forward network weights following algorithm summarized section pre-trained encoder network demonstrates strong ability generalize images outside training dataset transfer learning. oc-nn objective equation tested different model architecture’s applied feed-forward network coupled output transfer learned encoder. feedforward network alone illustrated synthetic data experiments. model configurations section describe network configurations autoencoder oc-nn feed-forward network baseline methods used comparision. mnistusps cifar− data consisting images convolution autoencoder network parameters illustrated section deployed. pfam dataset comprises protein sequences long short-term memory autoencoder illustrated section used obtain feature representations. convolution autoencoder network parameters although observed deeper networks used encode representative features tend achieve better performance exist four-fold options related network parameters chosen number convolutional filters filter size strides convolution operation activation applied. tuned grid search additional hyper-parameters including number hidden-layer nodes learning drop-out rates regularization parameter sampled uniform distribution range initial weight matrices sampled uniform distribution within range autoencoder trained using meansquared error loss function. success batch normalization architecture exponential linear units found convolutional+batch-normalization+elu layers provide better representation convolutional filters. architecture used experiments contain four layers encoder part four layers decoder portion network. network parameters chosen first second layers third fourth layers encoder decoder. adam optimizer ability learning rate automatically based models weight update history. long short-term memory autoencoder network parameters lstm autoencoder model consists recurrent neural networks encoder lstm decoder lstm formulation step size lstm encoder decoder equal length protein sequence. number hidden units within lstm cell autoencoder tuned grid search. lstm-ae encodes input protein sequence fixed vector dimension learning drop-out rates sampled uniform distribution range initial weight matrices sampled uniform distribution within range best performance. adam optimizer minimize squared error loss. oc-nn feed-forward network parameters feed-forward neural network consisting single hidden layer activation functions linear sigmoid relu trained oc-nn objective equation table summarizes feed-forward network configurations used experiments. optimal value parameter equivalent percentage anomalies data according respective outlier proportions. baseline model parameters proposed oc-nn method compared several state-of-the-art baseline models illustrated table table rcae models best performing parameter settings employed. hybrid models dbn-svdd extracted features deep belief network layers used train oc-svm linear radial basis function kernel formulation robust-pca oc-svm hybrid model robust features extracted scaling factor used input oc-svm. oc-svm isolation-forest models fraction outliers according outlier proportions summarized table experiments pfam data containing protein sequence distributed vector representation protein sequence obtained protvec model dimension used inputs baseline hybrid models. experimental results section present empirical results produced oc-nn model synthetic real data sets. perform comprehensive experiments demonstrate complex data sets ocnn significantly outperforms existing state-of-the-art anomaly detection methods. synthetic data single cluster consisting data points using generated normal points along anomalous points drawn normal distribution dimension intuitively latter normally distributed points treated anomalous corresponding points different distribution characteristics majority training data. data point flattened vector yielding training matrix testing matrix. simple feed-forward neural network model one-class neural network objective equation using network parameter settings described section results. figure near certainty anomalous points accurately identified outliers decision scores negative anomalies. evident that oc-nn formulation performs classical oc-svm detecting results. figure case mnist dataset accurately identified nominal data decision scores negative anomalies. similarly usps dataset accurately identified anomalies figure results obtained oc-nn performs state-of-theart baseline methods shown table observe mnist usps datasets less complicated spatial structures cifar- images oc-svm also produces best results. complex datasets proposed method significantly outperforms existing state-of-the-art anomaly detection methods described following sections. cifar- cifar- dataset consisting color images different classes. randomly selected images ‘dog used training deep convolutional autoencoder using network parameter settings illustrated section encoded representations ‘dog’ images dimension input feed-forward oc-nn model network hidden layer dimension oc-nn feed-forward network minimized alternatively following algorithm section different activations linear sigmoid rectified linear units test consisting images ‘dog’s images ‘cat’s used test proposed oc-nn model. intuitively latter images treated anomalous corresponding images different characteristics majority training data. results. figure near certainty ‘cat accurately identified outliers decision scores being negative. results table illustrates cifar− consisting complex data structures proposed oc-nn method significantly outperforms existing state-of-the-art anomaly detection methods. improved performance could attributed deep representations captured hidden layers oc-nn feedforward network customized anomaly detection. pfam pfam data consists families clans proteins. training randomly selected soluble protein sequences length utilized train long short-term memory autoencoder test consisting insoluble mnist usps section demonstrate ability oc-nn detect anomalous digits using mnist usps hand written digit data sets. mnist dataset create dataset training formulation paper usps dataset formulate challenging task detecting digits anomalies among digit wherein digits almost similar distribution characteristics formulation deep convolutional autoencoder model trained using nominal data network parameters illustrated section experiments conducted images digit mnist dataset images digit usps dataset comprise nominal data. intuitively images digits mnist images digit usps treated anomalous corresponding images different characteristics majority training data. autoencoder trained using mean-squared error loss function enable hidden layers encode high quality non-linear feature representations input data. pre-trained encoder network connected feedforward network trained oc-nn objective obtain decision scores algorithm section feed-forward network configuration used experiments summarized table table comparison baseline state-of-the-art systems results mean standard error performance metrics random training draws. highlighted cells indicate best performer. protein sequences length apart testing oc-nn model. lstm-ae trained using network parameter settings described section encoded representations obtained pre-trained lstm-ae consisting matrix input feed-forward network hidden layer dimension output unit. alternative minimization algorithm summarized section employed optimize oc-nn feed-forward network objective. feed-forward network parameters used experiment described section results. figure insoluble protein sequences accurately identified anomalies negative decision scores. table illustrates oc-nn model significantly better existing state-of-the-art methods. ability oc-nn model extract progressively rich representation complex sequential data within hidden layer feed-forward network induces better anomaly-detection capability. comparison training times figure compare training test times oc-nn existing methods. results shown scale four real data sets. evident oc-nn takes longest time train however large data sets oc-svm-rbf approaches likely suffer approach anomaly detection. oc-nn uses one-class like loss function train neural network. advantage oc-nn features hidden layers constructed specific task anomaly detection. approach substantially different recently proposed hybrid approaches deep learning features input anomaly detector. feature extraction hybrid approaches generic aware dbn-svdd-linear dbn-svdd-rbf protvec -rpca-oc-svm-linear protvec-rpca-oc-svm-rbf protvec-isolation-forest protvec-oc-svm-linear protvec-oc-svm-rbf table comparison baseline state-of-the-art systems results mean standard error performance metrics random training draws. highlighted cells indicate best performer. anomaly detection task. learn parameters oc-nn network proposed novel alternating minimization approach shown optimization subproblem oc-nn equivalent quantile selection problem. experiments complex image sequential data sets demonstrates oc-nn highly accurate. future work would like build deploy system anomaly detection based oc-nn. references martín abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin tensorflow large-scale machine learning heterogeneous distributed systems. arxiv preprint arxiv. charu aggarwal. outlier analysis springer. jerone andrews edward morton lewis griffin. detecting anomalous data using auto-encoders. international journal machine learning computing françois chollet keras. https//github.com/keras-team/keras. yong shean chong yong haur tay. abnormal event detection videos using spatiotemporal autoencoder. international symposium neural networks. springer vishwanathan alexander smola narasimha murty. simplesvm. proceedings twentieth international conference international conference machine learning. aaai press liang xiong chen jeff schneider. direct robust matrix factorizatoin anomaly detection. international conference data mining ieee. huan constantine caramanis sujay sanghavi. robust outlier pursuit. advances neural information processing systems. jason yosinski jeff clune yoshua bengio lipson. transferable features deep neural networks?. advances neural information processing systems. chong zhou randy paffenroth. anomaly detection robust deep autoencoders. proceedings sigkdd international conference knowledge discovery data mining. sarah erfani sutharshan rajasegarar shanika karunasekera christopher leckie. high-dimensional large-scale anomaly detection using linear one-class deep learning. pattern recognition robert finn penelope coggill ruth eberhardt sean eddy jaina mistry alex mitchell simon potter marco punta matloob qureshi amaia sangradorvegas pfam protein families database towards sustainable future. nucleic acids research d–d. jonathan hull. database handwritten text recognition research. ieee transactions pattern analysis machine intelligence sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. international conference machine learning. tony ming ting zhi-hua zhou. isolation forest. data mining icdm’. eighth ieee international conference ieee erik marchi fabio vesperini stefano squartini björn schuller. deep recurrent neural network-based autoencoders acoustic novelty detection. computational intelligence neuroscience bernhard schölkopf john platt john shawe-taylor alex smola robert williamson. estimating support high-dimensional distribution. neural computation nitish srivastava elman mansimov ruslan salakhudinov. unsupervised learning video representations using lstms. international conference machine learning.", "year": 2018}