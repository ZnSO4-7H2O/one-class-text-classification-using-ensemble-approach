{"title": "Modeling Multiple Annotator Expertise in the Semi-Supervised Learning  Scenario", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Learning algorithms normally assume that there is at most one annotation or label per data point. However, in some scenarios, such as medical diagnosis and on-line collaboration,multiple annotations may be available. In either case, obtaining labels for data points can be expensive and time-consuming (in some circumstances ground-truth may not exist). Semi-supervised learning approaches have shown that utilizing the unlabeled data is often beneficial in these cases. This paper presents a probabilistic semi-supervised model and algorithm that allows for learning from both unlabeled and labeled data in the presence of multiple annotators. We assume that it is known what annotator labeled which data points. The proposed approach produces annotator models that allow us to provide (1) estimates of the true label and (2) annotator variable expertise for both labeled and unlabeled data. We provide numerical comparisons under various scenarios and with respect to standard semi-supervised learning. Experiments showed that the presented approach provides clear advantages over multi-annotator methods that do not use the unlabeled data and over methods that do not use multi-labeler information.", "text": "learning algorithms normally assume annotation label data point. however scenarios medical diagnosis on-line collaboration multiple annotations available. either case obtaining labels data points expensive time-consuming semi-supervised learning approaches shown utilizing unlabeled data often beneﬁcial cases. paper presents probabilistic semi-supervised model algorithm allows learning unlabeled labeled data presence multiple annotators. assume known annotator labeled data points. proposed approach produces annotator models allow provide estimates true label annotator variable expertise labeled unlabeled data. provide numerical comparisons various scenarios respect standard semisupervised learning. experiments showed presented approach provides clear advantages multi-annotator methods unlabeled data methods multi-labeler information. advances information technology made possible collect data increasingly faster rates. triggered favored collaborative aggregative forms data collection; example crowdsourcing phenomenon instance on-line data many speciﬁc subjects often analyzed processed multitude individuals entities general. clear instances include wikipedia forms form data organization creates machine learning problems associated efﬁcient utilization modeling processing information. various ways look problem. translated supervised learning context problem amounts labeler many labelers. novel scenario renders traditional supervised learning sub-optimal also creates exciting problems. reason highlighted noticing learning algorithm access labeler identity addition usual label values. turns piece information many cases interesting implications learning. multi-labeler setting important address real problems supervised learning suitable. include case ground-truth nature available expensive obtain recently several approaches undertaken address scenario. particular considered case labeler modeled associating overall accuracy across data. general idea explored case labeling accuracy dependent actual data point observed dependent annotator speciﬁc preferences paper addresses different facet problem. draw parallel semi-supervised learning explore question exploit data annotated labeler annotated labelers. natural approach would consist ignoring unlabeled data point. valid appropriate models efﬁcient. seen previous multi-labeler models would treat unlabeled data manner. however recent years many semi-supervised learning methods classiﬁcation introduced complete comprehensive review semi-supervised learning algorithms provided main scenarios commonly considered training semisupervised model transductive inductive scenarios. transductive setting learner needs observe unlabeled testing data training; therefore although accurate transductive models need retrained every time test sample classiﬁed. result transductive algorithms satisfy run-time requirements many real-world applications including medical diagnosis applications patient cases need classiﬁed real-time part physician’s workﬂow. inductive setting testing data assumed present time model training. state-of-the approaches semi-supervised learning based weighted graph labeled unlabeled points constitute vertices graph similarities between data point pairs represented edge weights. given graph contains information spatial proximity training data main idea behind methods notion classiﬁcation function learned give similar values neighboring points. words value separator function change smoothly neighboring data points. paper present inductive semi-supervised algorithm takes advantage available unlabeled data also assumes training point several labels available different annotators problem building classiﬁers presence multiple labelers receiving increasing attention. reasons increased interest multi-labeler classiﬁcation problems that illustrated recently employing multiple non-expert annotators effective employing expert annotator building classiﬁer. setting convenient example many medical applications cost expert labeling high nonexpert annotator time considerably less expensive interesting medical application areas multi-labeler learning include computer-aided diagnosis radiology clinical data integration however application areas multiple-labeler learning vary widely interest type modeling increasing rapidly. include natural language aware previous work solving multilabeler classiﬁcation problem semi-supervised scenario another distinguishing factor paper unlike previous approaches exception supervised learning assumed annotator expertise consistent across input data. ﬂawed assumption many instances since annotator accuracy depend strongly characteristics given case. taking consideration paper classiﬁers build take account labelers better labeling type data points thus model annotator expertise also obtained. consider data points data point annotated fewer labelers/annotators. denote label provided i-th data point annotator labels individual labelers assumed correct consistent provided labelers. denote true label i-th data point paper requirement. compactness represent data using matrix matrix denotes matrix/vector transpose. consider problem data points given labels missing annotators primary goals produce estimate ground-truth classiﬁer predicting instances model expertise annotator function input element random variable speciﬁed domains. usual supervised semisupervised learning scenarios data point labeled normally information labeler identity. multi-labeler problem addressed paper points labeled zero labelers addition aware labeled points fundamental question optimally exploit multi-annotator information semisupervised learning setting. view interest utilizing unlabeled data points consider alternative choice conditional distribution z|x. based incorporating graph-based prior. this consider graph associate data point node weight edge particular consider prior given graph laplacian positive deﬁnite matrix representing valid distance measure. could example relate points closer heavily farther apart. graph laplacian extensively used semi-supervised learning approaches paper borrow concept adapt multi-labeler scenario proposed. tablishing label assigned labeler data point denoted depends real unknown label addition label also depend coordinates observed point dependency represented conditional distribution dependency true label alone allows take consideration differences annotator accuracies. dependency also sufﬁces model annotator’s biases toward classes model annotator-speciﬁc error rates classes. dependency considered good extent. dependency input allows take account different interesting properties associated annotators. particular longer need assume annotators equally good labeling data accuracy depends input presented. general able model annotator-speciﬁc input-speciﬁc properties class annotator knowledgeable labeling kind inputs compared kinds inputs. ability model properties recently addressed however alone address problem efﬁciently utilizing unlabeled data seen following section. based considerations following describe progression three different strategies properly incorporate unlabeled data model. ﬁrst alternative given data point posit unknown distribution relates point true label. basically classiﬁcation function. since imply labels independently distributed given observations call model. practical alternatives learning model possible particular reduce complexity estimating example could consider neighbors data point generally largest inﬂuence calculation approximate posterior index neighbors i-th data point using metric choice. possible practice numerical experiments suggest following section provides better alternative. variation graph-prior model sec. addresses potential issues posterior required approximation; limitation important limitation given fact prior distribution technically ﬁxed beforehand thus limiting model ﬂexibility. multiple ways develop em-type algorithm since could missing general recipe algorithm prescribes computing expectations missing random variables part e-step. graph prior depends data remains observed. overall estimate conditioned observed variables updated iteratively shown algorithm thus m-step optimize flgp respect annotators’ majority vote standard logistic regression classiﬁers trained labels annotator supervised multi-labeler logistic regression model version approach variance function input similar spirit semi-supervised support vector machine classiﬁer linear kernel svm-light trained labels annotators’ majority vote. parameters tuned validation using grid search. compare methods test advantage learning unlabeled data. addition comparing also test whether learning multi-labelers better labeler alone. method also test effect taking variance annotator’s accuracy labeling across different observations account classiﬁcation performance. performed experiments various datasets machine learning data repository ionosphere housing pima bupa wisconsin breast cancer wisconsin breast cancer numbers parenthesis indicate number samples features respectively. since multiple annotations datasets available need simulate several labelers different labeler expertise accuracy. order simulate labelers dataset proceeded follows ﬁrst clustered data subsets using k-means then assume simulated labelers expert cases belonging cluster labeling coincides ground truth; rest cases labeler makes mistake time figure displays plots stratiﬁed ﬁve-fold cross-validated accuracies different methods datasets proportion training data labeled increased. results show semi-supervised multi-labeler based logistic laplacian prior best accuracies almost proportion labeled training data cases datasets. note approach performed better svm-light able take account multiple labelers expertise model. better supervised methods able learn unlabeled data well. moreover better original method semisupervised also take effect variance annotator’s accuracy labeling across different observations account. e-step estimate every data m-step )update maximize e˜p] using lbfgs quasinewton approximation compute step gradient equations )update estimates using appropriate gradients. given learned model exist multiple ways interpret problem making label prediction given data point. focus inferring ground-truth data point known training time speciﬁcally would like estimate basically equivalent performing e-step described computing model-speciﬁc e-step. section compare ﬁnal semi-supervised multi-annotator model logistic graph laplacian prior baseline methods number machine learning repository benchmark data simulated labelers real data multiple labelers problem automatic assessment heart wall motion abnormalities information extracted ultrasound images experiments show results logistic graph prior compared model clear advantage able make unlabeled data shown sec. since existing semi-supervised multi-annotator models literature compare method following baselines testing different aspects model standard logistic regression classiﬁer trained labels figure accuracies various datasets different proportion labellings training data results show averages randomized splits training test sets cross-validation automatic wall motion abnormality detection data consists ultrasound image sequences heart motion cases labeled heart wall segment level group trained cardiologists. according standard protocol left ventricle heart wall segments. segments ranked according movement. simplicity converted labels binary data provides sixteen two-class classiﬁcation problems experiments used global local image features node calculated tracked contours. since doctor labels ground assume majority vote doctors fair approximation true labels. applied stratiﬁed ﬁve-fold cross-validation evaluate results. figure shows average ﬁve-fold cross-validated accuracies method different baselines increase proportion training data labeled annotators. since data actually comprised several classiﬁcation problems report average standard deviation results observe model outperforms baseline methods data terms average accuracies. extra unlabeled information helping improve performance take multiple annotator labels account learning classiﬁer. experience working applications multiple annotators available observed typically different annotators varying expertise importantly expertise varies based observation labeled. thus incorporated variability labeling function observation model. addition learning multiple annotator information also allow model learn unlabeled data. ability important many common domains exist large amounts unlabeled data. summary paper introduced probabilistic model properly learn data several labels labels available data point. addition model annotators’ varying expertise across accuracies reveal trade-off true positive rate false alarm rate. here also show average results model compared different baselines. proportion training points results shown figure logistic-gp model outperform others labeled points logistic-gp slightly better semi-supervised almost equal performance original using labeled svm-light performed reasonably well close method labels because ground truth data based majority vote annotators svm-light uses labels training. interestingly even though logistic-gp ground truth label labels labelers outperformed svm-light. note labels available logistic-gp together original learn multiple labelers outperformed others simply based majority label labels single annotators. classiﬁcation algorithms designed utilize labels provided annotator. however applications multiple labels data point provided multiple annotators available. paper addresses learn different annotations build better classiﬁer compared classiﬁcation algorithms learn labeler. input space. experiments employing real simulated annotators benchmark real medical data show model taking advantage extra information unlabeled data outperforms approaches learn labeled data. moreover learning multiple annotators improves classiﬁcation performance standard single annotator supervised semi-supervised classiﬁers. joachims. transductive inference text classiﬁcation using support vector machines. international conference machine learning pages morgan kaufmann", "year": 2012}