{"title": "Learning Visual Reasoning Without Strong Priors", "tag": ["cs.CV", "cs.AI", "cs.CL", "stat.ML"], "abstract": "Achieving artificial visual reasoning - the ability to answer image-related questions which require a multi-step, high-level process - is an important step towards artificial general intelligence. This multi-modal task requires learning a question-dependent, structured reasoning process over images from language. Standard deep learning approaches tend to exploit biases in the data rather than learn this underlying structure, while leading methods learn to visually reason successfully but are hand-crafted for reasoning. We show that a general-purpose, Conditional Batch Normalization approach achieves state-of-the-art results on the CLEVR Visual Reasoning benchmark with a 2.4% error rate. We outperform the next best end-to-end method (4.5%) and even methods that use extra supervision (3.1%). We probe our model to shed light on how it reasons, showing it has learned a question-dependent, multi-step process. Previous work has operated under the assumption that visual reasoning calls for a specialized architecture, but we show that a general architecture with proper conditioning can learn to visually reason effectively.", "text": "paper show general model achieve strong visual reasoning language. conditional batch normalization recurrent neural network convolutional neural network show deep learning architectures built without strong priors learn underlying structure behind visual reasoning directly language images. demonstrate achieving state-of-the-art visual reasoning clevr ﬁnding structured patterns exploring internals model. model processes multi-modal question-image input using combined conditional batch normalization proven highly effective image stylization speech recognition traditional visual question answering tasks start explaining section describe model section batch normalization widely used technique improve neural network training normalizing activations throughout network respect mini-batch. shown accelerate training improve generalization reducing covariate shift throughout network explain deﬁne {fi...}n mini-batch samples corresponds input feature maps whose subscripts refers feature spatial location also deﬁne per-channel trainable achieving artiﬁcial visual reasoning ability answer image-related questions require multi-step high-level process important step towards artiﬁcial general intelligence. multi-modal task requires learning questiondependent structured reasoning process images language. standard deep learning approaches tend exploit biases data rather learn underlying structure leading methods learn visually reason successfully hand-crafted reasoning. show general-purpose conditional batch normalization approach achieves state-ofthe-art results clevr visual reasoning benchmark error rate. outperform next best end-to-end method even methods extra supervision probe model shed light reasons showing learned question-dependent multi-step process. previous work operated assumption visual reasoning calls specialized architecture show general architecture proper conditioning learn visually reason effectively. index terms deep learning language vision ability language reason every-day visual input fundamental building block human intelligence. achieving capacity visually reason thus meaningful step towards artiﬁcial agents truly understand world. advances image-based learning language-based learning using deep neural networks made huge strides difﬁcult tasks object recognition machine translation advances turn fueled research intersection visual linguistic learning recently proposed clevr dataset test multi-step reasoning language images traditional visual question-answering datasets simpler questions images often answered single glance. examples clevr shown figure structured multi-step reasoning quite difﬁcult standard deep learning approaches including successful traditional visual question answering datasets. previous work highlights standard deep learning approaches tend exploit biases data rather reason overcome visual pipeline extracts image features using conv layer resnet- pre-trained imagenet done clevr. image features processed convolution followed several model residual blocks feature maps ﬁnal classiﬁer. classiﬁer consists convolution feature maps global max-pooling two-layer hidden units outputs distribution ﬁnal answers. residual block starts convolution followed convolutions depicted figure drawing concatenate coordinate feature maps indicating relative spatial position image features residual block’s input classiﬁer’s input. train model end-to-end scratch adam early stopping validation weight decay batch size relu throughout visual pipeline using imagequestion-answer triplets training set. combined relu non-linearities empowers conditioning model manipulate feature maps target scaling down negating them shutting selectively thresholding them more. feature modulated independently giving conditioning model exponential number ways affect feature representation. since initially zero-centered ˆγic zero feature activations thus gradients. implementation output ∆ˆγic rather ˆγic simplicity rest paper explain method using ˆγic. model consists linguistic pipeline visual pipeline depicted figure linguistic pipeline processes question using gated recurrent unit hidden units takes learned -dimensional word embeddings. ﬁnal hidden state question embedding embedding model predicts parameters layer residual block linear projection trainable weight matrix bias vector clevr generated dataset tuples. images contain d-rendered objects various shapes materials colors sizes. questions multi-step compositional nature shown figure range counting questions comparison questions words long. answers word possible answers. programs additional supervisory signal consisting step-by-step instructions filter shape relate count answer question. program labels difﬁcult generate come real world datasets. model avoids using extra supervision learning reason effectively directly linguistic visual input. results clevr shown table model achieves overall state-of-the-art outperforming humans previous leading models often additional program supervision. notably outperforms stacked attention networks stacked attention networks highly effective visual question answering simpler questions previously leading model visual reasoning build reasoning making relevant baseline cbn. note also model’s pattern performance closely resembles humans models strong performance exist query attribute categories perhaps explained model’s close resemblance standard cnns traditionally excel classiﬁcation-type tasks. model also demonstrates strong performance complex categories count compare attribute. comparing numbers objects gives model difﬁculty understandably question type requires highlevel reasoning steps querying attributes counting comparing question type. best model beats model trained extra supervision program labels. shown table equivalent comparable model uses program labels signiﬁcantly underperforms method category. table clevr accuracy baseline methods competing methods method methods denoted extra supervisory information program labels. methods denoted data augmentation pre-trained cnn. understand model learns t-sne visualize parameter vectors random validation points modulating ﬁrst last layers model shown figure parameters ﬁrst last layers grouped low-level high-level reasoning functions necessary answer clevr questions equal color example query color close ﬁrst layer apart layer true equal shape last query shape equal size query size equal material query material. conversely equal shape equal size equal material parameters grouped last layer split ﬁrst layer. similar patterns emerge visualizing residual block activations. thus learns sort function-based modularity directly language image inputs witharchitectural prior modularity. simply end-toend training model learns handle different types questions differently also different types question subparts differently working low-level high-level processes proper approach answer clevr questions. additionally observe many points break previously mentioned clustering patterns meaningful ways. example figure shows count questions last layer parameters count questions close exist questions. closer examination reveals count questions answers either making similar exist questions. analysis model’s errors reveals counting mistakes off-by-one errors indicating model learned underlying concepts behind counting close relationships close numbers. shown figure model struggles questions require steps indicated length corresponding clevr programs; error rates questions requiring fewer steps around error rates questions requiring steps around three times higher. model correctly counts cyan objects yellow objects simultaneously answer number cyan yellow objects. fact answer number cyan blocks more less equal number yellow blocks. errors could prevented directly minimizing logical inconsistency interesting avenue future work orthogonal approach. types mistakes state-of-the-art visual reasoning model suggest work needed truly achieve human-like reasoning logical consistency. view clevr curriculum tasks believe meaningful advanced reasoning lies tackling last percentage points error. leading approach visual reasoning program generator execution engine model approach consists sequence-to-sequence program generator takes question outputs sequence corresponding tree composable neural modules twolayer residual block similar ours. tree neural modules assembled form execution engine predicts answer image. pg+ee model uses strong prior training program labels explicitly modeling compositional nature reasoning. approach learns reason directly textual input without using additional cues specialized architecture. modular approach part recent line work neural module networks these end-to-end module networks also tackle visual reasoning perform well approaches. methods also strong priors modeling compositionality reasoning using program-level supervision building permodule hand-crafted neural architectures speciﬁc functions. figure t-sne plots ﬁrst layer ﬁrst residual block last layer last residual block parameters grouped low-level reasoning functions ﬁrst layer high-level reasoning functions last layer. architecture conditions layers pre-trained resnet. show layers resnet also highly effective even complex problems. also show models learn carry multi-step processes reason structured low-level high-level. additionally essentially post-bn feature-wise afﬁne conditioning bn’s trainable scalars turned off. thus many interesting connections conditioning methods. common approach used example conditional dcgans concatenate constant feature maps conditioning information input convolutional layers amounts adding post-convolutional featurewise conditional bias. approaches lstms hierarchical mixtures experts gate input’s features function input amounts feature-wise conditional scaling restricted consists scaling shifting unrestricted giving capacity many related approaches. leave exploring connections in-depth future work. simple general model based show possible achieve state-of-the-art visual reasoning clevr without explicitly incorporating reasoning priors. show model learns underlying structure required answer clevr questions ﬁnding clusters parameters model; earlier parameters grouped low-level reasoning functions later parameters grouped high-level reasoning functions. simply manipulating feature maps effectively language inﬂuence carry diverse multi-step reasoning tasks image. unclear whether effective general conditioning information visual reasoning tasks well precisely effective. approaches employ similar repetitive conditioning perhaps underlying principle explains success approaches. regardless believe general powerful technique multi-modal conditional tasks especially complex structure involved. question many yellow things there? many cyan things there? many yellow things cyan things? yellow things cyan things? fewer yellow things cyan things? relation networks another leading approach visual reasoning. carry pairwise comparisons location extracted convolutional features image including lstm-extracted question features input mlp. element-wise resulting comparison vectors form another vector ﬁnal classiﬁer predicts answer. approach end-to-end differentiable trainable scratch high performance show table approach lifts explicitly relational aspect model freeing approach comparison-based prior well scaling difﬁculties pairwise comparisons spatial locations. line work. results show closely related conditional instance normalization able successfully modulate convolutional styletransfer network quickly scalably render image huge variety different styles simply learning output different parameters based target style. visual question answering answering general questions often natural images vries show performs highly real-world guesswhat? datasets demonstrating cbn’s effectiveness beyond simpler clevr images. would like thank developers pytorch elegant deep learning framework. also implementation based open-source code thank mohammad pezeshki dzmitry bahdanau yoshua bengio nando freitas joelle pineau olivier pietquin j´er´emie mary chin-wei huang layla asri smith helpful feedback discussions well justin johnson clevr test evaluations. thank nvidia donating dgx- computer used work. also acknowledge frqnt chist-era iglu project cper nord-pas calais coll`ege doctoral lille nord france feder data advanced data science technologies funding research. merrienboer g¨ulc¸ehre bougares schwenk bengio learning phrase representations using encoder-decoder statistical machine translation proc. emnlp vol. abs/. santoro raposo barrett malinowski pascanu battaglia lillicrap simple neural network module relational reasoning corr vol. abs/. available http//arxiv.org/abs/ andreas rohrbach darrell saenko learning reason end-to-end module networks visual question answering corr vol. abs/. available http//arxiv.org/abs/. vries strub mary larochelle pietquin courville modulating early visual processing language arxiv preprint arxiv. available http//arxiv.org/abs/. ghiasi kudlur dumoulin shlens exploring structure real-time arbitrary neural artistic stylization network corr vol. abs/. available http//arxiv.org/abs/. russakovsky deng krause satheesh huang karpathy khosla bernstein berg imagenet large scale visual recognition challenge international journal computer vision vol.", "year": 2017}