{"title": "MazeBase: A Sandbox for Learning from Games", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "This paper introduces MazeBase: an environment for simple 2D games, designed as a sandbox for machine learning approaches to reasoning and planning. Within it, we create 10 simple games embodying a range of algorithmic tasks (e.g. if-then statements or set negation). A variety of neural models (fully connected, convolutional network, memory network) are deployed via reinforcement learning on these games, with and without a procedurally generated curriculum. Despite the tasks' simplicity, the performance of the models is far from optimal, suggesting directions for future development. We also demonstrate the versatility of MazeBase by using it to emulate small combat scenarios from StarCraft. Models trained on the MazeBase version can be directly applied to StarCraft, where they consistently beat the in-game AI.", "text": "paper introduces mazebase environment simple games designed sandbox machine learning approaches reasoning planning. within create simple games embodying range algorithmic tasks variety neural models deployed reinforcement learning games without procedurally generated curriculum. despite tasks’ simplicity performance models optimal suggesting directions future development. also demonstrate versatility mazebase using emulate small combat scenarios starcraft. models trained mazebase version directly applied starcraft consistently beat in-game games important role artiﬁcial intelligence research since inception ﬁeld. core problems search planning explored naturally context chess recently served test-bed machine learning approaches example atari games investigated using neural models reinforcement learning gvg-ai competition uses suite arcade games compare planning search methods. emphasis learning understand environment rather testing algorithms search planning. framework deliberately lacks simulation facility hence agents cannot search methods determine next action unless predict future game states themselves. hand game components meant reused different games giving models opportunity comprehend function water tile. rules games provided agent instead must learned exploration environment. environment designed allow programmatic control game difﬁculty. allows automatic contruction curricula show important training complex models. games based around simple algorithmic reasoning providing natural path exploring complex abstract reasoning problems language-based grounded setting. contrasts games originally designed human enjoyment rather speciﬁc task. also differs recent surge work learning simple algorithms lack grounding. despite nature environment prefer text-based rather pixelbased representation. provides efﬁcient expressive representation without overhead solving perception problem inherent pixel based representations. easily allows different task speciﬁcations easy generalization models game settings. demonstrate training models mazebase successfully evaluating starcrafttm∗. mikolov discussion. using environment introduce simple games train range standard neural network-based models policy gradient also combine recent memnn model reinforcement learning evaluate games. results show current approaches struggle despite relatively simple nature tasks. also highlight clear areas future model exploration defer work. mazebase open-source platform implemented using torch downloaded https//github.com/facebook/mazebase. mazebase environment thought small practical step towards ideas discussed length mikolov particular interfacing agent environment natural language inspired discussions authors work. however ambitions local focusing border current models fail rather aiming global view path towards example speciﬁcally avoid algorithmic tasks require unbounded recursions loops plenty difﬁculty learning simple if-then statements. furthermore example games described below allow large numbers training runs noise reinforcement discrete actions remains challenging even many samples. non-game environments recent work learning simple algorithms. demonstrating tasks sorting reversal inputs. algorithms instantiated games even simpler e.g. conditional statements navigation location involve interaction environment. approaches models trained reinforcement learning using discrete search allowing possibly delayed rewards discrete action spaces. games also involve discrete actions works inform choice reinforcement learning techniques. several works also demonstrated ability neural models learn answer questions simple natural language within restricted environment tasks present share many features weston input-output format games inter-operable stories. however training testing environment weston static unlike game worlds consider. developing game agents extensive literature. work similar mnih mnih reinforcement neural models training games. gvg-ai competition similar overall intent mazebase differs appropriate testing search-based methods since simulator provided. correspondingly many algorithms competition rely monte-carlo tree search methods. contrast mazebase designed sandbox supports development learning-based algorithms; search done agent must done agent’s predictions. similarly competitions organized around super mario pacman encourage search based heuristics speciﬁc game. hand mazebase designed encourage learning algorithms understand environment reuse knowledge games used incremental exploration certain core problems example basic logical reasoning addressed paper. furthermore easily support algorithmic curriculum training. also note given sandbox nature mazebase principle could used recreate games including benchmark versatility mazebase demonstrated ease able create proxy starcraft combat within environment. environment takes many basic ideas classical puddle world example basic grid structure water obstacles richer agent expected memorize given world regenerated game agents tested unseen worlds. game played rectangular grid. speciﬁc examples below dimensions range side course however user likes. location grid empty contain items. agent move four cardinal directions assuming item blocks agents path. items game pushableblock block impassable moved separate push actions. block moves direction push agent must located adjacent block opposite direction push. corner item simply marks corner board. goal depending task goals exist named individually. info items grid location specify task give information environment presented agent list sentences describing item game. example agent might block switch blue color. info change switch red. representation compatible format babi tasks introduced weston however note egocentric spatial coordinates coordinates meaning environment updates locations object action†. furthermore tasks involving multiple goals versions game. environment automatically sets visited goals. harder versions mechanism absent agent special action allows release breadcrumb environment enabling record locations visited. experiments below unless otherwise speciﬁed report results games explicit ﬂag. environments generated randomly distribution various items. example usually specify uniform distribution height width percentage wall blocks water blocks although game world simple allows rich variety tasks. work explore require different algorithmic components ﬁrst isolation combination. components include conditional reasoning if-then statements statements. basic arithmetic comparison small numbers. manipulation altering environment toggling switch moving pushablefigure examples multigoal light tasks. note layout dimensions environment varies different instances task agent shown blob goals shown yellow. lightkey switch show cyan door magenta/red selected elements needed complex reasoning tasks although limit ourselves combining given task. note direct parallels babi tasks avoid tasks require unbounded loops recursion joulin mikolov instead view algorithms vein following recipe cookbook. particular want agent able follow directions; game world host multiple tasks agent must decide based info items. demonstrate standard neural models challenging. tasks agent incurs ﬁxed penalty action makes encourages agent ﬁnish task promptly. experiments below addition stepping water block incurs additional penalty games maximum actions allowed. tasks deﬁne extra penalties conditions game end. multigoals task agent given ordered list goals info needs visit goals order. experiments below number goals ranges number active agent required visit ranges goals. agent given extra penalty visiting goal order visiting goal turn count towards visiting goals. game ends goals visited. task involves algorithmic component iterating list. exclusion info game speciﬁes list goals avoid. agent visit unmentioned goals. number goals ranges form number active goals ranges conditional goals game agent incurs penalty steps forbidden goal. task combines multigoals negation. conditional goals task destination goal conditional state switch. info form goal switch colored else goal experiments below number number colors range number goals note colors goals goals colors. task concludes agent reaches speciﬁed goal; addition agent incurs penalty stepping incorrect goal order encourage read info task requires conditional reasoning form if-then statement. switches task game random number switches board. agent told info toggle switches color agent choice color; best reward agent needs solve traveling salesman problem. experiments below number switches ranges number colors task ﬁnishes switches correctly toggled. special penalties task. task instantiates form statement. light game switch door wall blocks. agent navigate goal wrong side wall blocks. goal side wall agent directly there; otherwise needs move toggle switch open door going goal. special penalties game game ends agent reaches goal. task combines if-then reasoning environment manipulation. goto task agent given absolute location grid target. game ends agent visits location. solving task requires agent convert egocentric coordinate representation absolute coordinates. involves comparison small numbers. goto hidden task agent given list goals absolute coordinates told goals goal’s name. agent directly given goal’s location must read list goal locations. number goals ranges task also involves simple comparison operation. push block game agent needs push pushable block lays switch. considering large number actions needed solve task size limited maximum block water percentage reduced task requires manipulation environment. push block cardinal game agent needs push pushable block speciﬁed edge maze e.g. left edge. location along edge acceptable. limitation push block game applied. blocked door task agent navigate goal opposite side wall blocks light game. however pushableblock blocks wall instead door. requires if-then reasoning well environment manipulation. task compute ofﬂine optimal solution. tasks e.g. multigoals involves solving traveling salesman problem provides upper bound reward achievable. used comparison purposes only i.e. used training models. exception multigoals task markovian; multigoals markovian explicit visited experiments below. nevertheless tasks simple; although environment easily used build non-markovian tasks solving tasks without agent reason past actions already challenging. examples game shown https//youtu.be/kwnpjfrie. note building tasks easy operation mazebase environment indeed many implemented hundred lines code. investigate several different types model simple linear multi-layer neural nets convolutional nets end-to-end memory networks input format quite different approach outputs same probability distribution discrete actions {nsewtoggle switchpushnpush-spush-epush-w}; continuous baseline value predicting expected reward. consider models recurrent state-action sequence rnns lstms discussed above tasks markovian. linear simple baseline take existence possible word-location pair largest grid consider info item separate feature train linear classiﬁer action space features. construct input take bag-of-words representation items location. then concatanate features every possible locations info items. example different words possible locations additional info items input dimension would multi-layer neural network multiple fully connected layers separated tanh nonlinearity. input representation linear model. convolutional first represent location bag-of-words linear model. hence environment presented cube size feed four layers convolution items without spatial location represented words combined fully connected layer outputs convolutional layers; passed fully connected layers output actions memory network item game represented bag-of-words vectors. spatial location item also represented word within bag. e.g. door becomes vector {red door} {x=+y=-} {red door} {x=+y=-} word vectors dimension embedding vectors learned training time. consequence memory network learn spatial arrangement grid unlike convolutional network. otherwise architecture hops tanh nonlinearities. policy gradient training maximizes expected reward using unbiased gradient estimates. first play game feeding current state model sampling next action output. ﬁnishing game update model parameters reward given time length game. instead using single baseline value every state model output baseline value speciﬁc current state. accomplished adding extra head models outputting baseline value. beside maximizing expected reward policy gradient models also trained minimize distance baseline value actual reward. ﬁnal update rule better parallelism model plays learns games simultaneously spread multiple threads. training continued thousand parallel episodes amounts game plays. depending model type whole training process took hours cpus single machine. feature environment ability programmatically vary properties given game. ability construct instances game whose difﬁculty precisely speciﬁed instances shaped curriculum training demonstrate important avoiding local minima helps learn superior models. game many variables impact difﬁculty. generic ones include maze dimensions fraction blocks water. switch-based games number switches colors varied. goal based games combat game kiting vary number agents enemies well speed initial health. curriculum speciﬁed upper lower success thresholds respectively. success rate model falls outside interval difﬁculty generated games adjusted accordingly. game generated uniformly sampling variable affects difﬁculty range. upper limit range adjusted depending violated. note lower limit remains unaltered thus easiest game remains difﬁculty. last third training expose model full range difﬁculties setting upper limit maximum preset value. figure mazebase environment difﬁculty tasks varied programmatically. example multigoals game maximum size fraction blocks/water number goals varied. affects difﬁculty tasks shown optimal reward also reveals robust given model task difﬁculty. -layer reward achieved degrades much faster memnn modle inherent task difﬁculty. fig. shows performance different models games. model trained jointly games. given stochastic nature reinforcement learning trained model times picked single instance highest mean reward across tasks table appendix gives maxmean standard devision rewards task method. video showing trained memnn model playing games found https//youtu.be/kwnpjfrie. results revealed number interesting points. many games least models able learn reasonable strategy. models able learn convert egocentric absolute coordinates using corner blocks. could respond appropriately different arrangements light game make decent decisions whether directly goal ﬁrst open door. -layer networks able completely solve tasks pushable blocks. conditional goals exclusion models poorly. inspection appears adopted strategy blindly visiting goals rather visiting correct one. models able train jointly make game types artiﬁcially small; test time successfully games larger map. models able learn notion locations independently task hand tried test models unseen tasks never shown train time used vocabulary none models able succeed highlighting operating human level even extremely restricted setting. average memory network best methods. however games pushable blocks layer neural nets superior e.g. exclusion push block blocked door. although also trained layer neural result included similar rewards layer neural net. parameters either convolutional network memory network. example layer neural hidden layer size separate input outer product location word combination. size games orders magnitude parameters convolutional network memory network. nevertheless even large number trials architecture could learn many tasks. memory network seems superior games involving decisions using information info items whereas -layer neural better games pushable block note egocentric coordinates push block cardinal lesser extent push block models memorize local conﬁgurations block agent. methods signiﬁcant variance performance instances except linear model. however curriculum signiﬁcantly decreased variance methods especially -layer neural net. also tried supervised training results less reinforcement. exception learning breadcrumb action multigoals game. none models able learn breadcrumb mark visited locations without supervision. note tables show results explicit visited given environment. figure reward model jointly trained games without curriculum training. y-axis shows relative reward thus higher better. estimated optimal policy corresponds value value implies agent takes twice many steps complete task needed section mazebase implement several simple combat games. train agents using games test combat micro-management starcraft brood involving limited number troops kiting match-up control weakly armored fast ranged unit powerful melee ranged unit. unit needs alternate ﬂeeing opponents shooting weapon cooled down. goal asses ability models generalize rather whether make game environment close counterpart starcraft show environment used prototyping scenarios training actual game technically challenging. mazebase kiting scenario consists standard maze agent aims kill enemy bots. shot agent enemy prevented ﬁring small time interval introduce imbalance allowing agent shoot farther enemy giving agent signiﬁcantly less health bot; allowing enemy shoot frequently agent agent shot range squares bots shot range squares. enemy moves speed agent. accomplished rolling fumble time tries move probability agent health chosen uniformly enemy health uniformly distributed enemy shoot every turns agent shoot every turns. enemy follows heuristic attacking agent range cooldown attempting move towards agent closer squares away ignoring farther squares. scenario modeled mazebase agents health points bots hitpoints randomly chosen agents bots range cooldown bots heuristic attacking closest agent attacked agent before continuing attack follow agent killed. kiting scenarios randomly noise agents inputs account fact encounter vocabulary playing starcraft. time numerical value enemies’ agents’ health cooldown taken random value. train memnn model using difference armies points loss overall battle reward signals. also scenarios inside starcraft brood war. used bwapi connect game torch framework. receive game state send orders enabling reinforcement learning loop. train layer neural network memnn models using protocol. features used categorical represent points weapon cooldown positions unit. used multi-resolution encoding position reduce number parameters. case multiple units controlled independently. take action every frames architectures hyper-parameter settings used kiting game models able learn basic tactics focusing weaker opponents kill ﬁrst results rate built-in starcraft nearly perfect results kiting video https//youtu.be/hnsra_uark shows example gameplay memnn model starcraft kiting hard scenario. finally test models trained environment directly starcraft. make modiﬁcations models minimal changes interface success rate models trained maze tested starcraft comparable training directly starcraft showing environment effectively used sandbox exploring model architectures hyper-parameter optimization. mazebase enivronment allows easy creation games precise control behavior. allowed quickly devise simple games embodying algorithmic components evalaute using range neural models. ﬂexibility environment enabled curricula created game aided training models resulted superior test performance. even curriculum cases models fell short optimal performance. memory networks able solve tasks fully-connected models table rates starcraft built-in shows hand-coded baseline strategy always attacking weakest enemy shows memnn trained tested starcraft. last shows memnn trained entirely inside mazebase tested starcraft modiﬁcations tuning except scaling inputs. convnets could although overall performance similar. suggests existing neural models lack fundamental abilities needed solve algorithmic reasoning. potential candidates include ability plan forecast outcome actions sophisticated memory also showed mazebase environment used develop model architectures trained starcraft convincingly beat in-game range simple combat settings. indirectly also environment build games approximate task interest enabling training models perform effectively target task without exposure training. table reward different models games without curriculum. cell contains numbers best performing mean runs standard deviation runs different random initialization. estimated-optimal shows estimated highest average reward possible game. note estimates based simple heuristics exactly optimal. xiaoxiao singh satinder honglak lewis richard wang xiaoshi. deep learning real-time atari game play using ofﬂine monte-carlo tree search planning. advances neural information processing systems mnih volodymyr kavukcuoglu koray silver david graves alex antonoglou ioannis wierstra daan riedmiller martin. playing atari deep reinforcement learning. nips deep learning workshop. mnih volodymyr kavukcuoglu koray silver david rusu andrei veness joel bellemare marc graves alex riedmiller martin fidjeland andreas ostrovski georg petersen stig beattie charles sadik amir antonoglou ioannis king helen kumaran dharshan wierstra daan legg shane hassabis demis. human-level control deep reinforcement learning. nature shaker togelius yannakakis weber shimizu hashiyama soreson pasquier mawhorter takahashi smith baumgarten mario championship level generation track. special issue ieee transactions procedural content generation", "year": 2015}