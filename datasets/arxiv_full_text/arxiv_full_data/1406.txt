{"title": "Deep Neural Networks - A Brief History", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "Introduction to deep neural networks and their history.", "text": "information hierarchical subsequent level processing extracts abstract /global invariant features. words automatically learn features data aggregate purpose recognizing objects images. shall illustrate work example area face recognition. there inputs images first level processing simple image characteristics edges extracted. second subsequent levels complex parts image formed finally output layer recognize human faces. contrast using traditional approach first step known preprocessing expert guides process extracting features used recognizing faces. common part different approaches output layer labeled data needed perform supervised learning i.e. assigning names/labels faces. supervised majority successful applications used semi-supervised mode unsupervised extracting features hidden layers followed supervised learning output layer. fully supervised mode frequently used algorithm backpropagation ramp/rectifier activation function efficient networks many layers. supervised approach however contradicts idea deep learning classical backpropagation learning sigmoid replaced ramp function =max spectrum area fully unsupervised little progress reported far. image recognition easily solvable humans. specifically inspired although still vague understanding human brain processes information. depending goal brain modeling distinguish approaches. goal model brain’s neural circuits area called neuroinformatics computational neuroscience question validating generated model well experimental biological data? approach neuron model frequently used spiking appropriate learning rule. hand goal solve practical problem face recognition validation question changes model efficient? latter case important whether simple complex neuron model specific learning rule used. type modeling known neuromorphic computing. endows enormous storage capacity. define storage capacity ratio number patterns stored retrieved size network network consisting neurons retrieve correctly stored patterns according formula n/). thus network neurons store patterns neurons grows patterns. latter number patterns enough human store remember every single image word situation etc. encountered lifetime. fact human brain even bigger storage capacity group neurons store many different patterns phenomenon known polysynchrony fortunately people remember everything time born although well documented cases individuals remembered everything past day. comparison current artificial neural networks incomparably smaller largest using tens thousands neurons. reasons size neural networks designed solve domainspecific problems versus solving problems many domains time. example network designed start defining building blocks dnn. neuron model performs basic computations learning rule updates weights/synapses neurons network architecture specifies neurons topologically arranged interconnected. detail artificial neuron model also increases computational complexity. first simple model neuron called threshold neuron developed mcculloch pitts calculates product input vector weight vector neuron higher transfer function fires/generates output first spiking neuron model developed hodgkin huxley later received nobel prize. modeled squid’s giant neuron treated component neuron including membrane electrical component. model described stimulus injected current voltage/membrane potential leakage current potassium sodium channels conductances e.g. ms/cm ms/cm reversal potentials e.g. channel gating/activation variables better understand look equivalent electric circuit shown figure membrane modeled capacitor potassium sodium channels conductances neuron’s membrane potential i.e. difference intracellular extracellular potentials. according kirchhoff’s currents zero current membrane dv/dt written shorter form corresponding electric circuit shown figure similar hodgkin huxley’s. models potassium channel refractory properties adaptation stimuli mimics excitatory inhibitory post synaptic potentials neuron. psps illustrated figure refer back potentials later describe learning rules. where membrane potential membrane resting potential potassium resting potential inhibitory resting potential excitatory resting potential transmembrane potentials transmembrane conductances membrane resting conductance potassium resting conductance inhibitory resting conductance excitatory resting conductance decay time constant threshold value resting value threshold decay threshold constant tmem membrane time constant tmem =c/g current membrane sc/g current injected cell rise threshold membrane capacitance postfiring potassium increment working mcgregor’s model illustrated figure using network three neurons presynaptic feed post-synaptic neuron spikes generated types pre-synaptic neurons shown bottom panel figure positive excitatory negative inhibitory inputs feed post-synaptic neuron integrates rises threshold post-synaptic neuron fires spike; four spikes generated postsynaptic neuron shown panel figure notice threshold neuron changes time. simplest spiking neuron model developed izhikevich model biological neuron functions except accurately mimics several types /shapes postsynaptic potentials generated human brain neurons. described zhikevich model became popular simplicity allows building networks consisting thousands neurons. using however found increasing strength stimulus caused fire higher higher frequency biologically plausible neurons cannot fire absolute refractory period needed restoration membrane potentials matter strength input. thus corrected condition neuron firing changing added additional check account refractory property neurons. figure illustrates firings four types original izhikevich neurons figure shows firings neurons modification. modified model used modeling multi-column multi-layer model neocortex possible using original izhikevich model start noticing almost learning rules based degree konorski’s observation presynaptic neuron repeatedly fires postsynaptic neuron within short time synaptic strength increased otherwise decreased. credit observation often given hebb although konorski published year earlier practical learning rules i.e. equations corresponding observation specified much later computational scientists imilar case giving credit original inventor involves popular backpropagation learning rule first specified statisticians robbins monroe called stochastic approximation method. however credit rule neural networks literature given rumelhart hinton williams found werbos specified rule also neural networks dozen years them. simplest learning rule called perceptron one-layer feed-forward neural networks defined rosenblatt backpropagation rule fact perceptron’s rule extension many-layer networks. extending networks however became possible step threshold function used perceptron replaced differentiable sigmoid function. seemingly small change explosion neural networks research stagnated almost years minsky papert stated neural networks useless solving complex problems. kohonen specified winner-takes-all learning rule. rule closely perceptron backpropagation mimics learning processes taking place biological neural circuits. states neuron whose weight vector closest input’s vector winner increases weight even closer input pattern vector. often number neurons close neighborhood winning neuron also adjust weights. first rule networks spiking neurons called spike time-dependent plasticity specified song miller abbot swiercz cios staley specified another rule spiking neurons called synaptic activity plasticity rule rules compared figure konorski’s observation translated rules following recipe adjustment strength synaptic connections pre-synaptic neuron post-synaptic neuron takes place every time postsynaptic neuron fires according function specified either stdp sapr. positive means pre-synaptic neuron fired post-synaptic neuron strength increased. negative means pre-synaptic neuron fired post-synaptic neuron fired strength decreased. difference rules sapr uses function continuous differentiable also dynamic uses actual post-synaptic potential functions modify connection strengths neurons. words adjustments depend shape sapr turn depends shape chosen postsynaptic functions given neural circuit. left part sapr function figure chosen inhibitory right part chosen excitatory psp; function shapes figure contrast stdp rule uses static function meaning adjustments always same; depend shape inhibitory/excitatory psps given igure comparison sapr stdp latter fixed former changes depending shape excitatory inhibitory post synaptic functions neurons. network architecture cognitive tasks. architecture distinguishing factors several types neural networks dnn. follows neural networks hidden layer kohonen’s self-organizing feature dnn. digression popular decision tree algorithm perform deep learning either spite hierarchical architecture since uses original features hierarchy transformed features. ierarchical processing information brain first discovered neurophysiologists hubel wiesel studied cat’s visual system; work awarded nobel prize. observed brain’s hierarchical processing information also level processing brain extracts general features performed complex cells aggregate features extracted previous level process recognize objects input image. first level brain focuses recognizing specific simple patterns input images vertical horizontal elements present input images extracted simple cells. hubel wiesel thus originators ideas leading development dnn. easy notice figures today similar architectures. system implemented using neuron model mcculloh pitts outputs/fires inputs threshold outputs otherwise. changing threshold value neuron perform logical operations conjunction disjunction. conjunction achieved follows threshold relatively high inputs presynaptic neurons required fire neurons recognize different line orientations images vertical horizontal diagonal. neuron also perform disjunction threshold relatively input three presynaptic neurons fires illustrated figure first column image digit four neurons simple cells first layer perform conjunctions recognize three-element line patterns four neurons complex cells second layer perform disjunctions aggregate simple patterns complex ones until output layer digit recognized. parlance conjunction called convolution disjunction spatial pooling simple cell feature extractor/detector complex cell feature aggregator/analyzer. difference described simple scheme feature extraction happens without human intervention igure illustration simple complex cells extract specific features input images; implementation features extracted aggregated neocognitron recognize digit first researcher design direct precursor using hubel wiesel’s discoveries fukushima called network neocognitron. figure illustrates features image letter first picked simple cells aggregated complex cells order recognize letter output. s-layer simple cells extracts features previous stage hierarchy c-layer complex cells ensures tolerance shifts features extracted s-layer. became popular term deep learning coined widely accepted around development efficient learning algorithms hardware speed-ups gpus. particular lecun hinton krizhevski made significant impact field. comparison architecture neocognitron shown figure architecture lecun’s convolutional network shown figure shows great similarity. layer weights trained supervised mode. often used approach perform feature extraction input hidden layer idea autoencoder. method often used perform unsupervised learning boltzman machine. explain autoencoder works using feed-forward neural network backpropagation learning vertical composition meaning operation performed first hidden layer original input also performed second layer output first hidden layer etc. also assume input image size number neurons first hidden layer smaller task autoencoder learn outputs first hidden layer learning reconstruct inputs small distortion. words autoencoder learns compressed version inputs. respect autoencoder similar performs clustering. oosely speaking outputs first hidden layer neurons trained recognize specific features linear combinations original image features edges different positions orientations. meant saying features automatically learned/extracted deep neural networks. process repeated output second hidden layer takes input output first hidden layer. outcome previously extracted features edges aggregated complex features silhouettes objects. supervised learning used train weights last hidden output layer order assign labels input images. nstead describing neocognitron convolutional neural network lecun many excellent online resources exist describe network called irnn inspired works hubel wiesel fukushima irnn hidden layers perform explicit clustering operations images purpose extracting features level hierarchical processing. irnn consists input layer output layer hidden layers shown figure sensory layer extracts local features images. role hidden layer aggregate local features generate higher level semi-global features. output layer supervised mode associates semi-global features known labels. notice irnn operates like semi-supervised convolutional dnn. uses windowing based biological observation neuron connected sensory system receives inputs portion sensory neurons. igure shows stacks neurons represented small balls. generated represent explained figure three subimages/windows three input images clustered using novel image similarity measure first subimages similar clustered together neuron since subimage found quite different subimages creates cluster second neuron generated. weights initially first subimage pixel values vector later updated represent cluster center scanning entire images result might shown figure notice center neurons created represent image details nose eyes mouth periphery background images single neurons/clusters needed. figure architecture network spiking neurons; high-level block diagram. recurrent synaptic connections excitatory neurons feature extraction layer. synaptic connections excitatory neurons sensory/feature extraction layer inhibitory neurons feature extraction layer semi-global features. clustering subimages level performed layers disabled. finally associative layer associates images recognition codes like person person using winner-takes-all learning rule. note number clusters/neurons irnn predetermined user depends similarity subimages. characteristic feature irnn data become available already trained network used ways. data points labeled additional training continues data. however data labeled output layer needs trained. networks described including convolutional used simple spiking neuron models basic processing units. possible perform deep learning using networks spiking neurons? shin used network face recognition without preprocessing images. network self-organizes level hierarchical processing. even output layer spiking neurons used labeling faces contrast popular supervised methods backpropagation; similar approach later used et.al. specifically spiking neuron model used mcgregor’s sapr stdp learning rules self-organization neurons; self-organization essence clustering operation. figure shows architecture network. sensory layer serves relay input image increases input dimension image size image size nxm. hidden layer feature extraction layer composed excitatory inhibitory neurons keeping ratio close observed human brain. notice supervised training performed layer. instead multiple submitting input images required negligible change result self-organization. shown using sapr rule gave better results recognizing face images using stdp. recognition layer also uses spiking neurons neuron spikes known input face image recognizes person. network performed particularly well rotated partially occluded images. short network uses images input extracts features without training sensory feature extracting layers. contrast using autoencoder seen supervised training method label compressed pattern input pattern. human participation. unfortunately history science tells technologies often accompanied high dose hype exception this. described below groups researchers shown spectacular failing image recognitions tasks trivial humans. experiment researchers used trained slightly modified images called adversarial examples. network seen original images training. modification perceptible human difference original adversarial image; latter slightly different statistical properties. example tell difference image slightly modified image. however latter input alexnet trained original image failed recognize another work researchers took opposite approach. namely modified image used training resemblance whatsoever original image. example image looking like static noise recognized lenet peacock also certain recognition decision. described shortcomings outweigh many advantages. however lots research needed answer question failed experiments. think increasingly important computational researchers team neuroscientists come better algorithms image recognition algorithms cannot easily fooled require first place work neuroscientists better understand processes used brain recognition tasks. easy fooling recognition tasks easily recognized humans poses serious cybersecurity risk. modern society heavily relies machine learning techniques like performing many everyday tasks medical diagnosis self-driving cars investing financial assets even legal system. since researchers shown relatively easy come adversarial examples automated systems much depend produce possibly disastrous results. thus increasingly important researchers safety features deep learning algorithms developing something software engineers long time assure safety code. start with researchers routinely training adversarial examples addition original ones make systems secure. references ukushima neocognitron self organizing neural network model mechanism pattern recognition unaffected shift position. biological cybernetics hebb organization behavior. wiley keniston cios modeling multisensory convergence network spiking neurons reverse engineering approach. ieee transactions biomedical engineering smith swiercz staley rickard montero kurgan cios recognition partially occluded rotated images network spiking neurons. ieee transactions neural networks track jacobs cios biological restraint izhikevich neuron model essential seizure modeling. proc. int. ieee embs conference neural engineering diego november track jacobs cios simulating vertical horizontal inhibition short term dynamics multi-column multi-layer model neocortex. international journal neural systems", "year": 2017}