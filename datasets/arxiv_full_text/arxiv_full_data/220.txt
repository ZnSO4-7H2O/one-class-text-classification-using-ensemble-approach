{"title": "Gaussian Prototypical Networks for Few-Shot Learning on Omniglot", "tag": ["cs.LG", "cs.CV", "cs.NE", "stat.ML"], "abstract": "We propose a novel architecture for $k$-shot classification on the Omniglot dataset. Building on prototypical networks, we extend their architecture to what we call Gaussian prototypical networks. Prototypical networks learn a map between images and embedding vectors, and use their clustering for classification. In our model, a part of the encoder output is interpreted as a confidence region estimate about the embedding point, and expressed as a Gaussian covariance matrix. Our network then constructs a direction and class dependent distance metric on the embedding space, using uncertainties of individual data points as weights. We show that Gaussian prototypical networks are a preferred architecture over vanilla prototypical networks with an equivalent number of parameters. We report state-of-the-art performance in 1-shot and 5-shot classification both in 5-way and 20-way regime (for 5-shot 5-way, we are comparable to previous state-of-the-art) on the Omniglot dataset. We explore artificially down-sampling a fraction of images in the training set, which improves our performance even further. We therefore hypothesize that Gaussian prototypical networks might perform better in less homogeneous, noisier datasets, which are commonplace in real world applications.", "text": "propose novel architecture k-shot classiﬁcation omniglot dataset. building prototypical networks extend architecture call gaussian prototypical networks. prototypical networks learn images embedding vectors clustering classiﬁcation. model part encoder output interpreted conﬁdence region estimate embedding point expressed gaussian covariance matrix. network constructs direction class dependent distance metric embedding space using uncertainties individual data points weights. show gaussian prototypical networks preferred architecture vanilla prototypical networks equivalent number parameters. report state-ofthe-art performance -shot -shot classiﬁcation -way -way regime omniglot dataset. explore artiﬁcially down-sampling fraction images training improves performance even further. therefore hypothesize gaussian prototypical networks might perform better less homogeneous noisier datasets commonplace real world applications. humans able learn recognize object categories single small number examples. demonstrated wide range activities hand-written character recognition motor control acquisition high level concepts replicating kind behavior machines motivation studying few-shot learning. parametric deep learning performing well settings abundance data. general deep learning models high functional expressivity capacity rely slowly iteratively trained supervised regime. inﬂuence particular example within training therefore small training designed capture general structure dataset. prevents rapid introduction classes training. contrast few-shot learning requires fast adaptation data. particular k-shot classiﬁcation refers regime classes unseen training must learned using labeled examples. non-parametric models k-nearest-neighbors overﬁt however performance strongly depends choice distance metric. architectures combining parametric non-parametric models well matching training test conditions performing well k-shot classiﬁcation recently. vectors clustering classiﬁcation. divide batch support query images embedding vectors support deﬁne class prototype typical embedding vector given class. proximity used classiﬁcation. model call gaussian prototypical network maps image embedding vector estimate image quality. together embedding vector conﬁdence region around predicted characterized gaussian covariance matrix. gaussian prototypical networks learn construct direction class dependent distance metric embedding space. show model preferred using additional trainable parameters compared vanilla prototypical networks. goal show allowing model express conﬁdence individual data points reach better results. also experiment intentionally corrupting parts dataset order explore extendability method noisy inhomogeneous real world datasets weighting individual data points might crucial performance. report knowledge state-of-the-art performance -shot -shot classiﬁcation -way -way regime omniglot dataset. studying response model down-sampled data hypothesize advantage might even higher lower quality inhomogeneous datasets. paper structured follows describe related work section proceed introduce methods section episodic training scheme also presented there. discuss omniglot dataset section experiments section finally conclusions presented section non-parametric models k-nearest neighbors ideal candidates few-shot classiﬁers allow incorporation previously unseen classes. however sensitive choice distance metric. using distance space inputs directly produce high accuracies connection image class pixels non-linear. straightforward modiﬁcation metric embedding learned used classiﬁcation yielded good results demonstrated approach using matching networks proposed effect learning distance metric pairs images. noteworthy feature method training scheme mini-batch tries mimic data-poor test conditions sub-sampling number classes well numbers examples each. demonstrated approach improves performance few-shot classiﬁcation. therefore well. instead learning dataset directly recently proposed train lstm predict updates few-shot classiﬁer given episode input. approach referred meta-learning. meta-learning reaching high accuracies omniglot demonstrated task-agnostic meta-learner based temporal convolutions proposed combinations parametric non-parametric methods successful few-shot learning recently. approach speciﬁc classiﬁcation images attempt solve problem meta-learning. build model presented maps images embedding vectors uses clustering classiﬁcation. novel feature model predicts conﬁdence individual data points learned image-dependent covariance matrix. allows construct richer embedding space images projected. clustering direction class-dependent distance metric used classiﬁcation. paper ﬁrst explore prototypical networks described extend architecture call gaussian prototypical network allowing model reﬂect quality individual data points predicting embedding vectors well conﬁdence regions around them characterized gaussian covariance matrix. vanilla prototypical network comprises encoder maps image embedding vector. batch contains subset available training classes. iteration images class randomly split support query images. embeddings support images used deﬁne class prototypes embedding vectors typical class. proximity query image embeddings class prototypes used classiﬁcation. encoder architectures vanilla gaussian prototypical networks differ. difference encoder outputs interpreted used metric embedding space constructed. gaussian networks part encoder output used construct covariance matrices embedding vectors allows model reﬂect predictive power quality individual data points. multi-layer convolutional neural network without explicit ﬁnal fully connected layer encode images high-dimensional euclidean vectors. vanilla prototypical network described encoder function taking image transforming vector height width input image number channels. embedding dimension vector space hyperparameter model. trainable weights encoder. gaussian prototypical network output encoder concatenation embedding vector relevant components covariance matrix rd×d. therefore radius covariance estimate. single real number sraw generated image characterize size conﬁdence interval around embedding vector. covariance matrix form diag calculated encoder output sraw. conﬁdence estimate therefore directionsensitive. method proved efﬁcient usage additional parameters omniglot dataset suspect preference might dataset-speciﬁc less homogeneous datasets could prefer complex covariance estimates. diagonal covariance estimate. dimension covariance estimate embedding space. sraw generated image characterize size conﬁdence interval around embedding vector. therefore covariance matrix form diag calculated encoder output sraw. allows network express direction-dependent conﬁdence data point although conﬁdence ellipsoid always remains axis-aligned embedding space axes. full covariance estimate. full covariance matrix output data point. method proved needlessly complex tasks given therefore explored further. used down-sampled gray-scale omniglot images dimension input. -layer architecture pooling results volume shape embedding dimension plus relevant parts covariance matrix equal number ﬁlters last later. using tensorflow padding stride ﬁlters spatial extent. ﬁnal layer equivalent fully-connected layer. using encoder architectures small architecture architecture. small architecture corresponded used used validate experiments respect previous state-of-the-art results. architecture used effect increased model capacity accuracy. basic building block used sequence layers equation explored different methods translating covariance matrix output encoder actual covariance matrix. since primarily deal inverse covariance matrix predicting directly. relevant part encoder output sraw. methods follows softplus softplus applied componentwise. since softplus guarantees encoder make data points less important. value also limited above. approaches prove beneﬁcial training. best models used regime initial training. sigmoid sigmoid applied componentwise. since sigmoid guarantees encoder make data points less important. value bounded above encoder therefore constrained. sigmoid therefore used explore effect oﬀset scale softplus oﬀset scale initialized trainable. best models used regime late-stage training ﬂexible data-driven component prototypical model episodic training regime described training subset classes chosen total number classes training classes support examples chosen random well query examples. encoded embeddings support examples used deﬁne particular class prototype lies embedding space. distances query examples positions class prototypes used classify query examples calculate loss. gaussian prototypical network covariance embedding point estimated well. diagram process shown figure gaussian prototypical network radius diagonal covariance matrix output together embedding vector used weight embedding vectors corresponding support points particular class well calculate total covariance matrix class. distance class prototype query point calculated pc)t centroid prototype class inverse covariance matrix. gaussian prototypical network therefore able learn class direction-dependent distance metric embedding space. found speed training accuracy depend strongly distances used construct loss. conclude best option work linear euclidean distances i.e. speciﬁc form loss function used presented algorithm diagram embedding space gaussian prototypical network shown figure sample embedding space training shown appendix figures illustrates clustering similar characters used classiﬁcation. study settings covariance matrix diagonal summarized section radius case identity matrix calculated encoder output image. diagonal case diag similarly calculated encoder output image. figure diagram function gaussian prototypical network. encoder maps image vector embedding space covariance matrix also output image support images used deﬁne prototypes covariance matrices particular class. distances centroids encoded query images modiﬁed total covariance class used classify query images. distances shown dashed gray lines particular query point. figure diagram showing embedding space gaussian prototypical network. image mapped embedding vector encoder. covariance matrix also output encoder. overall covariance matrix class computed well prototypes classes covariance matrix class used locally modify distance metric query points critical part prototypical network creation class prototype available support points particular class. propose variance-weighted linear combination embedding vectors individual support examples solution. class support images encoded embedding vectors prototype i.e. centroid class deﬁned corresponds optimal combination gaussians centered individual points overall class gaussian hence name network. elements effectively equations therefore correspond weighting examples allows network down-weight examples less important deﬁning class therefore makes architecture suitable noisy inhomogeneous otherwise imperfect datasets. one-shot regime networks trained single labeled vector deﬁning class. means vector becomes class prototype covariance matrix also inherited class. covariance comes play modifying distances query points. full algorithm described algorithm estimate accuracy model test classify whole test every number support points range number query points particular therefore omniglot provides examples class. accuracies aggregated particular stage model training k-shot classiﬁcation accuracy function determined. since using designated validation ensure impartiality considering test results highest training accuracies calculate mean standard deviation. that prevent optimizing result test furthermore obtain error bounds resulting accuracies. evaluate models -way -way test classiﬁcation directly compare existing literature. used omniglot dataset. omniglot contains character classes alphabets hand-written gray-scale pixel examples each. down-sampled subtracted mean inverted them. using recommended split training alphabets test alphabets suggested used training included overall unique character classes test them. class figure example augmentation class count rotations. original character rotated rotation deﬁned class. enhances number classes also introduces degeneracies symmetric characters. overlap training test datasets. separate validation ﬁne-tune hyperparameters chose best performing model based training accuracies alone extend number classes augmented dataset rotating character deﬁned rotation character class own. approach used example augmented character shown figure increased number classes -fold. total training therefore included images test images. rotational augmentation characters rotational symmetry nonetheless deﬁned multiple classes. even hypothetical perfect classiﬁer would able differentiate e.g. character rotated accuracy reachable. improve training utilization ability predict covariances characters gaussian network purposefully down-sampled part training experiments. details provided section results suggest omniglot dataset simple fully utilize ability gaussian network estimate covariance matrices. hypothesize full strength method would show inhomogeneous datasets varying quality individual data points commonly case real world applications. conducted large number few-shot learning experiments omniglot dataset. gaussian prototypical networks explored different embedding space dimensionalities ways generating covariance matrix encoder capacities also compared vanilla prototypical networks showed gaussian variant favorable particular efﬁcient using additional parameters predict single number embedding point general explored size encoder gaussian/vanilla prototypical network comparison distance metric number degrees freedom covariance matrix gaussian networks dimensionality embedding space. also explored augmenting input dataset down-sampling subset encourage usage covariance estimates network found improves -shot performance. using adam optimizer initial learning rate halved learning rate every episodes epochs. models implemented tensorflow single nvidia google cloud. training time model less day. trained models classes training time tested classes classiﬁcation. best-performing models also conducted ﬁnal classiﬁcation test compare results literature. training class present mini-batch comprised support points found limiting number support points leads better classiﬁcation accuracies. could intuitively understood matching training regime test regime. remaining images class used query points. detailed results experiments summarized table explored ways estimating covariance matrix covariance output encoder detailed section also veriﬁed provided covariance estimate needlessly complex using encoder outputs covariance estimates advantageous using number parameters additional embedding dimension. holds true radius estimate making precise covariance estimate radius estimate outperforms diagonal estimate vanilla prototypical network number parameters. table test results encoder architecture comparing effect dimensionality covariance matrix well embedding space ﬁnal accuracy. relates different methods converting encoder output covariance matrix. radius estimate covariance adds dimension encoder output. diagonal estimate doubles number encoder outputs. gauss embedding dimension diagonal covariance therefore number parameters vanilla network radius estimate adds dimension therefore comparable vanilla model embedding dimensionality. damage column signiﬁes training purposefully partially down-sampled training. embedding vector) however diagonal estimate seem help performance effect shown figure table best performing model initially trained undamaged dataset epochs. training continued images down-sampled down-sampled down-sampled epochs. down-sampled down-sampled epochs down-sampled epochs. choices quite arbitrary optimized over. purposeful damage dataset encouraged usage covariance estimate increased -shot results shown table figure partially shows omniglot dataset high quality simple testbed approach. training loss curves shown figure training test accuracies functions iteration also shown figure figure effect down-sampling part training k-shot test accuracy. version trained purposefully damaged data outperforms trained unmodiﬁed data learned utilize covariance estimates better. figure loss function iteration. yellow vertical lines show learning rate halved. beneﬁcial effect learning rate halving visible beginning. segment corresponds training partially down-sampled training therefore higher loss. conducted veriﬁcation experiments small architecture reached comparable results summarized table table also shows training regime i.e. data points deﬁning class leads worse performance. effect higher capacity model shown figure comparison models results literature presented table knowledge models outperform state-of-the-art -shot -shot results -way -way test-time classiﬁcation omniglot. -way -shot classiﬁcation particular reaching close perfect performance therefore conclude complex dataset needed few-shot learning algorithms development. figure training accuracy compared test accuracy. plot shows training accuracy large gaussian prototypical network compares -shot -shot test performance also compares results current state-of-the-art. order validate assumption gaussian prototypical network outperforms vanilla version ability predict covariances individual embedded images therefore possibility down-weight them studied distribution predicted values best performing network trained partially down-sampled training set. purposefully down-sampled part data studied resulting distribution covariances. down-sampling image changes mean variance. encoders built batch normalization layer block meaning particular value output changes based current batch. since model trained batch normalization turning study covariances would lead irrelevant results. table results veriﬁcation experiments small architectures. state -way classiﬁcation -shot -shot number support points class training. training done regime. gaussian prototypical model shows dimensionality estimated covariance matrix. table best results experiments compared papers. training done regime. knowledge models statistically signiﬁcant state-of-the-art performance -shot -shot -way classiﬁcation well -shot -way classiﬁcation. perform comparably current state-of-the-art -shot -way case. undamaged dataset vast majority covariance estimates took value. stays true even artiﬁcially introducing damage down-sampling. however distributions shifted effect batch normalization last layer. better represent meaning individual inverse covariances aligned histograms frequent values match other. approach useful dominant values correspond output differences inﬂuence classiﬁcation. result shown figure figure predicted covariances original dataset partially down-sampled version gaussian network learned down-weight damaged examples predicting higher apparent heavier tail yellow distribution. distributions aligned together difference leading edge value inﬂuence classiﬁcation. paper proposed gaussian prototypical networks few-shot classiﬁcation improved architecture based prototypical networks tested models omniglot dataset explored different approaches generating covariance matrix estimate together embedding vector. showed gaussian prototypical network outperforms vanilla prototypical network comparable number parameters therefore architecture choice beneﬁcial. found estimating single real number embedding vector works better estimating diagonal full covariance matrix. suspect lower quality less homogeneous datasets might prefer complex covariance matrix estimate. contrary found best results obtained trains network -shot regime. extended size model managed reach best knowledge stateof-the-art performance -shot -shot classiﬁcation -way -way test regime managed better accuracies -shot classiﬁcation) artiﬁcially down-sampling fractions training dataset encouraging network fully utilize covariance estimates. especially -way classiﬁcation results close perfect performance therefore conclude development few-shot classiﬁcation focus complex datasets omniglot. hypothesize ability learn embedding well uncertainty would even beneﬁcial poorer-quality datasets commonplace real world applications. there down-weighting data points might crucial faithful classiﬁcation. supported experiments down-sampling omniglot. would like thank poole yihui quek useful discussions brainstorming. part work done class project stanford university convolutional neural networks visual recognition provided google cloud credit coupons partially supported usage.", "year": 2017}