{"title": "Attention Tree: Learning Hierarchies of Visual Features for Large-Scale  Image Recognition", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "One of the key challenges in machine learning is to design a computationally efficient multi-class classifier while maintaining the output accuracy and performance. In this paper, we present a tree-based classifier: Attention Tree (ATree) for large-scale image classification that uses recursive Adaboost training to construct a visual attention hierarchy. The proposed attention model is inspired from the biological 'selective tuning mechanism for cortical visual processing'. We exploit the inherent feature similarity across images in datasets to identify the input variability and use recursive optimization procedure, to determine data partitioning at each node, thereby, learning the attention hierarchy. A set of binary classifiers is organized on top of the learnt hierarchy to minimize the overall test-time complexity. The attention model maximizes the margins for the binary classifiers for optimal decision boundary modelling, leading to better performance at minimal complexity. The proposed framework has been evaluated on both Caltech-256 and SUN datasets and achieves accuracy improvement over state-of-the-art tree-based methods at significantly lower computational cost.", "text": "large-scale classiﬁcation computationally inefﬁcient complexity increases linearly number categories. classiﬁers modeled mimic brain-like cognitive abilities lack remarkable energy-efﬁcient processing capability brain. brain carries enormously diverse complex information processing deal constantly varying world power budget seeking attain brain’s efﬁciency draw inspiration underlying processing mechanisms design multi-class classiﬁcation method accurate computationally efﬁcient. mechanism known saliency based selective attention shown fig. simpliﬁes complex visual tasks characteristic features selectively activates particular areas brain based feature information input presented visual images brain associates already learnt features visual appearance object types perform recognition facilitates brain learn host information limited capacity also speeds recognition process. interestingly note signiﬁcant similarity among underlying characteristic features images across multiple objects real world applications. presents opportunity build efﬁcient visual recognition system incorporating inter-class feature similarities relationships. work propose computationally efﬁcient multiclass classiﬁcation method attention tree exploits feature similarity among multiple classes dataset tree structure composed binary classiﬁers. resultant atree learns hierarchy features speciﬁc deeper tree top-down manner. similar state-of-the-art deep learning convolutional networks convolutional layers exhibit genericto-speciﬁc transition learnt features case dlns entire network utilized recognition particular test input. contrast construction attention tree incorporates effective active pruning dataset training individual tree nodes resulting efﬁcient instance-speciﬁc classiﬁcation path. addition later sections attention model captures inter intra class feature similarity build tree hierarchy decision paths varying lengths even class. provides substantial beneﬁts test speed computational efﬁciency large-scale problems maintaining competitive classiﬁcation accuracy. abstract—one challenges machine learning design computationally efﬁcient multi-class classiﬁer maintaining output accuracy performance. paper present tree-based classiﬁer attention tree large-scale image classiﬁcation uses recursive adaboost training construct visual attention hierarchy. proposed attention model inspired biological selective tuning mechanism cortical visual processing. exploit inherent feature similarity across images datasets identify input variability recursive optimization procedure determine data partitioning node thereby learning attention hierarchy. binary classiﬁers organized learnt hierarchy minimize overall test-time complexity. attention model maximizes margins binary classiﬁers optimal decision boundary modelling leading better performance minimal complexity. proposed framework evaluated caltech- datasets achieves accuracy improvement state-ofthe-art tree-based methods signiﬁcantly lower computational cost. attention mechanisms critical component brain’s cognitive performance. mechanisms enable brain process overwhelming visual stimuli limited capacity selectively enhancing information relevant one’s current behaviour massive growth digital image data social media surveillance camera among others growing demand computing platforms perform cognitive tasks. computing platforms limited resources terms processing power battery life. hence researchers strongly motivated design efﬁcient large-scale image recognition methods enable resource constrained devices cognitive intelligence several brain-inspired computing models including support vector machines random forest adaboost proven successful image recognition. however classiﬁers scale well increasing number image categories. deep learning networks like convnets achieved state-of-the-art accuracies even surpassing human performance imagenet dataset. however criticized enormous training cost computational complexity. similarly oneversus-all linear popular classiﬁers propose different ways construct hierarchical classiﬁcation tree. however methods rely greedy prediction algorithm class prediction single path tree. algorithms achieve sublinear complexity accuracy typically sacriﬁced errors made higher nodes hierarchy cannot corrected later. researchers also looked developing efﬁcient effective feature representations large-scale classiﬁcation problems learn discriminative features using deep convolutional networks achieve stateof-the-art accuracy. please note proposed atree orthogonal models since method various feature respresentations explore accuracy efﬁciency tradeoff. hence optimize different features work rather compare efﬁciency beneﬁts approach existing hierarchical methods. proposed atree model draws inspiration tree-based methods different focus design evaluation strategies. mentioned methods greedy prediction algorithm achieve good tradeoff efﬁciency complexity. novelty work recursive adaboost training uniﬁed principled optimization procedure determine data partitioning based feature similarity. turn enables binary construct maximum-margin hyperplane optimal decision boundary modeling leading better performance. addition organizing binary classiﬁers hierarchical tree structure attention hierarchy reduces complexity. variant boosted tree algorithm combines adaboost based decision tree construct atree. proposed attention based classiﬁcation framework resolves problems associated standard decision tree methods discussed above. simple twoclass problem training stage atree consists phases first construct visual feature hierarchy atree using adaboost training algorithm recursively wherein tree node complex classiﬁer works optimal feature tree level partitioning inputs. partitioned input data obtained particular node used train left right sub-trees. thus training data subsequent nodes tree continuously pruned construction atree leading computationally efﬁcient training. recursive boosting procedure intrinsically embeds clustering learning stage similar feature clusters created automatic hierarchical fashion. feature hierarchy resultant pruned data space ﬁxed node/branch tree ﬁrst phase train standard binary right left partitioned subsets input data tree node. selective attention mechanism observed frontal fig. parietal cortex involved generation control salient attentional signals example attention tree hierarchies formed basis different semantic categories inter-class relationships object types real-world broad semantic categories different object classes. example recognise sensible learn speciﬁc appearance details. instead ﬁrst learn general vehicle-type features learn discriminative details thus learn hierarchy features generalizing object instances like wheeled vehicle→motor vehicles→cars→bmw. presented motorbike object attention hierarchy associates category objects already learnt wheeled vehicle features learns discriminative details corresponding motorbike types. node atree associated different features based inter-class relationships. evident fig. attention tree method bears resemblance selective attention mechanism brain exploiting feature similarity implicit relationships among different visual data learn meaningful hierarchy recognition. decision tree ensemble methods class boosting techniques proposed lowering testing complexity machine-learning problems suffer major limitations ensemble learning weak learners combined complex classiﬁer high accuracy. number weak classiﬁers order hundreds reasonable performance large-scale problems. thus ensemble methods become computationally expensive larger datasets. existing models deviate biological attention based visual processing human brain perform one-againstrest classiﬁcation. case learning algorithm fails maintain general-to-speciﬁc feature hierarchy turns ineffective well computationally inefﬁcient. class work one-versus-all one-versus methods explored convert multi-class problem multiple binary classiﬁcation problems. models classes organized hierarchical tree. also methods incorporate class relationships feature similarities. extension methods include error correcting output codes utilize feature sharing build generalized robust classiﬁers. discussed earlier methods yield good classiﬁcation accuracy. however strong classiﬁers instead constructing single strong classiﬁer linear combination weak learners. tree utilizes features learnt previous nodes construct subsequent nodes. traverse tree classiﬁers learn speciﬁc features useful classifying hard inputs correctly preserving feature information learnt early nodes easy input samples. attention tree learning implementation central idea attention tree algorithm feature-based attention optimize search procedure inherent vision problem. model attention addresses reduction number candidate image subsets feature subsets required object recognition selectively tuning visual processing hierarchy. theory described closely related neuroscience works present neurobiological concepts primate visual attention. algorithm shows procedure training attention tree two-class problem. phase tree recursively trained. learns preserves hierarchy features essential understanding underlying image representations efﬁcient classiﬁcation. node classiﬁer learnt using adaboost algorithm described identiﬁes optimum feature separate training inputs particular node corresponding sub-branches. shown adaboost essentially approximating logistic regression. convenience notation denote output computed classiﬁer tree node depending upon probabilities computed classiﬁer node training divided dlef dright passed sub-branches training following nodes tree. tree expands subset input samples passed subsequent nodes. thus ﬁnal nodes leaves tree consist input samples belonging particular class. please refer fig. overview tree structure input sub-sampling obtained attention model. later section give detailed explanation input sub-sampling hierarchical feature learning achieved attention model. phase algorithm binary trained node tree using dright dlef training sub-samples obtained phase training labels assigned input corresponding subsets training binary svm. training size decreases successive nodes traverse measure select feature used categorize multiple category objects broad classes. then training algorithm simple two-class atree used design attention hierarchy. again clusters multiple classes automatically formed. test time branches nodes depending upon output binary classiﬁer activated relevant input. hence approach time energy efﬁcient since involves instance-speciﬁc selective activation nodes. next brieﬂy discuss adaboost learning framework shortcomings associated explain intuition behind modifying standard adaboost training procedure effectively used construct feature-based hierarchy atree. adaboost algorithm combines simple weak classiﬁers form ﬁnal classiﬁer output ﬁnal strong classiﬁer sign). weak classiﬁers thought feature basis vectors. given training samples adaboost maintains probability distribution samples. then adaboost calls weaklearn algorithm trains weak learner classiﬁer weighted sample series iterations ..t}. distribution updated iteration minimize overall error finally adaboost uses weighted linear combination weak learners obtain ﬁnal output adaboost weaklearn algorithm explained detail eqn. shows maximum value error. large-scale problems tends complex saturates iterations thus adaboost algorithm fails reach global error minima. possible solution avoiding design better weak classiﬁers effectively separate classes. however would increase computational complexity computing classiﬁers easy samples correctly classiﬁed weak classiﬁers weights misclassiﬁed higher weights. weight distribution captures information selected features given iteration. however weight update rule normalization iteration information previously selected features might lost. result misclassiﬁcation correctly classiﬁed samples earlier iterations present epoch. thus algorithm maintain generic-to-speciﬁc transition learning weak classiﬁers proves ineffective iterations. address this build attention tree tree complexity problem hence also reduces. turn enables better decision boundary modeling computational complexity subsequent nodes improved classiﬁcation performance. adding svms nodes learnt feature hierarchy enables attention tree model achieve state-of-the-art accuracies challenging benchmark databases signiﬁcantly lower cost. negative subsets. training samples passed branches tree node. weights sub-trees re-computed based node classiﬁer’s output. case tree based adaboost training converges standard boosting algorithm wherein feature hierarchy learnt. experiments discussed section value easy inputs correctly classiﬁed general features nodes unnecessarily passed bottom nodes classiﬁcation. result computational inefﬁciency defeating purpose attention tree. then training sample either passed right left sub-tree leads constrained partition. case hard confusing classes assigned sub-trees causing overﬁtting data subsequent nodes. lead decline accuracy. however test complexity since length tree short leading quicker decision cost degraded performance. samples whose output probability lies range considered hard confusing ones. hard samples passed left right sub-trees training hard confusing inputs/classes ignored training corresponding node phase adopted relaxed hierarchy structure done enhance accuracy attention tree. understood decision boundary becomes progressively non-linear model hard confusing classes dataset traverse atree. hard confusing instances ignored passed bottom nodes construct better decision boundary models thereby decreasing overall error. case hard classes passed bottom nodes svms construct overﬁtted models complex data instances thereby decreasing accuracy considerably. section vary threshold build constrained relaxed hierarchical attention models analyze tradeoff computational efﬁciency accuracy approaches. conserve feature transition attention model propose simple method extending two-class training model multi-class one. traditionally boosting algorithms multi-class weak learners construct multiclass ﬁnal strong classiﬁer however large number classes constructing reasonably accurate multiclass weak learners turns highly computationally expensive. seen earlier observe feature similarity across classes used decompose multi-class problem hierarchy two-class problems. algorithm shows procedure training multi-class attention tree. algorithm ﬁrst ﬁnds optimum feature across multiple classes separates input patterns classes uses -class training procedure learn subsequent classiﬁer nodes tree. experiments observed feature chosen transforming multi-class -class problem often feature selected algorithm construct node atree. intuitively ﬁrst selection features selected subsequent nodes help making stronger accurate decision.thus similar objects different classes clustered together initial nodes hierarchy. tree expands classes gradually apart. tree terminated algorithm common feature partition inputs thus leaf tree corresponds particular class. attention hierarchy learned phase algorithm invoked train svms node hierarchy. attention tree composed nodes used testing. instances easily distinguished general features identiﬁed svms nodes. svms bottom nodes perform accurate classiﬁcation hard instances dataset. input instance presented root node branch higher output probability node activated. based path activated output nodes instance traverses attention hierarchy leaf node ﬁnal decision made. note subset classes eliminated tree node tree traversed. attention based hierarchy thus scales sublinearly respect number classes. current data deluge presents vision problems hefty task recognizing hundreds thousands classes sub-linearly growing attention tree model useful. fig. attention tree learning hierarchy formed synthesized dataset points. features weak classiﬁers position distance speciﬁc lines fig. shows example attention tree learns divides samples synthesized dataset points. dataset consists inputs belonging classes samples clustered together termed hard inputs. samples passed sub-branches tree forming successive nodes. node tree partitions inputs subsets. division intuitive right orange points distant remaining inputs clustered together. tree expands hard inputs sets clustered together. data points assumed features corresponding image classes clearly seen hierarchy formed coherent basic generic-to-speciﬁc feature transition theory attention model. consider example recognizing ferrari sample vehicle images consisting motorbikes cars ﬁrst intuitive step recognize vehicles sample look ferrari shaped object sub-sample remaining images. attention tree tries model intuitive behavior learning feature hierarchy. ﬁrst level tree uses feature distinguish non-red vehicles vehicles. down tree uses speciﬁc features perform accurate classiﬁcation. attention model automatically learns feature hierarchy without need pre-specify feature clusters. pruning input data traverse attention model reduces complexity original multi-class problem. turn enables better decision boundary modelling bottom nodes atree compared node resulting improved classiﬁcation performance. noteworthy observation attention model comprises multiple decision paths different lengths. fig. tree consists leaf nodes every level. given input decision reached earlier leaf node yielding optimal speedup testing. referring ferrari example non-red since learnt tree nodes used measuring complexity accuracy attention model kernel-type selection plays role determining overall computational efﬁciency/performance given multi-class problem. case svms linear kernels complexity classiﬁer node same overall test complexity proportional number classiﬁers evaluated reach decision. however case non-linear kernels complexity classiﬁer proportional number support vectors. computational model devised optimize number support vectors maximizing computational beneﬁts. clear training algorithm multi-class attention tree always result balanced partitioning classes particular node observed fig. given classiﬁer number support vectors classiﬁer deﬁne cost function reﬂects average efﬁciency |z−| number classes assigned positive negative labels respectively. fraction negative classes similarly fraction positive classes. attention hierarchy learned estimate |z−|. ideal case instances class |z−| classes pruned evaluating cost proportional number kernel evaluations. average cost discarding particular class |z+|. similarly average cost eliminating class instances belonging positive subset |z−|. given proportion positive negative classes average cost eliminating class given eqn. thus select number support vectors minimizes overall cost function yielding competitive accuracy. evaluation metrics classiﬁcation accuracy test speed discuss beneﬁts approach. classiﬁcation accuracy mean per-class accuracy reported standard estimating multi-class classiﬁcation performance. test speed distinguish cases based kernel type selection classiﬁers atree nodes. ﬁrst case corresponds linear classiﬁers overall fig. sample illustration atree relaxed constrained hierarchy images classiﬁed ﬁrst level without traversing whole tree. imbalanced decision tree structure separates model decision tree methods traverse entire tree reach decision even within particular class inputs equal. example recognizing person standing plain background much easier he/she midst crowd. ideally algorithms spend effort proportional difﬁculty inputs irrespective whether belong class existing works focus optimizing computational complexity based inter-class feature variability. contrast imbalanced method captures inter intra class feature variability expanding attention tree thus yielding computational beneﬁts. previously discussed threshold serves useful control parameter construct either relaxed constrained models attention tree. fig. demonstrates sample relaxed/constrained hierarchy -class problem. instances class hard inputs dataset. constrained hierarchy clearly seen instances forced left sub-node. case likely root node misclassify test instance class overﬁtting. however decision path recognizing class short. observe improvement efﬁciency cost accuracy. relaxed hierarchy extra classiﬁer evaluation required recognize class increases computational cost. however accuracy case better addition extra classiﬁer node minimizes overﬁtting complex distribution data. addition relaxed hierarchy captures intra-class feature variability class seen constrained model. relaxed model instances class relatively easy classiﬁed level hard passed level accurate classiﬁcation. contrast constrained model instances class passed level classiﬁcation. sample demonstration clear modulated control accuracy efﬁciency atree. test complexity proportional number evaluated classiﬁers. linear kernel report mean number classiﬁer evaluations test instances. second case corresponds nonlinear kernel svms. mentioned earlier complexity classiﬁer proportional number support vectors. speciﬁcally number classiﬁers evaluated classiﬁer support vectors denote classiﬁer support vectors respectively. classiﬁers evaluated independently without caching kernel computations then number kernel computations method proves inefﬁcient number classiﬁers large. efﬁcient approach would caching kernel computations different classiﬁers reusing whenever possible. then number kernel computations reduces latter approach report test speed non-linear kernels used. compare method various existing approaches one-vs-all one-vs-one dagsvm tree-based hierarchy marszalek regularization parameter chosen cross validation training set. categories least images class standard muti-class object recognition dataset. randomly sampled images class used half training remaining half testing. features used standard spatial histograms visual words based dense sift like used extended gaussian kernel based distance. however since linear kernel histogram based features gives poor accuracy used explicit feature transformation approximate implicit feature mapping kernel. linear applied transformed feature. varied computational parameters tree marszalek method atree obtain tradeoff accuracy speed. here computational parameters deﬁned respectively varied achieve complexity accuracy tradeoff. clearly seen atree performs better linear nonlinear kernels. instance case linear kernel atree achieves best accuracy around complexity one-vs-all relaxed hierarchical model achieving speedup also atree modelled constrained hierarchy achieves higher speed accuracy degradation respect one-vs-all. however achieve similar speed methods marszalek tree suffer accuracy degradation. please note atree achieves consistently better accuracy performance best result reported linear non-linear kernels. evaluate atree model scene classiﬁcation dataset. dataset captures full variety scene categories. used well-sampled categories class images used training test. image representation used spatial pyramid histogram intersection kernel transformed spatial histogram oriented gradient pyramid approximate implicit feature mapping histogram intersection kernel) linear kernel caltech- varied tradeoff accuracy speed tree marszalek method atree fig. shows results. performance improvement linear/non-linear kernels similar consistent results caltech-. instance histogram intersection kernel method signiﬁcantly improved accuracy complexity compared onevs-all however marszalek reduce relative complexity respectively attain similar accuracy onevs-all. performance atree improves increased highest accuracy observed higher best result reported test speed method achieves maximum speed compared one-vs-all even improved accuracy methods never meet speedup irrespective accuracy. addition linear kernel atree achieves slightly improved accuracy respect one-vs-all dagsvm faster. fact decline accuracy atree faster one-vs-all. however /marszalek accuracy degradation higher upto .%/.% achieve similar speed results validate atree effective reduce maintaining competitive accuracy comparison hierarchical tree-based implementations. fig. worth noting non-linear kernels used lower depth tree necessarily lead lower computational complexity. large closer depth tree account constrained partitioning inputs left right sub-trees. ideally accuracy decline lower complexity cases number classiﬁer evaluations less. however observe accuracy complexity worse. reason that although fewer number classiﬁer evaluations required cases involves large number support vectors increases overall complexity. fig. comparison tradeoff accuracy relative complexity caltech computational complexity normalized complexity one-vs-all. note one-vs-one relative complexity linear kernel fig. comparison tradeoff accuracy relative complexity sun. computational complexity normalized complexity one-vs-all. note one-vs-one relative complexity linear kernel classes dataset case learn model spatial using linear kernel. fair comparison method match level accuracy one-vs-all. seen fig. complexity method grows sublinearly compared discussed earlier atree model gives rise imbalanced tree leaf nodes even beginning attention hierarchy. thus observe method grows slightly lesser rate atree builds feature hierarchy label space automatically. fig. shows attention tree formed subset sampled images caltech- dataset. observe images similar features clustered together initial node gradually apart tree traversed. conforming imbalanced attention model observe certain classes zebra tire classiﬁcation done earlier nodes confusing classes passed down. addition also observe intrafig. present sampling ﬁrst three levels atree constructed entire caltech- dataset showing different classes assigned separated left right subtrees. caltech- atree hierarchy observe assignment classes sub-nodes many cases correlates human vision i.e. images different classes tion program sponsored marco darpa semiconductor research corporation national science foundation intel corporation national security science engineering faculty fellowship. fig. atree formed sub-sample selected images caltech- assigned sub-tree look similar humans. atree hierarchy partitioning classes ﬁrst levels correlates human-deﬁned concepts. e.g. natural outdoor scenes indoor man-made scenes. also hierarchy starts partitioning classes large visual distances identiﬁes subtle discrepancies bottom nodes coherence concepts visual stimuli decomposition human brain. suggests biological plausibility effectiveness attention model image classiﬁcation. proposed novel neuro-inspired visual feature learning construct efﬁcient accurate tree-based classiﬁer attention tree large-scale image classiﬁcation. learning algorithm based biological attention mechanism observed brain selects speciﬁc features greater neural representations. atree uses principled optimization procedure extract knowledge relationships object types integrates visual appearance learning. evaluated method caltech- datasets obtained signiﬁcant improvement accuracy efﬁciency. fact atree outperforms one-vs-all method accuracy yields lower computational complexity compared state-of-the-art tree-basedmethods proposed framework intrinsically embeds clustering learning procedure identiﬁes inter intra class variability. importantly proposed atree learns hierarchy systematic less greedy grows sublinearly number classes hence proves effective large-scale classiﬁcation problems. noteworthy mention current atree framework suffers overﬁtting training dataset small. overﬁtting behaviour checked modulating depth atree also adopting relaxed hierarchy structure confusing hard inputs passed right left sub-nodes. additionally tree pruning methods used control overﬁtting research done explore overﬁtting problem. geebelen suykens vandewalle reducing number support vectors classiﬁers using smoothed separable case approximation ieee transactions neural networks learning systems vol. zhang delving deep rectiﬁers surpassing human-level performance imagenet classiﬁcation proceedings ieee international conference computer vision wang shen barnes zheng fast robust object detection using asymmetric totally corrective boosting ieee transactions neural networks learning systems vol. paisitkriangkrai shen hengel scalable stagewise approach large-margin multiclass loss-based boosting ieee transactions neural networks learning systems vol. probabilistic boosting-tree learning discriminative models classiﬁcation recognition clustering tenth ieee international conference computer vision volume vol. ieee panda sengupta conditional deep learning energy-efﬁcient enhanced pattern recognition design automation test europe conference exhibition ieee", "year": 2016}