{"title": "Multivariate Information Bottleneck", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "The Information bottleneck method is an unsupervised non-parametric data organization technique. Given a joint distribution P(A,B), this method constructs a new variable T that extracts partitions, or clusters, over the values of A that are informative about B. The information bottleneck has already been applied to document classification, gene expression, neural code, and spectral analysis. In this paper, we introduce a general principled framework for multivariate extensions of the information bottleneck method. This allows us to consider multiple systems of data partitions that are inter-related. Our approach utilizes Bayesian networks for specifying the systems of clusters and what information each captures. We show that this construction provides insight about bottleneck variations and enables us to characterize solutions of these variations. We also present a general framework for iterative algorithms for constructing solutions, and apply it to several examples.", "text": "information pervised nique. given joint distribution method constructs partitions informative tleneck already classification spectral analysis. general extensions allows consider data partitions proach utilizes systems tion captures. tion provides insight enables variations. work iterative lutions symmetric non-negative equals variables independent. needed average clustering find informative want lose irrelevant time maintain relevant extracting partitions method clustering variable principle information maximize consequences clustering ysis paradigm. eral underlying tion theoretic ters capture data relevance ious components calls principle verbs direct objects uments tissues galaxies cases objects \"correct\" measure would like rely purely co-occurrences vant information\" best possible way. formally amples treat detail clustering fitness target model model scribed gout. using interpretation principle tion tradeoff verge. show combine deterministic annealing mation tradeoff possible mention consider words based multiple parts speech complex gene­ expression start notation. random variable names letters denote specific values taken variables. ital letters ables sets denoted boldface lowercase let­ ters statement used shorthand joint distribution variable evant\" variable namely seek partition auxiliary mapping mutual information minimized maximized. depen­ relevant dency relations relations hand want predict multi-information bution factored marginals. concept small product dis­ lose much approximating tribution. like mutual information measures average number bits gained joint compression variables thus conditional sponds markov independence assumption order recall markov indepen­ dence assumptions d{piig) mea­ butions consistent sured terms extent independencies vio­ lated since independent given parents. thus consistent multi-information original variate case using semantics previous section. given observed variables instead partition variable consider corre­ spond different partitions served variables. variables variablesland speci­ fied using since assume vari­ variables ables restrict l�afs. thus stochastic generalization balance information example simple example consider application g��� figure variational specifies compresses g��� specifies want predict choice dags yc\"\"' resulting construction lead independencies specified general cannot instead tradeoff requiring tion consistent information minimal gout. tions consistent appendix essence terms distort close conditional involved replace words understand perfonns conditional differences theorem small since reduces different values exponential conditional value intuition compressing large variables example concrete parallel orem chronous variant ditional ditional distortion previous variable able. f-/= p-the main difference update asynchronous implications theorem asynchronous iterations consistent converge stationary optimization make independent lution consists predictive crease present curves. ters increase relevant pression remains unchanged. next splits fects accordingly splits. sion variables simultaneously dependentl preserve high possible. mation curves combination formative example preserve original hand stage thus word-clusters preserve tion category vari­ able. explored. tive procedure dures. ality reduction techniques bottleneck tion important contrast aimed preserving information specific data defined user. dimensions ters clusters hierarchy agreement additionally tion category second words maximize gious' 'homosexual' 'peace 'religion'. accordingly maximized approximately others). \"soft\" aspect relevant words assigned word 'clinton' assigned clusters dealing signed ability) given second transform latter term divergence extracting term -ep) similar transformation applies te�s deal third bayes rule rewrite smce involve ignore term vergence term subtract finally neck. advances neural information processing sys­ tems tishby lahav. objec­ tive spectral classification galaxies information bottleneck tronomical society\" mnras clusters information pages text classification. european colloquium infor­ bialek. information bot­ proofs sketch proofs main theorems. start theorem onary points subject proof basic idea find stati normalization multi­ pliers lagrangian ig;n {jig following lemma. differentiate lemma aa;l�i�j) term differentiate mutual information pears note ignore terms depend value since absorbed normaliza­ tion constant. ignored. terms refer equating dividing fol­ lowing equation. stationary ig'\" {jig\"\"'. reduces lagrangian call require gin. also recall lagrangian equivalent grangian ig'n introduce auxiliary grangian subject gout. constraints without edges. easy coincide projections onto space distributions gout g;n. ions get. using properties lemma choice consistent gout respectively t.cl minimize taking derivatives equating following self consis­", "year": 2013}