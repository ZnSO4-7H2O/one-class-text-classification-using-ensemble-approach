{"title": "Learning to Acquire Information", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "We consider the problem of diagnosis where a set of simple observations are used to infer a potentially complex hidden hypothesis. Finding the optimal subset of observations is intractable in general, thus we focus on the problem of active diagnosis, where the agent selects the next most-informative observation based on the results of previous observations. We show that under the assumption of uniform observation entropy, one can build an implication model which directly predicts the outcome of the potential next observation conditioned on the results of past observations, and selects the observation with the maximum entropy. This approach enjoys reduced computation complexity by bypassing the complicated hypothesis space, and can be trained on observation data alone, learning how to query without knowledge of the hidden hypothesis.", "text": "consider problem diagnosis simple observations used infer potentially complex hidden hypothesis. finding optimal subset observations intractable general thus focus problem active diagnosis agent selects next most-informative observation based results previous observations. show assumption uniform observation entropy build implication model directly predicts outcome potential next observation conditioned results past observations selects observation maximum entropy. approach enjoys reduced computation complexity bypassing complicated hypothesis space trained observation data alone learning query withknowledge hidden hypothesis. active diagnosis agent attempts discover hidden hypothesis asking series well chosen questions. instance sushi shop might wish learn preference customer asking series comparison questions; network monitor might wish detect faulty link making sequence connectivity queries. difﬁculty diagnosis lies cost making observations often expensive obtain results observations. practical task subset observations yields information. krause guestrin demonstrated greedy strategy iteratively maximizes information gain obtain subset observations nearoptimal; golovin krause extended result adaptive case agent chooses next most-informative observation based outcomes previous observations. paper consider setting agent using greedy strategy adaptive case. however even greedy scheme signiﬁcant computational cost. execution time information gain must computed candidate observation requires computing entropy terms integrate complex hypothesis space computationally infeasible. example case detecting link failures total number possible failure conﬁgurations number links. prior work developed various approximation methods make information gain computable. rish zhen bellala simplify complex hypothesis space exploiting independence structures graphical model using problem speciﬁc assumptions restrict hypothesis space. cases observations rule hypothesis completely maintain version space hypotheses consistent observations made attempt shrink size observations. tong nowak directly reduce size version space. seung maintains sample version space called committee selects observation maximally reduces size committee. joshi tong lewis consider special case hypothesis classiﬁer observations data points. employ heuristic maintains single current-best classiﬁer selects datapoint closest decision boundary. holub aims reduce entropy entire dataset uses committee classiﬁers estimate entropy. propose approach active diagnosis fundamentally different previous approaches wish select next most-informative observation without explicitly modeling maintaining notion hypothesis space modeling implications observations directly. intuition observations might redundant given observations network example node connected node single path connectivity implies links path functional. general implications probabilistic rather deterministic given current observations model outcome unseen observation ok). result assumption uniform observation entropy next mostinformative observation precisely observation maximum entropy conditioned observations already made entropy easily computed model directly neuralnetwork called implication model. trained supervised fashion dataset observation data dataset training examples contain values possible observation dimensions associated underlying true hypotheses require labeled true hypothesis. implication model trained used observation collection algorithm adaptively selects next most-informative observation conditioned previous observations. informative observations made deliver best hypothesis observations problem speciﬁc manner. prove assumption uniform observation entropy next most-informative observation made without explicit modeling hypothesis space modeling implications observations instead model dual advantages computationally efﬁcient trainable unlabeled data. evaluate approach several distinct benchmark problems provide problem-speciﬁc algorithms delivering best hypothesis informative observations made. compare results reasonable baseline algorithms give intuition approach consider simpliﬁed one-player variant game battleship rather sinking ships given board task infer structure board itself consist locations orientations ships. algorithm solve problem keep possible boards additional observation discard boards incompatible observation results miss boards ship occupying coordinate discarded). query chosen maximizing disagreement between remaining boards intuitively coordinate occupied ship half remaining board unoccupied half query coordinate would narrow choice compatible boards half. algorithm terminates board remaining set. drawback aforementioned algorithm computation disagreement requires enumeration remaining boards set. game battleship exponentially many number boards makes ﬁrst query already infeasible compute. instead assume existence oracle which given past observations capable predicting probability miss arbitrary unseen coordinate. given oracle following question unseen coordinate confusing oracle turns assumption uniform observation entropy informative query coincides query maximally confuses oracle. existence oracle allows make informative query without enumerate space possible boards saving computation time. model oracle neural network trained ofﬂine task board-completion given complete board coordinates labeled either miss random subset coordinates obscured hiding labels forming partially observable board neural network tasked produce supervised learning. formalize speciﬁcs approach. section formally deﬁne problem active diagnosis show uniform observation entropy assumption next most-informative test conditioned observation entropy maximizer present algorithm active diagnosis. issue taking advantage independence structures graph develops efﬁcient loopy belief propagation algorithms approximate entropy. bellala develops problem-speciﬁc ranking function orders observations according entropy. however approaches problem speciﬁc difﬁcult generalize kinds problems. observation problem made hidden hypothesis random variable take values ﬁnite observation random variables take values dom. hypothesis drawn distribution given hypothesis observations drawn conditional distributions work consider discrete hypothesis space observations discrete domains. adopt greedy strategy proposed golovin krause aims maximize information gain observation time given current observations task choose additional observation among observations maximizes mutual information hidden hypothesis. brevity write past observations ok}. objective rather computing mutual information directly typical approach solves equivalent problem selecting observation minimizes posterior conditional entropy ﬁrst glance decomposition less difﬁcult entropy terms depend would suggest need computed. introduce assumption uniform observation entropy allows simplify constant. deﬁnition observation problem uniform observation entropy property hypothesis conditional entropy observation dimensions equal reasonable assumption many domains common example observations obtained noisy channel corrupts observations equally. observations deterministic uniform observation entropy property holds trivially. compute h|o−) require probability values |o−) possible naively compute probability summing hypothesis space would yield beneﬁt. however model probability directly using function approximator. result efﬁcient computation argmaxo h|o−) runs time total number observations depends complexity function approximator. call model |o−) implication model attempts deduce outcome future observation based past observations. describe train model section solve active diagnosis problem parts observation collection hypothesis delivery. observation adaptively chosen time step maximizing entropy h|o−) budget observations exhausted. observations used hypothesis delivery algorithm obtain hypothesis. observation collection algorithm detailed algorithm here function models query function returns value observation given hidden hypothesis trying discover. call function budget number times. scheme unique observation collection algorithm completely decoupled hypothesis delivery algorithm. decoupling allows explore different hypothesis delivery algorithms guarantee observations used deliver hypothesis well chosen. observation collection algorithm depends |o−) trained dataset annotated hypothesisspeciﬁc information. section describe model o−). ﬁrst describe neural network model that given simultaneously computes possible choices describe train neural network fashion similar sparse auto-encoder. although strategy generalized non-binary discrete observations restrict attention case binary observations following. neural network model consider model outputs possible choices observations simultaneously since observation collection algorithm must determine best amongst possible remaining observations. note variable length depending many observations made time picking next one. choose encode variablelength input lstm found simple feed-forward neural network allows value observation unknown value effective. model resembles simple auto-encoder single fully-connected hidden layer containing rectiﬁed linear units; however approach could straightforwardly extended deeper encoding/decoding network structure complex domains. input network vector follows ﬁrst draw number unif change probability crossentropy loss function softmax probability expected output experiments paper hidden layer contains units equipped relu activation function. implement train implication model using tensorflow. think model type sparse autoencoder rather sparsifying hidden layer sparsifying input layer. intuitively withsparsiﬁcation network learns identity implication sparsifying input layer forces network learn inter-relationships observations handful observations sufﬁcient recover full observations. note randomly ablating subset observation features learn model capable working subset observations could potentially work necessary observation collection following particular greedy policy chooses observation maximum entropy leading particular sequences observations made restricts queries. good avenue future work might train observation traces generated network specialize further particular distribution observations. section evaluate performance active diagnosis algorithm several simple active diagnosis problems. outline problems characteristics elaborating detail battleship variant classic battleship game goal infer conﬁgurations ships board queries board possible. hypothesis space consists approximately different conﬁgurations query coordinate results miss. sushi problem preference elicitation goal learn full preference order unseen user different types sushi pair-wise comparison queries possible. hypothesis space consists potential full rankings network consider fault localization task network nodes organized tree link chance failure. task learn efﬁcient scheme querying pair-wise connectivities localize failure queries possible. hypothesis space consists combinations failures. query pair-wise connectivity check restricted direct link checks additional pairs ﬁxed nodes measurement equipment. task infer locations ships queries possible. board grid ships size placed randomly arbitrary horizontal vertical orientations. adjacent placements allowed ships overlap other. figure example board. stage game observation collection algorithm selects most-informative observation updates belief space. figure illustrates belief space various numbers observations. note without observations model predicts ship likely located near center board rather toward edge. case observations model queried lower left without completely observing long ship middle board. consider kinds hypothesis delivery schemes. ﬁrst scheme deliver probabilities observations using implication model |o−) implicitly inferring hidden locations ships. second scheme constraint solver takes list observations made returns hypothesis consistent observations. comparison consider baseline algorithms random sampler rand samples unseen coordinates random. sink algorithm samples unseen coordinates random found queries neighboring coordinates coordinates found resumes random sampling. random sink algorithms initially mark coordinates miss update recorded. evaluate performance ﬁrst hypothesis delivery scheme output maximum likelihood guess coordinate. accuracy measured fraction correctly guessed coordinates. figure compares accuracy across coordinates function number observations; accuracy means coordinates guessed correctly. experiment consider different variant implication model single hidden-layer fully-connected model originally described -layer neural-network model contains convolutional neural-network layer fully-connected hidden layer. random algorithm improves linearly expected sink heuristic performs better random approach performs best performing better logistic regression figure considers experiment except observation error introduced condition variants similarly robust noise baseline algorithms. figure belief space observations particular board various numbers observations. intensity indicates probability coordinate colored dots indicates past observations green miss sushi data collected kamishima contains user preferences kinds sushi expressed full rankings. dataset also contains feature vectors describing individual type sushi describing users study omit. task following given user infer full ranking user pair-wise comparison queries possible? naively sorting problem obtain full ranking permutation items comparisons. however preference orderings uniformly random instance preference tuna indicate user’s liking cooked sushi sushi. evaluate accuracy using kendall correlation value means pair-wise orderings prediction ground truth agree other value indicates pair-wise orderings disagreement. related work dataset attempts discover full preference user based pair-wise queries user. souﬁani models sushi preferences generative process preferences caused combination user features sushi features perform elicitation user form pair-wise queries. without elicitation able infer user’s preference kendall correlation performs elicitation user form pair-wise queries instead learning full preference attempts recommend best sushi user. able always predict best sushi pair-wise comparisons. thus comparison consider various in-place sorting algorithms baseline. time pair-wise comparison question made take snapshot current array extract pair-wise orderings user preferences split preferences training preferences testing set. order train implication model handle novel permutations seen training augment training randomly sampled permutations addition preferences. experiment measure performance directly kendall correlation without delivering explicit ordering hypothesis figure compares observation collection algorithm various in-place sorting algorithms bubblesort bsort quicksort qsort mergesort msort. even without observations able obtain kendall correlation indicating underlying distribution preferevaluate performance second hypothesis delivery scheme constraint solver produce hypothesis form ships’ locations orientations using observations collected rand sink constraints hypothesis. measure accuracy number correctly predicted ship’s locations orientations means ship’s locations orientations correctly produced constraint solver given observations collected. figure compares number correctly guessed ships function number observations. algorithm performs best followed sink random performing worst. note constraint solvers known produce arbitrary hypotheses satisfy constraint constrained system despite this able consistently perform baseline algorithms. figure comparison algorithm baselines using constraint solver hypothesis delivery. accuracies averaged randomly generated boards. single-hidden-layer implication model considered here. ences uniform; comparison baseline sorting algorithms make assumptions underlying distribution thus starts correlation around scheme able infer full ranking user queries beating performance qsort takes queries. hypothesis space consists failure cases direct link probability failure. query pair-wise connectivity check restricted direct link checks additional pairs ﬁxed nodes. also considered case query chance error result shown figure note algorithm robust presence noise improves pair-wise preferences incrementally observation whereas deterministic baseline qsort msort sorts preferences assuming observations perfect. consider task fault localization network nodes organized tree. network without failure shown figure nodes network direct links forming spanning tree. hypothesis delivery implication model |o−) output probability failure directly connected pairs nodes. measure accuracy maximum likelihood guess pairs measure fraction correctly diagnosed link failures. compare approach naive localization scheme rand randomly picks unobserved direct link checks disconnected. random scheme always able diagnose link failures observations. however leverage network structure learn efﬁcient querying scheme utilizing connectivity queries additional pairs nodes. figure shows accuracy random algorithm. outperforms random algorithm time reaching accuracy observations. performs worse random scheme less observations query direct links ﬁrst observations additional pairs yield information without pinpointing exact failure. performs worse random scheme past observations. consequence greedy approach maximizes information gain observation time observations selected random approach optimal size capable answering failure queries. fails alternative optimal partly instances many links fail simultaneously weakens structures implication model depends sizes disconnected subgraphs link fails. dependency coefﬁcient measures fraction pair-wise connectivity would affected fail. located next leaf node network coefﬁcient close disconnecting breaks network equally sized components coefﬁcient close figure measures accuracy diagnosis dependency coefﬁcient. general higher dependency coefﬁcient means better diagnosis many pair-wise connectivities depend link successful pair-wise connection pairs imply link failed; conversely link fails pair-wise connections depend link fail well. property useful link many pair-wise connections depend would better chance correctly diagnosed. paper present approach active diagnosis domains complex hypothesis spaces. show given uniform observation entropy assumption problem choosing informative query reduces maximizing conditional observation entropy h|o−) computed |o−) without explicit modeling hypothesis space. describe neural network modeling |o−) called implication model demonstrate trained supervised fashion. evaluate approach several simple distinct benchmarks proof experiments depends assumption applicability assumption legitimate concern. true assumption valid diagnostic problems apply class problems kind sensor used gather signals different locations deterministic environment including robotic localization geostatistics optimal sensor placements. unique feature approach implication model trained unlabeled observation data supervised fashion. good direction future work would apply approach domains abundance observation data without annotations hypothesis generated implication model learns interesting structures observation data; would instructive quantify easy learn different kinds structures.", "year": 2017}