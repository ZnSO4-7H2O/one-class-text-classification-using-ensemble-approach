{"title": "Alternating Optimisation and Quadrature for Robust Control", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Bayesian optimisation has been successfully applied to a variety of reinforcement learning problems. However, the traditional approach for learning optimal policies in simulators does not utilise the opportunity to improve learning by adjusting certain environment variables: state features that are unobservable and randomly determined by the environment in a physical setting but are controllable in a simulator. This paper considers the problem of finding a robust policy while taking into account the impact of environment variables. We present Alternating Optimisation and Quadrature (ALOQ), which uses Bayesian optimisation and Bayesian quadrature to address such settings. ALOQ is robust to the presence of significant rare events, which may not be observable under random sampling, but play a substantial role in determining the optimal policy. Experimental results across different domains show that ALOQ can learn more efficiently and robustly than existing methods.", "text": "expected performance. however approach often requires testing policy prohibitively large number scenarios robust signiﬁcant rare events i.e. fails time rare events substantially affect expected performance. example rare localisation errors mean robot much nearer obstacle expected increasing risk collision. since collisions catastrophic avoiding maximising expected performance even though factors contributing collision occur rarely. cases na¨ıve approach rare events often enough learn appropriate response. instead propose approach called alternating optimisation quadrature speciﬁcally aimed towards learning policies robust rare events sample efﬁcient possible. main idea actively select environment variables simulator thanks gaussian process models returns function policy environment variables then time-step bayesian quadrature turn select policy environment setting respectively evaluate. apply aloq number problems results demonstrate aloq learns better faster multiple baselines. also demonstrate policy learnt aloq simulated hexapod transfers successfully real robot. frank mannor precup also consider problems posed sres. particular propose approach based importance sampling efﬁciently evaluating policies whose expected value substantially affected rare events. approach based temporal difference methods take bo-based policy search approach. unlike methods well suited settings sample efﬁciency paramount and/or assumptions underlie methods cannot veriﬁed. importantly assume prior knowledge sres directly alter probability events policy evaluation. contrast strength aloq requires environment variables controlled simulator without assuming prior knowledge bayesian optimisation successfully applied variety reinforcement learning problems. however traditional approach learning optimal policies simulators utilise opportunity improve learning adjusting certain environment variables state features unobservable randomly determined environment physical setting controllable simulator. paper considers problem ﬁnding robust policy taking account impact environment variables. present alternating optimisation quadrature uses bayesian optimisation bayesian quadrature address settings. aloq robust presence signiﬁcant rare events observable random sampling play substantial role determining optimal policy. experimental results across different domains show aloq learn efﬁciently robustly existing methods. consideration applying reinforcement learning physical setting risk expense running trials e.g. learning optimal policy robot. anconsideration robustness learned policies. since typically infeasible test policy contexts difﬁcult ensure works broadly intended. fortunately policies often tested simulator exposes environment variables state features unobserved randomly determined environment physical setting controllable simulator. paper considers environment variables help learn robust policies. although running trials simulator cheaper safer running physical trials computational cost simulated trial still quite high. challenge develop algorithms sample efﬁcient i.e. minimise number trials. settings bayesian optimisation sample-efﬁcient approach successfully applied multiple domains recently ciosek whiteson also proposed based algorithm offer setting environment variable gradually changed based observed trials. since offer method suffers disadvantages mentioned earlier. also assumes environment variable affects initial state otherwise leads unstable estimates. williams santner notz consider problem setting call design computer experiments similar setting speciﬁcally consider sres. proposed gp-based approach marginalises environment variable alternating however unlike aloq method based acquisition function makes computationally expensive reasons discussed section applicable discrete environment variables. include method baseline experiments. results presented section show that compared aloq method unsuitable settings sres. further method computationally expensive fails even outperform baseline randomly samples environment variable step. krause also address optimising performance presence environment variables. however address fundamentally different contextual bandit setting learned policy conditions observed environment variable. pilco modelbased policy search method achieves remarkable sample efﬁciency robot control pilco superﬁcially resembles aloq difference pilco models transition dynamics aloq models returns function policy environment variable. pilco fundamentally suited setting. first assumes transition dynamics gaussian learned hundred observed transitions often infeasible complex environments second even simple environments pilco able learn transition dynamics setting environment variable observed physical trials leading major violations gaussian assumption environment variables cause sres. policies found simulators rarely optimal deployed physical agent reality exist inability simulator model reality perfectly. epopt tries address ﬁnding policies robust simulators different settings parameters. first multiple instances simulator generated drawing random sample simulator parameter settings. trajectories sampled instances used batch policy optimisation algorithm aloq ﬁnds risk-neutral policy epopt ﬁnds riskaverse solution based maximising conditional value risk feeding policy optimisation sampled trajectories whose returns lower cvar. risk-neutral setting epopt reduces underlying policy optimisation algorithm trajectories randomly sampled different instances simulator. approach sres often enough learn appropriate response demonstrate experiments. pinto also suggest method address problem ﬁnding robust policies. method learns policy training simulator adversarial nature i.e. simulator settings dynamically chosen minimise returns policy. method requires signiﬁcant prior knowledge able simulator settings provides right amount challenge policy. furthermore consider settings sres. provide principled quantifying uncertainties associated modelling unknown functions. distribution functions fully speciﬁed mean function covariance function in-depth treatment) encode prior belief nature function. prior combined observed values update belief function bayesian generate posterior distribution. prior mean function often assumed convenience. popular choice covariance function class stationary functions form implies correlation function values points depends distance them. regression assumed observed function values sample multivariate gaussian distribution. prediction point connected observations mean covariance functions. conditioning observed data comproperty generating estimates uncertainty associated prediction makes particularly suited ﬁnding optimum using requires acquisition function guide search balance exploitation exploration given observations next point evaluation actively chosen maximises acquisition function. figure aloq models return function predicted mean based observed data; predicted return different together uncertainty associated them given aloq marginalises computes associated uncertainty used actively select john deﬁning current optimal evaluation i.e. argmaxxi seeks maximise expected improvement current optimum max{ contrast depend directly incorporates uncertainty prediction deﬁning upper bound controls exploration-exploitation tradeoff. tribution. using regression compute prediction given observed data gaussian whose mean variance computed analytically particular choices covariance function analytical solution exists approximate mean variance monte carlo quadrature sampling predictions various given observed data also devise acquisition functions actively select next point evaluation. natural objective select minv imises uncertainty i.e. argminx nature depend thus computationally feasible evaluate. uncertainty sampling alternative acquisition function chooses v|d). maximum posterior variance argmaxx although simple computationally cheap reducing uncertainty since evaluating point highest prediction uncertainty necessarily lead maximum reduction uncertainty estimate integral. samples estimates integral typically requires large less sample efﬁcient used cheap evaluate. many merits philosophically practically brought o’hagan hennig osborne girolami below describe active bayesian assume access computationally expensive simulator takes input policy environment variable produces output return belong compact sets respectively. also assume access probability distribution known priori posterior distribution estimated whatever physical trials conducted. note require perfect simulator uncertainty dynamics physical world modelled i.e. environment variables simulator parameters whose correct ﬁxed setting known certainty. deﬁning assume dataset objective optimal policy argmax first consider na¨ıve approach consisting standard application disregards performs input attempts estimate formally approach models zero mean function suitable covariance function given variation different settings treated noise. estimate na¨ıve approach applies sampling timestep. approach almost surely fail sampling sres often enough learn suitable response. contrast method aloq models acknowledging inputs. main idea behind aloq given acquisition function select evaluation acquisition function select conditioning πl+. prediction mean covariance computed using continuous apply monte carlo quadrature. although requires sampling large number evaluating corresponding feasible since evaluate expensive simulator computationally cheaper argmaxπ αaloq. note although possible deﬁne ei-based acquisition function max{ alternative choice aloq prohibitively expensive compute practice. stochastic renders analytically intractable. approximating using monte carlo sampling would require performing predictions points i.e. observed paired possible settings environment variable infeasible even moderate computational complexity predictions scales quadratically number predictions. acquisition function viewed performing policy evaluation approach. since presence sres leads high variance returns associated given policy critical importance minimise uncertainty associated estimate expected return policy. formalise objective acquisition function aloq selects minimising posterior variance yielding also tried uncertainty sampling experiments. unsurprisingly performed worse good reducing uncertainty associated expected return policy explained section properties aloq thanks convergence guarantees using aloq converges scheme relies also converges. unfortunately best knowledge existing convergence guarantees apply methods actively select points does. course expect active selection improve rate convergence algorithms non-active versions. however empirical results section show practice aloq efﬁciently optimises policies presence sres across variety tasks. aloq’s computational complexity dominated matrix inversion sample size dataset cubic scaling common methods involving gps. integral estimation iteration requires predictions selecting requires maximising acquisition function requires estimating together uncertainty associated fortunately well suited since estimate together uncertainty associated illustrated figure chosen aloq selects minimising acquisition function quantifying uncertainty selected aloq evaluates simulator updates datapoint estimate thus although approach described actively selects unlikely perform well practice. observation presence sres seek address aloq implies scale varies considerably e.g. returns case collision collision. nonstationarity cannot modelled stationary kernel. therefore must transform inputs ensure stationarity particular employ beta warping i.e. transform inputs using beta cdfs parameters beta distribution support given beta function. beta particularly suitable purpose able model variety warpings based settings parameters aloq transforms dimension independently treats corresponding hyperparameters. assume working transformed inputs rest paper. resulting algorithm able cope sres returns iteration still poor since evaluation leads noisy approximation true expected return. particularly problematic high dimensional settings. address this intensiﬁcation i.e. re-evaluation selected policies simulator essential. therefore aloq performs simulator calls timestep. ﬁrst evaluation selected bo/bq scheme described earlier. second stage evaluated selected using θ∗|ˆπ∗ using acquisition function support θnθ} estimate mean variance straightforward update beta warping parameters transform inputs. update condition dataset estimate acquisition function select argmaxπ αaloq acquisition function select θn|πn argminθ perform simulator call obtain update θ∗|ˆπ∗ using find argmaxπi acquisition function perform second simulator call obtain update evaluate aloq applied simulated robot control task including variation known priori must inferred data hexapod locomotion task experiments test functions clearly show element aloq necessary settings sres presented supplementary material. compare aloq several baselines na¨ıve method described previous section; method williams santner notz refer wsn; simple policy gradient method reinforce state-of-the-art policy gradient method trpo show importance component aloq also perform experiments ablated versions aloq namely random quadrature aloq sampled randomly instead chosen actively; unwarped aloq perform beta warping inputs; one-step aloq intensiﬁcation. plotted results median independent runs. details experimental setups variability performance found supplementary material. robotic simulator experiment evaluate aloq’s performance robot control problem implemented kinematic simulator. goal conﬁgure three controllable joints robot gets close possible predeﬁned target point. collision avoidance ﬁrst setting assume robotic part mobile robot localised itself near target. however localisation errors small possibility near wall joint angles lead colliding wall incurring large cost. minimising cost entails getting close target possible avoiding region wall present. environment variable setting distance wall. figures show expected cost conﬁgurations timestep method. aloq unwarped aloq rq-aloq greatly outperform baselines. reinforce trpo relatively sample inefﬁcient exhibit slow rate improvement performance fails converge all. figure shows learned conﬁgurations well policy would learned aloq wall shaded region represents possible locations wall. plot illustrates aloq learns policy gets closest target. furthermore based algorithms learn avoid wall active selection allows aloq quickly smart quadrature allows efﬁciently observe rare events accurately estimate boundary. readability presented conﬁgurations algorithms performance comparable aloq. joint breakage next consider variation instead uncertainty introduced localisation settings ﬁrst joint carry probability breaking consequently incurs large cost. minimising cost thus entails getting close target possible minimising probability joint breaking. figures shows expected cost conﬁgurations timestep method. since continuous setting requires discrete slightly different version discretised equidistant points. results similar previous experiment except baselines perform worse. particular na¨ıve baseline reinforce seem converged suboptimal policy since witnessed sres. figure shows learned conﬁgurations together policy would learned sres shaded region represents joint angles lead failure. ﬁgure illustrates aloq learns qualitatively different policy algorithms avoids joint angles might lead breakage still getting close target faster methods. readability present conﬁgurations competitive algorithms. performance reinforce trpo baselines relatively sample inefﬁcient. however question arises whether methods eventually optimal policy. check this iterations batch size trajectories repeated collision avoidance joint breakage settings. expected cost conﬁgurations iteration presented figure baselines solve tasks settings without sres i.e. possibility collision breakage however settings sres converge rapidly suboptimal policy unable recover even much longer since don’t experience sres often enough. especially striking collision avoidance task trpo converges policy relatively high probability leading collision. setting unknown consider setting known priori must approximated using trajectories baseline policy. setting instead directly setting robot arm’s joint angles torque applied joint ﬁnal joint angles determined torque unknown friction joints setting torque high lead joint breaking incurs large cost. simulator proxy real trials well simulated trials. ﬁrst case simply sample uniform prior baseline policy observed returns compute approximate posterior aloq compute optimal policy posterior comparison also compute corresponding optimal policy show active selection advantageous also compare policy learned rq-aloq. samples makes sense keep sample size relatively computational efﬁciency ﬁnding aloq policy however show aloq robust approximation comparing performance aloq policies used much larger sample size posterior distribution. evaluation drew samples granular posterior distribution measured returns three policies samples. average cost incurred aloq policy lower incurred policy lower rq-aloq policy. aloq ﬁnds policy slightly underperforms policy cases avoids sres experienced rq-aloq policies. consider hexapod locomotion task setup similar demonstrate experimentally. objective cross ﬁnish line ﬁxed distance starting point. failure cross line leads large negative reward reward completing task inversely proportional time taken. possible subset legs damaged broken deployed physical setting. experiments assume that based prior experience front back legs shortened removed probability respectively independent legs leading possible conﬁgurations. excluded middle legs experiment failure relatively lower impact hexapod’s movement. conﬁguration legs acts environment variable. figure shows setting. applied aloq learn optimal policy given damage probabilities restricted search policies archive created figure shows aloq ﬁnds policy much higher expected reward rq-aloq. also shows policy generates maximum reward none legs damaged broken demonstrate aloq learns policy applied physical environment also deployed best aloq policy real hexapod. order limit number physical trials required evaluate aloq limited possibility damage rear legs. learnt hexapod locomotion task robots move fully controlled environments complex natural ones face inevitable risk getting damaged. however expensive even impossible decommission robot whenever damage condition prevents completing task. hence desirable develop methods enable robots recover failure. intelligent trial error shown recover various damage conditions thereby prevent catastrophic failure. deployment it&e uses simulator create archive diverse locally high performing policies intact robot mapped lower dimensional behaviour space. robot becomes damaged deployment uses quickly policy archive highest performance damaged robot. however respond damage occurred. though learns quickly performance still poor learning initial trials damage occurs. mitigate effect propose aloq learn simulation policy highest expected performance across possible damage conditions. deploying policy instead policy optimal intact robot minimise expectation negative effects damage period it&e learned recover. paper proposed aloq novel approach using perform sample-efﬁcient robust presence signiﬁcant rare events. empirically evaluated aloq different simulated tasks involving robotic simulator hexapod locomotion task showed also applied settings distribution environment variable unknown priori successfully transfers real robot. results demonstrated aloq outperforms multiple baselines including related methods proposed literature. further aloq computationally efﬁcient require restrictive assumptions made environment variables.", "year": 2016}