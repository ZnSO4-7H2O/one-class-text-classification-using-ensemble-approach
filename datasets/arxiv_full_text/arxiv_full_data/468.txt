{"title": "Making Neural QA as Simple as Possible but not Simpler", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "Recent development of large-scale question answering (QA) datasets triggered a substantial amount of research into end-to-end neural architectures for QA. Increasingly complex systems have been conceived without comparison to simpler neural baseline systems that would justify their complexity. In this work, we propose a simple heuristic that guides the development of neural baseline systems for the extractive QA task. We find that there are two ingredients necessary for building a high-performing neural QA system: first, the awareness of question words while processing the context and second, a composition function that goes beyond simple bag-of-words modeling, such as recurrent neural networks. Our results show that FastQA, a system that meets these two requirements, can achieve very competitive performance compared with existing models. We argue that this surprising finding puts results of previous systems and the complexity of recent QA datasets into perspective.", "text": "recent development large-scale question answering datasets triggered substantial amount research end-toend neural architectures increasingly complex systems conceived without comparison simpler neural baseline systems would justify complexity. work propose simple heuristic guides development neural baseline systems extractive task. ingredients necessary building high-performing neural system ﬁrst awareness question words processing context second composition function goes beyond simple bag-of-words modeling recurrent neural networks. results show fastqa system meets requirements achieve competitive performance compared existing models. argue surprising ﬁnding puts results previous systems complexity recent datasets perspective. question answering important end-user task intersection natural language processing information retrieval systems bridge ir-based search engines sophisticated intelligent assistants enable directed information retrieval process. systems ﬁnding precisely piece information sought user instead documents snippets containing answer. special form namely extractive deals extraction direct answer question large-scale extractive datasets sparked research interest development end-to-end neural systems. typical neural architecture consists embedding- encoding- interactionanswer layer systems describe several innovations different layers architecture special focus developing powerful interaction layer aims modeling word-by-word interaction question context. although variety extractive systems proposed competitive neural baseline. systems built call top-down process proposes complex architecture validates design decisions ablation study. ablation studies however remove single part overall complex architecture therefore lack comparison reasonable neural baseline. raises question whether complexity current systems justiﬁed solely empirical results. relation answer question answering question easily possible applying simple context/type matching heuristic. heuristic aims selecting answer spans match expected answer type close important question words actual answer would easily extracted heuristic. work propose aforementioned context/type matching heuristic guideline derive simple neural baseline architectures extractive task. particular develop simple neural bag-of-words recurrent neural network baseline namely fastqa. crucially models make complex interaction layer model interaction question context computable features word level. fastqa’s strong performance questions necessity additional complexity especially interaction layer exhibited recently developed models. address question evaluating impact extending fastqa additional interaction layer doesn’t lead systematic improvements. finally contributions following definition evaluation bowrnn-based neural baselines guided simple heuristic; bottom-up evaluation fastqa system increasing architectural complexity revealing awareness question words application enough reach stateof-the-art results; iii) complexity comparison between fastqa complex architectures well in-depth discussion usefulness interaction layer; qualitative analysis indicating fastqa mostly follows heuristic thus constitutes strong baseline extractive begin motivating architectures deﬁning proposed context/type matching heuristic type answer span correspond expected answer type given question correct answer surrounded context question precisely surrounded many question words. similar heuristics frequently implemented explicitly traditional systems e.g. answer extraction step moldovan however work heuristic merely used guideline construction neural systems. following denote hidden dimensionality model question tokens context tokens embedding embedding layer responsible mapping tokens corresponding n-dimensional representation typically done mapping word corresponding word embedding using embedding matrix s.t. another approach embed word encoding corresponding character sequence s.t. work convolutional neural network ﬁlter width max-pooling time explored refer reader additional details. approaches combined concatenation s.t. ﬁnal embedding becomes type matching baseline extract span question refers expected lexical answer type extracting either question word ﬁrst noun phrase question question words what which leads correct questions. encode concatenating embedding ﬁrstlast word together average embedding words within lat. concatenated representations transformed fully-connected layer followed tanh non-linearity note refer fully-connected layer following s.t. rn×m similarly encode potential answer span context i.e. spans speciﬁed maximum number words concatenating embedding ﬁrstlast word together average embedding words within span. surrounding context potential answer span give important clues towards type answer span finally answer span compute average wiqb wiqw scores token-windows left right respective -span. results total scores weighted trainable scalar parameters summed compute contextmatching score gctxt. answer span scoring ﬁnal score span typecontext matching score gtype gctxt. model trained minimize softmax-cross-entropy loss given scores spans. although baseline closely models intended heuristic several shortcomings. first cannot capture compositionality language making detection sensible answer spans harder. furthermore semantics question dramatically reduced representation expected answer-type scalar word-in-question features. finally answer spans restricted certain length. account shortcomings introduce another baseline relies application single bi-directional recurrent neural networks followed answer layer separates prediction start answer span. lample demonstrated birnns powerful recognizing named entities makes sensible choice context encoding allow improved type matching. context matching similarly achieved birnn informing locations question tokens appearing context wiq-features. important recognize model implicitly learn capture heuristic limited abstract level rnn-based model called fastqa consists three basic layers namely embedding- encodinganswer layer. embeddings computed explained layers described detail following. illustration basic archiinstance nominal modiﬁers left span apposition right span additionally concatenate average embeddings words left right span respectively. concatenated span representation comprises total different embeddings transformed fully-connected layer tanh non-linearity ˜xse finally concatenation representation span representation elementwise product i.e. ˜xse] serve input feed-forward neural network hidden layer computes type score gtype span context matching order account number surrounding words answer span measure question answer span match introduce word-in-question features. computed context word explained following weighted wiqw feature context word deﬁned deﬁnes basic similarity score based word-embeddings. motivated hand intuition question tokens rarely appear context likely important answering question hand fact question words might occur morphological variants synonyms related words context. latter captured using word embeddings instead words whereas former captured application softmax operation ensures infrequent occurrences words weighted heavily. encoding following describe encoding context analogous question. allow interaction embeddings described ﬁrst projected jointly n-dimensional representation transformed single highway layer similar want encoder aware question words feed binaryweighted word-in-question feature addition embedded context words input. complete input rn+×lx encoder therefore deﬁned follows mentioned before utilize encoder parameters question context except projection matrix shared. however initialized s.t. context question encoding identical beginning training. finally able encoder question context features question. answer layer encoding context question ﬁrst compute weighted n-dimensional question representation note representation context-independent computed once i.e. additional wordby-word interaction context question. encoder embedded tokens encoded form composition function. prominent type encoder recurrent neural network also used work. feeding additional word-level features similar rarely done exception wang interaction layer research focused interaction layer responsible wordby-word interaction context question. different ideas explored attention coattention bi-directional attention multi-perspective context matching ﬁne-grained gating ideas enriching encoded context weighted states question cases also context. gathered individually context state concatenated serve input additional rnn. note layer omitted completely fastqa therefore constitutes main simpliﬁcation previous work. answer layer finally systems divide prediction start another network. complexity ranges using single fully-connected layer employing convolutional neural networks recurrent deep highway-maxout-networks. introduce beam-search extract likely answer span simple -layer feed-forward network. explore necessity interaction layer architecturally comparable existing models extend fastqa additional interaction layer particular introduce representation fusion enable exchange information passages context question context representation fusion deﬁned weighted addition state i.e. n-dimensional representation respective co-representation. context state corresponding co-representation retrieved attention rest context question respectively fused beam-search prediction time compute answer span highest probability employing beam-search using beam-size means ends top-k starts predicted span highest overall probability predicted ﬁnal answer. many neural architectures extractive conceived recently. systems broken four basic layers individual innovations proposed. high-level illustration systems show figure following compare system detail existing models. embedder embedder responsible embedding sequence tokens sequence ndimensional states. proposed embedder similar existing ones used example yang representation. sake brevity describe technical details layer appendix extension focus work merely serves representative complex architectures described performance squad newsqa datasets measured terms exact match mean answer token-based measure originally proposed rajpurkar also account partial matches. model model trained spans length keep computation tractable. leads upper bound accuracy squad newsqa. pre-processing steps lowercase inputs tokenize using spacy. binary word question feature computed lemmas provided spacy restricted alphanumeric words stopwords. throughout experiments hidden dimensionality dropout input embeddings mask words rate -dimensional ﬁxed word-embeddings glove employed adam optimization initial learning-rate halved whenever measure development dropped epochs. used mini-batches size tokenize input whitespaces non-alphanumeric characters binary word question feature computed words appear context. throughout experiments hidden dimensionality variational dropout input embeddings mask words rate dimensional ﬁxed word-embeddings glove employed adam optimization initial learning-rate halved whenever measure development dropped checkpoints. checkpoints occurred every mini-batches containing examples. cutting context length newsqa contains examples large contexts contexts larger tokens order efﬁciently train models. ensure least best answers still present remaining tokens. note restriction employed training. bilstm bilstm wiqb bilstm wiqw bilstm wiqb+w fastqa∗ intrafusion fastqa∗ intra inter fastqa∗ char-emb. fastqaext∗ char-emb. fastqa beam-size fastqaext beam-size table shows individual contributions model component incrementally added plain bilstm model without features character embeddings beam-search. crucial performance boost stems introduction either features however extensions also achieve notable improvements typically beam-search slightly improves results shows probable start necessarily start best answer span. general results interesting many ways. instance surprising simple binary feature like wiqb dramatic effect overall performance. believe reason fact encoder withknowledge actual question account every possible question might asked i.e. keep track entire context around token recurrent state. informed encoder hand selectively keep track question related information. abstract concrete entities respective types rarely case many entities type occur question. example person mentioned question context encoder needs remember question-person mentioned concrete name person. another interesting ﬁnding fact additional character based embeddings notable effect overall performance already observed improvements employing representation fusion allow interaction. shows sophisticated interaction layer help. however differences substantial indicating extension offer systematic advantage. comparing state-of-the-art neural baseline achieves good results datasets instance outperforms feature rich logistic-regression baseline squad development nearly reaches bilstm baseline system shows half third questions squad newsqa respectively answerable simple neural baseline. however state-of-the-art systems quite large indicates employing logistic regression match-lstm dynamic chunk reader fine-grained gating multi-perspective matching dynamic coattention networks bidirectional attention flow r-net table ofﬁcial squad leaderboard singlemodel systems test date submitting model. rajpurkar wang jiang yang wang xiong published. note systems regularly uploaded improved squad. results presented tables clearly demonstrate strength fastqa system. competitive previously established stateof-the-art results datasets even improves newsqa. quite surprising considering simplicity fastqa putting existing systems complexity datasets especially squad perspective. extended version fastqaext achieves even slightly better results outperforming reported results prior submitting model competitive squad benchmark. order answer question compare fastqa system without complex word-byword interaction layer representative models interaction layer namely fastqaext dynamic coattention network measured timespace-complexity fastqaext reimplementation relation fastqa found fastqa twice fast systems requires less memory compared fastqaext respectively. addition looked systematic advantages fastqaext fastqa comparing squad examples development answered correctly fastqaext incorrectly fastqa fastqa wins studied average questionanswer length well question types sets could systematic difference. observation made manually comparing kind reasoning needed answer certain question. ﬁnding aligns marginal empirical improvements especially newsqa between systems indicating fastqaext seems generalize slightly better offer particular systematic advantage. therefore argue additional complexity introduced interaction layer necessarily justiﬁed incremental performance improvements presented especially memory run-time constraints exist. besides empirical evaluations section provides qualitative error inspection predictions squad development dataset. analyse errors made fastqa system detail highlight basic abilities missing reach human level performance. prominent type mistake lack ﬁnegrained understanding certain answer types another error lack co-reference resolution context sensitive binding abbreviations also model sometimes struggles capture basic syntactic structure especially respect nested sentences important separators like punctuation conjunctions ignored manual examination errors reveals mistakes directly attributed plain application heuristic. similar analysis reveals analyzed positive cases covered heuristic well. therefore believe model wrt. empirical results models well mostly learn simple context/type matching heuristic. ﬁnding important reveals extractive system solve complex reasoning types chen used classify squad instances order achieve current state-ofthe-art results. children’s book corpus paved construction end-to-end neural architectures reading comprehension. thorough analysis chen however revealed dailymail/cnn easy still quite noisy. datasets constructed eliminate problems including squad newsqa msmarco previous question answering datasets mctest trec-qa small successfully train end-to-end neural architectures models discussed required different approaches. traditional statistical systems relied linguistic pre-processing pipelines extensive exploitation external resources knowledge bases feature-engineering. paradigms include template matching passage retrieval work introduced simple context/type matching heuristic extractive question answering serves guideline development neural baseline system. especially fastqa rnn-based system turns efﬁcient neural baseline architecture extractive question answering. combines simple ingredients necessary building currently competitive system awareness question words processing context composition function goes beyond simple bag-of-words modeling. argue important ﬁnding puts results previous complex architectures well complexity recent datasets perspective. future want extend fastqa model address linguistically motivated error types thank sebastian riedel philippe thomas leonhard hennig omer levy comments early draft work well anonymous reviewers insightful comments. research supported german federal ministry education research projects sides bbdc software campus references martin abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing lukasz kaiser manjunath kudlur josh levenberg rajat monga sherry moore derek murray jonathon shlens benoit steiner ilya sutskever paul tucker vincent vanhoucke vijay vasudevan oriol vinyals pete warden martin wicke yuan xiaoqiang zheng. tensorflow largescale machine learning heterogeneous distributed systems karl moritz hermann tom´aˇs koˇcisk´y edward grefenstette lasse espeholt mustafa suleyman phil blunsom. teaching machines read comprehend. nips adam trischler tong wang xingdi yuan justin harris alessandro sordoni philip bachman kaheer suleman. newsqa machine comprehension dataset. arxiv. section explain connection weighted word-in-question feature deﬁned term frequency word occurring question context respectively. facilitate analysis repeat equations point dencies. limitation mainly information bottleneck posed ﬁxed size internal state. hence hard proposed baseline model answer questions require synthesizing evidence different text passages. passages typically connected co-referent entities events. consider following example newsqa dataset correctly answer question representations rochester york contain information refers brittanee drexel. connection example established mention mother co-referent mention mom. fusing information context representation hmom hmother allows crucial information mentioning brittanee drexel close correct answer. enable model co-referring mentions normalized similarity measure context state retrieve co-state using ﬁnally fuse representations state respective co-state representations gated addition call procedure associative representation fusion. introduce recurrent representation fusion sequentially propagate information gathered associative fusion neighbouring tokens e.g. representation mother containing additional information brittanee drexel representations rochester york. achieved recurrent derived formula shows wiqw context word would become simple combination term frequencies within context question similarity score redeﬁned note holds true ﬁnite value chosen real world queries bing search engine human generated answers based relevant documents. focus extractive question answering work limit queries training queries whose answers directly extractable given documents. found queries fall category. evaluation however performed entire development test respectively makes impossible answer subset yes/no questions properly. sake simplicity concatenate given paragraphs treat single document. since queries msmarco lower-cased also lower-cased context. ofﬁcial scoring measure msmarco generative models rouge-l bleu-. even though model extractive extracted answers generated. results shown table strong performance purely extractive system generative msmarco dataset notable. shows answers bing queries mostly extracted directly documents without need complex generative approach. since initial experiment generative using extractive methodology used training prepost-processing dataset models especially wang jiang unclear comparability systems limited. representation fusion question context states similar intra-fusion procedure. applied context representations intra-fusion employed. associative fusion performed attention weights question states context states ˜hj). co-state computed context state note softmax normalization applied context tokens question word close zero context positions therefore co-state close zero-vector. therefore question related context states receive non-empty co-state. rest inter-fusion follows procedure intra-fusion resulting context representations serve input answer layer. existing interaction layers typically combine representations retrieved attention concatenation feed input additional approach considered light-weight version interaction. although system designed answer question extracting answers given context also employed generative question answering datasets microsoft machine reading comprehension msmarco contains", "year": 2017}