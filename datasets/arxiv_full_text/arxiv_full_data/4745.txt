{"title": "On the Use of Non-Stationary Policies for Stationary Infinite-Horizon  Markov Decision Processes", "tag": ["cs.LG", "cs.AI"], "abstract": "We consider infinite-horizon stationary $\\gamma$-discounted Markov Decision Processes, for which it is known that there exists a stationary optimal policy. Using Value and Policy Iteration with some error $\\epsilon$ at each iteration, it is well-known that one can compute stationary policies that are $\\frac{2\\gamma}{(1-\\gamma)^2}\\epsilon$-optimal. After arguing that this guarantee is tight, we develop variations of Value and Policy Iteration for computing non-stationary policies that can be up to $\\frac{2\\gamma}{1-\\gamma}\\epsilon$-optimal, which constitutes a significant improvement in the usual situation when $\\gamma$ is close to 1. Surprisingly, this shows that the problem of \"computing near-optimal non-stationary policies\" is much simpler than that of \"computing near-optimal stationary policies\".", "text": "consider inﬁnite-horizon stationary γ-discounted markov decision processes known exists stationary optimal policy. using value policy iteration error iteration well-known compute stationary policies ǫ-optimal. arguing guarantee tight develop variations value policy iteration computing non-stationary policies ǫ-optimal constitutes signiﬁcant improvement usual situation close surprisingly shows problem computing near-optimal non-stationary policies much simpler computing near-optimal stationary policies. given inﬁnite-horizon stationary γ-discounted markov decision process consider approximate versions standard dynamic programming algorithms policy value iteration build sequences value functions policies follows arbitrary bellman optimality operator value policy policies greedy respect iteration term accounts possible approximation bellman operator evaluation throughout paper assume error terms satisfy kǫkk∞ assumption well-known algorithms share following performance bound api) theorem loss running policy instead optimal policy satisﬁes particular close consequently constant bound commonly believed conservative practical applications. interestingly constant appears many works analyzing algorithms algorithms generalization constant) tight suggesting cannot improved. indeed bound show section knowledge never argued literature also tight avi. even though theory optimal control states exists stationary policy optimal main contribution paper show looking non-stationary policy lead much better performance bound. section show deduce non-stationary policy avi. section describe original policy iteration variations compute non-stationary policies. algorithms prove performance bound reduced better standard bound theorem signiﬁcant close surprisingly show problem computing near-optimal non-stationary policies much simpler computing near-optimal stationary policies. present contributions next section begins precisely describing setting. consider inﬁnite-horizon discounted markov decision process possibly inﬁnite state space ﬁnite action space probability kernel reward function bounded max-norm rmax discount factor. stationary deterministic policy maps states actions. write immediate reward stochastic kernel associated policy value policy function mapping states expected discounted rewards received following state value clearly bounded vmax rmax/. well-known characterized unique ﬁxed point linear bellman operator associated policy γpπv. similarly bellman optimality operator maxπ unique ﬁxed point optimal value maxπ policy greedy w.r.t. value function greedy policies written finally policy optimal value equivalently tπ∗v∗ though known always exists deterministic stationary policy optimal will article consider non-stationary policies introduce related notations. given sequence stationary policies denote periodic non-stationary policy takes ﬁrst action according second according according πk−m+ starts again. formally written bound theorem tight sense exists bound reached. best knowledge similar argument never provided literature. turns used showing tightness also applies avi. show section. example consider γ-discounted deterministic depicted figure involves states state self-loop action zero reward state possible choices either move state zero reward stay fact equality implies exists policy greedy takes optimal move action states stay action value leaving algorithm possibility choosing suboptimal stay action state yielding value vπk+ matching upper bound goes inﬁnity. since example shows bound theorem tight improving performance bounds imply modify algorithms. following sections paper shows considering non-stationary policies instead stationary policies interesting path follow. usually considered generating sequence values also implicitely produces sequence policies instead outputing last policy simply propose output periodic non-stationary policy loops last generated policies. following theorem shows indeed good idea. theorem iteration loss running non-stationary policy instead optimal policy satisﬁes tends inﬁnity exactly recovers result theorem general bound factor better standard bound theorem choice optimizes bound consists looping policies generated start leads following bound rest section devoted proof theorem important step proof lies following lemma implies sufﬁciently rather good approximation value vπkm non-stationary policy lemma given sequence value functions induce many sequences policies since greedy policy exist particular value function. results holds possible choices greedy policies. present similar results policy iteration ﬂavour. unlike previous section output needed changed improving bound api-like algorithm slightly involved. section describe analyze algorithms output non-stationary policies improved performance bounds. non-stationary policy growing period following ﬁndings non-stationary policies consider following variation iteration instead computing value last stationary policy compute periodic non-stationary policy loops policies generated start initial policy chosen arbitrarily. thus iteration iteration nonstationary policy made stationary policies refer growing period. prove following performance bound algorithm though improved asymptotic performance bound algorithm described drawbacks ﬁnite iteration bound somewhat unsatisfactory term form γkvmax even error cannot guarantee that similarly standard policy iteration generates sequence policies increasing values points motivate introduction another algorithm. initial non-stationary policy built sequence arbitrary stationary policies unlike previous algorithm non-stationary policy involves last greedy stationary policies instead them thus ﬁxed period. strict generalization standard algorithm coincides algorithm prove following performance bound theorem loss running non-stationary policy instead optimal policy satisﬁes rest section develops proof performance bound. central argument proof following lemma shows similarly standard algorithm policy improvement property. lemma iteration algorithm value vπk+m non-stationary policy differs πk+m every steps chooses oldest policy πk−m+ policy takes ﬁrst action instead newest πk+. also according πk−m+ runs πkm; equivalently since loops πkπk− πk−m+ πk−m+πkm seen -step right rotation πkm. error shows policy πk+m better rotation πkm. πk+m thus recover well-known policy improvement theorem standard recalled theorem standard performance bound computing approximately optimal stationary policy standard algorithms. arguing bound tight particular providing original argument proposed three dynamic programming algorithms output non-stationary policies performance bound signiﬁcantly reduced made think non-stationary policies lead better performance bounds. work author considers problems ﬁnite-horizon computes non-stationary policies performance bounds inﬁnite-horizon problems computes stationary policies performance bounds non-stationary policies computed context ﬁnite-horizon problems; fact nonstationary policies also useful inﬁnite-horizon stationary context knowledge completely new. best performance improvements obtained algorithms consider periodic nonstationary policies period grows inﬁnity thus require inﬁnite memory look like practical limitation. however proposed algorithm parameter allows make trade-off quality approximation amount conjecture asymptotic bound non-asymptotic bounds theorems tight. actual proof conjecture left future work. important recent works literature involve studying performance bounds errors controlled norms instead max-norm natural supervised learning algorithms used approximate evaluation steps api. since proof based componentwise bounds like pioneer works topic believe extension analysis norm analysis straightforward. last least important research direction plan follow consists revisiting many implementations building stationary policies turn algorithms look non-stationary policies study precisely analytically well empirically.", "year": 2012}