{"title": "Mode Regularized Generative Adversarial Networks", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.NE"], "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem.", "text": "†tong che∗ ‡yanran †§athul paul jacob †yoshua bengio ‡wenjie †montreal institute learning algorithms universit´e montr´eal montr´eal canada ‡department computing hong kong polytechnic university hong kong §david cheriton school computer science university waterloo waterloo canada {tong.cheap.jacobyoshua.bengio}umontreal.ca {csylicswjli}comp.polyu.edu.hk although generative adversarial networks achieve state-of-the-art results variety generative tasks regarded highly unstable prone miss modes. argue behaviors gans particular functional shape trained discriminators high dimensional spaces easily make training stuck push probability mass wrong direction towards higher concentration data generating distribution. introduce several ways regularizing objective dramatically stabilize training models. also show regularizers help fair distribution probability mass across modes data generating distribution early phases training thus providing uniﬁed solution missing modes problem. generative adversarial networks demonstrated potential various tasks image generation image super-resolution object generation video prediction objective train parametrized function maps noise samples samples whose distribution close data generating distribution. basic scheme training procedure train discriminator assigns higher probabilities real data samples lower probabilities generated data samples simultaneously trying move generated samples towards real data manifold using gradient information provided discriminator. typical setting generator discriminator represented deep neural networks. despite success gans generally considered hard train training instability sensitivity hyper-parameters. hand common failure pattern observed training gans collapsing large volumes probability mass onto modes. namely although generators produce meaningful samples samples often modes behind phenomenon missing modes problem widely conceived major problem training gans many modes data generating distribution represented generated samples yielding much lower entropy distribution less variety data generating distribution. issue subject several recent papers proposing several tricks architectures stabilize gan’s training encourage samples’ diversity. however argue general cause behind problems lack control discriminator training. would like encourage manifold samples produced generator move towards real data using discriminator metric. however even train discriminator distinguish manifolds control shape discriminator function manifolds. fact shape discriminator function data remedy problem propose novel regularizer training target. basic idea simple powerful addition gradient information provided discriminator want generator take advantage similarity metrics much predictable behavior norm. differentiating similarity metrics provide stable gradients train generator. combining idea approach meant penalize missing modes propose family additional regularizers objective. design metrics evaluate generated samples terms diversity modes distribution fairness probability mass. metrics shown robust judging complex generative models including well-trained collapsed ones. regularizers usually bring trade-off model variance bias. results shown that correctly applied regularizers dramatically reduce model variance stabilize training missing mode problem once positive least negative effects generated samples. also discuss variant regularized algorithm even improve sample quality compared dcgan baseline. goodfellow able generate interesting local structure globally incoherent images various datasets. mirza osindero enlarges gan’s representation capacity introducing extra vector allow generator produce samples conditioned beneﬁcial information. motivated this several conditional variants applied wide range tasks including image prediction normal wang gupta image synthesis text reed edge isola real-time image manipulation temporal image generation zhou berg saito matsumoto vondrick texture synthesis style transfer video stylization wand researchers also stretching gan’s limit generate higher-resolution photo-realistic images. denton initially apply laplacian pyramid framework generate images high resolution. level lapgan generator discriminator convolutional networks. alternative lapgan radford successfully designs class deep convolutional generative adversarial networks signiﬁcant improvements unsupervised image representation learning. another line work aimed improving gans feature learning including features latent space image space. motivation features different spaces complementary generating perceptual natural-looking images. perspective researchers distances learned features losses training objectives generative models. larsen combine variational autoencoder objective utilize learned features discriminator gans better image similarity metrics. shown learned distance discriminator great help sample visual ﬁdelity. recent literature also shown impressive results image super-resolution infer photo-realistic natural images upscaling factors ledig sønderby nguyen despite promising successes gans notably hard train. although radford provide class empirical architectural choices critical stabilize gan’s training would even better train gans robustly systematically. salimans propose feature matching technique stabilize gan’s training. generator required match statistics intermediate features discriminator. similar idea adopted zhao addition feature distances dosovitskiy brox found counterpart loss image space improves gan’s training stability. furthermore researchers make information spaces uniﬁed learning procedure dumoulin trains generator also encoder discriminator trained distinguish joint distributions image latent spaces produced either application encoder training data application generator latent prior. contrast regular training discriminator attempts separate distributions image space. parallelly metz stabilize gans unrolling optimization discriminator considered orthogonal work ours. work related vaegan terms training autoencoder jointly model. however variational autoencoder vaegan used generate samples whereas autoencoder based losses serves regularizer penalize missing modes thus improving gan’s training stability sample qualities. demonstrate detailed differences various aspects appendix training procedure viewed non-cooperative player game discriminator tries distinguish real generated examples generator tries fool discriminator pushing generated samples towards direction higher discrimination values. training discriminator viewed training evaluation metric sample space. generator take advantage local gradient provided discriminator improve itself namely move towards data manifold. take closer look root cause instabilities training gans. discriminator trained generated real examples. pointed goodfellow denton radford data manifold generation manifold disjoint equivalent training characteristic function close data manifold generation manifold. order pass good gradient information generator important trained discriminator produces stable smooth gradients. however since discriminator objective directly depend behavior discriminator parts space training easily fail shape discriminator function expected. exampledenton noted common failure pattern training gans vanishing gradient problem discriminator perfectly classiﬁes real fake examples around fake examples nearly zero. cases generator receive gradient improve itself. another important problem training gans mode missing. theory generated data real data come dimensional manifold discriminator help generator distribute probability mass missing modes near- probability generator samples areas appropriately concentrated towards regions closer however practice since manifolds disjoint tends near real data samples large modes usually much higher chance attracting gradient discriminator. typical model since modes similar values reason generator cannot collapse major modes. words since discriminator’s output nearly fake real data respectively generator penalized missing modes. compared objective generator optimization targets supervised learning stable optimization point view. difference clear optimization target generator learned discriminator. supervised models optimization targets distance functions nice geometric properties. latter usually provides much easier training gradients former especially early stages training. inspired observation propose incorporate supervised training signal regularizer discriminator target. assume generator generates samples sampling ﬁrst ﬁxed prior distribution space followed deterministic trainable transformation sample space together also jointly train encoder assume similarity metric data space ex∼pd regularizer data generating distribution. encoder trained minimizing reconstruction error. geometric intuition regularizer straight-forward. trying move generated manifold real data manifold using gradient descent. addition gradient provided discriminator also match manifolds geometric distances metric. idea adding encoder equivalent ﬁrst training point point mapping manifolds trying minimize expected distance points manifolds. addition metric regularizer propose mode regularizer penalize missing modes. traditional gans optimization target generator empirical missing mode problem caused conjunction facts areas near missing modes rarely visited generator deﬁnition thus providing examples improve generator around areas missing modes nonmissing modes tend correspond high value generator perfect discriminator take strong decisions locally obtain high value even near non-missing modes. example consider situation figure gradient generator pushes generator towards major mode close mode generator gradients push towards minor mode however possible zero probability prior distribution given observation consider regularized model metric regularizer. assume minor mode data generating distribution. know good autoencoder located close mode since sufﬁcient training examples mode training data mode regularizer ex∼pd optimization target generator encourage move towards nearby mode data generating distribution. achieve fair probability mass distribution across different modes. large scale datasets celeba example regularizers discussed improve diversity generated samples quality samples good without carefully tuning hyperparameters. propose algorithm training metric-regularized gans stable much easier tune producing good samples. proposed algorithm divides training procedure gans steps manifold step diffusion step. manifold step match generation manifold real data manifold help encoder geometric metric loss. diffusion step distribute probability mass generation manifold fairly according real data distribution. example manifold-diffusion training follows train discriminator separates samples data optimize respect regularized loss e)+λd)] order match manifolds. diffusion step train discriminator distributions train maximize since distributions nearly dimensional manifold discriminator provides much smoother stable gradients. detailed training procedure given appendix figure quality generated samples. denotes sample softmax output trained classiﬁer labels overall label distribution generated samples. intuition behind score strong classiﬁer usually high conﬁdence good samples. however inception score sometimes good metric purpose. assume generative model collapse image. although model perfect inception score high entropy entropy. instead labelled datasets propose another assessment visual quality variety samples mode score distribution labels training data. according human evaluation experiences mode score successfully measures important aspects generative models i.e. variety visual quality metric. however datasets without labels labels sufﬁcient characterize every data mode metric work well. instead train third party discriminator real data generated data model. similar discriminator used train generator. view output discriminator estimator quantity proof) probability density generator density data distribution. prevent learning perfect separation inject zero-mean gaussian noise inputs training training test test real dataset. test sample discrimination value close conclude mode corresponding missing. although cannot measure exactly number modes missing good estimator total probability mass missing modes. perform classes experiments mnist. mnist dataset assume data generating distribution approximated dominant modes deﬁne term mode connected component data manifold. order systemically explore effect proposed regularizers models terms improving stability sample quality large scale grid search different hyper-parameters mnist dataset. grid search based pair randomly selected loss weights hyper-parameter settings regularized list search ranges table grid search similar proposed zhao please refer detailed explanations regarding hyper-parameters. evaluation ﬁrst train -layer classiﬁer mnist digits apply compute mode scores generated samples models. resulting distribution mode score shown figure clearly proposed regularizer signiﬁcantly improves mode scores thus demonstrates beneﬁts stabilizing gans improving sample qualities. figure different hyperparameters mnist generation. values regularized listed corresponding samples. best samples grid search regularized gan. order quantitatively study effect regularizers missing modes concatenate three mnist digits number single image train dcgan baseline model modes dataset. digits image sampled different table results compositional mnist modes. proposed regularization allows substantially reduce number missed modes well divergence measures plausibility generated samples performances compositional experiment measured metrics. miss represents classiﬁer-reported number missing modes size numbers model never generates. stands divergence classiﬁer-reported distribution generated numbers distribution numbers training data results shown table help proposed regularizer number missing modes divergence drop dramatically among sets compositional mnist dataset proves effectiveness regularizer preventing missing modes problem. test effectiveness proposal harder problems implement encoder dcgan algorithm train model different hyper-parameters together dcgan baseline celeba dataset. provide detailed architecture regularized dcgan appendix also employ third party discriminator trained injected noise metric missing mode estimation. implement this noise input layer discriminator network. model estimated independently train noisy discriminator mode estimator architecture hyper-parameters generated data training data. apply mode estimator test data. images high mode estimator outputs viewed missing modes. table number images missing modes celeba estimated third-party discriminator. numbers brackets indicate dimension prior denotes standard deviation added gaussian noise applied input discriminator regularize mdgan achieves high reduction number missing modes comparison methods comparison result shown table proposed regularized-gan mdgan outperform baseline dcgan models settings. especially mdgan suppresses models showing superiority modes preserving. also that although sharing architecture dcgan -dimensional noise performs quite worse -dimensional noise input. contrary regularized performs consistently. better understanding models’ performance want ﬁgure models miss modes. visualizing test images associated missed modes instructive. figure left three images missed models. rare training data second image type background third thus viewed small modes situation. three images considered hardest test data learn. nonetheless best model mdgan still capture certain small modes. seven images right figure missed dcgan. sideface paleface black berets special attributes among images proposed mdgan performs well them. quantitative evaluation manually examine generated samples regularized whether proposed regularizer side-effects sample quality. compare model vaegan dcgan terms sample visual quality mode diversity. samples generated models shown figure figure samples generated different generative models. compared model directly take decent samples reported corresponding papers code repositories. note mdgan samples globally coherent locally sharp textures. mdgan regularized-gan generate clear natural-looking face images. although ali’s samples plausible sightly deformed comparison mdgan. samples vaegan dcgan seem globally less coherent locally less sharp. sample quality worth noting samples mdgan enjoy fewer distortions. four models majority generated samples suffer sort distortion. however samples generated mdgan level distortion lower compared four compared models. attribute help autoencoder regularizer alter generation manifolds. generator able learn ﬁne-grained details face edges. result mdgan able reduce distortions. fair comparison also recommend readers refer original papers dumoulin larsen radford reported samples compared. samples https//github.com/ishmaelbelghazi/ali/blob/master/paper/celeba_ samples.png reverted original size. dcgan samples https //github.com/newmu/dcgan_code/ terms missing modes problem instructed individuals conduct human evaluation generated samples. achieve consensus mdgan wins terms mode diversities. people pointed mdgan generates larger amount samples side faces models. select several side face samples figure clearly samples maintain acceptable visual ﬁdelity meanwhile share diverse modes. combined quantitative results convincing regularizers bring beneﬁts training stability mode variety without loss sample quality. although gans achieve state-of-the-art results large variety unsupervised learning tasks training considered highly unstable difﬁcult sensitive hyper-parameters while missing modes data distribution even collapsing large amounts probability mass modes. successful training usually requires large amounts human computing efforts tune hyper-parameters order stabilize training avoid collapsing. researchers usually rely experience published tricks hyper-parameters instead systematic methods training gans. provide systematic ways measure avoid missing modes problem stabilize training proposed autoencoder-based regularizers. idea geometric metrics provide stable gradients trained discriminators combined encoder used regularizers training. regularizers also penalize missing modes encourage fair distribution probability mass generation manifold. thank naiyan wang jianbo yuchen ding saboya yang support. also want thank huiling zhen helpful discussions junbo zhao providing details grid search experiments ebgan model well anders boesen lindbo larsen kindly helping running vaegan experiments. appreciate valuable suggestions comments anonymous reviewers. work described paper partially supported nserc calcul quebec compute canada canada research chairs cifar national natural science foundation china research grants council hong kong hong kong polytechnic university references emily denton soumith chintala fergus deep generative image models using laplacian pyramid adversarial networks. advances neural information processing systems vincent dumoulin ishmael belghazi poole alex lamb martin arjovsky olivier mastropietro aaron courville. adversarially learned inference. arxiv preprint arxiv. goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems christian ledig lucas theis ferenc husz´ar jose caballero andrew aitken alykhan tejani johannes totz zehan wang wenzhe shi. photo-realistic single image super-resolution using generative adversarial network. arxiv preprint arxiv. nguyen jason yosinski yoshua bengio alexey dosovitskiy jeff clune. plug play generative networks conditional iterative generation images latent space. arxiv preprint arxiv. jiajun chengkai zhang tianfan william freeman joshua tenenbaum. learning probabilistic latent space object shapes generative-adversarial modeling. neural information processing systems jun-yan philipp kr¨ahenb¨uhl shechtman alexei efros. generative visual manipulation natural image manifold. proceedings european conference computer vision similar architectures compositional mnist celeba experiments. architecture based found dcgan radford apart discriminator generator dcgan encoder inverse generator reversing order layers replacing de-convolutional layers convolutional layers. particular attention batch normalization layers. dcgan batch normalization layers generator discriminator. however classes data batch normalization layers generator. come sampled noise come encoder. implementation separate batch statistics classes data generator keeping parameters layer shared. batch statistics kinds batches cannot interfere other. data sampled mixture gaussians standard derivation means gaussians placed around circle radius generator network relu hidden layers neurons. generates output samples uniform noise discriminator consists fully connected layer relu neurons mapping input real number. networks optimized adam optimizer learning rate regularized version choose comparison generator distribution standard proposed regularized shown figure figure comparison results mixture gaussians dataset. columns left shows heatmaps generator distributions number training epochs increases whereas rightmost column presents target original data distribution. shows standard result. generator hard time oscillating among modes data distribution able recover single data mode once. contrast bottom shows results regularized gan. generator quickly captures underlying multiple modes target distribution. appendix section demonstrate effectiveness uniqueness mode-regularized gans proposed paper compared larsen terms theoretical difference sample quality number missing modes. regard theoretical difference optimization vaegan relies probabilistic variational bound namely kl||p). variational bound together loss optimized several assumptions imposed vaegan ﬁrst assumption necessarily hold gans. found trained models dcgans real posterior even guaranteed mode mention anything close factorized gaussian. believe difference probabilistic framework essential obstacle tries objective vaegan regularizer. however algorithm plain auto-encoder instead objective. plain auto-encooders works better purposes long model able generate training samples always exists function encoder therefore viewed trained approximate real encoder conﬂicts good generator regularization objective. hence objectives used regularizers encoding prior knowledge good models able generate training samples. work essentially different vaegan. experiments also believe reason vaegan generates worse samples carefully tuned regularized gans. terms sample quality missing modes ofﬁcial code vaegan default setting. train vaegan epochs models epochs. fairness generated samples shown figure obvious difference samples vaegan’s samples face distortion consistent experimental results section conjecture distortions vaegan’s samples conﬂicts between objectives present above. words introduce auto-encoders regularizers models different vaegan’s. difference second assumption mentioned required approaches. framework auto-encoders helps alter generation manifolds leading fewer distortions ﬁne-grained details generated samples. figure samples generated models vaegan. third line samples generated self-trained vaegan model default settings. last line generated samples reported original vaegan paper. depict fair comparison. table number images missing modes celeba estimated third-party discriminator. numbers brackets indicate dimension prior denotes standard deviation added gaussian noise applied input discriminator regularize mdgan achieves high reduction number missing modes comparison vaegan. using proposed regularizers results huge drop number missing modes. conjecture reason vaegan performs metric missing modes samples generated quality discriminator classiﬁes samples mode. namely data generated away many real data modes. essentially model generates samples model misses modes. conduct fair evaluation vaegan methods also perform blind human evaluation. instructed individuals conduct evaluation sample variability. without telling generated vaegan generated methods four people agree method wins terms sample diversity. person thinks samples equally diverse. conclusion demonstrate proposed mode-regularized gans i.e. reg-gan mdgan different vaegan theoretically discussed above. differences empirically result better sample quality mode preserving ability main contributions.", "year": 2016}