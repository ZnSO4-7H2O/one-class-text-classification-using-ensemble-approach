{"title": "TerpreT: A Probabilistic Programming Language for Program Induction", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "We study machine learning formulations of inductive program synthesis; given input-output examples, we try to synthesize source code that maps inputs to corresponding outputs. Our aims are to develop new machine learning approaches based on neural networks and graphical models, and to understand the capabilities of machine learning techniques relative to traditional alternatives, such as those based on constraint solving from the programming languages community.  Our key contribution is the proposal of TerpreT, a domain-specific language for expressing program synthesis problems. TerpreT is similar to a probabilistic programming language: a model is composed of a specification of a program representation (declarations of random variables) and an interpreter describing how programs map inputs to outputs (a model connecting unknowns to observations). The inference task is to observe a set of input-output examples and infer the underlying program. TerpreT has two main benefits. First, it enables rapid exploration of a range of domains, program representations, and interpreter models. Second, it separates the model specification from the inference algorithm, allowing like-to-like comparisons between different approaches to inference. From a single TerpreT specification we automatically perform inference using four different back-ends. These are based on gradient descent, linear program (LP) relaxations for graphical models, discrete satisfiability solving, and the Sketch program synthesis system.  We illustrate the value of TerpreT by developing several interpreter models and performing an empirical comparison between alternative inference algorithms. Our key empirical finding is that constraint solvers dominate the gradient descent and LP-based formulations. We conclude with suggestions for the machine learning community to make progress on program synthesis.", "text": "study machine learning formulations inductive program synthesis; given input-output examples would like synthesize source code maps inputs corresponding outputs. aims work develop machine learning approaches problem based neural networks graphical models understand capabilities machine learning techniques relative traditional alternatives based constraint solving programming languages community. contribution proposal terpret domain-speciﬁc language expressing program synthesis problems. terpret similar probabilistic programming language model composed speciﬁcation program representation interpreter describes programs inputs outputs inference task observe input-output examples infer underlying program. terpret main beneﬁts. first enables rapid exploration range domains program representations interpreter models. second separates model speciﬁcation inference algorithm allowing proper like-to-like comparisons diﬀerent approaches inference. single terpret speciﬁcation automatically perform inference using four diﬀerent back-ends include machine learning program synthesis approaches. based gradient descent linear program relaxations graphical models discrete satisﬁability solving sketch program synthesis system. illustrate value terpret developing several interpreter models performing extensive empirical comparison alternative inference algorithms variety program models. perhaps surprising empirical ﬁnding constraint solvers dominate gradient descent lp-based formulations. conclude suggestions machine learning community make progress program synthesis. learning computer programs input-output examples inductive program synthesis fundamental problem computer science dating back least summers biermann ﬁeld produced many successes perhaps visible example flashfill system microsoft excel learning examples also studied extensively statistics machine learning communities. trained decision trees neural networks could considered synthesized computer programs would stretch label such. relative traditional computer programs models typically lack several features functional properties missing like ability interact external storage compact interpretable source code representation learned model explicit control absence precise control particular hindrance lead poor generalization. example whereas natural computer programs often built inductive bias control statements ensuring correct execution inputs arbitrary size models like recurrent neural networks struggle generalize short training instances instances arbitrary length. several models already proposed start address functional diﬀerences neural networks computer programs. include recurrent neural networks augmented stack queue memory neural turing machines memory networks neural gpus neural programmer-interpreters neural random access machines models combine deep neural networks external memory external computational primitives and/or built-in structure reﬂects desired algorithmic structure execution. furthermore shown trainable gradient descent. however absences noted above. first none models produce programs output. representation learned model interpretable source code. instead program hidden inside controllers composed neural networks decide operations perform learned program understood terms executions produces speciﬁc inputs. second still concept explicit control models. works raise questions whether models designed speciﬁcally synthesize interpretable source code contain looping branching structures whether searching program space using techniques developed training deep neural networks useful alternative combinatorial search methods used traditional ips. work make several contributions directions. address ﬁrst question develop models inspired intermediate representations used compilers like llvm trained gradient descent. models address deﬁciencies highlighted beginning section interact external storage handle non-trivial control explicit statements loops appropriately discretized learned model expressed interpretable source code. note concurrent works adaptive neural compilation diﬀerentiable forth implement similar ideas. design choice creating diﬀerentiable representations source code eﬀect inductive bias model diﬃculty resulting optimization problem. therefore seek rapidly experimenting diﬀerent formulations allow explore full space modelling variations. address second question concerning eﬃcacy gradient descent need specifying problem gradient based approach compared variety alternative approaches like-for-like manner. alternative approaches originate rich history programming languages community rich literature techniques inference discrete graphical models machine learning community. knowledge comparison previously performed. questions demand explore range model variants range search techniques models. answer issues same terpret probabilistic programming language specifying problems. terpret provides means describing execution model deﬁning parameterization interpreter maps inputs outputs using parametrized program. terpret description independent particular inference algorithm. task infer execution model parameters given execution model pairs inputs outputs. perform inference terpret automatically compiled intermediate representation particular inference algorithm. interpretable source code obtained directly inferred model parameters. driving design principle terpret strike subtle balance breadth expression needed precisely capture range execution models gradient descent based approach generalizes approach used kurach novel linear program relaxation approach based adapting standard linear program relaxations support gates translation problem ﬁrst-order logical formula existential constraints. terpret currently four back-end inference algorithms listed table gradientdescent linear program relaxations sketch program synthesis system allow back-ends used regardless speciﬁed execution model requires generalizations extensions previous work. gradient descent case generalize approach taken kurach lifting discrete operations operate discrete distributions leads diﬀerentiable system. linear program case need extend standard relaxation discrete graphical models support statements. section show adapt ideas gates linear program relaxations commonly used graphical model inference could serve starting point work lp-based message passing approaches finally built terpret becomes possible develop understanding strengths weaknesses alternative approaches inference. understand limitations using gradient descent problems ﬁrst terpret deﬁne simple example gradient descent fails alternative back-ends solve easily. studying example better understand possible failure modes gradient descent. prove exponentially many local optima example show empirically arise often practice perform comprehensive empirical study comparing diﬀerent inference back-ends program representations. show domains signiﬁcantly diﬃcult gradient descent others show results suggesting gradient descent performs best given redundant overcomplete parameterizations. however overwhelming trend experiments techniques programming languages community outperform machine learning approaches signiﬁcant margin. terpret probabilistic programming language tailored back-end inference algorithms including techniques based gradient descent linear programming highly-eﬃcient systems programming languages community terpret also allows program figure diagram basic block program representation. empty boxes connections denote per-block unknown parameters ﬁlled inference algorithm. choice block else conditions also unknown parameters. domain unknown parameters described small blue text. assignment unknown parameters yields program. novel linear program relaxation handle statement structure common execution models generalization smoothing technique kurach work execution model expressible terpret. report arranged follows brieﬂy introduce ‘basic block’ model section discuss features terpret needs support allow modeling rich execution models. section describe core terpret language illustrate explore diﬀerent modeling assumptions using several example execution models. include turing machine boolean circuits risc-like assembly language basic block model. section describe compilation terpret models four back-end algorithms listed table quantitative experimental results comparing back-ends aforementioned execution models presented section finally related work summarized section discuss conclusions future work section motivating example diﬀerentiable control flow graphs introductory example describe execution model would like ips. section describe model high level. later sections describe express model terpret perform inference. control graphs representation programs commonly used static analysis compiler optimizations. consist basic blocks contain sequences instructions jumps followed jump conditional jump instruction transfer control another block. cfgs expressive enough represent constructs used modern programming languages like c++. indeed intermediate representation llvm based basic blocks. ﬁrst model inspired cfgs limited restricted instructions support function calls. refer model basic block model. illustration model appears fig. detail specify ﬁxed number blocks registers take values given ﬁxed instructions implement basic figure high level view program synthesis task. forward execution traditional program interpreter shown analogy forward mode reverse mode terpret system. arithmetic operations like increment less-than. external memory written read using special instructions read write. instruction pointer keeps track block currently executed. block single statement parameterized argument registers instruction executed register store output. statement executed condition checked branch taken. condition parameterized choice register check equality based upon result instruction pointer updated equal block else block. identities blocks parameterization branch decision. model always start execution block special block used denote termination. program executed ﬁxed maximum number timesteps represent inputoutput examples initial state external memory assert particular elements ﬁnal memory desired value upon termination. terpret case precisely describe execution model—how statements executed instruction pointer updated—in translated fully diﬀerentiable interpreter basic block language intermediate representation passing back-ends. next sections describe detail terpret execution models speciﬁed back-ends work. front-end describing problem central aims disentangle description execution model inference task perform like-for-like comparisons diﬀerent inference approaches task. reference components solving problem illustrated fig. forward mode system analogous traditional interpreter reverse mode system infers representation source code given observed outputs inputs. even devising inference method need means parameterizing source code program also precise description interpreter layer’s forward transformation. section describes modeling tasks achieved terpret. terpret probabilistic programming language full grammar syntactically correct terpret programs shown fig. describe semantic features language following sections. illustration running example simple automaton shown fig. example ‘source code’ parameterised boolean array ruletable take input ﬁrst values binary tape length {tape tape}. forward execution interpreter could described following simple python snippet given observed output tape inference consistent ruletable easy problem instructive analyse terpret implementation automaton following sections. sections describe variable declaration control user deﬁned functions handling observations terpret. declarations assignments allow declarations give names magic constants line fig. additionally allow declaration parameters variables ranging ﬁnite domain using param compile-time constant parameters used model source code inferred whereas variables used model computation convenience arrays variables declared using syntax accessed foo. similar syntax available params. arrays unrolled compilation unique symbols representing element passed inference algorithm i.e. require special support inference backend. reason dimensions dimi indices need compile-time constants. example variable declarations seen lines fig. assignments declared variables allowed usual assignment operator instead written var.set better distinguish assignments constant variables. static single assignment form enforced legal variable appear multiple times target statements assignment appears diﬀerent cases conditional block. restriction variable written once. however note programs perform multiple writes given variable always translated corresponding forms. control terpret supports standard control-ﬂow structures if-else for. addition terpret uses unique structure. need latter induced requirement compile-time constants accessing arrays. thus element tape example need code like following access values ﬁrst values tape intuitively snippet simply performs case analyses possible values tape tape. simplify pattern introduce stmt control-ﬂow structure allows automate unrolling avoid back-ends require possible possible values determined with-statement transformed then stmt; elif then stmt; ...elif then stmt]; stmt denotes statement stmt terpret loops take shape range stmt compile-time constants. similar statement unroll loops explicitly compilation thus values generate stmt; stmt]; ...; stmt]. using statements thus describe evaluation example automaton const timesteps shown lines fig. operations terpret supports user-deﬁned functions facilitate modelling interpreters supporting non-trivial instruction sets. example apply function arguments arg. .argm function deﬁned standard python function additional decoration compileme specifying domains input output variables. modelling inputs outputs using statements preceding sections execution model fully speciﬁed connect model input/output observations drive program induction. statements constant model program input thus single input-output observation running example could written terpret follows. keep execution model observations separate store observation snippets separate preprocessor directives import observed pull appropriate snippets compilation also allow constant literals stored separately terpret execution model import values using preprocessor directives form hyperparam general want infer programs nobs input-output examples. simplest implementation achieves augmenting declaration additional array dimension size nobs wrapping execution model loop examples. examples outermost loops models appendix example execution models illustrate versatility terpret describe four example execution models. broadly speaking examples progress abstract execution models towards models closely resemble assembly languages risc machines. case present basic model three representative synthesis tasks table investigate. addition provide metrics diﬃculty task calculated minimal computational resources required solution. since diﬃculty synthesis problem generally depends chosen inference algorithm metrics primarily intended give sense scale problem. ﬁrst diﬃculty metric number structurally distinct programs would enumerated worst-case brute-force search second metric unrolled length steps synthesized program. automaton turing machine turing machine consists inﬁnite tape memory cells contain symbols head moves tape states execution step head unhalted state reads symbol current position tape writes symbol newvalue position moves direction speciﬁed direction adopts state newstate. source code turing machine entries control tables newvalue direction newstate conﬁgurations. move left right along tape invert binary symbols halting ﬁrst blank cell. insert symbol start tape shift symbols rightwards cell. halt ﬁrst blank cell. access element linked list. initial heap heap heap heap linklist linklist linked list represented heap adjacent pairs random order pointer head element linklist. terminate heap linklist.value. table overview benchmark problems grouped execution model. benchmark manually minimal feasible resources noted table automatically solve synthesis task minimal settings. straight-line programs boolean circuits complex model consider simple machine capable performing sequence logic operations registers holding boolean values. operation takes registers input outputs register reminiscent standard three-address code assembly languages. embed example realworld application analogies linking instruction electronic logic gates linking registers electronic wires drawn. analogy highlights beneﬁt interpretability model synthesized program describes digital circuit could easily translated real hardware terpret implementation execution model shown appendix loopy programs basic block model build loopy execution models take inspiration compiler intermediate languages modeling full programs graphs basic blocks. programs operate ﬁxed number registers byte-addressable heap store accessible special instructions read write. block instructions form regout instr regin regin followed branch decision regcond goto blockthen else goto blockelse representation easily transformed back forth higher-level program source code well executable machine code. instruction containing instructions zero lessthan read write noop. gives possible programs system registers basic blocks consider case registers heap memory cells store single data type integers range number memory cells heap. single data type allows intermediate values pointers heap represented registers heap cells. model focuses interpretability also builds observation results kurach nrams rnn-based controller chooses short sequence instructions execute next based observations current program state. however empirical evaluation reports correctly trained models usually picked sequence instructions ﬁrst step repeated another sequence program terminates. intuitively corresponds loop initialization followed repeated execution loop body something naturally expressed basic block model. loopy programs assembly model basic block model every expression followed conditional branch giving model great freedom represent rich control graphs. however useful programs often execute sequence several expressions branch. therefore beneﬁcial bias model create chains sequentially ordered basic blocks occasional branching necessary. achieved replacing basic blocks objects closely resemble lines assembly code. instruction augmented jump statements jump-if-zero branchaddr) jump-if-not-zero branchaddr) operation shown fig. line code acts like conditional branch assigned instr jnz} otherwise acts like single expression executes passes control next line code. assembly model express programs basic block model serves example design model aﬀects success program inference. back-ends solving problem terpret designed compiled variety intermediate representations handing diﬀerent inference algorithms. section outlines compilation steps back-end algorithms listed table back-end present compiler transfomation terpret primitives listed fig. back-ends useful present transformations intermediate graphical representation resembling factor graph speciﬁcally gated factor graph visualises terpret program. describe gated factor graphs provide mapping terpret syntax primitives models. section show compile terpret back-end solver. terpret gated factor graph description factor graph means representing factorization complex function probability distribution composition simpler functions distributions. graphs inputs outputs intermediate results stored variable nodes linked factor nodes describing functional relationships variables. terpret model deﬁnes structure factor graph inference algorithm used populate variable nodes values consistent observations. particular care needed describe factor graphs containing conditional branches since value variable conditions form known inference complete. means must explore branches inference. gated factor graphs used handle statements introduce additional terminology describe gated models below. throughout next sections refer terpret snippet shown fig. illustration. local unary marginal. restrict attention case variable discrete ﬁnite domain variable instantiate local unary marginal deﬁned support integral conﬁguration demand non-zero particular value inference techniques relax constraint consider continuous model relaxed models apply continuous optimization schemes which successful converge interpretable integral solution. following minka winn refer statements gates. precisely gates. statement consists condition body refer condition gate condition body gate body. work restrict attention cases gate conditions form constexpr. future work could relax restriction. gate path condition list variables values path conditions. need take order gate body executed. example fig. path condition innermost gate body lines commas denote conjunction. convention condition deepest gate’s statement last entry path condition. gates belong tree structure gate gate condition nested inside gate path condition parent path condition equally speak path condition factor path condition deeply nested gate factor contained figure interpreting terpret gated factor graph description. model inference task shown lines using terpret provide corresponding gated factor graph using symbols fig. solution inference task local marginal global scope drop superscript annotation refer parent-child relationships diﬀerent local marginals variable; local marginal parent gate parent local marginal gate marginal gate marginal gate’s condition gate marginals. parent gate fig. ﬁrst outer gate’s gate marginal second outer gate’s inner gate gate marginal gate forward marginals gradient descent back-end factor graphs discussed easily converted computation graphs representing execution interpreter following operations. strictly speaking notation handle case multiple gates identical path conditions; clearness notation assume gate path conditions unique. however implementation handles repeated path conditions fmgd approach initialize source nodes directed graph instantiating independent random variables param node variables onehot nodes associated distribution input observations form xi.set constant statement. fuzzy system distributions fully diﬀerentiable. therefore inference becomes optimization task maximize weight assigned observations updating parameter distributions {µp} gradient descent. fmgd approximation arises whenever derived variable depends several immediate input variables ungated graph occurs factor nodes fmgd operates approximation independent. case imagine local joint distribution constructed according deﬁnition independent unary marginal distributions distribution marginalize input variables obtain unary marginal propagated forward factor node correlations lost. next section explicitly deﬁne operations extend technique allow gates factor graph. worth noting spectrum approximations form joint distributions subgraphs size ranging single nodes full computation graph independent marginal distributions propagated subgraphs. moving spectrum trades computational eﬃciency accuracy correlations captured larger subgraphs. exploration spectrum could basis future work. forward marginals... fig. illustrates transformation graphical primitive allow diﬀerentiable forward propagation marginals factor graph. describe details factor gate primitives algorithm. factors. scope factor function contains immediate input variables immediate output restricted environment enumerate possible outputs |xi| possible input conﬁgurations form marginalise gates. include gates fmgd formulation follows. child inside gate subgraph gates controlled gate marginal described terpret code references active variables divide containing variables written-to execution containing variables written-to addition refer active variables variables used graph downstream gates paths terminate observed variables. i=xi subject restrictions factor functions. description valid condition xout xout domain variable used store output scenario condition could violated illustrated below function makesmall range contains elements outside xout however deterministic execution program encounter error path condition islarge guarantees invalid cases \\xout would never reached. general makes sense violate xout inside gate path condition ensures input values restricted domain xout. case simply enforce normalisation µout account leaked weight values xout. given random initialization marginals param variables techniques propagate marginals forwards terpret model reach variables associated observe value statement. cross entropy loss compare computed marginal observed value. reaches lower bound marginals representing params unit weight describe valid program explains single value points fmgd loss landscape. however fmgd approximation also lead local optima which encountered stall optimization uninterpretable point assigns weight several distinct parameter settings. reason several diﬀerent random initializations record fraction initializations converge global optimum. speciﬁcally approaches learning using model optimized fmgd. heuristics below inspired kurach designed avoid getting stuck local minima optimize hyperparameters heuristics random search. also include initialization scale gradient descent optimization algorithm random search setting parameters initialized uniformly simplex. setting smaller peakier distributions setting larger uniform distributions. gradient clipping. fmgd neural network depth grows linearly number time steps. mitigate exploding gradient problem globally rescaling whole gradient vector norm bigger hyperparameter value noise. added random gaussian noise computed gradients backpropagation step. following neelakantan decay variance noise training according following schedule ideally algorithm would explore loss surface global minimum rather entropy. ﬁxing particular conﬁguration early training process causing network stuck local minimum it’s unlikely leave. bias network away committing particular solution early iterations entropy bonus loss function. speciﬁcally softmax distribution network subtract entropy scaled coeﬃcient hyperparameter. coeﬃcient exponentially decayed rate another hyperparameter. fmgd uses logarithms computing cost function limiting values logarithms. well entropy. since inputs logarithms small lead values cost function ﬂoating-point arithmetic overﬂows. avoid problem replacing log) wherever logarithm computed small value curriculum learning. kurach used curriculum learning scheme involved ﬁrst training small instances given problem moving train larger instances error rate reduced certain value. benchmarks contain small number short examples less room curriculum learning helpful. manually experimented hand-crafted curricula hard problems lead improvements. explore hyperparameters optimization heuristics preliminary experiments manually chose distribution hyperparameter space random search hyperparameters. distribution broad enough disallow reasonable settings hyperparameters also narrow enough runs random search wasted parameter settings would never lead convergence. distribution hyperparameters ﬁxed random search experiments. linear program back-end turn attention ﬁrst alternative back-end compared fmgd. casting terpret program factor graph allows build upon standard practice constructing relaxations solving maximum posteriori inference problems discrete graphical models following sections describe apply techniques terpret models particular extend methods handle gates. relaxation inference problem phrased task ﬁnding highest scoring conﬁguration discrete variables xd−. score deﬁned local factor scores ×i∈sjxi joint conﬁguration space variables indices spanning scope factor simplest case alongside scoring functions build linear constraints overall linear objective function represent graphical model variables local unary associated marginals before local factor marginals factor ﬁnal constraints ﬁxed value variables marginalized local factor marginal result equal value local marginal assigns value ensures factors neighboring variables consistent local marginals. local marginals integral i.e. restricted becomes integer linear program corresponding exactly original discrete optimization problem. local marginals real-valued resulting guaranteed equivalent solution original problem fractional solutions appear. formally constraints deﬁne known local polytope outer approximation convex hull valid integral conﬁgurations local marginals case program synthesis fractional solutions problematic correspond discrete programs thus cannot represented source code executed instances. fractional solution found heuristics rounding cuts branch bound search must used order integral solution. linear constraints gated models extend relaxation cater models gates. gate instantiate local unary marginals factor path condition parent gate. main diﬀerence gate standard normalization constraints. normalization constraints handled. idea local marginal gate normalized gate marginal. thus local marginal gate path condition gate marginal factor local marginals. constraint enforcing local consistency factor local marginals unary local marginals augmented path condition superscripts figure summary construction mixed integer linear program gated factor graph. removing binary constraint parameters produces continuous relaxation. main text deﬁnition symbols. needs relationship diﬀerent local marginals parent-child consistency. variable. enforcing consistency parent-child local marginals. parent gate active need enforce consistency quite simple setting quantities equal; general multiple children gates active many them. children gates suppose active children. constraint thought setting parent local marginal weighted average children local marginals weights come children marginals capped corresponding gate marginal’s value. implies means must assign zero probability case active. removes possibility consideration clearly undesirable disjoint sets variables active diﬀerent children cases result infeasible solution instantiate ghost marginals local marginals variable case undeﬁned denote ghost marginal path condition entry value µx=∅ ghost marginals represent distribution values cases variable deﬁned normalization constraints deﬁned follows finally parent-child consistency constraints case variable active children. solution consider ghost marginal child cases. example constraint would following back-end core problem terpret induces simple linear integer constraint system. exploit mature constraint-solving systems implemented satisﬁability modulo theories back-end. this terpret instance translated constraints smt-lib standard standard solver called. translates terpret expressions smt-lib expressions integer variables shown fig. make unrolling techniques discussed earlier eliminate arrays loops statements. encountering function call part expression inlining i.e. replace call function deﬁnition formal parameters replaced actual arguments. means terpret statements expressed smt-lib expressions also means back-end supports small subset functions namely using terpret constructs. statement translated list constraints solution problem encoded terpret smt. together unrolling loops ﬁxed length approach reminiscent bounded model checking techniques given program search input shows behavior. instead take input given search program desired behavior. sketch back-end ﬁnal back-end consider based sketch program synthesis system allows programmers write partial programs called sketches leaving fragments unspeciﬁed holes. goal synthesizer automatically holes completed program conforms desired speciﬁcation. sketch system supports multiple forms speciﬁcations input-output examples assertions reference implementation etc. background. syntax sketch language similar language additional feature symbol represents unknown constant integer value. simple example sketch shown fig. represents partial program unknown integer simple assertion. harness keyword indicates synthesizer compute value complete function assertions satisﬁed input values example sketch synthesizer computes value expected. unknown integer values used encode richer hypothesis space program fragments. example sketch fig. uses integer hole describe space binary arithmetic operations. sketch language also provides succinct language construct specify expression choices rhs. sketch synthesizer uses counter-example guided inductive synthesis algorithm eﬃciently solve second order exists-forall synthesis constraint. idea algorithm divide process phases synthesis phase computes value unknowns ﬁnite input-output examples veriﬁcation phase checks current solution conforms desired speciﬁcation. current completed program satisﬁes speciﬁcation returns program desired solution. otherwise computes counter-example input violates speciﬁcation adds input-output examples continues synthesis phase. details cegis algorithm sketch found solar-lezama fig. present syntax-directed translation terpret compiling terpret sketch. language sketch. idea translation model param variables unknown integer constants synthesizer computes values parameters satisfy observation constraints. param integer value translation creates corresponding unknown integer value additional constraint unknown value less similarly param array values translation creates sketch array unknown integer values value constrained less statements translated assignment statements whereas observe statements translated assert statements sketch. user-deﬁned functions translated directly corresponding functions sketch whereas statements translated corresponding assignment statements. sketch translation terpret model fig. shown fig. analysis motivation work compare performance gradient based fmgd technique back-ends. present task back-ends solve easily fmgd found fail prevalence local optima. failure fmgd kurach neelakantan mention many random restarts careful hyperparameter search needed order converge correct deterministic solution. develop understanding loss surface arises using fmgd simpler setting believe sheds light local optima structure arises using fmgd generally. binary variables others unobserved. parity neighboring variables connected ring shape. suppose observed goal infer values terpret program follows refer parity chain model clearly optimal conﬁguration show analytically exponentially many suboptimal local optima fmgd fall into experimentally probability falling suboptimal local optimum grows quickly show exponentially many local optima give technique enumerating show gradient equal each. letting mi+exp main observation locally conﬁguration model parameters gives rise zero gradient conﬁguration implies conﬁguration sequence form also gives rise zero gradients involved m’s. choose conﬁguration alternating remaining values create local optimum. rule otherwise. create islands boundaries islands conﬁguration islands local optimum least conﬁgurations. formal proof appears appendix might wonder local optima arise practice. initialize randomly encounter suboptimal local optima? experiments section show answer yes. local optima avoided small models using optimization heuristics gradient noise opaask fask··· aksk) bsk) &&bsk ||bsk) opcask ssk;ssk =ask; =csk; assertask ==ask; returnask; bsk) {ssk} else {ssk} forcsk; v++) {ssk} forcsk; <csk; v++) {ssk} {int =ask;ssk} sdsk; sdsk =csk; intcsk]cksk] assert <csk; intcsk]cksk] ∈csk··· ∈cksk assert <csk; models grow larger able conﬁguration hyperparameters could solve problem random initialization. inference algorithms solve problems easily. example relaxation section lead integral solutions tree-structured graphical models case here. parity chain experiments provide empirical counterpart theoretical analysis previous section. speciﬁcally showed exponentially many local optima fmgd fall parity chain model necessarily mean local optima encountered practice. conceivable large basin attraction around global optimum smaller negligible basins attraction around suboptimal local optima. answer question vanilla fmgd random initialization parameters chosen initial parameters drawn uniformly simplex. measuring fraction runs converge global optimum gives estimate volume parameter space falls within basin attraction global optimum. results chain lengths appear vanilla fmgd table fmgd able solve small instances reliably performance quickly falls grows. shows basins attraction suboptimal local optima large. next optimization heuristics discussed section chain length draw random hyperparameter settings manually chosen hyperparameter distribution. hyperparameter setting runs diﬀerent random seeds measure fraction runs converge global optimum. best hypers table report percentage successes hyperparameter setting yielded best results. average hypers report percentage success across runs. note success rate best hypers anomaly able setting hyperparameters random initialization little eﬀect successful runs followed nearly identical learning trajectories. fig. plot optimization objective versus epoch. successful runs colored blue unsuccessful ones red. large cluster successful runs hyperparameter settings. experiments turn attention experimental results. primary better understand capabilities diﬀerent back-ends range problems establish trends regarding performance back-ends problem properties varied. benchmarks results present main results investigation benchmarking four inference techniques listed table twelve synthesis tasks listed table described section tasks split across four execution models increasing practicality model three tasks increasing diﬃculty. given task ensure fair test presenting four back-end compilers terpret program input-output examples. since diﬀerent back-ends diﬀerent approaches solve tasks comparable metric record wall time reach solution. exception fmgd algorithm timeout hours back-end task give algorithm single solution tune algorithms speciﬁc task. fmgd algorithm vanilla optimized form. vanilla case report fraction diﬀerent random initializations lead globally optimal solution also wall clock time epochs gradient descent algorithm optimized fmgd case follow similar protocol previous section allow training epochs. manually chosen distribution hyperparameters perform random search drawing random settings hyperparameters. setting learning diﬀerent random initializations. listk tasks runtime long fewer settings hyperparameters before optimized case report success rate best hyperparameters found also average across runs random search. back algorithm. clear tendency traditional techniques employing constraint solvers outperform machine learning methods sketch system able solve benchmarks timeout nevertheless machine learning methods qualitatively appealing properties. firstly primarily optimizers rather solvers additional terms could added cost function optimization programs desired properties resource usage). secondly fmgd makes synthesis task fully diﬀerentiable allowing incorporation larger end-to-end diﬀerentiable system encourages persevere analysis fmgd technique particular study surprising failure method simple boolean circuit benchmarks section table highlights precise formulation interpreter model interpreter models. aﬀect speed synthesis. basic block assembly models equally expressive assembly model biased towards producing straight line code minimal branching. cases synthesis successful assembly representation seen outperform basic block model terms synthesis time. anomaly optimized fmgd algorithm able solution decrement task using basic block model assembly model. could minimal solution program shorter basic block architecture assembly model observe section increasing size model adding superﬂuous resources help fmgd algorithm converge global optimum. however generally synthesis diﬃcult minimal solution already large. table benchmark results. fmgd present time seconds epochs success rate random restarts {vanilla best hypers average hypers} columns respectively. back-ends present time seconds produce synthesized program. symbol indicates timeout failure random restart converge. number provided input-output examples used specify task case. zooming fmgd boolean circuits stark contrast performance fmgd alternatives boolean circuit problems. controlled shift full adder benchmarks fmgd took long back-end. this runs random search. however successful runs. slow convergence runs converged local optimum epochs allocated cases not. thus decided allocate algorithm many epochs random search over. produce successes although few. controlled shift problem runs converged full adder runs converged. thus appear results could improved somewhat running fmgd longer. however given long runtimes fmgd relative sketch back-ends would change qualitative conclusions previous section. varying problem dimension take inspiration neural network literature approaches issue stagnation local minima increasing dimension problem. argued local optima become increasingly rare neural network loss surfaces dimension hidden layers increase instead saddle points become increasingly common exchanging local minima saddle points beneﬁcial dynamic learning rate schedules rmsprop eﬀective handling saddle points plateaus loss function. assess dimensionality aﬀects fmgd ﬁrst take minimal example boolean circuit domain task synthesizing nand gate. minimum solution task shown fig. along example conﬁguration resides local minimum fmgd loss surface. synthesis task involving gates wires total independent degrees freedom optimized global optimum. increasing available resources three gates three wires gives optimization problem dimensions several global minima. contrast learning trajectories cases shown fig. attempt infer presence saddle points exploring loss surface vanilla gradient descent small learning rate. temporary stagnation learning indication saddle-like feature. features clearly frequently encountered higher dimensional case also observe greater overall success rate observations consistent intuition dauphin suggesting success benchmark tasks provide resources required solve task. perform experiment full adder benchmark varying number wires gates used synthesized solution. results fig. show expected trend synthesis becoming successful number redundant resources increases minimal wires gates. furthermore clear increase expected time fmgd arrive solution increase problem size dramatically diﬀerent trend seen applying constraint solvers synthesis problem broadly speaking increasing number constraints problem increases time solution feature fmgd particularly interesting minimal resources required solve problem known synthesis. whereas over-provisioning resources usually harm back-ends consider help fmgd. discovered programs post-processed recover eﬃciency challenge benchmark leaving section note sketch solved benchmark tasks. provide goal future work introduce ﬁnal benchmark none back-ends currently able figure comparison learning trajectories increase resources available synthesizer. minimal solution producing nand gate locally optimal conﬁguration found fmgd plot trajectory random initializations learning highlighting successful runs blue failures red. adding extra wire gate changes learning trajectories signiﬁcantly introducing plateaus indicative saddle-points loss function. rmsprop rather navigate features figure eﬀect increasing dimension fmgd synthesis problem. vary number wires gates used synthesis full adder citcuit present percentage random initializations converge global solution expected time solution solution time sketch—backend shown comparison. figure comparison minimal full adder solution synthesized solution redundant resources provided. gates highlighted orange removed post-processing without aﬀecting output. merge contiguous sorted lists contiguous sorted list. ﬁrst entries initial heap heap heap heap pout pointer ﬁrst sorted sublist pout pointer head desired output list. elements sorted sublists larger unused cells heap initialized related work probabilistic programming graphical models many probabilistic programming systems specialized diﬀerent use-cases. dominant axis variability expressiveness language. probabilistic programming languages exempliﬁed church allow great freedom expressibility language including constructs like recursion higher order functions. cost expressibility language inference techniques cannot specialized thus systems tend general markov chain monte carlo methods inference. spectrum specialized systems like infer.net stan systems restrict models constructed predeﬁned building blocks support arbitrary program constructs like recursion higher order functions. generally support basic loops branching structure however. infer.net example loops unrolled statements handled special constructs known gates result program viewed ﬁnite gated factor graph message passing inference performed. terms terpret similar infer.net handling loops statements inspired infer.net. compared infer.net terpret extreme restrictions places upon modelling constructs. beneﬁt restricted language allows support broader range back-ends. looking forward infer.net provides inspiration terpret might extended handle richer data types like real numbers strings. another related line work casting program synthesis problem inference probabilistic models. gulwani jojic phrase program synthesis inference graphical model belief propagation inference. future work would like create belief propagation-based back-end terpret. problem inducing samplers probability distributions also cast problem inference probabilistic program lake induce probabilistic programs performing inference probabilistic model describing primitives composed form types instances types. neural networks memory common neural network architectures handling sequences inputs memory manifests highly compressed state network. problematic task hand requires relate inputs apart other recent models mitigate using tools long short-term memory gated recurrent units however recurrent units entirely solve problems long-range interactions range additional techniques employed improve results alternative solution problem extend networks providing access external storage. initial extensions provided stack scratch simpliﬁed stack controlling network learned push data data stack. recently similar ideas picked again leading stack queue-augmented recurrent nets memory networks freely addressable storage extensions additionally registers intermediate results neural networks learning algorithms recurrent neural networks access memory essentially learnable implementations neumann architecture. number recent advances build observation learn algorithms input-output data approaches diﬀer underlying execution models learning methods program domains share overall idea training deep neural network learns manipulate data repeatedly calling deterministic modules predeﬁned set. models able learn repeat simple algorithmic patterns interpretable; learned becomes evident actions concrete inputs. recent work improved aspect closest approach. support adaptive neural compilation machine model similar assembly model introduced. allows user sketch program input deep learning methods optimise result program chosen assembly language displayed easily. diﬀerentiable forth similar step direction learning task holes partial forth program. program synthesis area program synthesis recently seen renewed interest programming language community many synthesis techniques developed wide range problems including data wrangling inference eﬃcient synchronization concurrent programs synthesizing eﬃcient low-level code partial programs compilers low-power spatial architectures eﬃcient compilation declarative speciﬁcations statistical code completion automated feedback generation programming assignments techniques broadly categorized using three dimensions speciﬁcation mechanism complexity hypothesis space iii) search algorithm. diﬀerent forms speciﬁcations include input-output examples partial programs reference implementation program traces etc. hypothesis space possible programs typically deﬁned using domain-speciﬁc language designed expressive enough encode majority desired tasks time concise enough eﬃcient learning. finally common search algorithms include constraint-based symbolic synthesis algorithms smart enumerative algorithms pruning version-space algebra based search algorithms stochastic search also recent work learning inputs addition input-output examples guide synthesis algorithm synthesizing programs without examples performing joint inference program inputs recover compressed encodings observed data work targeting speciﬁcations based input-output examples form speciﬁcation natural work machine learning community. deﬁning hypothesis space probabilistic programming language terpret currently support compilation intermediate representations four inference algorithms. diﬀerence work previous work program synthesis community language built allow compilation diﬀerent inference algorithms enables like-to-like comparison. note another recent eﬀort sygus aims unify diﬀerent program synthesis approaches using common intermediate format based context-free grammars diﬀerent inferences techniques compared terpret language allows encoding richer programming models sygus also allows compilation gradient-descent based inference algorithms. discussion future work presented terpret probabilistic programming language specifying problems. terpret used combination fmgd back-end produce diﬀerentiable interpreters wide range program representations languages. terpret several back-ends including based linear programming strong alternatives programming languages community. biggest take-away experimental results methods programming languages signiﬁcantly outperform machine learning approaches. believe important take-away machine learning researchers studying program synthesis. however remain optimistic future machine learning-based approaches program synthesis wish discourage work area. quite opposite; hope work stimulates research area helps clarify machine learning likely useful. setting work minimal version program synthesis problem main challenge eﬃciently searching program space programs meet given input-output speciﬁcation. conclusion experiments gradient descent inferior constraint-based discrete search algorithms task. results also raise interesting question taken comparison nram model reported solve problems fmgd approach able would like better understand source discrepancy main diﬀerences execution model program parameterization. nram execution model instructions executed timesteps controller decide wire creates additional parallelism redundancy relative basic block assembly models. speculate makes possible nram model multiple hypotheses developed little overlap memory locations used. property make optimization easier. major diﬀerence controller. nram model uses neural network controller maps state registers operations performed. basic block assembly models done instruction pointer updated based upon control decisions program. speculate neural network controller operates less constrained space oﬀers bias search. suspect neural networks biased towards repeating circuits earlier stages training typical behavior ﬁrst predicting averages specializing make strongly input-dependent predictions. future work would like explore questions rigorously hopes ﬁnding general principles used develop robust inference algorithms. future work several extensions terpret would like develop. first would like extend terpret language several ways. would like support non-uniform priors program variables several data types would like support including ﬂoating point numbers strings richer data types. second would like continue expand number back-ends. natural next steps back-ends based local search markov chain monte carlo message passing inference graphical models perhaps taking inspiration sontag third would like build higher level languages terpret support compact speciﬁcation common terpret programming patterns. generally believe opportunities come improving discrete search setting re-phrasing program synthesis problem pattern-matching big-data problem augmenting speciﬁcation beyond input-output examples cases importance discrete search component decreases believe many opportunities machine learning. move forward directions believe terpret continue valuable makes easy build range diﬀerentiable interpreters used conjunction larger learning systems. acknowledgements thank several people discussions helped improve report minka discussions related gates relaxation; john winn several discussions related probabilistic programming gates; ryota tomioka discussions related fmgd loss surface; andy gordon pushing towards probabilistic programming formulation terpret; abdel-rahman mohamed discussions related neural networks program synthesis; jack feser ﬁrst non-author terpret user; aditya nori helpful discussions program synthesis; matej balog critical reading manuscript. proof lemma lemma island structures zero gradient. proof. notationally free parameters unnormalized probability equal probability given softmax; si+exp i.e. zero island-structured conﬁguration given. objective clearly suboptimal less island boundaries). thus island structure suboptimal local optimum. compileme const return compileme const return compileme const return compileme const return compileme const noop return compileme const equalitytest return else compileme equalitytestreg return else compileme equalitytestvalue return else compileme equalitytestline return else compileme valueequalszero return else compileme const nlines incline return const nlines compileme const maxint zero return compileme const maxint return const maxint compileme const maxint return const maxint compileme const maxint references frances allen. control analysis. sigplan notices volume pages rajeev alur rastislav bod´ık eric dallal dana fisman pranav garg garvit juniwal hadas kressgazit madhusudan milo martin mukund raghothaman shamwaditya saha sanjit seshia rishabh singh armando solar-lezama emina torlak abhishek udupa. syntax-guided synthesis. dependable software systems engineering pages carpenter. stan probabilistic programming language. journal statistical software kyunghyun bart merrienboer dzmitry bahdanau yoshua bengio. properties neural machine translation encoder-decoder approaches. corr abs/. http //arxiv.org/abs/.. sreerupa giles guo-zheng sun. using prior knowledge {nnpda} learn contextfree languages. proceedings conference advances neural information processing systems nips pages yann dauphin razvan pascanu caglar gulcehre kyunghyun surya ganguli yoshua bengio. identifying attacking saddle point problem high-dimensional non-convex optimization. advances neural information processing systems pages giles guo-zheng hsing-hen chen yee-chun dong chen. higher order recurrent networks grammatical inference. advances neural information processing systems pages noah goodman vikash mansinghka daniel keith bonawitz joshua tenenbaum. church language generative models. proc. uncertainty artiﬁcial intelligence edward grefenstette karl moritz hermann mustafa suleyman phil blunsom. learning transduce unbounded memory. advances neural information processing systems pages armand joulin tomas mikolov. inferring algorithmic patterns stack-augmented recurrent nets. advances neural information processing systems pages karol kurach marcin andrychowicz ilya sutskever. neural random-access machines. proceedings international conference learning representations http//arxiv.org/ abs/.. chris lattner vikram adve. llvm compilation framework lifelong program analysis transformation. code generation optimization international symposium pages ieee tomas mikolov armand joulin sumit chopra micha¨el mathieu marc’aurelio ranzato. learning longer memory recurrent neural networks. proceedings international conference learning representations michael mozer sreerupa das. connectionist symbol manipulator discovers structure context-free languages. advances neural information processing systems pages arvind neelakantan quoc ilya sutskever. neural programmer inducing latent programs gradient descent. proceedings international conference learning representations yura perov frank wood. automatic sampler discovery probabilistic programming approxinternational conference artiﬁcial general intelligence pages phitchaya mangpo phothilimthana tikhon jelvis rohin shah nishant totla sarah chasins rastislav bod´ık. chlorophyll synthesis-aided compiler low-power spatial architectures. pldi page scott reed nando freitas. neural programmer-interpreters. andrew reynolds morgan deters viktor kuncak cesare tinelli clark barrett. counterexamplearmando solar-lezama. program synthesis sketching. thesis eecs dept. berkeley armando solar-lezama rodric rabbah rastislav bodik kemal ebcioglu. programming sketching sainbayar sukhbaatar arthur szlam jason weston fergus. end-to-end memory networks. advances neural information processing systems annual conference neural information processing systems december montreal quebec canada pages abhishek udupa arun raghavan jyotirmoy deshmukh sela mador-haim milo martin rajeev alur. transit specifying protocols concolic snippets. pldi pages martin wainwright michael jordan. graphical models exponential families variational jason weston sumit chopra antoine bordes. memory networks. proceedings international conference learning representations http//arxiv.org/abs/.. wojciech zaremba tomas mikolov armand joulin fergus. learning simple algorithms examples. proceedings international conference machine learning icml pages", "year": 2016}