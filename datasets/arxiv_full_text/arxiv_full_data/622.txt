{"title": "Think Globally, Embed Locally --- Locally Linear Meta-embedding of Words", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "Distributed word embeddings have shown superior performances in numerous Natural Language Processing (NLP) tasks. However, their performances vary significantly across different tasks, implying that the word embeddings learnt by those methods capture complementary aspects of lexical semantics. Therefore, we believe that it is important to combine the existing word embeddings to produce more accurate and complete \\emph{meta-embeddings} of words. For this purpose, we propose an unsupervised locally linear meta-embedding learning method that takes pre-trained word embeddings as the input, and produces more accurate meta embeddings. Unlike previously proposed meta-embedding learning methods that learn a global projection over all words in a vocabulary, our proposed method is sensitive to the differences in local neighbourhoods of the individual source word embeddings. Moreover, we show that vector concatenation, a previously proposed highly competitive baseline approach for integrating word embeddings, can be derived as a special case of the proposed method. Experimental results on semantic similarity, word analogy, relation classification, and short-text classification tasks show that our meta-embeddings to significantly outperform prior methods in several benchmark datasets, establishing a new state of the art for meta-embeddings.", "text": "previous works studying differences word embedding learning methods shown word embeddings learnt using different methods different resources signiﬁcant variation quality characteristics semantics captured. example hill showed word embeddings trained monolingual bilingual corpora capture different local neighbourhoods. bansal gimpel livescu showed ensemble different word representations improves accuracy dependency parsing implying complementarity different word embeddings. suggests importance meta-embedding creating embedding combining different existing embeddings. refer input word embeddings meta-embedding process source embeddings. sch¨utze showed metaembedding different pre-trained word embeddings overcome out-of-vocabulary problem improve accuracy cross-domain part-of-speech tagging. encouraged above-mentioned prior results expect ensemble containing multiple word embeddings produce better performances constituent individual embeddings tasks. first vocabularies covered source embeddings might different trained different text corpora. therefore words equally represented source embeddings. even situations implementations word embedding learning methods publicly available might possible retrain embeddings text corpora methods originally trained might publicly available. moreover desirable metaembedding method require original resources upon trained corpora lexicons directly work pre-trained word embeddings. particularly attractive computational point view re-training source embedding methods large corpora might require signiﬁcant processing times resources. distributed word embeddings shown superior performances numerous natural language processing tasks. however performances vary signiﬁcantly across different tasks implying word embeddings learnt methods capture complementary aspects lexical semantics. therefore believe important combine existing word embeddings produce accurate complete meta-embeddings words. purpose propose unsupervised locally linear meta-embedding learning method takes pre-trained word embeddings input produces accurate meta embeddings. unlike previously proposed meta-embedding learning methods learn global projection words vocabulary proposed method sensitive differences local neighbourhoods individual source word embeddings. moreover show vector concatenation previously proposed highly competitive baseline approach integrating word embeddings derived special case proposed method. experimental results semantic similarity word analogy relation classiﬁcation short-text classiﬁcation tasks show meta-embeddings signiﬁcantly outperform prior methods several benchmark datasets establishing state meta-embeddings. representing meanings words fundamental task natural language processing popular approach represent meaning word embed ﬁxed-dimensional vector space contrast sparse high-dimensional counting-based distributional word representation methods cooccurring contexts word representation dense low-dimensional prediction-based distributed word representations obtained impressive performances numerous tasks sentiment classiﬁcation machine translation several distributed word embedding learning methods based different learning strategies proposed copyright association advancement artiﬁcial intelligence rights reserved. tors randomly initialised. therefore obvious correspondence dimensions word embeddings learnt even different runs method alone different methods moreover pre-trained word embeddings might different dimensionalities often hyperparameter experimentally. becomes challenging task incorporating multiple source embeddings learn single meta-embedding alignment dimensionalities source embeddings unknown. third local neighbourhoods particular word under different word embeddings show signiﬁcant diversity. example nearest neighbours word bank glove word sense insensitive embedding lists credit ﬁnancial cash whereas word sense sensitive embeddings created huang lists river valley marsh trained corpus. nearest neighbours different senses word bank captured different word embeddings. meta-embedding learning methods learn single global projection entire vocabulary insensitive local variations neighbourhoods overcome above-mentioned challenges propose locally-linear meta-embedding learning method requires words vocabulary source embedding without predict embeddings missing words meta-embed source embeddings different dimensionalities sensitive diversity neighbourhoods source embeddings. proposed method comprises steps neighbourhood reconstruction step projection step reconstruction step represent embedding word linearly weighted combination embeddings nearest neighbours source embedding space. although number words vocabulary particular source embedding potentially large consideration nearest neighbours enables limit representation handful parameters word exceeding neighbourhood size. weights learn shared across different source embeddings thereby incorporating information different source embeddings meta-embedding. interestingly vector concatenation found accurate meta-embedding method derived special case reconstruction step. next projection step computes meta-embedding word nearest neighbours source embedding spaces embedded closely meta-embedding space. reconstruction weights efﬁciently computed using stochastic gradient descent whereas projection efﬁciently computed using truncated eigensolver. noteworthy directly compare different source embeddings word reconstruction step projection step. important dimensions source word embeddings learnt using different word embedding learning methods aligned. moreover particular word might represented source embeddings. property proposed method attractive obviates need align source embeddings predict missing source word embeddings prior meta-embedding. therefore three challenges described solved proposed method. above-mentioned properties proposed method enables compute meta-embeddings different source embeddings covering million unique words. evaluate meta-embeddings learnt proposed method semantic similarity prediction analogy detection relation classiﬁcation short-text classiﬁcation tasks. proposed method signiﬁcantly outperforms several competitive baselines previously proposed metaembedding learning methods multiple benchmark datasets. sch¨utze proposed meta-embedding learning method projects meta-embedding word source embeddings using separate projection matrices. projection matrices learnt minimising squared euclidean distance projected source embeddings corresponding original source embeddings words vocabulary. propose extension metaembedding learning method ﬁrst predicts source word embeddings out-of-vocabulary words particular source embedding using known word embeddings. next method applied learn meta-embeddings union vocabularies covered source embeddings. experimental results semantic similarity prediction word analogy detection cross-domain tagging tasks show effectiveness ton+. contrast proposed method learns locallylinear projections sensitive variations local neighbourhoods source embeddings ton+ seen globally linear projections meta source embedding spaces. later section proposed method outperforms methods consistently benchmark tasks demonstrating importance neighbourhood information learning meta-embeddings. moreover proposed metaembedding method directly compare different source embeddings thereby obviating need predict source embeddings out-of-vocabulary words. locallylinear embeddings attractive computational pointof-view well optimisation require information local neighbourhood word. learning meta-embeddings several prior work shown incorporating multiple word embeddings learnt using different methods improve performance various tasks. example tsuboi showed using wordvec glove embeddings together tagging task possible improve tagging accuracy used embeddings. similarly turian ratinov bengio collectively used brown clusters hlbl embeddings nearest neighbour reconstruction ﬁrst-step learning locally linear meta-embedding reconstruct source word embedding using linearly weighted combination k-nearest neighbours. specifically construct word separately k-nearest neighbours reconstruction weight assigned neighbour found minimising reconstruction error deﬁned local distortions source embedding spaces. here indicator function returns true otherwise. uniformly randomly initialise weights neighbour stochastic gradient descent learning rate scheduled adagrad compute optimal values weights. initial learning rate maximum number iterations experiments. empirically found settings adequate convergence. finally normalise weights exact computation nearest neighbours given data point points requires pairwise similarity computations. must repeat process data point operation would require time complexity prohibitively large vocabularies consider typically therefore resort approximate methods computing nearest neighbours. speciﬁcally balltree algorithm efﬁciently compute approximate k-nearest neighbours time complexity tree construction data points. solution least square problem given subjected summation constraints found solving linear equations. time complexity step cubic neighbourhood size linear dimensionalities embeddings vocabulary size. however found iterative estimation process using described efﬁcient practice. signiﬁcantly smaller number words vocabulary often word reconstructed contained neighbourhood reconstruction weight computation converges proposed multi-view word embedding learning method uses two-sided neural network. adapt pre-trained cbow embeddings wikipedia click-through data search engine. problem setting different because source embeddings trained using word embedding learning method different resources whereas consider source embeddings trained using different word embedding learning methods resources. although method could potentially extended metaembed different source embeddings unavailability implementation prevented exploring possibility. goikoetxea agirre soroa showed concatenation word embeddings learnt separately corpus wordnet produce superior word embeddings. moreover performing principal component analysis concatenated embeddings slightly improved performance word similarity tasks. section discuss relationship proposed method vector concatenation. problem settings explain proposed meta-embedding learning method consider source word embeddings denoted although limit discussion source embeddings simplicity description proposed meta-embedding learning method applied number source embeddings. indeed experiments consider different source embeddings. moreover proposed method limited meta-embedding unigrams used n-grams length provided source embeddings n-grams. denote dimensionalities respectively sets words covered source embedding denoted source embedding word represented vector whereas word vector union containing words. particular note proposed method require word represented source embeddings operate union vocabularies source embeddings. meta-embedding learning problem learn embedding meta-embedding space dimensionality word word denote k-nearest neighbour embedding spaces respectively |n|). discussed already section different word embedding methods encode different aspects lexical semantics likely different local neighbourhoods. therefore requiring meta embedding consider different neighbourhood constraints source embedding spaces hope exploit complementarity source embeddings. huang huang used global contexts train multi-prototype word embeddings sensitive word senses glove pennington socher manning used global co-occurrences words corpus learn word embeddings collobert weston learnt word embeddings following multitask learning approach covering multiple tasks trained corpus hlbl containing word embeddings dimensions) cbow mikolov proposed continuous bag-of-words method train word embeddings intersection vocabularies words whereas union although word embedding used source select abovementioned word embeddings goal paper compare differences performance source embeddings using source embeddings prior work perform fair evaluation. particular could word embeddings trained algorithm different resources different algorithms resources source embeddings. defer evaluations extended version conference submission. evaluation tasks standard protocol evaluating word embeddings embeddings task measure relative increase performance task. four extrinsic evaluation tasks semantic similarity measurement measure similarity words cosine similarity corresponding embeddings measure spearman correlation coefﬁcient human similarity ratings. rubenstein goodenough’s dataset rare words dataset stanford’s contextual word similarities dataset simlex dataset addition miller charles’ dataset validation dataset tune various hyperparameters although skip-gram embeddings shown outperform embeddings used source sch¨utze therefore consistent comparisons prior work decided include skip-gram source. projection meta-embedding space second step proposed method compute meta-embeddings words using reconstruction weights computed section speciﬁcally meta-embeddings must minimise projection cost deﬁned ﬁnding space minimises hope preserve rich neighbourhood diversity source embeddings within meta-embedding. summations combined re-write follows here matrix element smallest eigenvalue zero corresponding eigenvector discarded projection. eigenvectors corresponding next smallest eigenvalues symmetric matrix found without performing full matrix diagonalisation operations involving left multiplication required sparse eigensolvers exploit fact expressed product sparse matrices. moreover truncated randomised methods used smallest eigenvectors without performing full eigen decompositions. experiments neighbourhood sizes words source embeddings equal project dimensional metaembedding space. source word embeddings previously proposed pre-trained word embedding sets source embeddings experiments hlbl hierarchical log-bilinear embeddings released turian ratinov bengio corpus) here vocabulary constrained intersection concatenation deﬁned missing words source embedding. alternatively could zero-vectors missing words predict word embeddings missing words prior concatenation. however consider extensions beyond simple concatenation baseline consider here. hand common neighbourhood obtained either limiting extending neighbourhoods entire vocabulary shows neighbourhood constraints ﬁrst step proposed method seen reconstructing neighbourhood concatenated space. second step would meta-embeddings preserve locally linear structure concatenated space. drawback concatenation increases dimensionality meta-embeddings compared source-embeddings might problematic storing processing meta-embeddings singular value decomposition create matrix arranging conc vectors union source embedding vocabularies. words missing particular source embedding assign zero vectors source embedding’s dimensionality. next perform unitary matrices diagonal matrix contains singular values select largest left singular vectors create dimensional embeddings words. using validation dataset multiplying singular values technique used weight latent dimensions considering salience singular values result notable improvements experiments. meta-embedding results using dataset best values neighbourhood size dimensionality proposed method. plan publicly release metaembeddings acceptance paper. summarise experimental results different methods different tasks/datasets table table rows show performance individual source embeddings. next perform ablation tests hold-out source embedding four meta-embedding method. evaluate statistical significance best performing individual source embedding dataset. semantic similarity benchmarks fisher transformation compute conﬁdence intervals spearman correlation coefﬁcients. datasets used clopper-pearson binomial exact conﬁdence intervals word analogy detection using cosadd method solve word-analogy questions google dataset semeval dataset speciﬁcally three given words fourth word correctly answers question what? cosine similarity vectors maximised. relation classiﬁcation diffvec dataset containing triples form covering relation types. train -nearest neighbour classifer target tuple measure cosine similarity vector offset word embeddings remaining tuples dataset. ranked tuple relation target tuple considered correct match. compute classiﬁcation accuracy entire dataset evaluation measure. short-text classiﬁcation binary short-text classiﬁcation datasets stanford sentiment treebank movie reviews dataset review represented bag-of-words compute centroid embeddings words represent review. next train binary logistic regression classiﬁer crossvalidated regulariser using train portion dataset evaluate classiﬁcation accuracy using test portion dataset. baselines concatenation simple baseline method combining pre-trained word embeddings concatenate embedding vectors word produce metaembedding source embedding normalised prior concatenation source embedding contributes equally measuring word similarity using product. also observed sch¨utze found conc performs poorly without emphasising glove cbow constant factor used conjunction hlbl huang source embeddings. interestingly concatenation seen special case reconstruction step described section this denote concatenation column vectors rd+d. then reconstruction error deﬁned written follows table results word similarity analogy relation short-text classiﬁcation tasks. task best performing method shown bold. statistically signiﬁcant improvements best individual source embedding indicated asterisk. among individual source embeddings glove cbow stand best embeddings. observation conﬁrmed ablation results removal glove cbow often results decrease performance. performing after concatenating always result improvement. global projection reduces dimensionality meta-embeddings created concatenation. result indicates different source embeddings might require different levels dimensionality reductions applying single global projection always guarantee improvements. ensemble methods source embeddings shown rows ton+ proposed sch¨utze detailed section evaluate tasks here conduct fair consistent evaluation used publicly available meta-embeddings without retraining ourselves. overall table proposed method obtains best performance tasks/datasets. benchmarks improvement statistically signiﬁcant best single source embedding. moreover ablation results proposed method show that although different source embeddings important different degrees using source embeddings obtain best results. different source embeddings trained different resources optimising different objectives. therefore different words local neighbours predicted different source embeddings complementary. unlike methods proposed method never compares different source embeddings’ vectors directly neighbourhood reconstruction weights. consequently proposed method unaffected relative weighting source embeddings. contrast conc highly sensitive weighting. fact conﬁrmed performance scores conc method decreased points weight tuning described section unnecessity weight tuning thus clear advantage proposed method. investigate effect dimensionality meta-embeddings learnt proposed method figure neighbourhood size measure performance semantic similarity measurement tasks varying overall performance peaks around behaviour explained fact smaller dimensions unable preserve information contained source embeddings whereas increasing beyond rank weight matrix likely generate noisy eigenvectors. figure study effect increasing neighbourhood size equally words source embeddings ﬁxing dimensionality metaembedding initially performance increases neighbourhood size saturates. implies practice small local neighbourhood adequate capture differences source embeddings. complementarity resources shown empirically section using proposed method possible obtain superior metaembeddings diverse source embeddings. important scenario meta-embedding could potentially useful source embeddings trained different complementary resources resource share little common vocabulary. example source embedding might trained wikipedia whereas second source embedding might trained tweets. evaluate effectiveness proposed metaembedding learning method settings design following experiment. select dataset largest among semantic similarity benchmarks contains unique words human-rated word-pairs semantic similarity. next randomly split words sets different overlap ratios. select sentences january dump wikipedia contains words sets. create corpora roughly equal number sentences procedure different overlap ratios. train skipgram negative sampling corpus create source embedding glove corpus create source embedding finally proposed method meta-embed figure shows spearman correlation human similarity ratings cosine similarities computed using word embeddings dataset meta-embeddings created using proposed method concatenation baseline figure meta embeddings obtain best performance across overlap ratios. improvements larger overlap corpora smaller diminishes corpora becomes identical. result shows proposed meta-embedding learning method captures complementary information available different source embeddings create accurate word embeddings. moreover shows considering local neighbourhoods source embeddings separately obviate need predict embeddings missing words particular source embedding limitation method proposed sch¨utze proposed unsupervised locally linear method learning meta-embeddings given pre-trained source embeddings. experiments several tasks show accuracy proposed method outperforms previously proposed meta-embedding learning methods multiple benchmark datasets. future plan extend proposed method learn cross-lingual meta-embeddings incorporating cross-lingual well monolingual information.", "year": 2017}