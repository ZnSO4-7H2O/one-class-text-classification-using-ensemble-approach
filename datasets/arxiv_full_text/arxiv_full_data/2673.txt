{"title": "VAE with a VampPrior", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Many different methods to train deep generative models have been introduced in the past. In this paper, we propose to extend the variational auto-encoder (VAE) framework with a new type of prior which we call \"Variational Mixture of Posteriors\" prior, or VampPrior for short. The VampPrior consists of a mixture distribution (e.g., a mixture of Gaussians) with components given by variational posteriors conditioned on learnable pseudo-inputs. We further extend this prior to a two layer hierarchical model and show that this architecture with a coupled prior and posterior, learns significantly better models. The model also avoids the usual local optima issues related to useless latent dimensions that plague VAEs. We provide empirical studies on six datasets, namely, static and binary MNIST, OMNIGLOT, Caltech 101 Silhouettes, Frey Faces and Histopathology patches, and show that applying the hierarchical VampPrior delivers state-of-the-art results on all datasets in the unsupervised permutation invariant setting and the best results or comparable to SOTA methods for the approach with convolutional networks.", "text": "many diﬀerent methods train deep generative models introduced past. paper propose extend variational auto-encoder framework type prior call \"variational mixture posteriors\" prior vampprior short. vampprior consists mixture distribution components given variational posteriors conditioned learnable pseudo-inputs. extend prior layer hierarchical model show architecture coupled prior posterior learns signiﬁcantly better models. model also avoids usual local optima issues related useless latent dimensions plague vaes. provide empirical studies datasets namely static binary mnist omniglot caltech silhouettes frey faces histopathology patches show applying hierarchical vampprior delivers state-of-the-art results datasets unsupervised permutation invariant setting best results comparable sota methods approach convolutional networks. learning generative models capable capturing rich distributions vast amounts data like image collections remains major challenges machine learning. recent years diﬀerent approaches achieving goal proposed formulating alternative training objectives loglikelihood utilizing variational inference latter approach could made especially eﬃcient application reparameterization trick resulting highly scalable framework known variational auto-encoders various extensions deep generative models proposed enrich variational posterior recently noticed fact prior plays crucial role mediating generative decoder variational encoder. choosing simplistic prior like standard normal distribution could lead overregularization consequence poor hidden representations paper take closer look regularization term variational lower bound inspired analysis presented re-formulating variational lower bound gives regularization terms average entropy variational posterior cross-entropy averaged variational posterior prior. crossentropy term minimized setting prior equal average variational posteriors training points. however would computationally expensive. instead propose prior variational mixture posteriors prior vampprior short. moreover present twolevel combined prior learn powerful hidden representation. follow line research improving making prior ﬂexible. propose prior mixture variational posteriors conditioned learnable pseudo-data. allows variational posterior learn potent latent representation. propose two-layered generative model layers stochastic latent variables based vampprior idea. architecture eﬀectively avoids problems unused latent dimensions. assumed parameterized weights prior expressed using standard normal distribution decoder utilizes suitable distribution data consideration e.g. bernoulli distribution binary data normal distribution continuous data parameterized weights idea variational lower-bound consists parts namely reconstruction error regularization term encoder prior. however re-write training objective obtain regularization terms instead ﬁrst component negative reconstruction error second component expectation entropy variational posterior last component cross-entropy aggregated posterior prior. second term objective encourages encoder large entropy every data case. last term aims matching aggregated posterior prior. usually prior chosen advance e.g. standard normal prior. however could prior optimizes elbo maximizing following lagrange function lagrange multiplier variational auto-encoder vector observable variables vector stochastic latent variables. further parametric model joint distribution. given data typically maximizing average marginal log-likelihood respect parameters. however model parameterized neural network optimization could diﬃcult intractability marginal likelihood. possible overcoming issue apply variational inference optimize following lower bound various ways optimizing lower bound continuous could done eﬃciently re-parameterization yields variational auto-encoder architecture therefore learning consider monte carlo estimate second expectation using sample points ﬁrst component objective function seen expectation negative reconstruction error forces hidden representation data case peaked speciﬁc value. contrary second third components constitute kind regularization drives encoder match prior. variational posterior high variance. contrary ﬁrst part objective causes posteriors variance diﬀerent latent explanations data case. eﬀects distinguish vampprior prior utilized connection empirical bayes idea empirical bayes also known type-ii maximum likelihood hyperparameters prior latent variables maximizing marginal likelihood case vampprior pseudo-inputs alongside parameters posterior hyperparameters prior maximizing elbo respect them. thus approach closely related fact formulates kind bayesian inference combines variational inference approach. connection information bottleneck shown aggregated posterior optimal prior within formulation. result closely related information bottleneck approach aggregated posterior naturally plays role prior. interestingly vampprior brings formulations together highlights close relation. similar conclusion thorough analysis close relation between vampprior presented hierarchical inactive stochastic latent variable problem typical problem encountered training inactive stochastic units vampprior seems eﬀective remedy issue simply prior designed rich multimodal preventing term pulling individual posteriors towards simple prior. inactive stochastic units problem even worse learning deeper vaes reason might stochastic dependencies within deep generative model top-down generative process bottom-up variational process. result less information obtained real data deeper stochastic layers making prone become regularized towards prior. order overcome issues like overﬁtting overregularization high computational complexity optimal solution i.e. aggregated posterior approximated mixture variational posteriors pseudo-inputs number pseudo-inputs d-dimensional vector refer pseudo-input. pseudo-inputs learned backpropagation thought hyperparameters prior alongside parameters posterior importantly resulting prior multimodal thus prevents variational posterior over-regularized. hand incorporating pseudo-inputs prevents potential overﬁtting pick also makes model less expensive train. refer prior variational mixture posteriors prior comparison mixture gaussians prior simpler alternative vampprior still approximates optimal solution problem mixture gaussians prior diag suplementary material details. second coupling highly inﬂuences gradient single weight encoder given supplementary material details. diﬀerences |uk). thus close long gradient inﬂuenced pseudo-inputs dissimilar i.e. posterior produces diﬀerent hidden representations words since hold every training case gradient points towards solution including standard prior provide answer general question even need complex priors. utilizing mixture gaussians veriﬁes whether beneﬁcial couple prior variational posterior not. finally using subset real training images determines extent useful introduce trainable pseudo-inputs. experiments verifying empirically whether vampprior helps train representation better reﬂects variations data inspecting proposition two-level generative model performs better one-layered model. order answer questions compare diﬀerent models parameterized feed-forward neural networks convolutional networks utilize standard prior vampprior. order compare hierarchical vampprior state-of-the-art approaches used also autoregressive decoder. nevertheless primary goal quantitatively qualitatively assess newly proposed prior. alternative priors motivated vampprior analyzing variational lower bound. however could inquire whether really need complicated prior maybe proposed two-layered already suﬃciently powerful. order answer questions verify three alternative priors experiments modeled distributions using mlps hidden layers hidden units unsupervised permutation invariant setting. utilized gating mechanism element-wise nonlinearity utilized stochastic hidden units next replaced mlps convolutional layers gating mechanism. eventually veriﬁed also pixelcnn decoder. frey faces histopathology used discretized logistic distribution images datasets applied bernoulli distribution. learning adam algorithm normalized gradients utilized learning rate mini-batches size additionally boost generative capabilities decoder used warm-up epochs weights neural networks initialized according early-stopping look ahead iterations applied. vampprior used pseudoinputs datasets except omniglot utilized pseudo-inputs. vampprior data randomly picked training images instead learnable pseudo-inputs. denote hierarchical proposed paper mlps hvae hierarchical convolutional layers additionally pixelcnn decoder denoted convhvae pixelhvae respectively. quantitative results quantitatively evaluate method using test marginal log-likelihood estimated using importance sampling sample points table present comparison models standard prior vampprior. results approach comparison state-of-the-art methods gathered table static dynamic mnist omniglot caltech silhouettes respectively. first notice cases except application vampprior results substantial improvement generative performance terms test comparing standard normal prior conﬁrms supposition combination multimodality coupling prior posterior superior standard normal prior. further want stress vampprior outperforms priors like single gaussian mixture gaussians results provide additional evidence vampprior leads powerful latent representation diﬀers prior. also examined whether presented two-layered model performs better widely used hierarchical architecture vae. indeed newly proposed approach powerful even prior standard two-layered applying prior results additional boost performance. provides evidence usefulness multimodal prior. vampprior data gives slight improvement comparing prior application ﬁxed training data pseudo-inputs less ﬂexible mog. eventually coupling variational posterior prior introducing learnable pseudo-inputs gives best performance. additionally compared vampprior prior prior figure varying number pseudo-inputs/components. surprisingly taking pseudo-inputs help improve performance similarly considering mixture components also resulted drop performance. however notice vampprior ﬂexible prior outperforms mog. inspection histograms log-likelihoods shows distributions values heavy-tailed and/or bimodal. possible explanation characteristics histograms existence many examples relatively simple represent really hard examples comparing aptable test log-likelihood diﬀerent models standard normal prior vampprior. last datasets average bits data dimension given. case frey faces models standard deviation larger omitted table. hypothesized vampprior provides remedy inactive units issue. order verify claim utilized statistics introduced results hvae vampprior comparison two-level iwae presented given figure application vampprior increases number active stochastic units four times second level around times ﬁrst level comparing iwae. interestingly number mixture components great impact number active stochastic units second level. nevertheless even mixture component allows achieve almost three times active stochastic units comparing vanilla iwae. general application vampprior improves performance case layers stochastic units yields state-of-the-art results datasets models mlps. moreover approach gets closer performance models utilize convolutional neural networks one-layered inverse autoregressive achieves static mnist dynamic mnist onelayered variational lossy autoencoder obtains static mnist dynamic mnist hierarchical pixelvae gets dynamic mnist. datasets vlae performs better approach mlps achieves omniglot caltech silhouettes. figure comparison two-level iwae standard normal prior vampprior counterpart terms number active units varying number pseudo-inputs static mnist. order compare approach state-of-theart convolutional-based vaes performed additional experiments using hvae vampprior convolutional layers encoder decoder. next replaced convolutional decoder pixelcnn decoder pixelhvae able improve performance obtain static mnist dynamic mnist omniglot caltech silhouettes. results conﬁrmed vampprior combined powerful encoder ﬂexible decoder performs much better mlp-based approach allows achieve state-of-the-art performance dynamic mnist omniglot. qualitative results biggest disadvantage tends produce blurry images noticed eﬀect images generated reconstructed moreover standard produced digits hard interpret blurry characters noisy silhouettes. supremacy hvae vampprior visible values image generations reconstructions well sharper. performance vlae static mnist caltech silhouettes provided diﬀerent training procedure used paper. also examine pseudo-inputs represent training process interestingly trained pseudo-inputs prototypical objects moreover images generated chosen pseudo-input show model encodes high variety diﬀerent features shapes thickness curvature single pseudo-input. means model memorizing data-cases. worth noticing small-sample size and/or ﬂexible decoder pseudo-inputs hard train represent noisy prototypes latent variable model usually trained simple prior i.e. standard normal prior. dirichlet process prior using stickbreaking process proposed proposed nested chinese restaurant process. priors enrich generative capabilities however require sophisticated learning methods tricks trained successfully. diﬀerent approach autoregressive prior applies random noise. approach gives promising results allows build rich representations. nevertheless authors combine prior convolutional encoder autoregressive decoder makes harder assess real contribution autoregressive prior generative model. clearly quality generated images dependent decoder architecture. improving generative capabilities decoder inﬁnite mixture probabilistic component analyzers equivalent rank-one covariance matrix. appealing approach would deep autoregressive density estimators utilize recurrent neural networks gated convolutional networks however threat ﬂexible decoder could discard hidden representations completely turning encoder useless concurrently work variational memory proposed. approach shares similarities vampprior terms learnable memory multimodal prior. nevertheless main diﬀerences. first prior explicit mixture sample components. second showed optimal prior requires coupled variational posterior. experiments showed vampprior improves generative capabilities noticed generative performance comparable standard normal prior. claim figure images generated pixelhvae vampprior chosen pseudo-input left corner. images represent subset trained pseudo-inputs diﬀerent datasets. comparable results sota models datasets. additionally generations reconstructions obtained hierarchical vampprior better quality ones achieved standard vae. believe worthwhile pursue line research presented paper. applied prior image data would interesting behaves text sound sequential aspect plays crucial role. already showed combining vampprior convolutional nets powerful autoregressive density estimator beneﬁcial thorough study needed. last least would interesting utilize normalizing hierarchical variational inference ladder networks adversarial training within vampprior vae. however leave investigating issues future work. recently vampprior shown perfect match information-theoretic approach learn latent representation additionally authors proposed weighted version vampprior paper followed line thinking prior critical element improve deep generative models particular vaes. proposed prior expressed mixture variational posteriors. order limit capacity prior introduced learnable pseudo-inputs hyperparameters prior number chosen freely. further formulated two-level generative model based vampprior. showed empirically applying prior indeed increase performance proposed generative model successfully overcome problem inactive stochastic latent variables particularly challenging generative models multiple layers stochastic latent variables. result achieved state-of-the-art", "year": 2017}