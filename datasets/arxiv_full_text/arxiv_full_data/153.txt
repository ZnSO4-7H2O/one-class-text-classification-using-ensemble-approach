{"title": "Genetic Architect: Discovering Genomic Structure with Learned Neural  Architectures", "tag": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "abstract": "Each human genome is a 3 billion base pair set of encoding instructions. Decoding the genome using deep learning fundamentally differs from most tasks, as we do not know the full structure of the data and therefore cannot design architectures to suit it. As such, architectures that fit the structure of genomics should be learned not prescribed. Here, we develop a novel search algorithm, applicable across domains, that discovers an optimal architecture which simultaneously learns general genomic patterns and identifies the most important sequence motifs in predicting functional genomic outcomes. The architectures we find using this algorithm succeed at using only RNA expression data to predict gene regulatory structure, learn human-interpretable visualizations of key sequence motifs, and surpass state-of-the-art results on benchmark genomics challenges.", "text": "human genome billion base pair encoding instructions. decoding genome using deep learning fundamentally differs tasks know full structure data therefore cannot design architectures suit such architectures structure genomics learned prescribed. here develop novel search algorithm applicable across domains discovers optimal architecture simultaneously learns general genomic patterns identiﬁes important sequence motifs predicting functional genomic outcomes. architectures using algorithm succeed using expression data predict gene regulatory structure learn humaninterpretable visualizations sequence motifs surpass state-of-the-art results benchmark genomics challenges. deep learning demonstrates excellent performance tasks computer vision text many ﬁelds. deep learning architectures consist matrix operations composed non-linearity activations. critically problem domain governs matrix weights shared. convolutional neural networks dominant image processing translational equivariance encoded convolution operation; recurrent networks dominant sequential data temporal transitions captured shared hidden-to-hidden matrices. architectures mirror human intuitions priors structure underlying data. genomics excellent domain study might learn optimal architectures poorlyunderstood data intuition local patterns long-range sequential dependencies affect genetic function much structure remains discovered. genome challenging data type although tens thousands whole genome sequences understand small subset base pairs within sequence. genetic code allows annotate genome encoding proteins grammar decoding rest non-coding sequences important gene regulation evolution species susceptibility diseases. availability wealth genomic assays allows directly measure function speciﬁc regions genome creating enormous opportunity decode non-coding sequences. however overwhelming volume data makes decoders genome quite complex. design application domain-speciﬁc inspired human foveal attention global glances drive sequential local focus attention components added neural networks yielding state-of-the-art results tasks diverse caption generation machine translation protein sublocalization differentiable programming. main architectural implementations hard attention network’s focus mechanism non-differentiably samples available input soft attention component outputs expected glimpse using weighted average. beyond biological inspiration components enable improved performance excellent intepretability. techniques applied interpreting neural networks without changing architectures zeiler fergus springenberg simply heuristics ﬁnding relevant regions input work existing modern neural network components. previous groups demonstrated excellent progress applying deep learning genomics. alipanahi lanchantin provide initial results task learning sequences transcription factor bind using convolutional architectures. problem appears suited convolution motifs determining binding expected modular setup task allow learning signiﬁcant long-term dependencies. particular alipanahi demonstrated single-layer convolutional neural network deepbind outperformed tested machine learning approaches predicting probe intensities protein binding microarrays dream challenge showed architecture generalized related task predicting transcription factor binding sites sequencing measurements bound dna. subsequently lanchantin showed deeper network addition highway layers improved deepbind results majority cases tested addition basset architecture trained predict motifs accessible sequencing regions open chromatin able half ﬁrst layer convolutional ﬁlters human tfbss. deep learning algorithm development often dependent knowledge human domain experts. researchers domains computer vision natural language processing spent much time tuning architectures genomics. challenge genomics insufﬁcient understanding biology limits ability inform architectural decisions based data. early genomic deep learning architectures shown promising results undertaken limited exploration architectural search space possible components. addition components work well together evidence optimal component choice highly dependent domain. accordingly design novel road-map applying deep learning data limited prior understanding developing iterative architecture search standard cutting-edge neural building blocks. prior approaches architecture search focus ﬁnding best architecture single step rather sequentially learning architecture space iteratively improving models bergstra bengio snoek framework understands results allowing sequentially narrow search space learn combinations components important. since algorithm limits important hyperparameters best ranges longer dominate search space discover additional hyperparameters important help create highly tuned architecture. sequential nature allows fork architectural search independent subspaces coadapted components thus enabling search parallel branch exponentially efﬁcient considering union promising architectures. heart framework interactive visualization tool given hyperparameter optimization produces common patterns best datapoints presents information highly-interpretable decision trees showing effective architectural subspace plots interactions signiﬁciant hyperparameters informing general domain intuition guiding future experiments. framework general enough applied domains orthogonal existing hyperparameter optimization algorithms. algorithms applied inner loop sequential search tool interprets results informs user domain manually prune search space. employ genetic architect discover optimal architecture novel genome annotation task regression predict lineage-speciﬁc gene expression based genomic sequence inputs stages architecture search required. figure shows sequential process architecture search important ﬁndings stage process tool-guided division search separate promising architectures. splitting effective architectures separate branches optimization tool identiﬁes high-performing architecture-speciﬁc choices difﬁcult notice architectures mixed together. application tool demonstrates power reﬁning architectural components dominate results uncover additional hyperparameter combinations perform well together. several examples encounter tool design architectures genomics follow removal batch normalization demonstrated clear superiority exponential linear units dimensionality reduction middle convolutional network module beneﬁcial recurrent-based architectures contrast non-recurrent architectures required wider layers search architectures using soft attention found fully-connected layers preferred convolutional layers made processing global information important. finally proceeding several steps optimization unintuitive result bidirectional lstms help attentional models ﬁnal models learned genetic architect consist several initial layers convolutions residual blocks lstm layer case promoternet architecture attention-based dimensionality reducing step followed fully-connected layers. previous approaches genome annotation convolutional networks ideal detecting local features. however closely approximating structure genomic information would take account real genome sequence disjointed local features input type recurrent architectures generally excel. addition larger sequences analyze neural network must learn focus important parts sequence integrate information derived part contextual information previously-seen sequence. such long genomic sequences seem ideal recurrent attentional models learned. tfbs binary classiﬁcation task proposed alipanahi used benchmark basic motivation learn classiﬁer correctly predicts empirical binding data training sample short sequences sequences separate test tfbs figure schematic hyperparameter optimization ﬁnal architecture designs. overview steps taken hyperparameter optimization generate attentionnet promoternet. attentionnet architecture. promoternet architecture. figure results attentionnet transcription factor binding site task. attentionnet models outperform deepmotif deepbind models trained corresponding datasets. represents difference different datasets. mean attention mask sequences experiment. recovery transcription factor motifs visualization attention masks produced attentionnet example sequences. input target data tfbs classiﬁcation task consists datasets average sequences characters dataset. sequence string base pairs transformed array one-hot encoding. sequence associated label indicates sequence tfbs. dataset represents different chromatin immunoprecipitation sequencing experiment speciﬁed transcription factor sequence dataset potential binding site. positive example negative example generated. data included tfbs classiﬁcation task derive encode chip-seq experiments performed transformed human cell lines figure results promoternet immgen lineage-speciﬁc expression prediction task. comparison predicted versus observed gene expression deepbind deepmotif promoternet architectures. visualization attention mask selected promoter sequences. mean attention mask promoters. visualization attention masks learned models trained data single lineages. addition tfbs classiﬁcation problem neural network architectures could extended treat much broader complex variety problems interpreting biological data. here develop novel genomic benchmark task ilsep requires regression predict empirically-determined related target data namely prediction amount various biological entities produced different cellular contexts given input genomic sequence. input dataset ilsep task one-hot encoded input promoter sequences corresponding ﬂoating point gene expression outputs ranging split dataset using -fold cross validation obtain predictions promoter gene expression pairs. benchmark performance attentionnet models learned hyperparameter optimization described published state-of-the-art neural network models tfbs task deepbind deepmotif compare architectures train models datasets lanchantin head-to-head comparison dataset attentionnet outperforms deepmotif cases mean across datasets attentionnet improving deepmotif deepbind interpretable information sequence features important consideration genomic learning tasks fundamental understanding biology important prediction power. hypothesize performed well tfbs classiﬁcation task would able make biologically meaningful inferences sequence structure. show mean attention weights across positive sequences show distinct footprint transcription factor binding consistent known nucleotide preferences within sequence further visualizing attention mask across input sequences representative showed focusing attention parts sequence known regulatory could directly obtain motif sequences took nucleotides surrounding position highest attention sequences averaged across motifs. took maximum score nucleotide position queried results jaspar gold standard tfbs database motifs possible check correct corresponded least transcription factor. additionally searching recurring sequences attended recover total correct motifs. promoternet architecture demonstrates marked gain performance deepbind deepmotif architectures adapted ilsep regression task achieving average pearson correlation value out-of-sample predictions target expression values across lineages compared deepbind deepmotif respectively also train promoternet architectures single task regression separate model lineages cell type speciﬁc multi-task regression output unit cell types obtains similar improvements average pearson correlation value deepbind deepmotif. visualization attention mask weights promoternet model reveals attended locations promoter sequences genes selected highest mean expression across lineages enriched directly adjacent suggesting properties core promoter sequence constitute informative features genes show differences expression across lineages contrast attended locations promoter sequences genes maximal variance expression across lineages span much greater range positions. indicates genes greatest degree lineage-speciﬁc expression informative sequence features occur throughout promoter sequence. observation merits follow given previous reports performance classiﬁers cell type speciﬁc expression tasks trained proximal promoter information close random classiﬁer consistent accepted understanding proximal regions contain genomic elements control gene expression levels observe maximum average attention mask weights across promoters occurs center input sequences corresponds table search space explored attentionnet promoternet architectures including techniques maas graham shah ioffe szegedy hochreiter schmidhuber kingma sutskever srivastava hyperparameter values conv ﬁlter size nonlinearity relu leaky relu leaky relu using batch normalization convs true false number conv ﬁlters number convs dim. reduction using residual blocks dim. reduction true false type dim. reduction dim. reduction stride number convs dim. reduction using residual blocks dim. reduction number layers number units type using bidirectional rnns gradient clipping global reduction type number layers number units using batch normalization dropout probability regularization optimizer learning rate scale batch size core promoter elements required recruitment transcriptional machinery promoternet models trained multi-task regression result global attention mask output across lineages. investigate whether promoternet architecture capable learning distinct features lineage also visualize attention weights given promoter sequence separate models trained expression data single lineage. genes selected maximal variance expression demonstrate distinct patterns learned attention across lineages shared pattern attention learned control gene high mean expression lineages even lineage trained separate model tackle problem discovering architectures datasets human priors available. create novel architecture search framework domain agnostic capable sequential architectural subspace reﬁnement informing domain understanding composable existing hyperparameter optimization schemes. using search algorithm create stateof-the architectures signiﬁcant challenges domain genomics utilizing combination standard cutting-edge components. particular learned architecture capable simultaneous discovery local non-local patterns important subsequences sequential composition thereby capturing substantial genomic structure. layer conv ﬁlters conv ﬁlters residual block residual block residual block residual block residual block attention units dropout units dropout units dropout unit sigmoid layer conv ﬁlters conv ﬁlters conv ﬁlters conv ﬁlters maxpool stride residual block residual block residual block lstm units attention units dropout units dropout units dropout unit sigmoid input sequences ilsep task sequences spanning kilobase upstream downstream transcriptional start sites region promoter production gene initiated genes eukaryotic promoter database corresponding labels obtain expression data genes immune cell types immgen consortium april release contains data genes immune cell types quality control according published methods intersection datasets gene results dataset input promoter sequences expression value target pairs. create lineage-speciﬁc gene expression value targets combine cell types groups following lineage tree outlined previous work cells dendritic cells gamma delta cells granulocytes macrophages monocytes natural killer cells stem progenitor cells cells cells average expression values across samples within group highest mean expression across lineages tmsbx rplp hspa srgn rpla rpla ppia actg laptm eefa rplp pabpc gapdh actb highest variance expression across lineages plbd tyrobp iﬁtm plag gzma ctsh klrbc prkcq itgam sfpi msab aloxap fcerg gimap ilrb gimap iﬁtm cybb iﬁtm mpeg h-aa random control lrrcb frik syngr spint slca slca thoc phfa yifb erik pgam pcdhb plcb fabp srgap olfr arik arik anksb magea pygb rras slca hrik references karen simonyan andrea vedaldi andrew zisserman. deep inside convolutional networks visualising image classiﬁcation models saliency maps. arxiv preprint arxiv. jasper snoek hugo larochelle ryan adams. practical bayesian optimization machine learning algorithms. advances neural information processing systems pages anthony mathelier oriol fornes david arenillas chih-yu chen grégoire denay jessica wenqiang casper shyr rebecca worsley-hunt jaspar major expansion update open-access database transcription factor binding proﬁles. nucleic acids research page anirudh natarajan galip gürkan yardımcı nathan shefﬁeld gregory crawford ohler. predicting cell-type–speciﬁc gene expression regions open chromatin. genome research ./gr... http//genome.cshlp. org/content///.abstract. glenn maston sarah evans michael green. transcriptional regulatory elements human genome. annual review genomics human genetics ./annurev.genom.... http//dx.doi.org/./annurev. genom.... pmid ilya sutskever james martens george dahl geoffrey hinton. importance initialization momentum deep learning. proceedings international conference machine learning pages nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research jeff ericson scott davis lesh melissa howard diane mathis christophe benoist. immgen microarray gene expression data data generation quality control pipeline. www.immgen. org/protocols/immgenqcdocumentation_all-datageneration_.pdf. vladimir jojic shay katelyn sylvia joonsoo kang aviv regev daphne koller immunological genome project consortium identiﬁcation transcriptional regulators mouse immune system. nature immunology", "year": 2016}