{"title": "Compositional Distributional Semantics with Long Short Term Memory", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "We are proposing an extension of the recursive neural network that makes use of a variant of the long short-term memory architecture. The extension allows information low in parse trees to be stored in a memory register (the `memory cell') and used much later higher up in the parse tree. This provides a solution to the vanishing gradient problem and allows the network to capture long range dependencies. Experimental results show that our composition outperformed the traditional neural-network composition on the Stanford Sentiment Treebank.", "text": "proposing extension recursive neural network makes variant long short-term memory architecture. extension allows information parse trees stored memory register used much later higher parse tree. provides solution vanishing gradient problem allows network capture long range dependencies. experimental results show composition outperformed traditional neural-network composition stanford sentiment treebank. moving lexical compositional semantics vector-based semantics requires answers difﬁcult questions nature composition functions learn parameters functions data? number classes functions proposed answer ﬁrst question including simple linear functions like vector addition non-linear functions like deﬁned multi-layer neural networks vector matrix multiplication tensor linear mapping matrix tensor-based functions advantage allowing relatively straightforward comparison formal semantics fact multi-layer neural networks non-linear activation functions like sigmoid approximate trying answer second question advantages approaches based neural network architectures recursive neural network model convolutional neural network model even clearer. models paradigm take advantage general learning procedures based back-propagation rise ‘deep learning’ variety efﬁcient algorithms tricks improve training. since ﬁrst success model constituent parsing classes extensions proposed. class enhance compositionality using tensor product concatenating rnns horizontally make deeper extend topology order fulﬁll wider range tasks like zuidema dependency parsing paulus context-dependence sentiment analysis. proposal paper extension model improve compositionality. motivation that like training recurrent neural networks training rnns deep trees suffer vanishing gradient problem i.e. errors propagated back leaf nodes shrink exponentially. addition information sent leaf node root obscured path long thus leading problem capture long range dependencies. therefore borrow long short-term memory architecture recurrent neural network research tackle problems. main idea allow information parse tree stored memory cell used much later higher parse tree recursively adding memory memory cells bottom-up manner. errors propagated back structure vanish. information leaf nodes still preserved used directly higher nodes hierarchy. apply composition sentiment analysis. experimental results show composition works better traditional neural-network-based composition. outline rest paper follows. ﬁrst section give brief background neural networks including multi-layer neural network recursive neural network recurrent neural network lstm. propose lstm recursive neural networks section application sentiment analysis section section shows experiments. background multi-layer neural network multi-layer neural network neurons organized layers neuron layer receives signal neurons layer transmits output neurons layer computation given real vector contains activations neurons layer wi−i r|yi|×|yi−| matrix weights connections layer layer r|yi| vector biases neurons layer activation function e.g. sigmoid tanh softsign training minimize objective function parameter often negative likelihood). thanks back-propagation algorithm gradient ∂j/∂θ efﬁciently computed; gradient descent method thus used minimize recursive neural network recursive neural network where given tree structure recursively apply weight matrices inner node bottom-up manner. order works consider following example. assume constituent parse tree vectorial representations three words respectively. neural network consists weight matrix rd×d left children weight matrix rd×d right children compute vector parent node bottom manner. thus compute like training training uses gradient descent method minimize objective function gradient ∂j/∂θ efﬁciently computed thanks back-propagation structure algorithm model extensions employed successfully solve wide range problems parsing dependency parsing classiﬁcation paraphrase detection semantic role labelling neural network recurrent least directed ring structure. natural language processing ﬁeld simple recurrent neural network proposed elman extensions used tackle sequence-related problems machine translation language modelling network model thus theory used estimate probabilities conditioning long histories. computing gradients efﬁcient thanks back-propagation time algorithm practice however training recurrent neural networks gradient descent method challenging gradients ∂jt/∂hj vanish quickly after back-propagation steps addition difﬁcult capture long range dependencies i.e. output time depends inputs happened long time ago. solution this proposed hochreiter schmidhuber enhanced gers long short-term memory long short-term memory main idea lstm architecture maintain memory inputs hidden layer received time adding inputs hidden layer time memory cell. errors propagated back time vanish even inputs received long time still preserved play role computing output network corresponding gates; state memory cell; denotes element-wise multiplication operator; weight matrices bias vectors. sigmoid function output range activations gates seen normalized weights. therefore intuitively network learn input gate decide memorize information similarly learn output gate decide access memory. forget gate ﬁnally reset memory. section propose extension lstm model feature hierarchically combine information children compute parent vector; idea section extend lstm output children used also contents memory cells. network option store information processing constituents parse tree make available later processing constituents high parse tree. intuitively input gate lets lstm parent node decide important output j-th child important input gate activation close moreover lstm controls using forget gate degree information memory j-th child added memory. parameter sentiment class phrase vector representation node covering computed softmax function regularization parameter. like training mini-batch gradient descent method minimize gradient ∂j/∂θ computed efﬁciently thanks backpropagation structure adagrad method automatically update learning rate parameter. complexity analyse complexities lstmrnn models forward phase i.e. computing vector representations inner nodes classiﬁcation probabilities. complexities backward phase i.e. computing gradients ∂j/∂θ analysed similarly. complexities models dominated matrix-vector multiplications carried out. since number sentiment classes small compared consider matrix-vector multiplications computing vector representations inner nodes. sentence consisting words assuming parse tree binarized without unary branch inner nodes links leaf nodes inner nodes links inner nodes inner nodes. complexity forward phase thus approximately plex sentence containing main clause dependent clause could beneﬁcial information main clause passed higher levels. achieved values input gate forget gate child node covers dependent clause high values gates corresponding child node covering main clause. interestingly lstm even allow child contribute composition activating corresponding input gate ignore child’s memory deactivating corresponding forget gate. happens information given child temporarily important only. section introduce model using proposed lstm sentiment analysis. model named lstm-rnn extension traditional model traditional composition function equations replaced proposed lstm node covering phrase/word sentiment class available softmax layer compute probability assigning class similarly irsoy cardie ‘untie’ leaf nodes inner nodes weight matrix leaf nodes another inner nodes. hence respectively dimensions word embeddings vector representations phrases weight matrices leaf node inner node size weight matrices inner node another inner node size lstm architecture already applied sentiment analysis task instance model proposed http//deeplearning.net/tutorial/lstm. html. independently concurrently work developed similar models applying ltsm rnns. experiments difference problem training evaluating lstmrnn model fast took single core modern computer minutes train model sentences seconds evaluate sentences. dataset used stanford sentiment treebank consists -way ﬁne-grained sentiment labels phrases sentences. standard splitting also given sentences training development testing. average sentence length addition treebank also supports binary sentiment classiﬁcation removing neutral labels leading sentences training development testing. evaluation metric accuracy given lstm-rnn setting initialized word vectors glove word embeddings trained b-word corpus. initial values weight matrix uniformly sampled symmetric interval model tested three activation functions softmax tanh softsign leading sub-models. tuning sub-models development chose dimensions vector representations inner nodes learning rate regularization parameter mini-batch-size functions. sigmoid activation function difference clear seems lstmrnn performed slightly better. tanh-lstm-rnn softsign-lstm-rnn highest median accuracies ﬁne-grained classiﬁcation task binary classiﬁcation task respectively. model surprising sigmoid function performed well comparably functions ﬁne-grained task even better softsign function binary task given often chosen recent work. softsign function shown work better tanh deep networks however yield improvements experiment. lstm-rnn model tanh function general worked best whereas sigmoid function worst. result agrees common choice activation function lstm architecture recurrent network research compared models compare lstm-rnn previous experiment existing models naive bayes bigram features recursive neural tensor network convolutional neural network dynamic convolutional neural network among them binb neural model. rntn drnn extensions rnn. whereas rntn keeps structure uses matrix-vector multiplication tensor product composition purpose drnn makes deeper concatenating rnns horizontally. dcnn rely syntactic trees. uses convolutional layer max-pooling layer handle sequences different lengths. dcnn hierarchical sense stacks convolutional layers k-max pooling layers between. sentence represented input vector predict words appear table shows accuracies models. accuracies lstm-rnn taken network achieving highest performance runs development set. accuracies models copied corresponding papers. lstm-rnn clearly performed worse dcnn drnn tasks worse binary task. question lstm-rnn performed better traditional rnn. here based fact lstm rnns work similarly lstm recurrent neural networks borrow argument given bengio answer question. bengio explains lstm behaves like low-pass ﬁlter hence used focus certain units different frequency regions data. suggests lstm plays role lossy compressor keep global information focusing frequency regions remove noise ignoring high frequency regions. composition case could seen compression like recursive auto-encoder pre-training boost overperformance seeing lstm compressor might explain lstm-rnn worked better withpre-training. comparing lstm-rnn drnn gives hint improve model. experimental results lstm-rnn without glove word embeddings performed worse drnn drnn gained signiﬁcant improvement thanks dropout. finding method like dropout corrupt lstm memory might boost overall performance signiﬁcantly topic future work. performed -layer-drnn using dropout randomly remove neurons training. dropout powerful technique train neural networks plays role strong regularization method prohibit neurons co-adapting also considered technique efﬁciently make ensemble large number shared weight neural networks thanks dropout irsoy cardie boosted accuracy -layer-drnn ﬁne-grained task. second experiment tried boost accuracy lstm-rnn model. inspired irsoy cardie tried using dropout better word embeddings. dropout however work lstm. reason might dropout corrupted memory thus making training difﬁcult. better word embeddings however. used glove word embeddings trained b-word corpus. testing development chose values hyper-parameters ﬁrst experiment except setting learning rate also model times selected networks getting highest accuracies development set. table shows results. using glove word embeddings helpful lstm-rnn performed drnn ﬁne-grained task binary task. therefore taking account tasks lstmrnn glove word embeddings outperformed models. proposed composition method recursive neural network model extending long short-term memory architecture widely used recurrent neural network research. irsoy cardie used wordvec word embeddings trained b-word corpus whereas used glove word embeddings trained b-word corpus. fact achieved accuracy ﬁne-grained task binary task implementation performed worse conclude glove word embeddings suitable wordvec word embeddings.", "year": 2015}