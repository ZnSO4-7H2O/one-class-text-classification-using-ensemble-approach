{"title": "Deep Learning with Darwin: Evolutionary Synthesis of Deep Neural  Networks", "tag": ["cs.CV", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Taking inspiration from biological evolution, we explore the idea of \"Can deep neural networks evolve naturally over successive generations into highly efficient deep neural networks?\" by introducing the notion of synthesizing new highly efficient, yet powerful deep neural networks over successive generations via an evolutionary process from ancestor deep neural networks. The architectural traits of ancestor deep neural networks are encoded using synaptic probability models, which can be viewed as the `DNA' of these networks. New descendant networks with differing network architectures are synthesized based on these synaptic probability models from the ancestor networks and computational environmental factor models, in a random manner to mimic heredity, natural selection, and random mutation. These offspring networks are then trained into fully functional networks, like one would train a newborn, and have more efficient, more diverse network architectures than their ancestor networks, while achieving powerful modeling capabilities. Experimental results for the task of visual saliency demonstrated that the synthesized `evolved' offspring networks can achieve state-of-the-art performance while having network architectures that are significantly more efficient (with a staggering $\\sim$48-fold decrease in synapses by the fourth generation) compared to the original ancestor network.", "text": "enable widespread deep learning recent drive towards obtaining highly-efﬁcient deep neural networks strong modeling power. much work obtaining efﬁcient deep neural networks focused deterministically compressing trained deep neural networks using traditional lossless lossy compression techniques quantization deterministic pruning huffman coding hashing rather attempting take existing deep neural network compress smaller representation heuristically instead consider following idea deep neural networks evolve naturally successive generations highly efﬁcient deep neural networks? using example evolutionary progress towards efﬁciency nature recent study moran proposed eyeless mexican caveﬁsh evolved lose vision system generations high metabolic cost vision. therefore evolving naturally generations caveﬁsh lost vision system amount energy expended signiﬁcantly reduced thus improves survivability subterranean habitats food availability low. ability mimic biological evolutionary process task producing highly-efﬁcient deep neural networks successive generations considerable beneﬁts. study entertain different notion producing highly-efﬁcient deep neural networks introducing evolutionary synthesis deep neural networks successive generations based ancestor deep neural networks. idea leveraging evolutionary computation concepts training generating deep neural networks previously explored literature signiﬁcant differences previous studies study previous studies focused improving accuracy training deep neural networks best authors’ knowledge study ﬁrst explore focus notion evolutionary synthesis deep neural networks high network architectural efﬁciency successive generations. evolutionary computational approaches leveraged previous studies classical approaches genetic algorithms evolutionary programming study introduces probabilistic framework evolution mechanisms genetic encoding environmental conditions modeled probability distributions stochastic synthesis process leverages probability models produce deep neural networks successive generations. best abstract—taking inspiration biological evolution explore idea deep neural networks evolve naturally successive generations highly efﬁcient deep neural networks? introducing notion synthesizing highly efﬁcient powerful deep neural networks successive generations evolutionary process ancestor deep neural networks. architectural traits ancestor deep neural networks encoded using synaptic probability models viewed ‘dna’ networks. descendant networks differing network architectures synthesized based synaptic probability models ancestor networks computational environmental factor models random manner mimic heredity natural selection random mutation. offspring networks trained fully functional networks like would train newborn efﬁcient diverse network architectures ancestor networks achieving powerful modeling capabilities. experimental results task visual saliency demonstrated synthesized ‘evolved’ offspring networks achieve state-ofthe-art performance network architectures signiﬁcantly efﬁcient compared original ancestor network. shown considerable promise tremendous results recent years signiﬁcantly improving accuracy variety challenging problems compared machine learning methods however deep neural networks require high performance computing systems tremendous quantity computational layers possess leading massive quantity parameters learn compute. issue architectural complexity increased greatly recent years driven demand increasingly deeper larger deep neural networks boost modeling accuracy. such become increasingly difﬁcult take advantage complex deep neural networks scenarios computational energy resources scarce. manuscript received xxxx revised xxxx research supported canada research chairs programs natural sciences engineering research council canada ministry research innovation ontario. authors also thank nvidia hardware used study nvidia hardware grant program. best authors’ knowledge approach introduced study ﬁrst achieve evolution synthesis deep neural networks deep large neural network architectures demonstrated provide great performance recent years previous studies focused deep neural networks smaller shallower network architectures approaches used studies difﬁcult scale deep large network architectures. synaptic probability model encoding architectural traits ancestor network also environmental factor model mimic environmental conditions help drive natural selection random manner drives random mutation. speciﬁcally synapse synthesized randomly possible neurons descendant network based environmental factor model neurons descendant network synthesized subsequently based synthesized synapses. such architecture descendant network generation synthesized randomly synthesis probability expressed better intuitive understanding examine illustrative example impose environmental conditions using promote evolution highly efﬁcient deep neural networks. efﬁciency-driven evolutionary synthesis. main environmental factors encouraging energy efﬁciency evolution restrict resources available. example study moran proposed eyeless mexican caveﬁsh lost vision system generations high energetic cost neural tissue food availability subterranean habitats. study demonstrated cost vision resting metabolism eyed phenotype thus losing vision system evolution signiﬁcant energy savings thus improves survivability. such inspired computationally restrict resources available descendant networks encourage evolution highly-efﬁcient deep neural networks. considering aforementioned example descendant networks must take network architectures efﬁcient energy consumption original ancestor network able survive. main factor energy consumption quantity synapses neurons network. therefore mimic environmental constraints encourage evolution highly-efﬁcient deep neural networks introduce environmental constraint probabilistically constrains quantity synapses synthesized descendant network descendant networks forced evolve efﬁcient network architectures ancestor networks. therefore given synthesis probability formulated highest percentage synapses desired descendant network. random element network synthesis process mimics random mutation process promotes network architectural diversity. proposed evolutionary synthesis deep neural networks primarily inspired real biological evolutionary mechanisms. nature traits passed generation generation change successive generations factors natural selection random mutation giving rise diversity enhanced traits later generations. realize idea evolutionary synthesis producing deep neural networks introduce number computational constructs mimic following mechanisms biological evolution heredity natural selection iii) random mutation. heredity. here mimic idea heredity encoding architectural traits deep neural networks form synaptic probability models used pass traits generation generation. view synaptic probability models ‘dna’ networks. denote possible architecture deep neural network denoting possible neurons denoting possible synapses denoting synapse neurons encode architectural traits deep neural network denotes conditional probability architecture network generation given architecture ancestor network generation treat areas strong synapses ancestor network generation desirable traits inherited descendant networks generation descendant networks higher probability similar areas strong synapses ancestor network instead encode architectural traits deep neural network synaptic probability wg−k encodes synaptic strength synapse sg−k. modeling exponential distribution probability synapse network assumed independently distributed arrives normalization constant. natural selection random mutation. ideas natural selection random mutation mimicked introduction network synthesis process synthesizing descendant networks takes account ancestor network generation synaptic probability model constructed according using environmental constraint synthesis probability constructed according synthesize descendant nework generation synapse descendant network synthesized randomly follows uniformly distributed random number uniform distribution synthesized descendant networks generation trained fully-functional networks like would train newborn evolutionary synthesis process repeated producing successive generations descendant networks. investigate efﬁcacy proposed evolutionary synthesis highly-efﬁcient deep neural networks experiments performed using msra-b hkudatasets task visual saliency. task chosen given importance biological beings detect objects interest survival complex visual environments provide interesting insights evolution networks. three generations descendant deep neural networks synthesized within artiﬁcially constrained environment beyond original ﬁrstgeneration ancestor network. environmental constraint imposed synthesis study descendant networks total number synapses direct ancestor network possesses thus encouraging evolution highly-efﬁcient deep neural networks. network architecture original ﬁrst generation ancestor network used study details tested datasets performance metrics follow. network architecture. network architecture original ﬁrst generation ancestor network used study builds upon deep convolutional neural network architecture purpose image segmentation follows. outputs stacks architecture newly added stacks respectively. output stacks stacks. concatenated outputs stacks stack. output stack stacks. finally combined output stacks softmax layer produce ﬁnal segmentation result. details different stacks follows convolutional layers local receptive ﬁelds convolutional layers local receptive ﬁelds convolutional layers local receptive ﬁelds convolutional layers local receptive ﬁelds convolutional layers local receptive ﬁelds convolutional layers local receptive ﬁelds convolutional layers local receptive ﬁelds convolutional layers local receptive ﬁelds fig. msra-b image dataset dataset contains natural images divided images training validation test samples respectively. ground truth maps provided pixel-wise annotation. examples images corresponding ground truth maps msra-b image dataset shown here. fig. hku-is image dataset dataset contains natural images entire dataset used testing group descendant deep neural networks trained training group msra-b dataset. convolutional layers local receptive ﬁelds local receptive ﬁelds deconvolutional layers. datasets. msra-b dataset consists natural images corresponding ground truth maps salient objects images segmented pixel-wise annotation. dataset divided training validation testing groups containing images respectively. figure hku-is dataset consists natural images corresponding ground truth maps salient objects images segmented pixel-wise annotation. entire dataset used testing group descendant networks trained training group msra-b dataset. figure illustrates example images dataset corresponding ground truths. performance metrics. evaluate performance evolved descendant deep neural networks different generations score metrics computed descendant deep neural networks across test images msra-b dataset used training. reference performance metrics also computed original ﬁrst generation ancestor deep neural network. architectural efﬁciency successive generations. detailed experimental results describing number synapses architectural efﬁciency score mean absolute error presented table table msra-b hku-is datasets respectively. number insightful observations made respect change architectural efﬁciency successive generations descendant deep neural networks. generation number synapses architectural efﬁciency score first observed performance differences generation descendant networks next generation small msra-b performance differences small hku-is ﬁrst generations larger performance differences third fourth generations results indicate modeling power ancestor network well-preserved descendant networks. second observed descendant networks second third generations achieve state-of-theart scores msra-b state-of-the visual saliency method) network architectures signiﬁcantly efﬁcient compared ﬁrst generation ancestor network similar trend observed hku-is though persisting second generation achieving ∼-fold decrease synapses ancestor network). remarkable descendant network fourth generation maintains strong scores network architectures incredibly efﬁcient compared ﬁrst generation ancestor network. ∼-fold increase architectural efﬁciency maintaining modeling power clearly show efﬁcacy producing highly-efﬁcient deep neural networks successive generations proposed evolutionary synthesis. visual saliency variations successive generations. gain additional insights figure demonstrate example test images msra-b dataset hku-is dataset respectively along corresponding visual saliency maps generated descendant networks different generations. observed descendant networks generations consistently identiﬁed objects interest scene visually salient. also interesting observe fourth generation ∼-fold decrease synapses compared ﬁrst generation ancestor network ability distinguish ﬁne-grained visual saliency starts diminish. observations interesting that similar biological evolution show descendant networks evolved successive generations important traits retained ancestors less important traits diminish favor adapting environmental constraints experimental results show that taking inspiration biological evolution proposed evolutionary synthesis deep neural networks lead natural evolution deep neural networks successive generations highly efﬁcient powerful deep neural networks thus promising direction future exploration deep learning. a.w. conceived concept evolutionary synthesis deep learning proposed study. m.s. a.w. formulated evolutionary synthesis process proposed study. a.m. implemented evolutionary synthesis process performed experiments study. a.w. m.s. a.m. participated writing paper.", "year": 2016}