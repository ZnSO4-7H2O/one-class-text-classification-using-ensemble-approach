{"title": "From dependency to causality: a machine learning approach", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "The relationship between statistical dependency and causality lies at the heart of all statistical approaches to causal inference. Recent results in the ChaLearn cause-effect pair challenge have shown that causal directionality can be inferred with good accuracy also in Markov indistinguishable configurations thanks to data driven approaches. This paper proposes a supervised machine learning approach to infer the existence of a directed causal link between two variables in multivariate settings with $n>2$ variables. The approach relies on the asymmetry of some conditional (in)dependence relations between the members of the Markov blankets of two variables causally connected. Our results show that supervised learning methods may be successfully used to extract causal information on the basis of asymmetric statistical descriptors also for $n>2$ variate distributions.", "text": "relationship statistical dependency causality lies heart statistical approaches causal inference. recent results chalearn cause-eﬀect pair challenge shown causal directionality inferred good accuracy also markov indistinguishable conﬁgurations thanks data driven approaches. paper proposes supervised machine learning approach infer existence directed causal link variables multivariate settings variables. approach relies asymmetry conditional dependence relations members markov blankets variables causally connected. results show supervised learning methods successfully used extract causal information basis asymmetric statistical descriptors also variate distributions. relationship statistical dependency causality lies heart statistical approaches causal inference summarized famous statements correlation imply causation causation induces statistical dependency causes eﬀects terms well known statistical dependency necessary suﬃcient condition causality. unidirectional link notions used many formal approaches causality justify adoption statistical methods detecting inferring causal links observational data. inﬂuential causal bayesian network approach detailed relies notions independence conditional independence detect causal patterns data. well known examples related inference algorithms constraint-based methods like algorithms approaches founded probability theory shown accurate reconstructing causal patterns many applications. time restrict conﬁgurations causal inference applicable boundary essentially determined notion distinguishability deﬁnes markov equivalent conﬁgurations basis conditional independence tests. typical examples indistinguishability two-variable setting completely connected triplet conﬁguration impossible distinguish cause eﬀects means conditional unconditional independence tests. hand notion indistinguishability probabilistically sound hand contributed slow development alternative methods address interesting indistinguishable causal patterns. slow opinion misunderstanding meaning role notion indistinguishability. indistiguishability results rely main aspects refer speciﬁc features dependency state conditions possible distinguish certainty conﬁgurations. accordingly indistinguishability results prevent existence statistical algorithms able reduce uncertainty causal pattern even indistinguishable conﬁgurations. made evident appearance recent years series approaches tackle cause-eﬀect pair inference like igci lingam algorithms described common approaches alternative statistical features data detect causal patterns reduce uncertainty directionality. important step direction represented recent organization chalearn cause-eﬀect pair challenge good accuracy obtained basis observations pairs causally related variables supports idea alternative strategies designed infer success indistinguishable conﬁgurations. worthy remark best ranked approaches chalearn competition share common aspect infer statistical features bivariate distribution probability existence directionality causal link variables. success approaches shows problem causal inference successfully addressed supervised machine learning approach inputs features describing probabilistic dependency output class denoting existence directed causal link. suﬃcient training data made available conventional feature selection algorithms classiﬁers used return prediction. eﬀectiveness machine learning strategies case pairs variables encourages extension strategy conﬁgurations larger number variables. paper propose original approach learn multivariate observations probability variable direct cause another. task undeniably diﬃcult develop machine learning strategy descriptors relation existing members markov blankets variables used learn probability causal link exists variables. approach relies asymmetry conditional dependence relations members markov blankets variables causally connected. resulting algorithm predicts existence direct causal link variables multivariate setting creating features relationship based asymmetric section report results experiments assessing accuracy algorithm. experimental results based synthetic published data show approach competitive often outperforms state-of-the-art methods. order represent multivariate distribution like correlation partial correlation matrix. problems however arise case parameters informative case gaussian distributions identical causal conﬁgurations could associated diﬀerent parametric values thus making diﬃcult learning mapping. terms relevant describe distribution structural terms rather parametric terms. aspects taken consideration. first since want learning approach identify cause-eﬀect relationships need quantitative features describe structure multivariate distribution. second since asymmetry distinguishing characteristic causal relationship expect eﬀective features share asymmetric properties. paper information theory represent quantify notions dependence independence variables derive asymmetric features reconstruct causality dependency. structural notion described terms conditional mutual information notion markov blanket markov blanket variable dimensional distribution smallest subset eﬀective algorithms proposed literature infer markov blanket observed data feature selection algorithms also useful construct markov blanket given target variable rely notions conditional independence select relevant variables formal representation notion causality demands extension syntax probability calculus done introduction operator allows distinguish observation value manipulation variable extension accepted variable cause variable distribution diﬀerent marginal value extension probability notation made pearl allows formalize intuition causality asymmetric. another notation allows represent causal expression provided graphical models specifically directed acyclic graphs paper limit consider causal relationships modeled proved convenient tools understand notion causality. furthermore make assumption causal relationships existing variables interest described markov faithful means accurate dependencies independencies represented distribution using notion d-separation possible read graph sets nodes dependent conditioned third. asymmetric nature causality suggests want infer causal links dependency need features describe dependency share causality property asymmetry. suppose interested predicting existence asymmetric property causality want maximize chances reconstruct causality dependency need identify relevant asymmetric descriptors. order deﬁne useful asymmetric descriptors recourse markov blankets variables consider instance portion represented figure variable direct cause ﬁgure shows also markov blankets variables components i.e. direct causes direct eﬀects spouses spouses discuss assumptions section. given assumptions d-separation number asymmetric conditional dependence relations holds members instance conditioning eﬀect create dependence direct causes conditioning d-separate direct causes asymmetric properties four descriptors encouraging want exploit dependency related features infer causal properties data. however optimism undermined fact descriptors require already capability distinguishing causes eﬀects markov blanket given variable. unfortunately discriminating capability looking order escape circularity problem consider solutions. ﬁrst recourse preliminary phase prioritizes components markov blanket result starting point detect asymmetries improve classiﬁcation causal links. instance feasible using ﬁlter selection algorithm like mimr aims prioritize direct causes markov blanket searching pairs variables high relevance interaction. second solution related fact asymmetry four descriptors induces diﬀerence distributions information theoretic terms require distinction causes eﬀects within markov blanket. consequence replace descriptors descriptors actually estimated data. denote generic component markov blanket distinction cause eﬀect spouse. follows population made terms depending mixture three subpopulations ﬁrst made causes second made eﬀects third spouses respectively. follows distribution population ﬁnite mixture three distributions ﬁrst related causes second eﬀects third spouses. since moments ﬁnite mixture functions moments component derive properties resulting mixture properties component. instance show subpopulations identical elements third subpopulation ﬁrst mixture larger elements analogous subpopulation second mixture derive mixture distributions diﬀerent. previous results encouraging show though able distinguish diﬀerent components markov blanket notwithstanding compute quantities whose asymmetry informative causal relationconsequence measuring observed data statistics related distribution asymmetric descriptors obtain insight causal relationship variables. idea made explicit algorithm described following section. though results rely assumptions made before considerations worthy made. first main goal approach shed light existence dependency asymmetries also multivariate contributions. secondly expect second layer eventually compensate conﬁgurations compliant assumptions take advantage complementarity synergy descriptors discriminating causal conﬁgurations. rationale algorithm predict existence causal link variables multivariate setting creating features relationship members markov blankets variables using classiﬁer learn mapping features presence causal link. sets features summarize relation markov blankets ﬁrst accounts presence terms viceversa. instance evident cause expect highly ranked causal terms absent among causes second features based results previous section obtained summarizing distributions asymmetric descriptors quantiles. rationale algorithm asymmetries induce asymmetry distributions quantiles distributions provide information directionality causal link terms expect distribution variables return useful information cause eﬀect. note distributions would informative able rank terms markov blankets prioritizing direct causes since terms play major role asymmetries table algorithm improved choosing appropriate markov blanket selector algorithms like mimr ﬁlter. experiments derive information terms diﬀerence entropy terms estimated lazy learning regression algorithm making assumption gaussian noise. lazy learning returns leaveone-out estimation conditional variance easily transformed entropy normal assumption mutual information terms obtained using relations suggested reviewers interesting make complexity analysis approach ﬁrst important remark since approach relies classiﬁer learning phase time-consuming dependent number samples dimension. however step supposed performed user perspective relevant consider cost testing phase. given nodes test existence causal link required three steps performed computation markov blankets nodes. information ﬁlters used complexity cost computation mutual information case large complexity bounded ﬁlter preceded ranking algorithm complexity ranking limit number number members chosen rest procedure complexity related estimation number descriptors. paper used local learning regression algorithm estimate conditional entropies terms. given regression involves three terms complexity essentially related linearly number samples last step consists computation random forest predictions test set. since already trained complexity step depends number trees dimensionality number samples. experimental session addresses problem inferring causal links synthetic data generated linear non-linear conﬁgurations diﬀerent sizes. variables continuous dependency children parents modelled additive relationship assessment procedure relies generation number structures simulation them node observations according dependency dataset removed observations percent variables order introduce unobserved variables. generating dags storing descriptors associated positives examples negatives examples random forest classiﬁer trained balanced dataset implementation package randomforest default setting. independent test obtained considering independent number simulated dags. consider dags small medium conﬁgurations large conﬁguration. testing select positives examples negatives examples predictive accuracy trained random forest classiﬁer assessed test set. grow-shrink constraint-based structure learning algorithm hill-climbing score-based structure learning algorithm iamb incremental association constraint-based structure learning second part assessment relies simulated resimulated datasets proposed table datasets obtained simulating data known bayesian networks also resimulation real data used elicit causal network data simulated obtained network. split datasets portions training portion second portion testing. done order assess accuracy versions algorithm ﬁrst uses training samples generated previous section second includes training also datasets training portion. goal assess generalization accuracy algorithm respect distributions never encountered included training set. section compare algorithms implemented causal explorer software grow/shrink algorithm iamb incremental association-based markov blanket iambnpc iamb algorithm pruning phase interiambnpc iamb algorithm interleaved pruning ﬁlters based information theory mrmr mimr comparison done follows dataset node causal inference techniques return ranking inferred parents. ranking assessed terms average area precision recall curve t-test used assess auprc values signiﬁcantly diﬀerent methods. note higher auprc accurate inference method. summary paired comparisons reported table algorithm trained synthetic data table algorithm trained synthetic data training datasets. attitudes common respect causal inference observational data. ﬁrst pessimistic motivated consideration correlation imply causation. second optimistic driven fact causation implies correlation paper belongs evidently second school thought relies conﬁdence causality leaves footprints form stochastic dependency footprints detected retrieve causality observational data. results chalearn challenge preliminary results paper conﬁrm potential machine learning approaches predicting existence causality links basis statistical descriptors dependency. convinced open research direction learning techniques used reduce degree uncertainty existence causal relationships also indistinguishable conﬁgurations typically addressed conditional independence approaches. work focus discovering additional features multivariate distributions improve accuracy addressing assessing related classiﬁcation problems extending work partial ancestral graphs", "year": 2014}