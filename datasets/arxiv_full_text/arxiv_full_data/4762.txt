{"title": "A Parallel and Efficient Algorithm for Learning to Match", "tag": ["cs.LG", "cs.AI"], "abstract": "Many tasks in data mining and related fields can be formalized as matching between objects in two heterogeneous domains, including collaborative filtering, link prediction, image tagging, and web search. Machine learning techniques, referred to as learning-to-match in this paper, have been successfully applied to the problems. Among them, a class of state-of-the-art methods, named feature-based matrix factorization, formalize the task as an extension to matrix factorization by incorporating auxiliary features into the model. Unfortunately, making those algorithms scale to real world problems is challenging, and simple parallelization strategies fail due to the complex cross talking patterns between sub-tasks. In this paper, we tackle this challenge with a novel parallel and efficient algorithm for feature-based matrix factorization. Our algorithm, based on coordinate descent, can easily handle hundreds of millions of instances and features on a single machine. The key recipe of this algorithm is an iterative relaxation of the objective to facilitate parallel updates of parameters, with guaranteed convergence on minimizing the original objective function. Experimental results demonstrate that the proposed method is effective on a wide range of matching problems, with efficiency significantly improved upon the baselines while accuracy retained unchanged.", "text": "many tasks data mining related ﬁelds formalized matching objects heterogeneous domains including collaborative ﬁltering link prediction image tagging search. machine learning techniques referred learning-to-match paper successfully applied problems. among them class state-of-the-art methods named feature-based matrix factorization formalize task extension matrix factorization incorporating auxiliary features model. unfortunately making algorithms scale real world problems challenging simple parallelization strategies fail complex cross talking patterns subtasks. paper tackle challenge novel parallel eﬃcient algorithm feature-based matrix factorization. algorithm based coordinate descent easily handle hundreds millions instances features single machine. recipe algorithm iterative relaxation objective facilitate parallel updates parameters guaranteed convergence minimizing original objective function. experimental results demonstrate proposed method eﬀective wide range matching problems eﬃciency significantly improved upon baselines accuracy retained unchanged. many application tasks formalized matching objects heterogeneous domains association objects information objects given. refer objects domain queries targets distinction usually clear context. example collaborative ﬁltering given items manages users best match items using preference users items well features users items. another example image tagging wants associate tags images based tagged images well features tags images. recent years observed great success employing machine learning techniques referred learning-to-match paper solve matching problems. among existing approaches family factorization models make feature spaces encode additional information stand state-of-the-art matching tasks. examples include factorization machines featurebased latent factor models link prediction regression-based latent factor models refer class methods feature-based matrix factorization paper. basic idea formalize task extension plain matrix factorization incorporating features objects model. make full available information task improve accuracies. fact best performer many real world matching tasks. collaborative ﬁltering models using user feedback attribute content outperformed models including plain matrix factorization. search models calculating matching scores queries documents signiﬁcantly enhanced relevance ranking models also successfully employed link prediction adopted champion teams learning model conducted coordinate descent algorithm stochastic gradient descent algorithm. since matching problem usually large scale hundreds millions objects features more easily become hard manage. therefore necessary develop parallel eﬃcient algorithm fmf. exactly problem attempt address paper. making scalable eﬃcient much diﬃcult appears following challenges. first training requires simultaneous access features thus existing techniques parallelization matrix factorization directly applicable. second computation complexity coordinate descent algorithm still high easily fail single machine scale problem becomes large calling techniques signiﬁcantly accelerate computation. making repeating patterns least-squares probit losses scaled coordinate descent provide guarantee general convex loss functions. existed parallel coordinate descent algorithms complex feature dependencies cannot directly applied here. hogwild algorithm parallel stochastic gradient descent applied here generic algorithm thus still ineﬃcient fmf. paper tackle challenges developing parallel eﬃcient algorithm tailored learningto-match. algorithm referred parallel eﬃcient algorithm learning-to-match parallelizes accelerates coordinate descent algorithm iteratively relaxing objective facilitate parallel updates parameters avoiding repeated calculations caused features. main contributions paper follows. feature-based matrix factorization iteratively relaxes objective parallel updates parameters neatly avoids repeated calculations caused features general convex loss functions. algorithms minimizing original objective function veriﬁed extensive experiments. parallel algorithm automatically adjust rate parallel updates according conditions learning. ciency proposed algorithm four benchmark datasets. parallel algorithm achieves nearly linear speedup proposed acceleration helps parallel algorithm times faster hogwild algorithm average using threads. given importance models diﬃculty parallelization work paper represents signiﬁcant contribution study learning match. best knowledge ﬁrst eﬀort scalability general models. rest paper organized follows. section gives formal description generalized matrix factorization section explains eﬃcient coordinate descent algorithm. section describes parallelization coordinate descent algorithm. related work introduced section experimental results provided section finally paper concluded section section give formal deﬁnition learning-tomatch formulation feature-based matrix factorization. also present motivation parallelizing learning task. setting rather general subsumes many application problems. example collaborative ﬁltering user’s preference item interpreted matching score user item. social link prediction likelihood link nodes network regarded matching score nodes. search general document retrieval also formalized problem ﬁrst matching given query documents ranking documents based matching scores. query target instances heterogeneous feature spaces direct match generally impossible. instead feature vectors domains latent space perform matching images feature vectors latent space. calculate matching score query-target pair feature vectors feature spaces latent space. latent factors query instance target instance paper denote column matrix denote column matrix refer model equation model feature-based matrix factorization. model also interpreted linear matching function latent factors latent factor instance also fact observed models achieve state-of-the-art results many diﬀerent tasks outperforming models margin. example collaborative ﬁltering user feedback user attribute product attribute incorporated models improve accuracies prediction. search term vectors queries documents used features signiﬁcantly improve relevance ranking. models also give best results link prediction success models strongly indicates necessity scaling corresponding learning algorithms given existing algorithms still cannot easily handle large datasets. making repeating patterns least-squares probit losses scaled coordinate descent guarantee general convex loss functions. algorithms parallel coordinate descent cannot directly applied diﬃcult handle complex feature dependencies fmf. hogwild algorithm parallel stochastic gradient descent applied here generic algorithm thus still ineﬃcient fmf. best knowledge work paper ﬁrst eﬀort scalability learning-to-match i.e. feature-based matrix factorization. model shown figure contains many existing models feature-based matrix factorization special cases informative features available objects domains feature matrices contain indices objects. clearly features learning-to-match crucial accuracy task. usually model properly optimized produce higher accuracy prediction model plan matrix factorization model leverage information prediction particularly feature information model rely relations instances usually sparse. example task recommendation entries observed. tencent weibo changed update trivial unchanged. algorithm keeps making updated ensure convergence algorithm. eﬃcient algorithm learning-to-match shown algorithm intuitively updating corresponds minimizing quadratic upper bound original convex loss re-estimated round. formally values zero beginning. need sequentially update diﬀerent minimize assuming already note start update. inequality equation shows original loss function decreases round update hence proves convergence algorithm diﬀerentiable convex loss function. supy exploit standard technique learn model using coordinate descent shown algorithm here deﬁned using following thresholding function handle optimization norm regularization term aﬀects result function thus makes part algorithm independent regularization. note implicitly assume buﬀered kept date needed algorithm. denote numbers nonzero entries feature matrices denote numbers query target instances denotes average number nonzero features pair note time complexity give eﬃcient algorithm learning-to-match avoiding repeated calculations caused features. little works focused acceleration models. relevant scaling speciﬁc coordinate descent making repeating patterns features however specialized least-squares probit losses. although idea similar avoiding repeated calculations work extend idea general convex loss functions. exist repeated calculations summations query target calculating algorithm gives chance speed algorithm. introduce auxiliary variables calculated paper matrix operations algorithms taken advantages sparsity e.g. summations nonzero entries implicitly. time complex analysis implementations algorithms. caused parallel updates. speciﬁcally consider parallelizing accelerating algorithm statistics calculation preprocessing steps algorithm naturally separated several independent tasks thus fully parallelized. however strong dependency within update steps making parallelization diﬃcult task. discuss solve problem next. applied parallel. surprisingly condition hold many real world scenarios. need remove troublesome cross terms second line deriving adaptive estimation conﬂicts caused parallel updates speciﬁcally inequality relaxation performed iteratively optimization still attempts optimize original objective equation case much analogous expectation-maximization algorithm ﬁnding maximum-likelihood solution. change parallel update. since parallel update optimizes following shown parallel update ∆pks shrunken compared sequential update. intuitively depends co-occurrence features features rarely co-occur close means update aggressively. features co-occur frequently small need update conservatively. extreme case feature co-occurs other perfect parallelization without loss update eﬃciency. another extreme case duplicated features extremely conservative given size advantage algorithm automatically adjusts level conservativeness condition learning thus always ensures convergence algorithm regardless number threads nature dataset. thus parallel coordinate needs chosen balance convergence acceleration. fact need empirically choose instance covered nonzero features task size large enough fairly balanced way. feature indices generate disjoint subsets round. note sophisticated scheduling strategies select beyond scope paper interesting topic future research. models arguably successful approach learning-to-match. applied wide range real world problems especially models achieve state-of-the-art results outperforming models many diﬀerent tasks diﬀerent types features used. collaborative ﬁltering user feedback information user attribute information product attribute information incorporated models further enhance accuracies prediction. search term vectors queries documents utilized features signiﬁcantly improve relevance ranking. models also give best results link prediction works demonstrate eﬀectiveness learning-to-match models also create necessity parallelization learning algorithms. much eﬀort parallelizing process plain matrix factorization. example gemulla propose method distributed stochastic gradient descent introduce parallel coordinate descent algorithm alternating least square method proposed well recently zhuang improve eﬃciency parallel stochastic gradient descent making better scheduling updates. propose distributed algorithm nonnegative matrix factorization dyadic data analysis. method probabilistic latent semantic indexing parallelized google news recommendation however models parallelizing plain matrix factorization replies fact rows columns naturally separated parameters independently updated therefore canwork complex feature dependencies updating steps. little work focusing acceleration coordinate descent fmf. recent scales coordinate descent making repeating patterns features however specialized least-squares loss probit loss. although idea avoiding repeated calculations similar algorithm takes completely diﬀerent approach handle general convex loss functions. algorithms parallel coordinate descent cannot directly applied diﬃcult handle complex feature inequality indicates compared ideal case features co-occur parallel update’s contribution loss change scaled ηks. analysis also intuitively justiﬁes controls eﬃciency update. corresponds average number observed target instances query instance. similarly updates speedup times. therefore overall speedup algorithm algorithm least complexity parallel eﬃcient learningto-match algorithm described algorithm using threads algorithm computation cost round update +z)+|o|)) denotes variance applied problem. however time algorithm repeated calculations. using number threads analyzed time complexity sections algorithm. experiments parallel algorithm runs averagely times faster hogwild. another thread related work parallelization coordinate descent algorithms. studies parallelizing coordinate descent linear regression matrix factorization convergence algorithms depends spectrum covariance matrix changes round learning setting thus algorithms cannot directly applied problem. algorithm makes parallel update minimize upper bound re-estimated round ensure convergence also viewed kind minorization-maximization algorithm section introduce experimental results several matching tasks using benchmark datasets. ﬁrst conduct comparison accuracies feature-based matrix factorization plain matrix factorization. make comparisons accuracies eﬃciencies method parallel learning-to-match baselines including hogwild finally conduct analysis eﬃciency parallel learning algorithm. ﬁrst dataset yahoo music track yahoo music website. dataset among largest public datasets collaborative ﬁltering. oﬃcial split dataset experiments. features implicit feedback users well taxonomical information tracks albums artists addition indicators users tracks. item rating dataset choose square loss loss function root mean square error evaluation measure. second dataset tencent weibo social link prediction. task predict potential list celebrities user follow. dataset split training test data time test data split public private sets independent evaluations. training learning public test evaluation. logistic loss loss function mapk evaluation metric oﬃcially adopted competition matrix data extremely sparse average positive links user. furthermore users test following records training set. however lots additional information available including social network interaction records proﬁles users categories celebrities tags/keywords users. information used features task. third dataset automatic annotating images crawled flickr. dataset contains million images image associated average four tags. select frequently occurring tags set. randomly select images test rest images training set. bag-ofwords vector sift descriptors features images indicator vectors features tags. logistic loss chosen loss function. testing generate rank list tags evaluation metrics. fourth dataset also collaborative ﬁltering provided movielens. oﬃcial split dataset experiments. dataset added hogwild cannot yahoo music dataset high time complexity. addition indicators users movies implicit feedbacks users used features. similar yahoo music dataset choose square loss loss function rmse evaluation metric. implemented parallel eﬃcient algorithm learning-to-match using openmp. experiments conducted machine intel xeon utilize working threads reserve thread scheduling. directly applied problem mentioned section also implemented hogwild using openmp. matrix operations mentioned algorithms take advantages data sparsity. hogwild share codes elementary operations. make comparison investigate eﬀectiveness features. ﬁrst compare terms test rmse yahoo music dataset figure result model converges faster achieves better results model. result consistent result reported conﬁrms importance using features problem. test error ﬁrst decreases increases diﬀerent rounds training indicating training stop rounds. results tencent weibo dataset shown table since gives similar performance popularity based algorithm considers popularity target node thus result reported. suﬃx stands model using available features shown table also evaluate performance model social network information suﬃx sns. table dataset extremely biased toward popular nodes. however still possible improve results using social network information auxiliary features help achieve best performance. note plm--all achieved best result tencent weibo dataset performance flickr test shown table training test images overlap cannot make prediction thus adopt popularity scores baseline. result improve upon popularity method assign relevant tags using image content features. test rmse curves diﬀerent algorithms movielensm dataset shown figure result model converges faster achieves better results model. result demonstrates importance using features problem. test error ﬁrst decreases increases diﬀerent rounds training indicates training stopped figure gives training loss curves ﬁgure observe always converges following lower bound given beginning. consistent theoretical result convergence section start initial values perform well things changed however training goes tencent weibo dataset plm- converges slightly better rounds. movielens-m dataset although training loss curves plm- plm- almost same training loss lower end. fact loss function non-convex several updates example rounds values quite diﬀerent methods ﬁnally converge diﬀerent local minimums. ﬁgures also observe plm-k converges slower plm-k yahoo music dataset plm-k converges slower plm- tencent weibo dataset plm- converges slower plm- flickr dateset. plm- converges little slower plm- movielens-m dateset. consistent make comparison hogwild. since hogwild based stochastic gradient descent parameters learning rate need tuned. tuning parameters using cross validation training hogwild including learning rate coeﬃcient coeﬃcient obtained performance table including test error running time round training number rounds needed best test error. hogwild threads. running time round much shorter hogwild test errors similar. diﬀerence running time tencent weibo much movielens-m flickr datasets less features. consistent theoretical result time complexity section section furthermore hogwild needs training achieve best test errors. example hogwild needs rounds needs rounds achieve best test errors movielens-m. therefore total running time best performance much smaller finally evaluate scalability parallel learningto-match algorithm test average running time plm-k yahoo music dataset plm- tencent weibo dataset plm- flickr dataset movielens-m varying numbers threads evaluate improvement eﬃciency. shown figure speedup curves similar yahoo music tencent weibo flickr datasets curve converges earlier movielens-m dataset. movielens-m relatively smaller others plm- runs really fast movielens-m needs seconds threads used. although speedup gained parallelization much datasets parallel algorithm also provide accelerations. ﬁrst datasets achieve almost linear speedup less threads speedup gain slows threads. observe working threads still fully occupied conjecture turning point fact proposed parallel eﬃcient algorithm learning-to-match speciﬁcally feature-based matrix factorization general state-of-the-art approach. algorithm employs iterative relaxations solve conﬂicts caused parallel updates provable convergence guarantee minimizing original objective function accelerate computation avoiding repeated calculations caused features general convex loss functions. result algorithm easily handle data hundreds millions objects features single machine. extensive experimental results show algorithm eﬀective eﬃcient compared baselines. future work plan extend algorithm distributed setting instead current multi-threading better scheduling strategies making parallel updates guaranteed bound speedup apply technique developed paper parallelization learning methods markov chain monte carlo learning methods learning-to-match problem. parallel coordinate descent l-regularized loss minimization. getoor scheﬀer editors proceedings international conference machine learning icml pages york june acm. haglin. feature clustering accelerating parallel coordinate descent. bartlett pereira burges bottou weinberger editors advances neural information processing systems pages distributed nonnegative matrix factorization web-scale dyadic data analysis mapreduce. proceedings international conference world wide pages york acm. latent collaborative retrieval. langford pineau editors proceedings international conference machine learning icml pages york july omnipress. document similarities click-through bipartite graph metadata. proceedings sixth international conference search data mining wsdm pages york acm. large-scale parallel collaborative ﬁltering netﬂix prize. proceedings international conference algorithmic aspects information management aaim pages berlin heidelberg springer-verlag. lock-free approach parallelizing stochastic gradient descent. shawe-taylor zemel bartlett pereira weinberger editors advances neural information processing systems pages", "year": 2014}