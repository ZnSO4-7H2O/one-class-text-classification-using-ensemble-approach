{"title": "VAIN: Attentional Multi-agent Predictive Modeling", "tag": ["cs.LG", "cs.AI"], "abstract": "Multi-agent predictive modeling is an essential step for understanding physical, social and team-play systems. Recently, Interaction Networks (INs) were proposed for the task of modeling multi-agent physical systems, INs scale with the number of interactions in the system (typically quadratic or higher order in the number of agents). In this paper we introduce VAIN, a novel attentional architecture for multi-agent predictive modeling that scales linearly with the number of agents. We show that VAIN is effective for multi-agent predictive modeling. Our method is evaluated on tasks from challenging multi-agent prediction domains: chess and soccer, and outperforms competing multi-agent approaches.", "text": "multi-agent predictive modeling essential step understanding physical social team-play systems. recently interaction networks proposed task modeling multi-agent physical systems scale number interactions system paper introduce vain novel attentional architecture multi-agent predictive modeling scales linearly number agents. show vain effective multi-agent predictive modeling. method evaluated tasks challenging multi-agent prediction domains chess soccer outperforms competing multi-agent approaches. modeling multi-agent interactions essential understanding world. physical world governed well-understood multi-agent interactions including fundamental forces well macroscopic phenomena social world also governed multi-agent interactions often imperfectly understood. games chess simple well deﬁned rules move dynamics governed complex policies. modeling inference multi-agent interaction observational data therefore important step towards machine intelligence. deep neural networks much success machine perception e.g. computer vision natural language processing speech recognition problems usually temporal and/or spatial structure makes amenable particular neural architectures convolutional recurrent neural networks multi-agent interactions different machine perception several ways model simple interactions physics simulation context interaction networks proposed battaglia interaction networks model interaction physical interaction graph neural network. additive vector outputs interactions global interaction vector obtained. global interaction alongside object features used predict future velocity object. shown interaction networks trained different numbers physical agents generate accurate results simple physical scenarios nature interaction additive binary number agents small. although interaction networks suitable physical domain introduced signiﬁcant drawbacks prevent efﬁciently extensible general multiagent interaction scenarios. network complexity number objects typical interaction clique size. fundamental physics interactions simulated method resulting quadratic dependence higher order interactions become completely unmanageable. social lstm remedied pooling local neighborhood interactions. solution however cannot work scenarios long-range interactions. another solution offered battaglia several fully connected layers modeling high-order interactions. approach struggles objective select agents results distributed representation loses structure problem. work present vain novel multi-agent attentional neural network predictive modeling. vain’s attention mechanism helps modeling locality interactions improves performance determining agents share information. vain said commnet novel attention mechanism factorized interaction network made concrete sec. show vain model high-order interactions linear complexity number vertexes preserving structure problem lower complexity cases many fewer vertexes edges evaluation introduce non-physical tasks closely resemble real-world game-playing multi-agent predictive modeling well physical bouncing balls task. non-physical tasks taken chess soccer contain different types interactions different data regimes. interaction graph tasks known apriori typical nature. informal analysis architecture presented sec. method presented sec. description experimental evaluation scenarios presented sec. results provided sec. conclusion future work presented sec. related work work primarily concerned learning multi-agent interactions graph structures. seminal works graph neural networks presented scarselli another notable iterative graph-like neural algorithm neural-gpu notable works graph includes spectral networks work duvenaud ﬁngerprinting chemical molecules. related approaches learn multi-agent interactions graph structure interaction networks learn physical simulation objects exhibit binary relations communication networks presented learning optimal communications agents. differences approach vain previous approaches commnets analyzed detail sec. another recent approach pointnet every point point cloud embedded deep neural embeddings pooled globally. resulting descriptor used classiﬁcation segmentation. although related approach paper focused point clouds rather multi-agent systems. different approach presented social lstm learns social interaction jointly training multiple interacting lstms. complexity approach quadratic number agents requiring local pooling deals short range interactions limit number interacting bodies. attentional mechanism vain connection memory networks neural turning machines works dealing multi-agent reinforcement learning include much work board game bots approaches include chess backgammons concurrent work found arxiv concurrent submissions relevant work. santoro discovered architecture nearly identical interaction nets achieves excellent performance clevr dataset leave comparison clevr future work. vaswani architecture bears similarity vain achieving state-ofsection give informal analysis multi-agent interaction architectures presented interaction networks commnets vain. interaction networks model interaction neural network. simplicity analysis restrict interactions order. ψint interaction agents non-interacting features agent output given function ψint non-interacting features single step evaluation output entire system requires evaluations ψint. alternative architecture presented commnets interactions modeled explicitly. instead interaction vector computed agent ψcom. output computed single step evaluation commnet architecture requires evaluations ψcom. signiﬁcant drawback representation explicitly modeling interactions putting whole burden modeling often result weaker performance vain’s architecture preserves complexity advantages commnet addressing limitations comparison instead requiring full network evaluation every interaction pair ψint learns communication vector vain agent additionally attention vector vain. strength interaction agents modulated kernel function e|ai−aj|. interaction approximated cases kernel function good approximation relative strength interaction vain presents efﬁcient linear approximation preserves commnet’s complexity although physical interactions often additive many interesting cases additive. cases average instead used physical scenarios presented therefore always used whereas non-physical cases considered therefore averaging used). non-additive cases vain uses softmax section model interaction agents denoted a...an output either prediction every agent system-level prediction although possible multiple hops presentation uses single features extracted every agent denote features features guided basic domain knowledge figure schematic single-hop vain agent features embedded singleton encoder yield encoding attention computed vector agent attention-weighted embeddings attention weights computed softmax −||ai aj||. diagonal zero exclude self-interactions. iii) singleton codes concatenated pooled feature yield intermediate feature feature passed decoding network yield per-agent vector regression ﬁnal output network. vii) classiﬁcation scalar passed softmax. agent encoding functions singleton encoder single-agent features communication encoder interaction agents singleton encoding function applied agent features yield singleton encoding deﬁne communication encoding function encoding function applied agent features yield encoding attention vector attention vector used addressing agents information exchange sought. implemented fully connected neural networks contrast average pooling mechanism used commnets show yields better results. motivation average information relevant agents weights tmaxj give measure interaction agents. although naively operation scales quadratically number agents multiplied feature dimension rather full evaluation therefore signiﬁcantly smaller cost calculations carried algorithm. case number agents large cost still mitigated softmax operation often yields sparse matrix cases interaction modeled k-nearest neighbors calculation cheaper evaluating times cases even cheap operation expensive recommend default commnets truly complexity. pooled-feature concatenated original features form intermediate features regression problems per-agent output vain. classiﬁcation problems designed give scalar outputs. result passed softmax layer yielding agent probabilities several advantages vain interaction networks apparent representational power vain assume interaction graph pre-speciﬁed pre-specifying graph structure advantageous clearly known e.g. spring-systems locality makes signiﬁcant difference. many multi-agent scenarios graph structure known apriori. multiple-hops give vain potential model higher-order interactions although found advantageous experiments. complexity explained sec. vain features better complexity ins. complexity advantage increases order interaction. presented vain efﬁcient attentional model predictive modeling multi-agent interactions. section show model achieves better results competing methods lower computational complexity. perform experiments tasks different multi-agent domains highlight utility generality vain chess move soccer player prediction. chess piece prediction chess board game involving complex multi-agent interactions. several properties chess make particularly difﬁcult multi-agent perspective different types agents distinct behaviors. well deﬁned goal near-optimal policies professional games. many interactions non-local long ranged. given time multiple pieces interacting high-order clique although envisage deep cnns achieve best performance task objective chess test-bed multi-agent interactive system predictors using simple features every agent. recent attempts building optimal neural chess player please refer position illustrates challenges chess non-local interactions large variety agents blockers hidden implied threats high order interactions categories piece types chess category formed combination piece type color. types pawn rook knight bishop queen king colors black white. chess board consists squares every piece category situated particular board square methods evaluated task features output piece position input output label would mean piece features move next). possible input pieces case fewer pieces present missing pieces given feature values training evaluation task downloaded games fics games dataset on-line repository chess games. games used standard games professionally ranked players. randomly sampled games used training remaining games rand random piece selection. standard three hidden layers input one-hot encoding features pieces output index output agent. method requires indexing learned. per-piece embedding neural network scalar output. outputs input pieces softmax classiﬁer predicting output label. note method preserves structure problem model high-order interactions. one-hop network followed deep classiﬁer. classiﬁer predicts label next moving piece note deep classiﬁer removes structure problem. classiﬁer therefore learn index. soccer players team-player interaction promising application area end-to-end multi-agent modeling rules sports interaction quite complex easily formulated hand-coded rules. additional advantage predictive modeling self-supervised labeled data necessary. team-play situations many agents present interacting time making complexity method critical application. order evaluate performance vain team-play interactions soccer video player position dataset svpp dataset contains parameters soccer players tracked home matches played tromsø norwegian soccer team. sensors positioned home team player recorded player’s location heading direction movement velocity data re-sampled occur regular intervals. subsampled data sensor data rather raw-pixels. end-to-end inference raw-pixel data left future work. task evaluation predicting current state players position player time-step next seconds note task single frame rather several previous frames therefore encoders task. evaluated several methods task static trivial prediction -motion. linearly extrapolating agent displacement current linear velocity. alaf linear regressor predicting agent’s velocity using features including velocity also agent’s heading direction signiﬁcantly agent’s current ﬁeld position. figure soccer match used soccer task. chess position illustrating high-order nature interactions next move prediction. note cases vain uses agent positional sensor data rather raw-pixels. excluded second half anzhi match large sensor errors players bouncing balls following battaglia present simple physics-based experiment. scenario balls bouncing inside square container size identical balls constant size perfectly elastic. balls initialized random positions initial velocities sampled random balls collide balls walls collisions governed laws elastic collisions. task evaluate prediction displacement change velocity ball next time step. evaluate prediction accuracy method well interaction networks commnets found useful replace vain’s attention mechanism unnormalized attention function additive nature physical forces implementation soccer encoding decoding functions implemented fullyconnected neural networks layers hidden units relu activations. encoder outputs units. layer followed batchnorm layer vain batchnorm layers used. chess encoding decoding functions implemented fully-connected neural networks three layers width relu activations. followed batchnorm layers vain. bouncing balls encoding decoding function implemented fcns hidden units three layer. encoder outputs units. batchnorm units used. soccer architectures vain same. chess evaluate times smaller vain still takes times much computation used vain. bouncing balls computation budget balanced vain decreasing number hidden units constant factor. scenarios attention vector dimension shared features encoding vectors regression problems trained loss classiﬁcation problems trained cross-entropy loss. methods implemented pytorch linux environment. end-to-end optimization carried using adam regularization used. learning rate halved every epochs. chess prediction training took several hours tasks shorter training times smaller datasets. qualitative visualization ﬁrst look attention maps generated vain experimental scenarios. visualization serves tool understanding nature interactions agents. note vain receives feedback future prediction never receives explicit supervision nature interaction agents. bouncing balls fig. observe attention maps different balls bouncing balls scenario. position ball represented circle. velocity ball indicated figure visualization attention bouncing balls scenario. target ball blue others green. brightness ball indicates strength attention respect target ball. arrows indicate direction motion. left image ball nearer target ball receives stronger attention. right image ball collision course target ball receives much stronger attention nearest neighbor target ball. figure visualization attention soccer scenario. target ball blue others green. brightness ball indicates strength attention respect target ball. arrows indicate direction motion. example mean-ﬁeld type attention nearest-neighbors receive privileged attention also ﬁeld players receive roughly equal attention. goal keeper typically receives attention away. line extending center circle length line proportional speed ball. ﬁgure choose target ball paint blue. attention strength agent respect indicated shade circle. brighter circle stronger attention. ﬁrst scenario observe balls near target receive attention whereas balls suppressed. shows system exploits sparsity locality inherent multi-agent system. second scenario observe ball collision course target receives much stronger attention relative ball much closer target likely collide indicates vain learns important attention features beyond simple positional hand-crafted features typically used. soccer visualizations soccer scenario seen fig. positions players indicated green circles apart target player indicated blue circle. brightness circle chosen proportional strength attention player target player. arrows proportional player velocity. scenario attention nearest players strongest attention given ﬁeld players. goal keeper normally receives attention example mean-ﬁeld rather sparse attention. chess chess scenario attention maps easily interpretable. think interactions chess complex high-order. main visible trend stronger attention important nearby pieces. chess results next moving chess piece prediction seen table. method clearly outperforms competing baselines illustrating vain effective selection type problems i.e. selecting agents according criterion non-interactive method performs much better rand statistics moves. interactive methods naturally perform better interactions pieces important deciding next mover. interesting simple method performs better think table accuracy results next moving piece experiments. indexing-based methods performed better unigramstatistic based method method indexing models higher order interactions signiﬁcantly outperformed methods task. vain’s attention mechanism outperforms commnet ins. table soccer prediction errors three datasets evaluated leave-one-out protocol. evaluated methods. non-interactive methods motion linear velocity linear features features interactive methods interaction networks commnet vain methods performed better trivial no-motion. methods using features performed better velocity only better linear-only. interactive methods signiﬁcantly outperformed non-interactive methods vain outperforming methods. conclude task vain could capture multi-agent interaction without modeling interaction individually thus size classiﬁer hop−f ﬁnds hard recover indexes average pooling layer. shows one-hop networks followed fully connected classiﬁers struggle selection-type problems. method performs much better per-vertex outputs coupling agents. also performs signiﬁcantly better learn indexing. outperforms vanilla commnet showing advantages attentional mechanism. also outperforms followed per-agent softmax even though performs around times computation vain. soccer evaluated methods svpp dataset. prediction errors table. broken different time-steps different train test datasets splits. seen non-interactive baselines generally fare poorly task general conﬁguration agents informative motion agents beyond simple extrapolation motion. examples patterns picked include running back goal help defenders running team’s goal area join attack. linear model including features performs better velocity model non-linear per-player model features improves linear models. interaction network commnet vain signiﬁcantly outperform non-interactive methods. vain outperformed commnet achieving number encoder evaluations performed validates premise vain’s architecture model object interactions without modeling interaction explicitly. bouncing balls results bouncing balls experiments seen tab. physical scenario vain signiﬁcantly outperformed commnets achieves better performance interaction networks similar computation budgets. fig. difference increases small computation budgets. attention mechanism shown critical success method. table accuracy bouncing ball next step prediction. observe commnet little better no-interaction baseline noisy interaction vector. method vain improves interaction networks better commnet. figure accuracy differences vain different computation budgets vain outperforms spending computation budget larger networks rather many small networks even signiﬁcant small computation budgets. analysis limitations experiments showed vain achieves better performance architectures similar complexity equivalent performance higher complexity architectures mainly attention mechanism. ways attention mechanism implicitly encodes interactions system sparse agents signiﬁcantly interact agent attention mechanism highlight agents case commnets fail. mean-ﬁeld space found important interactions additive attention would correct weights mean ﬁeld. case commnets would work vain still improve them. vain less well-suited cases both interactions sparse important interactions give good representation interactions strong highly non-linear mean-ﬁeld approximation non-trivial. scenario body gravitation problem. interaction networks particularly well suited scenario vain’s factorization yield advantage. shown vain novel architecture factorizing interaction graphs effective predictive modeling multi-agent systems linear number neural network encoder evaluations. analyzed architecture relates interaction networks commnets. examples shown approach learned rules multi-agent system. interesting future direction pursue interpreting rules game symbolic form vain’s attention maps wij. initial experiments performed shown chess rules learned research required.", "year": 2017}