{"title": "Clustering Markov Decision Processes For Continual Transfer", "tag": ["cs.AI", "cs.LG", "68T05", "I.2.6"], "abstract": "We present algorithms to effectively represent a set of Markov decision processes (MDPs), whose optimal policies have already been learned, by a smaller source subset for lifelong, policy-reuse-based transfer learning in reinforcement learning. This is necessary when the number of previous tasks is large and the cost of measuring similarity counteracts the benefit of transfer. The source subset forms an `$\\epsilon$-net' over the original set of MDPs, in the sense that for each previous MDP $M_p$, there is a source $M^s$ whose optimal policy has $<\\epsilon$ regret in $M_p$. Our contributions are as follows. We present EXP-3-Transfer, a principled policy-reuse algorithm that optimally reuses a given source policy set when learning for a new MDP. We present a framework to cluster the previous MDPs to extract a source subset. The framework consists of (i) a distance $d_V$ over MDPs to measure policy-based similarity between MDPs; (ii) a cost function $g(\\cdot)$ that uses $d_V$ to measure how good a particular clustering is for generating useful source tasks for EXP-3-Transfer and (iii) a provably convergent algorithm, MHAV, for finding the optimal clustering. We validate our algorithms through experiments in a surveillance domain.", "text": "hassan mahmud†‡ majd hawasly† benjamin rosman†+ subramanian ramamoorthy† †school informatics university edinburgh edinburgh ‡now blue yonder ltd. london mobile intelligent autonomous systems group council scientiﬁc industrial research pretoria south africa. school computer science applied mathematics university witwatersrand south africa present algorithms effectively represent markov decision processes whose optimal policies already learned smaller source subset lifelong policy-reusebased transfer learning reinforcement learning. necessary number previous tasks large cost measuring similarity counteracts beneﬁt transfer. source subset forms ‘\u0001-net’ original mdps sense previous source whose optimal policy regret contributions follows. present exp--transfer principled policy-reuse algorithm optimally reuses given source policy learning mdp. present framework cluster previous mdps extract source subset. framework consists distance mdps measure policy-based similarity mdps; cost function uses measure good particular clustering generating useful source tasks exp--transfer provably convergent algorithm mhav ﬁnding optimal clustering. validate algorithms experiments surveillance domain. keywords reinforcement learning transfer learning markov decision process abstractions policy reuse markov chain monte carlo discrete optimisation. reinforcement learning markov decision processes well known framework machine learning modelling artiﬁcial agents agent’s task sequential decision making. problems policy learning mdps well posed terms objective maximising expected reward often based speciﬁc instance underlying markov decision process model explicitly known learning agent. realistic agents must cope environments variability wherein process generates many instances mdps within agent must solve optimisation problem. finding optimal policies respect unrestricted family possible mdps intractable often leads highly conservative practicable solutions. however many realistic environments actually occupy middle ground. many reinforcement learning problems great interest practice main source complexity partial variability task description rather complete change domain. stated language mdps domains involve family possible reward transition functions shared state-action space. typically result non-stationary behavior component describes problem. class problems human-machine interaction setting remains same participants change. recent highly successful example problem website morphing problem goal present user website view/interface adapted needs skill level particular user. interface present user determined adaptively based sequence choice links. even though setting remains dynamics deﬁning problem changes user i.e. user corresponds task corresponding change problem dynamics. given change dynamics algorithm determine best policy/sequence actions note since algorithm knows user arrived homepage always knows dynamics changed changed. formalism task corresponds particular reward transition function state action space remain across tasks. additionally reinforcement learning agent always told dynamics/task changed. paper present approach dealing problems based notion transfer learning. view task instance similar previously experienced ones although explicit measurement similar task instance previously seen one. objective devise efﬁcient transfer learning method reinforcement learning mdps enables learning agents learn efﬁciently enough useful domains mentioned above. motivating example develop experiments consider surveillance agent monitoring large geographical region agent faces sequence monitoring problems problem corresponds pattern inﬁltrators appear different locations. tasks similar inﬁltration patterns surveillance policy good them. task goal agent learn regions inﬁltrators appear choose appropriate surveillance policy. expect patterns completely different every time time cannot completely rule pattern emerging. former case recognize repetition take advantage fact reusing surveillance policies. latter case also determine scenario novel learn appropriate policy scenario. furthermore number previous patterns becomes large also need compactly re-represent procedure determining correct sample efﬁcient. perform well stationary policy optimal hindsight respect transition reward function pair attained. note speciﬁc pair policy substantially worse optimal policy pair. approach paper represents orthogonal setting wherein consider case learner tries track optimal policy i.e. always perform well best policy current transition reward function proviso learner told dynamics changed. also require learner possible perform better ‘pure’ reinforcement learning algorithms applied speciﬁc transition reward functions transferring information transition reward functions optimal policy learned previously. formally paper consider tlrl case ‘lifelong’ learning agent learns sequence mdps deﬁned discrete state action space differ terms transition reward distributions. assume distribution generating sequence unknown unlearnable setting goal agent possible reuse optimal policies previous mdps order learn efﬁciently. continual setting assume agent operates ﬁxed number episodes hence measure efﬁciency total reward accumulated learning task ﬁxed number episodes. reusing policy means optimal policies previous mdps results efﬁcient behavior keep using however described above problem setting that number previous tasks become large transfer becomes ineffective agent spends much time testing policies. instance possible solution problem subset previous policies call source policies well-deﬁned useful sense representative previous policies words source policies form analogue \u0001-net metric space space previous mdps respect appropriate distance mdps. paper present clustering based approach ﬁnding smaller subset source policies. overall approach illustrated ﬁgure main idea cluster previous mdps clusters number clusters determined discrete optimization choose representative element cluster obtain source mdps. optimal policies source mdps become source policies. approach choosing clustering corresponding source policies attempt ensure a-priori chosen source policies good representative previous tasks purposes transferring unknown target task. particular deﬁne transfer learning algorithm exp--transfer performance bound depends number source policies. hence explicitly measures good size clustering left task choosing clustering corresponding source policies. deﬁne distance function mdps takes distance mdps well optimal policy performs hence given goal reuse optimal policies another choose clustering within cluster pairwise distances elements cluster low. similarly choose source policy cluster optimal policy cluster figure ﬁgure illustrates overall algorithmic approach. method consists parts. ﬁrst part search-clusterings algorithm good clustering mdps repository used generate source policies. second part policies used exp--transfer perform transfer using policy reuse mdp. reducing number policies input exp--transfer helps choose best policy quickly. hand reducing policies risks leaving policy good performance mdp. bulk paper dedicated deriving principled trade contradictory goals given cost function show np-hard optimal clustering. introduce markov chain monte carlo based discrete optimization algorithm algorithm extension metropolis-hastings algorithm call metropolis-hastings auxiliary variables also thought extension simulated annealing stochastic temperature changes. simulated annealing well known algorithm discrete optimization requires carefully setting inﬁnite sequence parameters known temperature schedule. determining schedule practice ensure convergence acknowledged difﬁcult practically form. version algorithm search temperature optimal point simultaneously thereby handling problem setting schedule automatically. summarize overall continual transfer algorithm follows. agent continually learns optimal policies mdps presented sequence. learning optimal policy particular learning agent uses optimal policies previously solved mdps policy reuse transfer learning algorithm. make transfer effective ﬁxed intervals agent clusters previous mdps derive small subset source mdps whose optimal policies used input policy reuse algorithm. clustering chosen optimize regret transfer algorithm found using convergent discrete optimization algorithm. conclude brief introduction method noting transfer algorithm exp-transfer fact extension well known exp- algorithm non-stochastic multi-armed bandits performance bound fact regret bound type well known bandit algorithm literature. strategy cast policy reuse transfer learning problem bandit problem ‘pure reinforcement learning algorithm’ source policies remaining arms. regret bound exp- ensures minimize negative transfer never performing much worse pure reinforcement learning. discuss related work. evidenced survey paper signiﬁcant amount work done transfer learning reinforcement learning. mentioned previously lifelong learning reinforcement learning ﬁrst explicitly considered context learning robots. works main learn dynamics robot motion circumstance using function approximator learnt dynamics initial bias situation using explanation based learning framework. terms recent work tlrl different strands related work. ﬁrst work policy reuse second task encoding. ﬁrst best knowledge authors considered policy reuse algorithms presented there beginning every episode choose different source policies using softmax criteria accumulated reward chosen policy initial exploration policy switching q-learning exclusively. contrast extend exp- algorithm multi-armed bandits choose source policies q-learning result inherit theoretical guarantees. additionally consider problem source task selection whereas work major focus. closely related works authors consider best choose stationary policies. main distinction policy reuse requirement former perform much worse base algorithm look previous work uses smaller source tasks represent complete previous tasks. problem source task selection clustering seems considered carroll seppi introduce several measures task similarity consider clustering tasks according measures. distance functions introduced heuristic clustering algorithm used simple greedy approach. evaluation method several problems. contrast derive cost function clustering principled optimize regret exp--transfer policy reuse algorithm. additionally instead constructing cluster greedy fashion search clustering space using convergent discrete optimization algorithm. recent work also chooses selectively previous tasks setting paper collection tasks deﬁned stateaction space tasks state-action-state triples different tasks generated i.i.d. rather sequentially typical setting authors able bound error samples task used learn task. quite different setting ‘batch’ rather typical online sequential measures similarity terms actual transition reward functions rather policies values. additionally analysis algorithms derived assumption ﬁxed prior tasks rather continual lifelong learning setting consider. source task selection possible represent previous tasks overall goal ﬁnding abstractions exploiting commonality received considerable attention transfer learning community. work done deriving abstractions purposes transfer homomorphisms works similarity mdps deﬁned terms bisimulation states different mdps. bisimulation concept borrowed process algebra. context transfer learning mdps general formulation bisimulation isomorphism state action spaces preserved transition distribution every state-actionstate triple t)|f transition distribution mdps. unfortunately pure form bisimulation absolute take account reward function. papers mentioned above basic notion extended various ways address issues. however main issues bisimulation computational cost remains extensions well. another issue approaches that observed castro precup bisimulation worst case metric result requires heuristic modiﬁcations. tions. case however interested distance terms policy. result even though tasks might quite different terms value function might identical terms optimal policy approach capture another interesting line work uses different approach abstracting mdps protovalue function based approach proto-value functions introduced efﬁcient represent value function large state spaces linear combination functions called proto-value functions. main innovation approach that representing value function real function state space state-space treated manifold distance points/states determined reachability graph mdp. idea spectral-decomposition value function naturally lends transfer learning given task imagine using proto-value functions learned previous task initialize value function task. noted protovalue function based transfer issues terms scalability tractability. main difference work that homomorphism based approach similarity notion based policy similarity based similarity value functions. identifying policy similarity desirable tasks similar terms value function similar terms policy necessarily round. somewhat different approach ﬁnding abstractions adopted mdps related mapping state action state triples lower dimensional space consider triples equivalent representations found close. authors able transfer tasks inverted pendulum cart pole mountain showing cases approach able discover fact differential equations describing domains similar/identical forms. finally work also related notion equivalence probabilistic models inﬂuential early work bayesian network learning. instance chickering collaborators wrote series papers notion event equivalence score equivalence used render problem searching network structures somewhat tractable. shown notion event equivalence i.e. bayesian networks treated similar agree independence dependence relationships random variables used deﬁne local structure edit operations enable learning network structures. subsequently idea developed show considering notion equivalence possible achieve optimal structure identiﬁcation essentially greedy procedure. take inspiration work also note task comparing sequential decision making problems differs making predictions probabilistic model calling notions process similarity corresponding algorithms transfer. following proceed follows. present notation fundamental notions section deﬁne transfer learning algorithm framework measuring distance section respectively. sections presents clustering algorithm full continual transfer algorithm. present experiments section conclusion section preliminaries deﬁnitions denote probability expectation. ﬁnite deﬁned tuple ﬁnite states ﬁnite actions rewards. state transition distribution reward function random variable taking values finally discount rate. policy policy function a)q)). value function policy written a)q∗ π∗)). agent acts step takes action state moves next state reward goal agent learn observations choose action state. multiple optimal policies designate ﬁrst policy lexicographical order canonical policy. assume rmax known upper bound reward function. without loss generality sequel assume single initial state call policy \u0001-optimal transfer learning setting. transfer learning setting given previous mdps transfer tasks learn learning runs episodes. denote optimal policy previous value policy similarly denote reward transition functions respectively. assume rewards mdps fall within range deﬁne rmax rmin. section ﬁrst concretely deﬁne problem policy-reuse transfer learning deﬁne algorithm solving problem. deﬁne goal policy reuse design algorithm runs episodes target task given collection source policies ρ··· q-learning algorithm/policy performs nearly well best policy collection episodes. requirement important implications. first since policies contain q-learning q-learning converges optimal policy means algorithm required perform nearly well policy converges optimal policy words algorithm avoid negative transfer. second source policy near-optimal algorithm also required perform nearly well near-optimal policy words algorithm transfer task possible. derive algorithm show policy reuse cast instance nonstochastic multi-armed bandits problem hence classic exp- algorithm extended solve policy reuse. call extension exp--transfer derive regret bounds algorithm particular show exp--transfer performs nearly well best policy collection described above expectation respect randomness algorithm reward transition function deﬁning policy reuse problem policy reuse transfer problem target task source policies ρ··· seems policy reuse method transfer ﬁrst introduced algorithm introduced called probabilistic policy reuse goal algorithm balance using source policies pure reinforcement learning policy learning algorithm converges faster running pure itself. basic idea follows. beginning episode chooses policy among source policies \u0001-greedy q-learning policy using softmax criterion observed returns policies previous episodes. initiates policy-reuse episode step probabilistically chooses \u0001-greedy q-learning chosen policy probability choosing \u0001-greedy q-learning going episode. essence source policies serve initial exploration policy take agent paths optimal policy would result faster learning optimal policy. several aspects algorithm noteworthy. first even source policies contain optimal policy algorithm would deterministically switch q-learning initial phase. another aspect that intuitive connection soft-max criterion beneﬁt using policy actual connection made rigorous. issues arise fact goal policy reuse deﬁned concretely taking deﬁnition online learning algorithms deﬁne policy reuse problem concretely designing algorithm chooses policies every episode perform much worse policies q-learning episodes. discussed above requirement implies algorithm avoids negative transfer transfers from/reuses good policy formally deﬁnition transfer learning target task γnrn discounted return accumulated running episode reward step ¯xi. total discounted reward accumulated policy-reuse algorithm require section introduce non-stochastic multi-armed bandits problem show policy reuse problem cast instance problem. present exp--transfer solve problem extension/modiﬁcation classic exp- algorithm non-stochastic multi-armed bandits. discuss difference exp- exp--transfer section randomized algorithm called exp- developed minimizes expected regret expectation taken respect randomization algorithm. turns regret exp- satisﬁes requirements regret deﬁnition implies non-stochastic bandits approach extended setting solves problem policy reuse. given transformation algorithm exp--transfer choosing policy step given pseudocode form algorithm basic idea straightforward step chooses policy probability proportional adjusted cumulative observed reward policy term encourage exploring policy selected recently. rewards adjusted compensate fact that step algorithm observes payoff policy chosen step addition algorithm determines high probability particular source policy worse source policy eliminates consideration. idea remove know sub-optimal save episodes would wasted trying policy. detail main loop exp--transfer runs line line contains policies eliminated exp--transfer. line computes probability choosing policy proportional adjusted observed payoff policy plus exploration term next steps policy chosen probabilistically executed payoff observed normalized line record payoff executed policy elimination step. lines adjusted payoffs policies computed weights updated respectively. finally steps looks stationary policy checks average non-adjusted payoff satisﬁes elimination condition. policy removed elimination condition justiﬁed/obtained theorems derive regret bound algorithm. begin analysis exp--transfer ﬁrst note payoffs/discounted cumulative reward source policy i.i.d. payoff q-learning unknown non¯ stationary distribution strategy analyzing performance despite non-stationarity assume unknown adversary generating payoffs q-learning bound expected worst-case regret exp--transfer respect adversary particular adversarial/worst-case analysis assume three participants nature adversary player make following choices order. algorithm exp--transfer input policies source policies ρ··· exp- parameters interval eliminate policies; upper bound range step rewards; conﬁdence parameter eliminating source policies. select policy step probability increment execute policy one-episode observe discounted payoff ¯xit; normalize rmin−]/−]. q-learning policy xit. second step choices made exp--transfer. such regret bound holds expectation respect sources denote ierp ieet respectively. former expectation respect transition reward function latter expectation respect exp--transfer. crucial note taking expectations assume choices made adversary step taken ﬁxed main result section follows right side bound ﬁrst issue need check whether bound satisﬁes requirements deﬁnition deﬁnition requires iern +iea total discounted reward episodes expectations respect randomization reward transition function randomization algorithm consider edge case arms ever eliminated required. recall discussion deﬁnition satisfying regret requirements imply algorithm avoids negative transfer transfers possible. hence theorem shows exp--transfer indeed avoid negative transfer transfers possible. move away trade adherence requirement deﬁnition practical performance. indeed conducting experiments reported section observed setting suitably non-zero value greatly improves performance. however theoretical guarantees completely lost instance source policies eliminated part theorem says least source policy high probability strictly better policy. section brief discussion difference exp- exp-transfer. primary difference eliminate policies/arms improve practical performance algorithm. possible exp- because unlike case assumed arms i.i.d. payoffs means cannot estimated observations. leads slightly different analysis improved constant exp--transfer exp-. practical difference observe actually considerable without able outperform experiments recall section problem assume repository mdps optimal policies wish optimal policies source policies exp--transfer. following dilemma. number source policies increase spend time evaluating accruing sub-optimal reward process. hand choose subset policies repository risk leaving policies useful task. essentially faced problem trading size diversity source policies. section concretely deﬁne tradeoff problem optimizing cost function section describe algorithm optimize cost function deﬁned clusterings/partitions repository clustering used choose particular subset policies repository source policy set. ﬁrst show choose source given clustering deﬁne cost function measures well source achieves tradeoff mentioned above. cost function helps choose optimal clustering hence optimal source policies. constructing source policies given clustering given mdps repository a··· particular clustering ∪iai clusterings vary elements number cells given particular clustering obtain source policies choosing policy choose policy deﬁne distance function measures similar mdps terms optimal policies. mdps repository recall denotes value policy executed respectively initial state letting optimal policies mdps deﬁne optimal policy similarity mdps follows. deﬁnition motivated fact goal policy reuse optimal policy another. deﬁne source policies given clustering deﬁnition given clustering a··· deﬁne follows source policies corresponding ρ··· corresponds optimal policy deﬁnition illustrated figure means following. element minimizes maximum distance elements cluster hence lemma worst case sense best representative mdps cluster choosing optimal policy source policy ensure best worst-case representation mdps cluster make ﬁnal statement exact next section particular lemma bound regret exp--transfer respect optimal policy repository cost function clustering a··· clustering source policies ρ··· chosen deﬁned previous subsection. goal section quantify regret exp-transfer respect optimal policy repository mm··· consider case exp--transfer source policies ρ··· hence regret quantify good clustering transfer particular lower regret preferable clustering. result regret serve cost function choosing clustering derive source policies. proceeding note theorem computed regret exp--transfer section extend results derive regret respect figure ﬁgure sketches basic approach deriving source policies. black circles represent repository mdps. goal clusters derive source policies source tasks. ﬁgure illustrates idea cluster analogue \u0001-ball metric space according source mdps form analogue \u0001-net previous mdps respect function measures well policy performs hence source policies \u0001-net implies that given repository least source policy performance ‘\u0001-close’ performance optimal policy previous mdp. average diameter clusters weighted size clusters. therefore gives average distance cluster center element cluster. such measures much diversity repository captured chosen clusters. smaller smaller average distance cluster centers mdps hence diversity policies repository captured using these give average case quantiﬁcationof performance clustering used generate source policies used exp--transfer. transition reward functions deﬁne maxsa maxsa reward transition functions additionally worst case quantiﬁcation also possible. however assumptions underlying worst case seems weak identiﬁes mdps intuitively cost function correspondingly sufﬁciently discriminating i.e. dissimilar similar. particular experiments found cost function give intuitive clusters. discuss function appendix lemma derive cost function. first limiting case none policies eliminated case bound applies mdps repository. lemma assume repository equally likely minimum hence taking average mdps upper bound average regret respect optimal policies section introduce problem ﬁnding clustering minimizes cost. argue optimizing cost function hard sets stage developing discrete optimization algorithm next section. speciﬁcally show hard optimize upper bound costm cost cost deﬁned deﬁnition deﬁne average max-diameter clustering reduce minimum clique-cover problem problem ﬁnding clustering minimizes costm hence establish np-complete. start describing clique cover problem. graph vertices edges. subset clique edge minimum clique cover problem ﬁnding partition v··· clique minimum exists partition clique following theorem costm. theorem given graph time polynomial reduce minimum clique cover problem ﬁnding clustering mdps deﬁned state action spaces mina∈c costm. unfortunately proof minimizing cost hard fact minimizing upper bound costm hard leads conjecture minimizing cost hard well. reason optimize cost function need discrete optimization function develop next section. section contrasting approach previous approaches clustering mdps tlrl prior work mdps typically characterized ﬁnite number real valued parameters distance parameters determine similar mdps are. mdps clustered instance putting non-parametric dirichlet process prior parameters mdps using monte carlo inference methods clustering maximizes posterior probability. since notion similarity mdps based policies apply approach case need relatively compact parametric representation optimal policies metric relates policy-parameters values policies optimal policies uniquely characterize mdps. seems difﬁcult interesting policies seems reasonable ‘linear’ domains i.e. domains small change policy results corresponding small change value. runs completely counter main motivations using policy based clustering different mdps might identical near-identical optimal policies. given these motivated construct clustering algorithm adapted policy based clustering mdps. also compare method method wilson section simpliﬁed version domain approach recovers better clusterings. clustering. section argued optimizing cost hard. implies need discrete global optimization algorithm ﬁnding optimal clustering. section introduce problem discrete optimization approach solving sections derive analyze general optimization algorithm. finally section apply algorithm problem ﬁnding optimal clustering. goal solve discrete global optimization problem computing mina∈c cost possible clusterings mis. algorithms discrete optimization problems tend fall speciﬁc classes appropriate problem hand. present stochastic search algorithm problem figure ﬁgure illustrates stochastic search used solve function optimization thick black curve objective function optimized/minimized goal point curve attains minimum value stochastic search algorithm starts particular point time step jumps candidate point chosen according stochastic strategy. arrows shown possible stochastic search algorithm different runs likely going different sets points. candidate move towards away minimum point. kind optimization necessary objective function nice properties standard algorithms applicable. basic strategy construct distribution concentrates around optimum around clusterings cost. concentration property implies repeatedly sample distribution optimum good/low cost clustering high probability. however general exact sampling distributions difﬁcult algorithm samples approximately distribution using markov chain monte carlo approach comprehensive introduction mcmc metropolis hastings markov chains use. mcmc turns algorithm stochastic search algorithm. resulting algorithm understood simulated annealing stochastic temperature changes. figure describes simulated annealing qualitatively also contrasts algorithm mhav. simulated annealing stochastic search algorithm optimizing objective function search rule step jumps point figure ﬁgure compares simulated annealing mhav algorithm ﬁnding minimum objective function green arrows show search steps taken algorithms. simulated annealing decreases temperature parameter time effectively results function surface becoming effectively less less ﬂat. ﬁgure illustrates search failed incorrect temperature decrease schedule. unlike simulated annealing mhav jumps temperature values stochastically. green arrows search steps ﬁxed temperature arrows jumps temperatures. beginning section full description. probability max)− candidate distribution proposed next point given current point remaining term depends improvement temperature improvement term jumps lower f-valued points succeed probability jumps higher f-valued points succeed probability depending much higher point temperature chief feature simulated annealing user deﬁned determines practical success algorithm. sequence decreasing essence changes easy explore objective function surface higher temperature allowing search travel longer distances effect making objective function ﬂatter lower temperature restricting search points local current point effect making objective function steeper temperature sequence needs carefully higher temperature phase search travels longer distances places minimum point lower temperature phases search moves ‘local neighborhood’ minimum point ﬁnds point point close clearly fairly difﬁcult problem requires understanding objective function. figure illustrates algorithm mhav. algorithm similar simulated annealing search rule jumps better points succeed probability jumps worse points succeed probability depending much worse point also uses temperatures modify ﬂatness function however unlike simulated annealing mhav ﬁxed temperature schedule moves different temperatures stochastically essence mhav searches augmented space search rule form y)]] exp{max) λ]}. mhav searches temperature solution space simultaneously avoids difﬁcult problem needing temperature schedule. convergence still guaranteed convergence metropolis-hastings algorithm proof convergence speed convergence algorithm simple compared simulated annealing contrast). present algorithm steps. ﬁrst cast discrete optimization problem problem sampling speciﬁc distribution derived objective function present general metropolis-hastings scheme approximate sampling distribution present analyze adaptation metropolis-hastings discrete optimization call mhav mhav algorithm general optimization algorithm adapt problem searching optimal clustering section show convert problem global optimization problem sampling distribution. method discuss inspired basic idea behind simulated annealing assume goal minimize cost function deﬁned ﬁnite particular assume subset acceptable cost λ··· satisﬁes distribution typically intractable sample from. indeed clustering problem function cost clusterings hardness result cost appendix conjecture sampling fact intractable. hence need approximate sampling methods. approximate sampling turns sampling stochastic search objective function global minima. detailed previous subsection powerful augment search methods modify objective function introducing temperatures change ﬂatness function parameter serves precisely purpose. particular high modiﬁed objective function ‘steep’ ‘ﬂat’. given motivation section present metropolis-hastings algorithm used approximately sample arbitrary distributions section adapt algorithm sample subsection describe standard method approximately sample distribution large ﬁnite space next section method sample complete global optimization method. upper-case roman letters random variables lowercase letters refer realized values. following theory markov chains found markov chain state-space stochastic process taking values distribution called transition kernel chain represented |x|×|x| matrix also denoted entry markov chain said time-homogeneous pn|x) i.e. independent time consider time-homogeneous chains. distribution said stationary chain kernel satisﬁes result important used approximately sample distribution hard sample directly. idea construct markov chain stationary distribution theorem implies simulate long enough eventually start sampling metropolis-hastings chain gives standard deﬁne chain given input in-depth introduction). chain deﬁned irreducible kernel φ|x) acceptance probability easily checked satisﬁes detailed balance equation π)pm turn equivalent chain simulated long enough sample target distribution derive version chain sample approximately optimization using metropolis hastings auxiliary variables section show algorithm sample distribution deﬁned section hence perform optimization. call algorithm auxiliary variables introduced auxiliary variable enable perform global optimization. discussed above mhav thought simulated annealing without temperature schedule. adapt global optimization problem target brieﬂy note plug normalization term cancels algorithms never need compute irreducible kernel ¯pmh optimize convergence rate derived above minimizing setting parameters given proposal distribution required irreducible kernel target distribution start following result simpliﬁes deriving result lemma independent proof follows directly proof lemma need maximize however also depends unknown difﬁcult specify optimal values a-priori. however give heuristic arguments setting parameters terms increasing ‘ﬂow’ search process search space first used choose whether increase decrease value. ensure neutral value favor either direction ensure maximum search space. note step chain ¯pmh moves either space space. determine respectively often move often make search effective seems need make sure initially need explore quite settle explored sufﬁciently increasing value. hence recommendation signiﬁcantly smaller ideally ratio reﬂect difﬁcult expect close best even though parameter settings heuristic stress affects convergence speed ultimate convergence. contrast simulated annealing convergence guaranteed parameters carefully. searching optimal cluster solved using mhav algorithm. algorithm searching space clusterings given algorithm case objective function cost. complete speciﬁcation mhav problem probability randomized procedure converts deﬁne distribution randomized procedure follows. given {a··· choose uniformly random uniformly random place-holder/empty representing cluster. choose points uniformly random clustering resulting transfer note cluster additionally |ai| less cluster otherwise number ensures small higher probability less likely different moving large number points parameter user dependent experiments irreducible hence satisﬁes condition brief section combine algorithms presented full continual transfer algorithm listed algorithm algorithm runs phases phase solves using exp--transfer algorithm current source policies input. line function sourcep generates source policies ρ··· clustering optimal policy chosen according current phase satisﬁes runs search-clustering algorithm source tasks tasks solved far. performed three sets experiments illustrate various aspects efﬁcacy algorithm. baseline algorithm comparison ﬁrst domain multi-task hierarchical bayesian reinforcement learning algorithm proposed wilson hence compares approach alternative approach clustering tasks. latter domains compare probabilistic policy reuse introduced fernandez which mentioned section main prior work policy reuse algorithms. larger second third experiments report results proposed exp--transfer algorithm standard q-learning. additionally experiment using different forms clustering including clustering search-clustering algorithm clusters chosen manually hand greedy clustering procedure. greedy clustering algorithm choose threshold distance construct clusters selecting arbitrarily seed cluster ﬁnally adding mdps distance less threshold cluster. graphs present results best/lowest cost clustering found using various threshold values. table experiment setup matrix. table shows combinations algorithms clustering methods used experiments. ‘full’ refers search-clustering ‘sans’ means without kind clustering ‘hand-picked’ means using source policies selected believing optimal ‘greedy’ refers heuristic threshold-based method graph presented experiments results averaged different target tasks trials task. various parameters used clustering transfer algorithms given table present results experiments three different domains. section coloured grid world domain commonly used bayesian reinforcement learning compare clusters found proposed algorithm hierarchical bayesian method order motivate clustering approach. section present results simple windy corridor domain demonstrate clusters produced search-clustering algorithm. results show clusters found intuitive nature. section present results complex surveillance domain variant kinds problems considered instance experiment show performance algorithm combinations given table various numbers previous tasks task lengths. table values algorithm parameters. table gives values different parameters used various algorithms. refers common parameters reinforcement learning algorithms. parameter chosen illustrate effect parameter algorithm performance. parameter chosen allow high degree conﬁdence performance exp--transfer. parameters chosen according section chosen heuristically. value selected found restarting gave better results. chosen appropriate domains. wilson introduced method modelling unknown distribution tasks multitask reinforcement learning problem using hierarchical inﬁnite mixture model. two-layer model capable representing previously unknown number classes mdps well ﬁnding latent distribution parameters class. task model acts prior parameter space enables generation faster initial solution subsequently optimised additional task-speciﬁc learning. compare inﬁnite mixture model approach ours consider simpliﬁed version coloured maze domain wilson cell grid world coloured possible colours. colour assigned weight values differ tasks. agent navigates grid world corner diagonally opposite corner. done using four actions moves agent adjacent cell corresponding cardinal direction deterministically. every movement agent receives reward equal colour weights current cell adjacent four cells. goal agent maximise received rewards. task completely described parameters unit square. intended simplify visualisation resulting clusterings. sample tasks uniformly random unit square cluster using hierarchical inﬁnite mixture model framework. figure shows clustering results wilson figure shows clusters obtained method. note tasks line goes origin equivalent colour weight ratio thus optimal policy scaled values. result method able model fact summarise complete mdps reduced landmark mdps equivalent policies. hand modelling similarities tasks based task parameters fail realise equivalence subsequently result many clusters similar policies share local neighbourhood parameter space. figure tasks parameter space coloured maze domain. corresponds randomly sampled task. clusters obtained using hierarchical inﬁnite mixture model. asterisks means recovered clusters circles show three standard deviations. clusters obtained using clustering algorithm. colour represents different class. note method captures similarity policy much closely methods cluster directly parameter space. windy corridor domain illustrated figure domain consists parallel corridors ‘wind’ blowing south north along columns cells near entrance corridors. agent action possible cardinal direction moves direction deterministically. windy cell motion agent becomes probabilistic probability moving north moving desired direction depends probability wind cell task parameter ranging mdps domain distinguished values location goal state probability wind. possible wind probabilities together possible goal locations results total possible mdps. domain learned optimal value function mdps computed distance every pair mdps. used cluster mdps using search-clusterings algorithm. figure presents ﬁnal clusters found domain. ﬁgure shows best clustering found algorithm placed tasks goal state cluster. follows intuition because despite wind probability policies required mdps identical goal states identical different tasks different goal states. demonstrates search-clusterings algorithm capable recovering expected clusters thereby providing sanity check algorithm although note results stochastic vary runs. order illustrate effects clustering incrementally built full mdps presenting random order search-clustering algorithm cluster addition mdp. results shown figure note allocation mdps clusters remains largely consistent across presentation mdps. case algorithm recovers main clusters. surveillance domain illustrated figure domain goal agent catch inﬁltrators wish break target region. different vulnerable locations domain inﬁltrators choose subset v-locations inﬁltrate call target v-locations. type inﬁltrators deﬁned sequence visit target v-locations goal agent target locations surveil right sequence inﬁltrators. actions available agent motion actions cardinal directions well surveillance action deterministic outcome. every action taken results reward unsuccessful surveillance action results reward successful surveillance action results reward instead surveilling correct v-location agent surveils location adjacent receives reward v-location v-locations adjacent hence mdps corresponding different v-location sequences similar terms optimal policy/dv distance belong cluster pair v-locations adjacent case optimal policy yield near-optimal sequence rewards applied vice versa. present following results experiments combination different numbers previous mdps numbers target locations. results show complex transfer task better exp--transfer clustering performs compared probabilistic policy reuse complexity measured terms number previous mdps difﬁculty target task. compare performance exp--transfer probabilistic policy reuse q-learning complexity transfer problem increases. here complexity transfer problem number previous tasks complexity results referred clustering gains. figure incremental clustering windy corridor domain. incremental clustering left obtained running search-clustering increasing number mdps. mdps domain presented random order algorithm axis showing time ﬁrst presentation algorithm. points colour assigned cluster. right three zoomed-in time slices incremental clustering time slice shows intuitive interpretation mdps representation figure ﬁrst study effects clustering comparing performance exp--transfer probabilistic policy reuse without clustering. results presented section summarise complete experiment results provided appendix figure demonstrates performance algorithms different task variants surveillance domain either sequence three v-locations. ﬁgure measures clustering gain difference performance without clustering. ﬁgure figure surveillance domain. caricature surveillance domain shows surveillance locations marked blue. start location marked full domain used experiments gridworld v-locations. domain requires agent surveil different locations particular sequence receive positive reward location surveilled. surveilling wrong location results negative reward action taken gives reward target v-locations clustered spatially groups surveilling location cluster instead results reward episode show comparison q-learning. full results given appendix show exp- consistently outperforms q-learning large margin indicating algorithm escapes negative transfer. seen exp--transfer always beneﬁts using clustering. furthermore complex task better performance. observed general upwards trend curves increasing number previous mdps fact curve vlocations lies curve v-locations. result complete agreement expectations bandit-like algorithm lowering number arms result lower regret. addition result indicates clustering algorithm retains correct arms removal arms performance exp--transfer affected adversely. appears clustering help algorithm clustering additionally becomes detrimental task complexity increases. conjecture regarding reason probabilistic policy reuse uses source policies potential optimal policies rather exploration devices. clustering mdps remove arms hence reduce number exploration policies lowers scope exploration. turn results negative performance gain probabilistic policy reuse. figure trajectory examples. example four different types trajectories surveillance domain showing three groups four v-locations. task requires visiting vlocations identiﬁed numbers sequence. green line represents moving correct v-location sequence yielding reward black line represents moving incorrect v-location group correct reward line depicts movement incorrect group reward surveillance action taken. also interesting note figure shows gain terms ﬁnal reward obtained initial gain exp--transfer negative gain higher. however transfer complexity increases cumulative reward gain becomes positive exp--transfer continues decrease. given ﬁgures show beneﬁt clustering compare cumulative reward obtained exp--transfer clustering without clustering complex -target-v-locations problem figure result shows exp--transfer completely dominates difference becoming particularly stark number previous tasks increases compare performance exp--transfer using different types clustering methods reported table previous section examine change performance increasing complexity transfer tasks. summary results given figure again full results provided appendix seen exp--transfer using search-clustering obtain source policies outperforms case cluster previous tasks. conﬁrms result reported previous section. however addition also observe transfer using greedy clustering scheme performs well using search-clustering previous mdps figure clustering gains. ﬁgures show clustering gain exp--transfer probabilistic policy reuse. data-point curve y-value difference performance without clustering previous mdps. performance measured cumulative clustering gains total cumulative discounted reward episodes. performance measure ﬁnal clustering gains discounted reward ﬁnal episode. note point averaged different target tasks trials task. previous mdps search-clusterings signiﬁcantly better. search-clustering greedy clustering comparable numbers previous mdps largely structure domain every element group tasks similar every task group. agent surveils v-location group true v-location required task receives near-optimal rewards however greedy clustering would fail complex variant surveillance domain. understand issue recall beginning section mdps corresponding v-location sequences similar terms optimal policy hence belong cluster pair v-locations adjacent. case surveiling instead vice versa. hence within correct cluster mdps symmetry distance pair mdps similar. time distance mdps cluster quite different. greedy clustering start given cluster mdps figure cumulative reward summary. ﬁgure shows ﬁnal cumulative rewards episodes exp--transfer clustering probabilistic policy reuse without clustering surveillance domain -target-v-locations. x-axis shows number previous mdps. simple follows. complex domain correct cluster single distance elements cluster every cluster high distance non-centroid elements. effect v-locations group adjacent locations. change reward function group adjacent locations reward asymmetric designate single v-location yields reward surveilled instead vice-versa v-locations surveilling instead yields reward hence would v-location sequence case greedy clustering fail learn every cluster start centroid non-centroid mdps distance mdps would large. result complex domain given table that transfer using clusters produced search-clustering outperforms transfer using clusters discovered greedy clustering approach factor increase number previous mdps. recall point obtained averaging different target tasks. finally examine effect parameter performance original surveillance domain. recall parameter affects clustering algorithm search-clustering exp-transfer time duration terms number episodes transfer figure effect clustering methods. ﬁgure compares performance exp-transfer clustering greedy clustering procedure search-clustering. performance measured terms total cumulative discounted rewards episodes. table table gives performance exp--transfer complex surveillance domain using greedy clustering search-clusterings tasks target v-locations. performance measured terms total cumulative discounted rewards episodes averaging different target tasks. table shows performance search-clusterings remains steady performance greedy clustering falters previous tasks draw from. procedure run. performed experiments different combinations numbers previous mdps complexity. results qualitatively similar present graphs figure complex least complex transfer problem considered. relegate remaining graphs rest experiments appendix results show performance curve exp--transfer parameter lies curves exp--transfer parameter illustrates effect optimising transfer performance time duration. figure complex graph domain. complex graph domain consists groups adjacent v-locations. experiments used groups simplicity show groups here. surveilling central v-location instead others reward vice-versa survelling instead reward before consists surveilling correct sequence target v-locations depicted followed figure green line shows correct movement black line shows movement wrong v-location right group line corresponds moving wrong group surveilling paper developed framework concisely represent large number previous mdps smaller number source mdps transfer learning. presented principled online transfer learning algorithm principled evaluate source sets algorithm source set. idea cluster previous mdps representative element cluster source tasks. also presented extensive experiments show efﬁcacy method. discuss several interesting directions future work. paper considered discrete domains. however possible translate overall approach continuous setting. particular apply approach continuous space problem need pure algorithm evaluate policies deﬁnitions algorithms results hold true setting. algorithms exp--transfer mhav search-clustering distance function treats underlying mdps policies black boxes certain properties. discreteness never exploited required either algorithms analysis. finally pointing idea clustering tasks obtain representative much general. instance cost function derived different assumptions applied clustering approach. another example clustering approach also used multi-agent systems group together opponents according whether policy figure effect parameter. ﬁgures show learning curve exp-transfer different numbers time steps affects clustering arms chosen exp--transfer. parameters experiment given title respective ﬁgure. shown shorter exp--transfer lowest optimal. intermediate duration optimal remaining time optimal. figures respectively give curves lowest highest complexity task run. shaded areas indicate standard deviation. equally effective opponents group. also interesting implement methods algorithms scaled real version types problems considered paper. plan pursue extensions future work. proof direct application corollary possible algorithm diverges exp- number arms possibly decreases across time steps. proof ﬁrst part structurally similar proof theorem auer different crucial detail removal arms/policies. second part deal arms removed novel. above follows ˜wt+ |ct| |ct+| weights positive. follows update equation line exp--transfer. follows deﬁnition follows deﬁnition line exp--transfer. holds term exponential positive. finally follows identical reasoning used derive proof theorem proceed along similar lines theorem using fact sequel assume denominator exponent left hand side equation bound following simple well known consequence. assume i.i.d. samples drawn random variables assume satisfy then then triangle inequality union bound probability least ie). line exp--transfer remove source policy zj/nj zk/nk probability since arms implies removed union bound probability least every eventually removed. state proof. proof previous centroid cluster deﬁnition optimal policy used exp--transfer putting lemma theorem three inequalities together ie/t eliminated exp--transfer part theorem exists policy probability least relationship proof first given ordering elements identify vertex position ordering take mm··· mdps deﬁned state space action space transition function mdps case trivial reward function deﬁned follows. otherwise function used deﬁnition deﬁne cost function clusters. following identify vertex show optimal clustering corresponds maximal clique mapping recall costm ¯\u0001m. optimal clustering denote cluster belongs show mimi··· i··· form clique words show that equivalently contradiction assume since edge them diameter least turn implies costm consider clustering obtained putting cluster. clustering cost contradicting optimality hence clusters cost corresponds collection cliques partition denote collection cliques note collection cliques v··· partition correspond clustering mimj case |a|. assume collection cliques |j∗| corresponding clustering show costm costm resulting contradiction. note mimj hence diameter cost since deﬁnition |ai| |a∗|. contradiction indeed minimum clique cover showing problem minimum clique cover reduced problem ﬁnding optimal clustering. complete proof need show reduction takes polynomial time. cost computing setting reward function takes time constant proof show irreducibility show this ﬁrst note assumed exists consider particular path irreducible. exists yn−y probability deﬁnition probability ¯pmh transition inequality follows ¯acc· assumption lemma statement. hence total probability path yn−y ¯pmh lower bounded bnβnφy summing possible paths length going gives probability lower bounded bnβnφn assume bound probability ¯pmh going hence reach probability symmetric argument reach probability least positive ﬁniteness λis. putting together probability transitioning lower bounded shows ¯pmh irreducible. show ¯pmh aperiodic sufﬁcient note then probability ¯pmh returns state step ensures g.c.d. time steps ¯pmh returns state proof irreducible lemma construction chain stationary distribution. hence ﬁrst part theorem ¯pmh converges total variation. second part theorem ¯π||tv proof mentioned above proof follows closely proof theorem begin with ﬁrst note irreducibility ¯pmh diameter ﬁnite. hence deﬁnition pmh|x) ¯π). ¯mmh denote transition matrix kernel ¯pmh ¯mmh) ¯pmh|x) i.e. contains distribution ¯pmh. denote transition matrix then setting write another transition matrix. valid transition matrix note given ¯π]. summing elements ¯pmh|xi) whence sums furthermore deﬁnition entry also positive showing indeed valid transition matrix. ﬁrst equality deﬁnition k-step transitions. second equality obtained applying inductive hypothesis ¯mmh third fourth equality follows applying inductive hypothesis facts established above. ﬁnal equality obtained cancelling terms. ¯mmh converges words ¯π|| since ¯π||tv proof path assume path positive probability ¯pmh certain value respectively then deﬁnition ¯pmh probability path form cakkbkckk integers constant. then difference values probability path form cak)kbkck− c)k. since probability must also non-zero. hence paths positive probability invariant respect values since length shortest path positive probability proves lemma. proof need show that clusterings ﬁnite number re-arrangement steps sufﬁcient obtain clusters spread across ai··· order n··· points respectively. then non-zero probability created points non-zero probability points hence non-zero probability hence non-zero probability constructing appendix continues section derived case cost function clustering considering average case scenario. derive cost function worst case setting. begin deﬁne following worst case measure homogenity clustering. discuss quantiﬁcations worst average case approprireason strongly believe next chosen nature adate. versarially respect choice cluster nature chooses maximize maxaj∈a maxmi∈aj clearly worst case quantiﬁcation correct evaluate clustering. hand reason believe this average case might appropriate. instance consider clustering mdps contains mdps clusters clusters diameter sparse wide many domains would consider good clustering situation average case quantiﬁcation would maxi maxi whereas worst case quantiﬁcation would maxi intuitively seems little pessimistic indeed also observed similar results experiments sense worst case quantiﬁcation failed uncover clusters would intuitively consider good. hence rest paper average case quantiﬁcation deﬁne distance function. section give detailed cumulative reward curves algorithms clustering policy-reuse clustering policy-reuse clustering. results given figures results less show summary graphs showed. particular number previous tasks complexity task policy-reuse better algorithm. however complexity keeps increasing algorithm begins dominate versions policy-reuse showing clustering beneﬁcial. figure algorithm comparisons. ﬁgures compares performance exp-transfer clustering policy-reuse without clustering q-learning various settings task detailed plots summary results presented section figure algorithm comparisons continued. ﬁgures compares performance exp--transfer clustering policy-reuse without clustering q-learning various settings task detailed plots summary results presented section figure algorithm comparisons continued. ﬁgures compares performance exp--transfer clustering policy-reuse without clustering q-learning various settings task detailed plots summary results presented section figure time comparisions extended results. ﬁgures show time comparsion results transfer tasks addition figure title graphs show experiment setup. shaded areas give standard deviation learning curves. figure time comparisions extended continued. ﬁgures show time comparsion results transfer tasks addition figure title graphs show experiment setup. shaded areas give standard deviation learning curves. figure time comparisions extended continued. ﬁgures show time comparsion results transfer tasks addition figure title graphs show experiment setup. shaded areas give standard deviation learning curves.", "year": 2013}