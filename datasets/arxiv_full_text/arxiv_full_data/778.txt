{"title": "Deep Reinforcement Learning in Parameterized Action Space", "tag": ["cs.AI", "cs.LG", "cs.MA", "cs.NE"], "abstract": "Recent work has shown that deep neural networks are capable of approximating both value functions and policies in reinforcement learning domains featuring continuous state and action spaces. However, to the best of our knowledge no previous work has succeeded at using deep neural networks in structured (parameterized) continuous action spaces. To fill this gap, this paper focuses on learning within the domain of simulated RoboCup soccer, which features a small set of discrete action types, each of which is parameterized with continuous variables. The best learned agent can score goals more reliably than the 2012 RoboCup champion agent. As such, this paper represents a successful extension of deep reinforcement learning to the class of parameterized action space MDPs.", "text": "recent work shown deep neural networks capable approximating value functions policies reinforcement learning domains featuring continuous state action spaces. however best knowledge previous work succeeded using deep neural networks structured continuous action spaces. paper focuses learning within domain simulated robocup soccer features small discrete action types parameterized continuous variables. best learned agents score goals reliably robocup champion agent. such paper represents successful extension deep reinforcement learning class parameterized action space mdps. paper extends deep deterministic policy gradients algorithm parameterized action space. document modiﬁcation published version ddpg algorithm namely bounding action space gradients. found modiﬁcation necessary stable learning domain likely valuable future practitioners attempting learn continuous bounded action spaces. demonstrate reliable learning scratch robocup soccer policies capable goal scoring. policies operate low-level continuous state space parameterized-continuous action space. using single reward function agents learn locate approach ball dribble goal score empty goal. best learned agent proves reliable scoring goals though slower hand-coded robocup champion. robocup half-field-offense research platform exploring single agent learning multi-agent learning adhoc teamwork. features low-level continuous state space parameterized-continuous action space. speciﬁcally parameterized action space requires agent ﬁrst select type action wishes perform discrete list high level actions specify continuous parameters accompany action. parameterization introduces structure found purely continuous action space. rest paper organized follows domain presented section section presents background deep continuous reinforcement learning including detailed actor critic updates. section presents method bounding action space gradients. section covers experiments results. finally related work presented section followed conclusions. robocup international robot soccer competition promotes research robotics. within robocup simulation league works abstraction soccer wherein players ball ﬁeld -dimensional objects. however researcher looking quickly prototype evaluate different algorithms full soccer task presents cumbersome prospect full games lengthy high variance outcome demand specialized handling rules free kicks offsides. half field offense domain abstracts away difﬁculties full robocup exposes experimenter core decision-making logic focus challenging part robocup game scoring defending goals. agent receives state sensations must independently select actions. naturally characterized episodic multi-agent pomdp sequential partial observations actions part agents well-deﬁned episodes culminate either goal scored ball leaving play area. begin episode agent ball positioned randomly offensive half ﬁeld. episode ends goal scored ball leaves ﬁeld timesteps pass. example videos half field offense games viewed https//vid.me/snev https//vid.me/jqtw https//vid.me/bd. following subsections introduce low-level state action space used agents domain. agent uses low-level egocentric viewpoint encoded using continuously-valued features. features derived helios-agentd’s world model provide angles distances various on-ﬁeld objects importance ball goal players. figure depicts perceptions agent. relevant features include agent’s position velocity orientation stamina; indicator agent able kick; angles distances following objects ball goal field-corners penalty-box-corners teammates opponents. full list state features found https//github.com/mhauskn/ hfo/blob/master/doc/manual.pdf. figure left state representation uses low-level egocentric viewpoint providing features distances angles objects interest like ball goal posts corners ﬁeld opponents. right helios handcoded policy scores goalie. champion agent forms natural baseline comparison. half field offense features low-level parameterized action space. four mutuallyexclusive discrete actions dash turn tackle kick. timestep agent must select four execute. action continuously-valued parameters must also speciﬁed. agent must select discrete action wishes execute well continuously valued parameters required action. full parameterized actions dash moves indicated direction scalar power movement faster forward sideways backwards. turn turns indicated direction. tackle contests ball moving indicated direction. action useful playing opponent. kick kicks ball indicated direction scalar power directions parameterized range degrees. true rewards domain come winning full games. however reward signal sparse learning agents gain traction. instead introduce hand-crafted reward signal four components move ball reward provides scalar reward proportional change distance agent ball additional reward ikick given ﬁrst time episode agent close enough kick ball. kick goal reward proportional change distance ball center goal additional reward given scoring goal igoal. weighted components results single reward ﬁrst guides agent close enough kick ball rewards kicking towards goal ﬁnally scoring. necessary provide higher gain kick-to-goal component reward immediately following kick move-to-ball component produces negative rewards ball moves away agent. overall reward follows disappointing reward engineering necessary. however exploration task proves difﬁcult ever gain traction reward consists scoring goals acting randomly exceedingly unlikely yield even single goal reasonable amount time. interesting direction future work better ways exploring large state spaces. recent approach direction stadie assigned exploration bonuses based model system dynamics. deep neural networks adept general purpose function approximators widely used supervised learning tasks. recently however applied reinforcement learning problems giving rise ﬁeld deep reinforcement learning. ﬁeld seeks combine advances deep neural networks reinforcement learning algorithms create agents capable acting intelligently complex environments. section presents background deep reinforcement learning continuous action spaces. notation closely follows lillicrap deep model-free discrete action spaces performed using deep q-learning method introduced mnih employs single deep network estimate value function discrete action acting selects maximally valued output given state input. several variants explored. narasimhan used decaying traces hausknecht stone investigated lstm recurrency hasselt explored double q-learning. networks work well continuous state spaces function continuous action spaces output nodes network continuous trained output q-value estimates rather continuous actions. actor/critic architecture provides solution problem decoupling value learning action selection. represented using deep neural networks actor network outputs continuous actions critic estimates value function. actor network parameterized takes input state outputs continuous action critic network parameterized takes input state action outputs scalar q-value figure shows critic actor networks. however continuous action spaces equation longer tractable involves maximizing next-state actions instead actor network provide next-state action µ|θµ). yields critic loss following form value function critic learned gradient descent loss function respect however accuracy value function highly inﬂuenced quality actor’s policy since actor determines next-state action update target. critic’s knowledge action values harnessed learn better policy actor. given sample state goal actor minimize difference current output optimal action state critic used provide estimates quality different actions naively estimating would involve maximizing critic’s output possible actions maxa instead seeking global maximum critic network provide gradients indicate directions change action space lead higher estimated q-values ∇aq. obtain gradients requires single backward pass critic network much faster solving optimization problem continuous action space. note gradients common gradients respect parameters. instead gradients respect inputs ﬁrst used nfqca update actor network gradients placed actor’s output layer backpropagated network. given state actor forward produce action critic evaluates resulting gradients used update actor alternatively think updates simply interlinking actor critic networks forward pass actor’s output passed forward critic evaluated. next estimated q-value backpropagated critic producing gradients indicate action change order increase q-value. backwards pass gradients critic actor. update performed actor’s parameters. figure shows example update. updates critic rely assumption actor’s policy good proxy optimal policy. updates actor rest assumption critic’s gradients suggested directions policy improvement valid tested environment. come surprise several techniques necessary make learning process stable convergent. critic’s policy inﬂuences actor critic updates errors critic’s policy create destructive feedback resulting divergence actor critic both. resolve problem mnih introduce target-q-network replica critic network changes slower time scale critic. target network used generate next state targets critic update similarly target-actor-network combats quick changes actor’s policy. second stabilizing inﬂuence replay memory fifo queue consisting agent’s latest experiences updating mini-batches experience sampled uniformly memory reduces bias compared updating exclusively recent experiences. figure actor-critic architecture actor critic networks interlinked allowing activations forwards actor critic gradients backwards critic actor. gradients coming critic indicate directions improvement continuous action space used train actor network without explicit targets. actor update backwards pass generates critic gradients w.r.t. action. gradients back-propagated actor resulting gradients w.r.t. parameters used update actor. critic gradients w.r.t. parameters ignored actor update. finally updates applied respective networks per-parameter step size determined gradient descent algorithm. additionally target-actor target-critic networks updated smoothly track actor critic using factor shown figure actor critic employ architecture state inputs processed four fully connected layers consisting units respectively. fully connected layer followed rectiﬁed linear activation function negative slope weights fully connected layers gaussian initialization standard deviation connected ﬁnal inner product layer linear output layers four discrete actions another parameters accompanying actions. addition state features critic also takes input four discrete actions action parameters. outputs single scalar q-value. adam solver actor critic learning rate target networks track actor critic using complete source code agent available https//github.com/mhauskn/dqn-hfo domain https//github.com/mhauskn/hfo/. introduced background deep reinforcement learning continuous action space present parameterized action space. following notation parameterized action space markov decision process deﬁned discrete actions ak}. discrete action features continuous parameters rma. actions represented tuples paired associated parameters parameter output layer thus actor network simultaneously chooses discrete action execute parameterize action. training critic network receives input values output nodes four discrete actions action parameters. indicate critic discrete action actually applied environment continuous parameters associated discrete action. similarly updating actor critic provides gradients four discrete actions continuous parameters. seem critic lacking crucial information structure action space experimental results section demonstrate critic learns provide gradients correct parameters discrete action. exploration continuous action space differs discrete space. adapt \u0001-greedy exploration parameterized action space probability random discrete action selected sampled using uniform random distribution. associated continuous parameters experimentally anneal ﬁrst updates. lillicrap demonstrate ornstein-uhlenbeck exploration also successful continuous action space. half field offense domain bounds range continuous parameter. parameters indicating direction bounded parameters power bounded without enforcing bounds hundred updates observed continuous parameters routinely exceeding bounds. updates permitted continue parameters would quickly trend towards astronomically large values. problem stems critic providing gradients encourage actor network continue increasing parameter already exceeds bounds. explore three approaches preserving parameters intended ranges zeroing gradients perhaps simplest approach examine critic’s gradients parameter zero gradients suggest increasing/decreasing value parameter already upper/lower limit range indicates critic’s gradient respect parameter pmin pmax indicate respectively minimum bound maximum bound current activation parameter. squashing gradients squashing function hyperbolic tangent used bound activation parameter. subsequently parameters re-scaled intended ranges. approach advantage requiring manual gradient tinkering presents issues squashing function saturates. inverting gradients approach captures best aspects zeroing squashing gradients minimizing drawbacks. gradients downscaled parameter approaches boundaries range inverted parameter exceeds value range. approach actively keeps parameters within bounds avoiding problems saturation. example critic continually recommends increasing parameter converge parameter’s upper bound. critic decides decrease parameter decrease immediately. contrast squashing function would saturated upper bound range require many updates decrease. mathematically inverted gradient approach expressed follows noted approaches speciﬁc parameterized action space. domain featuring bounded-continuous action space require similar approach enforcing bounds. three approaches empirically evaluated next section. evaluate zeroing squashing inverting gradient approaches parameterized domain task approaching ball scoring goal. approach independently train agents. agents trained million iterations approximately episodes play. training agent took three days nvidia titan-x gpu. three approaches inverting gradient shows robust learning. indeed inverting gradient agents learned reliably approach ball score goals. none four agents using squashing zeroing gradients able reliably approach ball score. analysis squashing gradient approach reveals parameters stayed within bounds squashing functions quickly became saturated. resulting agents take discrete action maximum/minimum parameters timestep. given observed proclivity critic’s gradients push parameters towards ever larger/small values surprise squashing function quickly become saturated never recover. analysis zeroing gradient approach reveals problems parameters still overﬂow bounds instability gradient zeroing approach negates direct attempts increase parameter beyond bounds hypothesize ﬁrst problem stems gradients applied parameters inadvertently allow parameter overﬂow. empirically observed learned networks attempting dash power maximum reasonable critic network encourage actor dash faster. unstable learning observed zeroing gradient agents. instability well captured q-values critic losses shown figure it’s clear agent became unstable remaining stable agent showed clear results learning. results highlight necessity non-saturating functions effectively enforce action bounds. approach inverting gradients observed respect parameter boundaries without saturating. result critic able effectively shape actor’s policy. evaluation reliability quality invertinggradient policies presented next section. figure analysis gradient bounding strategies left/middle/right columns respectively correspond inverting/zeroing/squashing gradients approaches handling bounded continuous actions. first depicts learning curves showing overall task performance inverting gradient approach succeeds learning soccer task. second shows average q-values produced critic throughout entire learning process inverting gradient approach shows smoothly increasing q-values. zeroing approach shows astronomically high q-values indicating instability critic. squashing approach shows stable q-values accurately reﬂect actor’s performance. third shows average loss experienced critic update reward experienced critic loss expected rise past actions seen increasingly sub-optimal. inverting gradients shows growing critic loss outliers accounting rapid increase nearing right edge graph. zeroing gradients approach shows unstably large loss. squashing gradients never discovers much reward loss stays near zero. evaluate inverting gradient agents comparing expert agent independently created helios robocup-d team. agent robocup-d world championship source code subsequently released thus hand-coded policy represents extremely competent player high performance bar. additional baseline compare sarsa learning agent. state-action-reward-stateaction algorithm model-free on-policy reinforcement learning sutton barto sarsa agent learns simpliﬁed version featuring high-level discrete actions moving dribbling shooting ball. input given continuous features including distance angle goal center. tile coding sutton barto used discretize state space. experiences collected playing game used bootstrap value function. show deep reinforcement learning process reliable additional previous inverting-gradient agents independently train another inverting-gradient agents total seven agents ddpg−. seven agents learned score goals. comparing helios’ champion agent learned agents evaluated episodes quickly reliably score. seven ddpg agents outperform sarsa baseline remarkably three seven ddpg agents score reliably helios’ champion agent. occasional failures helios agent result noise action space occasionally causes missed kicks. contrast ddpg agents learn take extra time score goal become accurate result. extra time reasonable considering ddpg rewarded scoring experiences real pressure score quickly. encouraged deep reinforcement learning produce agents competitive even exceeding expert handcoded agent. figure left scatter plot learning curves ddpg-agents lowess curve. three distinct phases learning seen agents ﬁrst small rewards approaching ball learn kick ball towards goal start scoring goals around episode right ddpg-agents score nearly reliably expert baseline take longer video ddpg’s policy viewed https //youtu.be/lncl-je_. robocup soccer rich history learning. earliest examples andre teller used genetic programming evolve policies robocup soccer. using sequence reward functions ﬁrst encourage players approach ball kick ball score goal ﬁnally game. similarly work features players whose policies entirely trained hand-coded components. work differs using gradient-based learning method paired using reinforcement learning rather evolution. masson konidaris present parameterized-action formulation approaches model-free reinforcement learning environments. core approach uses parameterized policy choosing discrete action select another policy selecting continuous parameters action. given ﬁxed policy parameter selection q-learning optimize policy discrete action selection. next policy discrete action selection policy search method optimize parameter selection. alternating learning phases yields convergence either local global optimum depending whether policy search procedure guarantee optimality. contrast approach learning parameterized action space features parameterized actor learns discrete actions parameters parameterized critic learns action-value function. instead relying external policy search procedure able directly query critic gradients. finally parameterize policies using deep neural networks rather linear function approximation. deep networks offer theoretical convergence guarantees strong record empirical success. experimentally masson konidaris examine simpliﬁed abstraction robocup soccer co-locates agent ball start every trial features smaller action space consisting parameterized kick actions. however examine difﬁcult task scoring keeper. since domain hand-crafted closed-source it’s hard estimate difﬁcult task compared goal scoring task paper. competitive robocup agents primarily handcoded feature components learned optimized. macalpine employed layered-learning framework incrementally learn series interdependent behaviors. perhaps best example comprehensively integrating learning brainstormers competition neural network make large portion decisions spanning level skills high level strategy however work done prior advent deep reinforcement learning thus required constrained focused training environments skills. contrast study learns approach ball kick towards goal score within context single monolithic policy. deep learning methods proven useful various control domains. previously mentioned ddpg provide great starting points learning discrete continuous action spaces. additionally levine demonstrates ability deep learning paired guided policy search learn manipulation policies physical robot. high requirement data hurdle applying deep reinforcement learning directly onto robotic platforms. work differs examining action space latent structure parameterized-continuous actions. harder task scoring goalie left future work. additionally robocup domain presents many opportunities multi-agent collaboration adhoc-teamwork setting true multi-agent settings challenges multi-agent learning robocup domain examined prior work solutions translate deep reinforcement learning settings well. progress direction could eventually result team deep reinforcement learning soccer players. another interesting possibility utilizing critic’s gradients respect state inputs ∇sq. gradients indicate directions improvement state space. agent forward model able exploit gradients transition states critic ﬁnds favorable. recent developments model-based deep reinforcement learning show detailed next state models possible. paper presented agent trained exclusively deep reinforcement learning learns scratch approach ball kick ball goal score. best learned agent scores goals reliably handcoded expert policy. work address challenging tasks scoring goalie cooperating team still represents step towards fully learning complex robocup agents. generally demonstrated capability deep reinforcement learning parameterized action space. make possible extended ddpg algorithm presenting analyzing novel approach bounding action space gradients suggested critic. extension speciﬁc domain likely prove useful continuous bounded action space. authors wish thank yilun chen. work taken place learning agents research group artiﬁcial intelligence laboratory university texas austin. larg research supported part grants national science foundation afrl afosr yujin robot. additional support texas advanced computing center nvidia corporation. kalyanakrishnan shivaram yaxin stone peter. half ﬁeld offense robocup soccer multiagent reinforcement learning case study. lakemeyer gerhard sklar elizabeth sorenti domenico takahashi tomoichi robocup- robot soccer world volume lecture notes artiﬁcial intelligence springer verlag berlin isbn ----. macalpine patrick depinet mike stone peter. austin villa robocup simulation league champion overlapping layered learning. proceedings twenty-ninth aaai conference artiﬁcial intelligence january mnih volodymyr kavukcuoglu koray silver david rusu andrei veness joel bellemare marc graves alex riedmiller martin fidjeland andreas ostrovski georg petersen stig beattie charles sadik amir antonoglou ioannis king helen kumaran dharshan wierstra daan legg shane hassabis demis. human-level control deep reinforcement learning. nature february issn ./nature. http//dx.doi.org/./nature. narasimhan karthik kulkarni tejas barzilay regina. language understanding textbased games using deep reinforcement learning. corr abs/. http //arxiv.org/abs/.. junhyuk xiaoxiao honglak lewis richard singh satinder actionconditional video prediction using deep networks atari games. corr abs/. http//arxiv.org/abs/.. riedmiller martin gabel thomas. experiences complex competitive gaming domain reinforcement learning meets robocup. isbn http//ieeexplore.ieee.org/xpl/mostrecentissue.jsp? punumber=. stadie bradly levine sergey abbeel pieter. incentivizing exploration reinforcement learning deep predictive models. corr abs/. http//arxiv. org/abs/..", "year": 2015}