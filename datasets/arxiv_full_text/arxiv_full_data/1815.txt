{"title": "Max-Pooling Loss Training of Long Short-Term Memory Networks for  Small-Footprint Keyword Spotting", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "We propose a max-pooling based loss function for training Long Short-Term Memory (LSTM) networks for small-footprint keyword spotting (KWS), with low CPU, memory, and latency requirements. The max-pooling loss training can be further guided by initializing with a cross-entropy loss trained network. A posterior smoothing based evaluation approach is employed to measure keyword spotting performance. Our experimental results show that LSTM models trained using cross-entropy loss or max-pooling loss outperform a cross-entropy loss trained baseline feed-forward Deep Neural Network (DNN). In addition, max-pooling loss trained LSTM with randomly initialized network performs better compared to cross-entropy loss trained LSTM. Finally, the max-pooling loss trained LSTM initialized with a cross-entropy pre-trained network shows the best performance, which yields $67.6\\%$ relative reduction compared to baseline feed-forward DNN in Area Under the Curve (AUC) measure.", "text": "propose max-pooling based loss function training long short-term memory networks smallfootprint keyword spotting memory latency requirements. max-pooling loss training guided initializing cross-entropy loss trained network. posterior smoothing based evaluation approach employed measure keyword spotting performance. experimental results show lstm models trained using cross-entropy loss max-pooling loss outperform cross-entropy loss trained baseline feed-forward deep neural network addition max-pooling loss trained lstm randomly initialized network performs better compared cross-entropy loss trained lstm. finally max-pooling loss trained lstm initialized crossentropy pre-trained network shows best performance yields relative reduction compared baseline feed-forward area curve measure. index terms— lstm keyword spotting max-pooling keyword spotting active research area decades. different approaches proposed detect words interest speech utterances. solution general large vocabulary continuous speech recognition system applied decode audio signal keyword searching conducted resulting lattices confusion networks methods require relatively high computational resources lvcsr decoding also introduce latency. small-footprint keyword spotting systems increasingly attracting attention. voice assistant systems alexa amazon echo deploy keyword spotting system device stream audio cloud lvcsr keyword detected device. applications accurate on-device keyword spotting running memory critical needs high recall make devices easy false accepts mitigate privacy concerns. latency well. traditional approach employs hidden markov model model keyword background background includes non-keyword speech nonspeech noise etc. background model also named ﬁller model literatures. could involve loops simple speech/non-speech phones complicated cases normal phone confusing word set. viterbi decoding used search best path decoding graph. keyword spotting decision made based likelihood comparison keyword background models. gaussian mixture model commonly used past model observed acoustic features. becoming mainstream acoustic modeling approach extended include discriminative information incorporating hybrid dnn-hmm decoding framework recent years keyword spotting systems built convolutional neural network directly involved system decoding time framewise keyword posteriors smoothed. system triggered smoothed keyword posteriors exceed pre-deﬁned threshold. trade balancing false rejects false accepts performed tuning threshold. context information taken care stacking frames input. keyword spotting systems built recurrent neural network directly. particularly bidirectional lstm used search keywords audio streams latency hard constraint interested small-footprint keyword spotting system runs memory utilization latency. latency constraint makes bidirectional lstm proper principle. instead focus training unidirection lstm model using different loss functions cross-entropy loss max-pooling based loss decoding time system triggered keyword posterior smoothed averaging output sliding window threshold. considering practical case keyword spotting system designed lock time detection avoid unnecessary false accepts reduce decoding computational cost. remaining part paper organized follows section describes lstm based keyword spotting system includes lstm model training loss functions performance evaluation details. experimental setup results included section section conclusion future work. shown figure filter-bank energies used input acoustic features keyword spotting system. extract dimensional lfbes frames frame shift. lstm model used process input lfbes. system targets output layer non-keyword keyword. output keyword spotting system passed evaluation module decision making. different feed-forward networks rnns contain cyclic connections used model sequential data makes rnns natural model temporal information within continuous speech frames. however traditional structures suffer vanishing gradient problem prevents effectively modeling long context data. overcome this lstms contain memory blocks block contains memory cells well input output forget gates. three gates control information within associated memory block. sometimes projection layer added lstm output reduce model complexity typical lstm component projection layer shown figure sake clarity single lstm block shown here. matrices label connection weights. e.g. represent weight matrices input recurrent feedback cell respectively. note peephole connections diagonal matrices. terms represent bias vectors different components model. e.g. bias input gate activation. projection layer added lstm output. linearly maps lower dimensional representation recurrent signal. network output computed based projection layer output well. regarding activation functions logistic sigmoid function gate activations tanh cell input output softmax output layer. element-wise product vectors. caused inaccurate frame alignment around keyword segment boundaries. max-pooling loss viewed transition frame-level loss segment-level loss keyword spotting model training. alternative segment-level loss functions include different statistics frame-level keyword posteriors within keyword segment e.g. geometric mean etc. literatures training lstms using connectionist temporal classiﬁcation keyword spotting tasks well. addition architectures combine lstms cnns applied different tasks typically lstm added layers layers pooling used extracted features lstm input lstm output used prediction. denote cardinality target keyword set. consider word level labels total classes additional class used label frames aligned background. input frames keywords instances inside denote continuous frame index range whose frames aligned keyword. result k-dimensional target vector frames within represent collection frame index ranges keywords instances collection indices remaining frames aligned keyword represent target label frames inside label speciﬁc frame within whose posterior maximum. max-pooling loss proposed input sequence calculated ﬁrst item states calculate cross-entropy loss input frames aligned keyword. second item shows max-pooling keyword aligned frames. details frames segment aligned keyword back propagate single frame widely applied loss function training total number classes. given sequence frames feature vector frame denote k-dimensional output network denote corresponding target vector. cross-entropy loss frame calculated follows -of-k coding usually used target vector frame vector aligned class kdimensional vector value element elements denote aligned class frame. cross-entropy loss frame formulated propose train lstm keyword spotting using max-pooling based loss function. given lstm ability model long context information hypothesize need teach lstm every frame within keyword segment. instead want teach lstm highest conﬁdence time. lstm near keyword segment general seen enough context make decision. simple back-propagate loss last frame last several frames updating weights. initial experiments indicate lstm learn much scheme. hence employ max-pooling based loss function lstm pick informative keyword frames teach itself. also helps mitigate issues potentially training learn well ﬁrst epochs rather random keyword ﬁring. idea take advantages cross-entropy max-pooling loss training. cross-entropy trained lstm initial model start max-pooling training already learns basic knowledge target keywords. could provide better initialization point faster convergence better local optimum. consider posteriors smoothing based evaluation scheme. detect keyword given input audio system computes smoothed posteriors based sliding context window containing nctx frames. smoothed posterior keyword exceeds pre-deﬁned threshold considered ﬁring spike. system designed shut following nlck frames. lockout period length nlck purpose reducing unnecessarily duplicated detections keyword segment well reducing decoding computational cost. case allow short latency period nlat frames keyword segment. system ﬁres within nlat-frame window right keyword segment still consider ﬁring aligned corresponding keyword. latency window introduce signiﬁcant delay perception could mitigate possible issues inaccurate keyword alignment boundaries evaluation. finally ﬁrst ﬁring spike within keyword segment plus latency window considered valid detection. ﬁring spikes within keyword segment plus latency window outside keyword segment plus latency window counted false accepts. metrics used measure system performance miss rate mifigure illustrates idea evaluation approach. examples input audio streams. keyword segment length varies depending keyword spoken. keyword segment followed ﬁxed length latency window. keyword segments labeled blocks vertical line follow-on latency windows labeled blocks horizontal line ﬁll. system lock period design ﬁring spike. ﬁrst audio false accepts system ﬁring region outside keyword segment plus latency window. true accepts happen ﬁrst detection keyword segment plus latency window. true accepts could happen either keyword segment following latency window. second audio false accepts happen additional ﬁring spikes within keyword segment plus latency window already true accept. experiments word ’alexa’ chosen keyword. in-house far-ﬁeld corpus contains farﬁeld data collected different conditions. dataset contains order magnitude instances keyword background speech utterances largest previous studies training testing. data collected far-ﬁeld environment challenging task nature. considering large size corpus development partition sufﬁcient tune parameters since target keyword ’alexa’ binary target used experiments. frames background target label frames aligned keyword target train feed-forward model baseline based model structure training described adaptations experimental setup case. compare lstm models trained cross-entropy loss max-pooling loss. gpu-based distributed trainer described used experiments. performance based learning rate schedule used model training. elaborate training epoch loss degrades compared previous epoch learning rate halved current epoch repeated reduced learning rate. training process terminates either minimal learning rate maximum number epochs reached initial learning rate batch size tuned development set. baseline feed-forward four hidden layers nodes hidden layer. sigmoid function used activation. stack frames left frames right used form input feature vector. note right context cannot large since introduces latency. total parameters model. layerwise pre-training used dnn. initial learning rate training batch size projection layer dimension serves purpose memory well latency. input context consider frames left frames right. note still frames left context lstm input though lstm learns past frames’ information deﬁnition. lstm training setup aligned better comparison past information imposed lstm training. lstm parameters. random initialization lstm parameters initialized uniform distribution weights constant bias. initial learning rates chosen cases cross-entropy loss max-pooling loss randomly initialized model max-pooling loss initialized cross-entropy pre-trained model. evaluation approach described section test dataset. performance lstm models shown figure plot detection curves miss rate range i.e. case. false accept rate computed normalizing false accept counts total number test data utterances. x-axis labels false accept rate y-axis labels miss rate. lower numbers indicate better performance. blue solid curve represents baseline feed-forward trained using cross-entropy loss. lstm models trained using cross-entropy loss max-pooling loss random initialization max-pooling loss cross-entropy pre-training labeled green dashed dash-dot cyan dotted curves respectively. absolute numbers false accepts obscured paper conﬁdentiality reasons. instead plot false accept rates present work training small-footprint lstm spot keyword ’alexa’ far-ﬁeld conditions. loss functions employed lstm training crossentropy loss max-pooling loss proposed paper. smoothed posterior thresholding approach used evaluation. keyword spotting performance measured using miss rate false accept rate. show lstm performs better general. best lstm system trained using max-pooling loss crossentropy loss pre-training reduces number miss rate range. future work plan weighting max-pooling loss based lstm training i.e. scale back-propagated loss selected keyword frames. interest lstm performance improved varying model structures e.g. adding additional feed-forward layers lstm component. also plan benchmark max-pooling loss performance segmental level loss functions e.g. geometric mean framewise keyword posteriors within keyword segment keyword spotting experiments. miller d.r. kleber c.l. kimball colthurst lowe s.a. schwartz r.m. gish rapid accurate spoken term detection proceedings annual conference international speech communication association tsakalidis hsiao karakos ranjan saikumar zhang nguyen schwartz makhoul vietnamese telephone speech keyword spotting system ieee international conference acoustics speech signal processing selected miss rate range lstm models outperform baseline feed-forward dnn. terms different loss functions lstm training max-pooling loss random initialization superior cross-entropy loss. lstm trained using max-pooling loss cross-entropy loss pretraining yields best results. compute area curve numbers quantitative comparison different models. computed curves hence lower better. relative changes lstm models compared baseline summarized table experimental results indicate miss rate range compared cross-entropy loss trained baseline cross-entropy loss trained lstm results relative reduction auc. lstm model trained using maxpooling loss random initialization shows relative reduction auc. best performance comes lstm trained using max-pooling loss cross-entropy pre-training yields reduction compared baseline dnn. wilpon j.g. rabiner c.h. goldman e.r. automatic recognition keywords unconstrained speech using hidden markov models ieee transactions acoustics speech signal processing wilpon j.g. miller l.g. modi improvements applications word recognition using hidden markov modeling techniques ieee international conference acoustics speech signal processing panchapagesan khare matsoukas mandal hoffmeister vitaladevuni multi-task learning weighted cross-entropy dnn-based keyword spotting proceedings annual conference international speech communication association chen parada heigold small-footprint keyword spotting using deep neural networks ieee international conference acoustics speech signal processing nakkiran alvarez prabhavalkar parada compressing deep neural networks using rankconstrained topology proceedings annual conference international speech communication association sainath parada convolutional neural networks small-footprint keyword spotting proceedings annual conference international speech communication association tucker panchapagesan vitaladevuni model compression applied small-footprint keyword spotting proceedings annual conference international speech communication association sundar lehman j.f. singh keyword spotting multi-player voice driven games children proceedings annual conference international speech communication association scherer muller behnke evaluation pooling operations convolutional architectures object recognition proceedings international conference artiﬁcial neural networks senior a.w. beaufays long shortterm memory recurrent neural network architectures large scale acoustic modeling proceedings annual conference international speech communication association graves fernandez gomez schmidhuber connectionist temporal classiﬁcation labelling unsegmented sequence data recurrent neural networks proceedings international conference machine learning yue-hei hausknecht vijayanarasimhan vinyals monga toderici beyond short snippets deep networks video classiﬁcation proceedings ieee conference computer vision pattern recognition strom scalable distributed training using commodity cloud computing proceedings annual conference international speech communication association", "year": 2017}