{"title": "Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models  of Meaning", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "Deep compositional models of meaning acting on distributional representations of words in order to produce vectors of larger text constituents are evolving to a popular area of NLP research. We detail a compositional distributional framework based on a rich form of word embeddings that aims at facilitating the interactions between words in the context of a sentence. Embeddings and composition layers are jointly learned against a generic objective that enhances the vectors with syntactic information from the surrounding context. Furthermore, each word is associated with a number of senses, the most plausible of which is selected dynamically during the composition process. We evaluate the produced vectors qualitatively and quantitatively with positive results. At the sentence level, the effectiveness of the framework is demonstrated on the MSRPar task, for which we report results within the state-of-the-art range.", "text": "deep compositional models meaning acting distributional representations words order produce vectors larger text constituents evolving popular area research. detail compositional distributional framework based rich form word embeddings aims facilitating interactions words context sentence. embeddings composition layers jointly learned generic objective enhances vectors syntactic information surrounding context. furthermore word associated number senses plausible selected dynamically composition process. evaluate produced vectors qualitatively quantitatively positive results. sentence level effectiveness framework demonstrated msrpar task report results within state-of-the-art range. representing meaning words using distributional behaviour large text corpus well-established technique research proved useful numerous tasks. distributional model meaning semantic representation word given vector high dimensional vector space obtained either explicitly collecting co-occurrence statistics target word words belonging representative subset vocabulary directly optimizing word vectors objective function neural-network based architecture larger text constituents phrases sentences since uniqueness multi-word expressions would inevitably lead data sparsity problems thus unreliable vectorial representations. problem usually addressed provision compositional function purpose prepare vectorial representation phrase sentence combining vectors words therein. nature complexity compositional models vary approaches based deep-learning architectures shown especially successful modelling meaning sentences variety tasks mutual interaction distributional word vectors means compositional model provides many opportunities interesting research majority still remains explored. direction investigate lexical ambiguity affects compositional process. fact recent work shown shallow multi-linear compositional models explicitly handle extreme cases lexical ambiguity step prior composition present consistently better performance ambiguous counterparts ﬁrst attempt test observations deep compositional setting presented cheng promising results. furthermore second important question relates nature word embeddings used context compositional model. setting form word vectors means discriminating words based underlying semantic relationships; main goal word vector contribute bigger whole—a task syntax along semantics also plays important role. central point paper therefore compositional distributional model meaning word vectors injected information reﬂects syntactical roles training corpus. purpose work improve current practice deep compositional models meaning relation compositional process quality word embeddings used therein. propose architecture jointly training compositional model word embeddings imposes dynamic word sense induction word during learning process. note contrast recent work multi-sense neural word embeddings word senses learned without compositional considerations mind. furthermore make word embeddings syntax-aware introducing variation hinge loss objective function collobert weston goal predict occurrence target word context also predict position word within context. qualitative analysis shows vectors reﬂect semantic syntactic features concise way. current deep compositional distributional settings word embeddings internal parameters model purpose task speciﬁcally trained. work main considerations joint training step generic enough tied particular task. word embeddings derived compositional model learned data much diverse task-speciﬁc dataset reﬂecting wider range linguistic features. indeed experimental evaluation shows produced word embeddings serve high quality general-purpose semantic word space presenting performance stanford contextual word similarity dataset huang competitive even better performance well-established neural word embeddings sets. finally propose dynamic disambiguation framework number existing deep compositional models meaning multisense word embeddings compositional model original training step reﬁned according purposes speciﬁc task hand. context paraphrase detection achieve result close current state-ofthe-art microsoft research paraphrase corpus interesting aspect sideline paraphrase detection experiment that contrast mainstream approaches mainly rely simple forms clas. distributional models meaning distributional models meaning follow distributional hypothesis states words occur similar contexts similar meanings. traditional approaches constructing word space rely simple counting word represented vector numbers show frequently word co-occurs possible context words corpus text. contrast methods recent class distributional models treat word representations parameters directly optimized word prediction task instead relying observed cooccurrence counts models maximize objective function neural net-based architecture; mikolov example compute conditional probability observing words context around target word recent studies shown that compared co-occurrence counterparts neural word vectors reﬂect better semantic relationships words effective compositional settings syntactic awareness since main purpose distributional models measure semantic relatedness words relatively little effort making word vectors aware information regarding syntactic role word occurs sentence. cases vectors postag speciﬁc ‘book’ noun ‘book’ verb represented different vectors furthermore word spaces context target word determined means grammatical dependencies effective capturing syntactic relations approaches based simple word proximity. word embeddings trained neural settings syntactic information usually taken explicitly account notable exceptions. lexical level levy goldberg propose extension skip-gram model based grammatical dependencies. following different approach mnih kavukcuoglu weight vector context word depending distance target word. regard compositional settings hashimoto dependencybased word embeddings employing hinge loss objective hermann blunsom condition objectives types involved words. compositionality distributional models methods equip distributional models meaning compositional abilities come many different levels sophistication simple element-wise vector operators addition multiplication category theory latter work relational words represented multi-linear maps acting vectors representing arguments general models shallow sense functional parameters output produced direct interaction inputs; shown capture compositional meaning sentences adequate degree. idea using neural networks compositionality language appeared years seminal paper pollack recently re-popularized socher colleagues compositional architecture used works recursive neural network words composed following parse tree. particular variant recnn recurrent neural network sentence assumed generated aggregating words sequence furthermore recent work models meaning sentences utilizing concept convolutional neural network main characteristic acts small overlapping parts input vectors. models word embeddings weights compositional layers optimized task-speciﬁc objective function. generic objective trained general-purpose text corpus. focus recursive recurrent neural network architectures general ideas discuss principle model-independent. disambiguation composition regardless address composition models section rely ambiguous word spaces every meaning polysemous word merged single vector. especially cases homonymy word used describe completely unrelated concepts approach problematic semantic representation word becomes average senses inadequate express reliable way. address problem prior disambiguation step word vectors often introduced purpose word representations best given context composition takes place idea tested algebraic tensor-based compositional functions positive results. furthermore also found provide minimal beneﬁts recnn compositional architecture number phrase sentence similarity tasks latter work clearly suggests explicitly dealing lexical ambiguity deep compositional setting idea worth explored. treating disambiguation preprocessing step strategy less optimal neural setting would expect beneﬁts greater architecture disambiguation takes place dynamic fashion training. ready start detailing compositional model takes account considerations. issue lexical ambiguity covered section section deals generic training syntactic awareness. propose novel architecture learning word embeddings compositional model single step. learning takes places context recnn word embeddings parameters compositional layer optimized generic objective function uses hinge loss function. sentences compositional layer margin wish retain scores positive training examples negative ones. training parameters scoring layer compositional layers word representations jointly updated error back-propagation. output general-purpose syntax-aware word representations weights corresponding compositional model. extend model address lexical ambiguity. achieve applying gated architecture similar used multi-sense model neelakantan advancing main idea compositional setting detailed section assume ﬁxed number senses word. word associated main vector well vectors denoting cluster centroids equal number sense vectors. cluster centroids sense vectors randomly initialized beginning process. word training sentence prepare context vector averaging main vectors words context. context vector compared cluster centroids cosine similarity sense corresponding closest cluster selected representative current context. selected cluster centroid updated addition context vector associated sense vector passed input compositional layer. selected sense vectors word sentence updated backpropagation based objectives equations overall architecture model described previous section illustrated figure denotes concatenation vectors non-linear function parameters model. recnn case compositional process continues recursively following parse tree vector whole sentence phrase produced; hand assumes sentence generated left-to-right fashion taking consideration dependencies word adjacency. amend setting introducing novel layer compositional scores linguistic plausibility composed sentence phrase vector regard syntax semantics. following collobert weston convert unsupervised learning problem supervised corrupting training sentences. speciﬁcally sentence create sets negative examples. ﬁrst target word within given context replaced random word; original paper used enforce semantic coherence word vectors. syntactic coherence enforced second negative examples words context randomly shufﬂed. objective function deﬁned terms following hinge usual approach problem utilize classiﬁer acts concatenation sentence vectors work follow novel perspective speciﬁcally apply siamese architecture concept extensively used computer vision siamese networks also used past purposes best knowledge ﬁrst time setting applied paraphrase detection. model networks sharing parameters used compute vectorial representations sentences paraphrase relation wish detect; achieved employing cost function compares vectors. commonly used cost functions ﬁrst based norm second cosine similarity norm variation capable handling differences magnitude vectors. formally cost function deﬁned input sentences compositional layer refer sentence vectors) denotes paraphrase relationship sentences; stands margin hyper-parameter chosen advance. speciﬁc task consequence task-speciﬁc training dataset. however note replacing plausibility layer classiﬁer trained task hand taskspeciﬁc network transparently trains multisense word embeddings applies dynamic disambiguation idea singlestep direct training seems appealing consideration task-speciﬁc dataset used training probably reﬂect linguistic variety required exploit expressiveness setting full. additionally many cases size datasets tied speciﬁc tasks prohibiting training deep architecture. merit proposal that cases like these possible train generic model figure large corpus text produced word vectors compositional weights initialize parameters speciﬁc version architecture. result trained parameters reﬁned according task-speciﬁc objective. figure illustrates generic case compositional framework applying dynamic disambiguation. note sense selection takes place soft-max layer directly optimized task objective. compositional architectures recnn rnn. recnn case words composed following result external parser composition takes place sequence left right. avoid exploding vanishing gradient problem long sentences employ long short-term memory network training model minimize hinge loss equations plausibility layer implemented -layer network units hidden layer applied individual node parameters updated mini-batches adadelta gradient descent method ﬁrst step qualitatively evaluate trained word embeddings examining nearest neighbours lists selected words. compare results produced skipgram model mikolov language model collobert weston refer model sams results table show clearly model tends group words semantically syntactically related; example contrast compared models group words semantic level model able retain tenses numbers gerunds. observed behaviour comparable embedding models objective functions conditioned grammatical relations words; levy goldberg example present similar table dependency-based extension skip-gram model. advantage approach models twofold ﬁrstly word embeddings accompanied generic compositional model used creating sentence representations independently speciﬁc task; secondly training quite forgiving data sparsity problems general dependency-based approach would intensify result small corpus sufﬁcient producing high quality syntax-aware word embeddings. cosine similar= sentence vectors scaling shifting parameters optimized sigmoid function label. experiments follow section cost functions evaluated. overall architecture shown figure section pre-trained vectors compositional weights deriving sentence representations subsequently siamese network. dynamic disambiguation framework used sense vectors words updated training sense selection process gradually reﬁned. model pre-training experiments word representations compositional models pre-trained british national corpus general-purpose text corpus contains million sentences written spoken english. comparison train sets word vectors compositional models ambiguous multi-sense dataset huang dataset contains pairs words contexts occur therefore make contextual information order select appropriate sense ambiguous word. similarly neelakantan three different metrics globalsim measures similarity ambiguous word vectors; localsim selects single sense word based context computes similarity sense vectors; avgsim represents word weighted average senses given context computes similarity between weighted sense vectors. compute report spearman’s correlation embedding similarities human judgments addition skipgram collobert weston models also compare cbow model multi-sense skip-gram model neelakantan among methods mssg model capable learning multi-prototype word representations. embeddings show performance localsim avgsim measures performance competitive mssg globalsim hierarchical soft-max objective function. compared original model version presents improvement .%—a clear indication effectiveness proposed learning method enhanced objective. last experiments proposed compositional distributional framework evaluated microsoft research paraphrase corpus contains pairs sentences. binary classiﬁcation task labels provided human annotators. apply siamese network detailed section msrpc used datasets evaluating paraphrase detection models size prohibitory attempt training deep architecture. therefore training rely much larger external dataset paraphrase database ppdb contains million paraphrase pairs million phrasal paraphrases million paraphrase patterns capture syntactic transformations sentences. phrasesentence-level paraphrase pairs additional training contexts ﬁne-tune generic compositional model parameters word embeddings train baseline models. original training msrpc used validation deciding hyperparameters margin error function number training epochs. vectors. table cosine error function consistently outperforms norm-based compositional models providing another conﬁrmation already well-established fact similarity semantic vector spaces better reﬂected lengthinvariant measures. effectiveness disambiguation proceed compare effectiveness compositional models using ambiguous vectors multi-sense vectors respectively. error function cosine similarity following results previous evaluation. dynamic disambiguation applied test methods selecting sense vectors hard case vector plausible sense selected soft case vector prepared weighted average sense vectors according probabilities returned soft-max layer baseline simple compositional model based vector addition. dynamic disambiguation models additive baseline compared variations simple prior disambiguation step applied word vectors. achieved ﬁrst selecting word sense vector closest average word vectors sentence composing selected sense vectors without considerations regarding ambiguity. baseline model prior disambiguation variants trained separate logistic regression classiﬁers. results shown table overall disambiguated vectors work better ambiguous ones improvement signiﬁcant additive model; there simple prior disambiguation step produces gains. deep compositional models simple prior disambiguation still helpful small improvements result consistent ﬁndings cheng small gains prior disambiguation models ambiguous models clearly show deep architectures quite capable performing elementary form sense selection intrinsically part learning process itself. however situation changes dynamic disambiguation framework used gains ambiguous version become signiﬁcant. comparing ways dynamic disambiguation numbers soft method gives slightly higher producing total gain ambiguous version recnn case. note that stage advantage using dynamic disambiguation framework simple prior disambiguation still small seek reason behind recursive nature architecture tends progressively hide local features word vectors thus diminishing effect ﬁne-tuned sense vectors produced dynamic disambiguation mechanism. next section discusses problem provides solution. role pooling problems recursive recurrent compositional architectures especially grammars strict branching structure english given composition usually product terminal non-terminal; i.e. single word contribute meaning sentence extent rest sentence whole below vp]s case contribution words within verb phrase ﬁnal sentence representation faded recursive composition mechanism. inspired related work computer vision attempt alleviate problem introducing average pooling layer sense vector level adding resulting vector sentence representation. expect sentence vector reﬂect local features words sentence help classiﬁcation direct way. results deep architectures shown table substantial improvements deep nets. importantly effect dynamic model positive addition recnn recnn+pooling recnn+pooling+c rnn+pooling rnn+pooling+c mihalcea islam inkpen fernando stevenson smith socher madnani eisenstein table also includes results models trained single step word sense vectors randomly initialized beginning process. that despite large size training results much lower ones obtained using pre-training step. demonstrates importance initial training general-purpose corpus resulting vectors reﬂect linguistic information that although obtainable task-speciﬁc training make great difference result classiﬁcation. notice using distributional properties alone cannot capture efﬁciently subtle aspects sentence example numbers human names. however even small differences aspects sentences lead different classiﬁcation result. therefore train additional logistic regression classiﬁer based embeddings similarity also handengineered features. ensemble classiﬁer original one. terms feature selection follow socher blacoe lapata following features difference sentence length unigram overlap among sentences features related numbers table report results original model ensembled model compare performance existing models. implemented models disambiguation performed guarantee best performance. ensembling original classiﬁer improve result previous section another second best result reported main contribution paper deep compositional distributional model acting linguistically motivated word embeddings. effectiveness syntax-aware multi-sense word vectors dynamic compositional disambiguation framework used demonstrated appropriate tasks lexical sentence level respectively positive results. aside also demonstrated beneﬁts siamese architecture context paraphrase detection task. architectures tested work limited recnn ideas presented principle directly applicable kind deep network. future step test proposed models convolutional compositional architecture similar kalchbrenner authors would like thank three anonymous reviewers useful comments well kalchbrenner grefenstette early discussions suggestions paper simon ˇsuster comments ﬁnal draft. dimitri kartsaklis gratefully acknowledges ﬁnancial support afosr. germ´an kruszewski. systematic comparison context-counting context-predicting semantic vectors. proceedings annual meeting association computational linguistics volume william blacoe mirella lapata. comparison vector-based representations semantic composition. proceedings joint conference empirical methods natural language processing computational natural language learning pages association computational linguistics. isabelle guyon yann lecun cliff moore eduard s¨ackinger roopak shah. signature veriﬁcation using siamese time delay neural network. international journal pattern recognition artiﬁcial intelligence jianpeng cheng dimitri kartsaklis edward grefenstette. investigating role prior disambiguation deep-learning composind workshop tional models meaning. learning semantics nips montreal canada december. ronan collobert jason weston. uniﬁed architecture natural language processing deep proneural networks multitask learning. ceedings international conference machine learning pages acm. dipanjan noah smith. paraphrase identiﬁcation probabilistic quasi-synchronous proceedings joint conferrecognition. ence annual meeting international joint conference natural language processing afnlp volume -volume pages association computational linguistics. samuel fernando mark stevenson. semantic similarity approach paraphrase detection. proceedings annual research colloquium special interest group computational linguistics pages citeseer. raia hadsell sumit chopra yann lecun. dimensionality reduction learning invariant computer vision pattern recogmapping. nition ieee computer society conference volume pages ieee. kazuma hashimoto pontus stenetorp makoto miwa yoshimasa tsuruoka. jointly learning word representations composition functions proceedusing predicate-argument structures. ings conference empirical methods natural language processing pages doha qatar october. association computational linguistics. karl moritz hermann phil blunsom. role syntax vector space models compositional semantics. proceedings annual meeting association computational linguistics pages soﬁa bulgaria august. association computational linguistics. eric huang richard socher christopher manning andrew improving word representations global context multiple word prototypes. proceedings annual meeting association computational linguistics long papers-volume pages association computational linguistics. yangfeng jacob eisenstein. discriminative improvements distributional sentence simproceedings conference ilarity. empirical methods natural language processing pages seattle washington october. association computational linguistics. kalchbrenner edward grefenstette phil blunsom. convolutional neural network modelling sentences. proceedings annual meeting association computational linguistics june. dimitri kartsaklis mehrnoosh sadrzadeh. prior disambiguation word tensors constructproceedings sentence vectors. conference empirical methods natural language processing pages seattle washington october. association computational linguistics. dimitri kartsaklis mehrnoosh sadrzadeh stephen pulman. separating disambiguation procomposition distributional semantics. ceedings conference natural language learning pages soﬁa bulgaria august. dimitri kartsaklis kalchbrenner mehrnoosh sadrzadeh. resolving lexical ambiguity proceedtensor regression models meaning. ings annual meeting association computational linguistics pages baltimore june. association computational linguistics. omer levy yoav goldberg. dependencybased word embeddings. proceedings annual meeting association computational linguistics pages baltimore maryland june. association computational linguistics. nitin madnani joel tetreault martin chodorow. re-examining machine translation metrics paraphrase identiﬁcation. proceedings conference north american chapter association computational linguistics human language technologies pages association computational linguistics. neural word representations tensor-based compoproceedings consitional settings. ference empirical methods natural language processing pages doha qatar october. association computational linguistics. jeff mitchell mirella lapata. vector-based proceedings models semantic composition. acl- pages columbus ohio june. association computational linguistics. andriy mnih koray kavukcuoglu. learning word embeddings efﬁciently noise-contrastive estimation. advances neural information processing systems pages vinod nair geoffrey hinton. rectiﬁed linear units improve restricted boltzmann machines. proceedings international conference machine learning pages arvind neelakantan jeevan shankar alexandre passos andrew mccallum. efﬁcient nonparametric estimation multiple embeddings word vector space. proceedings emnlp. jeffrey pennington richard socher christopher manning. glove global vectors word representation. proceedings empiricial methods natural language processing long min-yen tat-seng chua. paraphrase recognition dissimilarity signiﬁcance proceedings conferclassiﬁcation. ence empirical methods natural language processing pages association computational linguistics. tomas mikolov martin karaﬁ´at lukas burget cernock`y sanjeev khudanpur. recurrent neural network based language model. interspeech annual conference international speech communication association makuhari chiba japan september pages tomas mikolov ilya sutskever chen greg corrado jeff dean. distributed representations words phrases compositionality. advances neural information processing systems pages vasile philip mccarthy mihai lintean danielle mcnamara arthur graesser. paraphrase identiﬁcation lexico. flairs confersyntactic graph subsumption. ence pages richard socher eric huang jeffrey pennin christopher manning andrew dynamic pooling unfolding recursive autoencoders paraphrase detection. advances neural information processing systems pages richard socher cliff chris manning andrew parsing natural scenes natural language recursive neural networks. proceedings international conference machine learning pages yuheng chen xiaogang wang xiaoou tang. deep learning face representation joint identiﬁcation-veriﬁcation. advances neural information processing systems pages stephen mark dras robert dale c´ecile using dependency-based features paris. protake para-farce paraphrase. ceedings australasian language technology workshop volume wen-tau kristina toutanova john platt christopher meek. learning discriminative projections text similarity measures. proceedings fifteenth conference computational natural language learning pages portland oregon june. association computational linguistics.", "year": 2015}