{"title": "The Stochastic Gradient Descent for the Primal L1-SVM Optimization  Revisited", "tag": ["cs.LG", "cs.AI"], "abstract": "We reconsider the stochastic (sub)gradient approach to the unconstrained primal L1-SVM optimization. We observe that if the learning rate is inversely proportional to the number of steps, i.e., the number of times any training pattern is presented to the algorithm, the update rule may be transformed into the one of the classical perceptron with margin in which the margin threshold increases linearly with the number of steps. Moreover, if we cycle repeatedly through the possibly randomly permuted training set the dual variables defined naturally via the expansion of the weight vector as a linear combination of the patterns on which margin errors were made are shown to obey at the end of each complete cycle automatically the box constraints arising in dual optimization. This renders the dual Lagrangian a running lower bound on the primal objective tending to it at the optimum and makes available an upper bound on the relative accuracy achieved which provides a meaningful stopping criterion. In addition, we propose a mechanism of presenting the same pattern repeatedly to the algorithm which maintains the above properties. Finally, we give experimental evidence that algorithms constructed along these lines exhibit a considerably improved performance.", "text": "abstract. reconsider stochastic gradient approach unconstrained primal l-svm optimization. observe learning rate inversely proportional number steps i.e. number times training pattern presented algorithm update rule transformed classical perceptron margin margin threshold increases linearly number steps. moreover cycle repeatedly possibly randomly permuted training dual variables deﬁned naturally expansion weight vector linear combination patterns margin errors made shown obey complete cycle automatically constraints arising dual optimization. renders dual lagrangian running lower bound primal objective tending optimum makes available upper bound relative accuracy achieved provides meaningful stopping criterion. addition propose mechanism presenting pattern repeatedly algorithm maintains properties. finally give experimental evidence algorithms constructed along lines exhibit considerably improved performance. support vector machines extensively used linear classiﬁers either space patterns originally reside high dimensional feature spaces induced kernels. appear successful addressing classiﬁcation problem expressed minimization objective function involving empirical risk time keeping complexity classiﬁer. measures empirical risk various quantities proposed -norm loss functions widely accepted ones giving rise optimization problems known ll-svms svms typically treat problem constrained quadratic optimization dual space. early stages development efﬁcient implementation hindered quadratic dependence memory requirements number training examples fact rendered prohibitive processing large datasets. idea applying optimization subset training order overcome diﬃculty resulted development decomposition methods although methods improved convergence rates practice superlinear dependence number examples even cubic still lead excessive runtimes dealing massive datasets. recently so-called linear svms taking advantage linear kernels order allow parts written primal notation succeeded outperforming decomposition svms. considerations motivated research alternative algorithms naturally formulated primal space long advent linear svms mostly connection large margin classiﬁcation linearly separable datasets problem directly related l-svm. indeed case -norm loss takes place empirical risk equivalent formulation exists renders dataset linearly separable high dimensional feature space. alternative algorithms references therein) mostly based perceptron simplest online learning algorithm binary linear classiﬁcation characteristic work primal space online manner i.e. processing example time. cycling repeatedly patterns update internal state stored weight vector time appropriate condition satisﬁed. ability process example time algorithms succeed sparing time memory resources consequently become able handle large datasets. since l-svm problem known admit equivalent maximum margin interpretation mapping appropriate space fully primal large margin perceptron-like algorithms appear unable deal task. nevertheless somewhat diﬀerent approach giving rise online algorithms developed focuses minimization regularized -norm soft margin loss stochastic gradient descent notable representatives approach pioneer norma pegasos gives rise kind perceptron-like update important ingredient shrinking current weight vector. shrinking always takes place pattern presented algorithm modiﬁcation suﬀered weight vector loss incurred. thus lack meaningful stopping criterion algorithm without user intervention keeps running forever. sense algorithms question fundamentally diﬀerent mistake-driven large margin perceptron-like classiﬁers terminate ﬁnite number updates. proof even asymptotic convergence output ﬁnal hypothesis exist probabilistic convergence results results terms average hypothesis. present work reconsider straightforward version primal unconstrained l-svm problem assuming learning rate inversely proportional number steps. therefore algorithm regarded margin perceptron unlearning addresses l-svm problem keeping track number updates caused pattern parallel weight vector updated according perceptron-like rule. sense uses dual variables rather considered linear which however possesses ﬁnite time bound achieving predeﬁned relative accuracy. either norma speciﬁc dependence learning rate number steps pegasos projection step update single example contributing gradient observe algorithm transformed classical perceptron margin margin threshold increases linearly number steps. obvious gain observation shrinking weight vector step amounts nothing increase step counter unit instead costly multiplication components generally non-sparse weight vector scalar. another beneﬁt arising simpliﬁed description able demonstrate easily cycle data complete epochs dual variables deﬁned naturally expansion weight vector linear combination patterns margin errors made satisfy automatically constraints dual optimization. important consequence unexpected result relevant dual lagrangian expressed terms total number margin errors number complete epochs length current weight vector provides lower bound primal objective function gives measure progress made optimization process. indeed virtue strong duality theorem dual lagrangian primal objective coincide optimality. therefore assuming convergence optimum upper bound relative accuracy involving dual lagrangian deﬁned oﬀers useful practically achievable stopping criterion. moreover provide evidence favor asymptotic convergence optimum testing experimentally vanishing duality gap. finally aiming performing updates expense costly inner product calculation propose mechanism presenting pattern repeatedly algorithm consistently interesting properties. paper organized follows. section describes algorithm properties. section give implementational details deliver experimental results. finally section contains conclusions. mapping feature space higher dimensionality placing position distance additional dimension i.e. extending construct embedding data so-called augmented space advantage embedding linear hypothesis augmented space becomes homogeneous. following augmentation reﬂection respect origin negatively labeled patterns performed allowing uniform treatment categories patterns. deﬁne regularization parameter controlling complexity classiﬁer given dataset size minimization regularized empirical risk respect equivalent minimization objective function learning rate stands subgradient respect since -norm soft margin loss piecewise diﬀerentiable usually imposed convergence analysis stochastic obtain update otherwise. update classical perceptron algorithm margin which however margin threshold condition increases linearly number presentations patterns algorithm independent whether lead change weight vector thus counts number times pattern presented algorithm corresponds number updates weight vector instead weight vector updated satisﬁed meaning margin error made gives naturally rise online algorithms. therefore choose examples presented algorithm random. however l-svm optimization task batch learning problem better tackled online algorithms classical conversion algorithms batch setting. done cycling repeatedly possibly randomly permuted training dataset using last hypothesis prediction. traditional procedure presenting training data algorithm complete epochs case shortly additional advantage exists lower bound optimal value objective function minimized expressed terms quantities available run. existence lower bound provides estimate relative accuracy achieved algorithm. since time presented algorithm exactly times. then taking account time dual variable consequently dual variable associated complete epochs given fully primal dual variables weight vector linear combination patterns margin errors made obey complete epochs automatically constraints encountered dual optimization. surprising result allows construct dual lagrangian provides lower bound optimal value jopt objective assuming obtain upper bound relative accuracy /jopt achieved algorithm keeps running. thus ﬁrst time primal algorithm relative accuracy stopping criterion. also worth noticing involves total number margin errors require although automatic satisfaction constraints dual variables important means suﬃcient ensure vanishing duality consequently convergence optimal solution. demonstrate convergence optimum relying dual optimization theory must make sure karush-kuhn-tucker conditions satisﬁed. approximate satisfaction demands patterns substantial loss ones dual variables equal least extremely close moreover patterns expect dual variables also satisfy constraints limit patterns presented algorithm selected randomly equal probability since asymptotically selected equal number times. course computationally expensive evaluate epoch exact primal objective. thus approximate calculation loss using value weight vector last time pattern presented algorithm preferable. exploit already computed inner product needed order decide whether condition satisﬁed. approximate calculation gives value relative accuracy larger times stopping criterion proceed proper calculation primal objective. comparison coeﬃcient given empirically value close dual variables values play role non-bound support vectors. dual variable associated k-th pattern equal ctk/t number epochs k-th pattern found margin error. apparent exists number epochs matter large after pattern consistently found margin error dual variable tend asymptotically zero reﬂecting accumulated eﬀect shrinking weight vector suﬀers time pattern presented algorithm. therefore algorithm necessary ingredients asymptotic satisfaction conditions vanishing duality gap. potential danger remains however exist patterns belong categories occasionally either become margin errors although time become classiﬁed suﬃciently large margin despite fact time margin errors. hope time changes weight vector become smaller smaller events become rare leading eventually convergence optimal solution. discussion cannot regarded formal proof asymptotic convergence algorithm. believe however provide convincing argument assuming convergence duality eventually tend zero lower bound primal objective given proposition approach optimal primal objective jopt thereby proving convergence optimum achieved. instead make stronger assumption convergence optimum then course vanishing duality follows strong duality theorem. case stopping criterion exploiting upper bound discussion assumes epoch pattern presented algorithm. however consider option presenting pattern repeatedly times algorithm aiming performing updates expense calculation costly inner product proposition analysis following still valid condition patterns epoch presented exactly number times algorithm. then epoch regarded equivalent usual epochs single presentations patterns algorithm result increase amount equal many consecutive presentations pattern algorithm lead margin error i.e. update remaining presentations necessarily corresponding increase amounts pure shrinking not. time plus-step takes place minus-step takes place at+· thus plus-step adds quantity kykk minus-step quantity clearly consecutive presentations algorithm holds at+ℓ multiple updates introduced discussion context related present work given however proper treatment presence regularization term -norm soft margin loss provided. instead forward-backward splitting approach adopted multiple update absence regularizer followed pure regularizer-induced shrinkings. follows means consecutive minus-steps condition still violated additional minus-step must take place. thus ﬁrst treat subcase max{kykk kykk condition initially satisﬁed still satisﬁed number plus-steps since quantity kykk added plus-step non-positive. thus accordance since )/λ] leading remains kykk consider interval subdivided integer satisfying belonging subinterval condition initially violated still violated minus-steps plus-steps quantity kykk added plus-step non-positive. thus accordance since leads )/λ]+ ℓ−ℓ. subcase kykk case complicated. kykk condition initially satisﬁed still satisﬁed plus-steps since thus consistent kykk kykk] leading remains examined case kykk interval interval expressed union subintervals integer satisfying belong subinterval. also assume pattern presented consecutive times algorithm result plus-steps minus-steps taken place quantity added pκ+κ− κ+−κ−λ satisﬁes pκ+κ− increases either ﬁrst reach value ﬁrst reach value former case pκ+κ−. means condition becomes equal case minus-step must take place. thus steps taking place reached value minus-steps. latter case pκ+κ− means plus-steps becomes equal case plus-step must take place. thus steps taking place reached value plus-steps. cases accordance kykk kykk kykk] implement three types algorithms along lines previous section. ﬁrst plain algorithm random selection examples denoted sgd-r terminates maximum number tmax steps reached. pseudocode given section dual variables case satisfy constraints result relative accuracy cannot used stopping criterion. algorithm relative accuracy pseudocode also given section denoted sgd-s designates epoch pattern presented single time tmax full epochs exhausted. variation algorithm denoted sgdm replaces epoch usual update multiple update multiplicity sgd-s sgd-m comparison coeﬃcient takes value unless otherwise explicitly stated. algorithms performing primal objective expected perform better linear kernels employed. therefore feature space experiments chosen original instance space. consequence algorithms naturally compared linear svms. among choose svmperf ﬁrst cutting-plane algorithm training linear svms optimized cutting plane algorithm svms dual coordinate descent algorithm margin perceptron unlearning also include study pegasos finally brieﬂy considered svmsgd sgd-qn algorithms implemented single precision. datasets used training binary adult datasets compiled platt training physics dataset real-sim news webspam datasets multiclass covertype dataset full reuters dataset. number instances attributes listed table case covertype dataset sources available http//users.auth.gr/costapan source available http//svmlight.joachims.org source available http//cmp.felk.cvut.cz/~xfrancv/ocas/html source available http//www.csie.ntu.edu.tw/~cjlin/liblinear. used source available http//users.auth.gr/costapan source available http//ttic.uchicago.edu/~shai/code source available http//leon.bottou.org/projects/sgd source available https//www.hds.utc.fr/~bordesan/dokuwiki/doku.php?id=ensgdqn http//research.microsoft.com/en-us/projects/svm/ http//osmot.cs.cornell.edu/kddcup/datasets.html http//www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets http//archive.ics.uci.edu/ml/datasets.html http//www.jmlr.org/papers/volume/lewisa/lyrl_rcvv_readme.htm study binary classiﬁcation problem ﬁrst class versus rest consider binary text classiﬁcation tasks ccat classes versus rest. physics covertype datasets rescaled multiplying features experiments conducted intel core processor running windows vista. codes compiled using compiler cygwin. first perform experiment aiming demonstrating algorithms able obtain extremely accurate solutions. speciﬁcally algorithm sgd-s employing single updating attempt diminish subsequent experiments include bias term algorithms order keep number complete epochs possible increase comparison coeﬃcient number epochs required gets stabilized. procedure entail course shortest training time concern experiment. table give values number epochs needed achieve values. multiple slower increase thus sgd-s achieves general relative accuracy closer stopping criterion exhaustion maximum number steps tmax which however given values multiples dataset size ratio tmax/m considered analogous number epochs algorithm sgd-s since equal values quantities indicate identical numbers updates. input parameter sgd-s sgd-m relative accuracy parameter δstop before-run relative accuracy δstop stopping threshold after-run relative accuracy. svmperf input parameter ocas primal objective value .jopt relative tolerance taking default value diﬀerence training time pegasos sgd-r equal values tmax/m attributed diﬀerence implementations. diﬀerence tmax/m sgd-r sgd-s attributed diﬀerent procedure choosing patterns presented algorithm. finally diﬀerence number epochs sgd-s sgd-m reﬂects eﬀect multiple updates. noted runtime sgd-s sgd-m several calculations primal dual objective included required checking satisfaction stopping criterion. sgd-s sgd-m using exhaustion maximum number tmax epochs stopping criterion runtimes would certainly shorter. progressive decrease training time move pegasos sgd-m sgd-r sgd-s additive eﬀect several factors. factors eﬃcient implementation algorithms exploiting change variable given presentation patterns sgd-s sgd-m complete epochs sgd-m multiple updating. overall improvement made sgd-m pegasos quite substantial. certainly statistically faster diﬀerences sgdlarge especially largest datasets. moreover sgd-s sgd-m considerably faster svmperf statistically faster ocas. pegasos failed process covertype dataset numerical problems. tables contain results experiments involving algorithms linear svms although general characteristics resemble ones previous case diﬀerences magniﬁed intensity optimization task. certainly training time linear svms scales much better increases. moreover clearly outperforms ocas datasets. sgd-m still statistically faster svmperf slower ocas. finally pegasos runs often numerical problems. contrast decreases diﬀerences among algorithms alleviated. apparent results reported tables sgd-r sgd-s sgd-m appear statistically faster linear svms. also pegasos outperforms svmperf datasets ocas majority them. seemingly lowering favors algorithms. concluding experimental investigation also consider algorithms svmsgd sgd-qn implemented single precision. fair comparison implemented algorithms sgd-m single precision well. svmsgd sgd-qn perform random permutations dataset rather assume already shuﬄed. thus provided dataset produced sgd-m result ﬁrst random permutation dataset given. fact computational cost random permutation included runtime svmsgd sgd-qn gives algorithms certain advantage becomes crucial tasks requiring short runtimes. reason consider values penalty parameter much smaller table contains results experiments involving algorithms svmsgd sgd-qn sgd-m observe statistically svmsgd slowest fastest. moreover sgd-m statistically outperforms sgd-qn preference sparse multidimentional datasets. case news dataset sgd-qn failed reach required relative accuracy. reexamined classical approach primal unconstrained l-svm optimization task made contributions concerning theoretical practical issues. assuming learning rate inversely proportional number steps simple change variable allowed simplify algorithmic description demonstrate scheme presenting patterns algorithm complete epochs naturally deﬁned dual variables satisfy automatically constraints dual optimization. opened obtaining estimate progress made optimization process enabled adoption meaningful stopping criterion something algorithms lacking. moreover made possible qualitative discussion conditions asymptotically satisﬁed provided weight vector obeyed optimal solution. practical side exploiting simpliﬁed algorithmic description employing mechanism multiple updating succeeded substantially improving performance algorithms. optimization tasks medium intensity algorithms constructed comparable even faster state-of-the-art linear svms.", "year": 2013}