{"title": "Understanding Deep Convolutional Networks", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Deep convolutional networks provide state of the art classifications and regressions results over many high-dimensional problems. We review their architecture, which scatters data with a cascade of linear filter weights and non-linearities. A mathematical framework is introduced to analyze their properties. Computations of invariants involve multiscale contractions, the linearization of hierarchical symmetries, and sparse separations. Applications are discussed.", "text": "deep convolutional networks provide state classiﬁcations regressions results many high-dimensional problems. review architecture scatters data cascade linear ﬁlter weights non-linearities. mathematical framework introduced analyze properties. computations invariants involve multiscale contractions linearization hierarchical symmetries sparse separations. applications discussed. training samples }i≤q data vector high dimension dimension often larger images large size signals. deep convolutional neural networks recently obtained remarkable experimental results give state performances image classiﬁcation thousands complex classes speech recognition bio-medical applications natural language understanding many domains. also studied neuro-physiological models vision multilayer neural networks computational learning architectures propagate input data across sequence linear operators simple non-linearities. properties shallow networks hidden layer well understood decompositions families ridge functions however approaches extend networks layers. deep convolutional neural networks introduced implemented linear convolutions followed non-linearities typically layers. complex programmable machines deﬁned potentially billions ﬁlter weights bring diﬀerent mathematical world. many researchers pointed deep convolution networks computing progressively powerful invariants depth increases relations networks weights non-linearities complex. paper aims clarifying important principles govern properties networks architecture weights diﬀer applications. show computations invariants involve multiscale contractions linearization hierarchical symmetries sparse separations. conceptual basis ﬁrst step towards full mathematical understanding convolutional network properties. high dimension considerable number parameters dimensionality curse. sampling uniformly volume dimension requires number samples grows exponentially applications number training samples rather grows linearly possible approximate samples strong regularity properties allowing ultimately reduce dimension estimation. learning algorithm including deep convolutional networks thus relies underlying assumption regularity. specifying nature regularity core mathematical problem. circumvent curse dimensionality reducing variability dimension without sacriﬁcing ability approximate done deﬁning variable contractive operator reduces range variations still separating diﬀerent values linearization strategy used machine learning reduce dimension linear projector. low-dimensional linear projection separate values function remains constant direction high-dimensional linear space. rarely case linearizes high-dimensional domains remains constant. dimension reduced applying low-dimensional linear projector finding dream kernel learning algorithms explained section deep neural networks conservative. progressively contract space linearize transformations along remains nearly constant preserve separation. directions deﬁned linear operators belong groups local symmetries introduced section understand diﬃculty linearize action high-dimensional groups operators begin groups translations diﬀeomorphisms deform signals. capture essential mathematical properties extended general deep network symmetries section linearize diﬀeomorphisms preserve separability section shows must separate variations diﬀerent scales wavelet transform. implemented multiscale ﬁlter convolutions building blocks deep convolution ﬁltering. general deep network architectures introduced section iterate linear operators ﬁlter linearly combine diﬀerent channels network layer followed contractive non-linearities. understand non-linear contractions interact linear operators section begins simpler networks recombine channels layer. deﬁnes non-linear scattering transform introduced wavelets separation linearization role. resulting contraction linearization separability properties reviewed. shall sparsity important separation. section extends ideas general class deep convolutional networks. channel combinations provide ﬂexibility needed extend translations larger groups local symmetries adapted network structured factorizing groups symmetries case linear operators generalized convolutions. computations ultimately performed ﬁlter weights learned. relation groups symmetries explained. major issue preserve separation margin across classiﬁcation frontiers. deep convolutional networks ability separating network ﬁbers progressively invariant specialized. give rise invariant grandmother type neurons observed deep networks paper studies architectures opposed computational learning network weights outstanding optimization issue supervised learning computes approximation function training samples }i≤q domain high dimensional open subset low-dimensional manifold. regression problem takes values whereas classiﬁcation values class indices. example classiﬁcation classes reduced versus binary classiﬁcations. binary classiﬁcation speciﬁed equal class. approximate signφ minimizes training error study strategies compute change variables linearizes deep convolutional networks operate layer layer linearize progressively depth increases. classiﬁcation regression problems symmetries linearize level sets need directions along vary locally linearize directions order linear space. tempting local data analysis along possible training includes close neighbors vary. groups symmetries come translations diﬀeomorphisms illustrate diﬃculty linearize high dimensional symmetries provide ﬁrst mathematical ground analyze convolution networks architectures. group local symmetries constant local range symmetries preserve since continuous subset consider groups operators transport vectors continuous parameter. called groups group diﬀerential structure. translations diﬀeomorphisms interpolate samples deﬁne respectively time-series images volumetric data. translation group example group. action distance |g|g identity euclidean norm function locally invariant translations suﬃciently small translations change deep convolutional networks compute convolutions assume translations local symmetries dimension group number generators deﬁne group elements products. equal translations powerful symmetries deﬁned variables images. many image classiﬁcation problems also locally invariant small deformations provide much stronger constraints. means locally invariant diﬀeomorphisms transform diﬀerential warping know advance local range diﬀeomorphism symmetries. example classify images hand-written digits certain deformations preserve digit class modify class another digit. shall linearize small diﬀeomorphims space local symmetries linearized global symmetries optimizing linear projectors preserve values thus reduce dimensionality. norm normalization factor often radon-nikodim property proves transforms almost everywhere diﬀerentiable sense gateaux. |g|g small closely approximated bounded linear operator gˆateaux diﬀeomorphism translates points supu∈rn |g|. |∇g| matrix norm jacobian matrix small diﬀeomorphisms correspond supu |∇g| applying diﬀeomorphism transforms points u−g). distance thus multiplied scale factor bounded ∇g∞. distance diﬀeomorphism understand linearize transport shall begin translations diﬀeomorphisms. deep network architectures covariant translations linear operators implemented convolutions. compute invariants translations linearize diﬀeomorphisms need separate scales apply non-linearity. implemented cascade ﬁlters computing wavelet transform pointwise contractive non-linearity. section extends tools general group actions. wavelet transform diﬀeomorphism acts local translation scaling variable aside translations linearize small diﬀeomorphism must linearize scaling action. done separating variations diﬀerent scales wavelets. deﬁne wavelets dilated −jnψk. wavelet transform computes local average scale variations scales wavelet convolutions obtain sparse representation. means mostly zero besides high amplitude coeﬃcients corresponding variations match scale sparsity plays important role non-linear contractions. audio signals sparse representations usually obtained least intermediate frequencies within octave similar half-tone musical notes. done choosing wavelet frequency bandwidth less octave k/kψ images must discriminate image variations along diﬀerent spatial orientation. obtained separating angles πk/k oriented wavelet rotated intermediate rotated wavelets approximated linear interpolations wavelets. figure shows wavelet transform image scales angles subsampled intervals large amplitude coeﬃcients shown white. filter bank wavelet transforms computed fast multiscale cascade ﬁlters core deep network architectures. scale deﬁne low-pass ﬁlter increases averaging scale band-pass ﬁlters compute wavelet phase removal wavelet coeﬃcients oscillate scale translations smaller modiﬁes complex phase wavelet complex sign real. oscillations averaging outputs zero signal. necessary apply non-linearity removes oscillations. modulus computes positive envelop. averaging ψjk) outputs non-zero coeﬃcients locally invariant scale thus satisﬁes metric indeed wavelet coeﬃcients deformed written wavelet coeﬃcients deformed wavelets. small deformations produce small modiﬁcations wavelets localized regular. resulting modiﬁcations wavelet coeﬃcients order diﬀeomorphism metric |g|diﬀ scale separation limitations local multiscale invariants dominated pattern classiﬁcation applications music speech images called mel-spectrum audio sift type feature vectors images. limitations comes loss information produced averaging reduce loss computed short time scales audio signals small image patches pixels. consequence capture large scale structures important classiﬁcation regression problems. build rich local invariants large scale suﬃcient separate scales wavelets must also capture scale interactions. similar issue appears physics characterize interactions complex systems. multiscale separations used reduce parametrization classical many body systems example multipole methods however apply complex interactions quantum systems. interactions across scales small larger structures must taken account. capturing interactions low-dimensional models major challenge. shall deep neural networks scattering transforms provide high order coeﬃcients partly characterize multiscale interactions. deep convolutional networks computational architectures introduced providing remarkable regression classiﬁcation results high dimension describe architectures illustrated figure iterate linear operators including convolutions predeﬁned pointwise non-linearities. convolutional network takes input signal image. internal network layer depth indexed translation variable usually subsampled channel index layer computed applying linear operator followed pointwise non-linearity since classiﬁcation regression functions invariant covariant translations architecture imposes covariant translations. output translated input translated. since linear thus written convolutions operators propagates input signal last layer cascade spatial convolutions deﬁnes translation covariant operators progressively wider supports depth increases. non-linear function square centered whose width depend upon width spatial scale layer equal ﬁlters wjkj width convolutions subsampled neural networks include many side tricks. sometimes normalize amplitude dividing norm coeﬃcients neighborhood eliminates multiplicative amplitude variabilities. instead subsampling regular grid pooling select largest coeﬃcients sampling cell. coeﬃcients also modiﬁed subtracting constant adapted coeﬃcient. applying rectiﬁer constant acts soft threshold increases sparsity. usually observed inside network coeﬃcients sparse activation. deep network output provided classiﬁer usually composed fully connected neural network layers supervised deep learning algorithms optimize ﬁlter values wjkj order minimize average classiﬁcation regression error training samples }i≤q. variables network ﬁlter update done back-propagation algorithm computed stochastic gradient descent regularization procedures dropout. high-dimensional optimization non-convex despite presence many local minima regularized stochastic gradient descent converges local minimum providing good accuracy test examples rectiﬁer non-linearity usually preferred resulting optimization better convergence. however requires large number training examples. several hundreds examples class usually needed reach good performance. instabilities observed network architectures additions small perturbations produce large variations happens norms matrices larger hence ampliﬁed cascaded. however deep network also strong form stability illustrated transfer learning deep network layer optimized particular training databasis approximate diﬀerent classiﬁcation functions ﬁnal classiﬁcation layers trained databasis. means learned stable structures transferred across similar learning problems. deep network alternates linear operators contractive non-linearities analyze properties cascade begin simpler architecture combine multiple convolutions across channels layer. show network coeﬃcients obtained convolutions reduced number equivalent wavelet ﬁlters. deﬁnes scattering transform whose contraction linearization properties reviewed. variance reduction loss information studied reconstructions stationary processes. rectiﬁer modulus thus remove non-linearity output averaging ﬁlter wjh. indeed averaging ﬁlter applied positive coeﬃcients thus computes positive coeﬃcients aﬀected contrary operator wavelet scattering transform introduced changing network ﬁlters modiﬁes equivalent band-pass ﬁlters ψjk. fast wavelet transform algorithm rotation dilated ﬁlter dilation rotation single mother wavelet scattering order order coeﬃcients ψjk) wavelet coeﬃcient computed loss information averaging compensated higher order coeﬃcient. ψjk) complementary invariants. measure interactions variations scale within distance along orientation frequency bands deﬁned scale interaction coeﬃcients missing ﬁrst order coeﬃcients. strongly contracting order coeﬃcients amplitude decrease quickly increases images audio signals energy scattering coeﬃcients becomes negligible emphasize diﬀeomorphism continuity section explains wavelet transform deﬁnes representations lipschitz continuous actions diﬀeomorphisms. scattering coeﬃcients order computed applying wavelet transforms. prove thus deﬁnes representation lipschitz continuous action diﬀeomorphisms. exists contractive pointwise operator rectiﬁers max. relies commutation properties wavelet transforms diﬀeomorphisms. shows action small diﬀeomorphisms linearized scattering coeﬃcients. classiﬁcation scattering vectors restricted coeﬃcients order amplitude negligible beyond. translation scattering well adapted classiﬁcation problems main source intra-class variability translations small deformations ergodic stationary processes. example intra-class variabilities hand-written digit images essentially translations deformations. mnist digit data basis applying linear classiﬁer scattering coeﬃcients gives state classiﬁcation errors. music speech classiﬁcation short time intervals modeled ergodic stationary processes. good music speech classiﬁcation results obtained scattering transform image texture classiﬁcation also problems intra class variability modeled ergodic stationary processes. scattering transforms give state results wide range image texture databases compared descriptors including power spectrum moments. softwares retrieved www.di.ens.fr/data/software. stationary processes analyze information loss study reconstruction scattering coeﬃcients stochastic framework stationary process. raise variance separation issues sparsity plays role. also demonstrates importance second order scale interaction terms capture non-gaussian geometric properties ergodic stationary processes. consider scattering coeﬃcients order stationary process ψjk)... ψjmkm remains stationary convolutions pointwise operators preserve stationarity. spatial averaging provides non-biased estimator expected value scattering moment operators average ergodic stationary process progressively larger scales. prove scattering moments characterize complex multiscale properties fractals multifractal processes brownian motions levi processes mandelbrot cascades figure first original images. second realization gaussian process second covariance moments. third reconstructions ﬁrst second order scattering coeﬃcients. figure shows several examples images pixels. ﬁrst three images realizations ergodic stationary textures. second gives realizations stationary gaussian processes second order covariance moments textures. third column shows vorticity ﬁeld two-dimensional turbulent ﬂuid. gaussian realization thus kolmogorov type model restore ﬁlament geometry. third gives reconstructions scattering coeﬃcients limited order scattering vector computed maximum scale wavelets diﬀerent orientations. thus completely invariant translations. dimension scattering moments restore better texture geometries gaussian models obtained covariance moments. geometry mostly captured second order scattering coeﬃcients providing scale interaction terms. indeed ﬁrst order scattering moments reconstruct images similar realizations gaussian processes. first second order scattering moments also provide good models ergodic audio textures fourth image sparse wavelet coeﬃcients. case image nearly perfectly restored scattering coeﬃcients random translation. reconstruction centered comparison. section explains wavelet coeﬃcients sparse rectiﬁer absolute value contractions inverting scattering transform non-linear inverse problem requires recover lost phase information. sparsity important role phase recovery problems translating randomly last motorcycle image deﬁnes non-ergodic stationary process whose wavelet coeﬃcients sparse. result reconstruction random initialization diﬀerent preserve patterns important classiﬁcation tasks. surprising since much less scattering coeﬃcients image pixels. reduce number scattering coeﬃcients reaches number pixels reconstruction good quality little variance reduction. translation ergodic. applying wavelet ﬁlters destroy important structures sparse wavelets. next section addresses issues. impressive texture synthesis results obtained deep convolutional networks trained image data bases much output coeﬃcients. numerical reconstructions also show also recover complex patterns birds airplanes cars dogs ships network trained recognize corresponding image classes. network keeps form memory important classiﬁcation patterns. scattering transforms translation group restricted deep convolutional network architectures suﬀer variance issues loss information. shall explain channel combinations provide ﬂexibility needed avoid limitations. analyze general class convolutional network architectures extending tools previously introduced. contractions invariants translations replaced contractions along groups local symmetries adapted deﬁned parallel transports network layer. network structured factorizing groups symmetries depth increases. implies linear operators written generalized convolutions across multiple channels. preserve classiﬁcation margin wavelets must also replaced adapted ﬁlter weights separate discriminative patterns multiple network ﬁbers. separation margin network layers ρwjxj− computed operators contract separate components shall also needs prepare next transformation consecutive operators strongly dependant. contractive linear operator reduce space volume avoid instabilities cascading operators layer must separate write function fj−. simplify explanations concentrate classiﬁcation separation margin condition operator computes linear projection preserves margin condition resulting dimension reduction limited. contract space non-linearly preserve margin must reduce distances along non-linear displacements transform parallel transport displacements preserve classes deﬁned local symmetries transformations fj−. deﬁne local invariant group transformations must process orbit {¯g.xj−}g∈g. however applied non-linear transformations ¯g.xj− xj−. idea deep network proceed steps. write first computes approximate mapping orbit {¯g.xj−}¯g∈g parallel transport. parallel transport deﬁned operators acting write figure multiscale hierarchical networks computes convolutions along ﬁbers parallel transport. deﬁned group symmetries acting index layer filter weights transported along ﬁbers. index space called gj-principal ﬁber bundle diﬀerential geometry illustrated figure orbits ﬁbers indexed equivalence classes pj/gj. globally invariant action play important role separate ﬁber indexing continuous group sampled along intervals values interpolated between. translation case sampling intervals depend upon local invariance increases translations. used joint scattering transforms proposed unsupervised convolution network learning proposition proves hierarchical embedding implies convolution gj−. proposition group embedding implies indexed exists wjh.b cpj− ¯g.xj ρxj− wj¯g.v. write wj¯g.b pj/gj. decomposed g.xj ρg.xj− wjb). g.xj−) xj−) change variable wjg.b) wjb). hence wj¯g.b) wj.b h.wjh.b). inserting ﬁlter expression proves remains constant locally transported along give several examples groups ﬁlters wjh.b. however learning algorithms compute ﬁlters directly prior knowledge group ﬁlters wjh.b optimized variations along captures large variance within class. indeed variance reduced next ρwj+. generators interpreted principal symmetry generators analogy principal directions pca. generalized scattering scattering convolution along translations replaced convolution along combines diﬀerent layer channels. results translations essentially extended general case. wjh.b averaging ﬁlter computes positive coeﬃcients non-linearity removed. ﬁlter wjh.b support single ﬁber indexed figure translation case need linearize small deformations include much local symmetries low-dimensional group gj−. small diﬀeomorphism non-parallel transport along ﬁbers perturbation parallel transport. modiﬁes distances pairs points scaling factors. linearize diﬀeomorphisms must localized ﬁlters whose supports diﬀerent scales. scale parameters typically diﬀerent along diﬀerent generators hj−. filters constructed wavelets dilated diﬀerent scales along generators group linear dimension reduction mostly rigid movements small local symmetry groups associated linear non-linear physical phenomena rotations scaling colored illuminations pitch frequency shifts. group rotations. rigid movements non-commutative group often includes local symmetries. images group becomes transport rotates wavelet ﬁlter ﬁlters often observed ﬁrst layer deep convolutional networks action parallel transport deﬁned r×so small diﬀeomorphisms correspond deformations along translations rotations sources local symmetries. rototranslation scattering linearizes wavelet ﬁlters along translations rotations roto-translation scattering eﬃciently regress physical functionals often invariant rigid movements lipschitz continuous deformations. example quantum molecular energies well estimated sparse regressions scattering representations audio pitch pitch frequency shift complex example non-linear symmetry audio signals. diﬀerent musical notes instrument pitch shift. harmonic frequencies multiplied factor dilation note duration changed. narrow band-pass ﬁlters pitch shift approximatively mapped translation along modiﬁcation along time action thus two-dimensional translation pitch shift also comes deformations along time log-frequencies deﬁne much larger class symmetries diﬀ. two-dimensional wavelets along linearize small time log-frequency deformations. deﬁne joint time-frequency scattering applied speech music classiﬁcations transformations ﬁrst proposed neurophysiological models audition manifolds patterns group associated complex transformations increases. needs capture large transformations diﬀerent patterns class example chairs diﬀerent styles. consider training samples {xi}i class. iterated network contractions transform vectors j−}i much closer. distances deﬁne weighted graphs sample underlying continuous manifolds space. manifolds clearly appear high-level patterns chairs cars together poses colors. opposed manifold learning deep network ﬁlters result global optimization computed high dimension. principal symmetry generators associated common transformations manifolds examples preserve class capturing large intra-class variance. approximatively mapped parallel transport ﬁlters wjh.b. diﬀeomorphisms non-parallel transports corresponding high-dimensional displacements manifolds xj−. linearizing equivalent partially ﬂatten simultaneously manifolds explain manifolds progressively regular network depth increases involves open mathematical questions. sparse support vectors concentrated reduction data variability contractions. explain classiﬁcation margin preserved thanks existence multiple ﬁbers adapting ﬁlters instead using standard wavelets. ﬁbers indexed separation instruments increase dimensionality avoid reducing classiﬁcation margin. prevent collapsing vectors diﬀerent classes distance close minimum margin vectors close classiﬁcation frontiers. called multiscale support vectors analogy support vector machines. avoid contracting distance separated along diﬀerent ﬁbers indexed separation achieved ﬁlters wjh.b transform sparse supports diﬀerent ﬁbers next contraction ρwj+ reduces distances along ﬁbers indexed across preserves distances. ﬁbers become specialized invariants grandmother neurons observed deep layers convolutional networks strong response particular patterns invariant large class transformations. model choice ﬁlters wjh.b adapted produce sparse representations multiscale support vectors. provide sparse distributed code deﬁning invariant pattern memorisation. memorisation numerically observed deep network reconstructions restore complex patterns within class. emphasize groups ﬁbers mathematical ghost behind ﬁlters never computed. learning optimization directly performed ﬁlters carry trade-oﬀ contractions reduce data variability separation preserve classiﬁcation margin. paper provides mathematical framework analyze contraction separation properties deep convolutional networks. model network ﬁlters guiding non-linear contractions reduce data variability directions local symmetries. classiﬁcation margin controlled sparse separations along network ﬁbers. network ﬁbers combine invariances along groups symmetries distributed pattern representations could suﬃciently stable explain transfer learning deep networks however framework. need complexity measures approximation theorems spaces high-dimensional functions guaranteed convergence ﬁlter optimization fully understand besides learning striking similarities multiscale mathematical tools treatment symmetries particle statistical physics expect rich cross fertilization high-dimensional learning physics development common mathematical language. hinton dahl mohamed jaitly senior vanhoucke nguyen sainath kingsbury deep neural networks acoustic modeling speech recognition ieee signal processing magazine", "year": 2016}