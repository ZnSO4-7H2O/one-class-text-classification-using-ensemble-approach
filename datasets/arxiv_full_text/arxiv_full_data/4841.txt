{"title": "Learning Multiagent Communication with Backpropagation", "tag": ["cs.LG", "cs.AI"], "abstract": "Many tasks in AI require the collaboration of multiple agents. Typically, the communication protocol between agents is manually specified and not altered during training. In this paper we explore a simple neural model, called CommNet, that uses continuous communication for fully cooperative tasks. The model consists of multiple agents and the communication between them is learned alongside their policy. We apply this model to a diverse set of tasks, demonstrating the ability of the agents to learn to communicate amongst themselves, yielding improved performance over non-communicative agents and baselines. In some cases, it is possible to interpret the language devised by the agents, revealing simple but effective strategies for solving the task at hand.", "text": "many tasks require collaboration multiple agents. typically communication protocol agents manually speciﬁed altered training. paper explore simple neural model called commnet uses continuous communication fully cooperative tasks. model consists multiple agents communication learned alongside policy. apply model diverse tasks demonstrating ability agents learn communicate amongst themselves yielding improved performance non-communicative agents baselines. cases possible interpret language devised agents revealing simple effective strategies solving task hand. communication fundamental aspect intelligence enabling agents behave group rather collection individuals. vital performing complex tasks real-world environments actor limited capabilities and/or visibility world. practical examples include elevator control sensor networks communication also important success robot soccer partially observed environment communication agents vital coordinate behavior individual. model controlling agent typically learned reinforcement learning speciﬁcation format communication usually pre-determined. example robot soccer bots designed communicate time step position proximity ball. work propose model cooperating agents learn communicate amongst taking actions. agent controlled deep feed-forward network additionally access communication channel carrying continuous vector. channel receive summed transmissions agents. however agent transmits channel speciﬁed a-priori learned instead. communication continuous model trained back-propagation thus combined standard single agent algorithms supervised learning. model simple versatile. allows applied wide range problems involving partial visibility environment agents learn task-speciﬁc communication aids performance. addition model allows dynamic variation time number type agents important applications communication moving cars. consider setting agents cooperating maximize reward environment. make simplifying assumption full cooperation agents thus agent receives independent contribution. setting difference agent controller viewing pieces larger model controlling agents. taking latter perspective controller large feed-forward neural network maps inputs agents actions agent occupying subset units. speciﬁc connectivity structure layers instantiates broadcast communication channel agents propagates agent state. explore model range tasks. some supervision provided action others given sporadically. former case controller agent trained backpropagating error signal connectivity structure model enabling agents learn communicate amongst maximize objective. latter case reinforcement learning must used additional outer loop provide training signal time step communication model describe model used compute distribution actions given time agent’s view state environment. input controller concatenation state-views controller mapping output concatenation discrete actions agent. note single controller encompasses individual controllers agents well communication agents. controller structure detail architecture built modules take form multilayer neural networks. number communication steps network. takes input vectors agent hidden state communication outputs vector main body model takes input concatenated vectors rescales communication vector number communicating agents. note also permutation invariant thus order agents matter. used. takes input state-view ﬁrst layer model encoder function form encoder problem dependent outputs feature vector tasks single layer neural network. unless otherwise noted output model decoder function used output distribution space actions. takes form single layer network followed softmax. produce discrete action sample distribution call communication neural takes state-view agents passes encoder iterates equations obtain samples actions agents according model extensions local connectivity alternative broadcast framework described allow agents communicate others within certain range. agents present within figure overview commnet model. left view module single agent note parameters shared across agents. middle single communication step agents modules propagate internal state well broadcasting communication vector common channel right full model showing input states agent communication steps output actions agent. agents move enter exit environment change time. setting model natural interpretation dynamic graph vertices connected vertex current time. edges within graph represent communication channel agents equivalent belief propagation furthermore multi-layer nets vertex makes model similar instantiation ggsnn work skip connections tasks useful input encoding communication steps beyond ﬁrst layer. thus agent step have temporal recurrence also explore network recurrent neural network achieved simply replacing communication step eqn. time step using module every time step actions sampled note agents leave join swarm time step. single layer network obtain plain rnns communicate other. later experiments also lstm module. related work model combines deep network reinforcement learning several recent works applied methods multi-agent domains atari games assume full visibility environment lack communication. rich literature multi-agent reinforcement learning particularly robotics domain amongst fully cooperative algorithms many approaches avoid need communication making strong assumptions visibility agents environment. others communication pre-determined protocol notable approaches involve learning communicate agents partial visibility kasai varshavskaya distributed tabular-rl approaches simulated tasks. giles evolutionary algorithm rather reinforcement learning. guestrin single large control collection agents factored message passing framework messages learned. contrast approaches model uses deep network agent control communication. marl perspective closest approach concurrent work foerster also uses deep reinforcement learning multi-agent partially observable tasks speciﬁcally riddle problems necessitate multi-agent communication. like approach communication learned rather pre-determined. however agents communicate discrete manner actions. contrasts model multiple continuous communication cycles used time step decide actions agents. furthermore approach amenable dynamic variation number agents. neural similarities model differs ordering input assumed employs convolution opposed global pooling approach model regarded instantiation construction scarselli expanded particular output model ﬁxed point iterating equations convergence using recurrent models. recurrence equations unrolled ﬁxed number steps model trained backprop time. work require model recurrent neither reach steady state. additionally regard eqn. pooling operation conceptually making model single feed-forward network local connections. experiments baselines describe three baselines models compare model. independent controller simple baseline agents controlled independently without communication them. write per-agent controller applied independently. advantages communication-free model modularity ﬂexibility. thus deal well agents joining leaving group able coordinate agents’ actions. fully-connected another obvious choice make fully-connected multi-layer neural network takes concatenation input outputs actions using multiple output softmax heads. equivalent allowing arbitrary matrix ﬁxed size. model would allow agents communicate share views environment. unlike model however modular inﬂexible respect composition number agents controls even order agents must ﬁxed. discrete communication alternate agents communicate discrete symbols meaning symbols learned training. since contains discrete operations differentiable reinforcement learning used train setting. however unlike actions environment agent output discrete symbol every communication step. viewed internal time steps agent communication output treated action agent given time step directly employ policy gradient communication step agent output index sampled according simple demonstration lever pulling task start simple game requires agents communicate order win. consists levers pool agents. round agents drawn random total pool agents must choose lever pull simultaneously agents round ends. goal pull different lever. correspondingly agents receive reward proportional number distinct levers pulled. agent identity nothing else thus implement game commnet communication steps skip connections encoder lookup-table entries layer neural relu non-linearities takes concatenation outputs vector. decoder linear layer plus softmax producing distribution levers sample determine lever pulled. compare independent controller architecture model except communication zeroed. results shown table metric number distinct levers pulled divided averaged trials seeing batches size training. explore reinforcement direct supervision cases commnet performs signiﬁcantly better independent controller. appendix analysis trained model. table results lever game commnet independent controller models using different training approaches. allowing agents communicate enables succeed task. multi-turn games section consider multi-agent tasks using mazebase environment reward training signal. ﬁrst task control cars passing trafﬁc junction maximize minimizing collisions. second task control multiple agents combat enemy bots. experimented several module types. feedforward module single layer network communication steps used. module also used single layer network shared parameters across time steps. finally used lstm modules hidden layer size modules skip-connections. tasks trained epochs epoch weight updates rmsprop mini-batch game episodes total models experience episodes training. repeat experiments times different random initializations report mean value along standard deviation. training time varies hours days depending task module type. trafﬁc junction consists -way junction grid shown fig. time step cars enter grid probability parrive four directions. however total number cars given time limited nmax occupies single cell given time randomly assigned three possible routes every time step possible actions advances cell route brake stay current location. removed reaches destination edge grid. cars collide locations overlap. collision incurs reward rcoll affect simulation way. discourage trafﬁc gets reward rtime every time step number time steps passed since arrived. therefore total reward time number collisions occurring time number cars present. simulation terminated steps classiﬁed failure collisions occurred. represented one-hot binary vector encodes unique current location assigned route number respectively. agent controlling observe cars vision range communicate cars. figure left trafﬁc junction task agent-controlled cars pass junction without colliding. middle combat task model controlled agents ﬁght enemy bots tasks agent limited visibility thus able location agents. right visibility environment decreases importance communication grows trafﬁc junction task. state vector agent thus concatenation vectors dimension |r|. table show probability failure variety different model module pairs. compared baseline models commnet signiﬁcantly reduces failure rate module types achieving best performance lstm module also explored partial visibility within environment effects advantage given communication. vision range agent decreases advantage communication increases shown fig. impressively zero visibility commnet model still able succeed time. table shows results easy hard versions game. easy version junction one-way roads harder version consists four connected junctions two-way roads. details game variations found appendix discrete communication works well easy version commnet local connectivity gives best performance hard case. analysis communication attempt understand agents communicate performing junction task. start recording hidden state agent corresponding communication vectors ˜ci+ fig. fig. show projections communication hidden state vectors respectively. plots show diverse range hidden states clustered communication vectors many close zero. suggests hidden state carries information agent often prefers communicate others unless necessary. possible consequence broadcast channel everyone talks time no-one understand. appendix norm communication vectors brake locations. table trafﬁc junction task. left failure rates different types model module function commnet consistently improves performance baseline models. right game variants. easy case discrete communication help still less commnet. hard version local communication least well broadcasting agents. figure left first principal components communication vectors multiple runs trafﬁc junction task fig. majority silent distinct clusters also present. middle three clusters probe model understand meaning right first principal components hidden state vectors runs left corresponding color coding. note many silent communication vectors accompany non-zero hidden state vectors. shows pathways carry different information. better understand meaning behind communication vectors simulation cars recorded communication vectors locations whenever braked. vectors belonging clusters fig. consistently emitted cars speciﬁc location shown colored circles fig. also strongly correlated braking locations indicated happen relevant avoiding collision. combat task simulate simple battle involving opposing teams grid shown fig. team consists agents initial positions sampled uniformly square around team center picked uniformly grid. time step agent perform following actions move cell four directions; attack another agent specifying nothing. agent attacks agent health point reduced inside ﬁring range agents need time step cooling attack cannot attack. agents start health points health reaches team agents team die. simulation ends team wins neither teams within time steps model controls team training team consist bots follow hardcoded policy. policy attack nearest enemy agent within ﬁring range. approaches nearest visible enemy agent within visual range. agent visible bots inside visual range individual bot. shared vision gives advantage team. input model agent represented one-hot binary vectors encoding unique team location health points cooldown. model controlling agent also sees agents visual range model gets reward team loses draws game. addition also reward times total health points enemy team encourages attack enemy bots. table rates combat task different communication approaches module choices. continuous consistently outperforms approaches. fully-connected baseline worse independent model without communication. right explore effect varying number agents agent visibility. even agents team communication clearly helps. table shows rate different module choices various types model. among different modules lstm achieved best performance. continuous communication commnet improved module types. relative independent controller fully-connected model degraded performance discrete communication improved lstm module type. also explored several variations task varying number agents team setting increasing visual range agents area. result tasks shown right side table using commnet model consistently improves rate even greater environment observability vision case. babi tasks apply model babi dataset consists tasks requiring different kind reasoning. goal answer question reading short story. formulate multi-agent task giving sentence story agent. communication among agents allows exchange useful information necessary answer question. input j’th sentence story question sentence. encoder representation convert vectors. module consists two-layer relu non-linearities. communication steps ﬁnal hidden states together pass softmax decoder layer sample output word model trained supervised fashion using cross-entropy loss correct answer hidden layer size weights initialized train model epochs learning rate mini-batch size adam optimizer used training data validation optimal hyper-parameters model. results version babi task shown table along baselines model outperforms lstm baseline worse memnn model speciﬁcally designed solve reasoning long stories. however successfully solves tasks including ones require information sharing agents communication. discussion future work introduced commnet simple controller marl able learn continuous communication dynamically changing agents. evaluations four diverse tasks clearly show model outperforms models without communication fully-connected models models using discrete communication. despite simplicity broadcast channel examination trafﬁc task reveals model learned sparse communication protocol conveys meaningful information agents. code model found http//cims.nyu.edu/~sainbar/commnet/. aspect model fully exploit ability handle heterogenous agent types hope explore future work. furthermore believe model scale gracefully large numbers agents perhaps requiring sophisticated connectivity structures; also leave future work. acknowledgements authors wish thank daniel y-lan boureau advice guidance. fergus grateful support cifar. references bengio louradour collobert weston. curriculum learning. icml busoniu babuska schutter. comprehensive survey multiagent reinforcement learning. tarlow brockschmidt zemel. gated graph sequence neural networks. iclr littman. value-function reinforcement learning markov games. cognitive systems research mnih kavukcuoglu silver rusu veness bellemare graves riedmiller fidjeland ostrovski petersen beattie sadik wierstra legg hassabis. human-level control deep reinforcement learning. nature pearl. reverend bayes inference engines distributed hierarchical approach. aaai peng wong. towards neural network-based reasoning. arxiv preprint silver huang maddison guez sifre driessche schrittwieser antonoglou panneershelvam lanctot mastering game deep neural networks tree search. nature sukhbaatar szlam weston fergus. end-to-end memory networks. nips sutton barto. introduction reinforcement learning. press tampuu matiisen kodelja kuzovkin korjus vicente. multiagent cooperation tan. multi-agent reinforcement learning independent cooperative agents. icml tieleman hinton. lecture .—rmsprop divide gradient running average recent policy gradient state speciﬁc baseline delivering gradient model. denote states episode actions taken states length episode. baseline scalar function states computed extra head model producing action probabilities. beside maximizing expected reward policy gradient models also trained minimize distance baseline value actual reward. thus ﬁnishing episode update model parameters analyze commnet model trained supervision lever pulling task. supervision uses sorted ordering agent assign target actions. agent concatenate hidden layer activations game playing. fig. shows plot vectors color intensity represents agent’s smooth ordering suggests agents communicating enabling solve task. curriculum learning make training easier. ﬁrst epochs training parrive linearly increased next epochs. finally training continues another epochs. learning rate ﬁxed throughout. also implemented additional easy hard versions game latter shown fig.. easy version junction one-way roads grid. arrival points possible routes. curriculum increase ntotal parrive harder version consists four connected junctions two-way roads shown fig. arrival points different routes arrival point. ntotal increased parrive curriculum. visualize average norm communication vectors fig. brake locations spatial grid fig. four incoming directions location communication signal stronger. brake pattern shows cars coming left never yield directions. here simple position encoding convert sentences ﬁxed size vectors. also initial communication used broadcast question agents. since temporal ordering sentences relevant tasks special temporal word module layer network skip connection since correct answer training supervised learning using cross entropy cost hidden layer size weights initialized train model epochs learning rate mini-batch size adam optimizer used training data validation optimal hyper-parameters model.", "year": 2016}