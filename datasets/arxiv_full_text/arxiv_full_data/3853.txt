{"title": "Training RNNs as Fast as CNNs", "tag": ["cs.CL", "cs.NE"], "abstract": "Common recurrent neural network architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU) architecture, a recurrent unit that simplifies the computation and exposes more parallelism. In SRU, the majority of computation for each step is independent of the recurrence and can be easily parallelized. SRU is as fast as a convolutional layer and 5-10x faster than an optimized LSTM implementation. We study SRUs on a wide range of applications, including classification, question answering, language modeling, translation and speech recognition. Our experiments demonstrate the effectiveness of SRU and the trade-off it enables between speed and performance. We open source our implementation in PyTorch and CNTK.", "text": "common recurrent neural network architectures scale poorly intrinsic difﬁculty parallelizing state computations. work propose simple recurrent unit architecture recurrent unit simpliﬁes computation exposes parallelism. majority computation step independent recurrence easily parallelized. fast convolutional layer faster optimized lstm implementation. study srus wide range applications including classiﬁcation question answering language modeling translation speech recognition. experiments demonstrate effectiveness tradeenables speed performance. open source implementation pytorch cntk. recurrent neural networks core state-of-the-art approaches large number natural language tasks including machine translation language modeling opinion mining situated language understanding question answering many advancements architectures increasing capacity. however networks difﬁcult scale. learning sequential dependencies central recurrent architectures limit parallelization potential. results slow development makes rigorous parameter tuning intractable. similar problems occur deployment slow inference creates challenges real-time systems scale. paper describe simple recurrent unit recurrent architecture balances serial parallelized computation. evaluate show speed gains provides generalize across core tasks maintaining even improving overall performance common architectures. recurrent networks process sequences symbols symbol time. commonly used architectures including long short-term memory gated recurrent units computation step depends completing previous step. result contrast operations convolution attention recurrent computations less amenable parallelization. propose majority computation step without depending completing previous computations allows easily parallelize result computation combined fast recurrent structure. figure illustrates difference approaches. even naive implementation approach leads improvements performance advantage enabling optimization particularly ﬁtting existing hardware architectures. removing dependencies time steps expensive operations allows parallelize across different dimensions time steps. also perform cuda-level optimization compiling figure illustration difference common architectures approach common architectures entire computation step depends completing previous step. impedes parallelization steps. contrast propose process input step independently inputs recurrent combination relatively lightweight computation majority computation easily parallelized. figure average processing time batch samples using cudnn lstm word-level convolution convd proposed sru. number tokens sequence feature dimension feature width. section details setup used. experiment diverse core problems evaluate architecture including text classiﬁcation question answering language modeling machine translation speech recognition. approach competitive even outperforms common recurrent convolutional architectures delivering signiﬁcant speedups. also study relation speed performance show provides ﬁne-grained control tradeoff two. recurrent architectures including lstm gating control information alleviate vanishing exploding gradient problems. deﬁne gate composed single-layer feed-forward network sigmoid activation.the gate output used point-wise multiplication operation combine inputs example current previous time stamps. computation feed-forward network especially matrix multiplication expensive operation process point-wise multiplication relatively lightweight. design decision making gate computation dependent current input recurrence. leaves point-wise multiplication computation dependent previous steps. matrix multiplications involved feed-forward network easily parallelized. basic form includes single forget gate. given input time compute linear transformation forget gate complete architecture also includes skip connections shown improve training deep networks large number layers highway connections reset gate computed similar forget gate reset gate used compute output state combination internal state input complete architecture existing architectures previous output state recurrence computation. example lstm forget gate vector computed including rht− breaks independence parallelization dimension hidden state depends computation wait fully computed. similar design choices present variants used throughout computation. propose completely drop connection gating computations step states step ct−. given sequence input vectors {x··· different independent computed parallel. computation bottleneck architecture three matrix multiplications equations computing equations operations element-wise fast compute. optimizing similar lstm optimized cudnn lstm formulation permits optimizations. first matrix multiplications across time steps batched signiﬁcantly improves computation intensity utilization. grouping matrix multiplications equations single batch formulated sequence length rn×d resulting matrix hidden state size. input mini-batch sequences would tensor size second element-wise operations sequence fused kernel function parallelized across dimensionality hidden state without fusion operations addition sigmoid activation would invoke separate function call incur additional kernel launching latency data moving costs. algorithm shows pseudocode fused kernel function. implementation bidirectional similar matrix multiplications directions batched fused kernel created handle parallelize directions. improving common architectures sequence processing recently received signiﬁcant attention approach closely related recent work recurrent convolutions kernel networks quasirnn rcnn quasi-rnn incorporate word-level convolutions recurrent unit sequential gated pooling. generalizes rcnn provides theoretical view linking model class sequence kernels. viewed simpliﬁed version special case rcnn quasi-rnn window size highway connections srivastava added facilitate increased network depth. discuss relation quasi-rnn detail evaluate effect differences appendix algorithm mini-batch version forward pass deﬁned equations indices sequence length mini-batch size hidden state dimension input input sequences batch grouped matrix multiplication result bias terms output output internal states. various strategies proposed scale network training speciﬁcally speed recurrent networks cuda-level optimization inspired cudnn lstm cudnn lstm requires optimization steps requires optimizations produce signiﬁcant speed-up. convolution-based quasi-rnn architecture uses similar cuda-level optimizations convd operation batched matrix multiplications. topic improving learning times also studied. example goyal addressed stability issues distributed training large mini-batches improve training time. approach combined training procedures. design simple recurrent architectures related architectures raises questions representational power. balduzzi ghifary applies type-preserving transformations discuss capacity various simpliﬁed architectures. recent work demonstrated connection neural networks kernels particular shows broad model class including word-level seen embedding sequence similarity functions string kernels hidden space. layer stacking interpreted using higher-order sequence similarities introduces non-linearity representational power. empirically show achieve compelling results stacking multiple layers. evaluate text classiﬁcation question answering language modeling machine translation speech recognition tasks. tasks provides broad coverage application computation challenges. training time benchmarks ranges minutes days unless noted otherwise timing experiments performed pytorch desktop machine single nvidia geforce intel core processor cuda cudnn variational dropout addition standard dropout regularization. tanh experiments unless speciﬁed otherwise. main question study performance-speed trade-off provides comparison recurrent architectures. stack multiple layers directly substitute recurrent convolutional modules. minimize hyper-parameter tuning architecture engineering fair comparison. efforts non-trivial impact results beyond scope experiments. much possible model conﬁgurations identical prior work. dataset classiﬁcation tasks movie review sentiment subjectivity customer reviews polarity trec question type mpqa opinion polarity stanford sentiment treebank following wordvec embeddings trained billion google news tokens. word embeddings normalized unit vectors ﬁxed training. setup train encoders last output state predict class label given sentence. two-layer encoder hidden dimensions. provides data four-layer rnn. also compare model ﬁlter windows original work. adam default learning rate weight decay. train epochs perform -fold cross validation standard split speciﬁed. result averaged independent trials. tune dropout probability among report best results. table exact match scores various models squad also report total processing time epoch time spent computations. outperforms lstm models times faster cudnn lstm. also list state-of-theart test results metrics listed leaderboard december state-of-the-art methods rnns potentially beneﬁt approach. results table presents test results benchmarks. model consistently outperforms models across datasets. figure shows validation performance relative training time cudnn lstm model. implementation signiﬁcantly faster cudnn lstm. example movie review task model completes training epochs within seconds cudnn lstm takes seconds. dataset stanford question answering dataset squad largest machine comprehension datasets includes questionanswer pairs extracted wikipedia articles. standard train development sets. setup experiment document reader model compare variants lstm original setup sru. open source re-implementation. minor differences version performs worse compared reported results using training options. following author suggestions learning rate instead adamax optimizer separately tuned dropout rates word embeddings. gives results comparable original paper. models trained epochs batch size ﬁxed learning rate hidden dimensionality dropout input word embeddings layers lstm layers. results table summarizes results state-of-the-art results december lstm models achieve exact match score. results comparable original work models achieve exact match score outperforming lstm models. moreover exhibits speed-up reduction total training time. recent state-of-the-art methods rnns encode text potentially beneﬁt approach. dataset penn treebank corpus processed data splits taken mikolov data contains tokens truncated vocabulary following standard practice training data treated long sequence split mini batches models trained using truncated back-propagation-through-time table language modeling perplexities dataset. models comparison trained using similar regularization learning strategy variational dropout used except cudnn lstm; input output word embeddings tied except learning rate decay used models.. also report time training epoch including entire architecture only. setup largely follow conﬁguration prior work batch size truncated back-propagation steps. dropout probability input embedding output softmax layer. standard dropout variational dropout probability stacked layers. initial learning rate gradient clipping. train epochs start decrease learning rate factor epochs. identity activation function results table shows perplexity results. parameter budget million fair comparison. cudnn lstm obtains perplexity seconds epoch. result worse prior work. attribute difference lack variational dropout support cudnn implementation. obtains better perplexity compared cudnn lstm prior work reaching three recurrent layers layers. also provides better speed-perplexity trade-off training six-layer takes seconds epoch. machine translation dataset english→german translation task. pre-process training corpus following standard practice translation pairs left processing. news-test- data used test concatenation news-test- news-test- used development set. setup opennmt extend pytorch version implementation. opennmt uses seqseq model recurrent encoder-decoder architecture attention default model provides hidden state decoder step input step although potentially improve translation quality impedes parallelization slows training. disable option. models trained hidden state word embedding size epochs initial learning rate batch size modify default opennmt dropout rate weight decay leads better results implementations. klein klein cudnn lstm cudnn lstm setup cudnn lstm cudnn lstm cudnn lstm state-of-the-art results gnmt ensemble convss ensemble transformer transformer table english-german translation results list total number parameters number excluding word embeddings setup disables input signiﬁcantly reduces training time. timings performed single nvidia titan pascal gpu. also list recent state-of-the-art results. results table shows translation results. obtain better bleu scores compared reports opennmt results stacked layers achieves bleu score cudnn lstm achieves using parameters training time. scales better stack many layers without signiﬁcant time increase. additional layer encoder decoder adds four minutes training epoch adding lstm layer adds minutes. comparison operations take minutes. observe over-ﬁtting development even using layers. also include recent state-of-the-art results table experiments default model provided opennmt baseline architecture. non-rnn architectures achieved signiﬁcantly better bleu scores en-ge translation task opennmt default model. combining state-of-the-art architectures transformer important research direction plan explore future work. instance minutes spent recurrent layer. reduction time enables introducing recurrent computations transformer architecture. dataset switchboard- corpus training data includes hours speech sides conversations speakers. test data includes hours speech sides conversations evaluation. setup kaldi feature extraction decoding training initial hmm-gmm models. standard kaldi recipes train maximum likelihood-criterion context-dependent speaker adapted acoustic models mel-frequency cepstral coefﬁcient apply forced alignment generate labels neural network acoustic model training. computational network toolkit instead pytorch. experiment uni-directional bi-directional models without state-level minimum bayes risk training tri-gram based language model instead table word error rate speech recognition timing numbers based naive implementation cntk. cuda-level optimizations performed. also report number tokens processed second training time. results table summarizes results. cntk uses special batching algorithms rnns hence able customized kernel. however even without kernel optimization faster lstm using number parameters. also achieves better results wer. adding highway connections lstm performs slightly worse baseline. removing dependency internal state lstm improve speed causes slight decrease performance. appendix includes experiments different highway structures number layers. present simple recurrent unit recurrent architecture fast easily scales layers. evaluation variety speech recognition tasks demonstrates effectiveness sru. open source implementation facilitate future research. thank alexander rush yoon help machine translation experiments danqi chen help squad experiments. also thank adam yala insightful comments. special thanks hugh perkins support experimental environment setup runqi yang answering questions code pytorch community enabling ﬂexible neural module implementation. kyunghyun bart merriënboer ça˘glar gülçehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder–decoder statistical machine translation. proceedings conference empirical methods natural language processing yann dauphin angela michael auli david grangier. language modeling gated convolutional networks. proceedings international conference machine learning greg diamos shubho sengupta bryan catanzaro mike chrzanowski adam coates erich elsen jesse engel awni hannun sanjeev satheesh. persistent rnns stashing recurrent weights on-chip. international conference machine learning godfrey holliman mcdaniel. switchboard telephone speech corpus research development. proc. international conference acoustics speech signal processing priya goyal piotr dollár ross girshick pieter noordhuis lukasz wesolowski aapo kyrola andrew tulloch yangqing kaiming accurate large minibatch training imagenet hour. arxiv preprint arxiv. kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition ozan irsoy claire cardie. opinion mining deep recurrent neural networks. proceedings conference empirical methods natural language processing association computational linguistics ./v/d-. http//aclanthology.coli.uni-saarland.de/pdf/d/d/d-.pdf. sébastien jean kyunghyun roland memisevic yoshua bengio. using large target vocabulary neural machine translation. proceedings annual meeting association computational linguistics international joint conference natural language processing kalchbrenner edward grefenstette phil blunsom. convolutional neural network modelling sentences. proceedings annual meeting association computational linguistics association computational linguistics brian kingsbury tara sainath hagen soltau. scalable minimum bayes risk training deep neural network acoustic models using distributed hessian-free optimization. interspeech guillaume klein yoon yuntian deng jean senellart alexander rush. opennmt opensource toolkit neural machine translation. proceedings system demonstrations regina barzilay tommi jaakkola. molding cnns text non-linear non-consecutive convolutions. proceedings conference empirical methods natural language processing. association computational linguistics hrishikesh joshi regina barzilay tommi jaakkola kateryna tymoshenko alessandro moschitti lluís màrquez. semi-supervised question retrieval gated convolutions. proceedings conference north american chapter association computational linguistics human language technologies. association computational linguistics liangyou xiaofeng santiago cortes vaillo andy liu. dcu-ictcas system german-english translation task. proceedings ninth workshop statistical machine translation huma lodhi craig saunders john shawe-taylor nello cristianini chris watkins. text classiﬁcation using string kernels. journal machine learning research minh-thang luong hieu pham christopher manning. effective approaches attentionbased neural machine translation. empirical methods natural language processing association computational linguistics hongyuan mohit bansal matthew walter. talk how? selective generation using lstms coarse-to-ﬁne alignment. proceedings conference north american chapter association computational linguistics human language technologies ./v/n-. http//aclweb.org/anthology/ yajie miao jinyu yongqiang wang shi-xiong zhang yifan gong. simplifying long shortterm memory acoustic models fast training decoding. acoustics speech signal processing ieee international conference ieee tomas mikolov martin karaﬁát lukas burget cernock`y sanjeev khudanpur. recurrent neural network based language model. interspeech annual conference international speech communication association makuhari chiba japan september dipendra misra john langford yoav artzi. mapping instructions visual observations actions reinforcement learning. proceedings conference empirical methods natural language processing association computational linguistics arxiv preprint https//arxiv.org/abs/.. pang lillian lee. sentimental education sentiment analysis using subjectivity summarization based minimum cuts. proceedings annual meeting association computational linguistics association computational linguistics stephan peitz joern wuebker markus freitag hermann ney. rwth aachen german-english machine translation system proceedings ninth workshop statistical machine translation daniel povey arnab ghoshal gilles boulianne lukas burget ondrej glembek nagendra goel mirko hannenmann petr motlicek yanmin qian petr schwarz silovsky georg stemmer karel vesely. kaldi speech recognition toolkit. automatic speech recognition understanding workshop daniel povey vijayaditya peddinti daniel galvez pegah ghahremani vimal manohar xingyu yiming wang sanjeev khudanpur. purely sequence-trained neural networks based lattice-free mmi. interspeech press lior wolf. using output embedding improve language models. proceedings conference european chapter association computational linguistics tara sainath oriol vinyals andrew senior hasim sak. convolutional long short-term memory fully connected deep neural networks. ieee international conference acoustics speech signal processing frank seide gang chen dong feature engineering context-dependent deep neural networks conversational speech transcription. automatic speech recognition understanding ieee workshop ieee noam shazeer azalia mirhoseini krzysztof maziarz andy davis quoc geoffrey hinton jeff dean. outrageously large neural networks sparsely-gated mixture-of-experts layer. arxiv preprint arxiv. richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts. recursive deep models semantic compositionality sentiment treebank. proceedings conference empirical methods natural language processing october mingxuan wang zhengdong hang wenbin jiang liu. gencnn convolutional architecture word sequence prediction. proceedings annual meeting association computational linguistics international joint conference natural language processing wenhui wang yang furu baobao chang ming zhou. gated self-matching networks reading comprehension question answering. proceedings annual meeting association computational linguistics volume huijia jiajun zhang chengqing zong. empirical exploration skip connections sequential tagging. proceedings coling international conference computational linguistics technical papers december yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan klaus macherey google’s neural machine translation system bridging human machine translation. arxiv preprint arxiv. eversole seltzer guenter kuchaiev seide wang droppo huang zhang zweig rossbach currey stolcke slaney. introduction computational networks computational network toolkit. technical report microsoft research http//cntk.codeplex.com. zhang dong michael seltzer jasha droppo. speech recognition predictionadaptation-correction recurrent neural networks. acoustics speech signal processing ieee international conference ieee yuchen zhang jason michael jordan. -regularized neural networks improperly learnable polynomial time. proceedings international conference machine learning julian georg zilly rupesh kumar srivastava koutník jürgen schmidhuber. recurrent highway networks. proceedings international conference machine learning quasi-rnn developed goal speeding computation recurrent neural networks. quasi-rnn design aims combine k-gram convolutions adaptive pooling instead traditional order-oblivious pooling pooling average pooling. similar convolutional architectures k-gram ﬁlter width used throughout experiments reported. quasi-rnn based adding light recurrence convolutional network instance recurrent architectures introduced prior work focused theoretical characteristics networks generalization focus implementation details including practical optimizations applicability wide range tasks. computation similar degenerate case setting k-gram ﬁlter quasi-rnn. computation reduced simple feed-forward transformation loses properties quality convolution. architecture results present also differ quasi-rnn technical decisions cuda optimization quasi-rnn implementation fo-pooling done cuda kernel function. rest element-wise computation performed separate function calls software library choice allow type architecture achieve full potential. contrast implement element-wise fusion enable speed optimization activation highway connections quasi-rnn differ non-linear activation function applied. quasi-rnn follows common practice convolutional models non-linear activation applied convolution operation pooling contrast activation function applied compute internal state following derivation also includes highway connections architecture better generalization deep networks. effect element-wise fusion compare speed fo-pooling implementation used quasi-rnn fused kernel implementation used sru. implement version unidirectional bi-directional uses fo-pooling separate element-wise function calls pytorch. figure presents speed comparison classiﬁcation tasks squad fused element-wise kernel achieves speed improvement across benchmarks. effect activation highway connection compare performance variants different activation functions quasi-rnn ﬁlter width classiﬁcation squad datasets. classiﬁcation tasks train using using adam optimizer default learning rate weight decay dropout probability tuned values perform three trials -fold cross validation model dataset. report average test accuracy model conﬁgurations achieve best development results. squad train models maximum epochs using admax optimizer learning rate perform three independent trials report average performance. figure table table summarize results performs least good quasi-rnn often outperforms illustrates advantages design choices including highway connections activation implementation. observe optimal choice activation function varies depending task. example found relu activation work best classiﬁcation benchmarks identity activation performs best squad relu performs worst. effect highway connections best highlighted question answering dataset. without highway connections observe performance decrease stacking four recurrent layers contrast adding highway connections results absolute improvement exact match score. figure also shows figure left relative speed improvement fused kernel fo-pooling kernel various benchmarks. timings performed desktop machine geforce intel core processor. right mean exact match score -layer quasi-rnn squad function number epochs. models trained maximum epochs using admax optimizer learning rate figure comparison quasi-rnn classiﬁcation benchmarks. perform independent trials -fold cross validation model dataset. report average test accuracy model conﬁgurations achieve best result. models trained using adam optimizer default learning rate weight decay dropout probability tuned values figure comparison quasi-rnn squad benchmark. perform independent trials maximum training epochs. report average exact match score model conﬁguration. models trained using admax learning rate following sainath weights randomly initialized uniform distribution range biases initialized without generative discriminative pre-training neural network models unless noted otherwise trained cross-entropy criterion using truncated bptt. momentum used ﬁrst epoch momentum used subsequent epochs apply constraint regularization weight experiment uni-directional bi-directional models. train uni-directional model unroll frames utterances mini-batch. also delayed output lstm frames suggested context. train bidirectional model latency-controlled method described zhang processed utterances simultaneously. addition vanilla model also experiment state-level minimum bayes risk training train recurrent model smbr criterion adopted two-forward-pass method described zhang processed utterances simultaneously. input features models -dimensional ﬁlterbank features computed every milliseconds additional -dimensional pitch features. output targets -contextdependent triphone states numbers determined last hmm-gmm training stage. setup potentially improve incorporate recent techniques applicable sru. example lf-mmi sequence training i-vectors speaker adaptation speaker perturbation data augmentation applied povey techniques also used sru. moreover different highway variants grid lstm also boost model. baseline identify lstm baseline used section experiment varying number layers parameters. table shows performance different settings. follow setup best lstm baseline using layers units layer. table word error rate lstm baselines switchboard- corpus lstm cells layer lstm cells layer lstm cells layer. lstm projection contains cells -node linear projection layer added layer output. effect highway transform dimensionality input must equal computation however case example ﬁrst layer linear projection match dimensions layer every layer. table shows adding transformation also square matrix signiﬁcantly reduces word error rate using number parameters. transformation outside recurrent loop parallelized computed efﬁciently. table word error rate comparison effect transformation highway connection. includes transform ﬁrst layer align dimensionality. second line includes transformation every layer. effect depth study effect depth performance. table shows word error rate different number layers. model outperforms lstm model layers number parameters provide speed-up even though using non-optimized implementation cntk. speed gains mainly result requiring less matrix multiplications. best performance achieved layers. table performance speed function depth speech recognition report word error rate number tokens processed second training time. timing numbers based naive implementation cntk. cuda-level optimizations performed. initial experiments observed depth important performance width. therefore focus experiments effect network depth. leave conclusive experiments trade-offs depth width future work.", "year": 2017}