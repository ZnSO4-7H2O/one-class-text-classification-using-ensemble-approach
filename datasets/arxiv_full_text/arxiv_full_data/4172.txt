{"title": "Learning an attention model in an artificial visual system", "tag": ["cs.CV", "cs.AI"], "abstract": "The Human visual perception of the world is of a large fixed image that is highly detailed and sharp. However, receptor density in the retina is not uniform: a small central region called the fovea is very dense and exhibits high resolution, whereas a peripheral region around it has much lower spatial resolution. Thus, contrary to our perception, we are only able to observe a very small region around the line of sight with high resolution. The perception of a complete and stable view is aided by an attention mechanism that directs the eyes to the numerous points of interest within the scene. The eyes move between these targets in quick, unconscious movements, known as \"saccades\". Once a target is centered at the fovea, the eyes fixate for a fraction of a second while the visual system extracts the necessary information. An artificial visual system was built based on a fully recurrent neural network set within a reinforcement learning protocol, and learned to attend to regions of interest while solving a classification task. The model is consistent with several experimentally observed phenomena, and suggests novel predictions.", "text": "target centered fovea ﬁxates fraction second visual system extracts necessary information. movements proactive rather reactive predict actions advance merely respond visual stimuli good evidence much active vision humans results reinforcement learning part organism’s attempt maximize performance interacting environment accordingly train artiﬁcial visual system within paradigm. network explicitly engineered perform certain task contain explicit memory component rather memory virtue recurrent topology. learning takes place model-free setting using policy gradient techniques. network displays attributes human learning decision making gradual conﬁdence increase along accumulated evidence skill transfer namely ability pre-learned skill certain task order improve learning related difﬁcult task selectively attending information relevant task hand ignoring irrelevant objects ﬁeld view. designed artiﬁcial visual system task learning attention model control saccadic movements subsequent classiﬁcation digits. refer task attention-classiﬁcation task. similar many ways presented simpliﬁed model human visual system consisting small region center high resolution analogous human fovea larger concentric regions sub-sampled lower resolution analogous peripheral visual system humans. trained tested classiﬁcation handwritten digits mnist data small part image visible time. speciﬁcally full resolution available fovea -by- pixels -by- pixels ﬁrst peripheral region double size fovea sub-sampled period match size fovea pixels. similarly second peripheral region quadruple size fovea sub-sampled period comparison typical digit mnist database abstract—the human visual perception world large ﬁxed image highly detailed sharp. however receptor density retina uniform small central region called fovea dense exhibits high resolution whereas peripheral region around much lower spatial resolution. thus contrary perception able observe small region around line sight high resolution. perception complete stable view aided attention mechanism directs eyes numerous points interest within scene. eyes move targets quick unconscious movements known saccades. target centered fovea eyes ﬁxate fraction second visual system extracts necessary information. artiﬁcial visual system built based fully recurrent neural network within reinforcement learning protocol learned attend regions interest solving classiﬁcation task. model consistent several experimentally observed phenomena suggests novel predictions. neuroscientists cognitive scientists many tools disposal study brain neural networks general including electroencephalography single-photon emission computed tomography functional magnetic resonance imaging microelectrode arrays name few. however amount information level control afforded tools remotely resemble available engineer working artiﬁcial neural network. engineer manipulate neuron time force certain excitations intervene ongoing processes collect much data network needed level detail. wealth information enabled reverse engineering research artiﬁcial neural networks leading insights inner workings trained artiﬁcial neural networks. suggests indirect approach studying brain training biologically plausible neural network model exhibit complex behavior observed real brains reverse engineering result. line approach designed artiﬁcial visual system based fully recurrent unlayered neural network learns perform saccadic movements. saccadic movements quick unconscious task-dependent motions following demand attention direct targets require high resolution fovea. targets usually detected within peripheral visual system occupies -by- pixels image. location observation within image available movements observation location constrained image boundaries. instead locations outside image boundaries observed black pixels. consists inputs projected upon network input weights neural network consisting neurons connected recurrent weights outputs classiﬁer responsible identifying digit explored image attention model output yatt responsible directing locations based information represented network state output consists neuron possible digit. trial identity highest valued neuron interpreted network’s classiﬁcation. progression single trial follows principal stages random digit selected mnist training location across image randomly selected. observation current location projected upon network along pre-existing information within network state recurrent weights implemented fully recurrent neural network. network topology similar echo state network recurrent neural connections drawn randomly constrained particular topology layered feedforward networks long short-term memory networks. random trajectory glimpses probability trajectory observed reward ﬁxed baseline computed random location ﬁrst glimpse indicates averaging trajectories. viewed partially observable markov decision process write distributions describing agent number glimpses. stochastic gradient ascent performed output weights wout. recurrent weights randomly selected spectral radius remain ﬁxed throughout training. likelihood respect internal weight matrix takes similar form. however recurrent connections learned simulations. since information accumulated neural network time mixed state network obvious potential extract useful historic information exploited within attention model solution. training uses gradient ascent local maxima estimated expected reward therefore converge sub-optimal maxima make full potential system. order test memory trained network similar trained attention-classiﬁcation task. ﬁrst recurrent weights random whereas second ‘forget’ historic information setting recurrent weights matrix zero. memory found depend size fovea. fig. shows performance system across training epochs case large fovea. initially memory advantage attention model still poor stage leading relatively uninformative glimpses information several glimpses results better classiﬁcation. however attention mechanism improves last glimpse becomes highly informative memoryless network information last glimpse corrupted memory previous glimpses advantage. fact found information well-placed glimpse sufﬁces classify digit success rate case driving network solution ﬁnding single good glimpse location across digit classiﬁcation based glimpse without regard rest trajectory. situation different smaller fovea classiﬁcation single glimpse becomes harder. seen figure memory outperforms without memory small fovea case. human visual system acts maximize information relevant task order assess whether behaves similarly characterize relevant information context task. since network classiﬁcation depends linearly network state last time step quantify task-relevant information best linear separation network state class classes. accordingly linear discriminant analysis acts projection minimizes distance samples cluster time maximizes distance clusters distance within class measured variance samples belonging class taken mean distances across classes. distance classes deﬁned variance class centers. trained attention-classiﬁcation task glimpses digit. trained tested cases. ﬁrst case system usual network state vector recorded last glimpse digit test set. second case location last glimpse chosen randomly rather following learned attention model. results illustrated figure state network projected ﬁrst eigenvectors separation signiﬁcantly better full attention model compared random last glimpse. conclude that least attention model acts maximize task-relevant information last glimpse better random walk. fig. results linear discriminant analysis state last time step. corresponds single trial represents projection network state ﬁrst eigenvectors dots colored according digit presented network. left random last glimpse. right full attention model. biological learning often displays ability skill learned simple task order improve learning harder related task e.g. proﬁciency tennis beneﬁcial learning racquetball even seemingly unrelated tasks skiing example test whether transfer learning possible trained learn attention model classiﬁcation digits resulting solution served initial condition learning full task classifying digits. seen fig. pre-learned attention learn much faster also achieved better result training. next test network distracting object random position around digit. observed behavior similar ﬁrst glimpse happened fall location object digit within peripheral view ignored distraction directed towards digit. however case ﬁrst glimpse falls location distracting object visible peripheral view failed locate digit. eyes directed visually salient points ﬁeld view rather ones relevant task hand accordingly introduced highly salient object training images. object square approximately size digit maximum brightness whereas digits handwritten displayed grayscale. object inserted ﬁxed position relative digit always right hand side digit. trained network successfully avoids unnecessary ﬁxations salient object. cases ﬁrst glimpse falls upon area digit object within peripheral visual region object seems completely ignored. perhaps interesting case object visible ﬁrst glimpse within peripheral view. case learned exploit fact digit always located left object consistently performs saccades left. thus presence distracting object harmful performance actually beneﬁcial. fig. colored squares represent foveal view -by- pixels time step going blue red. left middle images show cases ﬁrst glimpse observes distracting object within peripheral view. left image shows case ﬁrst glimpse observes distracting object digit within peripheral view. example seen fig. lines trajectories starting different point test grid followed last glimpse green lines trajectories correct classiﬁcation lines trajectories false classiﬁcation. happen fall location digit seen directs gaze towards square chooses classify thus earning expected reward better nothing. learning demonstration implemented avs. demonstration differs supervision ways. first demonstration continuous applied sparsely time order suggest trajectories system. second demonstration required provide best solution system system maintains freedom explore even improve upon demonstration achieved providing network sparse naive suggestion attention model. example trajectories system directed center digit last glimpse. partial direction resulted signiﬁcant improvement speed learning ﬁnal success rate observed ﬁgure jeff pelz roxanne canosa diane kucharczyk jason babcock silver daisei konno. portable eyetracking study natural movements. proceedings spie pages volodymyr mnih nicolas heess alex graves koray kavukcuoglu. recurrent models visual attention. advances neural information processing systems pages demonstration system made possible manipulating exploration noise. exploration noise gaussian white noise probability greater zero accept value. since output system given time function noise force output speciﬁc value setting exploration noise watt tanh particular time step yatt demonstrated output attention model yatt determined exploration noise bring system desired output. long demonstration kept sparse enough would practice break assumption noise gaussian white noise. noise system essential part likelihood gradient therefore system would arrive desired output particular time step also learn experience. shown simple artiﬁcial visual system implemented recurrent neural network using policy gradient reinforcement learning trained perform classiﬁcation objects much larger central region high visual acuity. receiving classiﬁcation based reward system develops active vision solution directs attention towards relevant parts image task-dependent way. importantly internal network memory plays essential role maintaining information across saccades ﬁnal classiﬁcation achieved combing information current visual input previous inputs represented network state. within generic active vision system without speciﬁcally crafted features able explain several features characteristic biological vision good classiﬁcation performance using reinforcement learning based highly limited central vision resolution peripheral vision gathering task-relevant information active search transfer learning ignoring task-irrelevant distractors learning guidance. beyond providing model biological vision results suggest possible avenues cost-effective image recognition artiﬁcial vision systems.", "year": 2017}