{"title": "SPLBoost: An Improved Robust Boosting Algorithm Based on Self-paced  Learning", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "It is known that Boosting can be interpreted as a gradient descent technique to minimize an underlying loss function. Specifically, the underlying loss being minimized by the traditional AdaBoost is the exponential loss, which is proved to be very sensitive to random noise/outliers. Therefore, several Boosting algorithms, e.g., LogitBoost and SavageBoost, have been proposed to improve the robustness of AdaBoost by replacing the exponential loss with some designed robust loss functions. In this work, we present a new way to robustify AdaBoost, i.e., incorporating the robust learning idea of Self-paced Learning (SPL) into Boosting framework. Specifically, we design a new robust Boosting algorithm based on SPL regime, i.e., SPLBoost, which can be easily implemented by slightly modifying off-the-shelf Boosting packages. Extensive experiments and a theoretical characterization are also carried out to illustrate the merits of the proposed SPLBoost.", "text": "every training sample adjust every step weights correctly classiﬁed samples current learner decreased weights misclassiﬁed samples increased. reweighting gives rise adaboost always pays attention samples hard classify ignores easy-to-classify samples extent training next weak learner. many practical applications demonstrated success adaboost producing satisfactory accurate strong classiﬁers interesting many cases test error seems decrease consistently level rather gradually increase weak learners added means prone overﬁt result difﬁcult adaboost determine number weak learners. spite this classiﬁers produced adaboost always acceptable especially training samples corrupted outliers shown adaboost algorithm builds additive logistic regression model minimizing expected risk based exponential loss function easy loss increase rapidly increase magnitude negative margin means signiﬁcantly enlarge functions large noises training since large negative value naturally degenerates performance approach presence heavy noises/outliers. aiming remedying poor robustness issue adaboost many studies conducted improve performance dealing data corrupted outliers. mainly adopted methodology design robust loss function boosting optimized gradient descent like strategies resolve optimization problem. robust loss needs designed increase evidently slower magnitude becomes larger suppress effect large noises outliers. although robust boosting algorithms proven able better performance adaboost training data outliers natural defects idea directly design optimize loss functions. although easy optimize convex loss functions robust enough eliminate impact outliers. fact proved non-convex loss functions often better performances convex loss functions. however non-convex loss functions produce non-convex optimization problems difﬁcult solve stable solutions. paper taking advantage robustness self-paced learning regime come thought robust boosting algorithms. main abstract—it known boosting interpreted gradient descent technique minimize underlying loss function. speciﬁcally underlying loss minimized traditional adaboost exponential loss proved sensitive random noise/outliers. therefore several boosting algorithms e.g. logitboost savageboost proposed improve robustness adaboost replacing exponential loss designed robust loss functions. work present robustify adaboost i.e. incorporating robust learning idea self-paced learning boosting framework. speciﬁcally design robust boosting algorithm based regime i.e. splboost easily implemented slightly modifying off-theshelf boosting packages. extensive experiments theoretical characterization also carried illustrate merits proposed splboost. natural ways deal train strong learning machine directly training variety machine learning methods expect obtained learning machine could satisfactory prediction accuracy; train number weak learners slightly better accuracies randomly guessing together speciﬁc strong learner could better accuracy weak learners. latter basic idea ensemble learning. important excellent ensemble learning framework boosting widely applied many machine learning problems simplicity good performance. adaboost commonly-used boosting algorithms proven effective easy implement various classiﬁcation problems. given training data vector-valued feature {−}. known adaboost produce αtft weak learner trained weighted training data step constant calculated based classiﬁcation accuracy predict label sample sign particular adaboost gives weight initialized statistics zongben xi’an p.r. china. e-mails wangkdgmail.com yao.s.wanggmail.com timmy.zhaoqiangmail.com dymengmail.xjtu.edu.cn zbxumail.xjtu.edu.cn. firstly propose robust boosting algorithm named splboost incorporates self-paced learning adaboost framework. mentioned above always good idea improve robustness adaboost directly modifying loss function motivates utilize another efﬁcient achieve goal. recently shown self-paced learning effective robust learning regime achieved satisfactory results deal many machine learning computer version problems. basic idea self-paced learning give weights training samples weights samples larger losses smaller weights zero corresponding losses large enough. combining selfpaced learning adaboost splboost proven able improve robustness adaboost. secondly proposed splboost algorithm easily embedded off-the-shelf adaboost package. besides splboost variations easily designed integrating directly boosting packages like logitboost lboost improve robustness presence heavy noises/outliers. thirdly prove splboost exactly complies widely known majorization-minimization algorithm implemented latent objective function based non-convex loss function. clearly explains theoretically splboost could robust adaboost. robust insight also holds variations splboost. addition alternately optimizing sub-problems easy solve rather directly optimizing latent objective function splboost keep away annoying non-convex optimization problem better local optimal solution. rest paper organized follows. shall provide brief review boosting algorithms self-paced learning section then splboost algorithm theoretical analysis presented section iii. section shows experimental results several synthetic data sets. several concluding remarks ﬁnally made section aforementioned biggest defect adaboost easily overﬁt outliers inspires studies improving robustness adaboost generally speaking three factors affect robustness boosting algorithms loss function compute weak learners regularization. based factors many robust boosting algorithms suggested. exponential loss function real adaboost algorithm additive logistic regression model stage-wise optimizing expected risk discrete adaboost. this friedman proposed different robust boosting algorithms logitboost gentleboost. logitboost algorithm uses newton steps optimizing logistic loss robust exponential loss. easy logistic loss function assigns fewer penalties samples negative margins whose absolute values large usually outliers. logitboost easy overﬁt outliers. gentleboost robust boosting algorithm proposed different logitboost optimizing underlying loss function. basically gentleboost optimizes exponential loss function adaboost does. main difference gentleboost real adaboost gentleboost computes weak learners using adaptive newton step logitboost does. real adaboost update half log-ratio numerically unstable lead large updates sample weights. however updates gentleboost leading conservative sample weights. consequently inﬂuence outliers exert gentleboost weaker adaboost. although loss functions boosting algorithms referred different other resulting different performances convex. theoretical properties boosting algorithms based convex loss functions extensively studied. e.g. minimum convex loss function easy compute gives rise simplicity aforementioned boosting algorithms. however disadvantage convex loss function obvious. shown convex loss function robust enough tolerate noise result boosting algorithms based convex loss naturally insensitive outliers. speciﬁcally long servedio proved boosting algorithm based convex loss functions easily affected random label noise present sample example named long/servedio problem cannot learned popular boosting algorithms. such results presented long servedio lead studies boosting algorithms non-convex loss functions. based boost-by-majority algorithm brownboost freund proposed robust boosting algorithm named robustboost robust outliers adaboost logitboost. loss function robustboost non-convex changes boosting process largest difference robustboost popular ones. robustboost improves robustness allowing pre-assigned error margin maximization step updates solves differential equation updates preassigned target error remaining time algorithmic process least preassigned parameters difﬁcult implement limits application robustboost. boosting algorithms design various robust loss functions restricted penalties misclassiﬁed samples large margins design computation methods compute weak learners based loss functions. however common framework cannot always satisfactory. speciﬁcally convex loss functions proven robust enough especially large label noise outlier although non-convex loss functions possess better antinoise ability induce non-convex optimization problem compute weak learners intractable task general. paper instead directly designing robust loss functions propose robust boosting algorithm named splboost combining classical discrete adaboost algorithm robust learning idea self-paced learning. next subsection shall give simple introduction self-paced learning. humans animals often learn examples randomly presented organized meaningful order gradually includes easy fewer concepts complex ones. inspired principle humans animals learning bengio proposed learning paradigm named curriculum learning origin self-paced learning. curriculum learning model learned gradually including easy complex samples training improve accuracy model. obviously curriculum learning proper ranking function assigns learning priorities training samples. satisfactory model quality curriculum i.e. ranking function important oftentimes derived predetermined heuristics particular problems real applications. lead inconsistency ﬁxed curriculum dynamically learned models. alleviate aforementioned issue kumar proposed model named self-paced learning instead derived predetermined heuristics curriculum design embedded regularization term learning objective. formulation self-paced learning follows parameter controlling learning pace denotes loss function calculates cost ground truth label estimated label denotes model parameter inside decision function seen loss sample discounted latent weight variable objective minimize weighted training loss together selfpaced regularizer generally solved alternative search strategy method variables divided disjoint blocks iteration block variables optimized keeping blocks ﬁxed. ﬁxed weighted training determine function class search within function optimizes objective function based predeﬁned loss function. view limitations methods convergence rate much sensitivity outliers masnadi-shirazi vasconcelos showed problem classiﬁer design identical problem probability elicitation probability elicitation seen reverse procedure solving classiﬁcation problem provides insights relationship loss function minimum risk optimal classiﬁer. this derived loss function named savage loss trades convexity boundedness. using loss proposed so-called savageboost i.e. robust boosting algorithm outlier resistant adaboost logitboost. clearly form savage loss shows unlike exponential loss logistic loss penalty always increases fast speed savage loss non-convex quickly becomes constant considering weights misclassiﬁed samples large margins could large savageboost sensitive outliers compared adaboost logitboost. requirements design robust loss function gentle penalty misclassiﬁed samples large margins design numerically stable algorithm optimize current objective function obtain weak learner step. based requirements miao proposed robust boosting algorithms named rboost rboost deep connection savageboost. boosting algorithms optimize conditional expected risk based savage loss function robust loss function deﬁned easy savage loss function similar form savage loss function difference secondorder factor denominator makes savage loss give gentler penalty misclassiﬁed samples large margins savage loss function. such proposed rboost could insensitive outliers savageboost. fact reasons weaken robustness savageboost results weak learners savageboost output required posterior probability estimation estimate posterior probability difﬁcult classiﬁcation savageboost computes weak learners numerically stable. avoid drawbacks savageboost rboost algorithms carefully designed optimize savage loss function. precisely rboost algorithm uses adaptive newton step solve minimization problem computes weak learner based current classiﬁer conditional expected risk maximally decreased. rboost algorithm designed make rboost algorithm restricts weak learner algorithms regression methods adaptive general weak learner algorithms. robust loss function numerically stable methods compute weak learners rboost rboost algorithms could good performance noisy data. based many variations self-paced learning regime proposed self-paced re-ranking selfpaced learning diversity self-paced curriculum learning self-paced multiple-instance-learning applications self-paced learning many machine learning computer version tasks like objective detector adaptation long-term tricking visual category discovery face identiﬁcation speciﬁc-class segmentation learning demonstrated effectiveness especially robustness dealing severally corrupted data. meng zhao provided theoretical evidences illustrate insights self-paced learning. proved algorithm solve problem exactly accords majorization minimization algorithm implemented latent nonconvex objective function. work laid theoretical foundation spl. comparing adaboost thing common them training samples different losses unequal endowed different weights. adaboost assign weights samples different. adaboost samples large losses paid attention given larger weights. contrary samples losses larger certain constant thought outliers weights zero. account fact reason adaboost robust adaboost assigns large weights samples large losses samples usually outliers expect provide complementary assistance restrict sample weights adaboost improve robustness. robust boosting algorithm named splboost proposed based idea. introduce details splboost present theoretical results next section. vector-valued feature {−}. known adaboost algorithm builds additive logistic regression model minimizing expected risk based exponential loss function iteration supposing current classiﬁer adaboost seeks weak learner following optimization problem loss minimization problem appears many machine learning problems. ﬁxed optimization problem global optimum easily calculated following formulation hard self-paced learning implements automatic selection samples trains model selected samples. update ﬁxed samples losses smaller certain threshold considered easy samples selected training rest considered difﬁcult learned abandoned update ﬁxed classiﬁer trained selected easy samples. parameter corresponds model determines ability model learn difﬁcult samples. small model learn easy samples small losses grows samples larger losses learned train mature model. so-called sp-regularizer negative l-norm induces variable takes binary values i.e. scheme called hard weighting. hard weighting determine whether sample selected good enough sometimes also want discriminate importance samples. jiang theoretically abstracted intrinsic conditions sp-regularizer proposed several soft weighting schemes linear soft weighting logarithmic soft weighting. zhao used self-paced learning matrix factorization proposed soft weighting schemes named mixture weighting hybrid soft hard weighting. proposed novel polynomial soft weighting regularizer adjustable parameter multi-objective selfpaced learning model. soft weighting assigns real-valued weights reﬂects latent importance samples training faithfully reasonable general hard weighting. following shall list formulation linear soft weighting mixture weighting polynomial soft weighting respectively together closed-form solutions linear soft weighting weighted least squares problem obtain weak learner equivalent implement adaboost except latent weight variable thus imitating approach adaboost train training data sample weights viwi rather given directly optimize determine firstly layers iteration algorithm outer iteration update classiﬁer inner iteration optimal latent weight variable weak learner weight current outer iteration. inner iteration starts latent weight variable initialized optimal value provide last outer iteration experiments show case inner iteration rapidly converge signiﬁcant difference case inner iteration runs step case inner iteration keeps running converged. such actually inner iteration step speedup algorithm implementation. easy objective splboost step minimize weighted exponential loss together self-paced regularizer. adaboost exponential loss directly minimized outliers whose losses usually large easy paid attention splboost overcomes problem assigns different weights exponential losses training samples. although different spregularizer induce different format always zero loss large usually means corresponding sample likely outlier. splboost eliminate negative inﬂuence outliers training data large extent improve robustness adaboost. shall solve alternative search strategy popular iterative process. order distinguish iterative process iterative process splboost updating classiﬁer call former inner iteration latter outer iteration. every inner iteration ﬁxed optimization problem global optimum whose form different different sp-regularizer presented relevant papers especially sp-regularizer negative l-norm hard weighting plug exponential loss formula easily calculated following formulation secondly choosing parameter easy losses samples larger latent weight variable samples could zero means samples would selected training process. thus actually represents tolerance algorithm toward noises outliers. larger tolerant algorithm noises outliers less samples considered outliers would abandoned furthermore large enough spboost algorithm degenerates adaboost. contrary smaller rigorous algorithm large noises outliers samples would abandoned. apparently value huge inﬂuence performance algorithm thus important select appropriate practice usually select proper cross validation. weak learner produced algorithm restricted losses samples calculate ﬁrst outer iteration step weight ﬁrst weak learner. considering samples whose losses larger samples misclassiﬁed ﬁrst weak learner could selected training next outer iteration falls usually means many samples would abandoned accuracy weak learner. avoid kind unreasonable situation adopt warm start produre i.e. ﬁrst outer iteration steps large number instead input value obtaining corresponding weak learners would reset input value. such ﬁrst weak learners trained adaboost result samples determined whether selected based classiﬁer bad. according experience reasonable ﬁrst three outer iterations satisfactory tuned cross validation. different adaboost sample weight e−yi) splboost modiﬁes sample weight viwi introducing latent weight variable shall show reason splboost robust. fig. fig. illustrates sample weight various boosting algorithms including adaboost logitboost savageboost rboost splboost sp-regularizer linear soft weighting. illustrates sample weight splboost various sp-regularizers including hard weighting linear soft weighting polynomial soft weighting ﬁxed illustrates sample weight different boosting algorithms including adaboost logitboost savageboost rboost splboost sp-regularizer linear soft weighting. easy observe adaboost pays much attention misclassiﬁed samples large margins usually outliers. thus adaboost usually sensitive outliers. logitboost weights misclassiﬁed samples large margins smaller adaboost still larger weights samples thus logitboost still easily affected outliers. popular robust boosting algorithms non-convex loss functions i.e. savageboost rboost give small weights misclassiﬁed samples large margins thus usually insensitive outliers. splboost margin misclassiﬁed samples larger certain constant determined weights samples could zero. splboost thoroughly eliminate inﬂuence always misclassiﬁed samples always outliers. fig. illustrates sample weight splboost different spregularizers ﬁxed different sp-regularizers provide different distributions sample weight suited different training data. distributions spiculate others gentle could zero margins misclassiﬁed samples large enough guarantees robustness outliers. exactly equivalent update algorithm case inner iteration converged step ﬁnally learned weak classiﬁer weight denote αtft surrogate function ˜fλt− ˜fλt theorem various off-the-shelf theoretical results algorithm used explain properties splboost. particularly based well-known convergence theory algorithm lower-bounded latent splboost objective monotonically decreasing iteration. thus weak convergence result splboost directly obtained. loss ˜fλ) number various sp-regularizers calculated presented need plug exponential loss function formulas latent splboost losses various spregularizers. fig. illustrates popular loss functions subsection shall provide theoretical analysis splboost could show clear theoretical evidence clarify splboost capable performing robust especially outlier/heavy noise cases. convenience brieﬂy write exponential loss function e−yi+αf e−y+αf following. according theorem derived that latent weight variable conducted sp-regularizer ˜fλ) calculated given ﬁxed fλ). proof. assume completed times outer iteration classiﬁer ft−. denote weak classiﬁer weight learned inner iteration outer iteration alternative search steps next iteration explained standard scheme. precisely cases dealt with. case inner iteration converged step denote surrogate function including exponential loss logistic loss savage loss savage loss loss latent splboost loss sp-regularizer linear soft weighting. easy fig. that compared original exponential loss function latent splboost loss evident suppressing effect large losses. loss larger certain threshold determined parameter latent splboost loss ˜fλ) becomes constant thereafter rationally explains splboost shows good robustness outliers heavy noises. misclassiﬁed samples large margins constant splboost losses thus effect model training zero gradients. corresponding original splboost model latent weight variable large-loss samples thus samples inﬂuence training weak learners. actually determines degree suppressing effect splboost loss large losses. larger gentler suppressing effect vice versa. suppressing effect completely disappears latent splboost loss hard weighting sp-regularizer degenerates exponential loss. fig. illustrates latent splboost loss various sp-regularizers including hard weighting linear soft weighting polynomial soft weighting ﬁxed different sp-regularizers give different shapes latent splboost loss becomes constant loss larger certain constant guarantees robustness outliers heavy noises. fig. illustrates various loss functions including exponential loss logistic loss savage loss savage loss loss latent splboost loss sp-regularizer linear soft weighting. illustrates latent splboost loss various sp-regularizers including hard weighting linear soft weighting polynomial soft weighting ﬁxed easy splboost actually optimization problem nonconvex loss function. different many robust boosting algorithms directly optimize non-convex objective functions splboost decomposes minimization robust difﬁcult-to-solve non-convex problem much easier optimization problems respect latent weight variable weak learner sense splboost avoids difﬁculty non-convex optimization simpliﬁes solving problem. loss functions used boosting algorithms important effect robustness algorithms outliers heavy noises. moreover reweighting strategy boosting algorithm directly comes loss function used directly determines much attention algorithm pays various samples. thus reasonable reweighting strategy necessary robust boosting algorithm weaken interference outliers training good reweighting strategy give least possible weights outliers. synthetic data underlying distribution samples information outliers added samples known thus easy determine optimal bayes decision boundary classiﬁcation problem observe rationality distribution sample weights. compare different reweighting strategies different boosting algorithms ﬁrst evaluate proposed splboost algorithm adaboost robust boosting algorithm including logitboost savageboost rboost robustboost synthetic gaussian data directly visualize experimental results data dimensions. experimental settings results follows. ﬁrst generate samples negative positive classes randomly select classes reverse labels samples selected considered outliers. obtained two-class training data outliers train classiﬁers using aforementioned boosting algorithms. adaboost splboost robustboost weak learner algorithms classiﬁcation methods classiﬁcation tree selected weak learner splboost hard weighting used sp-regularizer. logitboost rboost weak learner algorithms restricted regression methods regression tree cart used weak learner. competing boosting algorithms. sub-ﬁgures fig. pluses represent positive training samples cross marks represent negative training samples. blue squares represent training samples generated gaussian distribution negative class labeled positive samples. actually outliers added negative class. similar circles represent outliers added positive class. black circles represent training samples whose sample weights splboost i.e. samples considered outliers thus effects weak learner training splboost algorithm. visually observe distribution sample weights various boosting algorithms sizes pluses cross marks blue squares circles proportion sample weights corresponding training samples. since sample weights training samples marked black circles sizes black circles relationship weights. such easily observe training samples boosting algorithms focus samples sample weights splboost. basically several observation made fig. firstly fig. points large sizes surrounded circles blue squares means training samples large sample weights outliers. reveals adaboost sensitive outliers. actually loss used adaboost makes much attention misclassiﬁed samples large margins samples usually outliers. secondly seen fig. weights outliers still larger correct training samples though logitboost gives gentler weight outliers adaboost. common drawback convex loss functions stated thirdly fig. fig. fig. show performances savageboost rboost robustboost respectively satisfactory adaboost logitboost non-convex loss functions. addition compared saveboost rboost makes sizes outliers smaller designing numerical stably solver. still assigns relatively large weights number points however certainly affect performance practice. robustboost weights outliers near bayes decision boundary still larger points means robustboost somehow affected outliers. fourthly based fig. different gives different performance splboost. speciﬁcally compared fig. fig. smaller algorithm rigorous outliers samples considered outliers. contrary fig. larger algorithm tolerant fewer samples considered outliers. addition splboost performs much better competing fig. optimal bayes decision boundary sample weights adaboost logitboost savageboost rboost robustboost splboost sp-regularizer splboost hard weighting. adult bank blood connect- magic miniboone ozone pima parkinsons pb-t-or-d ringnorm spambase titanic oocmerld st-german-credit twonorm vc-classes experiment settings follows. every data randomly select samples training data rest test data. evaluate robustness compared boosting algorithms randomly select certain proportion training data points labels. noise levels respectively. maximum iteration step fold cross validation procedure used determine appropriate number iteration steps boosting algorithms parameters. last experiment synthetic gaussian data weak learner adaboost splboost robustboost chosen logitboost rboost regression tree cart used weak learner. illustrate robust performance splboost different sp-regularizers implement splboost four popular sp-regularizers namely hard weighting linear soft weighting polynomial soft weighting denoted hard linear lowerconvex upperconvex respectively. procedure repeated times average testing error rates change different noise levels various boosting algorithms plotted fig.. seen fig. outliers training data performance adaboost heavily depraved comparable logitboost savageboost rboost robustboost splboost conﬁrms adaboost sensitive noisy data. also data sets splboost gives lower test errors boosting algorithms reveals splboost best robustness among compared methods. additionally hard signiﬁcantly difference performance splboost using four different sp-regularizers. better demonstrate performance compared algorithms rank performance total cases calculates ratio data sets boosting algorithm among top-n ranked ones results summarized fig. easily seen cases performance splboost much better competing boosting algorithms clearly conﬁrms splboost better resistance large noise outliers. boosting interpreted gradient descent technique minimize underlying loss function loss function determines robustness algorithm. convex loss functions exponential loss used adaboost logistic loss used logitboost proven sensitive outliers. non-convex loss functions savage loss used savageboost savage loss used rboost illustrated superior robustness popular convex losses however solving non-convex optimization problem derived non-convex losses easy task. paper instead designing loss function combine classical discrete adaboost algorithm selfpaced learning regime robust algorithm framework attracting troumendous attention machine learning computer vision. thus come robust boosting algorithm named splboost. experiments shows splboost could superior performance popular ones outliers exist training data. however still interesting works need done future. hand hard splboost treated general framework improve robustness various boosting algorithms besides adaboost. such popular boosting algorithms logitboost lboost better performance. hand although proven equivalence splboost algorithm implemented latent non-convex objective function detailed theoretical properties splboost including consistency convergence rate error bound needed investigated. yoav freund robert schapire. desicion-theoretic generalization european on-line learning application boosting. conference computational learning theory pages springer paul viola michael jones. rapid object detection using boosted cascade simple features. proceeding ieee conference computer vision pattern recognition pages ieee paul viola michael jones. robust real-time face detection. jonathan cheung-wai chan desir´e paelinckx. evaluation random forest adaboost tree-based ensemble classiﬁcation spectral band selection ecotope mapping using airborne hyperspectral imagery. remote sensing environment taghi khoshgoftaar jason hulse amri napolitano. comparing boosting bagging techniques noisy imbalanced data. ieee transactions systems cybernetics-part systems humans vladimir koltchinskii dmitry panchenko. empirical margin distributions bounding generalization error combined classiﬁers. annals statistics pages qiguang miao ying maoguo gong jiachen jianfeng song. rboost label noise-robust boosting algorithm based nonconvex loss function numerically stable base learners. ieee transactions neural networks learning systems yoshua bengio j´erˆome louradour ronan collobert jason weston. curriculum learning. proceedings annual international conference machine learning pages jiang deyu meng teruko mitamura alexander hauptmann. easy samples ﬁrst self-paced reranking zero-example multimedia search. proceedings international conference multimedia pages qian zhao deyu meng jiang zongben alexander hauptmann. self-paced learning matrix factorization. proceedings twenty-ninth aaai conference artiﬁcial intelligence pages aaai press maoguo gong deyu meng qiguang miao. multi-objective self-paced learning. proceedings thirtieth aaai conference artiﬁcial intelligence pages aaai press jiang deyu meng shoou-i zhenzhong shiguang shan alexander hauptmann. self-paced learning diversity. advances neural information processing systems pages jiang deyu meng qian zhao shiguang shan alexander proceedings hauptmann. self-paced curriculum learning. twenty-ninth aaai conference artiﬁcial intelligence pages aaai press dingwen zhang deyu meng junwei han. co-saliency detection self-paced multiple-instance learning framework. ieee transactions pattern analysis machine intelligence kevin tang vignesh ramanathan fei-fei daphne koller. shifting weights adapting object detectors image video. advances neural information processing systems pages james supancic deva ramanan. self-paced learning long-term tracking. proceedings ieee conference computer vision pattern recognition pages ieee yong kristen grauman. learning easy things ﬁrst selfpaced visual category discovery. proceeding ieee conference computer vision pattern recognition pages ieee liang keze wang deyu meng wangmeng zhang. active self-paced learning cost-effective progressive face identiﬁcation. appear ieee transactions pattern analysis machine intelligence pawan kumar haithem turki preston daphne koller. inlearning speciﬁc-class segmentation diverse data. ternational conference computer vision pages ieee", "year": 2017}