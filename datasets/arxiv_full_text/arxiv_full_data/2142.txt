{"title": "Bootstrapping with Models: Confidence Intervals for Off-Policy  Evaluation", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "For an autonomous agent, executing a poor policy may be costly or even dangerous. For such agents, it is desirable to determine confidence interval lower bounds on the performance of any given policy without executing said policy. Current methods for exact high confidence off-policy evaluation that use importance sampling require a substantial amount of data to achieve a tight lower bound. Existing model-based methods only address the problem in discrete state spaces. Since exact bounds are intractable for many domains we trade off strict guarantees of safety for more data-efficient approximate bounds. In this context, we propose two bootstrapping off-policy evaluation methods which use learned MDP transition models in order to estimate lower confidence bounds on policy performance with limited data in both continuous and discrete state spaces. Since direct use of a model may introduce bias, we derive a theoretical upper bound on model bias for when the model transition function is estimated with i.i.d. trajectories. This bound broadens our understanding of the conditions under which model-based methods have high bias. Finally, we empirically evaluate our proposed methods and analyze the settings in which different bootstrapping off-policy confidence interval methods succeed and fail.", "text": "data large variance importance sampled returns algorithms require prohibitively large amounts data produce meaningful conﬁdence bounds. current state-of-the-art high conﬁdence off-policy evaluation discrete continuous settings concentration inequality tailored distribution importance sampled returns unfortunately amount data required tight conﬁdence bounds preclude method data-scarce settings robotics. instead exact high conﬁdence thomas demonstrated approximate bounds obtained bootstrapping importance sampled policy returns improve data-efﬁciency order magnitude concentration inequalities. work propose approximate high conﬁdence off-policy evaluation methods combination bootstrapping learned models environment’s transition dynamics. methods straightforward implement empirically demonstrated outperform importance-sampling methods. ﬁrst contribution model-based bootstrapping directly combines bootstrapping learned models environment’s dynamics off-policy value estimation. since mb-bootstrap uses direct model-based estimates policy value exhibit bias settings. characterize settings derive upper bound model bias models estimated arbitrary distributions trajectories. second algorithmic contribution weighted doubly robust bootstrapping combines bootstrapping recently proposed weighted doubly robust estimator uses model lower variance importance sampling estimators without adding model bias estimate. empirically evaluate methods high conﬁdence off-policy evaluation tasks. results show methods data-efﬁcient existing importance sampling based approaches. finally combine theoretical empirical results make speciﬁc recommendations different off-policy conﬁdence bound methods practice. formalize problem markov decision process deﬁned states actions probability mass function deﬁning distribution next states state action bounded non-negative reward function discount factor probability mass function initial states. agent samples actions policy probability mass function conditioned current state. policy deterministic abstract autonomous agent executing poor policy costly even dangerous. agents desirable determine conﬁdence interval lower bounds performance given policy without executing said policy. current methods exact high conﬁdence off-policy evaluation importance sampling require substantial amount data achieve tight lower bound. existing model-based methods address problem discrete state spaces. since exact bounds intractable many domains trade strict guarantees safety data-efﬁcient approximate bounds. context propose bootstrapping off-policy evaluation methods learned transition models order estimate lower conﬁdence bounds policy performance limited data continuous discrete state spaces. since direct model introduce bias derive theoretical upper bound model bias model transition function estimated i.i.d. trajectories. bound broadens understanding conditions model-based methods high bias. finally empirically evaluate proposed methods analyze settings different bootstrapping off-policy conﬁdence interval methods succeed fail. reinforcement learning agents application real world critical establish performance policies high conﬁdence executed. example deploying poorly performing policy manufacturing robot slow production worst case damage robot harm humans working around insufﬁcient policy high off-policy predicted value want specify lower bound policy’s value correct pre-determined level conﬁdence. problem known high conﬁdence offpolicy evaluation problem. propose data-efﬁcient approximate solutions problem. high conﬁdence off-policy model-based methods require large amounts data limited discrete settings continuous settings current methods high conﬁdence off-policy evaluation rely importance sampling existing domain appears proceedings international conference autonomous agents multiagent systems durfee larson winikoff paulo brazil. copyright international foundation autonomous agents multiagent systems rights reserved. length state-action history return trajectory γtr. policy transition dynamics induce distribution trajectories write denote trajectory sampled executing expected discounted return policy deﬁned eh∼π]. given trajectories behavior policy evaluation policy conﬁdence level propose methods approximate conﬁdence lower bound probability least importance sampling importance sampling method policy trajectory deﬁne importance weight time policy estimator trajectory deﬁned lower variance version importance sampling off-policy evaluation per-decision importance sampling pdis overload notation deﬁne batch estimator trajectories batch pdis estimator deﬁned similarly. variance batch estimators reduced weighted importance sampling per-decision weighted importance sampling deﬁne weighted importance weight time trajectory pdwis provided support subset support pdis unbiased potentially high variance estimators. pdwis less variance unweighted counterparts introduce bias. sampled policy pdwis introduce particular form bias. consider sample random variables sample i.i.d. distribution sample compute sample estimate parameter example population mean ﬁnite sample would like specify accuracy without placing restrictive assumptions sampling distribution bootstrapping allows estimate distribution whence conﬁdence intervals derived. starting sample create samples deﬁne notation discrete mdps however results hold continuous replacing summations integrals probability mass functions probability density functions. algorithm bootstrap conﬁdence interval input evaluation policy data trajectories conﬁdence level required number bootstrap estimates input output conﬁdence lower bound bootstrap distribution sample independently sampling replacement. compute distribution approximates distribution allows compute sample conﬁdence bounds. work efron detailed introduction bootstrapping. bootstrapping strong guarantees bootstrap conﬁdence intervals lack ﬁnite sample guarantees. using bootstrapping requires assumption bootstrap distribution representative distribution statistic interest false ﬁnite sample. therefore characterize bootstrap conﬁdence intervals semi-safe\" possibly false assumption. contrast lower bounds concentration inequalities bootstrapped lower bounds thought approximating allowable error rate instead upper bounding however bootstrapping considered safe enough high risk medical predictions practice well established record producing accurate conﬁdence intervals context policy evaluation thomas established bootstrap conﬁdence intervals provide accurate lower bounds high conﬁdence off-policy evaluation setting. primary contribution work incorporate off-policy estimators models bootstrapping decrease data requirements needed produce tight lower bound. section propose model-based weighted doubly robust bootstrapping estimating conﬁdence intervals off-policy estimates. first present pseudocode computing bootstrap conﬁdence lower bound off-policy estimator proposed methods instantiations general algorithm. deﬁne off-policyestimate method takes data trajectories policy generated model value-functions used control variate higher variance pdwis expectation. control variate term expectation zero thus unbiased estimator pdwis statistically consistent estimator intuitively uses information error estimating expected return model lower variance pdwis return. refer reader thomas brunskill jiang in-depth discussion estimators. since estimates shown achieve lower mean squared error several domains propose wdr-bootstrap uses off-policyestimate algorithm although biased statistical consistency property pdwis ensures bootstrap estimates wdr-bootstrap converge correct estimate increases. thus free out-of-class model bias empirical results shown acheive lower domains model converges incorrect model however thomas brunskill also demonstrated situations evaluation efﬁcient acheiving variance pdwis weights high. empirically analyze trade-offs using estimators bootstrapping off-policy conﬁdence bounds. note wdr-bootstrap three options model used estimate control variate model provided model estimated build model every bootstrap data compute practice priori model unavailable computationally expensive build model value function model bootstrap data set. thus experiments estimate single model trajectories value functions single model compute estimate bound provides insight settings mb-bootstrap likely unsuccessful. bound related model bias bounds literature discuss survey related work. defer full derivation appendix section introduce additional assumption ﬁnite. methods proposed paper applicable ﬁnite inﬁnite horizon problems however bias upper bound currently limited episodic ﬁnite horizon setting. expectation importance-sampled kullback-leibler divergence. kl-divergence information theoretic measure frequently used similarity measure probability distributions. result tells bias depends ﬁrst source bias dependent modeling assumptions made. using assumptions lead conservative estimates will practice prevent mb-bootstrap overestimating lower bound. second source bias problematic since even bootstrap model estimates converge different value next section propose bootstrapping recently proposed weighted doublyrobust estimator order obtain data-efﬁcient lower bounds settings model bias large. later present also propose weighted doubly robust bootstrapping combines bootstrapping recently proposed off-policy estimator settings estimator exhibit high bias. estimator based per-decision weighted importance sampling uses model reduce variance estimate. doubly robust estimator origins bandit problems extended jiang ﬁnite horizon mdps. thomas brunskill extended inﬁnite horizon mdps combined weighted weights produce weighted estimator. given model state state-action value functions ˆvπe ˆqπe estimator deﬁned estimator used off-policyestimate algorithm describes simpliﬁed version bootstrapping method presented thomas reward function approximated theoretical results assume known proposed methods applicable assumption fails hold. different distribution trajectories model distribution trajectories seen executing true mdp. since model building techniques build model transitions even transitions come sampled trajectories express theorem terms transitions since unknown impossible estimate terms corollary however approximated common supervised learning loss functions negative likelihood crossentropy. express corollary terms either negative log-likelihood cross-entropy minimize bound observed transitions. case discrete state-spaces approximation upper bounds dkl. continuous state-spaces approximation correct within average differential entropy problem-speciﬁc constant. theorem corollary extended ﬁnite sample bounds using hoeffding’s inequality corollary allows compute upper bound proposed theorem however practice dependence maximum reward makes bound loose subtract lower bound found mb-bootstrap. instead observe characterizes settings estimator exhibit high bias. speciﬁcally estimate bias build model obtains training error negative log-likelihood cross-entropy loss functions error importance-sampled correct difference distribution. result holds regardless whether true transition dynamics representable model class. ﬁrst domain standard mountaincar task literature domain agent attempts drive underpowered hill. cannot drive straight hill successful policy must ﬁrst move reverse another hill order gain momentum reach goal. states discretized horizontal position velocity agent choose accelerate left right neither. time-step reward except terminal state build models done jiang lack data pair causes deterministic transition also previous work importance sampling shorten horizon problem holding action constant updates environment state modiﬁcation changes problem horizon done reduce variance importance-sampling. policy chooses actions uniformly random sub-optimal policy solves task approximately steps. domain build tabular models cannot generalize observed pairs. compute model action value function ˆqπe state value function ˆvπe value-iteration wdr. monte carlo second domain continuous two-dimensional cliffworld point mass agent navigates series cliffs reach goal agent’s state four dimensional vector horizontal vertical position velocity. actions acceleration values horizontal vertical directions. reward negative proportional agent’s distance goal magnitude actions taken ||st ||at||. agent falls cliff receives large negative penalty. domain hand code deterministic policy agent samples sampling behavior policy except greater variance. domain dynamics linear additive gaussian noise. build models ways linear regression regression nonlinear polynomial basis functions. ﬁrst model class choice represents ideal case second case true dynamics outside learnable model class. results refer mb-bootstraplr mb-bootstrappr estimator using linear regression polynomial regression respectively. dynamics mean bootstrap models mb-bootstraplr wdr-bootstraplr quickly converge correct model amount data increases since build models linear regression. hand dynamics mean models mb-bootstrappr wdr-bootstrappr quickly converge incorrect model since regression nonlinear polynomial basis functions. similarly evaluate wdr-bootstraplr wdr-bootstrappr. domain estimate conﬁdence lower bound proposed methods importance sampling bca-bootstrap methods thomas best knowledge methods current state-of-the-art approximate high conﬁdence off-policy evaluation. monte carlo roll-outs domain. domain computed lower bound trajectories varied logarithmically. generate trajectories times compute lower bound method trajectories. mountain cliffworld large number trials required empirical error rate calculations. plotting average lower bound across methods average valid raise average make method appear produce tighter average lower bound fact higher error rate. prior work normalize returns rewards pdis normalizing reduces variance estimator. importantly improves safety. since majority importance weights close zero minimum return zero tends underestimate policy value. preliminary experiments showed without normalization bootstrapping resulted conﬁdent bounds. thus normalize experiments. figure displays average empirical conﬁdence lower bound found method domain. ideal result lower bound large possible subject given statistically consistent method achieve ideal result main point comparison method gets closest fastest. general trend note proposed methods—mb-bootstrap wdr-bootstrap— closer ideal result less data methods. figure displays empirical error rate mb-bootstrap wdr-bootstrap shows approximate allowable error domain. mountaincar methods outperform purely methods reaching ideal result. also note methods produce approximately average lower bound. modelling assumption lack data results transition form negative model bias lowers performance mb-bootstrap. therefore even though eventually converge faster produce good estimates even model inaccurate. negative bias also leads pdwis producing tighter bound small data sets although overtaken mb-bootstrap wdr-bootstrap amount data increases. figure shows mb-bootstrap wdr-bootstrap error rate much lower required error rate figure shows lower bound looser. since mb-bootstrap wdr-bootstrap variance estimators average bound tight error rate. also notable since bootstrapping approximates allowable error rate methods worse data extremely sparse cliffworld ﬁrst note mb-bootstrappr quickly converges suboptimal lower bound. practice incorrect model lead bound high loose here mb-bootstrappr exhibits negative bias converge bound loose. dangerous positive bias make method unsafe. theoretical results suggest bias high evaluating since polynomial basis function models high training error errors importance-sampled correct off-policy model estimation. compute bound section subtract value bound estimated mb-bootstrappr lower bound estimate unaffected bias. unfortunately theoretical bound depends largest possible return rmax thus removing bias straightforward reduces data-efﬁciency gains bias fact much lower. second notable trend also negatively impacted incorrect model. figure wdrbootstraplr starts tight bound increases there. wdr-bootstrappr incorrect model performs figure average empirical lower bound mountain cliffworld domains. plot displays lower bound computed method varying amounts trajectories. ideal lower bound line labelled results demonstrate proposed model-based bootstrapping weighted doubly robust bootstrapping tighter lower bound less data previous importance sampling bootstrapping methods. clarity omit pdis cliffworld outperformed pdwis. error bars two-sided conﬁdence interval. worse pdwis larger using incorrect model decreases variance pdwis term less correct model would still expect less variance tighter lower bound pdwis itself. possibility error estimate model value functions coupled inaccurate model increases variance wdr. result motivates investigating effect inaccurate model state-value state-action-value functions control variate functions certain error continuous setting. lower bound ﬁrst bounding model bias caused error discrete model’s transition function. bound computable error transition bounded inapplicable estimating bias continuous state-spaces. model-based pac-mdp methods used synthesize policies approximately optimal high probability methods applicable discrete mdps require large amounts data. bounds error estimates inaccurate model introduced discrete mdps contrast present bound model bias computable continuous discrete mdps. ross bagnell introduce bound similar corollary model-based policy improvement assume model estimated transitions sampled i.i.d. given exploration distribution since bootstrap trajectories bound inapplicable setting. paduraru introduced tight model bias bounds i.i.d. sampled transitions general mdps i.i.d. trajectories directed acyclic graph mdps made assumptions structure deriving bound. previous work used bootstrapping handle uncertainty texplore algorithm learns multiple decision tree models subsets experience represent uncertainty model predictions white white time-series bootstrapping place conﬁdence intervals value-function estimation policy learning. thomas brunskill introduce estimate model-based estimator’s bias using combination bootstrapping methods related combination bootstrapping none address problem conﬁdence intervals off-policy evaluation. proposed bootstrapping methods incorporate models produce tight lower bounds off-policy estimates. describe advantages disadvantages make recommendations practice. clearly mb-bootstrap inﬂuenced quality estimated transition dynamics. mb-bootstrap build models importance-sampled approximation error expect data-efﬁcient methods. dataefﬁciency comes cost potential bias. theoretical results show bias unavoidable model-class choices. however chosen model-class learned approximation error model bias low. practice model prediction error off-policy evaluation evaluated held subset model fails generalize unseen data another off-policy method preferable. importance-sampling test error gives measure well model estimated trajectories generalize evaluating proposed wdr-bootstrap method provides variance bias method high conﬁdence off-policy evaluation. properties allow wdr-bootstrap outperform different variants sometimes perform well better mbbootstrap. contrast mb-bootstrap wdr-bootstrap achieves data-efﬁcient lower bounds remaining free model bias. since wdr-bootstrap free model bias preferred method model quality unknown domain hard model. figure empirical error rate mountain cliffworld domains. lower bound computed times method count many times lower bound true methods correctly approximate allowable error rate conﬁdence lower bound. concentration inequalities used returns lower bounds off-policy estimates concentration inequality approach notable produces true probabilistic bound policy performance. similar approximate method proposed bottou unfortunately approachs requires prohibitive amounts data shown less data-efﬁcient bootstrapping jiang evaluated estimator safe-policy improvement compute conﬁdence intervals method similar student’s t-test conﬁdence interval shown less data-efﬁcient bootstrapping disadvantage wdr-bootstrap requires model’s value functions known states state-action pairs occur along trajectories continuous state action spaces requires either function approximation monte carlo evaluation. variance either method increase variance estimate. note remains bias free provided ea∼πe ensures control variate term expected value zero even ˆqπe biased estimate policy’s action value function model. trajectory dataset small. bias problematic conﬁdence bounds lower bound exhibit positive bias. bias problem general high conﬁdence off-policy evaluation harmless speciﬁc case high conﬁdence off-policy improvement. setting purpose test decide conﬁdent evaluation policy better behavior policy. fact lower bound still less unsafe policy improvement step avoided. similarly know wdr-bootstrap likely less variance is-based methods less bias mb-bootstrap. general high conﬁdence off-policy evaluation tasks model estimation error high ispdis-based bootstrapping provides safest approximate high conﬁdence off-policy evaluation method. noted normalizing returns rewards important factor using methods safely. since importance weights close zero estimate pulled towards zero corresponds underestimating value. safety critical underestimating preferable overestimating lower bound. experiments used normalization found unnormalized returns high variance used safely bootstrapping. finally settings safety must strictly guaranteed concentration inequalities shown outperform exact methods data available exact methods preferred theoretical guarantees. special cases occur real world high conﬁdence offpolicy evaluation deterministic policies unknown deterministic mb-bootstrap since importance weights equal zero time step chose action deterministic problematic method since produce trajectories lack variety action selection data. also note importance-sampled training error assessing model quality inapplicable setting. unknown occur domain trajectories knowledge policy produced trajectories. example medical domain could data treatments outcomes doctor’s treatment selection policy unknown. setting importance sampling methods cannot applied mb-bootstrap provide conﬁdence interval policy. current literature exists special cases unbiased bounds continuous settings. introduced straightforward novel methods—mbbootstrap wdr-bootstrap—that approximate conﬁdence intervals off-policy evaluation bootstrapping learned models. empirically methods yield superior data-efﬁciency tighter lower bounds performance evaluation policy state-of-the-art importance sampling based methods. also derived bound expected bias learning models minimize error dataset trajectories sampled i.i.d. arbitrary policy. together empirical theoretical results enhance understanding bootstrapping off-policy conﬁdence intervals allow make recommendations settings different methods appropriate. ongoing research agenda includes applying techniques within robotics. robotics off-policy challenges arise data scarcity deterministic policies unknown behavior policies challenges suggest mbbootstrap appropriate robots exhibit complex non-linear dynamics hard model. understanding ﬁnding solutions high conﬁdence off-policy evaluation across robotic tasks inspire innovation applied domains well. acknowledegments would like thank phil thomas matthew hausknecht daniel brown stefano albrecht ajinkya jain useful discussions insightful comments. work taken place personal autonomous robotics learning agents research group artiﬁcial intelligence laboratory university texas austin. pearl research supported part larg research supported part afosr raytheon toyota at&t lockheed martin. josiah hanna supported graduate research fellowship. peter stone serves board directors cogitai inc. terms arrangement reviewed approved university texas austin accordance policy objectivity research. appendix proves theoretical results contained main text. convenience proofs given discrete state action sets. results hold continuous states actions replacing summations states actions integrals changing probability mass functions probability density functions. model bias evaluation behavnlh) log|s a)). thus bound applies maximum likelihood model learning. continuous domains transition function probability density function entropy negative negative log-likelihood crossentropy loss functions always bound model bias. case bound approximates true bias bound within constant. finite sample bounds proof. corollary follows applying hoeffding’s inequality theorem expanding terms samples done derivation corollary drop logarithm terms contain unknown functions. dropping terms equivalent expressing corollary terms cross-entropy negative log-likelihood loss functions. proof. theorem follows lemma importancesampling identity transition probabilities cancel importance weight leaving bounding theorem terms superdipankar patrice simard snelson. counterfactual reasoning learning systems example computational advertising. journal machine learning research intervals when which what? practical guide medical statisticians. statistics medicine pages lloyd chambless aaron folsom richey sharrett paul sorlie david couper moyses szklo javier nieto. coronary heart disease risk prediction atherosclerosis risk communities study. journal clinical epidemiology", "year": 2016}