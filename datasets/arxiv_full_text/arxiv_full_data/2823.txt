{"title": "On the Connection between Differential Privacy and Adversarial  Robustness in Machine Learning", "tag": ["stat.ML", "cs.AI", "cs.CR", "cs.LG"], "abstract": "Adversarial examples in machine learning has been a topic of intense research interest, with attacks and defenses being developed in a tight back-and-forth. Most past defenses are best-effort, heuristic approaches that have all been shown to be vulnerable to sophisticated attacks. More recently, rigorous defenses that provide formal guarantees have emerged, but are hard to scale or generalize. A rigorous and general foundation for designing defenses is required to get us off this arms race trajectory. We propose leveraging differential privacy (DP) as a formal building block for robustness against adversarial examples. We observe that the semantic of DP is closely aligned with the formal definition of robustness to adversarial examples. We propose PixelDP, a strategy for learning robust deep neural networks based on formal DP guarantees. PixelDP networks give theoretical guarantees for a subset of their predictions regarding the robustness against adversarial perturbations of bounded size. Our evaluation with MNIST, CIFAR-10, and CIFAR-100 shows that PixelDP networks achieve accuracy under attack on par with the best-performing defense to date, but additionally certify robustness against meaningful-size 1-norm and 2-norm attacks for 40-60% of their predictions. Our experience points to DP as a rigorous, broadly applicable, and mechanism-rich foundation for robust machine learning.", "text": "adversarial examples machine learning topic intense research interest attacks defenses developed tight back-and-forth. past defenses best-effort heuristic approaches shown vulnerable sophisticated attacks. recently rigorous defenses provide formal guarantees emerged hard scale generalize. rigorous general foundation designing defenses required arms race trajectory. propose leveraging differential privacy formal building block robustness adversarial examples. observe semantic closely aligned formal deﬁnition robustness adversarial examples. propose pixeldp strategy learning robust deep neural networks based formal guarantees. pixeldp networks give theoretical guarantees subset predictions regarding robustness adversarial perturbations bounded size. evaluation mnist cifar- cifar- shows pixeldp networks achieve accuracy attack best-performing defense date additionally certify robustness meaningful-size -norm -norm attacks predictions. experience points rigorous broadly applicable mechanism-rich foundation robust machine learning. deep neural networks perform exceptionally well many artiﬁcial including safetysecurity-sensitive applications selfdriving cars malware classiﬁcation face recognition critical infrastructure systems robustness malicious attacks important many applications recent years become increasingly clear dnns extremely vulnerable broad range attacks. among possible attacks broadly surveyed adversarial examples adversary ﬁnds small perturbations correctly classiﬁed inputs cause model produce erroneous prediction often adversary’s choosing adversarial examples pose serious threats security-critical applications. classic example adversary attaches small human-imperceptible sticker onto stop sign causes self-driving recognize yield sign. adversarial examples also demonstrated domains reinforcement learning generative models since initial demonstration adversarial examples numerous attacks defenses proposed building another. example model distillation proposed robust defense subsequently broken similarly claimed unlikely adversarial examples fool machine learning models real-world rotation scaling introduced even slightest camera movements. however demonstrated attack strategy robust rotation scaling. back-and-forth clearly advanced state adversarial believe ﬁeld needs rigorous theory-backed foundations defense arms’ race trajectory. recently research provable defenses emerged solution shown scale real-world-sized dnns. paper proposes using differential privacy theory rigorous mathematical foundation defense adversarial examples. somewhat surprisingly observe semantic closely aligned formal deﬁnition robustness adversarial examples. establish relationship formally informally framework randomizing algorithms running databases rows adding noise outputs enforce particular privacy semantic individual rows small sets rows database. privacy semantic enforced small changes database result small bounded changes output distribution. separately robustness adversarial examples deﬁned ensuring small changes input result drastic changes dnn’s predictions thus think dnn’s inputs databases parlance individual features rows observe randomizing outputs dnn’s prediction function enforce small number pixels image guarantees robustness predictions adversarial examples change number pixels. connection expanded standard attack norms including -norms. based dp-robustness connection develop pixeldp ﬁrst strategy learning robust models based theory pixeldp lets models give theoretical guarantees subset predictions regarding robustness adversarial perturbations bounded size. speciﬁcally inputs pixeldp gives robust prediction guarantees inputs vicinity inputs model would given different prediction. pixeldp incorporates architecture additional layer called noise layer randomizes network’s computation bound sensitivity changes input adapt dnn’s training prediction procedures account randomization leverage bounds reason robustness individual predictions. experience implementing pixeldp three image classiﬁcation dnns evaluating networks three datasets suggests that pixeldp’s accuracy benign testing examples state-of-the-art defenses lack formal robustness guarantees accuracy attacked testing examples also matches defenses predictions pixeldp deems robust substantially precise predictions defenses. design space afforded general ﬂexible mature theory large presents numerous untapped opportunities improving robustness future. commit making public code experimental settings. hope prototypes spur exploration large rigorous design space opened pixeldp improve robustness section provides formal framework aligning concepts differential privacy robustness adversarial attacks. prove connection restricted case generalize background theory concerned whether output computation database reveal information individual records database. prevent information leakage randomness introduced computation details individual records hidden randomness certain sense. here parameters quantify strength privacy guarantee. standard deﬁnition metric hamming metric simply counts number entries differ databases. small values standard guarantee implies changing single entry database cannot change output distribution much. shall discuss different metrics correspond norms model viewed function mapping inputs typically vector numerical feature values output. multiclass classiﬁcation tasks output label possible labels; regression tasks output real number. work focus multiclass classiﬁcation models good image classiﬁcation type task research adversarial examples focuses. discuss extensions regression models formally multiclass classiﬁcation model function maps n-dimensional inputs label possible labels. models often predict probability distribution possible labels given input return label highest probability i.e. argmaxy∈y adversarial examples background adversarial examples class attack models particularly studied context deep neural networks multiclass image classiﬁcation. attacker constructs small change given ﬁxed input wildly changes predicted output. notationally input denote adversarial version input change perturbation introduced attacker. vector pixels i’th pixel image change i’th pixel. natural constrain amount change attacker allowed make input simplicity measure p-norm change denoted p-norm deﬁned maxi|αi|. also commonly used -norm small -norm attack permitted arbitrarily change entries input; example attack image recognition system self-driving cars based putting sticker ﬁeld vision attack small p-norm attacks larger values require changes pixels small aggregate sense changes spread many features. change lighting condition image correspond attack latter attacks generally considered powerful easily remain invisible human observers. p-norm ball radius given classiﬁcation model ﬁxed input attacker able craft successful adversarial example size given p-norm attacker thus tries small change change predicted label. attacks dichotomized follows untargeted attacks adversary tries change predicted label label targeted attacks adversary tries change predicted label particular label targeted attacks generally challenging achieve untargeted attacks although completely undefended networks shown highly vulnerable types attacks robustness deﬁnition intuitively predictive model regarded robust output insensitive small changes plausible input encountered deployment. formalize notion ﬁrst must establish qualiﬁes plausible input. difﬁcult literature adversarial examples settled deﬁnition. instead model insensitivity input changes typically assessed inputs test data used model training work shall also adopt approach. next must establish deﬁnition sensitivity small changes input. model insensitive robust attacks p-norm given input multiclass classiﬁcation model based label probabilities equivalent equations provide link robustness adversarial examples. regard feature values input records database regard probability distribution output randomized algorithm input range -pixellevel differentially private -pixeldp) model satisﬁes formally equivalent standard deﬁnition terminology emphasize context apply deﬁnition. proposition suppose satisﬁes -pixeldp respect -norm metric input ﬁrst inequality obtain lower-bound second inequality obtain upper-bound hypothesis proposition statement implies lower-bound higher upper-bound turn implies condition equation robustness based preceding connection formulate pixeldp learning strategy generic broadly applicable approach increase robustness models. high level strategy techniques ensure learned model sensitive small changes input. focusing models image classiﬁcation fig. shows example three-layer architecture changes introduced network pixeldp strategy changes summarized introduction noise layer randomizes enforce -pixeldp changes training prediction procedures account added noise. overview pixeldp dnn’s functioning detail current design layer procedures probabilities also determined using standard randomized. nevertheless still train randomized architecture standard dnns trained notable modiﬁcations. describe training procedure robustness properties afforded pixeldp architecture concern probability distribution output given input potentially complex nature layers following noise layer difﬁcult compute output probabilities. therefore resort monte carlo methods estimate probabilities prediction time; probabilities used reason robustness predictions shown components pixeldp strategy noise layer training procedure prediction procedure present rich design space. subsequent sections describe design options experience date give recommendations navigate choices section gives sense broader design space affords robustness adversarial examples. noise layer noise layer enforces pixeldp inserting noise inside using well-known mechanisms laplacian gaussian mechanisms. rely upon sensitivity pre-noise layers sensitivity function deﬁned maximum change output produced small change input given distance metrics input output assuming compute sensitivity prenoise layers noise layer leverages laplace gaussian mechanisms follows. every invocation network input noise layer computes coordinates independent random variables noise distribution fig. pixeldp architecture. blue original dnn. noise layer provides guarantees. noise added inputs following layers distribution rescaled sensitivity computation performed layer noise layer. predictions multiple noise draws measure noiseinduced probability distribution labels. deﬁnes images probability distributions labels pixeldp turns randomized mechanism that upon input returns random label arbitrary randomization applied mechanism remain sensitive small input changes reduce sensitivity pixeldp alters computation initial layers satisfy -pixeldp regard subsequent computation randomized computation post processing output. detail suppose computation number initial layers label subsequent computation produces replace randomized function satisﬁes -pixeldp. concretely achieve introducing noise layer adds zero-mean noise output standard deviation proportional sensitivity obtain randomized function which post-processing property also satisﬁes -pixeldp. overall randomized mechanism returns random label upon input property small changes input image change distribution outputted label much resulting pixeldp architecture nonity analysis i.e. computing pre-noise function case process depends choose place noise layer dnn. recall post-processing property ensures dnn’s ﬁnal output remains -pixeldp independent noise inserted. gives great ﬂexibility placing noise layer opens space possibilities. full investigation design space beyond scope paper experience several options reveals placing noise layer earlier network simpliﬁes sensitivity analysis pre-noise function. option noise image. straightforward placement noise layer right input layer equivalent adding noise individual pixels image. case makes sensitivity analysis trivial identity function construction attack bound. option noise first layer. another option place noise ﬁrst hidden layer usually simple standard many dnns. example image classiﬁcation networks often start convolution layer. cases dnns start layer linear operations followed non-linearity rectiﬁed linear unit commonly used. standard initial layers analyzed sensitivity computed follows. linear layers consist linear operator matrix form sensitivity matrix norm deﬁned supxxp≤w indeed deﬁnition directly implies wpqxp. linearity construction attack bound directly that convolution layer linear usually explicitly expressed matrix form abstractly reshape input rndin vector input size number input channels write convolution rndout×ndin matrix column ﬁlter maps corresponding given input channel zero values. column convolution formed coefﬁcients kernel correspond single input channel. reshaping input change sensitivity. option noise deeper network. imagine adding noise later network using fact applying functions have instance relu sensitivity linear layer followed relu bound sensitivity linear layer alone. combination property leveraged bound paper provides bounds several types layers. however approach sensitivity analysis difﬁcult generalize. layers batch normalization popular image classiﬁcation networks appear amenable bounds. another example input transformation commonly used image processing called image standardization makes image mean zero unit variance transformation makes sensitivity depend input attack since attacker reduce variance image thus making denominator smaller. approach. based experience recommend adding noise early network bounding sensitivity easy taking advantage dp’s post-processing property carry sensitivity bound network. evaluates compares designs noise input noise ﬁrst convolution layer reveals interesting accuracy/robustness tradeoffs. leave full examination tradeoffs placements future work. training procedure achieving pixeldp guarantees requires adding noise prediction time achieving good accuracy requires incorporating noise layer upon training well. loss function optimization algorithms stochastic gradient descent original network. however unless noise directly image training noise raises signiﬁcant challenge. magnitude added noise scales sensitivity layers preceding noise layer optimization procedure must made aware effect parameters sensitivity. propose approaches dealing challenge. option optimize sensitivity. option optimization balance competing goals making better predictions keeping prenoise layers’ sensitivity small. this leverage reparametrization trick denote parameters pixeldp drawn pixeldp noise using laplace mechanism. denote sensitivity highlight dependency parameters ﬁrst part dnn. perform updates need estimate following gradient usual monte carlo estimate gradient number draws often exhibits high variance practical. reparametrization trick computes ∇w∆wg estimate approximately pqzl) leverages gradient distribution’s scale tends lower variance. like laplace gaussian distributions rewritten hence similarly amenable reparametrization. option bound sensitivity. second option force pre-noise sensitivity smaller constant noise distribution’s standard deviation entire process. since case noise distribution ﬁxed avoid problem described previous paragraph. normalize columns linear layers regular optimization process ﬁxed noise variance. projection step described gradient step sgd. makes pre-noise layers parseval tight frames enforcing pre-noise layers thus alternate step ﬁxed noise variance projection step. subsequent layers original left unchanged. approach. options always available. example bounding requires complex projection described methods always available. hand computing slow since bounding method exists using ﬁrst method describe seems expensive comparison. methods exist approach choose empirical comparison ﬁrst method ﬂexible since allows optimization method scale sensitivity impose constraints. conversely bounding sensitivity often impose stronger constraints strictly necessary imposes singular values keeping highest singular value sufﬁcient) reduce complexity optimization impose regularization helps practice. prediction procedure others proposed adding noise training procedure increase robustness adversarial examples know published work incorporates noise prediction procedure. incorporating noise prediction procedure achieving formal robustness guarantees pixeldp. fig. shows pixeldp prediction procedure differs traditional procedure ways. typical network prediction procedure network given input obtain probability distribution labels apply argmax function select label highest probability. pixeldp noise layer induces probability distribution labels prediction procedure changes follows. first given input base prediction multiple runs network draw laplacian/gaussian noise. multiple runs part monte carlo computation empirically estimate noise-induced distribution labels. runs network’s argmax predictions construct histogram many times noisy outputs label. error bounds histogram account measurement error. treat label binomial variable compute one-sided clopperpearson interval apply bonferroni correction number labels bounds valid time. resulting histogram denoted accurately estimates noiseinduced label distribution second instead giving prediction every input pixeldp yields robust predictions. this implements robustness test fig. uses bounds compute much adversary hope change label probability given maximum attack size bounds overlap attack within size able change highest probability prediction deemed robust respect attack size argmax label histogram returned. robustness test given following proposition proposition suppose satisﬁes -pixeldp respect changes size p-norm metric using notation proposition respectively upper lower bound interval input contrary typical settings protecting pixels’ privacy bounding changes output distribution. thus sound multiple noise draws estimate full output distribution. oretical defense goal construct network change mind prediction time actual attack sizes permissible deem predictions robust course training defend tiny attack size imposing high prediction robustness threshold predictions result pixeldp making zero robust predictions hence parameters must somewhat aligned. approach. practice thresholded robustness test provides ﬂexibility deciding attack bounds acceptable given network trained ﬁxed value. shows robustness larger attacks correlates higher precision hence operators prefer choosing higher values hand higher values imply fewer number predictions made model moreover ability make many robust predictions ﬁxed much depends number noise draws performed prediction time turn impacts prediction performance. fewer noise draws means greater measurement error bounds hence looser pixeldp bounds fewer robust predictions especially higher prediction robustness threshold noise draws means slower prediction times. shows practice hundreds draws sufﬁcient retain large fraction predictions even fairly large robustness thresholds. extensions section described initial design pixeldp case models image classiﬁcation. focused design directions experience. however believe idea using general mature theory basis robust much broader applicability opens vaster design space explored future. first general theory believe pixeldp learning strategy much broadly applicable description implies. requirement changes output intermediate layer adversarial input changes bounded -norm. general speciﬁc dnns image classiﬁcation problems. example real-valued outputs also handled discrete-valued inputs furthermore take advantage plausible structural restrictions adversary. example certain parts input secure adversarial manipulation sensitivity layer need determined respect changes unsecure parts. existing defenses adversarial examples boast broad applicability built upon hacks tricks speciﬁc image dnns leverage classiﬁcation speciﬁc optimization fig. robustness test example. histograms noiseinduced label distributions three-label values error bars show measurement error bounds adversarial change. left -pixeldp. prediction robust lower-bound argmax prediction overlaps upper-bound right pixeldp. bounds overlap hence prediction robust. case able change prediction. prediction image robust. fig. illustrates values histograms constructed prediction procedure estimate label distribution corresponding measurement error bars bounds much attacker hope change probability label perturbing input particular attack size given another complementary measure robustness prediction. instead looking binary signal whether prediction robust attack size compute maximum attack size robust individual prediction. predictions smaller others bigger. recall mechanisms noise standard deviation grows equation express linear function given standarddeviation used train predict solve maximum prediction robust lmax maxl∈r+ either ∆mult prediction robust attacks maximum size lmax. fig. original prediction robust attacks original size robust maximum robustness size between. thus implement robustness test establishing threshold hope protect practice returning true lmax call prediction robustness threshold inference-time parameter differ construction attack bound parameter used conﬁgure standard deviation noise added training prediction. separation parameters gives engineers ability thetable design space explored pixeldp dnns. implement defenses different attack bound norms requiring different mechanisms. noise either image ﬁrst convolution. latter case explore bounding sensitivity using reparametrization trick efﬁcient training. second literature mature point includes many techniques optimizations present rich design space pixeldp. used basic mechanisms using worst-case sensitivity analysis hence amount noise conservative. promising alternative build upon ﬁne-tuned technique smooth sensitivity uses data-dependent sensitivity bounds. less noise easy defend inputs potentially improving overall accuracy. defense major concerns pixeldp much defense affects accuracy benign unattacked samples whether defense effective increasing accuracy faced maliciously crafted samples. pixeldp concern repeated invocations network prediction time affect prediction latency. describing methodology evaluating aspects address concern turn methodology datasets baselines. evaluate pixeldp image classiﬁcation tasks three standard datasets listed table mnist consists greyscale handwritten digits easiest classify. cifar- cifar- consist small color images centered object classes respectively. dataset existing architectures train high performing classiﬁer. accuracy original networks held-out testing also shown table constitute baselines evaluation. mnist train convolutional neural network convolutions stride ﬁlters respectively. convolutions followed fully connected layer nodes softmax. regularizer weight decay rate trained momentum optimizer learning rate steps learning rate additional steps. cifar datasets state-of-the-art wide residual network speciﬁcally tensorﬂow implementation wide resnet default parameters including optimizer learning rate schedule. suggested documentation steps cifar- steps cifar-. change make remove image standardization step. common step image processing makes sensitivity input dependent removed pixeldp models. interestingly removing step also increases baseline’s accuracy kept change. pixeldp models. make preceding networks pixeldp regards -norm -norm bounded attacks. table shows different noise layers used explore design space. laplace mechanism sensitivity gaussian projection step bound sensitivity implement pixeldp regards -norm bounded attacks noise ﬁrst convolution. projection parameter resnets mnist. case sensitivity exactly actually turn quite larger. since noise standard deviation computed sensitivity downscale accordingly. method. measure various accuracy-related metrics samples held-out testing evaluate baseline undefended models. evaluate impact defense benign samples samples testing set. evaluate accuracy defense malicious samples state-of-the-art attack randomly picked samples testing evaluate pixeldp improves accuracy perturbed versions samples compared undefended networks networks defended bestperforming prior defense recent madry defense method falls best effort category shown withstand even recent attacks broke known defenses evaluation goal accuracy bening samples accuracy malicious samples performance explore multiple combinations design choices described table speciﬁes various conﬁgurations chosen best reﬂect tradeoffs space. conﬁguration also vary prediction robustness threshold measuring minimum attack size prediction must robust construction attack bound measuring attack size constructed defend against. higher values correspond increased levels noise since noise’s standard deviation depends experiments laplacian/gaussian noise drawn ﬁxed distribution scaled parameter accuracy metrics. typical metric evaluate traditional image classiﬁcation accuracy deﬁned fraction images correctly classiﬁes given testing set. contrast traditional dnns pixeldp dnns provide reason robustness individual predictions respect attacks desired size given prediction robustness threshold feature used applications take action predictions reach desired level robustness. cases accuracy entails correctness also robustness predictions. pixeldp dnns thus evaluate three metrics i=&isrobust) isrobust thresholded robustness test label histogram corresponding input robust accuracy denotes fraction testing pixeldp’s predictions correct robust given prediction robustness threshold denotes fraction predictions pixeldp deems robust threshold also correct. systems choose actuate pixeldp’s robust predictions metric especially important measures percentage correct predictions actions taken. predictions robust robust accuracy robust precision equivalent conventional accuracy. evaluation focus. evaluation shows small snapshot investigation did. example experimented multiple dnns dataset -norm -norm attacks focus dataset -norm attacks appropriate specify substantial changes results -norm networks. speciﬁc cases ∞-norm attacks speciﬁcally compare madry defended trained attacks. finally three datasets focus evaluation cifar- madry uses. evaluation section generalize main conclusions mnist cifar-. accuracy benign samples evaluate pixeldp behaves benign inputs using testing set. questions various noise levels impact network’s accuracy robust predictions? various design choices impact accuracy robust predictions? impact noise. evaluate impact noise added meet particular construction attack bound conventional accuracy robust accuracy robust precision. construct four cifar- resnets satisfying -pixeldp regard -norm bounded attacks four values .... higher values correspond larger noise standard deviation fig. shows robust accuracy robust precision networks robustness assessed growing threshold points axis show conventional accuracy. focusing conventional accuracy points fig. constructing network larger attacks progressively degrades accuracy. resnet least noise reaches accuracy compared baseline increasing noise levels yields respectively. defenses norm bounded attacks show trend accuracy higher attacks weaker thus easier defend against. sequence values accuracy respectively. hand looking robust accuracy lines fig. observe prediction robustness threshold increases inversion dependency accuracy noise appears pixeldp networks constructed larger attacks conﬁgured give robust predictions tend yield accurate predictions. example resnet constructed highest robust accuracy resnet fig. robust metrics construction attack bound prediction robustness threshold conﬁg cifar- resnet -norm bounds. left conventional accuracy robust accuracy conventional accuracy close baseline low-noise networks robust accuracy increases high-noise networks conﬁgured yield robust predictions right robust precision increases prediction robustness threshold meaning robust predictions tend better. however network makes fewer predictions grows. constructed becomes better past threshold. similarly resnet higher robust accuracy resnet -norm prediction robustness threshold. before observations -norm defenses consistent observation. fig. shows robust precision systematically increases prediction robustness threshold. time fraction robust predictions decreases threshold. implies yielding robust predictions tends increase precision expense recall. example prediction robustness threshold resnet makes robust predictions images reaching precision. resnet needs threshold beat baseline achieving accuracy predictions examples. threshold lowest-noise resnet precision since robustness linked network predicting higher probability correct label result suggests pixeldp probabilities good indicator correctness. thus noise introduces multi-parameter tradeoff accuracy robustness. building noise network hurts accuracy. however building noise network requiring yield robust predictions enhances quality predictions even compared baseline undefended networks. hand requiring network yield robust predictions lowers number predictions make. next examine design choices impact accuracy robust predictions. noise image first layer. show impact noise layer placement. fig. shows adding noise ﬁrst convolution opposed directly image gives slightly higher conventional accuracy cost lower robust accuracy higher prediction robustness thresholds. noise stronger tradeoff. example adding noise ﬁrst convolution brings conventional accuracy less robust attacks -norm bigger hand adding noise image reduces accuracy gives higher robust accuracy attacks -norm percentage points increase percentage points fig. shows effect resnet making higher number robust predictions given threshold results higher robust accuracy worse robust precision. depending use-case designs advantageous. optimizing bounding sensitivity training. interesting question happens exclude pixeldp’s noise layer training. pixeldp’s robustness guarantees depend randomizing predictions. evaluated na¨ıve design conventional accuracy network dismal even tiny amounts noise added prediction. hence adds noise prediction must training too. presented design options effectively optimizing bounding pre-noise sensitivity adding noise training without either optimizations also results poor performance. however least cifar choice proposed techniques appear signiﬁcant robustness threshold mechanisms’ robust accuracies differ percentage point. choose based convenience. datasets. experiments cifar- mnist models omit graphs space reasons. main conclusion making robust hurts conventional accuracy enhances quality robust predictions holds cases. cifar- -norm bounded attacks using yields accuracy accuracy compared baseline respective robust precision threshold prefig. adding noise image ﬁrst convolution. conﬁg cifar- resnet -norm bounds. concl adding noise ﬁrst convolution better robustness thresholds; adding noise image better higher thresholds. dictions dataset. however neither model makes robust predictions showing robust accuracy drops fast attack threshold. adding noise image level degrades accuracy robust precision improves robust accuracy. accuracy robust accuracy still mnist handle higher noise levels. accuracy slightly beating baseline. robust accuracy remains high dropping robust precision quickly reaches predictions testing set. accuracy longer tail robust accuracy. still bellow lower noise accuracy still robust accuracy. robust precision also goes predictions data. accuracy malicious samples standard method evaluate strength defense space measure accuracy defended model malicious samples obtained running state-of-the-art attack samples held-out testing apply method measure conventional accuracy also robust metrics defense uniquely affords. questions pixeldp’s accuracy attack compare state-of-the-art defenses? pixeldp’s accuracy attack scale larger datasets? attack methodology. projected gradient descent attack recently state-of-the-art. attack whitebox setting attacker access parameters hyperparameters attacked network except seed noise generator. attack ﬁrst images datasets’ held-out recent arxiv paper seemingly improves upon attack learned days prior submission. plan evaluate however attack appears satisfy principles rigorous attack formulated paper. testing set. quantify conventional accuracy under attack counting correct adversarial samples still classiﬁed ground truth label. quantify robust metrics similarly. customary methodology execute bounded control maximum size perturbation attack produce samples. denote bound empirical attack bound treat parameter. measure lattack either -norm ∞-norm. although defense currently support ∞-norm attacks state-of-the-art defense compare designed that hence apples-toapples comparison perform types attacks pixeldp madry networks. report pixel range -norm attack size pixel range corresponds -norm attack size pixel range; ∞-norm attack size pixel range corresponds ∞-norm attack size pixel range. appendix contains important lower-level details methodology. madry defense background. evaluate accuracy under attack increasing empirical attack bound compare accuracy state-of-the-art models provided madry cifar- models developed context ∞-norm attacks efﬁcient adversarial training strategy learn benign malicious samples attacks methodology best effort supports formal notion robustness individual predictions pixeldp. however madry models best-performing ones still unbroken recent attacks represent good comparison point pixeldp. fair comparison evaluate conventional accuracy robust metrics also evaluate -norm ∞-norm attacks. accuracy attack compared madry. fig. compares conventional accuracy pixeldp model madry model empirical attack bound fig. conventional accuracy attack pixeldp madry. conﬁg cifar- models. pixeldp model trained using -norm noise ﬁrst convolution. madry model optimized adversarial training using ∞-norm attacks left -norm bounded attacks. right ∞-norm bounded attacks. concl pixeldp madry -norm attacks; madry outperforms pixeldp ∞-norm attacks. fig. pixeldp robust precision madry accuracy under attack. conﬁg cifar- resnets -norm attack. models fig. madry’s robust precision equivalent conventional accuracy. concl pixeldp makes fewer correct predictions certain attack size. bound increases -norm ∞-norm attacks. models identical structurally pixeldp model conﬁgured noise ﬁrst convolution construction attack bound -norm attacks model achieves conventional accuracy madry. fact slightly better -norm empirical attack bounds corresponds -norm attack pixel range. models dramatically robust attack compared baseline model. ∞-norm attacks model shows much worse conventional accuracy madry. expected since ∞-norm attacks stronger -norm attacks defense designed protect. still encouraging pixeldp provides experimental protection baseline even attacks. robust metrics computed results shown fig. values prediction robustness threshold reﬂect beneﬁt gained application leverage theoretical guarantees ﬁlter non-robust predictions. focusing robust precision graph observe pixeldp’s robust predictions substantially correct madry’s fig. accuracy attack datasets. conﬁg mnist cifar- cifar- resnet -norm attack bounds. concl pixeldp much resilient attack cifar- pixeldp cifar-. predictions empirical attack bound conservative robustness test correct pixeldp’s predictions although makes fewer example selecting leads robust precision attack sizes even higher baseline network benign samples whose accuracy however prediction rate case extremely less conservative robustness thresholds pixeldp’s robust predictions still correct madry’s predictions retain much meaningful fraction predictions attack size interval. thus applications afford minority predictions pixeldp’s robust predictions -norm attack substantially precise madry’s. applications need every prediction pixeldp offers on-par accuracy -norm attack madry’s interestingly ﬁrst conclusion holds ∞-norm attacks second not. datasets. fig. shows conventional accuracy under attack baseline pixeldp model dataset. remark range empirical attack bound signiﬁcantly extended compared previous graphs pixeldp cifar- model significantly resilient pixeldp cifar- models. mnist attacks -norm baseline pixeldp models perform similarly attack however pixeldp model provides level protection larger-size attacks. cifar- baseline model extremely vulnerable attack accuracy drops zero attack -norm pixeldp model starts lower accuracy no-attack case accuracy increasing attack sizes downgrades gracefully remaining attack sizes norm even attack -norm pixeldp retains accuracy. result surprised investigated enhanced versions attacks speciﬁcally crafted break pixeldp cifar- pixeldp showed resilience cases. performance overheads ﬁnal concern pixeldp’s computational overhead training prediction. pixeldp adds little overhead training additions random noise tensor sensitivity computations. cifar- resnet baseline takes average training step. pixeldp versions take training step pixeldp impacts prediction substantially since uses multiple noise draws estimate probabilities labels. making prediction single image noise draw takes average. making draws brings requires found draws often necessary properly assess robustness implying prediction time seconds overhead. parallelizable resource consumption still problem. tighter measurement error bounds future answer. fense mechanism. several defenses proposed detecting adversarial examples leveraging statistical differences clean examples. approaches input transformations randomization obfuscate gradients leverage generative models best-effort defenses broken sometimes months publication empirical defense still holds madry based adversarial training compare evaluation. randomization defenses sound similar pixeldp major differences randomizing training prediction arbitrary way. pixeldp draws guarantees randomizing prediction enforce rigorous semantic. rigorous defenses. recent line work explores formal veriﬁcation safety properties unfortunately none techniques scale complex/large dnns; stands contrast pixeldp scales real-world-sized dnns without causing signiﬁcant performance drop. closer work cisse proposed parseval networks speciﬁc type whose lipschitz constant designed less bounding sensitivity -norm attacks. could train end-to-end parseval network perform direct comparison; training procedure never converges non-trivial accuracy. published results show faster degradation accuracy compared pixeldp dnns despite weaker one-step attacks. differential privacy previous work studies generalization properties shown learning algorithms satisfy respect training data statistical beneﬁts terms out-of-sample performance. learning algorithm rather predictor learn satisﬁes respect atomic units given test point. attacks. since initial adversarial example attacks dnns szegedy ﬂurry attacks aimed every tentative defense mechanism. attacks used various norms quantify power given attacker -norm -norm -norm ∞-norm stateof-the-art attacks include method evaluation plus recent method defenses obfuscating gradients best-effort defenses. arms race attackers defenders used multiple heuristics increase robustness dnns least empirically. example papernot proposed model distillation technique traditionally used simplifying dnns dedemonstrated connection robustness adversarial examples machine learning differential privacy theory. showed connection leveraged develop rigorous defense attacks effective defending attacks today’s state-of-the-art best-effort defense. however defense additionally provides robustness certiﬁcation meaningful subset predictions. compared certiﬁed defenses believe defense presents unique properties including broad applicability scalability complex fairly large dnns datasets rich design space whose exploration likely lead even effective defenses.", "year": 2018}