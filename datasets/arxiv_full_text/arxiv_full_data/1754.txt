{"title": "Order Matters: Sequence to sequence for sets", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "Sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences. In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences. For instance, it is not clear how to input a set of numbers into a model where the task is to sort them; similarly, we do not know how to organize outputs when they correspond to random variables and the task is to model their unknown joint probability. In this paper, we first show using various examples that the order in which we organize input and/or output data matters significantly when learning an underlying model. We then discuss an extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way. In addition, we propose a loss which, by searching over possible orders during training, deals with the lack of structure of output sets. We show empirical evidence of our claims regarding ordering, and on the modifications to the seq2seq framework on benchmark language modeling and parsing tasks, as well as two artificial tasks -- sorting numbers and estimating the joint probability of unknown graphical models.", "text": "sequences become ﬁrst class citizens supervised learning thanks resurgence recurrent neural networks. many complex tasks require mapping sequence observations formulated sequence-to-sequence framework employs chain rule efﬁciently represent joint probability sequences. many cases however variable sized inputs and/or outputs might naturally expressed sequences. instance clear input numbers model task sort them; similarly know organize outputs correspond random variables task model unknown joint probability. paper ﬁrst show using various examples order organize input and/or output data matters signiﬁcantly learning underlying model. discuss extension seqseq framework goes beyond sequences handles input sets principled way. addition propose loss which searching possible orders training deals lack structure output sets. show empirical evidence claims regarding ordering modiﬁcations seqseq framework benchmark language modeling parsing tasks well artiﬁcial tasks sorting numbers estimating joint probability unknown graphical models. deep architectures shown last years often yield state-of-the-art performance several tasks ranging image classiﬁcation speech recognition recently recurrent neural networks variants long short term memory network proposed hochreiter schmidhuber shown similar impressive performance several inherently sequential tasks. examples range machine translation image captioning speech recognition constituency parsing learning compute approaches follow simple architecture dubbed sequence-to-sequence input read completely using encoder either lstm input sequence convolutional network images. ﬁnal state encoder decoder lstm whose purpose produce target sequence token time. data naturally organized sequence sequence-to-sequence framework well suited. example chain rule used decompose joint probability sequences words implemented lstm without making conditional independence assumption. represent data either inputs outputs problems obvious order cannot determined? instance encode numbers task sort them? alternatively output detected objects image speciﬁc known order among them? priori choice ordering data presented model matter? purpose paper two-fold. first show even natural order known among input output objects might still yields better performance hence order matters. second propose approaches consider sets either inputs and/or outputs models evaluate perform various artiﬁcial real datasets. since sequence-to-sequence models proposed machine translation research community proposed several applications models perform mappings and/or sequences. example image captioning maps image sentence parsing maps sentence parse tree models computation problem statements solutions traveling salesman problem tour points scope paper review successful applications seqseq list already includes non-trivial examples mapping to/from objects necessarily sequences. recently many related models contributions proposed utilize concept external memories including rnnsearch memory networks neural turing machines element models utilize reading mechanism read external memories fully differentiable unlike traditional structured prediction algorithms approach relies chain rule serialize output random variables strong capabilities lstm networks model long-term correlation. similarly want assume known structured input done instance recursive neural networks encode sentences recursively trees. consider generic supervised task given training pairs pair input corresponding target. sequence-to-sequence paradigm corresponds tasks represented sequences possibly different lengths case reasonable model example using conditional probability chain rule decompose follows chain rule makes approach assumption free input corresponds sequence reasonable read sequentially however encode correspond naturally sequence? instance corresponds unordered elements? sequence like cats becomes although unnecessary cases argue that even sequences inputting and/or outputting different order could beneﬁcial. example sorting want employ divide-and-conquer strategy ﬁnds median element ﬁrst following sections discuss extend seqseq handle input sets output sets also show importance ordering variety tasks seqseq successfully applied include experimental results support claims extensions existing models. ﬁrst study extensions encoding sets. discussed previous section sequences read recurrent neural network compress contents vector. important invariance property must satisﬁed input swapping elements alter encoding. simple approach satisﬁes this fact commonly used encoding sentences bag-of-words approach. case representation simply reduction counts word embeddings similar embedding functions naturally permutation invariant. language domains naturally sequential replaced complex encoders recurrent neural networks take order account model higher order statistics data. unsatisfying property using reduction operation makes representation quite inefﬁcient model operates ﬁxed dimensional embedding regardless length set. unlikely representation succeed amount memory required encode length increase function thus argue even deep convolutional architectures suffer limitation though modiﬁcations exist section highlight prior work observed order inputs impacted performance seqseq models taking sequences input. principle order matter using complex encoder recurrent neural network universal approximators encode complex features input sequence believe reason order seems matter underlying non-convex optimization suitable prior. ﬁrst example experimented altering order sequences context machine translation. machine translation mapping function encodes sentence source language decodes translation target language reversing order input english sentence sutskever bleu score improvement allowed close model fully end-to-end model machine translation state-of-the-art models highly engineered. similarly constituency parsing mapping english sentence ﬂattened version constituency parse tree absolute increase score observed reversing english sentence furthermore preprocess data e.g. convex hull computation presented vinyals sorting points angle task becomes simpler result models obtained much faster train better note deﬁne ordering independent input sequence also ordering input dependent distinction also applies discussion output sequences sets section recent approaches pushed seqseq paradigm adding memory computation models allowed deﬁne model makes assumptions input ordering whilst preserving right properties discussed memory increases size order invariant. next sections explain modiﬁcation could also seen special case memory network neural turing machine computation depicted figure neural models memories coupled differentiable addressing mechanism successfully applied handwriting generation recognition machine translation general computation machines since interested associative memories employed content based attention. property vector retrieved memory would change randomly shufﬂed memory. crucial proper treatment input such. particular process block based attention mechanism uses following figure read-process-and-write model. indexes memory vector query vector allows read memories function computes single scalar lstm computes recurrent state takes inputs. state lstm evolves formed concatenating query resulting attention readout index indicates many processing steps carried compute state decoder. note permuting effect read vector process block lstm without inputs outputs performing steps computation memories lstm keeps updating state reading repeatedly using attention mechanism described previous section. block hidden state embedding permutation invariant inputs. eqs. details. points elements step time. original work vinyals used pointer mechanism which instead issuing readout memory weighted soft pointer uses pointer part loss. extended adding extra attention step pointer related process block described above difference attention reads happen interleaved pointer output. described later results found mechanisms complement other. architecture depicted figure seen special case neural turing machine memory network. satisﬁes property invariant order elements thus effectively processing inputs set. also note write component could simply lstm outputs ﬁxed dictionary. model though study combinatorial problems outputs pointers inputs pointer network. order verify model handles sets efﬁciently vanilla seqseq approach following experiment artiﬁcial data task sorting numbers given unordered random ﬂoating point numbers return sorted order. note problem instance setseq. used architecture deﬁned figure read module small multilayer perceptron number process module attention mechanism read numbers implemented steps lstm input output attending input embeddings followed lstm produce indices input numbers pointer network proper sorted order. also compared architecture vanilla seqseq architecture made input lstm connected output lstm produces indices input numbers pointer network note difference models encoding using either lstm architecture proposed previous section. multiple experiments varying number numbers sort well number processing steps read process write model. out-of-sample accuracies experiments summarized table baseline pointer network lstm input model better read-process-and-write model processing steps used soon least processing step allowed performance read-process-and-write model gets better increasing number processing steps. also that size task grows performance gets worse expected. also note processing steps glimpses writing module effectively unconditioned blindly point elements thus unsurprising performing worse model considered table lastly equipping writing module glimpses improves baseline model proposed modiﬁcation quite signiﬁcantly table sorting experiment out-of-sample sorting accuracy various problem sizes processing steps without glimpses. reported accuracies shown reaching training iterations point models converged none overﬁtted. higher better. considered problem encoding input sets; turn attention output representations. chain rule describes joint probabilities sets random variables perhaps simplest decomposition joint probability incur arbitrary restrictions thus long powerful model trainable exists order work without prior order information underlying problem generated despite this even powerful model like lstm employed output ordering still plays role successfully training models. section study effect ordering performance seqseq models several tasks. namely consider arbitrary orders variables model conditional probability distribution following order training examples. order matters experiment penntree bank standard language modeling benchmark. dataset quite small language modeling standards models data starved. trained medium sized lstms large amounts regularization estimate probabilities sequences words. consider three version dataset three orderings natural reverse ﬁxed -word reversal note -word reversal destroys underlying structure sentence makes modeling joint probability much difﬁcult since many higher order n-grams scrambled. ordering trained different model. results natural reverse matched perplexity development surprisingly -word reversal degraded perplexity points still achieving impressive result corpus perplexity. note however training perplexities also points higher indicates model trouble handling awkward ordering. thus even considering chain rule still properly models joint probability degradation observed confounding ordering chosen. task constituency parsing consists producing parse tree given sentence. model proposed vinyals sentence encoder lstm followed decoder lstm trained generate depth ﬁrst traversal encoding parse tree using attention mechanism. approach matched state-of-the-art results task. even though seemed sensible depth ﬁrst traversal many ways uniquely encode tree onto sequence. thus tried train small model using depth ﬁrst traversal another using breadth ﬁrst traversal figure example tree linearizes traversal schemes. model trained produce depth ﬁrst traversal linearized trees obtained score whereas producing breadth ﬁrst traversal trees much lower score showing importance picking right output ordering. take example outputting indices sorted inputs random numbers indeed deterministic function. choose output indices order treat result possible outputs given perfectly valid. training generated permutations picked uniformly random mapping place equal probability output conﬁgurations input thus formulation much less statistically efﬁcient. previous work found restricting much possible equivalence class outputs always better. instance output tour started lower indexed city followed counter-clockwise ordering. similarly output triangles sorted lexicographical order moved left right. cases improvements absolute accuracy observed. failing restrict output equivalence class generally implies much slower convergence instance sorting considering outputs sets output possible orderings convergence small never reached performance. sentences natural order words gives good clue order random variables model kind data might harder decide furthermore theory order matter bayes rule lets reorder conditional probabilities needed. practice however might order easier model another shown paper. purpose experiment demonstrate using controlled experiment. generated star-like graphical models random variables variable follows unconditional distribution others follow conditional distribution based value head variable. expect easier model joint distribution choosing ordering starts head variable. created several artiﬁcial datasets varying number random variables model training size randomness marginal distributions deterministic peaky were. problem trained lstms mini-batch iterations model joint probability head random variable shown ﬁrst shown last. recall model proposed dealing input sets given embedding inputs generic module able process inputs order. yields embedding satisfying property invariant reorderings whilst generic kinds computations input set. unfortunately placing joint probability random variables structure joint probability function unknown hard problem. fortunately thanks recurrent neural networks apply chain rule decomposes joint probability sequentially without independence assumptions. work focus using chain rule discarding naive decompositions strong unrealistic assumptions obvious drawback chain rule violates argument treating condition random variables particular order. even though principle order matter previous section shown indeed case certain orderings better others variety tasks likely parameterization joint probability non-convex nature optimization problem. proposed solution deal aforementioned drawback extremely simple train model decide best ordering apply chain rule. formally assume exists ordering maximally simpliﬁes task would like train model p|x). number possible orderings large length output best order unknown priori. maxπ computed either naively inexact search. note equation strictly improve regular maximum likelihood framework non-convexity found issue practice. besides scalable found that done naively picking ordering train model would pick random ordering would stuck permanently added ways explore space orderings follows pick ordering sampling according distribution proportional p|x). costs model evaluations crucially sampling p|x) done efﬁciently ancestral sampling requires evaluate instead initial attempt solve considered simpliﬁed version language modeling task described section simpliﬁed task consists modeling joint probability grams without context choice allowed small enough initially trying exactly best ordering possible ones. thus disregarded possible effects inexact search focused essential training dynamics model optimized picks best ordering maximizes current parameters reinforces ordering applying updates gradient w.r.t. parameters. eventually noted section found sampling superior terms convergence whilst simplifying complexity preferred solution used rest section. test framework converted -grams following -gram y=this y=is y=ﬁve y=gram -gram note adding original position alongside words makes set. thus shufﬂe arbitrarily without losing original structure sequence. ﬁrst experiment reinforces result section tests hypothesis order matters. training model follows natural order followed conditioned etc.) achieves validation perplexity instead picking perplexity drops results shown table note that easy case restrict search space orderings order clearly better other. note that pretraining phase decide orderings better represent data model trained. quickly model settles natural ordering yielding perplexity difﬁcult case order possible model settles orders small variations them. cases ﬁnal perplexity thus framework propose able good orderings without prior knowledge. plan recover optimal orderings ones unknown applying seqseq framework naively. lstms shown powerful models represent variable length sequential data thanks ability handle reasonably long term dependencies chain rule efﬁciently decompose joint distributions. hand problems expressed terms unordered elements either input outputs; cases data represented structure needs linearized lstm might ﬁrst goal paper shed light problems indeed show order matters obtain best performance. considered case unordered input data proposed read-process-and-write architecture case unordered output data proposed efﬁcient training algorithm includes search possible orders training inference. illustrated proposed approaches input output sets various experiments sorting graphical models language modeling parsing. would like thank ilya sutskever navdeep jaitly rafal jozefowicz quoc lukasz kaiser geoffrey hinton jeff dean shane google brain team useful discussions topic. also thank anonymous reviewers helped improving paper. hinton deng dahl mohamed jaitly senior vanhoucke nguyen sainath kingsbury deep neural networks acoustic modeling speech recognition. ieee signal processing magazine", "year": 2015}