{"title": "Deep Speaker Vectors for Semi Text-independent Speaker Verification", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "Recent research shows that deep neural networks (DNNs) can be used to extract deep speaker vectors (d-vectors) that preserve speaker characteristics and can be used in speaker verification. This new method has been tested on text-dependent speaker verification tasks, and improvement was reported when combined with the conventional i-vector method.  This paper extends the d-vector approach to semi text-independent speaker verification tasks, i.e., the text of the speech is in a limited set of short phrases. We explore various settings of the DNN structure used for d-vector extraction, and present a phone-dependent training which employs the posterior features obtained from an ASR system. The experimental results show that it is possible to apply d-vectors on semi text-independent speaker recognition, and the phone-dependent training improves system performance.", "text": "problem solved directions. ﬁrst direction employ various discriminative models enhance generative framework. example model gmm-ubms plda model i-vectors approaches provide signiﬁcant improvement baseline. another direction look discriminative features i.e. features sensitive speaker change largely invariant change irrelevant factors phone contents channels however improvement obtained ‘feature engineering’ much less signiﬁcant compared achievements obtained discriminative models plda. possible reason features human-crafted thus tend suboptimal practical usage. recent research deep learning offers idea ‘feature learning’. shown deep neural network task-oriented features learned layer layer features. example automatic speech recognition phone-discriminative features learned spectrum ﬁlter bank energies learned features powerful defeated conventional feature based frequency cepstral coefﬁcients dominated several decades favorable property dnns learning task-oriented features utilized learn speaker-discriminative features well. recent study shows possible least text-dependent tasks authors constructed model training objective discriminate speakers frame speaker-discriminative features read activations last hidden layer. tested method foot-print text-dependent speaker veriﬁcation task experimental results showed reasonable performance achieved dnn-based features although still difﬁcult compete i-vector baseline. paper extend application dnn-based feature learning approach semi text-independent tasks present phone-dependent training involves phone posteriors obtained system training. experimental results show dnn-based feature learning works well text-independent tasks actually even better text-dependent tasks phone-dependent training offers marginal consistent gains. abstract—recent research shows deep neural networks used extract deep speaker vectors preserve speaker characteristics used speaker veriﬁcation. method tested text-dependent speaker veriﬁcation tasks improvement reported combined conventional i-vector method. textindependent speaker veriﬁcation tasks i.e. text speech limited short phrases. explore various settings structure used d-vector extraction present phone-dependent training employs posterior features obtained system. experimental results show possible apply d-vectors semi text-independent speaker recognition phone-dependent training improves system performance. tion important biometric authentication technique widely used verify speakers’ identities. according text allowed speak enrollment test speaker veriﬁcation systems categorized either text-dependent text-independent. textdependent system requires words/sentences spoken enrollment test text-independent system permits words speak. paper focuses semi text-independent scenario words enrollment test constrained limited short phrases e.g. ‘turn radio’. limitation people speak different sentences enrollment test system performance signiﬁcantly deteriorated makes system acceptable practice. successful approaches speaker veriﬁcation based generative models unsupervised learning e.g. famous gaussian mixture model-universal background model framework number advanced models proposed based gmm-ubm architecture among i-vector model perhaps successful. despite impressive success gmmubm model subsequent i-vector model share intrinsic disadvantage unsupervised learning methods work supported national natural science foundation china grant also supported national basic research program china grant authors division technical innovation development tsinghua national laboratory information science technology research institute information technology tsinghua university. paper also supported sinovoice pachira. paper follows work difference extend application dnn-based feature learning approach semi text-independent tasks introduce phone-dependent training. mismatched content enrollment test speech task challenging. model employed speaker veriﬁcation ways. example dnns trained used replace model derive acoustic statistics i-vector model training. used replace plda improve discriminative capability ivectors. methods rely generative framework i.e. i-vector model. dnn-based feature learning presented paper purely discriminative without generative model involved. section presents dnn-based feature learning. ﬁrst describe main structure model learning process propose phone-dependent learning. finally difference i-vector approach dnn-based approach discussed. well-known dnns learn task-oriented features features layer layer. property employed phone-discriminative features learned low-level features fbanks even spectrum shown well-trained variations irrelevant learning task gradually eliminated input feature propagated structure layer layer. feature learning powerful primary fbank feature defeated mfcc feature carefully designed people dominated several decades. property also employed learn speakerdiscriminative features. actually researchers much effort looking features discriminative speakers effort mostly vain mfcc still popular choice. success dnns suggests direction speaker-discriminative features learned data instead crafted hand. learning easily done process rather similar difference speaker veriﬁcation learning goal discriminate different speakers. fig. presents structure used speakerdiscriminative feature learning. following convention input layer involves window -dimensional fbanks. work window size found optimal work. hidden layers consists units. units output layer correspond speakers training data number experiment. -hot encoding scheme used label target training criterion cross entropy. learning rate beginning halved whenever improvement cross-validation found. training process stops learning rate small improvement marginal. trained successfully speakerdiscriminative features read hidden layer. layer close output features speaker-discriminative. experiments show features extracted last hidden layer perform best similar observation test phase features extracted frames given utterance features averaged form speaker vector. following nomenclature call speaker vector ‘d-vector’. similar i-vectors d-vector represents speaker identity utterance speaker space. methods used i-vectors used d-vectors conduct test example computing cosine distance applying plda. speakerdiscriminative feature learning described previous section ‘blind learning’ i.e. features learned data without prior information. means learning purely relies complex deep structure model large amount data discover speaker-discriminative patterns. training data abundant often problem; however tasks limited amount data instance semi text-independent task hand blind learning tends difﬁcult many speaker-irrelevant variations involved data particularly phone contents. possible solution inform phone current input frame belongs simply achieved adding phone indicator input. however often easy phone alignment speech data. alternative phone indicator vector phone posterior probabilities easily obtained phone discriminant model. work choose model trained system produce phone posteriors. fig. illustrates structure phone posterior vector involved input. training process structure change. training involves randomly selected speakers results utterances total. prevent overﬁtting cross-validation containing utterances selected training data remaining utterances used model training including model d-vector approach matrix plda model i-vector approach. evaluation consists remaining speakers. text-dependent experiment evaluation performed particular phrase; semi text-independent experiment utterances evaluation cross evaluated resulting target trials non-target trials. ﬁrst experiment investigates performance dvector approach text-dependent speaker veriﬁcation tasks compare i-vector baseline. similar work reported reproduce work propose improvement leveraging text-independent data. results randomly selected phrases denoted respectively. phrase corresponding utterances selected training train i-vector system d-vector system respectively corresponding utterances evaluation selected perform test. means training data phrase consists utterances test consists utterances. i-vector system number gaussian mixtures i-vector dimension values chosen optimize performance. architecture d-vector system shown section iii. fair comparison dimension d-vector well. tests based three scoring methods basic cosine distance cosine distance reducing dimension score provided plda. table reports results terms equal error rate seen d-vector system obtains reasonable performance however results much worse ivector system. similar observations reported discussed section model dvector system enhanced borrowing data textindependent tasks. results reported table observed training data performance d-vector systems generally improved despite extra data recordings phrases. another observation training data plda model tends less note phone-dependent training important text-independent recognition. text-dependent recognition acoustic features limited small phones involving phone information training help much. kinds speaker vectors d-vector ivector fundamentally different. i-vectors based linear gaussian model learning unsupervised learning criterion maximum likelihood acoustic features. contrast d-vectors based neural networks learning supervised learning criterion maximum discrimination speakers. difference model structures learning methods leads signiﬁcant different properties vectors. i-vector ‘descriptive’ represents speaker constructing acoustic features. contrast d-vector ‘discriminative’ represents speaker removing speakerirrelevant variance. second i-vector regarded ‘global’ speaker description inferred ‘all’ frames utterance; however d-vector ‘local’ description inferred ‘each’ frame context information used inference. means d-vector tends superior short utterance i-vector tends perform better relative long utterance. i-vector approach relies enrollment data form reasonable distribution used discriminate different speakers; whereas d-vector approach relies ‘universal’ data learn speakerdiscriminative features. means large amount training data much important useful d-vector approach. experiments performed short phrase speech database provided pachira. entire database contains recordings short phrases speakers phrase contains chinese characters. speaker every phrase recorded times amounting utterances speaker. experiment examines d-vector approach semi text-independent task. dimension i-vectors d-vectors ﬁxed dimension ldaprojected vectors order systems involve amount parameters number gaussian components i-vector system utterances training dataset used train model i-vector model. results systems reported table iii. observed simple cosine distance d-vector system outperforms i-vector system signiﬁcant way. demonstrates discriminatively learned d-vectors discriminative speakers compared generatively learned i-vectors. however discriminative normalization methods employed performance i-vector system signiﬁcantly improved better d-vector system. discriminative methods contribute little d-vector system. supervising d-vectors discriminative already. nevertheless slight improvement suggests redundancy d-vectors. motivated idea hidden layer small number units inserted structure shown fig. dimension layer best choice test. compared approach regarded non-linear dimension reduction additional performance achieved method shown last column table iii. experiment phone posteriors included input structure shown fig. phone posteriors produced model trained chinese database consisting hours speech data. phone consists toneless initial ﬁnals chinese plus silence phone. results shown third table iii. seen phone-dependent training leads marginal consistent performance improvement d-vector system. nldr approach also applied additional gain obtained. following combine best i-vector system best d-vector system combination simply done interpolating scores obtained systems. results various values interpolation factor drawn fig. seen combination leads better performance appropriate best lowest obtain dataset far. paper investigated dnn-based discriminative feature leaning speaker recognition studied performance approach semi text-independent task. experimental results demonstrated dnn-based approach offer reasonable performance outperformed ivector baseline simple cosine distance. however discriminative normalization methods plda applied i-vector approach exhibits better performance. although beat i-vector approach present d-vector approach promising potentially improved several ways. particularly powerful probabilistic model d-vectors would deal inter-frame uncertainty considerably enhance system performance. leave future work. kenny boulianne ouellet dumouchel joint factor analysis versus eigenchannels speaker recognition ieee transactions audio speech language processing vol. speaker session variability gmm-based speaker veriﬁcation ieee transactions audio speech language processing vol. ehsan erik ignacio g.-d. javier deep neural networks small footprint text-dependent speaker veriﬁcation ieee international conference acoustic speech signal processing vol.", "year": 2015}