{"title": "Sample Efficient Deep Reinforcement Learning for Dialogue Systems with  Large Action Spaces", "tag": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "abstract": "In spoken dialogue systems, we aim to deploy artificial intelligence to build automated dialogue agents that can converse with humans. A part of this effort is the policy optimisation task, which attempts to find a policy describing how to respond to humans, in the form of a function taking the current state of the dialogue and returning the response of the system. In this paper, we investigate deep reinforcement learning approaches to solve this problem. Particular attention is given to actor-critic methods, off-policy reinforcement learning with experience replay, and various methods aimed at reducing the bias and variance of estimators. When combined, these methods result in the previously proposed ACER algorithm that gave competitive results in gaming environments. These environments however are fully observable and have a relatively small action set so in this paper we examine the application of ACER to dialogue policy optimisation. We show that this method beats the current state-of-the-art in deep learning approaches for spoken dialogue systems. This not only leads to a more sample efficient algorithm that can train faster, but also allows us to apply the algorithm in more difficult environments than before. We thus experiment with learning in a very large action space, which has two orders of magnitude more actions than previously considered. We find that ACER trains significantly faster than the current state-of-the-art.", "text": "form communication. recent advances artiﬁcial intelligence reinforcement learning established necessary technology build ﬁrst generation commercial spoken dialogue systems deployable regular household items. examples systems amazon’s alexa google’s home apple’s siri. initially built voice-command systems years systems become capable sustaining dialogues span turns. spoken dialogue systems complex solve many challenging problems once signiﬁcant uncertainty. recognise spoken language decode meaning natural language understand user’s goal keeping track history conversation determine information convey user convert information natural language synthesise sentences speech sounds natural. work focuses particular step pipeline devising policy determines information convey user given belief goal. policy traditionally planned hand using ﬂow-charts. manual inﬂexible process many drawbacks ultimately systems unable converse intelligently. overcome this policy optimisation problem formulated reinforcement learning problem formulation computer takes actions gets rewards. algorithm aims learn policy maximises rewards learning take best actions based state dialogue. since number possible states large complex universal function approximators neural networks deployed policy recent trend last years model text-totext dialogues neural network tackle sequence sequence model. initial attempts underestimate fact planning needed treat problem purely supervised fashion recently learning also applied yielding improvements focus traditional modular approaches everything describe also applicable end-to-end modelling. using neural networks policy optimisation challenging reasons. first often little training data available data often comes real humans. system able train quickly on-line setting training data gathered users make data gathered useful. neural networks often exhibit much bias high variance volume training data small making difﬁcult quickly train abstract—in spoken dialogue systems deploy artiﬁcial intelligence build automated dialogue agents converse humans. part effort policy optimisation task attempts policy describing respond humans form function taking current state dialogue returning response system. paper investigate deep reinforcement learning approaches solve problem. particular attention given actor-critic methods off-policy reinforcement learning experience replay various methods aimed reducing bias variance estimators. combined methods result previously proposed acer algorithm gave competitive results gaming environments. environments however fully observable relatively small action paper examine application acer dialogue policy optimisation. show method beats current state-of-the-art deep learning approaches spoken dialogue systems. leads sample efﬁcient algorithm train faster also allows apply algorithm difﬁcult environments before. thus experiment learning large action space orders magnitude actions previously considered. acer trains signiﬁcantly faster current state-ofthe-art. keyboard mouse touch. provide feedback user primarily visual clues display. human-computer interaction model unintuitive human user ﬁrst allows user express intent clearly long goal supported equipped sufﬁcient knowledge operate machine. spoken dialogue system aims make humancomputer interaction intuitive equipping computers ability translate human computer language thereby relieving humans burden creating intuitive interaction model. speciﬁcally objective help human user achieve goal speciﬁc domain using speech stable way. second success failure dialogue information available system train policy dialogue success depends crucially actions dialogue making difﬁcult determine individual actions contributed success failure dialogue. problem exacerbated large size state space system potentially never state twice. analyse algorithm detail highlighting theoretical advantages variance safe efﬁcient learning. test algorithm dialogue task delayed rewards test alongside state-of-the-art methods task conﬁrm ﬁndings human evaluation. rest paper organised follows. first give brief introduction dialogue management deﬁne main concepts. then section review reinforcement learning. followed in-depth description acer algorithm section then section describe architecture deployed allow application acer dialogue problem. results extensive evaluation given section vii. section viii give conclusions future work directions. dialogue manager take user’s dialogue acts semantic representation input determine appropriate response also format dialogue function chooses appropriate response called policy. role dialogue management two-fold tracking dialogue state optimising policy. call user’s overall goal dialogue user goal i.e. booking particular ﬂight ﬁnding information restaurant. user works towards goal every dialogue turn. dialogue turn short-term goal user called user intent. examples user intent conﬁrm system said inform system criteria request information something. belief tracker memory unit track user goal user intent dialogue history. state satisfy markov property depend previous state action taken. therefore state needs encode enough information happened dialogue previously maintain conversation. tracking dialogue history ensure state satisﬁes markov property. user intent derived dialogue act. deal inherent uncertainty input dialogue modelled partially observable markov decision process belief state vector representing probability distribution different goals intents histories occur dialogue. role belief tracker accurately estimate probability distribution normally done using version recurrent neural neural network policy probability distribution possible user actions given current belief state commonly written here action output belief tracker interpreted vector probabilities. order deﬁne optimal policy need introduce utility function describes good taking action state reward complete dialogue depends whether user successful reaching goal length conversation short successful dialogues preferred. thus last dialogue interaction gains reward based whether dialogue successful every interaction loses small constant reward penalising length dialogue. task policy optimisation maximise expected cumulative reward state following policy choosing optimal action possible actions finding optimal policy computationally prohibitive even simple pomdps. view partially observable markov decision process continuous-space markov decision process terms policy optimisation states belief states allows apply function approximation solve problem policy optimisation. approach adopt work. system actions dialogue acts system give response. called action space master action space. large size training dialogue policy action space difﬁcult. algorithms converge optimal policy converge slowly rare cases prohibitive computational demands. alleviate problem summary action space contains much smaller number actions. policy trained summary action space action selected policy needs converted master action. conversion heuristics attempts optimal slots inform given belief state. using summary action space provides clear beneﬁt simpler dialogue policy optimisation task. hand necessary heuristics master action space need manually constructed domain. normally dialogue management action space discrete. since training on-line i.e. happening user input acquired training constrained computation time prevent user wait system reply. however training step rarely bottleneck. learned policy words evaluate improve policy used make decisions. contrast off-policy methods evaluate improve policy different used generate data behaviour policy learned policy different. advantage off-policy methods optimal policy even choosing sub-optimal actions according behaviour policy. standard reinforcement learning algorithms require state space discrete. therefore belief state dialogue manager often discretised allow standard algorithms applied alternatively function approximation applied linear function approximation applied value functions. parametric function approximation limit optimality solution gpsarsa instead models q-function gaussian process. kernel function models correlation different states allows uncertainty estimated. crucial learning small number samples. recently neural network function approximation used approximate q-value function known deep q-learning obtaining human-level performance across challenging video games policy also modelled directly deep networks leading resurgence actor-critic methods actor improving policy interactions directed critic number deep learning algorithms previously applied dialogue management. shown enables learning strategic agents negotiation abilities operating high-dimensional state space. performance actor-critic models task-oriented dialogue systems analysed models also naturally bootstrapped small number dialogues supervised pre-training. reported superior performance compared gp-sarsa noise-free environment. however compared gp-sarsa utilise uncertainty estimates previously found crucial effective learning number recent works investigated end-to-end using gradient descent techniques belief tracking policy optimisation optimised jointly. while endto-end modelling goes beyond scope work note presented algorithm applicable also setting. paper builds recent breakthroughs deep reinforcement learning applies problem dialogue management. particular investigate recently proposed improvements actor-critic method goal stable sample efﬁcient learning algorithm performs well challenging policy optimisation tasks means belief state needs human interpretable. limits applicability neural networks belief tracking belief state compactly represented hidden layer neural network. every system action appropriate every situation example inform valid action beginning dialogue system received information kind entity user looking for. execution mask constructed designer ensures valid actions selected policy probability invalid actions zero. execution mask depends current belief state. note removing mask inherently complicates task policy learning policy learn select inappropriate actions based belief state. reinforcement learning agent interacts environment discrete time steps. time step agent observes environment belief state vector chooses action action space performing action agent observes reward produced environment. discount factor trades-off importance immediate future rewards. goal agent policy maximises expected discounted cumulative return every state. deﬁne value state-action pair policy q-value function expectation return belief state action reinforcement learning scenarios become challenging agent estimates value functions trial error interacting simulated real environment. estimates accurate limit inﬁnite observations state-action pair thus requirement behaviour policy maintain exploration i.e. keep visiting state-action pairs non-zero probability. behaviour policy policy used generate data learning. on-policy methods behaviour policy requires single weight. however estimation biased value function update current state based current estimate value function next state. leads slow convergence convergence all. constant controls bias-variance trade-off setting results equivalent estimation equation variance high bias. conversely setting results high variance many weights producted. advantage propagating ﬁnal reward starting state reduces bias. retrace algorithm attempts estimate current q-function off-policy interactions safe efﬁcient small variance. throughout discussion call method safe estimate proven converge updated estimate q-function qret domain. recent advances apply several methods including experience replay truncated importance sampling bias correction off-policy retrace algorithm trust region policy optimisation various challenging problems. core paper investigate extent advances applicable dialogue policy optimisation task large action space. methods recently combined actor critic experience replay algorithm tested gaming environments. explain actor critic experience replay detail investigate steps needed apply sds. unlike games methods previously applied investigate dialogues large uncertain belief states large action spaces. necessitates function approximation reinforcement learning previously examined methods data-inefﬁcient unstable computationally expensive. investigate acer means overcome limitations. order experience replay actor-critic method off-policy version actor-critic method needed. objective policy maximises expected discounted return. equivalent maximising value initial state input parameter vector states encountered proportions according sampling experience memory need estimate explicitly. estimating however difﬁcult off-policy interactions gathered according need q-function different current policy account this importance sampling weights could used. achieve stable learning estimation method achieves variance considering state-action pairs isolation applying weight each. bias correction term ensures estimate bias correction term active otherwise formulation equivalent equation active bias correction weight falls apply method called truncation bias correction trick overcome problem advantage function estimation. before estimated belief-action pairs sampled replay memory equation bias correction term however belief sampled memory actions considered weighted current policy qret formulated learns rewards learns belief-action pairs visited sampled replay memory. thus estimation available bias correction term output estimate advantage function term typically step size parameter gradient descent calculated assuming policy parameter space euclidian. however major shortcoming small changes parameter space lead erratic changes output policy could lead unstable learning learning rate small quick convergence. solved natural actor critic algorithm considering natural gradient framework introduces changes actor-critic model. instead approximating estimating closed-form equation compute update targets estimated nns. computed focus retrace proposed ideally need method safe variance efﬁcient possible. retrace solves trade-off setting traces dynamically based weights. near on-policy case efﬁcient weights preventing traces vanishing. variance weights clipped also safe goal discussion limited conveying intuition behind retrace full proof safety available investigate computational cost deriving qret na¨ıve way. episode sampled replay memory state-action pair need visit remaining part episode calculate expectation errors according equation quadratic element computational cost reduced linear deriving qret recursive way. episode trajectory sampled replay memory equation becomes expectation taken replay memory issue approximation weights potentially unbounded introducing signiﬁcant variance. solve problem clip weights constant min{c ρ)}. split equation parts involving truncated weight residual. also need estimate residual otherwise kullback-leibler divergence. thus instead directly restricting learning step-size natural gradient method approximate method restricting kullback-leibler divergence current policy parametrised updated policy parametrised learning rate method called trust region policy optimisation introduced method however relies repeated computations fisher matrices update prohibitively expensive. introduces efﬁcient trpo method adopt instead. description method largely follows additional explanations necessary adaptation discrete action-space domain. begin with proposes kl-divergence updated policy measured current policy separate average policy instead. stabilises algorithm preventing gaining momentum speciﬁc direction. instead restricted stay around stable average policy average policy parametrised represents running average previous policy parameters. updated softly learning step hyperparameter controls amount history maintain average policy. value close zero makes average policy forget history quickly reducing effect calculating distances average policy instead current one. value close prevent average policy adjust current policy slows adjustment process down. trpo formulated optimisation problem minimises l-distance vanilla gradient quadratic minimisation. addition divergence constraint formulated linear allow derive closed-form solution. since used parameter update denotes updated parameter vector. approximate divergence policy update using ﬁrst-order taylor expansion since constraint linear overall optimisation problem reduces simple quadratic programming problem. thus closed-form solution derived using conditions acer result methods presented section. on-policy exploration modiﬁed version acer experience replay sample memories achieve high sample efﬁciency. difference acer additionally employs trpo uses q-function estimator instead -function estimator critic. off-policy uses truncated importance sampling bias correction reduce variance weights without adding bias. retrace algorithm used compute targets based observed rewards safe efﬁcient bias variance. training algorithm presented pseudocode called master acer algorithm performs \u0001-greedy exploration i.e. optimal action learned probability random action probability hyperparameter batch size controls number dialogues considered training step controls number training steps dialogue gathered. investigate effect various hyperparameters section vii-e. design neural networks actor-critic dialogue management task. input belief state build hidden layers heads functions hidden layers shared predictors weight sharing activation function layers rectiﬁed linear unit chosen empirically faster training. activation function softmax converts inputs probability distribution values summing activation function output want unlimited range connections fully connected imposes least structural constraints architecture. perform experiments cambridge restaurants domain details given appendix domain belief state represented -dimensional vector. input layer consists neurons neurons. numbers chosen empirically goal mind force encode information belief state relevant bottleneck layer neurons thereby learning mapping generalises better. output vectors dimensionality action space. initially experiment summary action space actions case caminfo domain informable slots entity binary choice whether inform thus single inform action makes separate master actions differing inform want incorporate fact actions similar design architecture. achieve breaking policy summary policy corresponding dimensional summary action space payload policy corresponding choices payload inform action. break function similarly reconstruct -dimensional master policy master q-function follows summary action payload append corresponding summary values onto otherwise payload possible choices append probability choosing action payload modelled product probability choosing choosing also append allowing payload network learn offset achieved choosing particular payload. complete architecture illustrated figure important note architecture changed training algorithm unchanged. fact treated black acer. output -dimensional vector master action space. master actions gp-sarsa compare acer gp-sarsa algorithm. policy algorithm approximates q-function gaussian process therefore sample-efﬁcient kernel function deﬁnes correlations different parts input space. similarly acer gaussian process method needs adjusted deploy master action space. core kernel function case gp-sarsa deﬁned recall kernel function deﬁnes priori belief covariance belief-action pairs. kernel multiplication scalar product beliefs kronecker delta actions. latter effect different actions considered completely independent. might good approximation summary actions elaborate action kernel required master actions. could introduce idea inform actions slightly different payloads expected similar results belief state thus showing higher covariance. action kernel returns actions stem different summary actions. otherwise inform action differing payloads. case calculate kernel based cosine similarity payloads treating payloads vectors describing sets slots inform call summary action payload corresponding represented vector entry either depending whether corresponding slot informed writing normalised version payload vector ||ap|| kernel becomes bδs)ap refer proof valid kernel function. case gp-sarsa master action space training algorithm unchanged. kernel function adjusted incorporate idea similarity master actions. thus trained -dimensional master action space. important highlight limitations work. work addressing problem modelling policy large action space similarities system actions. contrary focus large action spaces establish relations actions either sharing weights neural network architecture section deﬁning special kernel functions section v-b. although might seem limiting practice task-oriented dialogue actions bear similarities. used addressed producing smaller summary space distinct actions believe proposed approach scales better removes hand-crafting leads better performance. latter hypothesis investigated next section. section evaluate performance acer incorporated sds. acer delivers best performance fastest convergence among compared nn-based algorithms implemented pydial dialogue toolkit also deploy algorithm challenging setting without execution mask aiding action selection. next investigate effect different hyperparameter selections algorithm’s stability then deploy acer master action space. finally investigate resilient different algorithms semantic errors changing testing conditions. experiments follows. first total number dialogues iterations broken milestones training total number iterations progresses snapshot state training saved milestone. separate iterations performed without training steps saved snapshots tested iterations. training exploration performed testing phase; instead \u0001-greedy greedy policy respect used derive next action. informs performance system stopped training speciﬁc milestone allowing observe speed convergence performance early milestones discounting exploration. evaluation times average results reduce variance arising different random initialisations. compare average per-episode reward obtained agent average number turns dialogue percentage successful dialogues. reward deﬁned successful dialogue minus number turns dialogue. number maximum turns limited which user achieve goal dialogue deemed unsuccessful. discount factor algorithms applicable. nn-based algorithms size minibatch training step performed algorithms employing experience replay replay memory capacity interactions. nn-based algorithms \u0001-greedy exploration used linearly reducing training process. agenda-based user simulator focus belief tracker experiments. details agenda-based user simulator consists goal randomly generated slot-value pairs entity user seeks must satisﬁed agenda dynamic stack dialogue acts user elicits order satisfy goal. simulated user consist deerministic stochastic decisions govern behaviour capable generating complex behaviour. typical dialogue starts user expressing looking waiting system prompt checks whether offered entity satisﬁes constraints. process sometimes changes goal asks something else making difﬁcult system satisﬁes goal. settles offered entity asks additional information address phonenumber. dialogue deemed successful offered entity needs match last user goal. also system must provide information user simulator asked for. reward delayed given dialogue. reward given partially completed tasks. turns. success rate acer remains percentage points acer requires fewer dialogue turns ultimately obtains somewhat higher rewards suggests slightly worse success rate acer presents shortcoming reward function rather algorithm algorithm optimises reward function. also observe acer exceeds performance nn-based methods terms speed convergence sample efﬁciency success rate rewards. experiments without execution mask compare success rates general expected algorithms converge slower without execution mask ﬁnal performance acer remain somewhat performances mask. also expected mapping learned rarely precise hard-coded solution problem shows faster initial convergence acer latter shows steady progress without unexpected dips performance. remain comparable every regard. acer several additional hyperparameters compared traditional algorithms. investigate effect hyperparameters algorithm’s performance. better illustrate differences tests challenging setting without execution mask. every analysed parameter kept rest hyperparameters values providing best results section importance weight threshold value upper bound weight; weights higher truncated. setting value high diminishes effect weight truncation value rely less accurate bias correction term. figure delivers highest convergence rate good ﬁnal performance. also wide range values difference ﬁnal performance suggesting algorithm relatively stable face varying hyperparameter. simulated semantic error rate training testing. learning rate instead simple gradient descent loss function adam optimiser associates momentum gradient discourage algorithm learning trivial policy subtract policy entropy loss function. acer-speciﬁc hyperparameters results given figure figure shaded represents conﬁdence interval. divergence constraint value constrains divergence updated policy running average policy. setting high allows radical jumps setting slows convergence setting results erratic changes performance acer sensible choices. average policy update weight figure average policy forgets history quickly allowing policy gain momentum direction thus preventing converging good performance. policy converges quickly results somewhat conservative algorithm divergence constraint keeps policy near slowly changing average. still converges good result somewhat slower case training iterations setting number training steps episode higher allows algorithm learn gathered experience. however high training might diverge policy moving much convergence quick performance good performance stays poor throughout. algorithm diverges completely. space actions. difﬁcult scenario orders magnitude actions. scenarios computational cost prohibitive needs invert gram matrix acer still performs well scenario might overall best method apply larger action spaces. acer prohibitive computational cost expected train much quickly. test hypotheses deploy acer master action space according section summary space experiments execution mask. convergence slower master action space. expected choose vastly higher number actions master action space however acer still surprisingly effective master action space converging performance summary space. note without modiﬁcation training algorithm underlying changed. acer achieves best results terms speed convergence ﬁnal performance master action space nn-based policy optimiser algorithms. investigate whether acer best choice algorithm master action space modify master action space according section v-b. compare acer summary master action spaces without execution mask figure acer show slower speed convergence master action space. expected random initialisation policy optimiser deals substantial uncertainty tends introduce errors ultimately want measure well policy optimiser learn optimal strategy face noisy semantic-level input. experiments control semantic error rate rate random noisy input introduced optimiser simulate error scenario. words semantic error rate means probability semantic concept presented dialogue manager incorrect. focus desirable properties policy. first ideally policy would learn trust input much questions sure user goal like real human would telephone line noisy. second ideal policy would adjust error rate training conditions would dynamically adjust conditions dialogue policy adjusts much training conditions said overﬁt. could severely limit policy’s deployability. test algorithms desirable properties. enac best known nn-based policy optimiser date compared acer acer also compared respective variants master action space. test follows ﬁrst train algorithms semantic error rate convergence execution mask. take fully trained policy test range semantic error rates ranging measure policies’ generalisation properties. something never case games aspect learning rarely examined utmost importance spoken dialogue systems. present results figure figure conﬁdence interval. success rate reward follow trends. expected general downwards trend algorithm semantic error rate increases. however apparent spike performance semantic error rate training process indicating none algorithms overﬁt setting. performance enac behind algorithms. acer closer performance summary space consistently beats acer summary space. master action space much less sensible initialisation summary space latter taking advantage hard-coded summary master action mapping method. however surprising experiments converged roughly performance success rate except summary ﬁnal success rate %-%. suggests acer handle large action spaces quite efﬁciently. knowledge ﬁrst time learning master action space scratch successfully attempted. sample efﬁcient acer challenging master action space without execution mask. however requires vastly computational resources experiment took hours acer days arguably extra computational cost overshadows disadvantage acer iterations converge. experiment settings quite idealised training testing policies perfect simulator semantic errors. however real life automatic speech recognition component likely make errors well spoken language understanding component. therefore reality pipeline surrounding policy implementation acer also able train efﬁciently master action space showing best performance among neural network-based policy optimisers reported suffers inherently high computational cost making algorithm unsuitable higher volume action spaces. cases fact acer trained well master action space indicates best currently known method train policies large action spaces. agents powered machine learning gain intelligence applied challenging domains. using master action space good example this hard-coded mapping summary action spaces used simplify task agent. however shown longer required train action space. algorithm ﬁnally bridge semantic summary master action spaces without help domain-speciﬁc code written explicitly mapping. three beneﬁts ﬁrst training master action space outperforms mapping based ﬁxed code uncertainty involved. second allows build generally applicable system less work required deploy differing domains. third allows consider domains vastly higher action spaces even clear convert action spaces small summary action spaces acer well research directions too. successful policy optimisers need sample efﬁcient able trained quickly avoid subjecting human users poor dialogue performance long. acer uses experience replay sample efﬁciency together many methods aimed reducing bias variance estimator achieve quick training. introduce many directions work could continued. recently combined supervised learning deep reinforcement learning investigate performance agent bootstrapped trained drl. acer compatible approach. decrease overall summary space given performed worse previous experiments. however experiments semantic errors hand-crafted rigid mapping summary actions master actions relied belief state best payload inform action. higher semantic error rate belief state noisy mapping perform optimally. highlights beneﬁts expanding scope artiﬁcial intelligence versatile hand-coded mappings especially mapping performs decision making uncertainty. previous sections training testing performed simulated user. test generalisation capabilities proposed methods evaluate trained dialogue policies interaction human users similar set-up recruit users amazon mechanical turk service volunteers call dialogue system rate around dialogues gathered. three policies trained semantic error rate accommodate errors using set-up previous sections. then learnt policies incorporated pipeline commercial system. mturk users asked restaurants particular features deﬁned given task. subjects randomly allocated three analysed systems. dialogue users asked whether judged dialogue successful translated reward measure. table presents averaged results standard deviation. models differ indiscernibly regards success rate performing well. however acer trained master action space achieves considerably higher reward models working summary action space. settings training summary master action space considered static action spaces only. framework entire policy would retrained action payload introduced. could hurt maintainability real-life dialogue system would expensive extend database schema list actions. ideally training algorithm could adapt changes made able retain pre-existing knowledge actions important topic investigate future. conﬁrm slot slot informable slot. action prompts user conﬁrm criteria slot already mentioned. errors accumulating decoding pipeline system deal considerable uncertainty attempt increase certainty user’s criteria using conﬁrm action want expensive restaurant? select slot slot informable slot. action prompts user select value slot speciﬁed list values. less open-ended request action open-ended conﬁrm action would like indian korean food?. inform method slots action provides information restaurant. associated method speciﬁes restaurant give information chosen. standard method choose ﬁrst result ontology matches user criteria speciﬁed far. method also byname case system believes user asked speciﬁc restaurant referring name information restaurant provided. method requested inform restaurant informed last alternatives pick another restaurant matches user’s criteria several properties restaurant binary choice whether system wants inform dialogue turn not. informable slots restaurants area food type description phone number price-range address postcode signature. note slots also requestable allowing user query restaurant based slots. slots area food type price-range. restaurant also name always inform thus system choice different ways inform restaurant. speciﬁc choice referred payload inform action. reqmore simple action prompts user caminfo domain inform actions actions making actions total. call action space master action space. summary space inform actions specify slots inform leaving separate inform actions actions total. example dialogue user looking restaurant medium price range system internally translates summary master actions. system responses written summary action master action hello hello inform request pricerange request pricerange inform inform inform reqalts inform alternatives inform request inform requested inform thankyou williams asadi zweig hybrid code networks practical efﬁcient end-to-end dialog control supervised reinforcement learning association computational linguistics p.-h. budzianowski ultes gasic young sampleefﬁcient actor-critic reinforcement learning supervised data dialogue management proceedings sigdial dhingra y.-n. chen ahmed deng towards end-to-end reinforcement learning dialogue agents information access association computational linguistics lane iterative policy learning end-to-end trainable taskoriented neural dialog models ieee automatic speech recognition understanding workshop wang bapst heess mnih munos kavukcuoglu freitas sample efﬁcient actor-critic experience replay international conference learning representations sutton mcallester singh mansour policy gradient methods reinforcement learning function approximation advances neural information processing systems vol. ultes rojas-barahona p.-h. vandyke casanueva budzianowski mrkˇsi´c t.-h. gaˇsi´c young pydial multi-domain statistical dialogue system toolkit demo. association computational linguistics schatzmann thomson weilhammer young agenda-based user simulation bootstrapping pomdp dialogue system conference north american chapter association computational linguistics gell´ert weisz received b.a. degree computer science tripos university cambridge stayed cambridge mphil student reading machine learning speech language technology. afterwards joined deepmind research engineer. paweł budzianowski received b.a. m.a. degrees faculty mathematics computer science adam mickiewicz university pozna´n since university cambridge ﬁrst mphil student reading machine learning speech language technology. afterwards begun dialogue systems group university cambridge. research interests include multi-domain policy management bayesian deep learning. pei-hao candidate supervision professor steve young dialogue systems group cambridge university. research interests centre applying deep learning reinforcement learning bayesian approaches dialogue management reward estimation building systems learn directly human interaction. published around peer-reviewed papers across speech conferences received best student paper award milica gaˇsi´c lecturer spoken dialogue systems cambridge university engineering department fellow murray edwards college. holds computer science mathematics university belgrade mphil computer speech text internet technology engineering university cambridge topic statistical dialogue modelling awarded epsrc plus award dissertation. published around journal articles peer reviewed conference papers received number best paper awards. elected committee member ieee sltc appointed board member sigdial.", "year": 2018}