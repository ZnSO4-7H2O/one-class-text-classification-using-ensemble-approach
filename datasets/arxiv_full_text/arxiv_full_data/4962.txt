{"title": "Deep Learning for Ontology Reasoning", "tag": ["cs.AI", "cs.LG"], "abstract": "In this work, we present a novel approach to ontology reasoning that is based on deep learning rather than logic-based formal reasoning. To this end, we introduce a new model for statistical relational learning that is built upon deep recursive neural networks, and give experimental evidence that it can easily compete with, or even outperform, existing logic-based reasoners on the task of ontology reasoning. More precisely, we compared our implemented system with one of the best logic-based ontology reasoners at present, RDFox, on a number of large standard benchmark datasets, and found that our system attained high reasoning quality, while being up to two orders of magnitude faster.", "text": "work present novel approach ontology reasoning based deep learning rather logic-based formal reasoning. introduce model statistical relational learning built upon deep recursive neural networks give experimental evidence easily compete with even outperform existing logic-based reasoners task ontology reasoning. precisely compared implemented system best logic-based ontology reasoners present rdfox number large standard benchmark datasets found system attained high reasoning quality orders magnitude faster. last years increasing interest application machine learning ﬁeld knowledge representation reasoning generally learning reason symbolic data—cf. e.g. gabrilovoch main motivation behind formalisms used today rooted symbolic logic allows answering queries accurately employing formal reasoning also comes number issues like difﬁculties handling incomplete conﬂicting uncertain information scalability problems. however many issues dealt effectively using methods context often subsumed notion statistical relational learning —cf. nickel recent survey. notice though reasoning tradeoff. hand models often highly scalable resistant disturbances data provide predictions even formal reasoning fails. hand however predictions correct certain probability only. contrast this formal reasoners often obstructed problems provide inferences correct certainty. believe combination ﬁelds i.e. important step towards human-level artiﬁcial intelligence. however exist elaborate reasoning systems already rather young ﬁeld believe boundaries yet. therefore work introduce approach based deep learning apply task reasoning ontological knowledge bases knowledge bases consist facts together formal description domain interest—the so-called ontology. reason chose task practical signiﬁcance well fact commonly comprises extensive formal reasoning. motivation employing deep learning however refers neural networks perform many sequential steps computation fairly obvious. last years deep learning applied wide variety problems tremendous success constitutes state-of-the-art ﬁelds like computer vision natural language processing today. interestingly also published attempts realize formal reasoning means deep nns. however focus rather restricted logics like natural logic real logic consider reasoning full generality. besides this »reasoning« appears connection deep learning mostly context nlp— e.g. socher furthermore provide experimental comparison suggested approach best logic-based ontology reasoners present rdfox several large standard benchmarks. thereby model achieves high reasoning quality orders magnitude faster. rest paper organized follows. next section review concepts approach built upon. section introduces suggested model full detail section discusses apply ontology reasoning. section evaluate model four datasets compare performance rdfox. conclude summary main results give outlook future research. mentioned introduction already work lies intersection traditionally quite separated ﬁelds namely krr. therefore section review important concepts areas required follow subsequent elaborations. central idea ﬁeld so-called ontologies. context ontology formal description concept domain e.g. part real world word »formal« emphasizes description needs speciﬁed means knowledge representation language clearly deﬁned semantics. this turn allows employ formal reasoning order draw conclusions based ontology. important aspect note ontology situated meta-level means might specify general concepts relations contain facts. however sequel talk number facts together ontology describes domain interest refer setting ontological knowledge base practice context description logics ontologies usually deﬁned terms unary binary predicates. thereby unary predicates usually referred concepts classes deﬁne certain categories e.g. individuals possess particular characteristic. contrast this binary predicates deﬁne relationships might exist pair individuals usually referred relations roles. really appealing ontologies usually deﬁne predicates also rules allow draw conclusions based them. could encompass simple inferences like every individual class women belongs class human well also much elaborate reasoning takes several classes relations account. notice view almost relational dataset ontology specify anything except classes relations exist data. based fact hardly ever encounter ontologies predicates arity greater practice conﬁne particular case subsequent treatment—the approach introduced work easily extended general case though. deﬁned terms unary binary predicates natural representation labeled directed multigraph individuals interpreted vertices every occurrence binary predicate directed edge. thereby edges labeled name according relation vertices incidence vector indicates classes belong notice however that depending used formalism okbs adhere so-called open-world assumption case fact true false unknown e.g. different classical ﬁrst-order logic. presence reﬂected according three-valued incidence vectors whose elements respectively indicate individual belongs class member same unknown. recursive special kind network architecture introduced order deal training instances given trees rather than commonly feature vectors. general deal directed acyclic graph since graph unrolled tree requirement leaf nodes vector representations attached them. example ﬁeld parse tree sentence node represents word given either one-hot-vector previously learned word embedding. unlike feed-forward networks recursive ﬁxed network structure deﬁne single recursive layer accepts vectors input maps common embedding. layer used reduce provided tree step step bottom-up fashion single vector left. resulting vector regarded embedding entire graph used e.g. input subsequent prediction task. rd×k rk×d rd×d×k nonlinearity applied element-wise commonly tanh. thereby term denotes bilinear tensor product computed multiplying every slice separately. computed tensor product addition actual input vectors tensor layer accepts another parameter used specify certain relation provided vectors. makes model powerful since separate weights kind relation. general recursive trained means stochastic gradient descent together straightforward extension standard backpropagation called backpropagation structure section present model we—due lack better name—refer relational tensor network basically rntn makes modiﬁed bilinear tensor layer. underlying intuition however quite different term »relational« emphasizes focus relational datasets. described previous section recursive allow computing embeddings training instances given dags. face relational dataset though training samples actually vertices graph namely induced entire relational dataset rather graph itself. however original framework recursive networks still make recursive layer order update representations individuals based structure dataset. deliberation reﬂected following modiﬁed tensor layer intuition quite straightforward. individuals relational dataset initially represented respective feature vectors parts total information actually hidden relations among them. however recursive network composed tensor layers like denoted equation incorporate data individual’s embedding. intuitively means basically apply recursive update tree individual thus compute according vector representation based relations involved adopted convention tensor layer updates individual represented based instance relation present data. furthermore relations considered dataset symmetric distinguish whether individual source target instance relation. accordingly model contain sets parameters relation updating source target denote respectively. means e.g. denotes embedding updated based foregoing considerations also explain differences equation original tensor layer given equation first foremost model added basically used tensor layer before predicated fact want update vector. furthermore affect argument nonlinearity independently since determine updated. lastly bias term right-hand side equation prevent kind default update irrespective individuals involved. also considered another application hyperbolic tangent calculations given equation order keep elements created embeddings would ensure cannot embeddings oddly large norm individuals involved large number relations. however since encounter problems like experiments decided option could introduce additional problems like vanishing gradients. already suggested before usually employ rtns order compute embeddings individuals used input speciﬁc prediction task. therefore makes sense train together model used computing predictions whenever talk sequel shall assume used together predictor care individual embeddings irrespective particular subsequent task simply feed-forward layer—or differentiable learning model—on train model reconstruct provided feature vectors. used kind relational autoencoder. training model straightforward switches back forth computing embeddings making predictions based them. training iteration start feature vectors individuals provided dataset. then ﬁrst step sample mini-batches triples dataset randomly update current embedding individuals triple means rtn. total number mini-batches considered step hyperparameter found experiments general necessary consider entire dataset. next sample mini-batches individuals dataset compute predictions based embeddings created previous step. makes sense consider individuals updated well still initial feature vectors embeddings. important model learn deal individuals involved relations maybe rare case practice. therefore experiments used mini-batches balanced respect this switched back step number soon previously updated individuals sampled once. ﬁeld exist approaches model effects relations individual embeddings terms tensor products—cf. e.g. nickel however methods belong category latent variable models based idea factorizing tensor describes structure relational dataset product embedding matrix well another tensor represents relations present data. actual learning procedure cast regularized minimization problem based formulation. contrast this computes embeddings training application means random process thus fundamentally different idea. discussed section okbs viewed dags thus application kind data straightforward. therefore left specifying prediction model want rtn. context kinds predictions interested namely membership individuals classes hand existence relations hand. perspective really different targets describe formally follows contains unary predicates binary predicates part training set. target functions deﬁned notice arguments functions individuals thus represented embeddings produced rtn. computing actual predictions embeddings basically employ model choice. work however conﬁne multinomial logistic regression i.e. simply single feed-forward layer well softmax rtn. ﬁrst additional original tensor layer given equation like used socher multinomial logistic regression well. targets regarded independent respect prediction clearly case computing individual embeddings. require embedding reﬂect information single individual speciﬁed semantics considered okb. therefore tensor layers need learn adjust individual vectors view unary binary predicates i.e. classes relations. account this train rtns—facing particular case ontology reasoning—on mini-batches consist training samples prediction targets. evaluate suggested approach realistic scenario implemented novel triple store called nets achieves ontology reasoning solely means rtn. nets provides simple sparql-like query interface allows submitting atomic queries well conjunctions system started ﬁrst step performs load learned weights disk—the actual learning process part nets right incorporated future versions. next observes whether previously generated embeddings individuals stored disk already loads well any. case however nets creates embeddings described above. step comparable usually referred materialization context database systems. traditionally database would compute valid inferences draw based provided data store somehow memory disk. contrast this nets accounts inferences simply adjusting individuals’ embeddings means trained obviously great advantages regarding memory requirements. note store actual inferences time rather compute demand later happens become necessary. subsequent processing queries entirely based embeddings employ kind formal reasoning all. this turn allows speeding necessary computations signiﬁcantly since dispatch »heavy-lifting« gpu. system implemented python performs mentioned above almost numeric computations using pycuda learning weights rtns used python along tensorflow maintain comparability evaluated approach datasets motik used experiments rdfox mentioned earlier rdfox indeed great benchmark since shown efﬁcient triple store present. comparison systems however refer interested reader motik test data consists four semantic different sizes characteristics. among real-world datasets fraction dbpedia claros well synthetic ones lubm uobm characteristics summarized table data available multiple formats made ontologies speciﬁed facts provided n-triples experiments. furthermore considered predicates appear least individuals database. necessary restriction ensure enough data learn properly. experiments conducted server cpus type intel xeon nvidia geforce titan test system hosted ubuntu server cuda cudnn gpgpu. notice however nets make multiprocessing -threading besides gpgpu means kind parallelization takes place gpu. therefore terms nets half resources disposal rdfox utilized experiments conducted motik table characteristics test datasets. quantities refer explicitly speciﬁed rather inferred data values parentheses describe classes relations respectively appear least individuals. predicated model datasets including inferences converted directed graphs using apache jena reasoner pellet ..—all import times reported table refer graphs. reduced size data stored disk approximately third original dataset. furthermore removed total individuals training together predicates involved test datasets similarly another validation—the results described table retrieved test sets. order assess quality nets evaluate accounts. first need consider predictive performance based embeddings computed underlying model second must ascertain efﬁciency system respect time consumption. start former. consider table reports accuracies well scores nets achieved held-out test sets averaged classes relations respectively. model consistently achieves great scores respect measures. notice however score critical criterion since predicates strongly imbalanced. nevertheless effectively learns embeddings allow discriminating positive negative instances. table contrast lists times nets import materialize datasets along respective measurements rdfox mentioned before materialization refers actual computation inferences usually depends expressivity ontology well number facts available. nets signiﬁcantly faster materialization step rdfox faster importing data. explained follows. first nets realizes reasoning means vector manipulations course much faster symbolic computations performed rdfox. second point rdfox makes extensive parallelization also importing data nets runs single process single thread cpu. notice however neither measures reported nets contains time training model. reason train mentioned earlier respect ontology rather entire okb. therefore actually consider training step part setup database system. datasets used experiments training took three four days each. presented novel method based deep learning used develop highly efﬁcient learning-based system ontology reasoning. furthermore provided experimental comparison best logic-based ontology reasoners present rdfox several large standard benchmarks showed approach attains high reasoning quality orders magnitude faster. interesting topic future research explore ways improve accuracy ontology reasoning. could achieved e.g. incorporating additional synthetic data and/or slight reﬁnements architecture. work supported engineering physical sciences research council grants ep/j/ ep/l/ ep/m/ well alan turing institute epsrc grant ep/n/. furthermore patrick supported epsrc grant oucl//ph oxford-deepmind graduate scholarship grant gaf_ogsmf-dmcs_.", "year": 2017}