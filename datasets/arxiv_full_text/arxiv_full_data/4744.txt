{"title": "Optimally fuzzy temporal memory", "tag": ["cs.AI", "cs.LG"], "abstract": "Any learner with the ability to predict the future of a structured time-varying signal must maintain a memory of the recent past. If the signal has a characteristic timescale relevant to future prediction, the memory can be a simple shift register---a moving window extending into the past, requiring storage resources that linearly grows with the timescale to be represented. However, an independent general purpose learner cannot a priori know the characteristic prediction-relevant timescale of the signal. Moreover, many naturally occurring signals show scale-free long range correlations implying that the natural prediction-relevant timescale is essentially unbounded. Hence the learner should maintain information from the longest possible timescale allowed by resource availability. Here we construct a fuzzy memory system that optimally sacrifices the temporal accuracy of information in a scale-free fashion in order to represent prediction-relevant information from exponentially long timescales. Using several illustrative examples, we demonstrate the advantage of the fuzzy memory system over a shift register in time series forecasting of natural signals. When the available storage resources are limited, we suggest that a general purpose learner would be better off committing to such a fuzzy memory system.", "text": "learner ability predict future structured time-varying signal must maintain memory recent past. signal characteristic timescale relevant future prediction memory simple shift register—a moving window extending past requiring storage resources linearly grows timescale represented. however independent general purpose learner cannot priori know characteristic prediction-relevant timescale signal. moreover many naturally occurring signals show scale-free long range correlations implying natural prediction-relevant timescale essentially unbounded. hence learner maintain information longest possible timescale allowed resource availability. construct fuzzy memory system optimally sacriﬁces temporal accuracy information scalefree fashion order represent prediction-relevant information exponentially long timescales. using several illustrative examples demonstrate advantage fuzzy memory system shift register time series forecasting natural signals. available storage resources limited suggest general purpose learner would better committing fuzzy memory system. natural learners face severe computational problem attempting predict future time varying signals. rather presented large training examples must compute on-line using continuously evolving representation recent past. basic question arises here–how much recent past required generate future predictions? maintaining past information memory comes metabolic cost; would expect strong evolutionary pressure minimize resources required. shift register accurately represent information recent past chosen timescale consuming resources grow linearly timescale. however prediction-relevant timescale signal generally unknown prior learning. moreover many examples naturally occurring signals scale-free long range correlations commonly known signals making natural prediction-relevant timescale essentially unbounded. focus following question independent general purpose learner forecast long range correlated natural signals optimal represent past information memory limited resources? argue solution construct memory reﬂects natural scale-free temporal structure associated uncertainties world. example timing event happened seconds represented accurately memory timing event happened seconds ago. sacriﬁcing temporal accuracy information memory leads tremendous resource conservation yielding capacity represent information exponentially long timescales linearly growing resources. moreover sacriﬁcing temporal accuracy information scale-free fashion learner gather relevant statistics signal optimal signal contains scale-free ﬂuctuations. mechanistically construct memory system imperative keep mind information represented memory self suﬃciently evolve real time without relying information instantaneous input already represented memory; reliance external information would require additional storage resources. paper describe fuzzy i.e. temporally inaccurate memory system represents information long timescales optimally sacriﬁces temporal accuracy maximally preserving prediction-relevant information past evolves self suﬃciently real time. layout paper follows. section based general properties long range correlated signals derive criterion optimally sacriﬁcing temporal accuracy prediction relevant information exponentially long time scales maximally preserved memory ﬁnite resources. however non-trivial construct fuzzy memory self suﬃcient way. section describe strategy construct self suﬃcient scale-free representation recent past. strategy based neuro-cognitive model internal time tilt mathematically equivalent encoding laplace transform past approximating inverse reconstruct fuzzy representation past. optimal choice memory nodes representation naturally leads self-suﬃcient fuzzy memory system. section illustrate utility fuzzy memory simple time series forecasting examples. using several diﬀerent time series show fuzzy memory enhances ability predict future comparison shift register number nodes. course representing recent past memory guarantee ability successfully predict future. crucial learn prediction-relevant statistics underlying signal eﬃcient learning algorithm. choice learning algorithm largely modular choice memory system. paper sidestep problem learning focus memory. place holder learning algorithm linear regression demonstrations time series forecasting. learner successfully predict current value fig. shows sample time series leading present moment. y-axis corresponds present moment right lies unknown future values time series. x-axis labeled shift register denotes memory buﬀer wherein accurately stored unique node. step forward time information stored node transferred left-neighbor current value enter ﬁrst node. shift register thus self suﬃciently evolve real time. longest time scale represented shift register linearly related number nodes. given limited number memory nodes optimal represent memory information relevant prediction maximally preserved? show achieved fuzzy buﬀer shown node fuzzy buﬀer holds average bin. fuzzy buﬀer widths bins increase linearly center bin; bins chosen tile past time line. clearly accuracy representing sacriﬁced process compressing information entire real number note attain capacity represent information exponentially long timescales. analysis show sacriﬁcing accuracy widths chosen leads maximal preservation prediction-relevant information. fig. sample time series power-law two-point correlation plotted w.r.t. time current time step taken ﬁgure contrasts information represented shift register fuzzy buﬀer. node fuzzy buﬀer linearly combines information containing multiple shift register nodes. dashed curve shows predictive information content time step relevant predicting future value time series. clear fuzzy buﬀer shown cannot evolve self suﬃciently real time; information lost compression moment required next moment recompute average. moment would explicitly require correctly update values fuzzy buﬀer. fuzzy buﬀer actually save resources representing time series. even though fuzzy buﬀer self suﬃcient extremely simple analyze; fuzzy buﬀer derive optimal binning strategy maximally preserve prediction-relevant information time series ﬁnite storage resources. reader willing take optimality linearly-increasing widths faith skip ahead section construct self-suﬃcient memory system property. quantify prediction relevant information contained ﬁrst review general properties long range correlated series. since represent memory actually understand generating mechanism underlying suﬃcient consider generated generic statistical algorithm arfima model basic idea behind arfima algorithm white noise time step fractionally integrated generate time series long range correlations hence time series viewed generated inﬁnite auto-regressive generating function. words generated past series instantaneous gaussian white noise input asymptotic behavior large obtained applying euler’s reﬂection formula stirling’s formula approximate gamma functions turns either small large pic–predictive information content taylor expansion shows linear leading order. hence small limit stochastic term plus history dependent term order restricting linear order clear total simply individual pics namely thus small limit simple measure predictive information also extensive quantity. entire accurately represented inﬁnite shift register total contained shift register clearly maximum attainable value small. example smaller values observed many natural signals large portion predictive information lies long timescales. shift register ill-suited represent information long range correlated series. pictot memory buﬀer increased nodes stored linear combination many rather single shift register. substantiated information theoretic considerations formulated information bottleneck method multi-dimensional gaussian variable systematically compressed lower dimensions linear transformations maximizing relevant information content although unidimensional time series consideration moment entire considered inﬁnite dimensional gaussian variable since point correlations irreducible. information. unfortunately single-node memory buﬀer self suﬃcient moment explicitly need entire construct value one-node buﬀer. unbiased choice require priori knowledge statistics time series simply consider uniform averaging bin. uniform averaging discards separate information time values contributing bin. given consideration question consider ranging shall examine eﬀect averaging within representing single memory node. compressing average would however lead error prediction error directly related extent within diﬀerent other. hence reduction bin. given monotonic functional form maximum memory node representing total number nodes nmax ﬁnite want represent information longest possible timescale straightforward choice pick successive bins completely tile past time line schematically shown fuzzy buﬀer label nodes fuzzy buﬀer ranging nmax denote starting point note fuzzy buﬀer represent information timescales order nnmax exponentially large compared timescales represented shift register nmax nodes. total fuzzy buﬀer picfb calculated summing pics bins. analytic tractability focus small values description fuzzy buﬀer corresponds ideal case wherein neighboring bins overlap uniform averaging performed within bin. critical property linearly increasing sizes ensures temporal accuracy information sacriﬁced optimally scale-free fashion. however ideal fuzzy buﬀer cannot self suﬃciently evolve real time every moment explicitly needed construction. next section present self suﬃcient memory system possesses critical property ideal fuzzy buﬀer diﬀers overlapping bins non-uniform weighted averaging within bins. extent self suﬃcient fuzzy memory system resembles ideal fuzzy buﬀer expect useful representing long range correlated signals resource-limited environment. section ﬁrst describe mathematical basis representing recent past scalefree fashion based neuro-cognitive model internal time tilt describe several critical considerations necessary implement representation recent past discrete memory nodes. like ideal fuzzy buﬀer described previous section memory representation sacriﬁce temporal accuracy represent prediction-relevant information exponential time scales. unlike ideal fuzzy buﬀer resulting memory representation self suﬃcient without requiring additional resources construct representation. real valued function presented real time construct memory represents past values activity distributed nodes accuracy falls scale-free fashion. achieved using columns nodes shown column estimates present moment column intermediate step used construct nodes column leaky integrators decay constants denoted leaky integrator independently gets activated value instant gradually decays according fig. scale-free fuzzy representation node column leaky integrator speciﬁc decay constant driven functional value moment. activity column transcribed moment operator represent past functional values scale-free fuzzy fashion column. current moment illustrates brieﬂy non-zero around reconstructed history column shows peaks approximately around value particular moment past thus smeared invariance–for linearly scale values hold shape function ﬁxed. sense represents history scale invariant smear. quantify much smear introduced estimate width peak standard deviation equation turns although sums values past time nonetheless self-suﬃcient memory representation. since local diﬀerential equation activity node independently evolve real time depending present value node input available moment. hence discrete nodes also evolves self suﬃciently real time. extent activity discrete nodes constructed discrete nodes memory system whole self suﬃciently evolve real time. since activity nodes choose discrete unlike ideal fuzzy buﬀer described section possible associate circumscribed node weighting function compact support. since weighting function associated neighboring nodes overlap other convenient view bins associated neighboring nodes partially overlapping. overlap neighboring bins implies information redundantly represented multiple nodes. shall show forthcoming subsections appropriately tuning parameters information redundancy minimized equally spread scale-free fashion. nodes could picked form memory buﬀer activity ultimately constructed nodes column whose values given operator take k-th derivative along axis discrete values deﬁne linear operator implements discretized derivative. notational convenience denote activity moment simply since column vector rows labeled construct derivative matrix corresponding non-zero entries columns corresponding three entries read coeﬃcients respectively r.h.s equation. thus entire matrix constructed chosen values. taking k-th power operator straightforwardly nodes calculated moment. spacing nodes small discretized k-th derivative accurate. uniform discretization axis shown relative error computation k-th derivative discretization order distance neighboring values based values corresponding turns relative error construction activity error also controlled choosing small large vice versa. practical purposes strategies adopted control error require small nodes. example relaxing requirement correspondence nodes choose separate closely spaced values exclusively compute activity small finally noted discretization error induced process considered error conventional sense numerical solutions diﬀerential equations. numerically evolving diﬀerential equations time-varying boundary conditions discretization error derivatives propagate leading large errors move farther away boundary late times. situation hand since activity node computed independently others discretization error propagate. moreover noted eﬀect discretization better viewed coarse-graining k-th derivative rather inducing error computing k-th derivative. fact node ultimately holds scale-free coarse-grained value input function suggests wouldn’t need exact k-th derivative construct activity. extent discretization scale-free representation constructed coarse-grained k-th derivative represent scale free coarse grained value input; however weighting function would exactly match words even discretized implementation accurately match continuum limit still accurately satisﬁes basic properties require fuzzy memory system. linearity equations implies noise input function exactly represented without ampliﬁcation. however random uncorrelated noise nodes discretized operator amplify noise small. turns choice nodes according results constant signal-to-noise ratio across time scales. uncorrelated noise standard deviation added activation nodes operator combines noise neighboring nodes leading noisy representation. spacing nodes neighboring node standard deviation noise generated view noise appropriate context compare magnitude representation delta function signal past time −k/s magnitude representation delta function signal approximately kke−ks/k. signal noise ratio delta calculation however represent realistic situation. nodes leaky integrators white noise present across time accumulate; nodes long time constants higher value fact standard deviation white noise substituting would node represents weighted temporal average past signal appropriate isolated delta function signal estimate signal noise ratio. appropriate compare temporally averaged noise temporally spread signal. consider signals. suppose temporally uncorrelated white noise like signal. nodes. consider purely positive signal sequence delta function spikes generated poisson process. total expected number spikes would generated representation single delta function. signal constant nodes. conclude realistic stationary signal spread time constant timescales long nodes chosen according nodes implies redundancy information representation column. truly scale-free memory buﬀer redundancy information representation equally spread time scales represented buﬀer. information redundancy quantiﬁed terms mutual information shared neighboring nodes buﬀer. appendix shown presence scale free input signals mutual information shared neighboring buﬀer nodes constant nodes distributed according consequently information redundancy uniformly spread uniform spread information redundancy intuitively understood analyzing delta function input spreads buﬀer nodes time progresses. fig. shows activity buﬀer nodes three points time following input. left panel values buﬀer nodes chosen equidistant. note time progresses activity smeared many diﬀerent nodes implying information redundancy large nodes representing long timescales. values buﬀer nodes chosen according note time progresses activity pattern smear instead activity whole gets translated overall reduction size. translational invariance activity pattern passes buﬀer nodes explains information redundancy neighboring nodes constant. translational invariance activity pattern analytically established follows. consider diﬀerent values corresponding activities value node buﬀer given pattern activity across nodes translationally invariant constant integer hold true need quantity independent possible quantity inside power form exponential form separately independent power form independent values given exponential form generally dependent except argument zero happens whenever integer given inﬁnitely many values condition holds. moreover small condition approximately hold hence translational invariance activity pattern holds values buﬀer nodes conform nodes accordance ensures information redundancy equally distributed buﬀer nodes. however equal distribution information redundancy suﬃcient; would also like minimize information redundancy. turns cannot arbitrarily reduce information redundancy without creating information loss. parameters tuned order balance information redundancy information loss. small given many nodes buﬀer respond input given moment past resulting information redundancy. hand large given information many moments past left unrepresented buﬀer nodes resulting information loss. need match simultaneously minimize information redundancy information loss. achieved information given moment past distributed neighboring buﬀer nodes. formalize this consider delta function input time past current moment look activity induced input four successive buﬀer nodes values minn nodes given instance seen node attains maximum activity intervening times node attains maximum activity information delta function input spread nodes. minimize information redundancy simply require nodes nodes almost zero activity. fig. activity four successive nodes response delta function input past moment lost. minimize information loss information redundancy value neither large small. note activities nodes almost zero activities illustrate information loss high values shows activity nodes values activities normalized node attains focusing case range values total activity nodes close zero. input represented node close represented intermediate values input node close represented node. avoid information loss require total activity nodes local minimum—in words minimum boundary seen ﬁgure turns exists local minimum summed activity nodes values compare performance self-suﬃcient fuzzy memory shift register time series forecasting simple illustrations. goal illustrate diﬀerences simple shift register self-suﬃcient fuzzy memory. interest representation time series sophistication learning algorithm simple linear regression algorithm learn forecast time series. consider three time series diﬀerent properties. ﬁrst generated fractionally integrating white noise manner similar described section second third time series obtained online library http//datamarket.com. second time series mean annual temperature earth year third time series monthly average number sunspots year measured zurich switzerland. three time series plotted corresponding point correlation function series plotted middle examination point correlation functions reveal diﬀerences series. fractionally-integrated noise series shows long-range correlations falling like power law. temperature series shows correlations near zero short ranges weak negative correlation longer times. sunspots data strong positive short-range autocorrelation longer range negative correlation balanced periodicity months corresponding year solar cycle. nmax denote total number nodes memory representation index corresponding node ranging nmax. shall denote value contained nodes time step time series sequentially shift register self-suﬃcient fuzzy memory representations evolved appropriately time step. values shift register nodes shifted downstream time step discussed section instant shift register held information exactly nmax time steps past. values self-suﬃcient fuzzy memory evolved described section time step value nodes recorded along value time series time step denoted used simple linear regression algorithm extract intercept regression coeﬃcients predicted value time series time step squared error prediction fig. time series forecasting. simulated time series long range correlations based arfima model white noise standard deviation average annual temperature earth year monthly average number sunspots year gives values series. middle shows point correlation functions. bottom shows error forecasting series using self-suﬃcient fuzzy memory using shift register accuracy forecast inversely related total squared error absolute measure accuracy factor intrinsic variability time series. bottom plot mean squared error divided intrinsic variance time series var. quantity would range closer zero accurate prediction. long range correlated series long range correlated series deﬁnition constructed yield point correlation decays power law. evident point correlation decaying always positive. since value series time step highly correlated value previous time step expect generate reasonable forecast using single node holds value previous time step. seen error forecast single node. adding nodes reduces error shift register self-suﬃcient fuzzy memory. given number nodes fuzzy memory always lower error shift register. seen curve blue since series generated fractionally integrating white noise mean squared error cannot principle lower variance white noise used construction. lower bound error achieved dotted line indicates bound. note fuzzy memory approaches bound smaller number nodes shift register. temperature series temperature series much noisy long range correlated series seems structureless. seen small values point correlations also reﬂected fact small number nodes error high. hence concluded reliable short range correlation exist series. knowing average temperature given year help much predicting average temperature subsequent year. however seems weak negative correlation longer scales could exploited forecasting. note additional nodes fuzzy memory performs better forecasting lower error forecasting shift register. fuzzy memory represent much longer timescales shift register equal size thereby exploit long range correlations exist. sunspots series sunspot series less noisy series considered oscillatory structure month periodicity. high short range correlations hence even single node holds value previous time step suﬃcient forecast error seen before nodes fuzzy memory consistently lower error forecasting shift register equal number nodes. note number nodes increased shift register improve accuracy fuzzy memory continues improve accuracy. single node fuzzy memory shift register essentially store information previous time step. variance series captured information ﬁrst node diﬀerence fuzzy memory shift register additional nodes numerically overwhelming viewed however qualitative diﬀerence properties signal extracted memory systems. order successfully learn month periodicity information high positive short range correlations suﬃcient essential also learn information negative correlations longer time scales. note negative correlations exist timescale months. hence order learn information timescales represented. shift register nodes cannot represent timescales fuzzy memory nodes can. illustrate possible learn periodicity using fuzzy memory forecast distant future values series. ﬁgure extend sunspots series predicting future months. regression coeﬃcients intercept extracted original series length next time steps predictions treated actual values memory representations evolved. fig. shows series generated using shift register nodes. solid tick mark x-axis represents point original series ends predicted future series begins. note series forecasted shift register immediately settles mean value without oscillation. fig. forecasting distant future. sunspots time series length extrapolated time steps future using shift register nodes self-suﬃcient fuzzy memory nodes. solid tick mark x-axis corresponds point original series ends predicted future series begins. time scale oscillations manifest represented shift register nodes. fig. shows series generated fuzzy memory nodes. note series predicted fuzzy memory continues oscillating fashion decreasing amplitude several cycles eventually settling mean value. possible fuzzy memory represents signal suﬃciently long time scale capture negative correlations two-point correlation function. course shift register many nodes capture long-range correlations predict periodic oscillations signal. however number nodes necessary describe oscillatory nature signal needs order periodicity oscillation case. would lead overﬁtting data. least case simple linear regression algorithm number regression coeﬃcients extracted data increases number nodes extracting large number regression coeﬃcients ﬁnite data unquestionably lead overﬁtting data. hence would ideal least number nodes required span relevant time scale. order ensure extracted regression coeﬃcients overﬁtted data split sunspots time series halves. extracted regression coeﬃcients using ﬁrst half training used second half testing predictions generated coeﬃcients. fig. plots testing error compared training error plotted noticeable fact testing error slightly higher training error shape plots similar fuzzy memory shift register goal capture oscillatory structure sunspot series within small number regression coeﬃcients could subsample lengthy shift register information positive negative correlations obtained. although subsampled shift register contains relatively nodes cannot self suﬃciently evolve real time; would fig. testing error forecasting. regression coeﬃcients extracted ﬁrst half sunspot series testing performed second half series. compare figure bottom row. training error self-suﬃcient fuzzy memory shift register subsampled shift register spacing subsampled shift register spacing need resources associated complete shift register order evolve memory moment. subsampling equidistantly spaced nodes shift register extracting corresponding regression coeﬃcients possible extend series oscillatory structure analogous however turns forecasting error subsampled shift register still higher forecasting error self-suﬃcient fuzzy memory. training error replotted fuzzy memory shift register along training error subsampled shift register equidistant node spacing even though subsampled shift register node spacing extends similar temporal range fuzzy memory captures oscillatory structure data fuzzy memory outperforms lower error. advantage fuzzy memory subsampled shift register comes property averaging many previous values long time scales rather picking single noisy value using prediction. property helps suppress unreliable ﬂuctuations could lead overﬁtting data. self-suﬃcient fuzzy memory holds predictively relevant information shift register number nodes long-range correlated signals hence performs better time series forecasting signals. however learning relevant statistics lengthy time series learning learning trials. learn learning trials learner must necessarily make generalizations based built-in assumptions environment. since fuzzy memory discards information precise time stimulus presentation temporal inaccuracy memory help learner make generalization. suppose useful learner learn temporal relationship events statistics world consistently follows delay period learning trial chosen unknown distribution. many learning trials learner relying shift register memory would able sample entire distribution delays learn precisely. real world learners learn much faster. fuzzy memory system represents past information smeared fashion single training sample distribution naturally learner make scale-free temporal generalization distribution delays temporal proﬁle generalization general match true distribution could learned many learning trials however fact available single learning trial provides tremendous advantage natural learners. seems natural wonder human animal memory resembles fuzzy memory system. animals evolved natural world predicting imminent future crucial survival. numerous behavioral ﬁndings animals humans consistent memory system scale-free representation past events human memory studies forgetting curve usually observed follow scale invariant power function humans asked reproduce discriminate time intervals exhibit characteristic scale-invariance errors produce characteristic feature humans wide variety animals like rats rabbits pigeons ﬁndings across behavioral tasks species suggest scale-free memory adaptive response world structure many temporal scales. consider fuzzy memory system context neural networks temporal memory. realized neural networks generic recurrent connectivity suﬃciently rich dynamics hold temporal memory past. analogous ripple patterns liquid surface contains information past perturbations instantaneous state recurrent network holds memory past simply extracted training linear readout layer. networks implemented either analog neurons– echo state networks spiking neurons–liquid state machines known non-chaotic dynamically stable long spectral radius largest eigenvalue connectivity matrix magnitude less one. abstractly networks ﬁxed recurrent connectivity viewed reservoir nodes eﬃciently used computational tasks involving time varying inputs including time series prediction timescale reservoirs tuned introducing leaky integrator neurons however reservoir ﬁnite nodes cannot memory inﬁnite past. fact criterion dynamical stability reservoir equivalent requiring fading memory deﬁne memory function reservoir precision inputs past moment reconstructed turns memory area memory function past times bounded number nodes reservoir nmax. exact shape memory function however depend connectivity within reservoir. simple shift register connectivity memory function step function nmax time steps past zero beyond nmax. generic random connectivity memory function decays smoothly sometimes exponentially sometimes power depending spectral radius linear recurrent networks turns memory function analytically tractable least special cases presence nonlinearity seems reduce memory. increasing number nodes reservoir memory increased. unless network well tailored e.g. orthogonal network divergent feed-forward network memory grows slowly sub-linearly self-suﬃcient fuzzy memory viewed special case linear reservoir speciﬁc tailored connectivity. nodes eﬀectively diagonal connectivity matrix making leaky integrators linear readout weights approximately extracts past inputs. white noise input signal memory function decays power exponent memory grows linearly number nodes. however described above accuracy reconstruction past relevant quantity interest here predictive information past interest. scale-free ﬂuctuations natural world imply isn’t necessary accurate; fact sacriﬁcing accuracy scale-free fashion lets represent predictive information exponentially long timescales. basic issues essential real-world machine learning applications ignored work sake theoretical simplicity. first simply focused scalar time varying signal serious learning problem would involve multidimensional signals. unlimited storage resources available dimension separately represented lengthy shift register. conserve storage resources associated time dimension could replace shift register self-suﬃcient fuzzy memory. work however address issue conserving storage resources compressing intrinsic dimensionality signal. general information relevant future prediction encoded combinations various dimensions signal called features. techniques based information bottleneck method slow feature analysis eﬃciently extract features. strategy representing time series scale invariantly fuzzy fashion could seen complementary techniques. instance slow feature analysis imposes slowness principle low-dimensional features change slowly interest. components time varying high dimensional signal represented temporally fuzzy fashion rather shift register could potentially extract slowly varying parts online fashion examining diﬀerences activities largest second issue ignored learning prediction mechanisms simply focusing memory representation. simplicity used linear regression predictor section serious application involve ability learn nonlinearities. support vector machines adopt elegant strategy using nonlinear kernel functions input data high dimensional space linear methods used standard method training svms time series prediction requires feeding data sliding time window words providing shift registers input. recently suggested rather using standard kernels sliding time window used recurrent kernel functions corresponding inﬁnite recurrent networks performance improved certain tasks suggests gradually fading temporal memory recurrent kernel functions eﬀective step-function memory shift register used standard svms time series prediction. training svms standard kernel functions along fuzzy memory inputs rather shift register inputs alternative strategy approaching problems involving signals long range temporal correlations. moreover since nodes contain temporal information needed construct fuzzy memory directly training svms inputs nodes could also fruitful. svms deep learning networks rely batch processing requires availability entire data prior learning. autonomous agents limited memory resources cannot adopt learning strategies. learning mechanism cannot rely information instantaneously available memory. online learning algorithm tailored fuzzy memory representation could potentially useful autonomous agents ﬁnite memory resources. signals long-range temporal correlations found throughout natural world. signals present distinct challenge machine learners rely shift-register representation time series. described method constructing self-suﬃcient scale-free representation temporal history. nodes chosen minimizes information redundancy information loss equally distributing time scales. although temporal accuracy signal sacriﬁced predictively relevant information exponentially long timescales available fuzzy memory system compared shift register number nodes. could extremely useful represent time series long-range correlations machine learning applications. information redundancy memory representation quantiﬁed deriving expressions mutual information shared neighboring nodes. input signal uncorrelated scale-free long range correlations shown information redundancy equally spread nodes note correlation nonzero decays exponentially large hence even temporally uncorrelated white noise input leads short range temporal correlations node. important emphasize temporal correlations introduced shift register. because shift register functional value moment passed downstream nodes without integrated temporal autocorrelation activity node simply reﬂect temporal correlation input function. consider instantaneous correlation activity diﬀerent nodes. instant activity diﬀerent nodes shift register uncorrelated response white noise input. diﬀerent nodes shift register carry completely diﬀerent information making mutual information zero. column since information smeared across diﬀerent nodes mutual information shared diﬀerent nodes non-zero. instantaneous correlation diﬀerent nodes instantaneous correlation activity nodes measure mutual information represented them. factoring individual variances nodes following measure mutual information. fact mutual information shared neighboring nodes non-vanishing implies redundancy representation information. require information redundancy equally distributed nodes need mutual information neighboring nodes constant. neighboring nodes constant. happen k+r+ exact value unimportant integral diverges however interested case large entire contribution integral comes region denominator integrand approximated eﬀect voss clarke noise music speech nature vol. mandelbrot fractal geometry nature. fransisco freeman field relations statistics natural images response properties cortical linkenkaer-hansen nikouline palva ilmoniemi long-range temporal correlations scaling behavior human brain oscillations journal neuroscience vol. rakitin gibbon penny malapani hinton meck scalar expectancy theory peak-interval timing humans journal experimental psychology animal behavior processes vol. maass natschl¨ager markram real-time computing without stable states framework neural computation based perturbations neural computation vol. vapnik statistical learning theory. york wiley mueller smola rtsch scholkopf kohlmorgen vapnik predicting time series support vector machines proceedings international conference analog neural networks", "year": 2012}