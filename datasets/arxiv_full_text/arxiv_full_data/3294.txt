{"title": "Learning with a Wasserstein Loss", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Learning to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that can be used to improve predictions. In this paper we develop a loss function for multi-label learning, based on the Wasserstein distance. The Wasserstein distance provides a natural notion of dissimilarity for probability measures. Although optimizing with respect to the exact Wasserstein distance is costly, recent work has described a regularized approximation that is efficiently computed. We describe an efficient learning algorithm based on this regularization, as well as a novel extension of the Wasserstein distance from probability measures to unnormalized measures. We also describe a statistical learning bound for the loss. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the Yahoo Flickr Creative Commons dataset, outperforming a baseline that doesn't use the metric.", "text": "learning predict multi-label outputs challenging many problems natural metric outputs used improve predictions. paper develop loss function multi-label learning based wasserstein distance. wasserstein distance provides natural notion dissimilarity probability measures. although optimizing respect exact wasserstein distance costly recent work described regularized approximation efﬁciently computed. describe efﬁcient learning algorithm based regularization well novel extension wasserstein distance probability measures unnormalized measures. also describe statistical learning bound loss. wasserstein loss encourage smoothness predictions respect chosen metric output space. demonstrate property real-data prediction problem using yahoo flickr creative commons dataset outperforming baseline doesn’t metric. consider problem learning predict non-negative measure ﬁnite set. problem includes many common machine learning scenarios. multiclass classiﬁcation example often predicts vector scores probabilities classes. semantic segmentation model segmentation support measure deﬁned pixel locations. many problems output learning machine non-negative multi-dimensional might cast predicting measure. speciﬁcally focus problems output space natural metric similarity structure known priori. practice many learning problems structure. imagenet large scale visual recognition challenge example output dimensions correspond object categories inherent semantic relationships captured wordnet hierarchy accompanies categories. similarly keyword spotting task iarpa babel speech recognition project outputs correspond keywords likewise semantic relationships. follows call similarity structure label space ground metric semantic similarity. figure wasserstein loss encourages predictions similar ground truth robustly incorrect labeling similar classes shown euclidean distance prediction ground truth number classes averaged different noise levels noise level averaged number classes. baseline multiclass logistic loss. severe error confusing breeds dogs. loss function incorporates metric might encourage learning algorithm favor predictions completely accurate least semantically similar ground truth. paper develop loss function multi-label learning measures wasserstein distance prediction target label respect chosen metric output space. wasserstein distance deﬁned cost optimal transport plan moving mass predicted measure match target applied wide range problems including barycenter estimation label propagation clustering knowledge paper represents ﬁrst wasserstein distance loss supervised learning. brieﬂy describe case wasserstein loss improves learning performance. setting multiclass classiﬁcation problem label noise arises confusion semantically near-equivalent categories. figure shows case ilsvrc categories siberian husky eskimo nearly indistinguishable. synthesize version problem identifying categories points euclidean plane randomly switching training labels nearby classes. wasserstein loss yields predictions closer ground truth robustly across noise levels shown figure standard multiclass logistic loss baseline comparison. section appendix describes experiment detail. main contributions paper follows. formulate problem learning prior knowledge ground metric propose wasserstein loss alternative traditional information divergence-based loss functions. speciﬁcally focus empirical risk minimization wasserstein loss describe efﬁcient learning algorithm based entropic regularization optimal transport problem. also describe novel extension unnormalized measures similarly efﬁcient compute. justify wasserstein loss showing statistical learning bound. finally evaluate proposed loss synthetic examples real-world image annotation problem demonstrating beneﬁts incorporating output metric loss. decomposable loss functions like divergence distances popular probabilistic vector-valued predictions component evaluated independently often leading simple efﬁcient algorithms. idea exploiting smoothness label space according prior metric explored many different forms including regularization post-processing graphical models optimal transport provides natural distance probability distributions metric spaces. optimal transport used formulate wasserstein barycenter probability distribution minimum total wasserstein distance given points probability simplex. propagates histogram values graph minimizing dirichlet energy induced optimal transport. wasserstein distance also used formulate metric comparing clusters applied image retrieval contour matching many problems however knowledge ﬁrst time used loss function discriminative learning framework. closest work paper theoretical study estimator minimizes optimal transport cost empirical distribution estimated distribution setting statistical parameter estimation. problem setup notation consider problem learning space measures ﬁnite size assume possesses metric called ground metric. measures semantic similarity dimensions output correspond elements perform learning hypothesis space predictors parameterized might linear logistic regression models example. standard statistical learning setting i.i.d. sequence training examples sampled unknown joint distribution px×y. given measure performance goal predictor minimizes expected risk y)]. typically difﬁcult optimize directly joint distribution px×y unknown learning performed empirical risk minimization. speciﬁcally solve information divergence-based loss functions widely used learning probability-valued outputs. along popular measures like hellinger distance distance divergences treat output dimensions independently ignoring metric structure given cost function optimal transport distance measures cheapest transport mass probability measure match joint probability measures marginals. important case cost given metric p-th power case called wasserstein distance also known earth mover’s distance paper work discrete measures. case probability measures histograms simplex ground truth output simplex deﬁne wasserstein loss. deﬁnition predicted value element given input ground truth value given corresponding label deﬁne exact wasserstein loss cost optimal plan transporting predicted mass distribution match target distribution penalty increases mass transported longer distances according ground metric learning optimize empirical risk minimization functional gradient descent. requires evaluating descent direction loss respect predictions unfortunately computing subgradient exact wasserstein loss quite costly follows. cuturi proposes smoothed transport objective enables efﬁcient approximation transport matrix subgradient loss. introduces entropic regularization term results strictly convex problem identifying matrix subject equality constraints column sums exactly matrix balancing problem well-studied numerical linear algebra efﬁcient iterative algorithms exist well-known sinkhorn-knopp algorithm. output vectors simplex used directly place approximate exact wasserstein distance closely large enough case gradient objective obtained optimal scaling vector sinkhorn iteration gradient given algorithm many learning problems however normalized output assumption unnatural. image segmentation example target shape naturally represented histogram. even prediction ground truth constrained simplex observed label subject noise violates constraint. generalize optimal transport unnormalized measures subject active study develop novel objective deals effectively difference total mass still efﬁcient optimize. propose novel relaxation extends smoothed transport unnormalized measures. replacing equality constraints transport marginals soft penalties respect divergence unconstrained approximate transport problem. resulting objective λγaγb represents element-wise division. previous formulation optimal transport matrix respect diagonal scaling matrix proposition transport matrix optimizing satisﬁes diagkdiag unlike previous formulation unconstrained respect gradient given ∇hwkl iteration given algorithm restricted normalized measures relaxed problem approximates smoothed transport figure shows normalized relative distance values large enough converges increase. also retains properties smoothed transport figure shows that normalized outputs relaxed loss converges unregularized wasserstein distance increase figure shows convergence iterations nearly independent dimension output space. assume composition softmax base hypothesis space functions mapping softmax layer outputs prediction lies simplex theorem probability least holds constant maxκκ mκκ. rademacher complexity measuring complexity hypothesis space rademacher complexity commonly used models like neural networks kernel machines decays training size. theorem guarantees expected wasserstein loss empirical risk minimizer approaches best achievable loss important special case minimizing empirical risk wasserstein loss also good multiclass classiﬁcation. one-hot encoded label vector groundtruth class. proposition multiclass classiﬁcation setting probability least holds predictor argmaxκ empirical risk minimizer. note instead classiﬁcation error actually bound expected semantic distance prediction groundtruth. section show wasserstein loss encourages smoothness respect artiﬁcial metric mnist handwritten digit dataset. multi-class classiﬁcation problem output dimensions corresponding digits apply ground metric metric encourages recognized digit numerically close true one. train model independently value plot average predicted probabilities different digits test figure figure top-k cost comparison proposed loss baseline note metric approaches metric treats incorrect digits equally unfavorable. case seen ﬁgure predicted probability true digit goes probability digits goes increases predictions become evenly distributed neighboring digits converging uniform distribution apply wasserstein loss real world multi-label learning problem using recently released yahoo/flickr creative commons dataset goal prediction select descriptive tags along random sets images each associated tags training testing. derive distance metric tags using wordvec embed tags unit vectors taking euclidean distances. extract image features matconvnet note tags highly redundant often many semantically equivalent similar tags apply image. images also partially tagged different users prefer different tags. therefore measure prediction performance top-k cost minj {κj} groundtruth tags {ˆκk} standard multiclass logistic loss linear combination wasserstein loss yields best prediction results. speciﬁcally train linear model minimizing training controls relative weight note taken alone baseline experiments. figure shows top-k cost test combined loss baseline loss. additionally create second dataset removing redundant labels original dataset simulates potentially difﬁcult case single user tags image selecting apply amongst cluster applicable semantically similar tags. figure shows performance algorithms decreases harder dataset combined wasserstein loss continues outperform baseline. figure show effect performance varying weight loss. observe optimum top-k cost achieved wasserstein loss weighted heavily optimum auc. consistent semantic smoothing effect wasserstein training favor mispredictions semantically similar ground truth sometimes cost lower ﬁnally show selected images test figure illustrate cases algorithms make predictions semantically relevant despite overlapping little ground truth. image left shows errors made algorithms. examples found appendix. dataset used available http//cbcl.mit.edu/wasserstein. wasserstein loss achieve similar trade-off choosing metric parameter discussed section however relationship smoothing behavior complex simpler implement trade-off combining loss. paper described loss function learning predict non-negative measure ﬁnite based wasserstein distance. although optimizing respect exact wasserstein loss computationally costly approximation based entropic regularization efﬁciently computed. described learning algorithm based regularization proposed novel extension regularized loss unnormalized measures preserves efﬁciency. also described statistical learning bound loss. wasserstein loss encourage smoothness predictions respect chosen metric output space demonstrated property real-data prediction problem showing improved performance baseline doesn’t incorporate metric. interesting direction future work explore connection wasserstein loss markov random ﬁelds latter often used encourage smoothness predictions inference prediction time.", "year": 2015}