{"title": "On Coarse Graining of Information and Its Application to Pattern  Recognition", "tag": ["cs.CV", "stat.ML"], "abstract": "We propose a method based on finite mixture models for classifying a set of observations into number of different categories. In order to demonstrate the method, we show how the component densities for the mixture model can be derived by using the maximum entropy method in conjunction with conservation of Pythagorean means. Several examples of distributions belonging to the Pythagorean family are derived. A discussion on estimation of model parameters and the number of categories is also given.", "text": "propose method based ﬁnite mixture models classifying observations number diﬀerent categories. order demonstrate method show component densities mixture model derived using maximum entropy method conjunction conservation pythagorean means. several examples distributions belonging pythagorean family derived. discussion estimation model parameters number categories also given. goals scientiﬁc study identify regularities observations classify possibly separate simpler structures categories. categories turn used make inferences objects interest. major advantage approach breaks complicated reality collection simpler structures. similar pattern recognition concern discovery regularities data computer algorithms used classify data diﬀerent categories independent ones point view analysis must start deﬁnition categories. suﬃcient information categories members easy task establish precise deﬁnition. however real life situations case notion category cannot precisely deﬁned. conditions fruitful approach consider category collection objects likely share properties. cases information available insuﬃcient reach certainty ought quantify degree believe object major bulk literature subject dedicated numerical aspect problem. acknowledging numerical challenges seriously compromise applicability method believe fundamental problem modelling categories partial knowledge condition important. following look class problems pattern recognition possession empirical distribution objects interests prior knowledge number categories involved. propose approach modelling empirical distributions based ﬁnite mixture models relies identifying relevant intensive properties category. order demonstrate method show conservation pythagorean means encountered class intensive properties conjunction maximum entropy method used derive functional form mixture model. also brieﬂy discuss extension conserved quantities also give short overview numerical challenges related inference problem. article restrict positive univariate continuous quantities. situations categories cannot deﬁned precisely probabilistic description might possible option. probabilistic framework talk likelihood object belonging category. assume experiment observation made suﬃcient uniquely determine category belongs example observations height people certain region/country underlying categories groups individual might belong cases considers random variable tries model probability density function approach model based called ﬁnite mixture models underlying assumption approach convex combination densities density represents single category. cases says ﬁnite mixture distribution ﬁnite mixture density function. parameters called mixing weights component densities mixture. context pattern recognition number categories density function describing distribution members category emphasis component densities necessarily belong family densities. component density represents best guess structure respective category existence independent categories. order able adopt mixture model speciﬁc problem given priori knows number categories requires tackles diﬀerent problems. ﬁrst problem determine achieve quantitative description state partial knowledge i.e. determining functional form component densities. second problem determine based available evidence i.e. empirical density. general objects category similar categories. similarity invokes notion properties coarser level distinguishes categories other. fact consider category homogeneous group members recognizably similar reasonable assume properties distinguish categories intrinsic independent coarse graining within category itself. coarse graining property concept ﬁnding component distributions. general coarse graining achieved ﬁrst grouping elements category blocks volume. following predetermined rule block replaced single element representing elements block. procedure iterated inﬁnitum. call property invariant coarse graining intensive. context category characterized distinguished others intensive properties. identifying relevant intensive properties often challenging. usually less challenging approach ﬁrst determine so-called extensive properties category. extensive property property additive coarse graining. coarse graining elements replace blocks step also inherit extensive properties respective block elements. moreover coarse graining step similarity homogeneity conditions exist among members category extensive properties scale independent choice speciﬁc block. this general results greatly reducing complexity analysis. however conceivable might discover many extensive properties might relevant classiﬁcation problem hand. respect choice relevant properties often problem dependent. nevertheless identifying describing extensive property means able function scaling factor captures essential features property. shown expectation function invariant respect coarse graining hence intensive. example particle mass extensive property system consisting collection particles. whilst expected mass particle intensive. following shall call intensive properties expressed form expectations conservation laws expected values functions constitute so-called pythagorean means. pythagorean means arithmetic geometric harmonic mean. precisely positive univariate continuous variable density pythagorean means deﬁned although conserved quantities restrict possible distribution elements category nonetheless still might inﬁnitely many distributions satisfy constraints. interested distribution conserves quantities interest allowing maximum degree freedom non-conserved quantities. shown among distributions fulﬁll constraints uncommitted distribution largest relative entropy four lagrange multipliers corresponding four constraints. shown maximizing functional equivalent solving corresponding euler-lagrange equation calculus variations results statement theorem. shall distribution share functional form belongs pythagorean family distributions. note even improper non-compact support long distribution normalizable. know normalization constant functional form prior values pythagorean means uniquely determined. moreover note narrow whilst broad inﬂuence negligible considered uniform distribution. following corollary direct consequence literature distribution known generalized inverse gaussian distribution well-known sub-classes inverse gaussian reciprocal inverse gaussian hyperbolic distributions familiar distributions arise pythagorean means conserved. example drops constraint arithmetic mean distribution known inverse gamma distribution. constraint harmonic mean dropped distribution gamma distribution. list longer examples demonstrate abundance diﬀerent variety distributions belonging pythagorean family. sake clarity narrowed discussions conservation pythagorean means. conservation laws possible even use. maxent method handle conserved quantities well. nonetheless recommended always conduct assessment conservation pythagorean means start analysis. outcome used prior along conserved quantities derive functional form component densities. summarize information functional form component densities number. technically known determining becomes standard problem statistical inference. assume observations randomly generated normalized histogram data considered empirical estimate consequently unknown estimated using bayesian methods. indeed follows bayes rule prior usually rough knowledge domain therefore common assume uniformly distributed domain. likelihood function depends assessment sources contribute deviation model data general problem speciﬁc likely estimate coincides global maximum called maximum posteriori probability estimate. usually intractability analytical form posterior distribution methods estimating monte carlo based illustration purpose fig. plotted example mixture model three component densities versus joint simulated histogram. discussions assumed number categories known. however often know number need estimate bayesian framework known model selection problem. indeed order estimate number categories need evaluate posterior distribution conditional bayes rule figure dots represent histogram numbers simulated mixture three gig-variates. dashed curves component densities variates times respective mixing weights. three dashed curves mixture model red. uniform probable value corresponds model largest evidence. often quite challenging good estimate evidence. methods monte carlo based pros cons. therefore choice method much application dependent. uncommon uses several diﬀerent methods order good estimate. overview used methods reader referred situations partial knowledge categories probabilistic description based ﬁnite mixture model possible approach. order determine component densities model start ﬁnding relevant extensive properties category coarse graining. taking expectation extensive properties lead right conservation laws conjunction maxent component densities. model parameters estimated empirical density data using standard bayesian methods.", "year": 2014}