{"title": "Hierarchical Subquery Evaluation for Active Learning on a Graph", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "To train good supervised and semi-supervised object classifiers, it is critical that we not waste the time of the human experts who are providing the training labels. Existing active learning strategies can have uneven performance, being efficient on some datasets but wasteful on others, or inconsistent just between runs on the same dataset. We propose perplexity based graph construction and a new hierarchical subquery evaluation algorithm to combat this variability, and to release the potential of Expected Error Reduction.  Under some specific circumstances, Expected Error Reduction has been one of the strongest-performing informativeness criteria for active learning. Until now, it has also been prohibitively costly to compute for sizeable datasets. We demonstrate our highly practical algorithm, comparing it to other active learning measures on classification datasets that vary in sparsity, dimensionality, and size. Our algorithm is consistent over multiple runs and achieves high accuracy, while querying the human expert for labels at a frequency that matches their desired time budget.", "text": "unlabeled examples. therefore work within popular graph based semi-supervised learning framework image represented vertex weighted graph weights encode similarity image feature vectors vertices already queried labels. whether human done providing class labels classiﬁcation datapoints performed directly feature space propagating available label information graph. designing graph based framework requires three steps building graph unlabeled datapoints feature-space selection criterion measuring informativeness possible queries selecting inference method evaluating criterion graph. many beneﬁts framework forming right combination three acknowledged challenge. steps especially inﬂuenced criterion chosen decide unlabeled image next query. particular expected error reduction attractive naive incarnations prohibitively costly. query oracle preceded computing subqueries unlabeled example; subquery simulates updated predictions would change individual datapoint received label oracle. therefore propose method graph construction good right crucially organizes data criterion exploited effectively. building graph construction main contribution proposed hierarchical subquery evaluation allows oracle label maximizes without compute exhaustively unlabeled images without heuristics hurt overall learning curve. many experiments show signiﬁcant beneﬁts computing traversing hierarchical representation data cope datasets broad variety sparsity dimensionality size balance exploration exploitation good accuracy quickly reﬁne decision boundaries needed within time budget speciﬁed user empirically highly consistent accuracy train good supervised semi-supervised object classiﬁers critical waste time human experts providing training labels. existing active learning strategies uneven performance efﬁcient datasets wasteful others inconsistent runs dataset. propose perplexity based graph construction hierarchical subquery evaluation algorithm combat variability release potential expected error reduction. speciﬁc circumstances expected error reduction strongest-performing informativeness criteria active learning. also prohibitively costly compute sizeable datasets. demonstrate highly practical algorithm comparing active learning measures classiﬁcation datasets vary sparsity dimensionality size. algorithm consistent multiple runs achieves high accuracy querying human expert labels frequency matches desired time budget. bespoke object recognizers almost mature enough useful people practice. major hurdle procure enough training labels tune semi-supervised model speciﬁed classiﬁcation task. unskilled mechanical turkers willing label images food image costs massive recruiting paying specialists like doctors scientists. whether experts part online crowd people need practical reliable active learning suggest unlabeled image they oracle label next. choosing query images right order gives better classiﬁcation fewer interrogations oracle. training session classiﬁer model starts unlabeled examples picks queries human label quickly re-trains classiﬁer process repeat queries selected among remaining labeling given dataset. experiments benchmark approach alternative criteria alternative graph constructions establish repeatability approach across different datasets. cover relevant related works recommend thorough overview active learning. active learning successfully applied many different computer vision problems including tracking image categorization object detection semantic segmentation image video segmentation human automatic oracles compared body work active learning general relatively active learning methods image classiﬁcation facilitate interactive annotation. challenge creating interactive algorithms time retrain model labeled example provided long performed incrementally. delay also exacerbated type active learning criterion used. propose object detection based efﬁcient incremental training hough voting forests. operating real-time system able predict annotation cost image provides feedback user. however exploit unlabeled data pool updating model. batra present system interactive image co-segmentation asks user annotate region deemed informative current model. wang perform cell image annotation using semi-supervised graph labeling approach exploit fast updating graph interactive annotations. unlike work explore merits different active learning criteria. semi-supervised active learning pool based active learning access unlabeled data front querying oracle. contrast standard supervised learning semi-supervised learning exploits structure unlabeled data. paper concerned graph based however proposed subquery evaluation scheme applied pool based active learning task unlabeled data available training. graph based datapoints represented nodes graph edges nodes encode similarity feature space. premise datapoints near feature space share label. graph based transductive algorithms efﬁcient evaluate closed form typically requiring simple matrix operations propagate label information around graph. graph based propose approach based deﬁning harmonic functions gaussian random ﬁelds. advantage method that unlike graph based formulations produces probability distribution possible class labels datapoint. real probabilities opens door broader range active learning strategies. method zhou adds additional regularization balancing information node receives labeled neighbors expense allowing labeled node change class. methods also possible include label regularization term address class imbalance data number datapoints increases quickly become infeasible perform large matrix inversions required many graph based algorithms. iterative algorithms require matrix inversion take many iterations converge options overcome scalability issue include reducing effective graph size using mixture models feature space nonparametric regression labels subset anchor nodes assuming data dimensionally separable order approximate eigenvectors normalized graph laplacian graph construction well known graph based methods highly sensitive choice edge weights standard approach graph construction ﬁrst sparsify fully-connected graph reweight remaining edges. sparsiﬁcation important higher dimensions distances away points become less meaningful. k-nearest neighbor distance thresholding common choices sparsiﬁcation. however suffer problem resulting graph uneven guarantee number edges node. approaches exist guarantee regular graphs computationally costly however small decrease graph quality possible build approximately regular graphs reduced cost reweighting step similarity measure datapoints must deﬁned. standard choice similarity kernel several methods proposed deﬁne suitable bandwidth parameter. labeled datapoints learned alternatively deﬁned dimension based average distance neighbors local distance direct optimization wang jointly learn graph structure label prediction minimizing cost function graph labeling. paper propose method graph reweighting inspired ideas dimensionality reduction active learning graphs many different active learning criteria exist literature. methods range random querying uncertainty sampling margin reduction density sampling expected model change expected error reduction optimal strategy would trade between exploration exploitation; initially exploring dimensional feature vector corresponding class label. split disjoint sets corresponding sets unlabeled labeled examples. active learning labeled examples initially empty oracle knows values deﬁne graph vertices corresponding pool examples edges represented connectivity weight matrix rn×n entry represents similarity feature space datapoints goal estimate distribution class labels nodes graph matrix notation distributions represented matrix different datapoint. propose method semi-supervised learning based gaussian random ﬁelds harmonic energy minimization harmonic energy minimization computed closed form using matrix operations graph laplacian unknown conditional distribution output input marginal input distribution. taking labeled data produce learner estimates class output distribution ˆpdl given input expected error learner space labels uncertainty high then annotations acquired exploit information perform boundary reﬁnement between classes. algorithms switch density based uncertainty sampling typically require hyperparameters dataset speciﬁc however complex approaches strive automatically expected error reduction performs trade naturally. instead measuring surrogate seeks datapoints make overall class distributions unlabeled data discriminative attempting reduce model’s future generalization error. mine example minimizes expected error current model size dataset. complexity stems needing retrain model subqueries unlabeled pool evaluate expected error. efﬁcient update methods commonly known algorithms exist e.g. graph based making full feasible small graphs. demonstrated superior performance active learning criteria combining gaussian ﬁelds formulation serves baselines. clustering approaches cope larger datasets different approaches proposed reduce number subqueries must evaluated. strategies include considering subsample full data using inherent structure data limit inﬂuence selection subqueries using manifold assumption methods cluster data original feature space. macskassy explores graph based metrics commonly used community detection identify cluster centers evaluated using eer. related hierarchical clustering method category discovery vatturi however limiting subqueries cluster centers clustering based approaches unable perform boundary reﬁnement. used deﬁne bounds sampling statistics. every samples randomly selected strict partition prespeciﬁed clustering shares label information within cluster. proposed method also uses hierarchical representation differs uses hierarchy efﬁcient sampling using added advantages graph based without sacriﬁcing ability reﬁne class boundaries. noted previously graph based algorithms sensitive choice similarity matrix datapoints label want corresponding afﬁnity high different want low. popular choice similarity kernel radial basis function expxi distance distances appropriate depending data representation introduced parameters control bandwidth kernel. single choice unlikely optimal across whole dataset. want model density local space. intuitively want larger value dense regions feature space smaller value sparse regions. deﬁne similarity based successful unsupervised technique dimensionality reduction. stochastic neighbor embedding nonsymmetric similarity points represented conditional probability. interpreted probability would pick neighbor assuming gaussian variance centered perform binary search perplexity given choice deﬁned criterion dictates pick datapoint giving lowest expected error labeled next. refer calculating expected error single unlabeled datapoint subquery; complexity single subquery linear number unlabeled datapoints. together subqueries internal calculations used determine next query sent oracle labeling. want next query within speciﬁed query budget. means sufﬁcient time perform subqueries possible unlabeled nodes since results quadratic cost instead must identify adaptive number best subqueries sample within allotted time ideally sub-linear number unlabeled nodes. smooth nature harmonic solutions respect proximity nodes graph creates redundancy densely sampling nodes; neighboring nodes likely produce similar reduction error labeled. hierarchical clustering graph example expected error. practice true conditional distribution unknown approximate using current estimate learner ˆpdl context active learning would like select oracle’s next query unlabeled data adding labeled data would result learner lower expected error. leads greedy selection strategy. first determine expected error combinations unlabeled example taking possible label {..c} learner added labeled data. calculate expectation risk across possible label values learner’s current posterior ˆpdl approximate unknown true distribution across provide integrated active learning framework exhaustively calculating expected error possible unlabeled nodes. even proposed matrix update efﬁciencies calculating expected error datapoint linear operation evaluating unlabeled examples results time commethod uses active learning criterion overcoming expense exhaustive sampling. without sacriﬁcing desirable exploration/exploitation properties issue previous subsampling approaches. discuss hierarchical subquery search method ﬁrst describe graph construction technique found work well criterion robust across wide variety datasets. figure hierarchical clustering subquery sampling strategy. hierarchical clustering built using shown tree. level every node tree represented unique allocation speciﬁc datapoint hierarchical algorithm determine subqueries perform; subquery evaluates expected error shown number inside node. active shown orange constructed containing children labeled nodes; evaluated ﬁrst subqueries prioritizing bottom. active expanded greedy fashion including children subquery lowest expected error shown pink. repeat process exhausted subquery budget. query oracle label chosen subquery lowest expected error figure exploits local correlations neighboring nodes. previous approaches reducing number subqueries included random sub-sampling using community detection propose candidates latter method equivalent performing breadth ﬁrst search cluster hierarchy graph communities represented high level clusters. similar breadth ﬁrst searches hierarchies used active learning albeit without criterion main advantage criterion trade-off reduction error achieved either labeling unknown region reﬁning decision boundaries current model typically exploration mode label nodes high hierarchy whereas detailed boundary reﬁnement occur leaves tree. breadth ﬁrst approach achieve good initial results active learner stuck exploratory mode since effectively sampling graph density measure. proposed approach allow measure perform exploration/exploitation trade-off still sub-sampling unknown nodes dramatically reduce number subqueries therefore cost. achieve performing adaptive search hierarchy. hierarchical subquery sampling authority-shift hierarchy creation provide illustrative example hierarchical clustering figure make authority-shift algorithm require feature space operates perplexity graph directly. technique produces hierarchical clustering graph authority seeking process allocating node local ‘authority’ node calculation explores steady state random walks graph appropriate scale. increasing scale parameter iteratively hierarchy clusters built form tree. approach advantages. first cluster tree represented speciﬁc datapoint used perform subquery. second clusters encode walks graph transition matrix used evaluate harmonic function therefore produce summary results calculating expected error datapoints cluster. subquery sampling overview hierarchical sampling algorithm provided figure differ previous breadth ﬁrst searching strategies allowing adaptive search tree greedily seek minimum reduction expected error. referring diagram consider data cluster hierarchy figure nodes already queried labeled; left side figure first build active unlabeled nodes containing children labeled nodes starting root. proceed perform batch subqueries active obtain expected error expand active adding children subquery current active minimum expected error children added active evaluated subqueries; right side figure process repeats exhausted budget subqueries select member active minimal expected error next query labeled oracle. prioritize subquery evaluation level hierarchy ranking nodes based total number descendants. table datasets used evaluation feat refer number datapoints dimensionality number classes representation. results presented areas learning curve learning curves subset datasets depicted figure method outperforms baselines including full despite requiring fewer subquery evaluations. advantage approach. reﬁne boundary classes need oracle label nodes edges clusters; usually found hierarchy. improves moves toward decision boundary active move tree criterion favors exploitation improvement exploration; exploration occurs labeling clusters tree. breadth ﬁrst search large number queries would performed reaching nodes exploitation depth. learning curve evolves boundary reﬁnement nodes become increasingly localized making unlikely found random subqueries alone. always take root node tree ﬁrst query observe empirically confer good performance makes algorithm deterministic. tree construction means entire hierarchy table describes vision standard machine learning datasets used experiments. chosen vary size density respective feature spaces different numbers classes. experiments start random queries construct graphs nearest neighbors based distance perplexity value query oracle times. method number subqueries number datapoints given dataset initial queries ﬁrst nodes hierarchy. data code available project webpage. graph construction graph based algorithms produce inferior performance poor graphs. using method evaluate graphs table compares perplexity based graph construction method four baseline algorithms testing contribution isolation. mean bandwidth kernel using average distance neighbors. binary constant value nodes connected zero elsewhere. bandwidth datapoint proportional k-nearest neighbors. finally local linear embedding approach perplexity based graph performs best overall. active learning criteria compare algorithm seven different baselines including random entropy margin based criteria full recent time varying combination approach ralf also compare different subquery evaluation strategies random breadth ﬁrst competing subquery strategies evaluated using number subqueries method. methods perplexity based graph exception ralf uses binary based graph representation. empirically found ralf perform worse using graphs. table summarizes overall results area learning curve unlabelled set. still greedy algorithm iteration therefore necessarily globally optimal. approach encourage exploration start queries performed active hierarchy observed offer improved performance. figure learning curves illustrating performance approach versus three baselines table shaded regions around learning curve represents standard deviation. method gives superior results compared deterministic results vary different runs. last plot illustrate effect increasing number subqueries method. number increases area curve. table comparison different graph construction methods. results represent area learning curves method perplexity based method outperforms baselines. noticeable exception cropped pascal dataset high variability class likely dataset conform clustering assumption semi-supervised learning. using iterative label propagation algorithm propagation steps prevents ralf overﬁtting dataset expense worse marginals. figure illustrates learning curves subset datasets. table depicts average time required present next query user different active learning methods. ralf scales linearly full soon becomes impractical number examples increases. average method computes queries second performs better methods terms accuracy. table average time query active learning methods different area-under-learning curve across datasets varying complexity. ralf pick next query second. method allow subqueries query rather full required method. accurate saving human effort speed also factor human oracle’s patience ﬁnite. generalizing slightly active learning approach performs accurately better effective computational complexity ebert computational compractice matlab implementation default settings combined subqueries needed pick oracle’s next query ﬁnished second even largest datasets tested. bigger datasets users algorithm fewer subqueries keep labeling interactive. main competitors good excelling speciﬁc datasets. therefore important validation approach considered accuracy efﬁciency generalizability variety situations. online supplementary material illustrates across datasets hierarchical subquery evaluation leads accurate results form steep learning curves large areas curve results consistent across multiple runs plotted standard deviation curve’s mean. tease apart impact hierarchical subquery evaluation perplexity-based graph construction gave graphs compatible baseline algorithms. among them without graphs performs worse ralf. within ﬂexible graph based framework choices also impact part supplemental ﬁles also show used ralf effective label propagation al.’s grf. several exciting avenues future work. approach transductive would attractive either embed datapoints existing graph online transfer learned parameters inductive model. would also interesting budget subqueries account labels taking oracle’s time effort others. finally similarity graph computed ofﬂine never updated. future wish label information user learn feature representation online. acknowledgements funding research provided epsrc grants ep/k/ ep/j/ ep/i/. macskassy. using graph-based metrics empirical risk minimization speed active learning networked data. nene nayar murase. columbia object image", "year": 2015}