{"title": "Expected Similarity Estimation for Large-Scale Batch and Streaming  Anomaly Detection", "tag": ["cs.LG", "cs.AI"], "abstract": "We present a novel algorithm for anomaly detection on very large datasets and data streams. The method, named EXPected Similarity Estimation (EXPoSE), is kernel-based and able to efficiently compute the similarity between new data points and the distribution of regular data. The estimator is formulated as an inner product with a reproducing kernel Hilbert space embedding and makes no assumption about the type or shape of the underlying data distribution. We show that offline (batch) learning with EXPoSE can be done in linear time and online (incremental) learning takes constant time per instance and model update. Furthermore, EXPoSE can make predictions in constant time, while it requires only constant memory. In addition, we propose different methodologies for concept drift adaptation on evolving data streams. On several real datasets we demonstrate that our approach can compete with state of the art algorithms for anomaly detection while being an order of magnitude faster than most other approaches.", "text": "present novel algorithm anomaly detection large datasets data streams. method named expected similarity estimation kernel-based able efﬁciently compute similarity data points distribution regular data. estimator formulated inner product reproducing kernel hilbert space embedding makes assumption type shape underlying data distribution. show ofﬂine learning expose done linear time online learning takes constant time instance model update. furthermore expose make predictions constant time requires constant memory. addition propose different methodologies concept drift adaptation evolving data streams. several real datasets demonstrate approach compete state algorithms anomaly detection order magnitude faster approaches. anomaly? anomaly element whose properties differ majority elements consideration called normal data. anomaly detection refers problem ﬁnding patterns data conform expected behavior. non-conforming patterns often referred anomalies typical applications anomaly detection network intrusion detection credit card fraud detection medical diagnosis failure detection industrial environments. example systems detect unusual network behavior used complement replace traditional intrusion detection methods based experts’ knowledge order defeat increasing number attacks computer based networks credit card transactions differ signiﬁcantly usual shopping behavior card owner indicate credit card stolen compromise data associated account occurred diagnosis radiographs supported automated systems detect breast cancers mammographic image analysis unplanned downtime production lines caused failing components serious concern many industrial environments. anomaly detection used detect unusual sensor information predict possible faults enabling condition-based maintenance novelty detection used detect interesting unusual galaxies astronomical data sloan digital survey obtaining labeled training data types anomalies often expensive. imagine labeling done human expert obtained costly experiments applications anomalies also rare trafﬁc safety space missions. hence problem anomaly detection typically unsupervised however implicitly assumed dataset contains anomalies. assumption reasonable since quite often possible collect large amounts data normal state system example usual credit card transactions network trafﬁc system attack. computational complexity memory requirements classical algorithms become limiting factor applied large-scale datasets occur nowadays. solve problem propose anomaly detection algorithm called expected similarity estimation explained later detail expose anomaly detection classiﬁer calculates score using inner product feature kernel mean distribution normal data show inner product evaluated constant time estimated linear time constant memory consumption designed solve large-scale anomaly detection problems. moreover proposed expose classiﬁer learned incrementally making applicable online streaming anomaly detection problems. learning data streams directly unavoidable many applications network trafﬁc monitoring video surveillance document feeds data arrives continuously fast streams volume large impractical store. present efﬁcient anomaly detection algorithm called expected similarity estimation training time prediction time memory requirements respect dataset size paper organised follows ﬁrst provide formal problem description including deﬁnition batch streaming anomaly detection. section provides overview related work comparison techniques. section introduces expose anomaly detection algorithm along necessary theoretical framework. subsequently show section expose applied streaming anomaly detection problems. expose computational performance subject section section empirically compare expose several state anomaly detectors. even though vast amount literature anomaly detection unique deﬁnition anomalies exactly anomaly detection section state problem anomaly detection batch streaming applications. deﬁnition input space observation measurable space containing values might take. denote realization measurement random variable make assumptions nature input space consist simple numerical vectors also contain images video data trajectories vehicles people. assume true distribution data. deﬁnition anomaly detection observation belong class normal data anomaly called label observation denoted random variable collection labels given measurable space called label space output space distribution observation stochastic depends label hence distributed according deﬁnition based outcome observation objective anomaly detection algorithm make prediction measurable space called prediction space sometimes decision space. prediction space necessarily equal label space especially anomaly detection classiﬁcation many algorithms calculate probability score label. score called anomaly score quantiﬁes likelihood belonging normal score determines degree certainty belongs scoring based algorithms ﬂexible techniques assign hard class labels since anomalies ranked prioritized according score domain speciﬁc discrimination threshold applied separate anomalies normal data. example deﬁne mapping based threshold domain speciﬁc threshold depends costs false positives false negatives deﬁnition measurable function called classiﬁer predictor. classiﬁer calculates prediction observation context anomaly detection goal good predictor distinguish normal anomalous data. however distribution typically unknown hence build classiﬁer solely based observations. estimation functional relationship input space prediction space called learning training. deﬁnition unsupervised batch learning access unlabeled independent realizations identically distributed form training set. anomaly detection make according following assumptions access small labeled fraction training data conﬁgure deﬁnition contrast batch learning full dataset permanently available online learning algorithms observe sequential order. typically algorithms limited memory thus store small previously observed samples. hence necessary continuously update prediction function based observations build predictor. generalized streaming anomaly detection data arrives possible inﬁnite sequence observations. moreover input label space distributions evolve time problem known concept drift. therefore necessary algorithm adapt changes e.g. forgetting outdated information incorporating knowledge classes algorithms assume recent observations carry relevant information older data. summary streaming anomaly detection following characteristics many approaches statistics machine learning used anomaly detection applicable high-dimensional large-scale problems vast amount information processed. review several algorithms focus computational complexity memory requirements. oldest statistical methods anomaly detection kernel density estimator time predictions slow large amounts data known problematic case increasing data dimensionality fitting parametric distributions normal gamma etc. problematic since general underlying data distribution unknown. therefore mixture gaussians often used surrogate true distribution example done smartsifter smartsifter handle multivariate data both continuous categorical observations. distance based models popular since easy implement interpret. knorr labels observation distance based outlier least fraction points dataset distance threshold point. authors proposed simple algorithms runtime cell-based version runs linear exponential dimension ramaswamy argues threshold difﬁcult determine proposes outlier score simply distance query point nearest neighbor. algorithm called knnoutlier suffers problem efﬁcient nearest neighbor search. input space dimension much larger ﬁnding nearest neighbor tree randomly distributed points takes time average. however hold high dimensions tree better exhaustive search also algorithm proposed ramaswamy used identify outliers given dataset. alternative algorithm proposed angiulli using distances k-nearest neighbors. simultaneously perform clustering anomaly detection integer programming optimization task. popular approaches data mining distance based novelty detection streams olindda extension minas represent normal data union spheres obtained clustering. representation becomes problematic data within cluster exhibits high variance since decision boundary becomes large detect novelties. algorithms designed incorporate novel classes model normal data hence barely applicable anomaly detection. stream outlier miner offers efﬁcient solution problem distance-based outlier detection windowed data streams using data structure called indexed stream buffer. continuous outlier detection aims improve efﬁciency storm reducing number range queries. nearest neighbor data description approximates local density using distances ﬁrst neighbor. algorithm simple often used baseline. also relatively slow approaching prediction. sophisticated local density based approach called local outlier factor considers point anomaly relatively points neighborhood. extended work data streams however relatively slow training time memory consumption. angle based outlier detection high-dimensional data proposed kriegel zimek able outperform however requires time prediction exact model full dataset replaced k-nearest neighbors query point one-class support vector machine kernel based method attempts hyperplane observations separated origin maximum margin. approach scale well large datasets predictions made high frequency. steinwart showed number support vectors grow linearly size dataset. exist one-class support vector machines learned incrementally hoeffding trees anytime decision trees mine high-speed data streams. hoeffding trees algorithm applicable solve unsupervised anomaly detection problem considered work since requires availability class labels. streaming half-space-trees randomly construct binary tree structure without data. selects dimension random splits half. tree counts number instances training node referred mass. score instance proportional mass leaf instance hits passing tree. obviously ensemble trees built-in constant time training linear however randomly splitting high-dimensional space yield tree sufﬁciently ﬁne-grained anomaly detection. rs-forest modiﬁcation hsta dimension splitted half random cut-point. also assumption instance scored streaming rs-forest receive true label instance always hold. isolation forest algorithm uses tree structure isolate instances anomaly score based path length instance. iforests achieve constant training time space complexity sub-sampling training ﬁxed size. characteristics relevant anomaly detection algorithms summarized table complexities given respect dataset size high-dimensional spaces. methods discussed scale large problems since either training time non-linear number samples time make single prediction increases dataset size present novel anomaly detection algorithm overcome problems. before random variable taking values measurable space primarily interested distribution normal data x|y=cn simply shorthand notation remainder work. next introduce deﬁnitions necessary following. hilbert space functions said reproducing kernel hilbert space evaluation functional continuous. function satisﬁes reproducing property called feature map. throughout work assume reproducing kernel hilbert space separable measurable. therefore assume input space separable topological space kernel continuous sufﬁcient separable mentioned introduction expose calculates score interpreted likelihood instance belonging distribution normal data uses kernel function measure similarity instances input space deﬁnition expected similarity respect distribution deﬁned reproducing kernel. intuitively query point compared points distribution show equation rewritten inner product feature kernel mean reformulation central importance enable efﬁciently compute quantities interest. given reproducing kernel kernel mean used embed probability measure rkhs manipulated efﬁciently. deﬁned follows. deﬁnition borel probability measure kernel embedding kernel mean deﬁned exists borel probability measures weaker assumption bounded. continue formulate central theorem work. theorem rkhs reproducing kernel expected similarity respect distribution expressed reformulation several desirable properties. point expose classiﬁer make prediction constant time. kernel mean learned expose needs calculate single inner product make prediction. however crucial aspects consider i.e. hilbert spaces integrals continuous linear forms general interchangeable. proof theorem thus weak integral show coincides strong integral. deﬁnition σ-ﬁnite measure space measurable. strong integrable norm lebesgue integrable anomaly detection cannot assume know distribution normal data however assume access independent realizations sampled common statistics estimate empirical distribution expose anomaly detector. empirical kernel embedding responsible linear training computational complexity expose. call expose model. important observations expose makes assumption type shape data distribution assumption wrong causing erroneous predictions. advantage statistical approaches approximate distribution directly parametric models. parallel distributed data processing scalable machine learning algorithms. formulation expose especially appealing kind operations. spmd technique achieve parallelism. ﬁrst programming paradigms line google’s mapreduce processing large data sets cluster assume partition dataset distinct collections distributed different computational nodes. obviously feature applied parallel instances also note partial sums section derived expose anomaly detection algorithm. showed expose expressed inner product kernel mean feature mapping query point need make prediction. evaluating inner product takes constant time estimating model done linear time constant memory. explain calculation detail section explore expose learned incrementally applied large-scale data streams. section show expose used online streaming anomaly detection. recap data stream often inﬁnite sequence observations instance arriving time source data example continuous sensor readings engine video stream surveillance cameras. section show online version expose fulﬁlls requirements starting last item list. proposition expose model learned incrementally model update performed time memory. figure illustration difference online learning model adaption concept drift. eight plots represent expose predictions observations indicated black dots. displays four snapshots increasing time left right data becomes available clusters. ﬁrst sampled left cluster later right. incrementally build model adding knowledge whereas bottom model evolves slowly forgets outdated observations online learning expose neither increase computational complexity memory requirements expose. also emphasize online learning yields exact model expose ofﬂine learning procedure. sometimes expected underlying distribution stream evolves time. property known concept drift example environmental monitoring deﬁnition normal temperature changes naturally seasons. also expect human behavior changes time requires redeﬁne anomalous actions are. fig. illustrate difference incremental learning proposition model adapts changes underlying distribution. following denote expose model time since equation necessarily hold concept drift adaptation implemented. work concerned detection concept drift show expose used efﬁciently common approaches concept drift adaption either utilize windowing forgetting mechanisms windowing straight forward technique uses buffer previous observations. whenever observation added window oldest discarded. efﬁciently implement windowing expose follows. proposition concept drift adaption data streams using sliding window mechanism implemented expose time memory consumption window size. proof given data stream window size whenever downside sliding window mechanism requirement keep past events memory. also sudden discard data point lead abrupt changes predictions classiﬁer sometimes desirable. another question choose correct window size. shorter sliding window allows algorithm react faster changes requires less memory though available data might representative noise much negative impact. hand wider window take long adapt concept drift. window size therefore often dynamically adjusted multiple competing windows used problems sliding window approaches avoided forgetting mechanism applied inﬂuence older data gradually vanishes. typically parameter used control tradeoff fast adaptation observations robustness noise data. realize forgetting mechanism expose replacing factor where observations integrated model. operation performed constant time summarized next proposition. proposition concept drift adaptation data streams using forgetting mecha« nism implemented expose time memory. proof direct consequence proposition general weighting ﬁxed using static window size called blind adaptation since model utilize information changes environment alternative informed adaptation could example external change detector weight samples concept drift detected. could also apply sophisticated decay rules making function introduced three different approaches learn model expose data streams. incremental learning approach evolving techniques. order make prediction observation made normalize calculated predicted score. necessary score would continuously change even exactly data would observed again. problem present batch version expose since model change anymore time make predictions. avoid problem divide total volume expose classiﬁer. emphasize calculation normalization constant change limiting behavior runtime memory derived earlier section since constant time access anyway. showed previous part expose expressed inner product kernel mean feature query point derived similar expression incremental streaming variants expose. however feature always calculated explicitly possible solution resort approximate feature maps review section. idea behind approximate feature maps figure comparison values calculated gaussian kernel approximation random images mnist dataset. individual plots show number kernel expansions affect approximation quality. color indicates density. efﬁciently create feature kernel known random kitchen sinks random kitchen sinks approximation based bochner’s theorem translation invariant kernels states kernel represented input space dimension. parameter determines number kernel expansions typically around larger result better kernel approximations monte carlo approach becomes accurate recently proposed approximation product calculated time complexity requiring storage. nyström approximation needs general less basis functions approach however approximation data dependent hence becomes erroneous underlying distribution changes able independent samples dataset. problem online learning streaming applications concept drift. therefore suggest avoid nyström feature context. random kitchen sinks nyström approximation common feature approximations. refer corresponding literature discussion approximate feature maps used well expose. recall previous sections expose uses inner product calculate score make predictions. using approximate feature possible explicitly represent feature function consequently also mean emphasize efﬁcient approximation showed here training time algorithm linear number samples evaluation predictions takes constant time. moreover need memory store model also independent input dimension section show several experiments expose compares state anomaly detection techniques prediction runtime performances. ﬁrst explain statistical test used compare investigated algorithms. operating characteristic precision-recall curves. webb warns averaging numbers debatable whether error rates different domains commensurable hence whether averaging error rates across domains meaningful demšar points also dangerous tests designed compare pair algorithms common example questionable procedure would comparing seven algorithms conducting paired t-tests many tests made certain proportion null hypotheses rejected random chance listing makes little sense. friedman test non-parametric statistical test ranks algorithms dataset individually starting best rank. purpose examine whether signiﬁcant difference performances individual algorithms. lets assume compare algorithms datasets rank j-th algorithm i-th dataset. denote average rank algorithm given called critical difference. studentised range statistic divided demšar also suggests visually represent results nemenyi test critical difference diagram fig. diagram compare algorithms datasets other. algorithms connected signiﬁcantly different performance. figure visualization post-hoc nemenyi test form critical difference diagram. position line indicates algorithms’s average rank. algorithms signiﬁcantly different connected bar. critical difference given double arrow left. experiment compare expose iforest oc-svm fastabod terms anomaly detection performance processing time learning task without concept drift. order comparable follow perform outlier selection task objective identify anomalies given dataset. performance analysis evaluation take following datasets often used literature comparison anomaly detection algorithms example several smaller benchmark datasets known anomaly classes ionosphere arrhythmia pima satellite shuttle biomed wisconsin breast cancer datasets described nominal binary attributes removed. larger datasets network intrusion data forest cover type kddcup instances follow setup obtain total attributes. furthermore high-dimensional image datasets mnist google street view house numbers scaled version mnist create features svhn. methodology suggested used create anomaly detection datasets mnist svhn following way. take images digit mnist normal instances. images remaining digits used anomalies. create dataset comprising normal instances random subset anomalies anomalies account elements set. repeat process mnist images digits digit classes svhn create anomaly detection datasets. subset anomalies independently sampled repetition experiment. table provides overview dataset properties anomaly classes deﬁned. experiment provide dedicated labeled random subset instances conﬁgure algorithms parameters. emphasize subset used evaluate predictive performance. parameter conﬁguration done pattern search using cross-validation. examples parameters optimized number nearest neighbors fastabod kernel bandwidth expose oc-svm number trees iforest. optimize different distance metrics various kernels functions common euclidean distance squared exponential kddcup forestcover mnist mnist mnist mnist mnist mnist mnist mnist mnist svhn svhn svhn svhn svhn svhn svhn svhn svhn shuttle satellite pima breastw arrhythmia ionosphere biomed kernel respectively. however remark choice functions pose possibility include domain expert knowledge system. experiment repeated times scores used perform friedman test. stated otherwise expose combination nyström’s approximation batch anomaly detection random kitchen sinks streaming experiments discussed section average scores experiment reported table whereas runtimes provided table algorithms failed larger datasets. example able process kddcup dataset high memory requirements tree data structure. however advantage tree data structure nearest neighbor lookup dimensions seen comparing runtime forestcover mnist. even though forestcover dataset times size mnist takes fraction time processed. advantage vanishes higher dimensions. fastabod exhibit good anomaly detection performance small datasets however fail soon apply medium-sized problems. values perform friedman post-hoc nemenyi tests. friedman test conﬁrms statistical signiﬁcant difference performances individual algorithms p-value critical difference diagram fig. observe expose performs signiﬁcant better iforest fastabod kde. signiﬁcant difference terms anomaly detection expose oc-svm conﬁrmed expose several orders magnitude faster large-scale high-dimensional datasets. kddcup forestcover mnist mnist mnist mnist mnist mnist mnist mnist mnist svhn svhn svhn svhn svhn svhn svhn svhn svhn shuttle satellite pima breastw arrythmia ionosphere biomed kddcup forestcover mnist mnist mnist mnist mnist mnist mnist mnist mnist svhn svhn svhn svhn svhn svhn svhn svhn svhn shuttle satellite pima breastw arrythmia ionosphere biomed experiments compare streaming variants expose hsta storm cod. algorithms blind methods adapt model regular intervals without knowing concept drift occurred not. combined concept drift detector make adaptation informed using dedicated subset data evaluate algorithm regular time intervals. holdout must reﬂect respective stream properties therefore evolve stream case concept drift. making prediction instance becomes available performance metric applied based prediction actual label instance. since predictions made stream directly special actions taken case concept drift. possible holdout method preferable since unbiased risk estimator balanced test number normal instances anomalies. disadvantage prequential method since deﬁnition data stream contains anomalies. problematic since storm assign hard class labels contrast classiﬁcation accuracy highly sensitive unbalanced data. therefore balanced accuracy deﬁned exist non-synthetic datasets anomaly detection concept drift. based multi-class datasets class represents single concept. example svhn dataset stream randomly sampled instances digits sequence instances digit appear ﬁrst instances digit digit every time steps calculate accuracy using holdout method dedicated random test contains instances normal class instances anomalies. figure sigmoid function used introduce smooth drift concept another. bernoulli distribution probability sample instance concept probability sample concept drift occurs deﬁnes duration concepts valid. similar proposed three digit data stream contains four different concepts. concept consists three digits usps handwritten digits dataset described. instances concept processed stream switches second concept concept randomly induce anomalies concept prequential method evaluation calculate balanced accuracy. datasets presented contain sudden concept drifts. bifet proposed methodology introduce smooth drift concepts. instances concepts classes consideration sampled according bernoulli distribution class probability smoothly changes class according sigmoid function concept drift occurs length drift interval. interval instances concepts belong class normal data. apply methodology usps create smooth digit drift dataset. start digit smoothly change digit using next drift digit occurs repeat digit before randomly anomalies concept prequential method evaluation. summarized dataset characteristics table sliding window length demonstrated obey appropriate trade drift adaptation model accuracy. therefore length algorithms datasets except γ-expose. change window length affects w-expose storm hsta way. unexpected window size determines number instances available algorithm. ﬁrst instances stream used conﬁgure algorithm parameters cross-validation using pattern search. detailed illustration svhn experiment shown fig. predictive performance algorithms relatively similar. observed that stream changes digit another accuracy suddenly drops indicates current model valid anymore. short period time model adapts accuracy recovers. expose performs average better storm hsta. possible interpretation result sound foundation probability theory approach. suboptimal performance hsta indicates random binary trees constructed hsta sufﬁciently ﬁne-grained high-dimensional datasets. interpretation supported experiments low-dimensional shuttle satellite data hsta performs better. average accuracies individual experiments found table statistical signiﬁcance observed γ-expose cod. could conﬁrm signiﬁcant difference algorithms illustrated critical difference diagram proposed algorithm expose perform anomaly detection large-scale datasets streams concept drift. although anomaly detection problem central importance many applications algorithms scalable vast amount data often confronted with. expose anomaly detection classiﬁer calculates score using inner product feature kernel embedding probability measures. kernel embedding technique provides efﬁcient work probability measures without necessity make assumptions underlying distributions. despite simplicity expose obeys linear computational complexity learning make predictions constant time requires constant memory. applied incrementally online model update also performed constant time. demonstrated expose used efﬁcient anomaly detection algorithm predictive performance best state methods signiﬁcant faster techniques discriminant power.", "year": 2016}