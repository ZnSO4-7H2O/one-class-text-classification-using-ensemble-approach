{"title": "Particle Swarm Optimization for Generating Interpretable Fuzzy  Reinforcement Learning Policies", "tag": ["cs.NE", "cs.AI", "cs.LG", "cs.SY"], "abstract": "Fuzzy controllers are efficient and interpretable system controllers for continuous state and action spaces. To date, such controllers have been constructed manually or trained automatically either using expert-generated problem-specific cost functions or incorporating detailed knowledge about the optimal control strategy. Both requirements for automatic training processes are not found in most real-world reinforcement learning (RL) problems. In such applications, online learning is often prohibited for safety reasons because online learning requires exploration of the problem's dynamics during policy training. We introduce a fuzzy particle swarm reinforcement learning (FPSRL) approach that can construct fuzzy RL policies solely by training parameters on world models that simulate real system dynamics. These world models are created by employing an autonomous machine learning technique that uses previously generated transition samples of a real system. To the best of our knowledge, this approach is the first to relate self-organizing fuzzy controllers to model-based batch RL. Therefore, FPSRL is intended to solve problems in domains where online learning is prohibited, system dynamics are relatively easy to model from previously generated default policy transition samples, and it is expected that a relatively easily interpretable control policy exists. The efficiency of the proposed approach with problems from such domains is demonstrated using three standard RL benchmarks, i.e., mountain car, cart-pole balancing, and cart-pole swing-up. Our experimental results demonstrate high-performing, interpretable fuzzy policies.", "text": "fuzzy controllers eﬃcient interpretable system controllers continuous state action spaces. date controllers constructed manually trained automatically either using expert-generated problem-speciﬁc cost functions incorporating detailed knowledge optimal control strategy. requirements automatic training processes found real-world reinforcement learning problems. applications online learning often prohibited safety reasons requires exploration problem’s dynamics policy training. introduce fuzzy particle swarm reinforcement learning approach construct fuzzy policies solely training parameters world models simulate real system dynamics. world models created employing autonomous machine learning technique uses previously generated transition samples real system. best knowledge approach ﬁrst relate self-organizing fuzzy controllers model-based batch fpsrl intended solve problems domains online learning prohibited system dynamics relatively easy model previously generated default policy transition samples expected relatively easily interpretable control policy exists. eﬃciency proposed approach problems domains demonstrated using three standard benchmarks i.e. mountain cart-pole balancing cart-pole swing-up. experimental results demonstrate high-performing interpretable fuzzy policies. work motivated typical industrial application scenarios. complex industrial plants like wind turbines already operated ﬁeld years. plants low-level control realized dedicated expert-designed controllers guarantee safety stability. low-level controllers constructed respect plant’s subsystem dependencies modeled expert knowledge complex mathematical abstractions ﬁrst principle models ﬁnite element methods. examples low-level controllers include self-organizing fuzzy controllers considered eﬃcient interpretable learning controllers produces interpretable high-level controllers scope paper proposed approach. especially real-world industry problems high interest since interpretable policies expected yield higher acceptance domain experts black-box solutions fundamental diﬀerence classical control theory machine learning approaches lies techniques address stability reward function design. classical control theory stability central property closed-loop controller. example lyapunov stability theory analyzes stability solution near point equilibrium. widely used design controllers nonlinear systems moreover fault detection robustness interest fuzzy systems problems addressed classical fuzzy control theory i.e. stability fault detection robustness make well suited serving low-level system controllers. controllers reward functions speciﬁcally designed purpose parameter training essential. contrast second view deﬁning reward functions typically applied high-level system control sample system’s latent underlying reward dynamic subsequently data perform machine learning. herein apply second view deﬁning reward functions capable utilizing sampled reward data controller training. note goal policy maximizes trajectory’s expected accumulated rewards referred return value without explicitly considering stability. several approaches autonomous training fuzzy controllers proven produce remarkable results wide range problems. jang introduced anfis fuzzy inference system implemented using adaptive network framework. approach frequently applied develop fuzzy controllers. example anfis successfully applied cart-pole balancing problem anfis training process training data must represent desired controller behavior makes process supervised machine learning approach. however optimal controller trajectories unknown many industry applications. system approximate nonlinear function debnath optimized gaussian membership function parameters nonlinear problems showed parameter tuning much easier conventional methods knowledge derivative complex mathematical equations required kothandaraman ponnusamy applied tune adaptive neuro fuzzy controllers vehicle suspension system. however similar anfis ﬁtness functions contributions dedicated expert formulas mean-square error functions depend correctly classiﬁed samples. best knowledge self-organizing fuzzy rules never combined model-based batch approach. proposed fuzzy particle swarm reinforcement learning approach diﬀerent fuzzy policy parameterizations evaluated testing resulting policy world model using monte carlo method combined return value number action sequences ﬁtness value maximized iteratively optimizer. batch consider applications online learning approaches classical temporal-diﬀerence learning prohibited safety reasons since approaches require exploration system dynamics. contrast batch algorithms generate policy based existing data deploy policy system training. setting either value function system dynamics trained using historic operational data comprising four-tuples form referred data batch. research past decades suggests batch algorithms satisfy real-world system requirements particularly involving neural networks modeling either state-action value function system dynamics moreover batch algorithms data-eﬃcient batch data utilized repeatedly training phase. fpsrl model-based approach i.e. training conducted environment approximation referred world model. generating world model real system data advance training fuzzy policy oﬄine using model several advantages. many real-world scenarios data describing system dynamics available advance easily collected. policies evaluated real system thereby avoiding detrimental eﬀects executing policy. expert-driven reward function engineering yielding closed-form diﬀerentiable equation utilized policy training required i.e. weighted yielding overall state-independent policy performance obtained averaging starting states using respective probabilities weight factors. thus optimal solutions problem many real-world problems cost executing potentially policy prohibitive. e.g. pilots learn ﬂying using ﬂight simulator instead real aircraft. similarly model-based real-world state transition function approximated using model ﬁrst principle model created previously gathered data. substituting place real-world state transition function obtain model-based fuzzy theory introduced zadeh based theory mamdani assilian introduced so-called fuzzy controller speciﬁed linguistic if-then rules whose membership functions activated independently produce combined output computed suitable defuzziﬁcation function. remainder paper organized follows. methods employed framework reviewed sections speciﬁcally problem ﬁnding policies formalized optimization task. addition review gaussian-shaped membership functions describe proposed parameterization approach. finally optimization heuristic searching optimal policy parameters diﬀerent extensions presented. overview proposed fpsrl approach derived diﬀerent methods given section experiments using three benchmark problems i.e. mountain problem balancing task complex swing-up challenge described section section also explain setup process world models introduce applied fuzzy policies. experimental results discussed section results demonstrate proposed fpsrl approach solve benchmark problems human-readable understandable. benchmark fpsrl compare obtained results neural ﬁtted iteration established technique. note technique chosen describe advantages limitations proposed method compared well-known widely available standard algorithm. biological learning animal interacts environment attempts action strategies maximize perceived accumulated reward. notion formalized area machine learning acting agent explicitly told actions implement. instead agent must learn best action strategy observed environment’s responses agent’s actions. common problems action aﬀects next reward subsequent rewards examples delayed eﬀects nonlinear change position force applied body mass delayed heating combustion engine. time step agent observes system’s state applies action state space action space. depending real-value reward herein focus determinexpressed functions recluding itself). neighborhood relations particles determined swarm’s population topology generally ﬁxed irrespective particles’ positions. note ring topology used experiments described section velocity vector contains cognitive component social component represent attraction given particle’s best position neighborhood’s best position respectively. velocity vector calculated follows inertia weight factor velocity position particle dimension positive acceleration constants used scale contribution cognitive social components ˆyij respectively. factors paper apply gaussian membership functions popular type membership function yields smooth outputs local never produces zero activation forms multivariate gaussian function applying product membership dimensions. deﬁne membership function rule follows algorithm population-based non-convex stochastic optimization heuristic. generally operate search space bounded sub-space ﬁnite-dimensional vector space position particle swarm represents potential solution given problem. particles iteratively multidimensional search space referred ﬁtness landscape. movement particle receives ﬁtness value position. ﬁtness value used update particle’s velocity vector velocity vectors particles certain neighborhood. next fpsrl step assumption number rules policy required. experiments started minimal rule benchmark calculated respective performances. then increased number rules compared performance policies fewer rules. process repeated performance respect dynamic models satisfactory. intuitive representation maximal achievable policy performance given certain discount factor respect particular model computed adopting trajectory optimization technique pso-p prior fpsrl training. note present results fpsrl using world models optimization technique. considered problem domain i.e. continuous smooth deterministic system dynamics known serve adequate world models. given batch previously generated transition samples training process data-eﬃcient training errors excellent indicators well model perform model-based training. nevertheless diﬀerent problem domains alternative types world models might preferable. example gaussian processes provide good approximation mean target value technique indicates level conﬁdence prediction. feature value stochastic system dynamics. second alternative modeling technique regression trees typically lacking data eﬃciency regression tree predictions less aﬀected nonlinearities perceived system dynamics rely closed-form functional approximation. population-based optimizer require gradient information ﬁtness landscape. utilizes neighborhood information systematically search valuable regions search space. note gradientdescent based methods evolutionary algorithms alternative techniques. benchmark underpowered must driven hill achieved building suﬃcient potential energy ﬁrst driving direction opposite ﬁnal direction. system fully described two-dimensional state space representing cars position velocity conducted experiments using freely available software benchmark system applies runge-kutta fourth-order method approximate closed loop dynamics. task agent sequence force actions experiments described following sections also conducted using software. objective benchmark apply forces cart moving one-dimensional track keep pole hinged cart upright position here four markov state variables pole angle pole angular velocity cart position cart velocity variables describe markov state completely i.e. additional information system’s past behavior required. task agent sequence force actions prevent pole falling figure schematic visualization proposed fpsrl approach. left right evaluates parameter vectors predeﬁned fuzzy rule representation given parameters model-based evaluation performed ﬁrst computing action vector state then approximative performance tuple computed predicting resulting state transition’s reward using nns. repeating procedure state successor states generates approximative trajectory state space. accumulating rewards using return computed state eventually used compute ﬁtness value drives swarm high performance policy parameterizations alternative techniques restricted area episode considered failure i.e. velocities become zero cart’s position pole’s angle become ﬁxed system remains failure state rest episode. policy apply force process simpliﬁed. necessary search optimal parameters half fuzzy policy rules. half parameter sets constructed negating membership functions’ mean parameters respective output values policy’s components. note membership function span width fuzzy rules negated membership functions must preserve shapes. cpsu benchmark based system dynamics benchmark. contrast benchmark position cart angle pole restricted. consequently pole swing through important property cpsu. since tion experiments created state variable. prior training respective data sets split blocks weight updates training computed utilizing training sets weights performed best given validation sets used training results. finally weights evaluated using generalization sets rate overall approximation quality unseen data. training sets benchmarks samples originate trajectories state transitions generated random walk benchmark dynamics. start states trajectories sampled uniformly conducted several experiments investigate eﬀect diﬀerent data sizes diﬀerent network complexities. results give detailed impression minimum amount data required successfully apply proposed fpsrl approach diﬀerent benchmarks adequate complexity data batch size. experiments conducted network complexities three hidden layers hidden neurons arctangent activation functions. training used vario-eta algorithm training networks executed parallel requires couple minutes. detailed overview approximation performance resulting models fpsrl rules created models comparison non-interpretable policies generated data sets given tables mean squared errors normalized output variables depicted respect generalization data sets. proposed fpsrl approach search parameterization fuzzy policy formed certain number rules. performance fpsrl policy related number rules rules generally allow sophisticated reaction system states. hand higher number rules requires parameters optimized makes optimizer’s search problem diﬃcult. addition complex rules tends diﬃcult even impossible interpret. thus determined rules suﬃcient benchmarks adequate performance cpsu benchmark achievable minimum four rules. output fpsrl policies continuous although semi-discrete output obtained increasing parameter well-documented methodology. used implementation teachingbox tool box. paper claim proposed fpsrl approach superior best algorithms terms performance; thus selected show degree diﬃculty benchmarks advantages limitations proposed method. recent developments deep produced remarkable results image-based online benchmarks future studies reveal performance batch-based oﬄine problems superior nfq. nevertheless methods attempt produce interpretable policies. visualize fuzzy policies plot respective membership functions analyze produced output sample states. graphical representation policy benchmark given fig. time consideration able understand policy’s outputs considered state. benchmark diﬀerent discontinuities dynamics make modeling process diﬃcult compared benchmark case. ﬁrst discontinuity occurs cart leaves restricted state space ends failure state i.e. soon diﬃculties benchmark discontinuity velocity dimension reaching goal rather long horizon required observe eﬀects applied actions. ﬁrst problem diﬃcult model goal area condition limited samples reaching goal using random policy. training errors lead situation models corkept area applying correct forces leads high reward transitions. problem could solved incorporating external knowledge goal area would result convenient training process. here explicitly want incorporate expert knowledge benchmarks. instead wanted demonstrate purely data-driven autonomous learning example. results given table show that despite diﬃculties well-performing policies learned using fpsrl nfq. modeling discrete changes task becomes even complicated samples transitions rare. contrast diﬃculties modeling benchmark dynamics rather simple policy balance pole without leaving restricted state space. discount factor consider policies data batch size sample transitions required build models adequate approximation quality training model-based policy. models trained sample transitions could correctly approximate eﬀects occurred entering failure-state area. further incorrectly predicted possibilities escape failure state balance pole subsequent time steps. model-based fpsrl technique exploited weaknesses produced policies perform well models demonstrated poor performance real benchmark dynamics. visual representation resulting fuzzy policies given fig. example illustrates situation potential problems policy observed visual inspection signiﬁcant advantage interpretable policies. contrast fpsrl could produce wellperforming non-interpretable policies even small data batch sizes. note weak models used fpsrl training used determine iteration produced best policy training episodes. experiments observed even models approximative quality figure fuzzy rules benchmark rules’ activations maximal nearly position implies ρ-dimension minor inﬂuence policy’s output. observation fact that benchmark simplistic high-performing policy exists i.e. accelerate direction current velocity. although trivial policy yields good performance problem better solutions exist. example stop driving left earlier certain position reach goal fewer time steps yields higher average return. depicted policy implements advantageous solution shown example section table results data number state transitions obtained random trajectories benchmark dynamics; models generalization errors best models able produce given certain amount data pre-deﬁned network complexity; policies performance real benchmark dynamics diﬀerent policy types trained/selected according performance using models left. policy setting training experiments performed obtain statistically meaningful results. presented results diﬀerent data batch sizes show benchmark dynamics rather easy model using nns. addition models signiﬁcantly greater errors generalization sets still suﬃcient training fuzzy policy using fpsrl selecting well-performing policy nfq. compared benchmarks results cpsu benchmark show completely diﬀerent picture terms performance training process. despite cpsu sharing underlying mathematical transition dynamics diﬀer following important aspects. first discontinuities state transitions occur owing absence failure state area. second planning horizon successful policy signiﬁcantly higher. latter makes particularly diﬃcult solution applying standard ﬁrst property makes cpsu good example strength proposed fpsrl approach. contrast proposed fpsrl could parameterization successful policies using four fuzzy rules assessing performance world models trained data batch sizes greater. data batch size transition samples containing goal area reward data model area correctly. however extremely high errors obtained generalization model training excellent indicators weakness. traditional create self-organizing fuzzy controllers either requires expert-designed ﬁtness function according optimizer ﬁnds optimal controller parameters relies detailed knowledge regarding optimal controller policy. either requirement diﬃcult satisfy dealing real-world industrial problems. however data gathered system controlled using default policy available many cases. fpsrl approach proposed herein data produce high-performing interpretable fuzzy policies problems. particularly problems system dynamics rather easy model adequate amount data resulting policy experimental results obtained three standard benchmarks demonstrated advantages limitations proposed model-based method compared well-known model-free approach. however results obtained problem reveal important limitation fpsrl i.e. training using weak approximation models. proposed approach exploit weaknesses potentially result poor performance evaluated using real dynamics. modeling techniques provide measure uncertainty predictions gaussian processes bayesian possibly overcome problems. recent developments modeling stochastic dynamic systems provide approximation mean next system state also compute uncertainty transitions state-action space. addition continuous state action spaces well long time horizons appear introduce obstacles training fuzzy policies. resulting policies obtained cpsu benchmark performed signiﬁcantly better generated standard nfq. however signiﬁcant advantages proposed method methods fact fuzzy rules easily conveniently visualized interpreted. suggested compact informative approach present fuzzy rule policies serve basis discussion domain experts. application proposed fpsrl approach industry settings could prove signiﬁcant interest because many cases data systems readily available interpretable fuzzy policies favored black-box solutions q-function-based modelfree approaches. project report based supported funds german federal ministry education research project number sole responsibility report’s contents lies authors. authors would like thank dragan obradovic clemens otte insightful discussions helpful suggestions.", "year": 2016}