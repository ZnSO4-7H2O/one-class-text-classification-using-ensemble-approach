{"title": "Approximate Policy Iteration Schemes: A Comparison", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "We consider the infinite-horizon discounted optimal control problem formalized by Markov Decision Processes. We focus on several approximate variations of the Policy Iteration algorithm: Approximate Policy Iteration, Conservative Policy Iteration (CPI), a natural adaptation of the Policy Search by Dynamic Programming algorithm to the infinite-horizon case (PSDP$_\\infty$), and the recently proposed Non-Stationary Policy iteration (NSPI(m)). For all algorithms, we describe performance bounds, and make a comparison by paying a particular attention to the concentrability constants involved, the number of iterations and the memory required. Our analysis highlights the following points: 1) The performance guarantee of CPI can be arbitrarily better than that of API/API($\\alpha$), but this comes at the cost of a relative---exponential in $\\frac{1}{\\epsilon}$---increase of the number of iterations. 2) PSDP$_\\infty$ enjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a number of iterations similar to that of API. 3) Contrary to API that requires a constant memory, the memory needed by CPI and PSDP$_\\infty$ is proportional to their number of iterations, which may be problematic when the discount factor $\\gamma$ is close to 1 or the approximation error $\\epsilon$ is close to $0$; we show that the NSPI(m) algorithm allows to make an overall trade-off between memory and performance. Simulations with these schemes confirm our analysis.", "text": "consider inﬁnite-horizon discounted optimal control problem formalized markov decision processes. focus several approximate variations policy iteration algorithm approximate policy iteration conservative policy iteration natural adaptation policy search dynamic programming algorithm inﬁnite-horizon case recently proposed non-stationary policy iteration algorithms describe performance bounds respect per-iteration error make comparison paying particular attention concentrability constants involved number iterations memory required. analysis highlights following points performance guarantee arbitrarily better comes cost relative—exponential —increase number iterations. psdp∞ enjoys best worlds performance guarantee similar within number iterations similar api. contrary requires constant memory memory needed psdp∞ proportional number iterations problematic discount factor close approximation error close show nspi algorithm allows make overall trade-off memory performance. simulations schemes conﬁrm analysis. consider inﬁnite-horizon discounted markov decision process possibly inﬁnite state space ﬁnite action space probability kernel reward function bounded rmax discount factor. stationary deterministic policy maps states actions. write pπ|s) stochastic kernel associated policy value policy function mapping states expected discounted rewards received following states value clearly bounded vmax rmax/. well-known characterized unique ﬁxed point linear bellman operator associated policy γpπv. similarly bellman optimality operator maxπ unique ﬁxed point optimal value maxπ policy greedy w.r.t. value function greedy policies written finally policy optimal value equivalently goal paper study compare several approximate policy iteration schemes. literature schemes seen implementing approximate greedy operator takes input distribution function returns policy approximately greedy respect sense that denotes es∼ν]. practice approximation greedy operator achieved p-regression so-called q-function—the stateaction value function— ﬁxed-point lstd approach used lagoudakis parr classiﬁcation problem operator hand shall describe several policy iteration schemes section section provide detailed comparative analysis performance guarantees time complexities memory requirements. section providing experiments illustrate behavior conﬁrm analysis. finally section conclude present future work. algorithms begin describing standard approximate policy iteration iteration algorithm switches policy approximately greedy respect value previous policy distribution g\u0001k+. error assigns positive weights every state easily seen algorithm generates sequence policies exact policy iterations since equation policies exactly greedy. cpi/cpi/api turn description conservative policy iteration proposed iteration uses distribution dπkν ν−—the discounted cumulative occupancy measure induced starting ν—for calling approximate greedy operator uses stepsize generate stochastic mixture policies returned successive calls approximate greedy operator explains adjective conservative stepsize chosen step leads improvement expected value policy given process initialized according distribution original article also describes criterion deciding whether stop continue. though adaptive stepsize stopping condition allows derive nice analysis practice conservative stepsize implemented line-search mechanism ﬁxed small value refer latter variation cpi. psdp∞ going describe algorithm ﬂavour similar api—in sense step full step towards deterministic policy— also conservative ﬂavour like cpi—in sense policies considered evolve slowly. algorithm natural variation policy search dynamic programming algorithm bagnell originally proposed tackle ﬁnite-horizon problems inﬁnite-horizon case; thus refer psdp∞. best knowledge however variation never used inﬁnite-horizon context. algorithm based ﬁnite-horizon non-stationary policies. given sequence stationary deterministic policies algorithm generate write πkπk− k-horizon policy makes ﬁrst action according second action according etc. value tπk− write empty non-stationary policy. note inﬁnite-horizon policy begins πkπk− denote value vσk... γkvmax. starting algorithm implicitely builds sequence non-stationary policies iteratively concatenating policies returned approximate greedy operator standard psdp algorithm bagnell considers horizon makes iterations algorithm consider indeﬁnite number iterations. algorithm stopped step theory describe suggests return policy starts non-stationary policy since approximately good ﬁnite-horizon policy consider inﬁnite-horizon problem natural output want practice inﬁnitehorizon policy loops shall denote practice controlling greedy step respect dπkν requires generate samples distribution. explained kakade langford sample done running trajectory starting following stopping step probability particular sample dπkν requires average samples underlying mdp. respect much simpler implement. forget oldest policy policy keep constant memory size πk−m+ step algorithm stopped output policy reduces furthermore assume reward function positive stop actions every state model lead terminal absorbing state null reward initialize inﬁnite sequence policies take stop action nspi reduces psdp∞. considered algorithms going describe bounds expected loss es∼µ using policy ouput algorithms instead optimal policy initial distribution interest function upper bound errors order derive theoretical guarantees ﬁrst need introduce concentrability coefﬁcients relate distribution wants guarantee distribution used algorithms. deﬁnition smallest coefﬁcients ∪{∞} sets deterministic stationary policies µpπpπ deﬁne following coefﬁcients finally smallest coefﬁcient dπ∗µ notations hand ﬁrst contribution provide thorough comparison algorithms. done table algorithm describe performance bounds required number iterations memory. make things clear display dependence respect concentrability constants expected loss corresponds weighted -norm loss relaxing goal controlling weighted p-norm allows introduce ﬁner coefﬁcients lack space consider here. practical point view psdp∞ need store policies generated start. memory required algorithmic scheme thus proportional number iterations prohibitive. next paragraph presents last algorithm article describe solution potential memory issue. nspi originally devised algorithmic scheme equation simpliﬁed variation non-stationary algorithm growing period algorithm respect equation difference nspigrowing resides fact approximate greedy step done respect value policy loops inﬁnitely instead value ﬁrst steps here. following intuition values close other ended considering psdp∞ simpler. nspigrowing suffers memory drawback psdp∞. interestingly work scherrer lesner contains another algorithm non-stationary ﬁxed period parameter directly controls number policies stored memory. similarly psdp∞ nspi based non-stationary policies. requires initial deterministic stationary policies iteratively generates shall depolicies m-horizon non-stationary policy runs note reverse order last policies πk−m+. also write formally m-periodic inﬁnite-horizon nonshall denote could also done straight-forward extension lstd non-stationary policies. table upper bounds performance guarantees algorithms. except references given bounds knowledge new. comparison based known bounds done ghavamzadeh lazaric ﬁrst bound nspi seen adaptation provided scherrer lesner restrictive ∞-norm setting. discount factor quality approximate greedy operator and—if applicable—the main parameters algorithms. psdp∞ required memory matches number iterations. bounds knowledge original. derivation results given appendix second contribution complementary comparative list bounds show exists hierarchy among constants appear bounds table directed graph figure constant descendent implication holds. important here means parent parent exists ﬁnite inﬁnite; words algorithm guarantee respect guarantee arbitrarily better constant thus overall best concentrability constant worst make picture complete distribution possible input distribution algorithm ﬁnite though case derivation order relations done appendix standard algorithm guarantees expressed terms only. since cpi’s analysis done respect performance guarantee arbitrarily better though opposite true. this however comes cost exponential increase time complexity since renow practical point view psdp∞ need store policies generated start. memory required algorithms thus proportional number iterations. even psdp∞ require much fewer iterations corresponding memory requirement still prohibitive situations small close explained nspi seen making bridge psdp∞. since nice time complexity best memory requirement nspi best performance guarantee nspi good candidate making standard performance/memory trade-off. ﬁrst bounds nspi table extends made terms left terms identical obtained psdp∞ possible right terms controlled thus made arbitrarily small increasing memory parameter analysis thus conﬁrms intuition nspi allows make performance/memory trade-off psdp∞ words soon memory becomes constraint nspi natural alternative psdp∞. section present experiments order illustrate empirical behavior different algorithms discussed paper. considered standard baseline. described kakade langford slow evaluate further. instead considered variations cpi+ identical except chooses step iteration line-search towards policy output greedy operator makes relatively small steps iteration. assess utility distribution approximate greedy step also considered variation described equation makes small steps differs fact approximate greedy step uses distribution instead dπkν. addition algorithms considered psdp∞ nspi values order assess quality consider ﬁnite problems exact value function computed. precisely consider garnet problems ﬁrst introduced archibald class randomly constructed ﬁnite mdps. correspond speciﬁc application remain representative kind might encountered practice. brief consider garnet problems branching factors greedy step used algorithms approximated exact greedy operator applied noisy orthogonal projection linear space dimension respect quadratic norm weighted uniform. parameter instances generated i.i.d. garnet mdps ≤i≤. cpi+ nspi psdp∞ times. algorithm compute iterations performance i.e. loss respect optimal policy. figure displays statistics random variables. algorithm display learning curve conﬁdence regions account variability across runs problems. supplementary material contains statistics respectively conditioned values gives insight inﬂuence parameters. experiments statistics make series observations. standard scheme much variable algorithms tends provide worst performance average. cpi+ display asymptotic performance average. slightly less variability much slower cpi+ always converges iterations api— naive conservative variation also simpler cpi—is empirically close being average slightly worse. cpi+ psdp∞ similar average performance variability psdp∞ signiﬁcantly smaller. psdp∞ algorithm overall gives best results. nspi indeed provide bridge psdp∞. increasing behavior gets closer psdp∞. nspi overall better cpi+ close psdp∞. relative observations stable respect number states actions interestingly differences algorithms tend vanish dynamics problem gets stochastic complies analysis based concentrability coefﬁcients ﬁnite dynamics mixes relative difference biggest deterministic instances. figure statistics instances. mdps i.i.d. distribution conditioned algorithm error measures iteration i.i.d. distribution central line learning curves gives empirical estimate overall average error three grey regions estimates respectively variability average error average standard deviation error variability standard deviation error ])k. ease comparison curves displayed range. considered several variations policy iteration schemes inﬁnite-horizon problems nspi psdp∞. particular explained fact—to knowledge unknown— recently introduced nspi algorithm generalizes psdp∞ figure synthesized theoretical guarantees algorithms. bounds knowledge new. ﬁrst important message work usually hidden constants performance bounds matter. constants involved bounds psdp∞ main terms nspi sorted worst best follows cπ∗. detailed hierarchy constants depicted figure knowledge ﬁrst time in-depth comparison bounds done hierarchy constants interesting implications beyond policy iteration schemes focusing paper. matter fact several dynamic programming algorithms namely ampi come guarantees involvpurely technical level several bounds come pair; fact introduced proof technique. bound improves state sense involves constant instead also enabled derive bounds worse terms guarantee better time complexity believe technique helpful future analysis algorithms. main insights analysis. guarantee arbitrarily stronger api/api expressed respect best concentrability constant comes cost —increase number itrelative—exponential erations. psdp∞ enjoys best worlds performance guarantee similar within number iterations similar api. contrary requires constant memory memory needed psdp∞ proportional number iterations problematic particular discount factor close approximation error close showed nspi algorithm allows make overall trade-off memory perforapi/nspi identical nspi bounds particular cases ﬁrst bounds nspi consider nspi. following proof technique scherrer lesner writing maxπ tπvπkm tπk+vπkm show that main assumption work algorithms disposal \u0001-approximate greedy operator. unreasonable compare algorithms basis since underlying optimization problems different complexities instance methods like look space stochastic policies moves space deterministic policies. digging understanding depth potentially hidden term \u0001—as done concentrability constants—constitutes natural research direction. last least numerical experiments support worst-case analysis. simulations garnet mdps various characteristics cpi+ psdp∞ nspi shown always perform signiﬁcantly better standard api. cpi+ psdp∞ performed similarly average psdp∞ showed much less variability thus best algorithm terms overall performance. finally nspi allows make bridge psdp∞ reaching overall performance close psdp∞ controlled memory. implementing instances algorithmic schemes running analyzing experiments bigger domains constitutes interesting future work. analysis kakade langford show learning steps ensure nice performance guarantee satisfy corollary shows performance bound coefﬁcient number iterations bound involves factor precise examination proof shows ampliﬁcation fact approximate greedy operator uses distribution dπkν instead fact using similar proof easy show satisﬁes following result. details numerical simulations domain approximations parameterized parameters written number states number actions branching factor specifying many possible next states possible state-action pair number features reward state-dependent given randomly generated garnet problem reward state uniformly sampled features chosen randomly ns×p feature matrix component randomly uniformly sampled discount factor experiments. algorithms discussed paper need repeatedly compute distribution dπν. words must able make calls approximate greedy operator applied value policy distribution implement operator compute noisy estimate value uniform white noise amplitude projects estimate onto space spanned respect ρ-quadratic norm applies greedy operator projected estimate. nutshell call approximate greedy operator amounts compute gπφρ). simulations series experiments callibrated perturbations algorithm signiﬁcantly perturbed much trial error ended considering following setting. used garnet problems number states number actions branching factor number features approximate value addition figure shows statistics overall parameter instances figure display statistics respectively conditioned values gives insight inﬂuence parameters.", "year": 2014}