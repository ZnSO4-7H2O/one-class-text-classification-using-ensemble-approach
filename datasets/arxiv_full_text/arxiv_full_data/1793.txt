{"title": "Learning a Natural Language Interface with Neural Programmer", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.7% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser.", "text": "learning natural language interface database tables challenging task involves deep language understanding multi-step reasoning. task often approached mapping natural language queries logical forms programs provide desired response executed database. knowledge paper presents ﬁrst weakly supervised end-to-end neural network model induce programs real-world dataset. enhance objective function neural programmer neural network built-in discrete operations apply wikitablequestions natural language question-answering dataset. model trained end-to-end weak supervision question-answer pairs require domain-speciﬁc grammars rules annotations elements previous approaches program induction. main experimental result paper single neural programmer model achieves accuracy using examples weak supervision. ensemble models trivial combination technique achieves accuracy competitive current state-of-the-art accuracy obtained traditional natural language semantic parser. databases pervasive store access knowledge. however straightforward users interact databases since often requires programming skills knowledge database schemas. overcoming difﬁculty allowing users communicate databases natural language active research area. common approach task semantic parsing process mapping natural language symbolic representations meaning. context semantic parsing yields logical forms programs provide desired response executed databases semantic parsing challenging problem involves deep language understanding reasoning discrete operations counting selection ﬁrst learning methods semantic parsing require expensive annotation question-program pairs annotation process longer necessary current state-of-the-art semantic parsers trained using question-answer pairs however performance methods still heavily depends domain-speciﬁc grammar pruning strategies ease program search. example recent work building semantic parsers various domains authors hand-engineer separate grammar domain figure neural programmer neural network augmented discrete operations. model runs ﬁxed number time steps selecting operation column table every time step. induced program transfers information across timesteps using selector variable output model stored scalar answer lookup answer variables. notorious difﬁculty handling discrete operations neural networks approaches rely complete programs supervision others tried synthetic tasks. work similar andreas dynamic neural module network. however method neural network employed search small candidate layouts provided syntactic parse question trained using reinforce algorithm hence method cannot recover parser errors trivial adapt parser task hand. additionally modules operations parametrized neural network difﬁcult apply method tasks require discrete arithmetic operations. finally experiments concern simpler dataset requires fewer operations therefore smaller search space wikitablequestions consider work. discuss related work section neural programmer neural network augmented discrete operations. produces program made operations result running program given table. operations make three variables selector scalar answer lookup answer updated every timestep. lookup answer scalar answer store answers selector used propagate information across time steps. input model receives question along table model runs ﬁxed number time steps selecting operation column table argument operation time step. training soft selection performed model trained end-to-end using backpropagation. approach allows neural programmer explore search space better sample complexity hard selection reinforce algorithm would provide. parameters model learned weak supervision signal consists ﬁnal answer; underlying program consists sequence operations selected columns latent. work develop approach semantic parsing based neural programmer. show learn natural language interface answering questions using database tables thus integrating differentiable operations typical neural networks declarative knowledge contained tables discrete operations tables entries. purpose make several improvements adjustments neural programmer particular adapting objective function make broadly applicable. earlier work neural programmer applied synthetic dataset. dataset expected answer entry given table position explicitly marked table. however real-world datasets certainly include markers lead many ambiguities particular answer number occurs literally table known priori whether answer generated operation selected table. similarly answer natural language phrase occurs multiple positions table known entry table actually responsible answer. extend neural programmer handle weaker supervision signal backpropagating decisions concern answer generated ambiguity. main experimental results concern wikitablequestions real-world question-answering dataset database tables examples weak supervision. dataset particularly challenging small size lack strong supervision also tables provided test time never seen training learning requires adaptation test time unseen column names. state-of-the-art traditional semantic parser relies pruning strategies ease program search achieves accuracy. standard neural network models like sequence-to-sequence pointer networks appear promising dataset conﬁrmed experiments below yield single-digit accuracies. comparison single neural programmer model using minimal text pre-processing trained end-to-end achieves accuracy. surprising result enabled primarily sample efﬁciency neural programmer enhanced objective function reducing overﬁtting strong regularization dropout weight decay. ensemble models even trivial combination technique achieves accuracy. section describe greater detail neural programmer model modiﬁcations made model. neural programmer neural network augmented discrete operations. model consists four modules selector module induces probability distributions every time step operations another columns. input selector obtained concatenating last hidden state question hidden state history current timestep attention vector obtained performing soft attention question using history vector. following neelakantan employ hard selection test time. history modeled simple tanh activations remembers previous operations columns selected model. input history timestep result concatenating weighted representations operations columns corresponding probability distributions produced selector previous timestep. history rnns learned backpropagation using weak supervision signal consists ﬁnal answer. below discuss several modiﬁcations model make broadly applicable easier train. operations model chosen closely match operations used baseline model operations except select frequent entry operate selected rows given selector variable. ﬁrst timestep rows table selected. built-in operations count returns number selected rows selector. select frequent entry operations computed every question output boolean tensor size size input table. entry output select operation entry matches phrase question. matched phrases question anonymized prevent overﬁtting. similarly frequent entry entry frequently occurring column. ﬁrst last previous next modify selector. print operation assigns selector selected column lookup answer. reset resets selector initial value. operation also serves no-op neural programmer makes three variables selector scalar answer lookup answer updated every timestep. variable lookup answer stores answers selected table scalar answer stores numeric answers provided table. induced program transfers information across timesteps using selector variable contains rows selected model. selectt− probabilities assigned selector operation column timestep respectively outputt output count operation timestep selector variable timestep obtained taking weighted average outputs remaining operations discussed appendix. lookup answert probability element input table ﬁnal answer predicted model. modify training objective neural programmer handle supervision signal available real-world settings. previous work position answers explicitly marked table answer entry table. however discussed section real-world datasets answer simply written introducing kinds ambiguities. first answer number number table known whether loss computed using scalar answer variable lookup answer variable. second answer natural language phrase phrase occurs multiple positions table know entry table actually responsible generating answer. extend neural programmer handle weaker supervision signal training computing loss prediction closest desired response. ground truth answer. divide lscalar number rows input table backpropagate examples loss greater threshold since leads instabilities training. answer list items element list compute entries table match element given ai}. tackle ambiguity introduced answer item occurs multiple entries table computing loss entry assigned highest probability model. construct }m×c indicates whether element input table part output. compute log-loss entry ﬁnal loss given deal ambiguity occurs ground truth number number also occurs table computing ﬁnal loss soft minimum lscalar llookup. otherwise loss example lscalar ground truth number llookup ground truth matches entries table. loss functions lscalar llookup different scales multiply llookup constant factor small exploration experiments. since employ hard selection test time among scalar answer lookup answer modiﬁed last timestep. variable last timestep ﬁnal output model. apply neural programmer wikitablequestions dataset compare different non-neural baselines including natural language semantic parser developed pasupat liang further also report results training sequence-tosequence model modiﬁed version pointer networks model implemented tensorflow model takes approximately train single tesla gpu. double-precision format store model parameters since gradients become undeﬁned values single-precision format. code available https//github.com/tensorflow/models/tree/master/neural_ programmer. train development test split given pasupat liang dataset contains examples training development testing respectively. tokenization number date pre-processing. examples answers neither table performance neural programmer compared baselines performance ensemble models competitive current state-of-the-art natural language semantic parser. number answers phrases selected table. ignore questions training model penalized evaluation following pasupat liang tables provided test unseen training hence requiring model adapt unseen column names test time. train examples provided table less rows since memory otherwise consider examples test time. timesteps experiments. words operations represented dimensional vectors hidden vectors question history also dimensional. parameters initialized uniformly randomly within range train model using adam optimizer mini-batches size hyperparameter adam others default values. since training small compared datasets neural network models usually applied rely strong regularization tune dropout rates regularization strength hyperparameter using grid search development data hyperparameters small exploration initial experiments. table shows performance model comparison baselines pasupat liang best result neural programmer achieved ensemble models. difference among models parameters model initialized different random seed. combine models averaging predicted softmax distributions models every timestep. generally believed neural network models require large number training examples compared simpler linear models good performance model table model ablation studies. dropout weight decay along boolean feature indicating matched table entry column selection signiﬁcant effect performance model. better results either using pre-trained word vectors pre-training question language modeling objective possible explanation word vectors obtained unsupervised learning suitable task consideration. example learned representations words like maximum minimum unsupervised learning usually close task counterproductive. consider replacing soft selection hard selection training model reinforce algorithm model fails learn experiment probably because model search millions symbolic programs every input question making highly unlikely program gives reward. hence parameters model updated frequently enough. understand difﬁculty task neural network models experiment neural network baselines sequence-to-sequence model modiﬁed version pointer networks input sequence-to-sequence model concatenation table question decoder produces output token time. consider examples whose input length less make running time reasonable. resulting dataset training development examples respectively. accuracy best model development hyperparameter tuning next experiment pointer networks select entries table ﬁnal answer. modify pointer networks two-attention heads select column select entries within column. additionally model performs multiple pondering steps table returning ﬁnal answer. train model lookup questions since model decoder generate answers. consider examples whose tables less rows resulting training development consisting examples respectively. accuracy best model development hyperparameter tuning results conﬁrm intuition discrete operations hard learn neural networks particularly small datasets real-world settings. table shows impact different model design choices ﬁnal performance. anonymizing phrases question match table entry seems small positive effect regularization much larger effect performance. column selection performed neelakantan using name column; however selection procedure insufﬁcient real-world settings. example column selected question table corresponding phrase question. hence select column additionally boolean feature indicates whether entry column matches phrase question. table shows addition boolean feature signiﬁcant effect performance. column operation column operation column operation column operation column operation column operation column operation column operation column table examples programs induced neural programmer generate correct answer development set. abbreviation operation frequent entry. model runs timesteps selecting operation column every step. model employs hard selection evaluation. column name displayed table operation picked step takes column input operation displayed reset operation. programs choose count ﬁnal operation produce number ﬁnal answer programs select print ﬁnal operation produce entries selected table ﬁnal answer. table statistics different sequence operations among examples answered correctly model development set. sequence operations table also point corresponding example programs table superlative operations include argmax argmin comparison operations include greater than less than greater equal less equal model induces program results scalar answer time induced program table lookup remaining questions. print select common operations used time respectively. table shows examples programs induced neural programmer yield correct answer development set. programs given table show characteristics learned model. first analysis indicates model adapt unseen column names test time. example question word outcome occurs times training replaced unknown word token. second model always induce efﬁcient program solve task. last questions table solved using simpler programs. finally model always induce correct program ground truth answer. example last programs result correct response input database tables. programs would produce correct response select operation matches entry table. table shows contribution different operations. model induces program results scalar answer time induced program table lookup remaining questions. commonly used operations model print select. conclude section suggest ideas potentially improve performance model. first oracle performance neural programmer models development averaging achieves implying still room improvement. next accuracy single model training higher accuracy development test set. difference performance indicates model suffers signiﬁcant overﬁtting even employing strong regularization. also suggests performance model could greatly improved obtaining training data. nevertheless limits performance improvements reasonably expect particular shown previous work questions random examples considered dataset answerable various issues annotation errors tables requiring advanced normalization. discuss detail various semantic parsing neural program induction techniques section brieﬂy describe relevant work. recently kocisky develop semi-supervised semantic parsing method uses question-program pairs supervision. concurrently work liang propose neural symbolic machine model similar neural programmer trained using reinforce algorithm discrete operations total timesteps hence inducing programs much simpler ours. neural networks also applied question-answering datasets require much arithmetic reasoning wang jiang neural network model state-of-the-art results reading comprehension task paper enhance neural programmer work weaker supervision signals make broadly applicable. soft selection training enables model actively explore space programs backpropagation superior sample complexity. experiments show model achieves performance comparable state-of-the-art traditional semantic parser even though training contains examples. knowledge ﬁrst instance weakly supervised end-to-end neural network model induces programs real-world dataset. acknowledgements grateful panupong pasupat answering numerous questions dataset providing pre-processed version dataset output semantic parser. thank david belanger samy bengio greg corrado andrew jeff dean nando freitas shixiang navdeep jaitly rafal jozefowicz ashish vaswani luke vilnis yuan barret zoph suggestions google brain team support. arvind neelakantan supported google fellowship machine learning. references mart´ın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro gregory corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing rafal j´ozefowicz lukasz kaiser manjunath kudlur josh levenberg man´e rajat monga sherry moore derek gormurray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda vi´egas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng. tensorﬂow large-scale machine learning heterogeneous distributed systems. arxiv mohit iyyer jordan boyd-graber leonardo batista claudino richard socher daum´e iii. neural network factoid question answering paragraphs. emnlp tomas kocisky gabor melis edward grefenstette chris dyer wang ling phil blunsom karl moritz hermann. semantic parsing semi-supervised sequential autoencoders. arxiv ankit kumar ozan irsoy jonathan james bradbury robert english brian pierce peter ondruska ishaan gulrajani richard socher. anything dynamic memory networks natural language processing. icml table list operations provided model along deﬁnitions. abbreviation operation frequent entry. cond true otherwise. comparison select reset operations independent timestep operations computed every time step. superlative operations frequent entry computed within column. operations calculate expected output respect membership probabilities given selector work probabilistic selection. discussed section output variables scalar answer lookup answer calculated using output count operations print operation respectively. selector computed using output remaining operations given", "year": 2016}