{"title": "On the Expressive Power of Overlapping Architectures of Deep Learning", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Expressive efficiency refers to the relation between two architectures A and B, whereby any function realized by B could be replicated by A, but there exists functions realized by A, which cannot be replicated by B unless its size grows significantly larger. For example, it is known that deep networks are exponentially efficient with respect to shallow networks, in the sense that a shallow network must grow exponentially large in order to approximate the functions represented by a deep network of polynomial size. In this work, we extend the study of expressive efficiency to the attribute of network connectivity and in particular to the effect of \"overlaps\" in the convolutional process, i.e., when the stride of the convolution is smaller than its filter size (receptive field). To theoretically analyze this aspect of network's design, we focus on a well-established surrogate for ConvNets called Convolutional Arithmetic Circuits (ConvACs), and then demonstrate empirically that our results hold for standard ConvNets as well. Specifically, our analysis shows that having overlapping local receptive fields, and more broadly denser connectivity, results in an exponential increase in the expressive capacity of neural networks. Moreover, while denser connectivity can increase the expressive capacity, we show that the most common types of modern architectures already exhibit exponential increase in expressivity, without relying on fully-connected layers.", "text": "expressive efﬁciency refers relation architectures whereby function realized could replicated exists functions realized cannot replicated unless size grows signiﬁcantly larger. example known deep networks exponentially efﬁcient respect shallow networks sense shallow network must grow exponentially large order approximate functions represented deep network polynomial size. work extend study expressive efﬁciency attribute network connectivity particular effect overlaps convolutional process i.e. stride convolution smaller ﬁlter size theoretically analyze aspect network’s design focus well-established surrogate convnets called convolutional arithmetic circuits demonstrate empirically results hold standard convnets well. speciﬁcally analysis shows overlapping local receptive ﬁelds broadly denser connectivity results exponential increase expressive capacity neural networks. moreover denser connectivity increase expressive capacity show common types modern architectures already exhibit exponential increase expressivity without relying fully-connected layers. fundamental attributes deep networks reason driving empirical success depth efﬁciency result states deeper models exponentially expressive shallower models similar size. formal studies depth efﬁciency include early work boolean thresholded circuits recent studies covering types networks used practice makes depth efﬁciency attribute desirable brings exponential increase expressive power merely polynomial change model i.e. addition layers. nevertheless depth merely among many architectural attributes deﬁne modern networks. deep networks used practice consist architectural features deﬁned various schemes connectivity convolution ﬁlter deﬁned size stride pooling geometry activation functions. whether relate expressive efﬁciency depth proven remains open question. order study effect network design expressive efﬁciency ﬁrst deﬁne efﬁciency broader terms. given network architectures architecture expressively efﬁcient respect architecture following conditions hold function realized size realized size exist function realized size cannot realized unless super-linear function exact deﬁnition sizes depends measurement care about e.g. number parameters number neurons. nature function condition determines type efﬁciency taking place exponential architecture said exponentially efﬁcient respect architecture polynomial expressive efﬁciency. additionally completely efﬁcient respect condition holds speciﬁc functions functions negligible set. paper study efﬁciency associated architectural attribute convolutions namely size convolutional ﬁlters importantly proportion stride. network architecture non-overlapping type size local receptive ﬁeld layer equal stride. case sets pixels participating computation neurons layer completely separated. stride smaller receptive ﬁeld network architecture overlapping type. latter case overlapping degree determined total receptive ﬁeld stride projected back input layer implication overlapping architecture total receptive ﬁeld stride grow much faster non-overlapping case. several studies shown non-overlapping convolutional networks theoretical merits. namely non-overlapping networks universal i.e. approximate function given sufﬁcient resources terms optimization conditions actually possess better convergence guaranties overlapping networks. despite above instances strictly non-overlapping networks used practice oord raises question non-overlapping architectures uncommon? additionally examining kinds architectures typically used recent years employ mixture overlapping nonoverlapping layers trend using ever smaller receptive ﬁelds well non-overlapping layers ever increasing role hence common networks used practice though strictly non-overlapping increasingly approaching non-overlapping regime raises question slightly overlapping architectures seems sufﬁcient tasks? following sections shed light questions analyzing role overlaps surrogate class convolutional networks called convolutional arithmetic circuits instead non-linear activations average/max pooling layers employ linear activations product pooling. convacs theoretical framework study convnets focused several works showing amongst things many results proven class typically transferable standard convnets well though prior works convacs considered non-overlapping architectures suggest natural extension overlapping case call overlapping convacs. analysis builds known relation convacs tensor decompositions prove overlapping architectures fact completely exponentially efﬁcient non-overlapping ones expressive capacity directly related overlapping degree. moreover prove even limited amount overlapping sufﬁcient attaining exponential separation. ground theoretical results demonstrate ﬁndings experiments standard convnets cifar image classiﬁcation dataset. section introduce class convolutional networks referred overlapping convolutional arithmetic circuits overlapping convacs short. class shares architectural features standard convnets including previously overlooked similar attempts model convnets convacs namely number layers unrestricted receptive ﬁelds strides crucial studying overlapping architectures. simplicity describe model case inputs spatial dimensions e.g. color images limiting convolutional ﬁlters shape square. begin presenting broad deﬁnition generalized convolutional layer fusion linear operation pooling function view convolutional layers motivated all-convolutional architecture replaces pooling layers convolutions stride width height equal depth also referred channels e.g. height equal channels referred stride role sub-sampling operation. spatial location output layer corresponds window slice input tensor size extended input channels whose top-left corner located exactly referred local receptive ﬁeld ﬁlter size. simplicity parts window slices extending beyond boundaries zero value. vector representing channels location output similarly vectors representing slice vector represents channels respective location inside window operation layer deﬁned follows rd×d referred weights biases layer respectively point-wise pooling function. illustration operation layer performs. deﬁnitions network simply sequence layers l’th layer speciﬁed local receptive ﬁeld stride output channels parameters pooling function classiﬁcation tasks output last layer network typically spatial dimensions i.e. vector output channel represents score function y’th class denoted inference perform maxy oftentimes common consider output ﬁrst layer network low-level feature representation input motivated observation learned features typically shared across different tasks datasets domain hence treat layer separate ﬁxed zeroth convolutional layer referred representation layer operation layer depicted applying ﬁxed functions window slices denoted i.e. entries output tensor layer given {fd}d∈i∈. notations output network viewed function entire network illustrated given non-linear point-wise activation function setting pooling functions i.e. circuit containing product operations hence referred convolutional arithmetic circuit convac. important emphasize convacs originally introduced cohen typically described different manner language tensor decompositions since vanilla convacs seen alternating sequence convolutions non-overlapping product pooling layers formulations coincide layers non-overlapping i.e. however layers overlapping i.e. exists formulation layers diverges give rise call overlapping convacs. given model extension convacs framework inherits many desirable attributes. first shares traits modern convnets i.e. locality sharing pooling. second shown form universal hypotheses space third underlying operations lend mathematical analysis based measure theory tensor analysis forth concept generalized tensor decompositions many theoretical results proven convacs could transferred standard convnets relu activations. finally empirical perspective tend work well many practical settings e.g. optimal classiﬁcation missing data compressed networks established non-overlapping network product pooling function equivalent vanilla convacs might wonder using overlapping layers instead could diminish overlapping networks represent. show case prove general claim network given architecture realize exactly functions networks using smaller local receptive ﬁelds includes non-overlapping case. proposition networks product pooling function. architecture derived removal layers stride decreasing local receptive ﬁeld layers choice parameters exists matching parameters function realized exactly equivalent speciﬁcally realize non-overlapping network order strides proof sketch. follows simple claims layer produce output equivalent layer smaller local receptive ﬁeld zeroing weights beyond smaller local receptive ﬁeld; layers receptive ﬁelds output equal input i.e. realize identity function. claims local receptive ﬁelds effectively shrank match local receptive ﬁelds additional layers stride could realizing identity mapping effectively removing app. complete proof. proposition essentially means overlapping architectures expressive nonoverlapping ones similar structure i.e. order non-unit strides. recall satisﬁes ﬁrst condition efﬁciency property introduced sec. regardless measure size network number parameters number neurons. following section cover preliminaries required show overlapping networks actually lead increase expressive capacity settings results exponential gain proving second condition expressive efﬁciency holds well. section describe methods analyzing expressive efﬁciency overlapping convacs foundation stating theorems. minimal background tensor analysis required follow work found sec. followed presenting methods sec. analysis. tensor rm⊗···⊗mn order dimension mode multi-dimensional array entries ad...dn simplicity henceforth assume dimensions equal i.e. central concepts tensor analysis tensor matricization i.e. rearranging entries shape matrix. disjoint partition indices p|p|} p|p| matrix holding entries entry ad...dn placed take broader deﬁnition neuron scalar values comprising output array arbitrary layer network. case output array width height equal channels number neurons layer index +|p|t=m|p|−t column index +|q|t=m|q|−t. lastly formally rs×. .×rs function vectors called template vectors grid tensor denoted rm⊗...⊗m deﬁned ad...dn begin discussion well-deﬁned measure efﬁciency. wish compare efﬁciency non-overlapping convacs overlapping convacs ﬁxed representation functions functions realizable non-overlapping convacs shared representation functions function subspace case overlapping convacs realize additional functions outside sub-space induced non-overlapping convacs. cannot therefore compare architectures directly need compare auxiliary objective. following work cohen shashua instead compare architectures concept grid tensors speciﬁcally grid tensor deﬁned output convac i.e. tensor unlike ill-deﬁned nature directly comparing functions realized convacs cohen shashua proved assuming ﬁxed representation functions linearly independent exists template vectors nonoverlapping convac architecture could represent possible grid tensors templates given sufﬁcient number channels layer. speciﬁcally template vector chosen non-singular. thus linearly independent representation functions compare different convacs whether overlapping minimal size required induce grid tensor knowing ﬁnite number always exists. straightforward direction separating expressive efﬁciency network architectures examining ranks respective matricized grid tensors. speciﬁcally denote grid tensors respectively pargreater. beneﬁt studying efﬁciency matrix rank attain separation bounds exact realization also immediately gain access approximation bounds examining singular values matricized grid tensors. brings following lemma connects upper-bounds previously found non-overlapping convacs grid tensors induced lemma score function non-overlapping convac ﬁxed linearly independent continuous representation functions layers. partition dividing spatial dimensions output representation layer equal parts either along horizontal vertical axis referred left-right top-bottom partitions respectively. then template vectors non-singular lemma essentially means sufﬁcient show overlapping convacs attain ranks super-polynomial size prove exponentially efﬁcient respect nonoverlapping convacs. next section analyze overlapping degree related rank cases leads exponentially large rank. section analyze expressive efﬁciency overlapping architectures. begin deﬁning measures overlapping degree used claims followed presenting main results sec. sake brevity additional results light recent work cohen shashua pooling geometry deferred app. figure illustrating total receptive ﬁeld total stride attributes l’th layer could seen projected receptive ﬁeld stride respect input layer. together capture overlapping degree network. omitted arguments notice non-overlapping networks total receptive ﬁeld always equals total stride network spatial dimension collapses total receptive ﬁeld grow encompass entire size representation layer. overlapping networks case total receptive ﬁeld could grow much faster. intuitively means values regions input layer apart would combined non-overlapping networks near last layers networks thus non-overlapping networks effectively shallow comparison overlapping networks. base intuition next section analyze networks respect point total receptive ﬁeld large enough. preliminaries place ready present main result theorem assume convac ﬁxed representation layer output channels width height equal followed layers l’th layer local receptive ﬁeld stride output channels. layer total receptive ﬁeld then choice parameters except null template vectors non-singular following equality holds analyze efﬁciency overlapping architectures ﬁrst formulate rigorously measurement overlapping degree given architecture. mentioned sec. deﬁning concepts total receptive ﬁeld total stride given layer denoted respectively. measurements could simply thought projecting accumulated local receptive ﬁelds ﬁrst layer illustrated represent type global statistics architecture. however note proposition entails given architecture could smaller effective total receptive ﬁeld settings parameters. leads deﬁne α-minimal total figure network architectures beginning large local receptive ﬁelds greater least output channels. according theorem almost choice parameters obtain function cannot approximated non-overlapping architecture number channels next last layer less given last remark central part proof simply construction example. first parameters simpler case ﬁrst layer greater quarter input satisfying conditions theorem. motivation behind speciﬁc construction pairing indices side partition local receptive ﬁeld designing ﬁlters output local application deﬁnes mostly diagonal matrix rank respect indices. rest parameters chosen output entire network results product entries matrices. matricization results matrix equivalent kronecker product mostly diagonal matrices. thus matricization rank equal product ranks matrices results exponential form finally extend example general case realizing operation ﬁrst layer example multiple layers small local receptive ﬁelds. app. deﬁnitions lemmas rely app. complete proof. combined lemma results following corollary corollary setting theorem choices parameters overlapping convac except negligible non-overlapping convac realizes grid tensor must size least complexity generic lower-bound might seem incomprehensible ﬁrst generality gives tools analyze practically kind feed-forward architecture. example analyze lower bound well known googlenet architecture lower bound equals making clear using non-overlapping architecture case infeasible. next focus speciﬁc cases derive intelligible lower bounds. according theorem lower bound depends ﬁrst layer total receptive ﬁeld greater quarter input. mentioned previous section non-overlapping networks happens spatial dimension collapses entails total receptive ﬁeld total stride would equal width representation layer substituting values results simply trivially meaning realize nonoverlapping network another non-overlapping network next last layer must least half channels target network. extreme examine case ﬁrst layer local receptive ﬁeld greater quarter input i.e. since layers following ﬁrst layer affect lower bound case applies arbitrary sequence layers illustrated evenly divided simplicity also assume stride less matrices equivalent could converted elementary column operations. thus lower bound results case consider case non-overlapping architecture satisﬁes lower bound order magnitude could already represent possible grid tensor. demonstrate point introduction polynomial change architecture i.e. increasing receptive ﬁeld exponential increase expressivity. though last example already demonstrates polynomially sized overlapping architecture could lead exponential separation practice employing large convolutions resource intensive. common best practice multiple small local receptive ﬁelds size typical values separated pooling layers i.e. layers stride local receptive ﬁeld equal simplicity assume illustration network. analyzing network theorem results following proposition proposition consider network comprising sequence blocks block begins layer whose local receptive ﬁeld stride followed layer local receptive ﬁeld stride output channels layers least spatial dimension representation layer h=l. then lower bound describe network greater equal proof sketch. ﬁrst closed-form expression total receptive ﬁeld stride layers given network. show layers whose total receptive ﬁeld greater ﬁrst layer satisﬁes conditions theorem closed-forms expressions simplify general lower bound case. app. complete proof. particular typical values lower bound least demonstrates even small amount overlapping already leads exponential separation non-overlapping case. grows size bound approaches earlier result shown large local receptive ﬁelds encompassing quarter image. grows size lower bound dominated strictly local receptive ﬁelds. also notice based proposition could also derive respective lower bound network following style architecture instead single convolutional layer every pooling layer layers local receptive ﬁeld case trivial show bound proposition holds typical values results lower bound least figure training accuracies standard convnets cifar- data augmentations results spatial augmentations presented color augmentations bottom row. network follows architecture proposition receptive ﬁeld using number channels across layers speciﬁed horizontal axis left plot. plot results respect total number parameters right plot. section show theoretical results sec. indeed hold practice. words exists tasks require highly expressive power overlapping architectures non-overlapping architectures would grow exponential factor achieve level performance. demonstrate phenomenon standard convnets relu activations follow architecture outlined proposition varying number channels size receptive ﬁeld conv layers. change made replace ×-pooling layers convolutional type standard ×max-pooling layers using number channels across layers. done purpose learned parameters located overlapping layers. speciﬁcally network blocks starting convolution channels stride relu activation followed max-pooling layer. ﬁfth conv-pool ﬁnal dense layer outputs softmax activations. train networks classiﬁcation cifar- dataset types data augmentation schemes spatial augmentations i.e. randomly translating horizontally ﬂipping image color augmentations following dosovitskiy i.e. randomly adding constant shift saturation luminance attribute separately addition randomly sampling multiplier saturation luminance. though typically data augmentation used purpose regularization employ sole purpose raising hardness regular cifar- dataset even small networks already overﬁt effectively memorize small dataset. separately test spatial color augmentation schemes emphasize empirical results cannot explained simply spatial-invariance type arguments. finally training carried epochs adam using standard hyper-parameters point loss considered networks stopped decreasing. report training accuracy augmented dataset value receptive ﬁeld plot respective training accuracies variable number channels source code reproducing experiments plots found https//github.com/huji-deep/overlapsandexpressiveness. quite apparent greater chosen less channels required achieve accuracy. moreover non-overlapping case channels required reach performance networks channels spatial augmentations means effectively exponentially channels required. even color augmentations able train non-overlapping networks reach even smallest overlapping network terms total number parameters clear separation overlapping non-overlapping types order magnitude increase number parameters overlapping non-overlapping architectures achieve similar training accuracy. somewhat surprising result though based limited experiments appears number parameters overlapping networks attain training accuracy suggesting perhaps smallest amount overlapping already attain beneﬁts overlapping provides increasing affect performance terms expressivity. ﬁnal remark also wish acknowledge limitations drawing conclusions strictly empirical experiments could alternative explanations observations e.g. effects overlapping optimization process. nevertheless theoretical results suggests less likely case. common belief amongst deep learning researchers depth factors success deep networks belief formalized depth efﬁciency conjecture. nevertheless depth many attributes specifying architecture deep networks could potentially important. paper studied effect overlapping receptive ﬁelds expressivity network found them broadly denser connectivity results exponential gain expressivity orthogonal depth. analysis sheds light many trends practices contemporary design neural networks. previous studies shown non-overlapping architectures already universal even certain advantages terms optimization real-world usage non-overlapping networks scarce. though could multiple factors involved results clearly suggest main culprit non-overlapping networks signiﬁcantly handicapped terms expressivity compared overlapping ones explaining former rarely used. additionally examining networks commonly used practice majority layers convolutional type small receptive ﬁeld fully-connected layers though obviously overlapping overlapping degree rather low. showed denser connectivity increase expressive capacity even common types modern architectures already exhibit exponential increase expressivity without relying fully-connected layers. could partly explain somewhat surprising observation probable networks sufﬁciently expressive practical needs simply already exponential regime expressivity. indeed experiments seems suggests same increases overlapping degree beyond limited overlapping case seems insigniﬁcant effects performance conjecture quite proven current work wish investigate future. relatively works studied role receptive ﬁelds neural networks. several empirical works demonstrated similar behavior showing classiﬁcation accuracy networks sharply decline degree overlaps decreased also showing gains using large local receptive ﬁelds insigniﬁcant compared increase computational resources. works studying receptive ﬁelds neural networks mainly focused learn data analysis direct implications speciﬁc works ground work potentially guiding architecture design quantifying expressivity given architecture. lastly studied effective total receptive ﬁeld different layers property similar nature total receptive ﬁeld measure degree input pixel affecting output activation. show common random initialization weights effective total receptive ﬁeld gaussian shape much smaller maximal total receptive ﬁeld. additionally demonstrate training effective total receptive ﬁeld grows size suggests weights initialized initial effective receptive ﬁeld conclude shown theoretically empirically overlapping architectures expressive advantage compared non-overlapping ones. theoretical analysis grounded framework convacs extend overlapping conﬁgurations. though proofs limited speciﬁc case previous studies already shown results could transferred standard convnets well using mathematical machinery. adapting analysis accordingly left future work experiments standard convnets already suggest core results hold case well. finally interesting outcome moving non-overlapping architectures overlapping ones depth network longer capped case models investigated cohen property examine future works alexey dosovitskiy jost tobias springenberg martin riedmiller thomas brox. discriminative unsupervised feature learning convolutional neural networks. advances neural information processing systems nips deep learning workshop wenjie yujia raquel urtasun richard zemel. understanding effective receptive field deep convolutional neural networks. advances neural information processing systems pages poole subhaneil lahiri maithreyi raghu jascha sohl-dickstein surya ganguli. exponential expressivity deep neural networks transient chaos. advances neural information processing systems nips deep learning workshop jost tobias springenberg alexey dosovitskiy thomas brox martin riedmiller. striving simplicity convolutional net. workshop track international conference learning representations christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. computer vision pattern recognition cvpr yaniv taigman ming yang marc’aurelio ranzato lior wolf. deepface closing humanlevel performance face veriﬁcation. computer vision pattern recognition cvpr. ieee computer society june aaron oord sander dieleman heiga karen simonyan oriol vinyals alex graves kalchbrenner andrew senior koray kavukcuoglu. wavenet generative model audio. corr abs/. base analysis convolutional arithmetic circuit architecture introduced cohen illustrated simply thought regular convnet linear activations product pooling layers instead common non-linear activations average/max pooling. speciﬁcally point input space network denoted represented n-length sequence s-dimensional vectors typically thought image corresponds local patches image. ﬁrst layer network referred representation layer consisting applying representation functions local patch giving rise feature maps. common setting point-wise activation representation functions selected parameterized representation layer reduces standard convolutional layer. possibilities e.g. gaussian functions diagonal covariances also considered cohen following representation layer hidden layers indexed begins conv operator convolutional layer input channels output channels sole exception parameters kernel could spatially unshared following conv layer spatial pooling takes products non-overlapping two-dimensional windows covering output previous layer pooling window size entire spatial dimension reducing output’s shape i.e. rl−-dimensional vector. ﬁnal layer maps vector dense linear layer network outputs denoted representing score functions classifying classes through argmaxy shown cohen functions following form called coefﬁcients tensor tensor order dimension mode sake discussion simply seen multi-dimensional array speciﬁed indices ranging entries given polynomials network’s conv weights. byproduct ﬁxed representation functions functions represented convacs subspace functions. theorem learn overlaps give rise networks almost always cannot efﬁciently implemented non-overlapping convac standard pooling geometry. however proven cohen shashua convac uses different pooling geometry i.e. input pooling layers strictly contiguous windows previous layer also cannot efﬁciently implemented standard convac standard pooling geometry. raises question whether overlapping operations simply equivalent convac different pooling geometry nothing more. answer question parts. first convac different pooling geometry might able implement function efﬁciently convac standard pooling geometry however reverse also true convac standard pooling implement functions efﬁciently convac alternative pooling. contrast convac uses overlaps still capable implement efﬁciently function non-overlapping convac standard pooling can. second also show overlapping architectures exponentially efﬁcient non-overlapping convac regardless pooling geometry. accomplished ﬁrst extending lemma case next theorem show overlapping architectures induce grid tensors whose matricized rank exponential equal partition indices proving indeed exponentially efﬁcient theorem settings theorem consider network whose representation layer followed layer local receptive ﬁeld stride output channels whose parameters unshared i.e. unique spatial location output layer opposed shared across them followed arbitrary layers whose ﬁnal output scalar. choice parameters except null template vectors non-singular matricized rank induced grid tensor equal equal partition indices. exact result holds parameters ﬁrst layers shared proof sketch. follow steps proof theorem however construct speciﬁc overlapping network attains rank possible matricizations induced grid tensor. instead construct separate network possible matricization. proves respect lebesgue measure network’s parameters space separately pooling geometry parameters lower bound hold measure zero. since ﬁnite union zero measured sets also measure zero lower bound respect possible pooling geometries holds almost everywhere concludes proof sketch. app. complete proof. important note though theorem shows pooling geometry less expressive overlapping networks standard pooling mean pooling geometry irrelevant. specifically know effect combining overlaps alternative pooling geometries together. additionally many times sufﬁcient expressivity main obstacle solving speciﬁc task inductive bias induced carefully chosen pooling geometry could help reduce overﬁtting. section preliminaries required understand proofs following sections. begin limited introduction tensor analysis followed quoting relevant known results relating tensors convacs. begin basic deﬁnitions operations relating tensors. rm⊗···⊗mn tensor order dimension mode i.e. ad...dn tensors orders dimensions modes respectively deﬁne tensor product order tensor vectors ordered tensor v⊗···⊗v called elementary tensor rank- tensor. generally tensor represented linear combination v⊗···⊗v known rank- decomposition decomposition minimal equality holds knows tensor rank given matrices ∈rm×m ∈rmn denote ⊗···⊗f linear transformation rm⊗···⊗mn rm⊗···⊗mn elementary tensor notations above holds that central concept tensor analysis tensor matricization. disjoint partition indices p|p|} p|p| q|q|} q|q|. -bymatrix holding entries entry ad...dn t=t+ matricization respect partition denoted placed index +|p| applying matricization operator·pq tensor product operator results kronecker product i.e. simply sets obtained subtracting number every element respectively. concrete terms kronecker product matrices rm×m rn×n results matrix rmn×mn holding aijbkl index column index important property kronecker product rank rank rank typically wish compute rank apq) ﬁrst decompose kronecker product matrices. matrices invertible matrix rank ofapq equals matrix rank offpq using notations describing general convac coefﬁcients tensor order dimension mode rs→r representation functions deﬁnitions non-overlapping convac said decompose coefﬁcients tensor different network architectures correspond known tensor decompositions shallow networks corresponds rank- decompositions deep networks corresponds hierarchical tucker decompositions. cohen shashua found matrix rank matricization coefﬁcients tensors could serve bound size networks decomposing conventional non-overlapping convac contiguous low-high partition rank network decomposes coefﬁcients tensor common case square inputs i.e. input shape natural represent indices pairs denoting spatial location patch ﬁrst argument denotes vertical location second denotes horizontal location. setting equivalent low-high partitions either left-right partition i.e. generally considering networks using pooling geometries i.e. strictly contiguous pooling windows pooling geometry exists corresponding partition though results cohen shashua strictly based matricization rank coefﬁcients tensors transferred matricization rank grid tensors well. grid tensors ﬁrst considered analyzing convacs cohen shashua template vectors deﬁne matrix rm×m notations place write grid tensor function representation functions linearly independent continuous choose template vectors non-singular according previous discussion tensor matricization means partition coefﬁcients tensor holds translates lower bound matricization rank coefﬁcients tensors turn serves lower bound size non-overlapping convacs. discussion leads proof lemma lemma previously stated proof lemma lemma proofs base results respect coefﬁcients tensor cohen shashua prove possible choose template vectors non-singular cohen shashua prove non-singular grid tensor coefﬁcients tensor matricization rank lemma hackbusch additionally quote following lemma regarding prevalence maximal matrix rank matrices whose entries polynomial functions lemma min{m polynomial mapping rm×n i.e. every holds polynomial function. exists point rank rk|ranka zero measure finally simplify notations layer product pooling function beneﬁt following proofs below. represent parameters l’th layer rd×r×r )}c∈] represents weights biases. rh×h×d rr×r input layer rd×h×h output following equality holds proposition direct corollary following claims claim rd×h×h rd×h×h function realized single layer local receptive ﬁeld stride output channels parameterized layer local receptive ﬁeld stride output b)}d channels parameterized ˜b)}d could also realize true unshared case layers. proof. claim trivially satisﬁed setting equal matching coordinates using zeros coordinates. similarly equal matching coordinates using ones coordinates. claim rd×h×h rd×h×h function realized layer local receptive ﬁeld stride parameterized b)}d exists assignment identity function true unshared case layers. wish show choices parameters except null grid tensor induced given network rank satisfying since entries matricized grid tensor polynomial function parameters according lemma sufﬁcient single example achieves bound. hence proof simply construction example. recall template vectors must hold matrix deﬁned {fj}m representation matrices non-singular matrix. additionally assume following claims output representation layer width height equal even number claims proofs however easily adapted general case. assume convac described theorem representation layer deﬁned according above followed layers l’th layer local receptive ﬁeld stride output channels. ﬁrst construct example achieves desired matricization rank simpler case ﬁrst layer following representation layer local receptive ﬁeld large enough i.e. larger recall ﬁrst layer total receptive ﬁeld equal local receptive ﬁeld. context speciﬁc construction presented following claim relies utilizing large local receptive ﬁeld match spatial location left side input right side pair respective output ﬁrst layer represent mostly diagonal matrix. rest parameters output entire network deﬁned tensor product mostly diagonal matrices. since matricization rank tensor product matrices equal product individual ranks results exponential form rank given theorem. claim assume convac deﬁned above ending single scalar output. parameters l-th layer denoted rd×r×r )}c∈]. function realized output network. additionally deﬁne min{d weights biases ﬁrst layer layer proof. proof either left-right top-bottom partition completely symmetric thus enough prove claim left-right case wish compute entry ad...d induced grid tensor arbitrary indices rm×h×h -order tensor output representation layer omji aforementioned indices begin setting parameters layers following ﬁrst layer equal computing along channels axis output ﬁrst layer followed global product resulting sums. achieve this ﬁrst assume w.l.o.g. layers non-overlapping proposition parameters second layer i.e. ones zeros respectively equivalent taking along channels axis spatial location followed taking products non-overlapping local receptive ﬁelds size layers simply take output ﬁrst channel output preceding layer setting parameters preceding claim describe example case total receptive ﬁrst layer already large enough satisfying conditions theorem. following claim extend result general case. accomplished showing network comprised layers local receptive ﬁelds r}l∈ strides s}l∈ output channels {d}l∈ effectively compute output ﬁrst layer claim inputs recall layer claim performs identical transformation patch input followed taking point-wise product far-away pairs transformed patches. thus motivation behind speciﬁc construction ﬁrst layers perform transformation using half output channels storing transformed patch location half storing transformed patch location farthest right constrained local receptive ﬁeld. equal transformed patches sitting still another shifted transformed patches. layers simply pass ﬁrst half channels using identity operation deﬁned claim continuously shifting half channels left bringing faraway patches closer together. finally last layer take halves multiple together. claim assume convac comprised layers described above output network limited scalar value. assume total stride l-th layer greater total stride α-minimal total receptive ﬁeld respectively convac comprised single layer local receptive ﬁeld stride output channels min{ proof. possible cases completely symmetric thus enough prove claim additionally assume w.l.o.g. setting layer pass-through according claim also assume α-minimal total receptive ﬁeld exactly equal total receptive ﬁeld l-th layer applying claim realize equivalent network smaller windows. finally case trivial thus assume left prove satisﬁes claim. rd×h×h output l-th layer width height output. additionally assume indices beyond bounds value zero i.e. assume zero padding applying convolutional operation layer. extend deﬁnition setting identify input network given above output ﬁrst layer follows since matricization entries matricization polynomial functions respect parameters network then according lemma parameters attain rank zero measure. since union zero measured sets also measure zero parameters except zero measure attain matricization rank partitions once concluding proof. α-minimal total receptive ﬁeld following claims analyze properties given network claim total stride total receptive ﬁeld l-th layer given network i.e. layer representation layer given following equations notice right term equation resembles binary representation. limit term represent number choosing complete term represent number speciﬁcally exists assignment terms equal thus begin proving analogue claim show given matricization grid tensor induced overlapping network realizing function matricization rank exponential. motivation behind construction parameters unshared utilized fact separate sets kernels local receptive ﬁelds size input spatial location. thus kernel connect index matching spatial location almost index speciﬁcally indices come different sets matricization shared case simply polynomially output channels simulate unshared case. claim arbitrary even partition exists assignment parameters network given theorem either unshared shared preceding claim found separate example matricization matricization rank exponential. following proof theorem leverage basic properties measure theory show almost everywhere induced grid tensor exponential matricization rank every possible even matricization without explicitly constructing example.", "year": 2017}