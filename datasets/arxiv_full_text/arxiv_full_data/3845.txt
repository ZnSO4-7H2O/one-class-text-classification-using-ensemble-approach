{"title": "Translating Neuralese", "tag": ["cs.CL", "cs.NE"], "abstract": "Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents' messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.", "text": "figure example interaction pair agents deep communicating policy. cars attempting cross intersection cannot other. exchanging message vectors agents able coordinate avoid collision. paper presents approach understanding contents message vectors translating natural language. backpropagation communication channel avoiding many challenging inference problems associated learning classical decentralized decision processes analysis strategies induced dcps remained challenge. example figure depicts driving game cars unable other must cross intersection without colliding. order ensure success clear cars must communicate other. number successful communication strategies possible—for example might report exact coordinates every timestep might simply announce whenever entering leaving intersection. messages communicated natural language would straightforward determine strategy employed. however agents instead communicate automatically induced protocol unstructured real-valued recurrent state vectors—an artiﬁcial language might call neuralese superﬁcially bears little resemblance natural language thus frustrates attempts direct interpretation. several approaches recently proposed learning decentralized deep multiagent policies coordinate differentiable communication channel. policies effective many tasks interpretation induced communication strategies remained challenge. propose interpret agents’ messages translating them. unlike typical machine translation problems parallel data learn from. instead develop translation model based insight agent messages natural language strings mean thing induce belief world listener. present theoretical guarantees empirical evidence approach preserves semantics pragmatics messages ensuring players communicating translation layer suffer substantial loss reward relative players common language. several recent papers described approaches learning deep communicating policies decentralized representations behavior enable multiple agents communicate differentiable channel formulated recurrent neural network. dcps shown solve variety coordination problems including reference games logic puzzles simple control appealingly agents’ communication protocol learned direct propose understand neuralese messages translating them. work present simple technique inducing dictionary maps neuralese message vectors short natural language strings given examples agents interacting agents humans interacting humans. natural language already provides rich tools describing beliefs observations plans—our thesis tools provide useful complement visualization ablation techniques used previous work understanding complex models structurally quite similar task machine translation pairs human languages interpretation neuralese poses number novel challenges. first natural source parallel data bilingual speakers neuralese natural language. second direct correspondence strategy employed humans agents even constrained communicate using natural language automated agent might choose produce different message humans given state. tackle challenges appealing grounding messages gameplay. approach based core insights natural language semantics messages similar meanings induce similar beliefs state world. based intuition introduce translation criterion matches neuralese messages natural language strings minimizing statistical distance common representation space distributions speaker states. explore several related questions figure overview approach—best-scoring translations generated reference game involving images birds. speaking agent’s goal send message uniquely identiﬁes bird left. translations seen learned model appears discriminate based coarse attributes like size color. human–human translation problems grounded gameplay. paper focus experiments speciﬁcally problem interpreting communication deep policies apply approach driving game figure reference games kind shown figure approach outperforms conventional machine translation criterion attempting interoperate neuralese speakers predicting state. variety approaches learning deep policies communication proposed essentially simultaneously past year. broadly labeled deep communicating policies; concrete examples include lazaridou foerster sukhbaatar policy representation employ paper similar latter these although general framework agnostic low-level modeling details could straightforwardly applied architectures. analysis communication strategies papers largely adhoc obtained clustering states similar messages emitted attempting manually assign semantics clusters. present work aims developing tools performing analysis automatically. closely related approach lazaridou also develop model assigning natural language interpretations learned messages; however approach relies supervised cluster labels targeted specifically towards referring expression games. attempt develop approach handle general multiagent interactions without assuming prior discrete structure space observations. literature learning decentralized multiagent policies general considerably larger includes work focused communication multiagent settings even communication using natural language messages approaches employ structured communication schemes manually engineered messaging protocols; sense automatically interpretable cost introducing considerable complexity training inference. evaluation paper investigates communication strategies arise number different games including reference games extended-horizon driving game. communication strategies reference games previously explored vogel andreas klein kazemzadeh reference games speciﬁcally featuring end-to-end communication protocols control side long line work considers nonverbal communication strategies multiagent policies another group related approaches focuses development general machinery interpreting deep models messages explicit semantics. includes visualization techniques approaches focused generating explanations form natural language games consider cooperative game players form given figure every step game player makes observation takes action sends message distributions together deﬁne policy assume shared players i.e. standard markov decision process actions alter world state generating observations players reward shared both. sages. goal work learn translate pairs languages generated different policies. speciﬁcally assume access policies game robot policy human policy would like representation behavior transparent human users order understand behavior inducing bilingual dictionaries message vectors natural language strings vice-versa. learned agents goal present tools interpretation learned messages agnostic details underlying algorithm acquiring them. generic model basis techniques developed paper. agent policy represented deep recurrent network network built communicating cells kind depicted figure every timestep agent receives three pieces information figure cell implementing single step agent communication foerster denotes multilayer perceptron; denotes gated recurrent unit dashed lines represent recurrent connections. observation current state world agent’s memory vector previous timestep message player. produces three outputs predicted value every possible action memory vector next timestep message send agent. sukhbaatar observe models form viewed specifying single weight matrices particular block structure. models thus trained using standard recurrent q-learning objective communication protocol learned end-to-end. human agents translation model develop requires representation distribution messages employed human speakers model human message generation process categorical simple multilayer perceptron model observations words phrases used human gameplay. mean message translation message standard machine translation problems answer likely co-occur parallel data large. parallel data even could observe natural language neuralese messages produced agents state would guarantee messages actually served function. answer must instead appeal fact natural language neuralese messages grounded common environment. given neuralese message ﬁrst compute grounded representation message’s meaning; translate natural-language message whose meaning similar. question form grounded meaning representation take. existing literature suggests broad approaches semantic representation meaning message given denotations world states felicitously predicated given existing context available listener. probabilistic terms says meaning message represented distribution induces speaker states. examples approach include guerin pitt pasupat liang pragmatic representation meaning message given behavior induces listener. probabilistic terms says meaning message represented distribution induces actions given listener’s observation examples approach include vogel gauthier mordatch language unique name every kind shape bottom language distinguishes shapes sides shapes many sides. imagine simple reference game following form player covertly assigned three shapes reference target communicates reference must pull lever labeled large small depending size target shape. blue language speakers achieve perfect success game language speakers succeed best three times. translate blue word hexagon language? semantic approach suggests translate hexagon many many uniquely identify hexagon produces distribution shapes closest truth. pragmatic approach instead suggests translate hexagon message guarantees listener pull correct lever large. order produce correct listener action translator might produce maximally inaccurate listener belief. exclusively concerned building translation layer allowed humans agents interoperate effectively possible would natural adopt pragmatic representation strategy. goals broader also want facilitate understanding specifically help users learned systems form true beliefs systems’ computational processes representational abstractions. example demonstrates pragmatically optimizing directly task performance sometimes lead translations produce inaccurate beliefs. zero messages give rise identical belief distributions increases grow dissimilar. translate would like compute minzh minzr intuitively equation says measure quality proposed translation asking following question contexts likely used frequently induce belief speaker states translation criterion directly encodes semantic notion meaning described section doubly intractable divergence outer expectation involve observations respectively; sums general possible compute efﬁciently. avoid this approximate equation sampling. draw collection samples prior world states generate sama|xb) sequence distractors term equation computed true sample distractors normalized averaged compute ﬁnal score. sampling accounts outer equation inner equation instead build approach around semantic representations meaning. preserving semantics allow listeners reason accurately content interpretation messages. might worry adopting semantics-ﬁrst view given guarantees effective interoperation humans agents using translation layer. fortunately section possible show players communicating semantic translator perform boundedly worse pairs players common language. section build intuition messages translated semantics deﬁne concrete translation model—a procedure constructing natural language neuralese dictionary given agent human interactions. understand meaning message represented distribution induces speaker states given listener context. formalize deﬁning belief distribution message context modeled listener performing single step bayesian inference using listener state message generation model compute posterior speaker states. general neither humans agents compute explicit representations posterior past work found humans suitably-trained neural networks modeled bayesian reasoners provides context-speciﬁc representation belief messages semantics must induce belief contexts occur. probabilistic formulation introduces outer expectation contexts providing ﬁnal measure quality translation reward function depends observations action. speaker language listener language listener different language suppose wish interact translator takes action xb)). respects semantics bilingual pair achieves boundedly worse reward monolingual pair speciﬁcally discussed section even committing semantic approach meaning representation still succeeded capturing nice properties pragmatic approach. section examined consequences mismatch primitives available languages. general would like measure approach’s robustness lack exact correspondence languages. case humans particular expect variety different strategies employed many correspond behavior learned agent. natural want assurance identify dcp’s strategy long human strategy mirrors second observation possible exactly recover translation strategy mixture humans playing different strategies this notion rationality fairly weak permits many suboptimal communication strategies requires listener well possible given ﬁxed speaker— ﬁrst-order optimality criterion likely satisﬁed richly-parameterized model trained gradient descent. quantities remaining form case neuralese determined agent policy natural language transcripts human interactions model maps world states distribution frequent utterances discussed section details model implementations provided appendix full translation procedure given algorithm translation criterion previous section makes reference listener actions all. shapes example section shows model performance might lost translation. thus reasonable whether translation model section make guarantees effect translation behavior. section explore relationship beliefpreserving translations behaviors produce examining effect belief accuracy strategy mismatch reward obtained cooperating agents. facilitate analysis consider simpliﬁed family communication games structure depicted figure games viewed subset family depicted figure consist steps listener makes observation sends single message speaker makes observation takes single action receives reward. emphasize results section concern theoretical properties idealized games presented provide intuition high-level properties approach. section investigates empirical behavior approach real-world tasks ideal conditions hold. suppose messages employed human strategies disjoint; suppose messages support every message translated message produced messages strategies ignored. observation follows immediately deﬁnition demonstrates distinctions approach conventional machine translation criterion. maximizing produce natural language message often produced contexts observed regardless whether message useful informative. contrast minimizing corresponds closely even rarely used. disjointness condition seemingly quite strong fact arises naturally many circumstances—for example players driving game reporting spatial locations absolute relative coordinates speakers color reference game discriminating based lightness hue. also possible relax condition require strategies locally disjoint case overlapping human strategies allowed recovered robot strategy context-weighted mixture these. tasks remainder paper evaluate empirical behavior approach translation. evaluation considers kinds tasks reference games navigation games. reference game players observe pair candidate referents. speaker assigned target referent; must communicate target listener performs choice action corresponding belief true target. paper consider variants reference game simple color-naming task complex task involving natural images birds. examples human communication strategies tasks obtain xkcd color dataset caltech– ucsd birds dataset figure tasks used evaluate translation model. reference games players observe pair reference candidates player assigned target player must guess based message driving game attempts navigate goal cars cannot other must communicate avoid collision. ﬁnal task consider driving task ﬁrst discussed introduction. task cars invisible other must navigate randomly assigned start goal positions without colliding. task takes number steps complete potentially involves much broader range communication strategies. obtain human annotations task recorded actions messages generated pairs human amazon mechanical turk workers playing driving game other. collected close games total messages exchanged held game traces test set. mechanism understanding behavior learned model allow human user correctly infer beliefs successfully interoperate accordingly report results belief behavior evaluations. table evaluation results reference games. colors task. birds task. whether model human listener speaker role translation based belief matching outperforms random machine translation baselines. baselines compare approach baselines random baseline chooses translation input uniformly messages observed training direct baseline directly maximizes p|z) accomplished sampling speaker training states labeled natural language strings. translation) focus developing automatic measures system performance. available training data develop simulated models human decisions; ﬁrst showing models track well human judgments conﬁdent evaluations correlate human understanding. employ following metrics belief evaluation evaluation focuses denotational perspective semantics motivated initial development model. successfully understood semantics message translating human listener form correct belief state produced. construct simple state-guessing game listener presented translated message state observations must guess state speaker message emitted. translating natural language neuralese learned agent model directly guess hidden state. neuralese natural language must ﬁrst construct model human listener strings back state representations; using training data simple regression model scores pairs using bag-of-words sentence representation. model human matches judgments real humans time colors task time birds task time driving task. gives conﬁdence model human gives reasonably accurate proxy human interpretation. behavior evaluation evaluation focuses cooperative aspects interpretability measure extent learned models able interoperate translation layer. case reference games goal semantic evaluation identical goal game perform additional pragmatic evaluation driving game. found reliable make human game traces construct speaker-only model human. evaluation selects full game trace human player replays human’s actions messages exactly evaluation measures quality natural-language-to-neuralese translator extent learned agent model investigated problem interpreting message vectors deep networks translating them. introducing translation criterion based matching listener beliefs speaker states presented theoretical empirical evidence criterion outperforms conventional machine translation approach recovering content message vectors facilitating collaboration humans learned agents. evaluation focused understanding behavior deep communicating policies framework proposed paper could much generally applied. encoder– decoder model thought kind communication game played encoder decoder analogously imagine computing translating beliefs induced encoding explain features input transmitted. current work focused learning purely categorical model translation process supported unstructured inventory translation candidates future work could explore compositional structure messages attempt synthesize novel natural language neuralese messages scratch. broadly work shows denotational perspective formal semantics provides framework precisely framing demands interpretable machine learning particularly ensuring human users without prior exposure learned model able interoperate predict behavior diagnose errors. table belief evaluation results driving game. driving states challenging identify based messages alone translation based belief achieves best overall performance directions. table behavior evaluation results driving game. scores presented form reward completion rate. less accurate either humans dcps shared language models employ translation layer obtain higher reward greater overall success rate baselines. cases model trained communicate natural language achieves somewhat lower performance. regardless whether speaker listener model human vice-versa translation based belief-matching criterion section achieves best performance; indeed translating neuralese color names natural language listener able achieve slightly higher score natively. suggests automated agent discovered effective strategy demonstrated humans dataset effectiveness strategy preserved translation. example translations reference games depicted figure figure driving game behavior evaluation driving game shown table belief evaluation shown table translation messages driving game considerably challenging reference games scores uniformly lower; however clear beneﬁt beliefmatching model still visible. belief matching leads higher scores belief evaluation directions allows agents obtain higher reward average example translations driving game messages shown figure supported facebook graduate fellowship berkeley huawei fellowship. grateful lisa anne hendricks assistance caltech–ucsd birds dataset liang huang sebastian schuster useful feedback. references jacob andreas klein. reasoning pragmatics neural listeners speakers. proceedings conference empirical methods natural language processing. daniel bernstein robert givan neil immerman shlomo zilberstein. complexity decentralized control markov decision processes. mathematics operations research kyunghyun bart merri¨enboer dzmitry bahdanau yoshua bengio. properties neural machine translation encoder-decoder approaches. arxiv preprint arxiv. jilles steeve dibangoye christopher amato olivier buffet franc¸ois charpillet. optimally solving dec-pomdps continuous-state mdps. journal artiﬁcial intelligence research jakob foerster yannis assael nando freitas shimon whiteson. learning communicate deep multi-agent reinforcement learning. advances neural information processing systems. pages michael frank noah goodman peter joshua tenenbaum. informative communication word production word learning. proceedings annual conference cognitive science society. pages yang oscar beijbom ning zhang trevor prodarrell. compact bilinear pooling. ceedings ieee conference computer vision pattern recognition. pages frank guerin jeremy pitt. denotational semantics agent communication language. proceedings ﬁfth international conference autonomous agents. pages lisa anne hendricks zeynep akata marcus rohrbach jeff donahue bernt schiele trevor darrell. generating visual explanations. european conference computer vision. springer pages sahar kazemzadeh vicente ordonez mark matten tamara berg. referitgame referring objects photographs natural scenes. proceedings conference empirical methods natural language processing. pages scott reed zeynep akata honglak bernt schiele. learning deep representations ﬁne-grained visual descriptions. proceedings ieee conference computer vision pattern recognition. pages marco tulio ribeiro sameer singh carlos guestrin. trust you? explaining predictions classiﬁer. proceedings sigkdd international conference knowledge discovery data mining. pages maayan roth reid simmons manuela veloso. reasoning joint beliefs executionproceedings time communication decisions. fourth international joint conference autonomous agents multiagent systems. pages hendrik strobelt sebastian gehrmann bernd huber hanspeter pﬁster alexander rush. visual analysis hidden state dynamics recurrent neural networks. arxiv preprint arxiv. ramakrishna vedantam samy bengio kevin murphy devi parikh chechik. context-aware captions context-agnostic supervision. arxiv preprint arxiv. adam vogel bodoia christopher potts daniel jurafsky. emergence gricean maxproceedims multi-agent decision theory. ings human language technology conference north american chapter association computational linguistics. pages hidden state message agent distribution actions observation world. single hidden layer units tanh nonlinearity used mlp. hidden state also size message vector size agents trained interaction world hausknecht stone using adam optimizer discount factor step size chosen reference games driving game. \u0001-greedy exploration strategy employed exploration parameter timestep given foerster found useful noise communication channel case isotropic gaussian noise mean standard deviation also helps smooth computing translation criterion. representational models discussed section translation criterion computed based quantity policy representation actually deﬁnes distribution additionally involving agent’s hidden state previous timestep. principle possible eliminate dependence introducing additional sampling step algorithm found simpliﬁed inference simply learn additional model directly. simplicity treat term log)/p) constant could accurately approximated learned density estimator. multilayer perceptron single hidden layer tanh nonlinearities size also trained adam step size exactly model parameters implement representations human speakers case vector taken distribution messages natural language inventory model trained maximize likelihood labeled human traces. tasks colors version xkcd dataset prepared mcmahan stone input feature vector simply representation color message inventory taken unigrams appear least times. birds dataset welinder natural language annotations reed model’s input feature representations ﬁnal -dimensional hidden feature vector compact bilinear pooling model pre-trained classiﬁcation. message inventory consists frequent bigrams appear natural language descriptions; example human traces generated every frequent pair dataset. players restricted messages words required send least message game. player paid game. games collected different road layouts represented grid presented players figure action space discrete players move forward back turn left turn right wait. divided -game training -game test set. message inventory consists messages sent times. input features consists indicators agent’s current position orientation goal position identity. data available download http//github.com/jacobandreas/neuralese.", "year": 2017}