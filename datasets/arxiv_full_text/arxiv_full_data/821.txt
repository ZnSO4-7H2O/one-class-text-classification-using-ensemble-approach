{"title": "Overcoming Exploration in Reinforcement Learning with Demonstrations", "tag": ["cs.LG", "cs.AI", "cs.NE", "cs.RO"], "abstract": "Exploration in environments with sparse rewards has been a persistent problem in reinforcement learning (RL). Many tasks are natural to specify with a sparse reward, and manually shaping a reward function can result in suboptimal performance. However, finding a non-zero reward is exponentially more difficult with increasing task horizon or action dimensionality. This puts many real-world tasks out of practical reach of RL methods. In this work, we use demonstrations to overcome the exploration problem and successfully learn to perform long-horizon, multi-step robotics tasks with continuous control such as stacking blocks with a robot arm. Our method, which builds on top of Deep Deterministic Policy Gradients and Hindsight Experience Replay, provides an order of magnitude of speedup over RL on simulated robotics tasks. It is simple to implement and makes only the additional assumption that we can collect a small set of demonstrations. Furthermore, our method is able to solve tasks not solvable by either RL or behavior cloning alone, and often ends up outperforming the demonstrator policy.", "text": "abstract— exploration environments sparse rewards persistent problem reinforcement learning many tasks natural specify sparse reward manually shaping reward function result suboptimal performance. however ﬁnding non-zero reward exponentially difﬁcult increasing task horizon action dimensionality. puts many real-world tasks practical reach methods. work demonstrations overcome exploration problem successfully learn perform long-horizon multi-step robotics tasks continuous control stacking blocks robot arm. method builds deep deterministic policy gradients hindsight experience replay provides order magnitude speedup simulated robotics tasks. simple implement makes additional assumption collect small demonstrations. furthermore method able solve tasks solvable either behavior cloning alone often ends outperforming demonstrator policy. found signiﬁcant success decision making solving games makes challenging apply robotics? difference difﬁculty exploration comes choice reward function complicated environment dynamics. games reward function usually given directly optimized. robotics often desire behavior achieve binary objective naturally induces sparse reward. sparse reward functions easier specify recent work suggests learning sparse reward results learned policies perform desired objective instead getting stuck local optima however exploration environment sparse reward difﬁcult since random exploration agent rarely sees reward signal. difﬁculty posed sparse reward exacerbated complicated environment dynamics robotics. example system dynamics around contacts difﬁcult model induce sensitivity system small errors. many robotics tasks also require executing multiple steps successfully long horizon involve high dimensional control require generalization varying task instances. conditions result situation agent rarely sees reward initially able learn all. method solve problem stacking several blocks given location random initial state. stacking blocks studied literature exhibits many difﬁculties mentioned long horizons contacts requires generalizing instance task. limit human demonstrations collected teleoperation virtual reality. using demonstrations able solve complex robotics task simulation beyond capability reinforcement learning imitation learning. primary contribution paper show demonstrations used reinforcement learning solve complex tasks exploration difﬁcult. introduce simple auxiliary objective demonstrations method annealing away effect demonstrations learned policy better demonstrations method resetting demonstration states signiﬁcantly improves speeds training policies. effectively incorporating demonstrations shortcircuit random exploration phase reach nonzero rewards reasonable policy early training. finally extensively evaluate method commonly used methods initialization learning demonstrations ﬁne-tuning show method signiﬁcantly outperforms them. learning methods decision making problems robotics largely divide classes imitation learning reinforcement learning imitation learning agent receives behavior examples expert attempts solve task copying expert’s behavior. agent attempts maximize expected reward interaction environment. work combines aspects solve complex tasks. imitation learning perhaps common form imitation learning behavior cloning learns policy supervised learning demonstration stateaction pairs. seen success autonomous driving quadcopter navigation locomotion struggles outside manifold demonstration data. dataset aggregation augments dataset interleaving learned expert policy address problem accumulating errors however dagger difﬁcult practice requires access expert training instead demonstrations. fig. present method using reinforcement learning solve task block stacking shown above. robot starts blocks labelled table random positions target position block. task move block target position. targets marked visualization spheres interact environment. targets placed order block robot forms tower blocks. complex multi-step task agent needs learn successfully manage multiple contacts succeed. frames rollouts learned policy shown. video experiments found http//ashvin.me/demoddpg-website fundamentally approaches limited take account task environment. inverse reinforcement learning another form imitation learning reward function inferred demonstrations. among tasks applied navigation autonomous helicopter ﬂight manipulation since work assumes knowledge reward function omit comparisons approaches. reinforcement learning reinforcement learning methods harder apply robotics heavily investigated autonomy could enable. robots learned play table tennis swing cartpole balance unicycle renewal interest cascaded success games especially ability large function approximators learn control pixels. robotics challenging general signiﬁcant progress. deep applied manipulation tasks grasping opening door locomotion however results attained predominantly simulation high sample complexity typically caused exploration challenges. robotic block stacking block stacking studied early days robotics task encapsulates many difﬁculties complicated tasks want solve including multi-step planning complex contacts. shrdlu pioneering works studied block arrangements terms logic natural language understanding. recent work task motion planning considers logical physical aspects task requires domainspeciﬁc engineering. work study agent learn task without need domain-speciﬁc engineering. learning forward models naturally trouble modelling sharply discontinuous dynamics contacts; although learn place block much harder problem grasp block ﬁrst place. one-shot imitation learns stack blocks generalizes target conﬁgurations uses demonstrations train system. heavily shaped reward used learn stack lego block another contrast method succeed fully sparse rewards handle stacking several blocks. combining imitation learning previous work combined reinforcement learning demonstrations. demonstrations used accelerate learning classical tasks cart-pole swing-up balance work initialized policies initialized forward models demonstrations. initializing policies demonstrations used learning baseball underactuated swing. beyond initialization show extract knowledge demonstrations using effectively throughout entire training process. recent approaches deep q-learning demonstrations ddpg demonstrations combine demonstrations reinforcement learning. dqfd improves learning speed atari including margin loss encourages expert actions higher q-values actions. loss make improving upon demonstrator policy impossible case method. prior work previously explored improving beyond demonstrator policy simple environments introducing slack variables method uses learned value actively inform improvement. ddpgfd solves simple robotics tasks akin insertion using ddpg demonstrations replay buffer. contrast prior work tasks consider exhibit additional difﬁculties interest robotics multi-step stabilize learning value equation usually computed using separate network whose weights exponential average time critic network. results smoother target values. note ddpg natural using demonstrations. since ddpg trained off-policy demonstration data off-policy training data. also take advantage action-value function learned ddpg better demonstrations. instead standard setting train agents parametrized goals lead general policies recently shown make learning sparse rewards easier goals describe task expect agent case specify desired positions objects. sample goal beginning every episode. function approximators take current goal additional input. handle varying task instances parametrized goals hindsight experience replay insight even failed rollouts reward obtained agent transform successful ones assuming state rollout actual goal. used off-policy algorithm assuming every state goal corresponding state every episode agent experiences store replay buffer twice original goal pursued episode goal corresponding ﬁnal state achieved episode agent intended reaching state beginning. first maintain second replay buffer store demonstration data format minibatch draw extra examples off-policy replay data update step. examples included actor critic update. idea introduced behaviours generalization varying goal states. previous work focuses speeding already solvable tasks show extend state demonstrations introducing methods incorporate demonstrations. consider standard markov decision process framework picking optimal actions maximize rewards discrete timesteps environment assume environment fully observable. every timestep agent state takes action receives reward evolves state xt+. reinforcement learning agent must learn policy maximize expected horizon agent optimizes discount factor future rewards. agent’s objective maximize expected return start distribution erisi∼eai∼π. variety reinforcement learning algorithms developed solve problem. many involve constructing estimate expected return given state taking action call action-value function. equation recursive version equation known bellman equation. bellman equation allows methods estimate resemble dynamic programming. ddpg method combines demonstrations method deep deterministic policy gradients ddpg off-policy model-free reinforcement learning algorithm continuous control utilize large function approximators neural networks. ddpg actor-critic method bridges policy gradient methods value approximation methods high level ddpg learns action-value function minimizing bellman error simultaneously learning policy directly maximizing estimated action-value function respect parameters policy. concretely ddpg maintains actor function parameters critic function parameters replay buffer tuples transition experienced. ddpg alternates running policy collect experience updating parameters. training rollouts collected extra noise exploration noise process. training step ddpg samples minibatch consisting tuples update actor critic networks. ddpg minimizes following loss w.r.t. update critic using loss directly prevents learned policy improving signiﬁcantly beyond demonstration policy actor always tied back demonstrations. next show account suboptimal demonstrations using learned action-value function. account possibility demonstrations suboptimal applying behavior cloning loss states critic determines demonstrator action better actor action overcome problem sparse rewards long horizon tasks reset training episodes using states goals demonstration episodes. restarts within demonstrations expose agent higher reward states during training. method makes additional assumption restart episodes given state true simulation. reset demonstration state ﬁrst sample demonstration demonstrations. uniformly sample state ﬁnal state achieved demonstration goal. roll trajectory given initial state goal usual number timesteps. evaluation time procedure. evaluate method several simulated mujoco environments. experiments simulated -dof fetch robotics parallel grippers manipulate objects placed table front robot. agent receives positions relevant objects table observations. control agent continuous -dimensional dimensions specify desired end-effector position dimension speciﬁes desired distance robot ﬁngers. agent controlled frequency. collect demonstrations virtual reality environment. demonstrator sees rendering observations agent records actions vive interface frequency agent. option accept reject demonstration; accept demonstrations judge mostly correct. demonstrations optimal. extreme example sliding task demonstrations successful agent still sees rewards demonstrations her. train models adam optimizer learning rate ./nd. discount factor demonstrations initialize function approximators deep neural networks relu activations regularization coefﬁcient ﬁnal activation function tanh output value scaled range action dimension. explore training sample random actions uniformly within action space probability every step noise process uniform maximum value action dimension. task-speciﬁc information including network architectures provided next section. perform three sets experiments. sec. provide comparison previous work. sec. solve block stacking difﬁcult multi-step task complex contacts baselines struggle solve. sec. viii ablations method show effect individual components. sliding. puck placed randomly table must moved given target location. target outside robot’s reach must apply enough force puck reaches target stops friction. pick-and-place. block placed randomly table must moved target location air. note fig. baseline comparisons tasks frames learned policy shown task. method signiﬁcantly outperforms baselines. right plot baseline always fails. fig. compares method without demonstrations behavior cloning. method signiﬁcantly faster learning tasks achieves signiﬁcantly better policies behavior cloning does. measuring number timesteps convergence exhibit speedup pushing speedup sliding method solves pick-and-place task baseline cannot solve all. pick-and-place task showcases shortcoming sparse reward settings even her. pick-and-place action grasp block. robot could manage grasp small fraction time discovers achieve goals reinforces grasping behavior. however grasping block random actions extremely unlikely. method pushes policy towards demonstration actions likely succeed. paper solves pick-and-place task initializing half rollouts gripper grasping block. addition pick-and-place becomes easiest three tasks tested. initialization similar spirit initialization idea takes advantage fact pick-and-place goal solved starting block grasped certain location. always true ﬁnding keyframe tasks would difﬁcult requiring engineering sacriﬁcing autonomy. instead method guides exploration towards grasping show method solve complex tasks longer horizon sparser reward study task block stacking simulated environment shown fig. physical properties previous experiments. experiments show approach solve task full learn policy stack blocks demonstrations measure communicate various properties method also show experiments stacking fewer blocks subset full task. stacking blocks mean blocks reach target locations. since target locations always start ﬁrst block already position. stacking blocks involves pick-and-place actions. solve stacking allow agent timesteps. means stack blocks robot executes actions seconds. recorded demonstrations stack blocks subsets demonstrations demonstrations stacking fewer blocks. demonstrations perfect; include occasionally dropping blocks method handle suboptimal demonstrations. still rejected half demonstrations excluded demonstration data knocked tower blocks releasing block. fig. ablation results stacking blocks fully sparse reward. method times random seeds. bold line shows median runs training plotted lighter color. note always success rate. method without resets learns faster ablations. method resets initially learns faster converges worse success rate. policy supervised learning. behavior cloning requires much less computation fairer comparison performed large hyperparameter sweep various networks sizes attention hyperparameters learning rates report success rate achieved best policy found. method exactly described hindsight experience replay using ddpg. bc+her method ﬁrst initializes policy ﬁnetunes policy described above. able learn much longer horizon tasks methods shown fig. stacking task extremely difﬁcult using without demonstrations chance grasping object using random actions close initializing policy demonstrations running also fails since actor updates depend reasonable critic although actor pretrained critic not. pretrained actor weights therefore destroyed ﬁrst epoch result better alone. attempted variants method initially critic trained replay data. however also fails without seeing on-policy data. results sparse rewards encouraging. able stack blocks fully sparse reward without resetting states demonstrations blocks fully sparse reward resetting. resets demonstration states step reward able learn policy stack blocks. section perform series ablation experiments measure importance various components method. evaluate method stacking blocks. perform following ablations best performing note step reward still sparse; robot sees reward change moves block target location. subtract make reward interpretable initial state ﬁrst block already target. layer networks hidden units layer tasks stacking fewer blocks. stacking blocks more attention mechanism actor larger network. attention mechanism uses layer network hidden units layer query states goals shared head. state goal extracted layer network hidden units layer attention mechanism. attention speeds training slightly change training outcomes. baselines stacking blocks. ours refers method described section iv-c. ours resets refers method described section iv-c resets demonstration states method uses behavior cloning learn policy. given demonstration transitions train because computational constraints limited random seeds method stacking blocks random seeds method stacking blocks random seed method stacking blocks. although careful draw conclusions random seeds results consistent collective experience training models. report median random seeds everywhere applicable. fig. ablation results longer horizon tasks step reward. upper shows success rate lower shows average reward ﬁnal step episode obtained different algorithms. stacking blocks random seeds method. median runs shown bold training plotted lighter color. note stacking blocks method always success rate. number blocks increases resets demonstrations becomes important learn task. q-filter method uses standard behavioral cloning loss instead loss equation means actor tries mimic demonstrator’s behaviour regardless critic. hindsight experience replay used. behavior cloning loss without behavior cloning loss method signiﬁcantly worse every task try. fig. shows training curve learning stack blocks fully sparse reward. without behavior cloning loss system slower learn. longer horizon tasks achieve success without loss. consider training curves stacking blocks shown fig. policy learns stack additional block. without behavior cloning loss agent access demonstrations demonstration replay buffer. allows view highreward states incentivizes agent stack blocks stronger disincentive stacking tower higher risky could result lower reward agent knocks block already correctly placed. risk fundamentally another instance agent ﬁnding local optimum shaped reward agent learns safer behavior pausing achieving certain reward. explicitly weighting behavior cloning steps gradient updates forces policy continue task. q-filter effective accelerating learning achieving optimal performance. fig. shows method without ﬁltering slower learn. issue behavior cloning loss demonstrations suboptimal learned policy also suboptimal. filtering q-value gives natural anneal effect demonstrations automatically disables loss better action found. however gives mixed results longer horizon tasks. explanation step reward case learning relies less demonstrations reward signal stronger. therefore training less affected suboptimal demonstrations. initializing rollouts within demonstration states greatly helps learn stack blocks hurts training fewer blocks shown fig. note even resets demonstration states helps ﬁnal success rate learning takes faster technique used. however since stacking tower higher risky agent learns safer behavior stopping achieving certain reward. resetting demonstration states alleviates problem agent regularly experiences higher rewards. method changes sampled state distribution biasing towards later states. also inﬂates values unrealistically. therefore tasks algorithm stuck solving subset full problem could hurt performance. present system utilize demonstrations along reinforcement learning solve complicated multi-step tasks. believe accelerate learning many tasks especially sparse rewards difﬁculties exploration. method general applied continuous control task success criterion speciﬁed demonstrations obtained. exciting future direction train policies directly physical robot. fig. shows learning pick-andplace task takes million timesteps hours real world interaction time. realistically trained physical robot short-cutting simulationreality entirely. many automation tasks found factories warehouses similar pick-and-place without variation initial goal states samples required could much lower. method expert needs loop train systems demonstrations collected users without knowledge machine learning robotics rewards could directly obtained human feedback. major limitation work sample efﬁciency solving harder tasks. could solve tasks learning methods method requires large amount experience impractical outside simulation. tasks physical robots sample efﬁciency improved considerably. also require demonstrations easy collect tasks. demonstrations available environment reset arbitrary states learn goal-reaching avoid using demonstrations reuse successful rollouts finally method resets demonstration states requires ability reset arbitrary states. although solve many long-horizon tasks without ability effective hardest tasks. resetting demonstration rollouts resembles curriculum learning solve hard task ﬁrst solving easier tasks. environment afford setting arbitrary states curriculum methods used. thank vikash kumar aravind rajeswaran valuable discussions. thank sergey levine chelsea finn carlos florensa feedback initial versions paper. finally thank openai providing supportive research environment. ross gordon bagnell reduction imitation learning structured prediction no-regret online learning proceedings international conference artiﬁcial intelligence statistics schulman trust region policy optimization proceedings twenty-ﬁrst international conference machine learning winograd understanding natural language. academic press", "year": 2017}