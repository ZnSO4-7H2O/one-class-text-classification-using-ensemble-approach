{"title": "Learning Transformations for Classification Forests", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "This work introduces a transformation-based learner model for classification forests. The weak learner at each split node plays a crucial role in a classification tree. We propose to optimize the splitting objective by learning a linear transformation on subspaces using nuclear norm as the optimization criteria. The learned linear transformation restores a low-rank structure for data from the same class, and, at the same time, maximizes the separation between different classes, thereby improving the performance of the split function. Theoretical and experimental results support the proposed framework.", "text": "effect various popular weak learner models found including decision stumps general oriented hyperplane learner conic section learner. general even high-dimensional data usually seek low-dimensional weak learners separate different classes much possible. high-dimensional data often small intrinsic dimension. example area computer vision face images subject handwritten images digit trajectories moving object well-approximated lowdimensional subspace high-dimensional ambient space. thus multiple class data often union lowdimensional subspaces. theoretical low-dimensional intrinsic structures often violated real-world data. example assumption lambertian reﬂectance show face images subject obtained wide variety lighting conditions accurately approximated -dimensional linear subspace. however real-world face images often captured additional pose variations; addition faces perfectly lambertian exhibit cast shadows specularities data low-dimensional subspace arranged columns single matrix matrix approximately low-rank. thus promising handle corrupted underlying structures realistic data such deviations ideal subspaces restore low-rank structure. recent efforts invested seeking transformations transformed data decomposed low-rank matrix component sparse error guillermo sapiro department electrical computer engineering department computer science department biomedical engineering duke university durham work introduces transformation-based learner model classiﬁcation forests. weak learner split node plays crucial role classiﬁcation tree. propose optimize splitting objective learning linear transformation subspaces using nuclear norm optimization criteria. learned linear transformation restores low-rank structure data class time maximizes separation different classes thereby improving performance split function. theoretical experimental results support proposed framework. classiﬁcation forests recently shown great success large variety classiﬁcation tasks pose estimation data clustering object recognition classiﬁcation forest ensemble randomized classiﬁcation trees. classiﬁcation tree hierarchically connected tree nodes i.e. split nodes leaf nodes. split node associated different weak learner binary outputs splitting objective node optimized using training set. testing split node evaluates arriving data point sends left right child based weak learner output. proposed image alignment extension multiple-classes applications cryo-tomograhy) discussed context salient object detection. methods build recent theoretical computational advances rank minimization. paper present formulation random forests propose learn linear discriminative transformation split node tree improve class separation capability weak learners. optimize data splitting objective using matrix rank nuclear norm convex surrogate learning criteria. show learned discriminative transformation recovers low-rank structure data class time maximize subspace angles different classes. intuitively proposed method shares attributes linear discriminant analysis method signiﬁcantly different metric. similar method reduces intra-class variations increases inter-class separations achieve improved data splitting. however adopt matrix nuclear norm criterion learn transformation appropriate data expected subspaces. shown later method signiﬁcantly outperforms method well state-of-the-art learners classiﬁcation forests. learned transformations help classiﬁcation task well e.g. subspace based methods classiﬁcation forest ensemble binary classiﬁcation trees tree consists hierarchically connected split nodes leaf nodes. split node corresponds weak learner evaluates arriving data point sends left right child based weak learner binary outputs. leaf node stores statistics data points arrived during training. testing classiﬁcation tree returns class posterior probability test sample forest output often deﬁned average tree posteriors. section introduce transformation learning split node dramatically improve class separation capability weak learner. learned transformation virtually computationally free testing time. learning transformation learners consider two-class data points {yi}n data point low-dimensional subspaces data arranged columns assume class labels known beforehand training purposes. denote points classes respectively points arranged denotes concatenation ||·||∗ denotes matrix nuclear norm i.e. singular values matrix. nuclear norm convex envelop rank function unit ball matrices nuclear norm optimized efﬁciently often adopted best convex approximation rank function literature rank optimization normalization condition ||t|| prevents trivial solution however effects different normalizations interesting subject future research. throughout paper keep particular form normalization already proven lead excellent results. shown later linear transformation restores lowrank structure data class time maximizes subspace angles classes. reduce intra-class variation introduce inter-class separations improve class separation capability weak learner. fundamental factor affects performance weak learners classiﬁcation tree separation between different class subspaces. important notion quantify separation subspaces smallest principal angle deﬁned note show next learned transformation using objective function maximizes angle subspaces different classes leading improved data splitting tree node. start presenting basic norm relationships matrices corresponding concatenations. theorem matrices dimensions concatenation figure learning transformation using denote angle subspaces indicated assign subspaces different classes using transform subspaces respectively. observe learned transformation increases inter-class subspace angle towards maximum proposed objective function reaches minimum column spaces classes orthogonal applying learned transformation equivalently reaches minimum angle subspaces classes maximized transformation i.e. smallest principal angle subspaces equals rank function objective function reaches minimum subspaces disjoint necessarily maximally distant. replace nuclear norm induced -norm norm frobenius norm shown appendix objective function minimized trivial solution prevented normalization condition ||t|| thus adopt nuclear norm major advantages favorable rank function matrix norms nuclear norm best convex approximation rank function helps reduce variation within classes objective function general optimized distance subspaces different classes maximized transformation helps introduce separations classes. illustrate properties mentioned learned transformation using synthetic examples fig. adopt simple gradient descent optimization method search transformation matrix minimizes shown fig. learned transformation increases inter-class reduces intrasubspace angle towards maximum class subspace angle towards minimum training i-th split node denote arriving training samples classes present node randomly divide classes categories. step purposely introduce node randomness avoid duplicated trees discussed later. learn transformation matrix ustiy− represent subspaces tiy+ respectively. weak learner model i-th split node deﬁned training testing i-th split node arriving sample uses feature assigned various techniques available perform implementation obtain evaluation. using k-svd method denote transformation learner −dt. †tiy| involves matrix multiplication computational complexity testing time. given data point paper considered square linear transformation size note that learn linear transformation size enable dimension reduction along transformation handle high-dimensional data. split node categories obtain training sets learn transformation optimized two-class problem. randomly class dividing strategy reduces multi-class problem two-class problem node transformation learning; furthermore introduces node randomness avoid generating duplicated trees. note non-convex employed gradient descent method converges local minimum. initializing transformation different random matrices might lead different local minimum solutions. identity matrix initialization paper leads excellent performance however understanding node randomness introduced adopting different initializations subject future research. section presents experimental evaluations using public datasets mnist handwritten digit dataset extended yaleb face dataset -scenes natural scene dataset. mnist dataset consists -bit grayscale handwritten digit images examples class. extended yaleb face dataset contains subjects near frontal pose lighting conditions images resized mnist extended yaleb datasets gives -dimensional feature. -scenes dataset contains images falling natural scene categories categories include images living rooms kitchens streets industrials etc. also present results data kinect datatset ﬁrst compare many learners tree context accuracy testing time; compare learners common random forests. training phase introduce randomness forests combination random training sampling randomized node optimization. train classiﬁcation tree different randomly selected training set. discussed reduces possible overﬁtting improves generalization classiﬁcation forests also signiﬁcantly reducing training time. randomized node optimization achieved randomly dividing classes arriving construct classiﬁcation trees extended yaleb face dataset compare different learners. split dataset halves randomly selecting lighting conditions training half testing. fig. illustrates proposed transformation learner model classiﬁcation tree constructed faces subjects. third column shows transformation learners split node enforce separation randomly selected categories clearly demonstrates data class concentrated different classes figure transformation learners classiﬁcation tree constructed faces subjects. root split node shown ﬁrst child nodes rows. ﬁrst column denotes training samples original subspaces different classes different colors. visualization data plotted dimension reduced using laplacian eigenmaps shown second column randomly divide arriving classes categories learn discriminative transformation using transformed samples shown third column clearly demonstrating data class concentrated different classes separated. fourth column shows ﬁrst dimension transformed samples third column. table construct classiﬁcation trees maximum depth using different learners reference purpose also include performance several subspace learning methods provide state-of-the-art classiﬁcation accuracies dataset. using single classiﬁcation tree proposed transformation learner already signiﬁcantly outperforms popular weak learners decision stump conic section trees used observe proposed learner also outperforms complex split functions lda. identity learner denotes proposed framework replacing learned transformation identity matrix. using single tree proposed approach already outperforms state-of-the-art results reported dataset. shown later randomness introduced performance general increases employing trees. learner higher complexity compared weak learners like decision stump performance random forests judged accuracy test time. increasing number trees increases accuracy maximum tree depth typically speciﬁed random forests limit size tree different algorithms like grow tree relying termination criterion. tree depth paper maximum tree depth. avoid under/over-ﬁtting choose maximum tree depth validation process. also implement additional termination criteria prevent training branch e.g. number samples arriving non-tree based methods d-ksvd lc-ksvd classiﬁcation trees decision stump decision stump conic section conic section identity learner transformation learner cost increased test time shown table learner exhibits similar test time weaker learners signiﬁcantly improved accuracy. increasing number trees learners approach accuracy cost orders magnitude test time. thus fact orders magnitude less trees learned matrix outperforms standard random forests illustrates importance proposed general transform learning framework. evaluate effect random training sampling using mnist dataset. mnist dataset training examples test examples. train classiﬁcation trees depth using randomly selected training samples shown fig. classiﬁcation accuracy increases increasing number trees fig. illustrates detail proposed transformation learner model trees. discussed increasing number trees increases accuracy cost increased test time. though reporting better accuracy hundreds trees option trees sufﬁcient illustrate trade-off accuracy performance. using -scenes dataset fig. evaluate effect randomness introduced randomly dividing classes arriving split node categories. randomly images class training used remaining data testing. train classiﬁcation trees depth using training samples. shown fig. classiﬁcation accuracy increases increasing number trees notice that trees accuracy already comparable state-of-the-art results reported dataset shown table general expect performance increases employing trees. figure transformation-based learners classiﬁcation tree constructed mnist dataset. root split node shown ﬁrst child nodes rows. ﬁrst column denotes training samples original subspaces different classes different colors. visualization data plotted dimension reduced using laplacian eigenmaps shown second column randomly divide arriving classes categories learn discriminative transformation using transformed samples shown third column clearly demonstrating data class concentrated different classes separated. fourth column shows ﬁrst dimension transformed samples third column. mization criteria learn transformation split node reduces variations/noises within classes increases separations classes. ﬁnal classiﬁcation results combines multiple random trees. thereby expect proposed framework robust noise. demonstrated effectiveness proposed learner classiﬁcation forests provided theoretical support experimental results reported diverse datasets. image. adopt kinect datatset provided pairs resolution depth body part images rendered mocap dataset. body parts background class represented unique color identiﬁers body part image. experiment testing poses dataset. ﬁrst poses training remaining poses testing. training sample pixels body part pose produce data points depth image. pixel represented using depth difference neighbors radius respectively forming -dim descriptor. train classiﬁcation trees depth using randomly selected training samples. shown fig. classiﬁcation accuracy increases increasing number trees fig. shows example input depth image groud truth body parts prediction using proposed method. references aharon elad bruckstein k-svd algorithm designing overcomplete dictionaries sparse representation. ieee trans. signal processing nov. matrices obtained concatenating matrices achieve minimum computing nuclear norm necessarily ones achieve corresponding minimum nuclear norm computation concatenation matrix easy show jiang davis learning discriminative dictionary sparse coding label consistent proc. ieee computer society conf. k-svd. computer vision patt. recn. colorado springs kuybeda frank bartesaghi borgnia subramaniam sapiro collaborative framework alignment classiﬁcation heterogeneous subvolumes cryo-electron tomography. journal structural biology lazebnik schmid ponce beyond bags features spatial pyramid matching recognizing natural scene categories. proc. ieee computer society conf. computer vision patt. recn. moosmann triggs jurie fast discriminative visual codebooks using randomized clustering forests. advances neural information processing systems peng ganesh wright rasl robust alignment sparse low-rank deproc. composition linearly correlated images. ieee computer society conf. computer vision patt. recn. francisco shen uniﬁed approach salient object detection rank matrix recovery. proc. ieee computer society conf. computer vision patt. recn. rhode island shotton girshick fitzgibbon sharp cook finocchio moore kohli criminisi kipman blake efﬁcient human pose estimation single depth images. ieee trans. patt. anal. mach. intell. yang gong huang linear spatial pyramid matching using sparse coding image classiﬁcation. proc. ieee computer society conf. computer vision patt. recn.", "year": 2013}