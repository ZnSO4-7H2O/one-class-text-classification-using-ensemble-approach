{"title": "Reinforcement Learning from Imperfect Demonstrations", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Robust real-world learning should benefit from both demonstrations and interactions with the environment. Current approaches to learning from demonstration and reward perform supervised learning on expert demonstration data and use reinforcement learning to further improve performance based on the reward received from the environment. These tasks have divergent losses which are difficult to jointly optimize and such methods can be very sensitive to noisy demonstrations. We propose a unified reinforcement learning algorithm, Normalized Actor-Critic (NAC), that effectively normalizes the Q-function, reducing the Q-values of actions unseen in the demonstration data. NAC learns an initial policy network from demonstrations and refines the policy in the environment, surpassing the demonstrator's performance. Crucially, both learning from demonstration and interactive refinement use the same objective, unlike prior approaches that combine distinct supervised and reinforcement losses. This makes NAC robust to suboptimal demonstration data since the method is not forced to mimic all of the examples in the dataset. We show that our unified reinforcement learning algorithm can learn robustly and outperform existing baselines when evaluated on several realistic driving games.", "text": "robust real-world learning beneﬁt demonstrations interactions environment. current approaches learning demonstration reward perform supervised learning expert demonstration data reinforcement learning improve performance based reward received environment. tasks divergent losses difﬁcult jointly optimize methods sensitive noisy demonstrations. propose uniﬁed reinforcement learning algorithm normalized actor-critic effectively normalizes q-function reducing q-values actions unseen demonstration data. learns initial policy network demonstrations reﬁnes policy environment surpassing demonstrator’s performance. crucially learning demonstration interactive reﬁnement objective unlike prior approaches combine distinct supervised reinforcement losses. makes robust suboptimal demonstration data since method forced mimic examples dataset. show uniﬁed reinforcement learning algorithm learn robustly outperform existing baselines evaluated several realistic driving games. deep reinforcement learning achieved signiﬁcant success many complex sequential decision-making problems. however algorithms usually require large amount interactions environment reach good performance initial performance *equal contribution department electrical engineering computer science berkeley department electrical engineering tsinghua university beijing china. correspondence yang <ygeecs.berkeley.edu> huazhe <huazhe xueecs.berkeley.edu>. nearly random clearly suboptimal often rather dangerous real-world settings autonomous driving. learning demonstration well-known alternative typically leverage reward presumes relatively small-scale noise-free demonstrations. develop robust algorithm learn value policy functions state action reward signals either come imperfect demonstration data environment. recent efforts toward policy learning suffer suboptimal initial performance generally leverage initial phase supervised learning and/or auxiliary task learning. several previous efforts shown demonstrations speed mimicking expert data temporal difference regularizer gradient-free optimization methods presume near-optimal demonstrations. obtained better initialization auxiliary task losses self-supervised manner; policy performance still initially random approaches. simple combination several distinct losses learn demonstrations; however appealing single principled loss applicable learning demonstration environment. approach normalized actor-critic uses uniﬁed loss function process off-line demonstration data on-line experience based underlying maximum entropy reinforcement learning framework approach enables robust learning corrupted demonstrations contains assumption optimality data required. normalized formulation soft q-learning gradient enables method also regarded variant policy gradient. evaluate approach minecraft game well realistic simulated environments torcs grand theft auto either discrete states tabular functions image input functions approximated neural networks. experimental results outperform previous approaches driving tasks modest amount demonstrations tolerating significant noise demonstrations method utilizes policy value functions evaluate given state-action value function optimal policy shows optimal state value function optimal policy could expressed bootstrapped q-value estimate obtained γvq). here reward received environment computed equation also derive policy gradient includes gradient form best knowledge ﬁrst propose uniﬁed objective capable learning demonstrations environments outperforms methods including ones explicit supervised imitation loss. reinforcement learning problem consider deﬁned markov decision process speciﬁcally characterized tuple sartγ states actions reward function transition function reward discount factor. agent interacts environment taking action given state receiving reward transiting next state. maximum entropy policy learning uses entropy augmented reward. optimal policy optimize discounted future rewards also maximize discounted future entropy action distribution weighting term balance importance entropy. unlike previous attempts adds entropy term single time step maximum entropy policy learning maximizes discounted future entropy whole trajectory. maximum entropy reinforcement learning many given demonstrations contains corresponding environment agent perform appropriate actions starts interaction continue improve. although number off-policy algorithms could principle used learn directly off-policy policy demonstrations in-environment transitions. method derived soft policy gradient objective function parametrization. speciﬁcally take gradient steps maximize future reward objective parametrize terms derived appendix updates actor critic ∇θjp difference term comparing nac’s actor update term soft update emphasize normalization effect term avoids pushing q-values actions demonstrated. mechanism explained section expectations taken respect demonstration transition samples behavioral policy proper policy gradient algorithm employ importance sampling correct mismatch. speciﬁc estimating e∼πq estimate constant prevents importance ratio large. although importance weights needed formalize method proper policy gradient algorithm empirical evaluation inclusion weights consistently reduces performance method. found omitting weights results better ﬁnal performance even training entirely demonstration data. reason ﬁnal algorithm importance sampling. summarize proposed method algorithm method uses samples demonstrations replay buffer rather restricting samples policy standard actor-critic methods. similar utilize target network compute stabilizes training process. parameters rapid network parameters target network demonstrations collected human trained policy network target network update frequency replay buffer number steps train demonstrations step ...} demonstration data standard methods suffer extremely poor performance trained entirely demonstration data. happen demonstration strongly biased sample environment transitions violates assumptions many off-policy methods. although closely related off-policy learning learning demonstrations different problems. section show q-learning completely fails demonstration data. intuition behind problem q-function trained good data understand action taken appropriate assign high q-value necessarily assign q-value alternative actions. framework soft optimality provides natural mechanism mitigate problem normalizing q-function actions. approach normalized actor-critic utilizes soft policy gradient formulations described section obtain q-function gradient reduces q-values actions observed along demonstrations. words without data indicate otherwise follow demonstrations. method well-deﬁned algorithm without auxiliary supervised loss hence able learn without bias face low-quality demonstration data. ﬁrst describe algorithm discuss performs well trained demonstrations. provide intuitive analysis section explain method learn demonstrations reinforcement learning methods q-learning cannot. states actions demonstrations generally higher q-values states. q-learning push values sampled state however values actions observed q-function knowing whether action good whether actions state good demonstrated action necessarily higher q-value actions demonstrated state. comparing actor update method soft q-learning update method includes extra term gradient −∇θvq. term falls naturally derive update policy gradient algorithm rather q-learning algorithm. intuitively term decrease increasing vice versa since −∇θvq different exp/α) decreasing prevent increasing actions demonstrations. aforementioned normalization effect emerges extra ∇θvq term. besides normalizing behaviors also less sensitive noisy demonstrations. since algorithm naturally resistant poor behaviors. could also similar analysis above. negative reward demonstrations tends decrease tends increase hence normalizing behavior reverse direction. moreover provides single principled approach learn demonstrations environments. avoids imitation learning. therefore besides natural robustness imperfect demonstrations uniﬁed approach comparing methods. maximum entropy reinforcement learning explored number prior works including several recent works extend deep reinforcement learning setting however works deal learning demonstration settings. propose maximum entropy methods learn environments. prior work studies learning demonstration task maximum entropy framefigure minecraft environment. learn policy moves agent goal. possible paths shown shorter optimal longer sub-optimal text details environment comparison dqfd environment work. unlike method loss derived objective similar bellman error method derived policy gradient. instead minimizing bellman errors policy gradient directly optimizes future accumulated reward. shown section method large performance advantage compared different objective function. method admits uniﬁed objective demonstrations environments also performs better alternative methods dqfd best knowledge proposed method ﬁrst uniﬁed method across demonstrations environments outperforms methods including ones explicit supervised imitation loss dqfd. prior learning demonstration efforts assume demonstrations perfect i.e. ultimate goal copy behaviors demonstrations. imitation learning approaches examples including extensions dagger proposed expert loop improves agent’s performance. recently explore adversarial paradigm behavior cloning method. another popular paradigm inverse reinforcement learning learns reward model explains demonstrations optimal behavior. instead assuming demonstrations perfect pure method allows imperfect demonstrations. method learns part demonstrations good part unlike methods simply imitate demonstrated behaviors. follow reinforcement learning expert demonstrations framework rewards actions available demonstrations. extra reward demonstrations allows method aware poor behaviors demonstrations. dqfd recent method also uses rewards demonstrations. combines imitation hinge loss q-learning loss order learn demonstrations transfer environments smoothly. imitation loss dqfd sensitive noisy demonstrations show experiment section. tempting apply various off-policy methods problem learning demonstration policy gradient variants q-learning retrace however emphasize off-policy learning learning demonstration different problems. off-policy methods convergence relies assumption visiting pair inﬁnitely many times. learning demonstration setting samples highly biased off-policy method fail learn anything demonstrations explained q-learning case section experiments address several questions beneﬁt demonstrations rewards? robust ill-behaved demonstrations? learn meaningful behaviors limited amount demonstrations? compare algorithm dqfd shown learn efﬁciently demonstrations preserve performance acting environment. methods include supervised behavioral cloning method q-learning soft q-learning version method importance sampling weighting q-learning soft q-learning without demonstrations. evaluate result grid world minecraft well realistic simulated environments torcs grand theft auto shown figure minecraft minecraft customized grid world environment. shown figure agent starts left would like reach ﬁnal goal agent walk green grass blue water ends episode. input agent current location. step agent move down left right. gets reward reaching goal otherwise. details please refer openai frozen-lake environment torcs torcs open-source racing game used widely experimental environment driving. goal agent drive fast possible track avoiding crashes. oval two-lane racing venue experiments. input agent gray scale image. agent controls vehicle step chooses actions cartesian product {left no-op right} no-op down}. design dense driving reward function encourages follow lane avoid collision obstacles. grand theft auto action-adventure video game goals similar part torcs game diverse realistic surrounding environment including presence vehicles buildings bridges. agent observes images environment. chooses possible actions {left-up rightup left no-op right down} reward function torcs. reward ×speed] damage damage indicator function whether vehicle damaged current state. lane ratio ratio distance lane center lane width. angle vehicle heading direction road direction. figure performances torcs game. x-axis shows training iterations. y-axis shows average total rewards. solid lines average values random seeds. shaded regions correspond standard deviation. left ﬁgure shows performance agent learn demonstrations right shows performance agent interact environments learning demonstrations. method consistently outperforms methods cases. dqfd method proposed learning demonstration phase dqfd combines hinge loss temporal difference loss. ﬁnetuning-in-environment phase dqfd combines hinge loss demonstrations loss demonstrations policy-generated data. alleviate over-ﬁtting issues also include weight decay following original paper. q-learning classic method ﬁrst train demonstrations replay buffer ﬁnetune environment regular q-learning. similar dqfd constant exploration ratio ﬁnetuning phase preserve performance obtained demonstrations. also train scratch baseline environment without demonstration. soft q-learning similar q-learning method entropy regularized reward. method proposed also include soft q-learning trained without demonstration another baseline. behavior cloning q-learning naive combining cross-entropy loss q-learning. first perform behavior cloning cross-entropy loss demonstrations. treat logit activations prior softmax layer initialization function ﬁnetune regular q-learning environment. normalized actor-critic importance sampling method importance sampling weighting term mentioned importance weighting term used correct action distribution mismatch demonstration current policy. understand basic properties proposed method design minecraft environment. experiment state simply location agent. tabular function. settings hope reveal differences algorithm algorithms incorporate supervised loss. shown figure paths reach goal. terms discounted reward shorter path favorable. make problem interesting provide longer suboptimal path demonstrations. found learning demonstration phase dqfd learned suboptimal path since methods access environment could possibly ﬁgure optimal path. methods ﬁnetune policies environment succeeds ﬁnding optimal path dqfd stucks suboptimal one. dqfd imitation loss thus preventing deviating original solution. compare method methods transitions. demonstrations collected trained q-learning expert policy. execute policy environment collect demonstrations. avoid deterministic executions expert policy sample action randomly probability on-demonstration in-environment performance dqfd methods gta. vertical line separates learning demonstration phase ﬁnetuning environment phase. method consistently outperforms dqfd phases. performances torcs game human demonstrations. dqfd performs well beginning overﬁts end. behavior cloning method much worse dqfd. method performs best convergence. ﬁgures performances demonstrations inside environments. show method performs better methods demonstrations. start ﬁnetuning performance method continues increase reaches peak performance faster methods. dqfd similar behavior lower performance. behavior cloning learns well demonstrations signiﬁcant performance drop interacting environments. methods ultimately learn interacting environment method dqfd start relatively high performance. empirically found importance weighted method perform well nac. reason might decrease gradient bias offset sufﬁciently increase gradient variance. without demonstration data q-learning soft q-learning suffer performance initial interactions environment. original pcl-r method fails learn even trained scratch environments. improved method able learn demonstrations learn environment. also test method challenging environment visual input game logic complex. limit environment execution speed compare method dqfd. shown fig. method outperforms dqfd demonstrations inside environment. many practical problems autonomous driving might large number human demonstrations demonstration available trained agent all. contrast scripted agent humans usually perform actions diversely multiple individuals single individual many learning demonstration methods study challenging case study different methods perform diverse demonstrations. collect human demonstrations asked non-expert human players play torcs hours each. human players control game combination four arrow keys rate trained agent. total collected around transitions. among them transitions used validation monitor bellman error. comparing data collected trained agent data diverse quality demonstrations improves naturally players familiar game. fig. observe behavior cloning method performs much worse dqfd. dqfd initially better method later surpassed method quickly might caused supervised hinge loss harmful demonstrations suboptimal. similar policy generated demonstrations case hard q-learning soft q-learning perform well. real world collected demonstrations might optimal. human demonstrations already shown imperfect demonstrations could large effect performance. study phenomenon principled manner collect versions demonstrations varying degrees noise. collecting demonstrations trained agent corrupt certain percentage demonstrations choosing nonoptimal actions data corruption process conducted interacting environment; therefore error affect collection followfigure left learning imperfect data imperfectness method clone suboptimal behaviors thus outperforms dqfd behavior cloning. right learning limit amount demonstrations. even minutes experience method could still learn policy comparable supervised learning methods. results available appendix including imperfect data ablations well data amount studies. steps. sets percentage imperfect data. left fig. show method performs well compared dqfd behavior cloning methods. supervised behavior cloning method heavily inﬂuenced imperfect demonstrations. dqfd also heavily affected severely behavior cloning. robust imitate suboptimal behaviors. results percentage imperfect data similar available appendix. section show comparisons among method methods different amounts demonstration data. trained agent collect three sets demonstrations include transitions each. experiments algorithm performs well amount data large comparable supervised methods even limited amount data. fig. show extremely limited amounts demonstration data method performs supervised methods. appendix show results transitions method outperforms baselines large margin transitions. summary method learn small amounts demonstration data dominates terms performance sufﬁcient amount data. experiments adopt natural reward maximizes speed along lane minimizes speed perpendicular lane penalizes agent hits anything. however informative rewards available many conditions. section study whether method robust less informative reward. change reward figure similar figure compare methods learning demonstrations except different reward speed. method still performs best. function square speed agent irrespective speed’s direction. reward encourages agent drive fast however difﬁcult work because agent learn driving off-road hitting obstacles reduce future speed. also hard because speed large numerical range. figure shows method still performs best convergence dqfd suffers severe performance degeneration. proposed normalized actor-critic algorithm reinforcement learning demonstrations. algorithm provides uniﬁed approach learning reward demonstrations robust potentially suboptimal demonstration data. agent ﬁne-tuned rewards training demonstrations simply continuing perform algorithm on-policy data. algorithm preserves improves behaviors learned demonstrations receiving reward interaction environment. references abbeel pieter andrew apprenticeship learning inverse reinforcement learning. proceedings twenty-ﬁrst international conference machine learning bojarski mariusz testa davide dworakowski daniel firner bernhard flepp beat goyal prasoon jackel lawrence monfort mathew muller zhang jiakai learning self-driving cars. arxiv preprint arxiv. ebrahimi sayna rohrbach anna darrell trevor. gradient-free policy architecture search adaptation. levine sergey vanhoucke vincent goldberg proceedings annual conference robot learning volume proceedings machine learning research pmlr http//proceedings.mlr.press/ v/ebrahimia.html. shixiang lillicrap timothy ghahramani zoubin turner richard levine sergey. q-prop sampleefﬁcient policy gradient off-policy critic. arxiv preprint arxiv. shixiang lillicrap timothy ghahramani zoubin turner richard sch¨olkopf bernhard levine sergey. interpolated policy gradient merging on-policy off-policy gradient estimation deep reinforcement learning. arxiv preprint arxiv. haarnoja tuomas tang haoran abbeel pieter levine sergey. reinforcement learning deep energybased policies. precup doina whye proceedings international conference machine learning volume proceedings machine learning research international convention centre sydney australia pmlr. http//proceedings.mlr. press/v/haarnojaa.html. hester todd vecerik matej pietquin olivier lanctot marc schaul piot bilal sendonaris andrew dulac-arnold gabriel osband agapiou john learning demonstrations real world reinforcement learning. arxiv preprint arxiv. jaderberg mnih volodymyr czarnecki wojciech marian schaul leibo joel silver david kavukcuoglu koray. reinforcement learning unsupervised auxiliary tasks. arxiv preprint arxiv. mnih volodymyr kavukcuoglu koray silver david rusu andrei veness joel bellemare marc graves alex riedmiller martin fidjeland andreas ostrovski georg human-level control deep reinforcement learning. nature munos r´emi stepleton harutyunyan anna bellemare marc. safe efﬁcient off-policy reinforcement learning. advances neural information processing systems piot bilal geist matthieu pietquin olivier. boosted bellman residual minimization handling expert demonjoint european conference machine strations. learning knowledge discovery databases springer ross st´ephane gordon geoffrey bagnell drew. reduction imitation learning structured prediction international conferno-regret online learning. ence artiﬁcial intelligence statistics wang ziyu bapst victor heess nicolas mnih volodymyr munos remi kavukcuoglu koray freitas nando. sample efﬁcient actor-critic experience replay. arxiv preprint arxiv. usually actor-critic method parametrizes neural network heads. section explore alternative parametrization qparametrization. instead outputting directly neural network computes parametrize based specifying ﬁxed mathematical transform note q-parametrization propose seen speciﬁc design network architecture. instead allowing output arbitrary values restrict network output pairs satisfy relationship. extra restriction harm network’s ability learn since relationship satisﬁed optimal solution. based q-parametrization derive update actor. note assume behavioral policy sample step trajectory thus dropping subscript goal maximize expected future reward thus taking gradient network architecture architecture parametrize parametrization also output based hyper-parameters replay buffer capacity million steps update target network every steps. initially learning rate linearly annealed ﬁrst training process kept constant gradients clipped reduce training variance. reward discount factor concatenate recent frames input neural network. methods entropy regularizer following truncate importance sampling weighting factor figure results varying amount demonstrations. left right ﬁgures show transitions respectively. method achieves superior performance large amount demonstrations comparable supervise methods smaller amount demonstrations. figure results introducing imperfect demonstrations. left ﬁgure shows imperfect actions right shows case method highly robust noisy demonstrations.", "year": 2018}