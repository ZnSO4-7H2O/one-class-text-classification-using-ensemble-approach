{"title": "Q($λ$) with Off-Policy Corrections", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "We propose and analyze an alternate approach to off-policy multi-step temporal difference learning, in which off-policy returns are corrected with the current Q-function in terms of rewards, rather than with the target policy in terms of transition probabilities. We prove that such approximate corrections are sufficient for off-policy convergence both in policy evaluation and control, provided certain conditions. These conditions relate the distance between the target and behavior policies, the eligibility trace parameter and the discount factor, and formalize an underlying tradeoff in off-policy TD($\\lambda$). We illustrate this theoretical relationship empirically on a continuous-state control task.", "text": "abstract. propose analyze alternate approach oﬀ-policy multi-step temporal diﬀerence learning oﬀ-policy returns corrected current q-function terms rewards rather target policy terms transition probabilities. prove approximate corrections suﬃcient oﬀ-policy convergence policy evaluation control provided certain conditions. conditions relate distance target behavior policies eligibility trace parameter discount factor formalize underlying tradeoﬀ oﬀ-policy illustrate theoretical relationship empirically continuous-state control task. reinforcement learning learning oﬀ-policy samples generated behavior policy used learn distinct target policy. usual approach oﬀ-policy learning disregard altogether discard transitions whose target policy probabilities low. example watkins’s cuts trajectory backup soon non-greedy action encountered. similarly policy evaluation importance sampling methods weight returns according mismatch target behavior probabilities corresponding actions. approach treats transitions conservatively hence unnecessarily terminate backups introduce large amount variance. many oﬀ-policy methods particular monte carlo kind option judge oﬀ-policy actions probability sense. however temporal diﬀerence methods maintain approximation value function along eligiblity traces providing continuous link one-step monte carlo approaches. value function assesses actions terms following expected cumulative reward thus provides directly correct immediate rewards rather transitions. show paper approximate corrections suﬃcient oﬀ-policy convergence subject tradeoﬀ condition eligibility trace parameter distance target behavior policies. extremes tradeoﬀ one-step q-learning on-policy learning. formalizing continuum tradeoﬀ main insights paper. particular propose oﬀ-policy return operator augments return correction term based current approximation qfunction. formalize three algorithms stemming operator oﬀ-policy special case on-policy policy evaluation oﬀ-policy control. policy evaluation onoﬀ-policy novel closely related several existing algorithms family. section discusses detail. prove convergence subject tradeoﬀ def= maxx measure dissimilarity oﬀ-policy-ness inherent maximum allowed backup length value taking value guarantees convergence withinvolving policy probabilities. desirable instabilities variance introduced likelihood ratio products importance sampling approach eligiblity trace oﬀ-policy actions. sutton barto mention variation call naive analyze algorithm ﬁrst time prove convergence small values although tradeoﬀ policy evaluation conservatively small values control. illustrate tradeoﬀ emerge empirically bicycle domain control setting. finally conclude placing algorithms context within existing work sion process composed ﬁnite state action spaces discount factor transition function mapping distribution reward function policy maps state distribution q-function mapping given policy deﬁne operator q-functions write greedy def= {π|π maxa denote greedy policies w.r.t. thus greedy. operators guaranteed converge respective ﬁxed points given sample experience sarsa updates q-function estimate iteration follows td-error sequence nonnegative stepsizes. need consider short experiences sample trajectories accordingly apply repeatedly. particularly ﬂexible weighted n-step operators taking yields usual bellman operator removes recursion approximate qfunction restores monte carlo sense. well-known trades bias bootstrapping approximate q-function variance using sampled multi-step return intermediate values usually performing best practice λ-operator eﬃciently implemented online setting mechanism called eligibility traces. section fact corresponds number online algorithms subtly diﬀerent sarsa canonical instance. finally make important distinction target policy wish estimate behavior policy actions generated. learning said on-policy otherwise oﬀ-policy. write denote expectations sequences assume conditioning wherever appropriate. throughout write supremum norm. reward trajectory augmented oﬀ-policy correction deﬁne diﬀerence expected q-value q-value taken action. thus much reward corrected determined approximation target policy probabilities. notice actions similarly valued correction little eﬀect learning roughly on-policy q-function converged correct estimates correction takes immediate reward expected reward respect exactly. indeed later ﬁxed point behavior policy deﬁne n-step λ-versions usual consider problems oﬀ-policy policy evaluation oﬀ-policy control. problems given data generated sequence behavior policies k∈n. policy evaluation wish estimate ﬁxed target policy control wish estimate algorithm constructs sequence estimates trajectories sampled applying rπkµk theorem consider sequence q-functions computed according algorithm ﬁxed policies maxx π−µ. conditions required convergence have almost surely contractions invoking classical stochastic approximation convergence ﬁxed point focus contraction lemmas crux proofs outline sketch online convergence argument. discussion theorem states exists degree converges oﬀ-policy-ness tradeoﬀ oﬀ-policy learning algorithm policy evaluation. control case result theorem weaker holds values smaller notice threshold corresponds policy evaluation case able prove convergence left open problem now. main technical diﬃculty lies fact control greedy policy respect current change drastically step next changes incrementally current oﬀer good oﬀ-policy correction evaluate greedy policy. order circumvent problem want slowly changing target policies example could keep ﬁxed slowly increasing periods time. seen form optimistic policy iteration policy improvement steps alternate approximate policy evaluation steps another option would deﬁne conjecempirical average ture deﬁning changes slowly becomes increasingly greedy could extend tradeoﬀ theorem analysis begin verifying ﬁxed points control settings respectively. prove contractive properties operators always contraction converge ﬁxed point contraction particular choices contraction coeﬃcients depend distance policies. finally give proof sketch online convergence algorithm next consider case target policy greedy respect value estimate following lemma states possible select small nonzero still guarantee convergence. assumption requires trajectories ﬁnite w.p. satisﬁed proper behavior policies. equivalently require trajectories eventually reach zero-value absorbing state. proof closely follows proposition requires rewriting update suitable form verifying assumptions proposition stepsize sequence satisﬁes assumption prop. assumptions require variance noise term bounded residual converge zero shown identically corresponding results assumption assumption satisﬁed. finally assumption satisﬁed lemmas policy evaluation control cases respectively. conclude k)k∈n converges respective settings w.p. sequence bicycle domain. conﬁguration average trials. marks lowest value causes divergence. right. maximum non-diverging function left-hand shaded region corresponds hypothesized bound. parameter settings right-hand shaded region produce meaningful policies. divergence. value determined highest safe choice result divergence. figure illustrates marked decrease safe value increases. note left-hand supporting hypothesis true bound appears clear maximum safe value depends particular notice stops diverging exactly predicted bound. section place presented algorithms context existing work focusing particular action-value methods. usual trajectory generated following behavior policy i.e. time sarsa updates q-function follows sarsa on-policy algorithm converges value function behavior policy. diﬀerent algorithms arise instantiating diﬀerently. table provides full details text specify revealing components update. one-step update general q-learning generalization expected sarsa arbitrary policies. refer direct eligibility trace extensions algorithms formed equations general expected sarsa unfortunately oﬀ-policy setting general converge value function target policy stated following proposition. proposition stable point general π)−r ﬁxed point operator exactly policy evaluation algorithm speciﬁcally on-policy induced on-policy correction serve variance reduction term expected sarsa leave variance analysis target policy probability methods algorithms directly descend basic sarsa often learning oﬀ-policy requires special treatment. example typical oﬀ-policy technique importance sampling classical monte carlo method allows sample available distribution obtain samples desired reweighing samples likelihood ratio according distributions. updates ordinary per-decision algorithm policy evaluation made follows however oﬀ-policy perhaps related closest tree-backup algorithm also discussed precup one-step td-error algorithms back tree neither requires knowledge behavior policy important diﬀerence weighting updates. oﬀ-policy precaution weighs updates along trajectory cumulative target probability trajectory point weighting simpliﬁes convergence argument allowing converge without restrictions distance drawback case near on-policy-ness product probabilities cuts traces unnecessarily show paper plain td-learning converge oﬀ-policy special treatment subject tradeoﬀ condition condition applies onoﬀ-policy without modiﬁcations. ideal algorithm able automatically traces case extreme oﬀ-policy-ness reverting near on-policy. perhaps popular version watkins dayan oﬀ-policy truncates return bootstraps soon behavior policy takes non-greedy action described following update formulated algorithms family oﬀ-policy policy evaluation control. unlike traditional oﬀ-policy learning algorithms methods involve weighting returns policy probabilities right conditions converge correct ﬁxed points. policy evaluation convergence subject tradeoﬀ degree bootstrapping distance policies discount factor control determining existence non-trivial ε-dependent bound remains open problem. supported telling empirical results bicycle domain hypothesize bound policy", "year": 2016}