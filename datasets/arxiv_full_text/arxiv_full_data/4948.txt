{"title": "Metacontrol for Adaptive Imagination-Based Optimization", "tag": ["cs.LG", "cs.AI"], "abstract": "Many machine learning systems are built to solve the hardest examples of a particular task, which often makes them large and expensive to run---especially with respect to the easier examples, which might require much less computation. For an agent with a limited computational budget, this \"one-size-fits-all\" approach may result in the agent wasting valuable computation on easy examples, while not spending enough on hard examples. Rather than learning a single, fixed policy for solving all instances of a task, we introduce a metacontroller which learns to optimize a sequence of \"imagined\" internal simulations over predictive models of the world in order to construct a more informed, and more economical, solution. The metacontroller component is a model-free reinforcement learning agent, which decides both how many iterations of the optimization procedure to run, as well as which model to consult on each iteration. The models (which we call \"experts\") can be state transition models, action-value functions, or any other mechanism that provides information useful for solving the task, and can be learned on-policy or off-policy in parallel with the metacontroller. When the metacontroller, controller, and experts were trained with \"interaction networks\" (Battaglia et al., 2016) as expert models, our approach was able to solve a challenging decision-making problem under complex non-linear dynamics. The metacontroller learned to adapt the amount of computation it performed to the difficulty of the task, and learned how to choose which experts to consult by factoring in both their reliability and individual computational resource costs. This allowed the metacontroller to achieve a lower overall cost (task loss plus computational cost) than more traditional fixed policy approaches. These results demonstrate that our approach is a powerful framework for using...", "text": "many machine learning systems built solve hardest examples particular task often makes large expensive run—especially respect easier examples might require much less computation. agent limited computational budget one-size-ﬁts-all approach result agent wasting valuable computation easy examples spending enough hard examples. rather learning single ﬁxed policy solving instances task introduce metacontroller learns optimize sequence imagined internal simulations predictive models world order construct informed economical solution. metacontroller component model-free reinforcement learning agent decides many iterations optimization procedure well model consult iteration. models state transition models action-value functions mechanism provides information useful solving task learned on-policy off-policy parallel metacontroller. metacontroller controller experts trained interaction networks expert models approach able solve challenging decision-making problem complex non-linear dynamics. metacontroller learned adapt amount computation performed difﬁculty task learned choose experts consult factoring reliability individual computational resource costs. allowed metacontroller achieve lower overall cost traditional ﬁxed policy approaches. results demonstrate approach powerful framework using rich forward models efﬁcient model-based reinforcement learning. signiﬁcant recent advances deep reinforcement learning control efforts train network performs ﬁxed sequence computations. introduce alternative agent uses metacontroller choose which many computations perform. imagines consequences potential actions proposed actor module reﬁnes internally executing world. metacontroller adaptively decides expert models evaluate candidate actions time stop imagining act. learned experts state transition models action-value functions function relevant task vary accuracy computational costs. metacontroller’s learned policy exploit diversity pool experts trading costs reliability allowing automatically identify expert worthwhile. draw inspiration research cognitive science neuroscience studied people meta-level reasoning order control internal models allocation computational resources. evidence suggests humans rely rich generative models world planning control reasoning adapt amount computation perform model demands task trade multiple strategies varying quality imagination-based optimization approach related classic artiﬁcial intelligence research bounded-rational metareasoning formulates meta-level selecting computations perform computations known cost. also build classic work schmidhuber used controller recurrent neural network world model evaluate improve upon candidate controls online. recently andrychowicz used fully differentiable deep network learn perform gradient descent optimization tamar used convolutional neural network performing value iteration online deep learning setting. similar work fragkiadaki made visual imaginations action planning. work also related recent notions conditional computation adaptively modiﬁes network structure online adaptive computation time allows variable numbers internal pondering iterations optimize computational cost. work’s contribution framework learning optimize metacontroller manages adaptive imagination-based optimization loop. represents hybrid system model-free metacontroller constructs decisions using actor policy manage model-free model-based experts. experimental results demonstrate metacontroller ﬂexibly allocate computational resources case-by-case basis achieve greater performance rigid ﬁxed policy approaches using computation required difﬁcult task. consider class fully observed one-shot decision-making tasks performance objective control which given initial state minimizes loss function known future goal state result forward process performance loss utility executing control world related optimal solution follows optimizing performance consider iterative optimization procedure takes input returns approximation order minimize optimization procedure consists controller iteratively proposes controls expert evaluates good controls are. iteration controller takes input information history previously proposed controls evaluations returns proposed control aims improve previously proposed controls. expert takes proposed control provides information quality control call opinion. opinion added history passed back controller loop continues steps ﬁnal control proposed. standard optimization methods principled heuristics proposing controls. gradient descent example controls proposed adjusting direction gradient reward respect control. bayesian optimization controls proposed based selection criteria probability improvement meta-selection criterion choosing among figure metacontroller architecture task. components part metacontroller agent except scene world part agent’s environment. manager takes scene history determines action take denoted orange lines. controller takes scene history computes control denoted blue lines. orange line ending circle switch reﬂects fact manager’s action affects behavior switch routes controller’s control either expert world. outcome reward expert along history action control memory produces next history. history back controller next iteration order allow propose controls based already tried. scenes consisted number planets different masses well spaceship task apply force spaceship time step simulation resulting trajectory would spaceship target steps simulation. white ring bullseye corresponds performance loss black ring loss blue ring loss ring loss yellow center loss less. depicts easy -planet scene depicts difﬁcult -planet scene. several basic selection criteria hoffman shahriari rather choosing several controllers work learns single controller instead focuses selecting multiple experts cases known inexpensive compute thus optimization procedure sets however many real-world settings expensive non-stationary advantageous approximation quantity gives information iterations iteration costs computational resources. however traditional optimizers either ignore cost computation select number iterations using simple heuristics. balance cost computation performance loss overall effectiveness approaches subject skill preferences practitioners them. second expert used step optimization? experts accurate expensive compute terms time energy and/or money others crude cheap. moreover reliability experts known priori limiting effectiveness optimization procedure. metacontroller address issues jointly optimizing choices many steps take experts use. consider family optimizers controller vary expert evaluators ek}. assuming controller experts deterministic functions number iterations sequences experts exactly determine ﬁnal control performance loss means transformed performance optimization optimization minkn notation used emphasize control function optimizer associated computational cost also exactly determine τkn. total loss optimal solution deﬁned minnk optimizing difﬁcult recursive dependency history discrete choices mean differentiable. optimize recast problem objective jointly optimize task performance computational cost. shown figure metacontroller agent comprised controller pool experts manager memory manager meta-level policy actions indexed determine whether terminate optimization procedure perform another iteration optimization procedure expert. speciﬁcally iteration controller produces control based history controls experts evaluations. manager also relying history independently decides whether optimization procedure perform another iteration evaluate proposed control expert memory updates history concatenating previous history hn−. coming back notion imagination-based optimization suggest iterative optimization process analogous imagining happen actually executing action world. details appendix algorithmic illustration metacontroller agent algorithm appendix. also deﬁne special cases metacontroller baseline comparisons. iterative agent manager uses single expert. number iterations pre-set single reactive agent special case iterative agent number iterations ﬁxed implies proposed controls executed immediately world evaluated expert. algorithmic illustrations iterative reactive agents algorithms appendix. standard deep learning building blocks e.g. multi-layer perceptrons rnns etc. implement controller experts manager memory effective approximating complex functions gradient-based reinforcement learning approaches could used well. particular constructed implementation able make control decisions complex dynamical systems controlling movement spaceship though note approach limited physical reasoning tasks. used mean-squared error adam training optimizer. experts implemented experts mlps interaction networks well-suited predicting complex dynamical systems like experiments below. expert parameters i.e. trained either on-policy using outputs controller off-policy data pairs states controls future states reward outcomes. objective expert different depending expert outputs. example objective could controller memory implemented controller parameters i.e. implemented memory long short-term memory parameters memory embeds history ﬁxedlength vector i.e. θµ). controller memory trained jointly optimize however objective includes often unknown differentiable. overcame approximating differentiable critic analogous used policy gradient methods appendices details. manager implemented manager stochastic policy samples categorkn ical distribution whose weights produced parameters categorical). trained manager minimize using reinforce deep algorithms could used instead. appendix details. evaluate metacontroller agent measured ability learn solve class physicsbased tasks surprisingly challenging. episode consisted scene contained spaceship multiple planets spaceship’s goal rendezvous mothership near center system exactly time steps enough fuel thrusters once. planets static gravitational force exerted spacecraft induced complex non-linear dynamics motion steps. spacecraft’s action space continuous maximum magnitude represented instantaneous cartesian velocity vector imparted thrusters. details appendix trained reactive iterative metacontroller agents versions spaceship task involving different numbers planets. iterative agent trained take anywhere zero ponder steps. metacontroller allowed take maximum ponder steps. considered three different experts differentiable expert used predict ﬁnal location spaceship expert used interaction network predict full trajectory spaceship true simulation expert world model. conditions metacontroller could exactly expert others allowed select experts. experiments true simulation expert used backpropagate gradients controller memory. experiments expert used learned critic. experiments experts critic shared parameters. trained metacontroller range different ponder costs different experts. details training procedure available appendix figure shows performance test reactive iterative agents different numbers ponder steps. reactive agent performed poorly task especially task difﬁcult. planets dataset able achieve performance loss average contrast iterative agent true simulation expert performed much better reaching ceiling performance figure test performance reactive iterative agents. line corresponds performance iterative agent trained ﬁxed number ponder steps datasets; line color indicates dataset controller trained cases performance refers performance loss left expert struggles task limited expressivity still beneﬁts pondering. middle expert performs almost well true simulation expert even though perfect model. right true simulation expert quite well task especially multiple ponder steps. datasets planets achieving performance loss planets dataset. experts also improve reactive agent minimum performance loss planets dataset respectively. figure also highlights important choice expert using true simulation experts iterative agent performs well. expert however performance substantially diminished. despite poor performance expert still beneﬁt pondering even steps iterative agent outperforms reactive counterpart. however comparing reactive agent iterative agent somewhat unfair iterative agent parameters expert memory. however given tends also increase performance ponder steps clear pondering—even highly inaccurate model—can still lead better performance model-free reactive approach. though iterative agents achieve impressive results expend computation necessary. example planet conditions performances true simulation iterative agents received little performance beneﬁt pondering three steps four planet conditions required least eight steps performance converged. computational resources cost number steps concern cost important economical. metacontroller learns choose number pondering steps balance performance loss cost computation. figure shows true simulation expert metacontroller take fewer ponder steps increases tracking closely minimum iterative agent’s cost curve adaptive behavior emerges automatically manager’s learned policy avoids need perform hyperparameter search best number iterations given metacontroller simply choose average number ponder steps take episode actually tailors choice difﬁculty episode. figure shows number ponder steps metacontroller chooses episode depends episode’s difﬁculty measured episode’s loss reactive agent. difﬁcult episodes metacontroller tends take ponder steps indicated positive slopes best lines proportionality persists across different levels subplot. figure test performance metacontroller single expert planets dataset. column corresponds different experts. lines indicate performance iterative agents different numbers ponder steps. points indicate performance metacontroller point corresponding different value x-coordinate point average across number ponder steps y-coordinate average loss. show total cost rather performance task different colors show result different different lines showing cost iterative controller different values error bars indicate conﬁdence intervals. point corresponding curve means metacontroller able achieve better speed-accuracy trade-off achievable iterative agent. line colors increasing brightness correspond increasing values taken bottom show performance loss point corresponds different value fact points curve means metacontroller agent learns perform better iterative agent equivalent number ponder steps. ability adapt choice number ponder steps per-episode basis valuable allows metacontroller spend additional computation episodes require total costs true simulation metacontrollers’ lower best achievable costs corresponding iterative agents respectively across range values tested even beneﬁt using metacontroller computational resource costs. consider rightmost points figure show performance loss true simulation metacontrollers low. remarkably points still outperform best achievable iterative agents. suggests advantage stopping pondering good solution found generally demonstrates metacontroller’s learning process lead strategies superior available less ﬂexible agents. metacontroller expert poor average performance high variance planet condition restricted focus section metacontrollers true simulation experts behaved. mlp’s poor performance crucial however following section analyzes multipleexpert metacontroller manages experts vary greater reliability. allow manager additionally choose experts rather relying single expert similar pattern results terms number ponder steps additionally metacontroller successfully able identify reliable network consequently uses majority time except cases cost network extremely high relative cost network pattern results makes sense given good performance metacontroller expert compared poor performance metacontroller expert. manager generally rely expert simply reliable source information. however metacontroller difﬁculty ﬁnding optimal balance experts step-by-step basis addition second expert yield much improvement single-expert metacontroller different versions achieving lower loss best iterative controller. believe mixed performance metacontroller multiple experts partially entropy term used encourage manager’s policy non-deterministic particular high values optimal thing always execute immediately without pondering. however entropy term manager encourage non-deterministic policy therefore likely ponder should—and experts unreliable— even suboptimal terms total loss despite fact metacontroller multiple experts result substantial improvement uses single expert emphasize manager able identify reliable expert majority time. still able choose variable number steps according difﬁcult task this itself improvement traditional optimization methods would require expert hand-picked ahead time number steps determined heuristically. figure relationship number ponder steps per-episode difﬁculty metacontroller. subplot’s x-axis represents episode difﬁculty measured reactive controller’s loss. y-axis represents number ponder steps metacontroller took. points individual episodes line best regression line conﬁdence intervals. different subplots show different values case clear positive relationship difﬁculty task number ponder steps suggesting metacontroller learns spend time hard problems less time easier problems. bottom plot ﬁtted slope correlation coefﬁcient values along conﬁdence intervals brackets. figure test performance metacontroller multiple experts planets dataset. left average number total ponder steps different values single-expert metacontrollers fewer ponder steps taken cost high taken cost low. right fraction ponder steps taken expert relative expert. majority cases metacontroller favors using expert much reliable. exceptions cases cost expert much higher relative cost expert. paper presented approach adaptive imagination-based optimization neural networks. approach able ﬂexibly choose computations perform well many computations need performed approximately solving speed-accuracy trade-off depends difﬁculty task. approach learns rely whatever source information useful efﬁcient. additionally consulting experts on-the-ﬂy approach allows agents test actions ensure consequences disastrous actually executing them. experiments paper involve one-shot decision task approach lays foundation built upon support complex situations. example rather applying force ﬁrst time step could turn problem trajectory optimization continuous control asking controller produce sequence forces. case planning approach could potentially combined methods like monte carlo tree-search experts would akin several different rollout policies choose from controller would akin tree policy. mcts implementations rollouts ﬁxed amount time passed approach would allow manager adaptively choose number rollouts perform policies perform rollouts with. method could also used naturally augment existing model-free approaches online model-based optimization using model-free policy controller adding additional experts form state-transition models. interesting extension would compare metacontroller architecture na¨ıve model-based controller performs gradient-based optimization produce ﬁnal control. expect metacontroller architecture might require fewer model evaluations robust model inaccuracies compared gradient-based method method access full history proposed controls evaluations whereas traditional gradient-based methods not. although rely differentiable experts metacontroller architecture utilize gradient information experts. interesting extension work would pass gradient information manager controller would likely improve performance further especially complex situations discussed here. another possibility train experts inline controller metacontroller rather independently could allow learned functionality tightly integrated rest optimization loop expense generality ability repurposed uses. method directly inspired human cognition suggests make agents much ﬂexible adaptive currently decision making tasks described here well planning control settings broadly. would like thank matt hoffman andrea tacchetti erez nando freitas guillaume desjardins joseph modayil hubert soyer alex graves david reichert theo weber scholz dabney others deepmind team helpful discussions feedback. peter battaglia razvan pascanu matthew danilo jimenez rezende koray kavukcuoglu. interaction networks learning objects relations physics. advances neural information processing systems yoshua bengio. deep learning representations looking forward. arxiv. r´emi coulom. efﬁcient selectivity backup operators monte-carlo tree search. international conferkaterina fragkiadaki pulkit agrawal sergey levine jitendra malik. learning visual predictive models physics playing billiards. proceedings international conference learning representations http//arxiv.org/abs/.. gl¨ascher nathaniel peter dayan john o’doherty. states versus rewards dissociable neural prediction error signals underlying model-based model-free reinforcement learning. neuron alex graves. adaptive computation time recurrent neural networks. arxiv. jessica hamrick kevin smith thomas grifﬁths edward vul. think again? amount mental simulation tracks uncertainty outcome. proceedings annual conference cognitive science society falk lieder thomas grifﬁths. strategy selection rational metareasoning. revision. falk lieder dillon plunkett jessica hamrick stuart russell nicholas thomas grifﬁths. algorithm selection rational metareasoning model human strategy selection. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature stuart russell eric wefald. principles metareasoning. artiﬁcial intelligence j¨urgen schmidhuber. on-line algorithm dynamic reinforcement learning planning reactive envidavid silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature here give precise deﬁnitions metacontroller agent. described main text iterative reactive agents special cases metacontroller agent therefore discussed here. pool experts ek}. expert maps goal states input states actions opinions. opinions either states-only states rewards rewards-only expert corresponds evaluator optimization routine i.e. approximation forward process manager policy decides whether send proposed control world expert evaluation order minimize formulation based used metareasoning systems details corresponding given appendix memory function maps prior history well recent manager choice proposed control expert evaluation updated history made available manager controller subsequent iterations. history step recursively deﬁned tuple concatenation prior history recently proposed control expert evaluation expert identity represents empty initial history. similarly ﬁnite histories step implement manager metacontroller agent draw inspiration metareasoning literature formulate problem ﬁnite-horizon markov decision process decision whether perform another iteration optimization procedure execute control world. state space consists goal states external states internal histories action space contains discrete actions correspond execute ponder ponder refers performing iteration optimization procedure expert. approximate solution stochastic manager policy manager chooses actions proportional immediate reward taking action state plus expected future rewards. construction imposes trade-off accuracy resources incentivizing agent ponder longer accurate experts problem harder. figure training part network. subplot arrows depict gradients. dotted arrows indicate backward connections part forward pass. colored nodes indicate weights updated. backpropagation occurs full forward pass training controller memory backpropagation-through-time beginning critic ﬂowing controller memory relevant expert controller again training manager using reinforce training experts training critic. critic approximate model performance loss used backpropagate gradients controller memory. means critic either action-value function approximates directly model system dynamics composed known loss function goal future states l◦f. train critic using procedure experts trained good expert even used critic. shown figure trained controller memory using backpropagation time actor-critic architecture. speciﬁcally rather assuming known differentiable critic backpropagate using notation indicate summed gradients following pascanu since already produced manager treated constant produce unbiased estimate gradient. convenient allows training controller manager separately testing controller’s behavior arbitrary actions post-training. discussed main text used reinforce algorithm williams train manager potential issue however training controller manager simultaneously controller result high cost early training thus manager learn always choose execute action. discourage manager learning essentially deterministic policy included regularization term based entropy generated datasets containing scenes different number planets dataset consisted training scenes testing scenes. target scene always located origin scene always mass units. located distance units away target distance sampled uniformly random. planets mass units located distance units away target sampled uniformly random. spaceship mass units located distance units away target. planets always ﬁxed spaceship always started beginning episode zero velocity. force vector planet spaceship gravitational constant mass planet mass spaceship distance centers masses planet spaceship location planet location spaceship. simulated environment using euler method i.e. acceleration velocity position spaceship respectively; damping constant; control force applied spaceship; step size. note zero timesteps except ﬁrst. implementation controller used two-layer units. ﬁrst layer used relu activations second layer used multiplicative interaction similar oord found work better practice. implementation memory used single lstm layer size implementation manager used fully connected layers units each relu nonlinearities. constructed three different experts test various controllers. true simulation expert world model consisted simulation timesteps expert interaction network previously shown able learn predict n-body dynamics accurately simple systems. consists relational module object module. case relational module composed hidden layers nodes each outputting effects encodings size effects together relational model input used input object model contained single hidden layer nodes. object model outputs velocity spaceship trained predict velocity every timestep spaceship’s trajectory. expert predicted ﬁnal location spaceship architecture controller. discussed appendix used critic train controller memory. always used expert critic except case true simulation expert used case also used true simulation critic. weights initialized uniformly random iteration training consisted gradient updates minibatch size total training iterations. additionally used waterfall schedule learning rates training iterations loss decreasing would decay step size trained controller memory together using adam optimizer gradients clipped maximum global norm manager trained simultaneously using different learning rate controller memory. experts also trained simultaneously different learning rates. learning rates determined using grid search small number values given table iterative agent table metacontroller expert table metacontroller experts. iterative agent trained take ﬁxed number ponder steps ranging metacontrollers allowed take variable number ponder steps maximum metacontroller single expert trained manager using additional values spaced logarithmically metacontroller multiple experts trained manager grid pairs values expert could values spaced logarithmically cases entropy penalty metacontroller iterative agent. iterative agent interaction network true simulation experts convergence also reliable small numbers ponder steps. convergence somewhat less reliable larger numbers ponder steps. believe scenes larger number ponder steps necessary solve task iterative agent effectively remember best control took last ponder steps complicated difﬁcult task perform. iterative agent expert convergence variable especially task harder seen variable performance planets dataset figure believe agent poor convergence would reliable better agent. metacontroller single expert. metacontroller agent single expert converged reliably corresponding iterative agent mentioned previous paragraph iterative agent take steps actually necessary causing perform less well larger numbers ponder steps whereas metacontroller agent ﬂexibility stopping found good control. hand found metacontroller agent sometimes performed many ponder steps large values believe entropy term added reinforce loss. ponder cost high optimal thing behave deterministically always execute however entropy term encouraged policy nondeterministic. plan explore different training regimes future work alleviate problem example annealing entropy term zero course training. metacontroller multiple experts. metacontroller agent multiple experts somedifﬁcult train especially high ponder cost interaction network expert. example note proportion steps using expert decrease monotonically figure increasing cost expert. believe also unexpected result using entropy term cases optimal thing actually rely expert time entropy term encourages policy non-deterministic. future work explore difﬁculties using experts complement better references mart´ın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin tensorflow large-scale machine learning heterogeneous systems http//tensorflow.org/. software available tensorﬂow.org. peter battaglia razvan pascanu matthew danilo jimenez rezende koray kavukcuoglu. interaction networks learning objects relations physics. advances neural information processing systems planet planet planet planet planet planet planet planet planet planet planet planets planets planets planets planets planets planets planets planets planets planets three planets three planets three planets three planets three planets three planets three planets three planets three planets three planets three planets four planets four planets four planets four planets four planets four planets four planets four planets four planets four planets four planets planets planets planets planets planets planets planets planets planets planets planets table hyperparameter values iterative controller. refers learning rate controller memory αein refers learning rate expert αemlp refers learning rate expert. figure cost best iterative controller compared managed controller. point represents total cost best iterative agent particular value versus total cost achieved metacontroller trained value best iterative agent chosen computing cost different number ponder steps choosing whichever number ponder stpes yielded lowest cost almost cases managed controller achieves lower loss iterative controller metacontroller expert cost lower iterative controller average metacontroller true simulation expert lower average. diederik kingma jimmy adam method stochastic optimization. arxiv. volodymyr mnih adri`a puigdom`enech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. proceedings international conference machine learning stuart russell eric wefald. principles metareasoning. artiﬁcial intelligence a¨aron oord kalchbrenner oriol vinyals lasse espeholt alex graves koray kavukcuoglu. table hyperparameter values metacontroller single expert. refers ponder cost refers learning rate controller memory refers learning rate manager αein refers learning rate expert αemlp refers learning rate expert. table hyperparameter values metacontroller experts. refers ponder cost interaction network expert τmlp refers ponder cost expert refers learning rate controller memory refers learning rate manager αein refers learning rate expert αemlp refers learning rate expert.", "year": 2017}