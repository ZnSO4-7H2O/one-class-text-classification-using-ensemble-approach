{"title": "Unitary Evolution Recurrent Neural Networks", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies. To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues of absolute value exactly 1. The challenge we address is that of parametrizing unitary matrices in a way that does not require expensive computations (such as eigendecomposition) after each weight update. We construct an expressive unitary weight matrix by composing several structured matrices that act as building blocks with parameters to be learned. Optimization with this parameterization becomes feasible only when considering hidden states in the complex domain. We demonstrate the potential of this architecture by achieving state of the art results in several hard tasks involving very long-term dependencies.", "text": "martin arjovsky amar shah yoshua bengio universidad buenos aires university cambridge universit´e montr´eal. yoshua bengio cifar senior fellow. ∗indicates ﬁrst authors. ordering determined coin ﬂip. recurrent neural networks notoriously difﬁcult train. eigenvalues hidden hidden weight matrix deviate absolute value optimization becomes difﬁcult well studied issue vanishing exploding gradients especially trying learn long-term dependencies. circumvent problem propose architecture learns unitary weight matrix eigenvalues absolute value exactly challenge address parametrizing unitary matrices require expensive computations weight update. construct expressive unitary weight matrix composing several structured matrices building blocks parameters learned. optimization parameterization becomes feasible considering hidden states complex domain. demonstrate potential architecture achieving state results several hard tasks involving longterm dependencies. deep neural networks shown remarkably good performance wide range complex data problems including speech recognition image recognition natural language processing however training deep models remains difﬁcult task. main issue surrounding training deep networks vanishing exploding gradients problems introduced hochreiter shown bengio necessarily arising trying learn reliably store bits information parametrized dynamical system. gradients propagated back network vanish credit assignment role backpropagation lost information small changes states past inﬂuence future states. gradients explode gradientbased optimization algorithms struggle traverse cost surface gradient-based optimization assumes small changes parameters yield small changes objective function. number time steps considered sequence states grows shrinking expanding effects associated state-to-state transformation individual time steps grow exponentially yielding respectively vanishing exploding gradients. pascanu review. although long-term dependencies problem appears intractable absolute parametrized dynamical systems several heuristics recently found help reduce effect self-loops gating units lstm recurrent architectures. recent work also supports idea using orthogonal weight matrices assist optimization paper explore orthogonal unitary matrices recurrent neural networks. start section showing novel bound propagated gradients recurrent nets recurrent matrix orthogonal. section discusses difﬁculties parameterizing real valued orthogonal matrices alleviated moving complex domain. discuss novel approach constructing expressive unitary matrices composition simple unitary matrices require computation memory state vector dimension unlike general matrices require computation memory. complex valued representations whilst model uses complex valued matrices parameters implementation optimization possible real numbers done theano along implementation details discussed section code used experiments available online. potential developed model learning long term dependencies relatively parameters explored section proposed architecture generally outperforms lstms previous approaches based orthogonal initialization. matrix orthogonal orthogonal matrices property preserve norm hence repeated iterative multiplication vector orthogonal matrix leaves norm vector unchanged. hidden unit vectors hidden layers neural network hidden layers objective trying minimize vanishing exploding gradient problems refer number layers grows. decay growth pointwise nonlinearity function following deﬁne norm matrix refer spectral radius norm norm vector mean l-norm. deﬁnition operator norms matrices vector weight matrices norm preserving fore cannot effectively bound sulting potentially exploding gradients. case equation proves tends exponentially fast grows resulting guaranteed vanishing gradients. argument makes rectiﬁed linear unit nonlinearity attractive choice unless activations killed layer maximum entry resulting layers relu nonlinearities thus unitary evolution rnns unitary matrices generalize orthogonal matrices complex domain. complex valued norm preserving matrix called unitary matrix conjugate transpose directly parametrizing unitary matrices gradient-based optimization applied straightforward gradient step typically yield matrix unitary projecting unitary matrices generally costs computation important feature unitary orthogonal matrices purpose eigenvalues absolute value following lemma proved shed light method used efﬁciently span large unitary matrices. lemma complex square matrix unitary eigendecomposition form vdv∗ denotes conjugate transpose. here cn×n complex matrices unitary diagonal |djj| furthermore real orthogonal matrix every eigenvalue eigenvector also complex conjugate eigenvalue corresponding eigenvector writing eiwj naive method learn unitary matrix would basis eigenvectors cn×n diagonal lemma informs construct real orthogonal matrix must ensure columns come complex conjugate pairs weights order achieve eiwj eiwk. neural network objective functions differentiable respect weight matrices consequently learned gradient descent. unacceptable given number learned parameters note calculating arbitrary vector requires computation. setting identity would satisfy conditions lemma whilst reducing memory computation requirements however would remain diagonal poor representation capacity. propose alternative strategy parameterize unitary matrices. since product unitary matrices unitary matrix compose several simple parameteric unitary matrices construct single expressive unitary matrix. four unitary building blocks considered appealingly permit storage computation matrix vector products. require storage matrix vector multiplication using fast fourier transform algorithm. major advantage composing unitary matrices form listed above number parameters memory computational cost increase almost linearly size hidden layer. weight matrix immensely large hidden layers feasible train whilst impossible traditional neural networks. mind work choose consider recurrent neural networks unitary hidden hidden weight matrices. claim ability large hidden layers hidden states norms preserved provides powerful tool modeling long term dependencies sequence data. suggest large memory crucial solving difﬁcult tasks long ranging dependencies smaller state dimension information necessarily eliminated mapping long sequence ﬁxed-dimension state. whilst permutation matrix complex parameterize represent real numbers implementation purposes. ﬁnal cost real differentiable perform gradient descent optimization learn parameters. construct real valued non-orthogonal matrix using similar parameterization motivation parameter reduction order magnitude industrial sized network. combined earlier work suggests possible create highly expressive matrices composing simple matrices parameters. following section explain details implement model illustrate bypass potential difﬁculties working complex domain. framework sidestep lack support complex numbers deep learning frameworks. consider multiplying complex weight matrix complex hidden vector real. trivially true represent generally complex function complex vector. write allows implement everything using real valued operations compatible deep learning framework automatic differentiation theano. case recurrent networks urnn follows hidden hidden mapping equation denote size complex valued hidden states input hidden matrix complex valued cnh×nin. learn initial hidden state parameter model. choosing appropriate nonlinearity trivial complex domain. discussed introduction using relu natural choice combination norm preserving weight matrix. ﬁrst experimented placing separate relu activations real imaginary parts hidden states. however found nonlinearity usually performed poorly. intuition applying separate relu nonlinearities real imaginary parts brutally impacts phase complex number making difﬁcult learn structure. speculate maintaining phase hidden states important storing information across large number time steps experiments supported claim. variation relu name modrelu ﬁnally chose. pointwise nonlinearity σmodrelu affects absolute value complex number deﬁned stability norm preserving operations network found performance sensitive initialization parameters. full disclosure reproducibility explain initialization strategy parameter below. reﬂection vectors initialized coordinate-wise uniform note reﬂection matrices invariant scalar multiplication parameter vector hence width uniform initialization unimportant. hidden units roughly preserved unitary evolution inputs typically whitened norm hidden states inputs linear outputs order magnitude seems help optimization. bias parameter nonlinearity. dimensional hidden space learn nonlinearity bias parameters dimension. note modrelu similar relu spirit fact concretely figure results copying memory problem time lags lstm able beat baseline times steps. conversely urnn able completely solve time length training iterations without getting stuck baseline. lstm models. show urnn shines quantitatively comes modeling long term dependencies exhibits qualitatively different learning properties models. chose handful tasks evaluate performance various models. tasks especially created pathologically hard used benchmarks testing ability model capture long-term memory handful optimization algorithms tried various models rmsprop lead fastest convergence stuck experiments however found irnn particularly unstable; without blowing incredibly learning rates gradient clipping. since performance poor relative models compare against show irnn curves ﬁgures. experiment learning rate decay rate lstm models clip gradients avoid exploding gradients. recurrent networks known trouble remembering information inputs seen many time steps previously therefore want test urnn’s ability recall exactly data seen long time ago. following similar setup outline copy memory task. consider categories {ai} input takes form length vector categories test range values ﬁrst entries sampled uniformly independently replacement {ai} represent sequence need remembered. next entries thought ’blank’ category. next single entry represents delimiter indicate algorithm required reproduce initial categories output. remaining entries required output sequence consists repeated entries followed ﬁrst categories input figure results adding problem tanh able beat baseline time length. lstm urnn show similar performance across time lengths consistently beating baseline. sequence exactly order. goal minimize average cross entropy category predictions time step sequence. task amounts having remember categorical sequence length time steps. simple baseline established considering optimal strategy memory available deem memoryless strategy. memoryless strategy would predict entries predict ﬁnal categories {ai} independently uniformly random. categorical cross entropy strategy experiments tanh activations irnn lstm urnn hidden layers size respectively. equates roughly parameters model. figure aside simplest case tanh surprisingly lstms almost exactly cost memoryless strategy. behaviour consistent results poor performance reported lstm similar long term memory problem. relatively iterations even recall sequences time steps. remarkable urnn stuck baseline whilst lstm behaviour suggests representations learned urnn qualitatively different properties lstm classical rnns. closely follow adding problem deﬁned explain task hand. input consists sequences length ﬁrst sequence denote consists numbers sampled uniformly random second sequence indicator sequence consisting exactly entries remaining entries ﬁrst entry located uniformly random ﬁrst half sequence whilst second entry located uniformly random second half. output entries ﬁrst sequence corresponding entries located second sequence. naive strategy predicting output regardless input sequence gives expected mean squared error variance independent uniform distributions. baseline beat. figure results pixel pixel mnist classiﬁcation tasks. urnn able converge fraction iterations lstm requires. lstm performs better mnist classiﬁcation urnn outperforms complicated task permuted pixels. chose hidden units tanh irnn lstm urnn. equates roughly parameters tanh irnn lstm almost urnn. models trained using batch sizes best results reported. results shown figure lstm urnn models able convincingly beat baseline time steps. models well mean squared error reach close urnn achieves lower test error it’s curve noisy. despite vastly parameters monitored lstm performance ensure overﬁtting. tanh irnn able beat baseline number time steps. report solve problem irnn require million iterations start learning. neither models came close either urnn lstm performance. stark difference ﬁndings best explained rmsprop signiﬁcantly higher learning rates momentum. task suggested algorithms pixels mnist sequentially required output class label end. consider tasks pixels read order pixels randomly permuted using randomly generated permutation matrix. model architectures adding problem used task except softmax category classiﬁcation. optimization algorithms convergence mean categorical cross entropy test data plot test accuracy figure urnn lstm perform applaudably well here. correct unpermuted mnist pixels lstm performs better achieving test accurracy versus urnn. however permute ordering pixels urnn dominates accuracy contrast lstm despite less quarter parameters. result state task beating irnn reaches close million training iterations. notice urnn reaches convergence less thousand iterations takes lstm times many ﬁnish learning. permuting pixels mnist images creates many longer term dependencies across pixels original pixel ordering structure local. makes necessary network learn remember complicated dependencies across varying time scales. results suggest urnn better able deal structure data lstm better suited local sequence structure tasks. figure left right. norms gradients respect hidden states i.e. iterations. norms hidden states distance hidden states ﬁnal hidden state. gradient norms urnns decay fast models training progresses. urnn hidden state norms stay much consistent time lstm. lstm hidden states stay almost number time steps suggesting able input information. ginning training iterations training adding problem. curves plotted figure clear ﬁrst urnn propagates gradients perfectly model exponentially vanishing gradients. iterations training model experiences vanishing gradients urnn best able propagate information much less decay. hidden state saturation. claim typical recurrent architectures saturate sense acquire information becomes much difﬁcult acquire information pertaining longer dependencies. took urnn lstm models trained adding problem computed forward pass newly generated data adding problem order show saturation effects plot norms hidden states distance state last figure experiments clear urnn suffer much models notice whilst norms hidden states urnn grow steadily time lstm grow fast stay constant time steps. behaviour suggest lstm hidden states saturate ability incorporate information vital modeling long complicated sequences. interesting lstm hidden state close whilst case urnn. again suggests lstm’s capacity information alter hidden state severly degrades sequence length. urnn suffer difﬁculty nearly badly. clear example phenomenon observed adding problem found pearson correlation lstm output prediction ﬁrst uniform samples suggests lstm learnt simply store ﬁrst sample unable incorporate information time reached discussion plethora ideas explored ﬁndings regards learning representation efﬁcient implementation. example hurdle modeling long sequences recurrent networks requirement storing hidden state values purpose gradient backpropagation. prohibitive since memory typically limiting factor neural network optimization. however since weight matrix unitary inverse conjugate transpose easy operate with. invertible nonlinearity function would longer need store hidden states since recomputed backward pass. could potentially huge implications would able reduce memory usage order number time steps. would make immensely large hidden layers possible perhaps enabling vast memory representations. paper demonstrate state performance hard problems requiring long term reasoning memory. results based novel parameterization unitary matrices permit efﬁcient matrix computations parameter optimization. whilst complex domain modeling widely succesful signal processing community exploit power complex valued representation deep learning community. hope work step forward direction. motivate idea unitary evolution novel mitigate problems vanishing exploding gradients. empirical evidence suggests urnn better able pass gradient information long sequences suffer saturating hidden states much lstms typical rnns rnns initialized identity weight matrix acknowledgments thank developers theano great work. thank nserc compute canada canada research chairs cifar support. would also like thank aglar gulc¸ehre david krueger soroush mehri marcin moczulski mohammad pezeshki saizheng zhang helpful discussions comments code sharing. references bengio yoshua simard patrice frasconi paolo. learning long-term dependencies gradient descent transactions neural networks difﬁcult. bergstra james breuleux olivier bastien fr´ed´eric lamblin pascal pascanu razvan desjardins guillaume turian joseph warde-farley david bengio yoshua. theano math expression compiler. proceedings python scientiﬁc computing conference kyunghyun merri¨enboer bart bahdanau dzmitry bengio yoshua. properties neural machine translation encoder–decoder approaches. eighth workshop syntax semantics structure statistical translation october collobert ronan weston jason bottou l´eon karlen michael kavukcuoglu koray kuksa pavel. natural language processing scratch. journal machine learning research glorot xavier bengio yoshua. understanding difﬁculty training deep feedforward neural networks. international conference artiﬁcial intelligence statistics hinton geoffrey deng dong dahl george mohamed abdel-rahman jaitly navdeep senior andrew vanhoucke vincent nguyen patrick sainath tara kingsbury brian. deep neural networks acoustic modeling speech recognition. signal processing magazine krizhevsky alex sutskever ilya hinton geoffrey imagenet classiﬁcation deep convolutional neural networks. neural information processing systems saxe andrew mclelland james ganguli surya. exact solutions nonlinear dynamics international learning deep linear neural networks. conference learning representations tieleman tijmen hinton geoffrey. lecture .rmsprop divide gradient running average recent magnitude. coursera neural networks machine learning yang zichao moczulski marcin denil misha freitas nando smola alex song wang ziyu. deep fried convnets. international conference computer vision", "year": 2015}