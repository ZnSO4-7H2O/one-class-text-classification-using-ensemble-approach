{"title": "Discriminative Probabilistic Models for Relational Data", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In many supervised learning tasks, the entities to be labeled are related to each other in complex ways and their labels are not independent. For example, in hypertext classification, the labels of linked pages are highly correlated. A standard approach is to classify each entity independently, ignoring the correlations between them. Recently, Probabilistic Relational Models, a relational version of Bayesian networks, were used to define a joint probabilistic model for a collection of related entities. In this paper, we present an alternative framework that builds on (conditional) Markov networks and addresses two limitations of the previous approach. First, undirected models do not impose the acyclicity constraint that hinders representation of many important relational dependencies in directed models. Second, undirected models are well suited for discriminative training, where we optimize the conditional likelihood of the labels given the features, which generally improves classification accuracy. We show how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities. We provide experimental results on a webpage classification task, showing that accuracy can be significantly improved by modeling relational dependencies.", "text": "want classify words model classifying solely using words appear page. however hypertext entirely. indicating internal perlinks ment even likely point similar classifying documents portant classification accuracy. therefore document collective class labels explicitly labels many supervised labeled labels pertext highly correlated. sify entity tions models relational used define joint probabilistic collection alternative markov networks first previous pose acyclicity constraint hinders tion many important rected models. second suited conditional tures racy. show train models effectively approximate probabilistic inference markov network learned model collective multiple related entities. sults webpage classification task showing accuracy relational entire collection lafferty tively graphical works introduce tional markov network relational structure domain easily related example linked documents topic. also capture links refer documents undirected defining coherent tures directed flexibility vast majority methods focused \"flat\" data data consisting identically-structured independent data sets innately many real-world cross-citations patents linked webpages papers social networks medical records data consist type characterized related link structure mation algorithm combined propagation also show approximate learned model collective tiple related webpage classification accuracy pendencies tional prms several link graph classification. system-predicted iteratively significant improvement using text document used neville main. slattery actual link graph sparse. large even problematic expressive rected graph graph structures. presence dependent event. representing volving cult. example likely interactions well generative works prms usually probability ofthe labels classification attributes. discriminate trade classification joint distribution non-label discriminatively trained tions independence sification markov networks. denote discrete random variables markov network defines joint distribution consists dency graph quantitative eters associated nodes necessarily connected single node also considered criminative simply markov networks ditional distribution. definition random variables condition markov network bel) random variables. markov network defines distribu­ tion fcec r/>c inition language templates select entire involving tiation. worrying more) interconnected acyclicity models. clique templates tities whose structure associate subgraphs generalization produces attributes work determined clique cliques associated webpage example would define markov which every link pages edge labels pages. fig. illustrates rolled clique templates lational) markov network weights given training tributes labels fully specifies likelihood attempt hood labels help avoid overfitting weights densely completely intractable schemes markov net­ wide variety works. chose belief propagation sim­ belief prop­ plicity algorithm agation duced pearl guaranteed correct singly connected ysis note meta­ data although mostly pages linking considered i.e. resulting classification. well-known logistic vector machines results tic)-using words meta-words. fig. show discriminative perform page often internal sections. section projects tion might contain member third advisees. fig. view section trated fine-grained kleinberg's section take advantage relation ument appears) attribute appears templates. label page introduced defined section path root html parse tree. training category signed frequent less breakdown faculty research. dent none note labels test data learning algorithm learn predict section labels. although correct appearing better words section section+link cliques) baseline experiments feature compares four fourth. formation baseline lational section model ignores links includes tor. general slightly discriminative lustrates tional ists+naive proposed bayes model generates label. separate existence pages' labels. ists+logistic connection logistic tion page label given words. model equiva­ lent expressive criminatively rather link model fully discriminative presented label given words link existence. sults provides model outperforms forms exists+naive illustrated cost training rameter estimation models trained eration hand types model", "year": 2012}