{"title": "Deep Bayesian Active Learning with Image Data", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).", "text": "even though active learning forms important pillar machine learning deep learning tools prevalent within deep learning poses several difﬁculties used active learning setting. first active learning methods generally rely able learn update models small amounts data. recent advances deep learning hand notorious dependence large amounts data. second many acquisition functions rely model uncertainty deep learning methods rarely represent model uncertainty. paper combine recent advances bayesian deep learning active learning framework practical way. develop active learning framework high dimensional data task extremely challenging sparse existing literature. taking advantage specialised models bayesian convolutional neural networks demonstrate active learning techniques image data obtaining signiﬁcant improvement existing active learning approaches. demonstrate mnist dataset well skin cancer diagnosis lesion images challenge many machine learning applications obtaining labelled data. long laborious costly process often making deployment systems uneconomical. framework system could learn small amounts data choose data would like user label would make machine learning much widely applicable. frameworks learning referred active learning used successfully ﬁelds medical diagnosis microbiology manufacturing active learning model trained small amount data acquisition function decides data points external oracle label. acquisition function selects points pool unlabelled data points pool points lying outside training set. oracle labels selected data points added training model trained updated training set. process repeated training increasing size time. advantage systems often result dramatic reductions amount labelling required train system even though existing techniques active learning proven useful variety tasks major remaining challenge active learning lack scalability high-dimensional data data appears often image form physician classifying scans diagnose alzheimer’s example expert clinician diagnosing skin cancer dermoscopic lesion images. perform active learning model able learn small amounts data represent uncertainty unseen data. severely restricts class models used within active learning framework. result approaches active learning focused dimensional problems handful exceptions relying kernel graph-based approaches handle high-dimensional data. recent years increased availability data domains attention within machine learning community shifted small data problems data problems increased interest data problems tools developed existing tools reﬁned handling high dimensional data within regimes. deep learning convolutional neural networks particular example tools. originally developed parse handwritten codes tools ﬂourished adapted point able beat human object recognition tasks techniques dropout used extensively regularise huge models often contain millions parameters even though active learning forms important pillar machine learning deep learning tools prevalent within deep learning poses several difﬁculties used active learning setting. first able handle small amounts data. recent advances deep learning hand notorious dependence large amounts data second many acquisition functions rely model uncertainty. deep learning rarely represent model uncertainty. relying bayesian approaches deep learning paper combine recent advances bayesian deep learning active learning framework practical way. develop active learning framework high dimensional data task extremely challenging sparse existing literature past years taking advantage specialised models bayesian convolutional neural networks demonstrate active learning techniques image data. using small model system able achieve test error mnist labelled images without relying unlabelled data achieves test error labelled images. comparison test error test error ladder network γ-model semi-supervised learning techniques additionally entire unlabelled training set. finally study realworld application diagnosing melanoma small number lesion images ﬁne-tuning convolutional neural network isic dataset past attempts active learning image data concentrated kernel based methods. using ideas previous research active learning dimensional data joshi used margin-based uncertainty extracted probabilistic outputs support vector machines used linear polynomial radial basis function kernels images picking kernel gave best classiﬁcation accuracy. analogously approaches used gaussian processes kernels model uncertainty. however dimensional features kernel. lastly making unlabelled data well acquire points using gaussian random ﬁeld model evaluating kernel images. compare last technique explain detail below. related work includes semi-supervised learning image data semi-supervised learning model given ﬁxed labelled data ﬁxed unlabelled data. model unlabelled data learn distribution inputs hopes information learning small labelled well. although learning paradigm fairly different active learning research forms closest modern literature active learning image data. compare techniques well section paper concentrate high dimensional image data need model able represent prediction uncertainty data. existing approaches rely kernel methods feed image pairs linear polynomial kernels capture image similarity input example. contrast rely specialised models image data particular convolutional neural networks unlike kernels above cannot capture spatial information input image cnns designed spatial information used successfully achieve state-of-the-art results perform active learning image data make bayesian equivalent cnns proposed bayesian cnns cnns prior probability distributions placed model parameters perform approximate inference bayesian model make stochastic regularisation techniques dropout originally used regularise models. shown dropout various choose pool points expected maximise information gained model parameters i.e. maximise mutual information predictions model posterior h−ep model parameters entropy given model weights points maximise acquisition function points model uncertain average exist model parameters produce disagreeing predictions high certainty. equivalent points high variance input softmax layer thus stochastic forward pass model would highest probability assigned different class. random acquisition unif unif function returning draw uniform distribution interval using acquisition function equivalent choosing point uniformly random pool. stochastic regularisation techniques used perform practical approximate inference complex deep models. inference done training model dropout every weight layer performing dropout test time well sample approximate posterior formally approach equivalent performing approximate variational inference distribution tractable family minimises kullback-leibler divergence true model posterior given training dtrain. dropout interpreted variational bayesian approximation approximating distribution mixture gaussians small variances mean gaussians ﬁxed zero. uncertainty weights induces prediction uncertainty marginalising approximate posterior using monte carlo integration bayesian cnns work well small amounts data possess uncertainty information used existing acquisition functions acquisition functions case classiﬁcation discussed next. next explore various acquisition functions appropriate image data setting develop tractable approximations bayesian cnns. tasks involving regression often predictive variance quantity derived acquisition function example might look images high predictive variance choose expert label hope decrease model uncertainty. however many tasks involving image data often phrased classiﬁcation problems. classiﬁcation several acquisition functions available parison current technique active learning image data relies svms. follow comparison closest modern models active learning image data semi-supervised techniques image data. semi-supervised techniques access much data active learning models still perform comparable terms them. finally demonstrate proposed methodology real world application skin cancer diagnosis small number lesion images relying ﬁne-tuning large model. next study acquisition functions bayesian trained mnist dataset acquisition functions assessed model structure convolution-reluconvolution-relu-max pooling-dropout-dense-relu-dropoutdense-softmax convolution kernels kernel size pooling dense layer units dropout probabilities models trained mnist dataset initial training data points validation points optimise weight decay standard test points rest points used pool set. test error model acquisition function assessed next section experiment acquisition functions assess empirically. compared baseline acquisition function uniformly acquires data points pool random various techniques active learning image data semi-supervised learning. followed real-world case study using cancer diagnosis. study proposed technique active learning image data. compare various acquisition functions relying bayesian uncertainty simple image classiﬁcation benchmark. study importance model uncertainty evaluating acquisition functions deterministic cnn. followed comacquisition using dropout approximation test time. decide data points acquire though used dropout following derivations above. repeated acquisition process times time acquiring points maximised acquisition function pool set. experiment repeated three times results averaged compared acquisition functions bald variation ratios entropy mean baseline random. found random mean under-perform compared bald variation ratios entropy variation ratios acquisition function seems obtain slightly better accuracy faster bald entropy. interesting mean seems perform similarly random samples points random pool set. lastly table give number acquisition steps needed test errors seen bald variation ratios entropy attain small test error much fewer acquisitions mean random. table demonstrates importance data efﬁciency expert using variation ratios model example would label less half number images would label acquired images random. assess importance model uncertainty bayesian evaluating three acquisition functions deterministic cnn. much like bayesian deterministic produces probability vector used acquisition functions point mass location setting model parameters deterministic models capture aleatoric uncertainty noise data cannot capture epistemic uncertainty uncertainty parameters minimise during active learning. models experiment still dropout regularisation comparison bayesian models deterministic models bald variation ratios entropy acquisition functions given bayesian models propagating uncertainty throughout model attain higher accuracy early converge higher accuracy overall. demonstrates uncertainty propagated throughout bayesian models signiﬁcant effect models’ measure conﬁdence. next compare method sparse existing literature active learning image data concentrating relies kernel method leverages unlabelled images evaluate kernel images similarity graph used share information unlabelled data. active learning performed greedily selecting unlabelled images labelled estimate expected classiﬁcation error minimised. referred mbr. hence compared acquisition functions bald variation ratios entropy random binary classiﬁcation task classiﬁcation accuracy shown note even random acquisition function coupled outperforms relies kernel. experimented version replaced kernel cnn. interesting note give improved results. continue comparison closest models active learning image data semi-supervised learning image data. semisupervised learning model given ﬁxed labelled data ﬁxed unlabelled data. model unlabelled dataset learn distribution inputs hopes information learning mapping outputs well. several semi-supervised models image data suggested recent years models benchmarks mnist given small number labelled images models make large unlabelled images large validation labelled images tune model hyper-parameters model structure models access much data active learning models still compare relevant models ﬁeld given constraint small amounts labelled data. test error active learning models various acquisition functions well semi-supervised models given table experiment comparable techniques validation points. model attains similar performance semi-supervised models example). rasmus ladder network attains error labelled images unlabelled images. however γ-model architecture directly comparable ours. γ-model attains error compared error ratio acquisition function relies additional unlabelled data. technique semi-supervised semi-sup. embedding transductive pseudo-label atlasrbf virtual adversarial ladder network ladder network active learning various acquisitions random bald entropy ratios table test error mnist labelled training samples compared semi-supervised techniques. active learning access acquired images. semi-supervised further access remaining images labels. following existing research large validation size ﬁnish assessing proposed technique real world test case. experiment melanoma diagnosis dermoscopic lesion images. task given image data skin segments malignant well benign lesions. task classify images malignant benign data used isic archive dataset collected order provide large public repository expertly annotated high quality skin images provide clinical support identiﬁcation skin cancer develop algorithms skin cancer diagnosis. speciﬁcally training data isbi skin lesion analysis towards melanoma detection part segmented lesion classiﬁcation task. data contains dermoscopic lesion images jpeg format exif tags removed. malignancy diagnosis lesions obtained expert consensus pathology report information. data contains lesion segmentation well use. model replicate model model achieved second place part segmented lesion classiﬁcation task code opensourced. model relies data augmentation positive examples ﬁne-tunes model model pre-trained imagenet layer model removed replaced dimensional output preceding last layer fully connected layers size followed dropout layer dropout probability architecture seems provide good uncertainty estimates observed data unbalanced containing negative examples positive examples since data small assess model performance reliably take large balanced test set. randomly partition data aside negative positive examples. experiments performed different random splits since even test size gives different accuracy different random splits. note random split repeat experiments three times average results respect ﬁxed test set. experiment active learning following following procedure. begin creating initial training negative examples positive examples training data well pool remaining data. experiment repetition pool shufﬂed anew. positive examples current training augmented following original training procedure model trained augmented training epochs convergence. batch size weight decay number training points dropout probability length-scale squared acquisition function used select informative images pool set. points removed pool added training original expert-provided labels points. process repeated pool points exhausted acquisition step reset model original pre-trained weights reset done order avoid local optima avoid confusing model performance improvement improvement resulting simply using longer optimisation time. acquisition test performance model logged using dropout samples. keep track number positive examples acquired acquisition. model performance assessed using area-under-the-curve seems informative metrics used gutman experimented average precision metric suggested gutman well managed results improving competition winner simply predicting points benign. might data imbalance. hand takes account possible decision-thresholds possible classify malignant image. figure well number acquired positive examples bald acquisition function well uniform acquisition function isic melanoma diagnosis dataset. random test splits assessed test experiment repeated three times different random seeds bald. even though variation ratios performs well mnist above function fails melanoma data since malignant images given slight higher probability malignant compared probability benign images malignant. result pool points given identical variation ratios acquisition value. experiment results given results reported test splits split experiment repeated three times performance results averaged ﬁxed split. test split report mean standard error. reported split number acquired positive examples reported well acquisition step. bald achieves better faster uniform acquires positive examples acquisition step uniform initial training set). demonstrates difﬁculties handling small data test split gives radically different results case even though acquisition function experiment relatively small standard error averaging acquisition functions different test splits would artiﬁcially increase standard error. lastly interesting experiment model trained entire pool i.e. settings second place winner isic task. ﬁrst test split model attains whereas second test split attains test splits worse bald’s converged acquisition steps. might bald avoided selecting noisy points near-by images exist multiple noisy labels different classes. points large aleatoric uncertainty uncertainty cannot explained away rather large epistemic uncertainty uncertainty bald captures order explain away i.e. reduce bayesian modelling deep learning demonstrated real-world application medical diagnosis. assessed performance techniques resetting models acquisition training convergence. done isolate effects acquisition functions came cost prolonged training times showed even long running time technique still reduces required expert labels thus reduces costs system. running time reduced resetting system potential price falling local optima. leave problem future research. deng dong socher richard li-jia fei-fei imagenet large-scale hierarchical image database. computer vision pattern recognition cvpr ieee conference ieee gutman david codella noel celebi emre helba brian marchetti michael mishra nabin halpern allan. skin lesion analysis toward melanoma detection challenge international symposium biomedical imaging hosted international skin imaging collaboration arxiv preprint arxiv. kaiming zhang xiangyu shaoqing jian. delving deep rectiﬁers surpassing humanlevel performance imagenet classiﬁcation. proceedings ieee international conference computer vision hernandez-lobato jose miguel adams ryan. probabilistic backpropagation scalable learning bayesian neural networks. proceedings international conference machine learning hinton geoffrey srivastava nitish krizhevsky alex sutskever ilya salakhutdinov ruslan improving neural networks preventing co-adaptation feature detectors. arxiv preprint arxiv. holub alex perona pietro burl michael entropycombased active learning object recognition. puter vision pattern recognition workshops cvprw’. ieee computer society conference ieee houlsby neil husz´ar ferenc ghahramani zoubin lengyel m´at´e. bayesian active learning classiﬁcation preference learning. arxiv preprint arxiv. joshi ajay porikli fatih papanikolopoulos nikolaos. multi-class active learning image classiﬁcation. computer vision pattern recognition cvpr ieee conference ieee kampffmeyer michael salberg arnt-borre jenssen robert. semantic segmentation small objects modeling uncertainty urban remote sensing images using deep convolutional neural networks. ieee conference computer vision pattern recognition workshops june kendall alex badrinarayanan vijay cipolla roberto. bayesian segnet model uncertainty deep convolutional encoder-decoder architectures scene understanding. arxiv preprint arxiv. kingma diederik mohamed shakir rezende danilo jimenez welling max. semi-supervised learning deep generative models. advances neural information processing systems srivastava nitish hinton geoffrey krizhevsky alex sutskever ilya salakhutdinov ruslan. dropout simple prevent neural networks overﬁtting. jmlr lafferty ghahramani combining active learning semi-supervised learning using gaussian proceedings ﬁelds harmonic functions. icml- workshop continuum labeled unlabeled data icml krizhevsky alex sutskever ilya hinton geoffrey imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems lecun yann boser bernhard denker john henderson donnie howard richard hubbard wayne jackel lawrence backpropagation applied handwritten code recognition. neural computation marcus daniel fotenos anthony csernansky john morris john buckner randy open access series imaging studies longitudinal data nondemented demented older adults. journal cognitive neuroscience pitelis nikolaos russell chris agapito lourdes. semi-supervised learning using unsupervised atlas. joint european conference machine learning knowledge discovery databases springer rasmus antti berglund mathias honkala mikko valpola harri raiko tapani. semi-supervised learning ladder networks. advances neural information processing systems rifai salah dauphin yann vincent pascal bengio yoshua muller xavier. manifold tangent clasadvances neural information processing siﬁer. systems", "year": 2017}