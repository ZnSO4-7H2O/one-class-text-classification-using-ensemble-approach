{"title": "Describing Videos by Exploiting Temporal Structure", "tag": ["stat.ML", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "abstract": "Recent progress in using recurrent neural networks (RNNs) for image description has motivated the exploration of their application for video description. However, while images are static, working with videos requires modeling their dynamic temporal structure and then properly integrating that information into a natural language description. In this context, we propose an approach that successfully takes into account both the local and global temporal structure of videos to produce descriptions. First, our approach incorporates a spatial temporal 3-D convolutional neural network (3-D CNN) representation of the short temporal dynamics. The 3-D CNN representation is trained on video action recognition tasks, so as to produce a representation that is tuned to human motion and behavior. Second we propose a temporal attention mechanism that allows to go beyond local temporal modeling and learns to automatically select the most relevant temporal segments given the text-generating RNN. Our approach exceeds the current state-of-art for both BLEU and METEOR metrics on the Youtube2Text dataset. We also present results on a new, larger and more challenging dataset of paired video and natural language descriptions.", "text": "recent progress using recurrent neural networks image description motivated exploration application video description. however images static working videos requires modeling dynamic temporal structure properly integrating information natural language description. context propose approach successfully takes account local global temporal structure videos produce descriptions. first approach incorporates spatial temporal convolutional neural network representation short temporal dynamics. representation trained video action recognition tasks produce representation tuned human motion behavior. second propose temporal attention mechanism allows beyond local temporal modeling learns automatically select relevant temporal segments given text-generating rnn. approach exceeds current state-of-art bleu meteor metrics youtubetext dataset. also present results larger challenging dataset paired video natural language descriptions. task automatically describing videos containing rich open-domain activities poses important challenges computer vision machine learning research. also variety practical applications. example figure high-level visualization approach video description generation. incorporate models local temporal dynamic videos well global temporal structure. local structure modeled using temporal feature maps temporal attention mechanism used combine information across entire video. generated word model focus different temporal regions video. simplicity highlight region maximum attention above. every minute hours video uploaded youtube. however video poorly tagged utility dramatically diminished automatic video description generation potential help improve indexing search quality online videos. conjunction speech synthesis technology annotating video natural language descriptions also potential beneﬁt visually impaired. image description generation already considered challenging task automatic generation video description carries additional difﬁculties. simply dealing sheer quantity information contained video data challenge. moreover video description involves generating sentence characterize video clip lasting typically seconds frames. often clips contain complex interactions actors objects evolve time. together amounts vast quantity information attempting represent information using single temporally collapsed feature representation likely prone clutter temporally distinct events objects potentially fused incoherently. therefore important automatic video description generator exploit temporal structure underlying video. argue categories temporal structure present video local structure global structure. local temporal structure refers ﬁne-grained motion information characterizes punctuated actions answering telephone standing actions relatively localized time evolving consecutive frames. hand refer global temporal structure video refer sequence objects actions scenes people etc. appear video. video description well termed video summarization typically look single sentence summarize rather elaborate sequence events. good image descriptions often focus salient parts image description argue good video description systems selectively focus salient features video sequence. recently venugopalan used so-called encoder–decoder neural network framework automatically generate description video clip. extracted appearance features frame input video clip using previously trained convolutional neural network features frames subsampled frames collapsed simple averaging result single vector representation entire video clip. indiscriminate averaging frames approach risks ignoring much temporal structure underlying video clip. instance possible tell order appearances objects collapsed features. paper introduce temporal attention mechanism exploit global temporal structure. also augment appearance features action features encode local temporal structure. action features derived spatio-temporal convolutional neural network temporal attention mechanism based recently proposed soft-alignment method used successfully context machine translation. generating description temporal attention mechanism selectively focuses small subset frames making possible generator describe objects and/or activities subset hand starts temporally spatially local motion descriptors video hierarchically extracts abstract action-related features. features preserve emphasize important local structure embedded video description generator. evaluate effectiveness proposed mechanisms exploiting temporal structure widely used open-domain video description dataset called youtubetext dataset consists video clips multiple descriptions video. also test proposed approaches much larger recently proposed dataset based descriptive video service tracks movies contains video clips. work makes following contributions propose novel cnn-rnn encoder-decoder architecture captures local spatio-temporal information. despite promising results generated prior work using static frame cnnrnn video description methods experiments suggest indeed important exploit local temporal structure generating description video. propose attention mechanism within cnnrnn encoder-decoder framework video description demonstrate experiments allows features obtained global analysis static frames throughout video used effectively video description generation. furthermore observe improvements brought exploiting global local temporal information complimentary best performance achieved temporal attention mechanism used together. section describe general approach based purely neural networks generate video descriptions. approach based encoder-decoder framework successfully used machine translation well image caption generation encoder-decoder framework consists neural networks; encoder decoder. encoder network encodes input continuous-space representation variable-sized continuous vectors decoder network generates corresponding output encoder representation case encoder decoder’s architecture must chosen according type output. output natural language sentence case automatic video description method choice. decoder runs sequentially output sequence. brief generate output step updates internal state based previous internal state well previous output encoder representation outputs symbol remaining section detail choices encoder decoder basic automatic video description system taken work builds. encoder convolutional neural network deep convolutional neural networks recently successful large-scale object recognition beyond object recognition task itself cnns trained object recognition found useful variety computer vision tasks object localization detection opened door ﬂood computer vision systems exploit representations upper intermediate layers generic high-level features vision. instance activation last fully-connected layer used ﬁxed-size vector representation feature last convolutional layer used spatial feature vectors case input video clip imagetrained used frame separately resulting single vector representation i-th frame. approach proposed used convolutional neural network work here also consider using demonstrated higher performance object recognition. decoder long short-term memory network discussed earlier natural recurrent neural network decoder output natural language sentence. empirically conﬁrmed contexts machine translation image caption generation video description generation open closed domains. among recently successful applications natural language generation noticeable used long short-term memory units variant gated recurrent units paper also variant lstm units introduced decoder. element-wise logistic sigmoid function time-dependent transformation function encoder features. order weight matrices input previous hidden state context encoder bias. word embedding matrix denote embedding vector word yt−. architecture composed three convolutional layer followed rectiﬁed linear activations local max-pooling. activation last convolution+relu+pooling layer preserves temporal arrangement input video abstracts local motion features obtain temporal feature vectors max-pooling along spatial dimensions feature vectors summarize content short frame sequences within video. finally feature vectors combined concatenation image features extracted single frames taken similar positions across video. fig. illustrates complete architecture described cnn. similarly object recognition trained pre-train activity recognition datasets. features previous section allows better represent short-duration actions subset consecutive frames. however representing complete video averaging local temporal features would jeopardize model’s ability exploit video’s global temporal structure. approach exploiting non-local temporal structure decoder selectively focus small subset frames time. considering subsets frames sequence model exploit temporal ordering objects actions across entire video clip avoid conﬂating temporally disparate events. approach also potential allowing model focus elements video short duration. methods collapse temporal structure risk overwhelming short duration elements. recently proposed soft attention mechanism allows decoder weight temporal feature vector vn}. approach used successfully exploiting spatial structure underlying trivial generate sentence lstm decoder. instance recursively evaluate sample returned sampled end-of-sequence symbol. also approximately sentence highest probability using simple beam search venugopalan used type lstm decoder automatic video description generation. however work feature transformation function consisted simple averaging i.e. vi’s elements returned encoder sec. averaging effectively collapses frames indiscriminate temporal relationships leading loss temporal structure underlying input video. section delve main contributions paper propose approach exploiting local global temporal structure automatic video description. exploiting local structure temporal structure videos level temporal features extracted encoder. speciﬁcally propose spatio-temporal convolutional neural network recently demonstrated capture well temporal dynamics video clips build higher-level representations preserve summarize local motion descriptors short frame sequences. done ﬁrst dividing input video clip spatio-temporal grid cuboids. cuboid represented concatenating histograms oriented gradients oriented motion boundary bins. transformation done order make sure local temporal structure well extracted reduce computation subsequence cnn. figure spatiotemporal convolutional neural network network trained activity recognition. then convolutional layers involved generating video descriptions. video description generation investigated studied work examples have however constrained domain videos well activities objects embedded video clips. furthermore tend rely hand-crafted visual representations video template-based shallow statistical machine translation approaches applied. contrast approach take propose paper aims open-domain video description generation deep trainable models starting low-level video representations including pixel intensities local motion features sense approach closely related recently introduced static image caption generation approaches based mainly neural networks neural approach static image caption generation recently applied video description generation venugopalan however direct adaptation underlying static image caption generation mechanism videos limited fact model tends ignore temporal structure underlying video. structure demonstrated helpful context event action classiﬁcation explored paper. recent work explored annotated video video description research underscored observation descriptions typically much relevant accurate descriptions visual content video compared movie scripts. present results using script based annotations well cooking activities. reﬂects relevance i-th temporal feature input video given previously generated words i.e. yt−. hence design function takes input previous hidden state lstm decoder summarizes previously generated words feature vector i-th temporal feature returns unnormalized relevance score attention mechanism allows decoder selectively focus subset frames increasing attention weights corresponding temporal feature. however explicitly force type selective attention happen. rather inclusion attention mechanism enables decoder exploit temporal pre-training model number widely used action recognition datasets. contrast formulations input consists features derived number state image descriptors. model also fully model entire volumes across video clip. paper state-of-the-art static convolutional neural network novel spatio-temporal model input video clips. modeling video using feedforward convolutional neural networks become increasingly popular recently however also stream research using recurrent neural networks modeling video clips. instance srivastava propose long short-term memory units extract video features. ranzato also models video clip however vectorquantizing image patches video clip. contrast approaches explored cnnrnn coupled models video description attention mechanism focus opendomain video description. youtubetext youtubetext video corpus well suited training evaluating automatic video description generation model. dataset video clips multiple natural language descriptions video clip. total dataset consists approximately video description pairs vocabulary approximately unique words. dataset opendomain covers wide range topics including sports animals music. following split dataset training video clips validation clips test consisting remaining clips. dataset recently introduced much larger number video clips accompanying descriptions existing video/description corpora youtubetext. contains video clips extracted movies along semi-automatically transcribed descriptive video service narrations. dataset consists video clips covering wide variety situations. follow standard split dataset training clips validation clips test clips suggested description preprocessing preprocess descriptions youtubetext datasets wordpunct tokenizer nltk toolbox.. preprocessing lowercasing rare word elimination. preprocessing numbers unique words youtubetext dataset. video preprocessing reduce computational memory requirement consider ﬁrst frames video appearance features googlenet used extract ﬁxed-length representation features extracted pool/x layer. select equally-spaced frames ﬁrst video feed obtain dimensional frame-wise feature vector. also apply spatio-temporal order extract local motion information. using without temporal attention simply -dimensional activation last fully-connection layer. combine temporal attention mechanism leverage last convolutional layer representation leading feature vectors size vector contatenated features resulting feature vectors elements. experimental setup models test four different model variations video description generation based underlying encoderdecoder framework results presented table encdec denotes baseline incorporating neither local global temporal structure. based encoder using googlenet discussed section lstm-based decoder outlined section enc-dec local incorporates local temporal structure integration proposed features googlenet features described above. enc-dec global adds temporal attention mechanism section finally encdec local global incorporates temporal attention mechanism model. models otherwise number temporal features experiments allow investigate whether contributions proposed approaches complimentary combined improve performance. all-zero frames make -frame long. perturb video along three axes form random crops taking multiple cuboids original cuboids ﬁnal representation average representations perturbed video clips. although model-free evaluation metrics ones used paper designed reﬂect agreement level reference generated descriptions intuitively clear well numbers reﬂect quality actual generated descriptions. therefore present video clips corresponding descriptions generated reference test dataset. unless otherwise labeled visualizations section best model exploits global local temporal structure video clips test youtubetext shown. clearly generated descriptions correspond well video clips. fig. show also sample video clips dataset. clearly model perform well dataset youtubetext already evident quantitative analysis sec. however still observe model often focuses correctly subset frames according word generated. instance left pane model generate second someone focuses mostly ﬁrst frame. also right panel model correctly attends second frame word types generated. local temporal features allowed correctly identify action frying opposed simply cooking. used adadelta gradient computed backpropagation algorithm. optimized hyperparameters using random search maximize log-probability validation set. training continued validation log-probability stopped increasing updates. mentioned earlier sec. trained activity recognition datasets. space limitation details regarding training evaluation activity recognition datasets provided supplementary material. evaluation report performance proposed method using test perplexity three model-free automatic evaluation metrics. bleu meteor cider evaluation script prepared introduced quantitative analysis ﬁrst block table present performance four different variants model using four metrics bleu meteor cider perplexity. subsequent lines table give comparisons prior work. ﬁrst three rows +local +global) show generally beneﬁcial exploit type temporal structure underlying video. although beneﬁt evident perplexity observe similar trend model-free metrics across youtubetext datasets. figure four sample videos corresponding generated ground-truth descriptions youtubetext frame corresponding word plot frame corresponds attention weight generated. left panel word road generated model focuses highly third frame road clearly visible. similarly bottom left panel model attends second frame generate word someone. bottom includes alternate descriptions generated model variations. global temporal structure addition frame-wise appearance information. propose novel convolutional neural network designed capture local ﬁne-grained motion information consecutive frames. order capture global temporal structure propose temporal attentional mechanism learns ability focus subsets frames. finally proposed approaches naturally together encoder-decoder neural video caption generator. empirically validated approach youtubetext datasets four standard evaluation metrics. experiments indicate models using either approach improve baseline model. furthermore combining approaches gives best performance. fact achieved state-of-the-art results youtubetext combination. given challenging nature task hypothesize performance dataset could signiﬁcantly improved incoporating another recently proposed dataset similar data used here. addition preliminary experimental results indicate performance gains possible leveraging image caption generation datasets coco authors would like thank developers theano acknowledge support following organizations research funding computing support nserc fqrnt samsung calcul quebec compute canada canada research chairs cifar.", "year": 2015}