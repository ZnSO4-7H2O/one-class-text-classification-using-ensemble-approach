{"title": "Sequential Update of Bayesian Network Structure", "tag": ["cs.AI", "cs.LG"], "abstract": "There is an obvious need for improving the performance and accuracy of a Bayesian network as new data is observed. Because of errors in model construction and changes in the dynamics of the domains, we cannot afford to ignore the information in new data. While sequential update of parameters for a fixed structure can be accomplished using standard techniques, sequential update of network structure is still an open problem. In this paper, we investigate sequential update of Bayesian networks were both parameters and structure are expected to change. We introduce a new approach that allows for the flexible manipulation of the tradeoff between the quality of the learned networks and the amount of information that is maintained about past observations. We formally describe our approach including the necessary modifications to the scoring functions for learning Bayesian networks, evaluate its effectiveness through an empirical study, and extend it to the case of missing data.", "text": "obvious need improving accuracy formance data observed. construction domains mation data. sequential parameters accom­ plished sequential date network structure still open problem. paper investigate bayesian structure approach nipulation learned tion maintained formally necessary learning tiveness case missing able systems errors initial adapt changes underlying tion. especially sequential situations past observations memory. examples situations systems collect data extended embedded systems limited memory constraints usually involve kept secondary inspection attempt previously model learned assumes data instances distributed current current model updates search frontier. shall amount space required related size search frontier. dynamics recording data raises fundamental functions scoring sume alternative training cedure however start recording different candidates ifications deal complication. \"extended\" mental learning rest paper organized follows sec­ tion briefly review current practice learning bayesian networks. incremental ical foundations. evaluation introduce extension section discussion main results work. common approach problem introduce scoring function works respect training best network compute observations. prior next iteration thus maintain potheses compute store posterior ology attractive property state time reasonable posterior initial structure priors iteration quired next instance network generate collected spirit cumulative fined several application. natural measure instances. sure well procedure log-loss number training instances whether observed correlations uine correlations model represent next iteration observe another instances stage however prior strongly generalizes siderable subset climbing search procedure candidate neighbors. networks change away evaluate neighbors maintaining consists neighbors nets many networks several arcs distinct generalizing search procedure define search frontier. frontier denote networks. iteration. choice determines main­ tained memory. contain sufficient statistics instance number instances sufficient statistics scoring network frontier nets). procedure determine next frontier accordingly. infor­ mation also remove sufficient statistics memory. section middle ground previous section. keep possible data records single network represent basic component tains sufficient statistics select amongst pos­ allow update procedure sible networks update. explaining proach detail introduce suffi denote sufficient statistics thus cannot evaluate closed-form. believe comparison respect different requires principled question. intuitively confidence families follows proposal based modifying existing scores learning bayesian networks. prop­ erty furthermore results show performs well practice. proposal best motivated score casted information words average encoding length instance true encoding length achieved approximates second term expression optimal encoding choice model. larger datasets coding decreases. term captures amount confidence learned parameters increases confident models. time less confident models require parameters. discussion instance different lengths. ideally tions reported literature different structures fortunately evaluating dataset. since start collecting records different families family parent might summarize larger number instances parent effectively evalua­ tion different datasets. underlying problem general model selection prob­ compare models model evaluated model evaluated respect assume sampled training course problem meaning­ underlying true case. scores inappropriate problem current form. score measures number bits required data assume model however much smaller would usually shorter description regardless good model problem occurs score. score evaluates probability dataset form specified model. again distribution much smaller probability usually larger since probability instance dataset product probability given previous ones. since term usually smaller probability longer sequences. course reset counters gathering effect restarts training this however would discard useful sequence. information gathered earlier parts sequence. alternatively adopt bayesian method compute since compare terms degrees belief candidate likely given available unfortu­ nately evaluate average score consistent score compare networks given data. compare models using data scores make choices. this note case average score assigned models original scores also want ensured limit score choose right model. make evaluation based log-loss standard measure performance since generated sets existing net­ works measure close learned models generating normalized erating distribution. properties. procedure measures additional instead true model. second normalized related cross-entropy probabilistic rewritten average based thus average normalized loss estimate discussion data complete sense data contains many real life applications incomplete data. source incompleteness come noisy attributes directly observed. terms measuring variable order evaluate given candidate network structure linear optimization gradient procedure. learning. addition ters assumption adapt sequential restrictions. deal describe learning posts sufficient statistics finally complex structure. timate stabilized stabilized remark memory usage incremental limits minimized prior knowledge constraints). space possible additionally estimate additions promising essentially show also mprove prob­ ability statisti used continuously intuitive justification sufficiently thus instead storing relays data. second justifies updates parameters seen. results parameters using decay parameter gradually decrease con­ tribution \"completed\" parameters). procedure guaranteed positive make progress sufficiently procedure second restrict shows expected sufficient statistics evaluate alternative choose structures current model score network respect observed data. simi-", "year": 2013}