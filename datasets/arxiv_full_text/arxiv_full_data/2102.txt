{"title": "Optimizing the CVaR via Sampling", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Conditional Value at Risk (CVaR) is a prominent risk measure that is being used extensively in various domains. We develop a new formula for the gradient of the CVaR in the form of a conditional expectation. Based on this formula, we propose a novel sampling-based estimator for the CVaR gradient, in the spirit of the likelihood-ratio method. We analyze the bias of the estimator, and prove the convergence of a corresponding stochastic gradient descent algorithm to a local CVaR optimum. Our method allows to consider CVaR optimization in new domains. As an example, we consider a reinforcement learning application, and learn a risk-sensitive controller for the game of Tetris.", "text": "prashanth ghavamzadeh strong incentive develop general cvar optimization algorithms. work propose cvar optimization approach applicable also controls distribution basis approach formula derive form conditional expectation. based formula propose samplingbased estimator cvar gradient optimize cvar stochastic gradient descent. addition analyze bias estimator result prove convergence stochastic gradient descent algorithm local cvar optimum. method allows consider cvar optimization domains. example consider reinforcement learning application learn risk-sensitive controller game tetris. knowledge cvar optimization domain beyond reach existing approaches. considering tetris also allows easily interpret results show indeed learn sensible policies. extending approach problems straightforward using standard penalty method techniques since component methods cvar gradient estimator provide here. another appealing property estimator naturally incorporates importance sampling important small cvar captures rare events. related work approach similar spirit likelihood-ratio method estimates gradient expected payoff. method successfully applied diverse domains queueing systems inventory management ﬁnancial engineering also reinforcement learning commonly known policy gradient method work extends method estimating gradient cvar payoff. conditional value risk prominent risk measure used extensively various domains. develop formula gradient cvar form conditional expectation. based formula propose novel sampling-based estimator gradient cvar spirit likelihood-ratio method. analyze bias estimator prove convergence corresponding stochastic gradient descent algorithm local cvar optimum. method allows consider cvar optimization domains. example consider reinforcement learning application learn risksensitive controller game tetris. conditional value risk established risk measure found extensive ﬁnance among ﬁelds. random payoff whose distribution parameterized controllable parameter α-cvar deﬁned expected payoff worst outcomes deterministic function random depend cvar optimization formulated stochastic program solved using various approaches payoff structure appropriate certain domains portfolio optimization investment strategy generally affect asset prices. however many important domains example queueing systems resource allocation reinforcement learning tunable parameters also control distribution random outcomes. since existing cvar optimization methods suitable cases increased interest risk-sensitive optimization recently domains exponential utility functions tamar castro mannor mean– variance. measures however consider different notion risk cvar. example mean– variance measure known underestimate risk rare catastrophic events risk-sensitive optimization receiving increased interest recently. mean-variance criterion considered tamar castro mannor prashanth ghavamzadeh morimura consider expected return cvar based risk-sensitive policy guiding exploration learning. method however scale large problems. borkar jain optimize cvar constrained objective using dynamic programming augmenting state space accumulated reward. such method suitable ﬁnite horizon small state-space scale-up problems tetris domain consider. function approximation extension mentioned using three time scales stochastic approximation algorithm. work three different learning rates decreased convergence determined slowest leading overall slow convergence. contrast approach requires single learning rate. recently prashanth used gradient formula proposition time-scale stochastic approximation scheme show convergence cvar optimization. besides providing theoretical basis work current convergence result obviates need extra time-scale results simpler faster algorithm. section present lr-style formula gradient cvar. gradient used subsequent sections optimize cvar respect parametric family. start formal deﬁnition cvar present cvar gradient formula dimensional random variables. extend result multi-dimensional case. bution function convemeaning everywhere continuous. also assume bounded. given conﬁdence level given inf{z∶ α-conditional-value-at-risk denoted rameter vector expressed gradient simple cases calculating gradient analytically intractable. therefore derive formula tion calculate gradient sampling. technical convenience make following assumption assumption continuous random variable bounded assumption gradients bounded. relaxing assumptions assumption formula gradient expressed )\u0004z≤να\u0004. proof. deﬁne level-set dθ={z∈∶ deﬁnition dθ≡] and∫z∈dθ deﬁnition ∫z∈dθ φα=α proof proposition similar spirit proof proposition involves additional difﬁculties applying leibnitz rule multidimensional setting. given reiterate relaxing assumptions possible technically involved left future work. next section show formula proposition leads effective algorithm estimating drawn i.i.d. joint distribution ﬁrst estimate using empirical empirical c.d.f. r≤z. estimate gradient ∆j;n≈ given ∂log fx\u0000y ∆j;n= r≤˜v. mator therefore ∆j;n also biased estimaφα. following analyze bound theorem assumption hold. ∆j;n w.p. n→∞. explicitly bound bias. denote p.d.f. deﬁne function eθ\u0002\u0002 ∂log ∂log fx\u0000y \u0002)\u0002r= assumption continuous ascending order selecting theऄαnअ contrast cvar formula proposition standard formula expectation baseline could arbitrary constant. note cvar case baseline speciﬁc seen proof accounts sensitivity level-set quite surprisingly speciﬁc baseline turns exactly typical application would correspond performance system proﬁt portfolio optimization total reward note order proposition gradient estimation algorithm needs access performance distribution parameters. typically system performance complicated function high-dimensional random variable. example queueing systems performance function trajectory stochastic dynamical system calculating probability distribution usually intractable. sensitivity trajectory distribution parameters however often easy calculate since parameters typically control trajectory generated. shall generalize proposition cases. utility generalization exempliﬁed section domain. denote n−dimensional random variable ﬁnite supportn denote sety. denote probability mass function fx\u0000y denote probability density funcmapping fromn×y consider random variable interested formula able furthermore graφα well deﬁned bounded. addition ∂log fx\u0000y {x∈n∶ following assumption ondy;θ. assumption setdy;θ written positive measuredy;θ=∑ly;θ different sampling distribution suitably modifying estimator keep unbiased. straightforward incorporate gradient estimators general gcvar estimator particular. space constraints since fairly standard textbook material provide full technical details supplementary material. empirical results show using indeed leads signiﬁcantly better performance. goal propose cvarsgd stochastic gradient descent algorithm based gcvar gradient estimator. describe cvarsgd algorithm detail show converges local optimum projection compact smooth boundary. purpose projection facilitate convergence algorithm guaranteeing iterates remain bounded practice chosen large enough contains local opdenote operator that given direction change parameter returns modiﬁed direction keeps within consider following ordinary differential equation proof given supplementary material based separating bias term bounded using result hong additional term bound using well-known results bias empirical quantiles. theorem assumptions hold. point contrast gcvar standard method. naively presume applying standard gradient estimator worst samples would work cvar gradient estimator. corresponds applying gcvar algorithm without subtracting baseline reward theorems show estimator would consistent. fact supplementary material give example gradient error approach arbitrarily large. sequel gcvar part stochastic gradient descent algorithm cvar optimization. asymptotically decreasing gradient bias established theorem necessary guarantee convergence procedure. furthermore bound theorem allow quantify many samples needed iteration convergence hold. variance reduction importance sampling quantiles i.e. close gcvar estimator would suffer high variance since averaging effectively samples. well-known issue sampling based approaches cvar estimation often mitigated using variance reduction techniques importance sampling variance estimator reduced using samples section show cvarsgd algorithm used policy-gradient type scheme optimizing performance criteria involve cvar total return. ﬁrst describe preliminaries setting describe algorithm. discrete time ﬁnite state spaces ﬁnite action spacea. time state assigns distribution actions fa\u0000h according observed history states then immediate random reward fρ\u0000sa transition probability fs′\u0000sa. denote received state transitions according terminal state reached w.p. policy terminates time i.e. tradiscrete part continuous part ρτ−. quantity interest total reward along trajectory stanimizes expected return policy gradient mate ∂v~∂θj perform stochastic gradient asj࣊ spirit policy gradient methods estimate ∂j~∂θj simulation using gcvar also known stochastic shortest path this decomposition restrictive used illustrate deﬁnitions section alternatively consider continuous state space discrete rewards long assumptions hold. using simulated trajectories finally update policy parameter according note transition probabilities generally known decision maker required estimating gradient using gcvar. policy-dependent terms required. markov policy depends current state sufﬁces achieve optimality cvar criterion necessarily case. b¨auerle show certain conditions augmentation current state function accumulated reward sufﬁces optimality. simulations used markov policy still obtained useful sensible results. assumptions required convergence algorithm reasonable setting domains reward takes discrete values. case speciﬁcally covered theory paper arbitrarily small smooth noise total reward results hold. since modiﬁcation negligible impact performance issue little importance practice. experiments reward discrete observe problem. experimental results examine tetris test case algorithms. tetris popular benchmark studied extensively. main challenge tetris large state space necessitates form approximation solution technique. many approaches learning controllers tetris described literature among approximate value iteration policy gradients modiﬁed policy iteration standard performance measure tetris expected number cleared lines game. here interested risk-averse performance measure captured cvar total game score. goal section compare performance policy optimized cvar criterion versus policy obtained using standard policy gradient method. show optimizing cvar indeed produces different policy characterized risk-averse behavior. note present best results literature obtained using modiﬁed policy iteration approach figure gcvar policy gradient. average return cvar return cvarsgd final policy parameters. note difference board well feature encourages risk taking. cvar standard policy-gradient iteration. histogram total return ﬁnal policies. lower plot zoom-in left-tail clearly shows risk-averse behavior cvarsgd policy. using policy gradients. emphasize goal compete results rather illustrate application cvarsgd. point however whether approach gabillon ghavamzadeh scherrer could extended handle cvar objective currently known. standard shapes order induce risksensitive behavior modiﬁed reward function game follows. score clearing lines respectively. addition limited maximum number steps game modiﬁcations strengthened difference risk-sensitive nominal policies induce tradeoff clearing many ’single’ lines proﬁt waiting proﬁtable less frequent ’batches’. used softmax policy feature thiery scherrer starting ﬁxed policy parameter obtained running several iterations standard policy gradient cvarsgd standard policy gradient enough iterations algorithms compromise explained fig. display reward distribution ﬁnal policies. observed left-tail distribution cvar policy signiﬁcantly lower standard policy. risk-sensitive decision maker results important especially left-tail contains catastrophic outcomes common many real-world domains ﬁnance. better understand differences policies compare ﬁnal policy parameters fig. signiﬁcant difference parameter corresponds board well feature. well succession unoccupied cells column left right cells occupied. controller trained cvarsgd smaller negative weight feature compared standard controller indicating actions create deep-wells repressed. wells lead high reward ﬁlled risky heighten board. compared cvarsgd version cvarsgd described supplementary material. fig. shows gcvarsgd converged signiﬁcantly faster improving convergence rate factor full details provided supplementary material. presented novel lr-style formula gradient cvar performance criterion. based formula proposed sampling-based gradient estimator stochastic gradient descent procedure cvar optimization guaranteed converge local optimum. knowledge ﬁrst extension method cvar performance criterion results extend cvar optimization domains. evaluated approach empirically domain learning risk-sensitive policy tetris. knowledge domain beyond reach existing cvar optimization approaches. moreover empirical results show optimizing cvar indeed results useful risksensitive policies motivates simulation-based optimization risk-sensitive decision making. authors thank odalric-ambrym maillard many helpful discussions. research leading results received funding european research council under european union’s seventh framework program grant agreement", "year": 2014}