{"title": "Interpreting Neural Network Judgments via Minimal, Stable, and Symbolic  Corrections", "tag": ["cs.LG", "cs.AI", "stat.ML", "68T01"], "abstract": "The paper describes a new algorithm to generate minimal, stable, and symbolic corrections to an input that will cause a neural network with ReLU neurons to change its output. We argue that such a correction is a useful way to provide feedback to a user when the neural network produces an output that is different from a desired output. Our algorithm generates such a correction by solving a series of linear constraint satisfaction problems. The technique is evaluated on a neural network that has been trained to predict whether an applicant will pay a mortgage.", "text": "paper describes algorithm generate minimal stable symbolic corrections input cause neural network relu neurons change output. argue correction useful provide feedback user neural network produces output different desired output. algorithm generates correction solving series linear constraint satisfaction problems. technique evaluated neural network trained predict whether applicant mortgage. machine learning used make decisions people real world extremely important able explain rationale behind decisions. unfortunately systems based deep learning often even clear explanation means; showing someone sequence operations computed decision provides little actionable insight. recent advances towards making deep neural networks interpretable using main approaches generating input prototypes representative abstract concepts corresponding different classes explaining network decisions computing relevance scores different input features however explanations provide direct actionable insights regarding cause prediction move undesirable class desirable class. paper argue speciﬁc class judgment problems minimal stable symbolic corrections ideal explaining neural network decision. term judgment paper refer particular kind binary decision problem user presents information algorithm supposed pass judgment input. distinguishing feature judgments relative kinds decision problems asymmetric; apply loan loan satisﬁed particularly care explanation; even bank care long aggregate algorithm makes bank money. hand much care algorithm denies mortgage application. true variety problems college admissions parole hiring decisions. cases user expects positive judgment would like actionable explanation accompany negative judgment. argue correction useful form feedback; could done differently elicit positive judgment? example applied mortgage knowing would gotten positive judgment debt income ratio lower extremely useful; actionable information adjust ﬁnances. argue however useful corrections minimal stable symbolic. first order correction actionable corrected input similar possible original offending input. example knowing lower would given loan useful knowing year billionaire nebraska would gotten loan useful. minimality must deﬁned terms error model speciﬁes inputs subject change how. bank loan example debt income loan amount subject change within certain bounds move another state satisfy bank. second suggested correction stable meaning neighborhood points surrounding suggested correction outcome also positive. example algorithm tells lower would gotten mortgage months later come back lower expect mortgage extremely disappointed bank says sorry said lower lower. even though neural network perfectly reasonable give positive judgments isolated points surrounded points negative judgments corrections lead isolated points useful. even better rather single point algorithm produce symbolic correction provides insight relationship different variables. example knowing someone like bank expects useful knowing single value. knowing something range would change function credit score would even useful still. paper present ﬁrst algorithm capable computing minimal stable symbolic corrections. given neural network relu activations algorithm produces symbolic description space corrections correction space change judgment. limit algorithm closest region volume given threshold. internally algorithm reduces problem series linear constraint satisfaction problems solved using gurobi linear programming solver show practice algorithm able good symbolic corrections minutes average small realistic networks. evaluate approach neural network trained mortgage data predicts whether given applicant default mortgage. ﬁrst introduce notations explaining algorithm computing minimal robust symbolic corrections given input neural network relu activation. model consider input network vector size network computes output layer speciﬁcally focused binary classiﬁcation problems judgement problem special binary classiﬁcation problem label preferable other. assume preferable throughout paper. judgement interpretation problem concerns providing feedback form corrections correction real vector input vector length mentioned previously desirable feedback minimal stable symbolic correction. ﬁrst introduce means concrete correction minimal stable. minimality deﬁned terms norm measures distance corrected input original input. simplicity norm measure sizes vectors throughout section section e-stable symbolic correction connected concrete corrections. concretely linear constraints represent symbolic correction. symbolic correction e-stable exists correction call correction stable region center inside deﬁne minimality deﬁne distance original input using distance stable region center smallest distance among stable region centers. formally e-stable empty deﬁne dise deﬁne judgement interpretation problem. deﬁnition given neural network input vector real value judgment interpretation e-stable symbolic correction minimum distance among e-stable symbolic corrections. figure shows example judgement interpretation. cross represents original input blue triangle represent corrected inputs according judgement interpretation. algorithm discovered person could gotten loan slightly adjusting interest rate. algorithm outlines approach judgment interpretation given neural network input vector besides inputs parameterized real integer former speciﬁes radius parameter stability deﬁnition latter speciﬁes many features allowed vary produce judgment interpretation. parameterize number features change high-dimension interpretations hard users understand. instance much easier user understand that mortgage would approved long change credit score keeping features were give complex interpretation involves features output judgment interpretation expressed system linear constraints form algorithm ﬁnds interpretation iteratively invoking procedure ﬁndprojectedinterpretation interpretation varies list features returns least distance. recall distance deﬁned dise minδ∈sδ evaluated solving sequence linear programming problems norm used. order judgment interpretation need linear constraints minimal stable veriﬁed none properties trivial satisfy given complexity real-world neural network. ﬁrst discuss address challenges high level dive details algorithm. address minimality single concrete correction minimum leveraging existing adversarial example param integer maximum number veri regions worklist ﬁndminimumconcretecorrection getactivations getregionfromactivations regions regions worklist append pophead infersimplexcorrection return generation technique generate linear constraints stable veriﬁable exploit fact relu-based neural networks piecewise linear functions. brieﬂy inputs activate neurons characterized linear constraints. characterize subset inputs classiﬁed adding additional linear constraint. similarly linear constraints represent veriﬁed corrections certain activations. call corrections veriﬁed linear region ﬁrst identify region initial concrete correction belongs grow regions identifying regions connected existing regions. finally infer linear constraints whose concrete corrections subset ones enclosed discovered regions. rection leveraging modiﬁed version fast signed gradient method minimizes distance concretely starting vector calculate iteratively adding modiﬁed gradient takes sign signiﬁcant dimension among selected features example original gradient modiﬁed gradient would obtain relu activations boolean vector boolean represents whether given neuron activated. finally obtain initial region falls invoking getregionfromactivations deﬁned below getregionfromactivations deﬁnition above notation refer layer activations ﬁxed formally zeros rows activation indicated rectiﬁer original layer produced zero. represent number relu layers |fj| represent number neurons layer. integer indexes neuron layer. vector real number weights bias neuron respectively. intuitively activationconstraints uses linear constraints encode activation neuron. generating initial region algorithm tries grow concrete corrections identifying regions connected existing regions. know whether region connected another efﬁciently? regions network neurons checking whether sets linear constraints intersect expensive high dimensions. intuitively regions likely connected activations differ relu. however entirely correct given region constrained activations also desired classiﬁcation. corrections face corresponding convex hulls face corresponds differing neuron. intuitively piece-wise function represented neural network sets concrete corrections adjacent linear pieces connected concrete corrections boundary them. following intuition deﬁne checkregionboundary checkregionboundary classconstraints featureconstraints) leveraging checkregionboundary algorithm uses worklist algorithm identify regions connected transitively connected initial region regions found number discovered regions reaches predeﬁned upper bound algorithm tries infer linear constraints whose corresponding concrete corrections contained discovered regions. moreover satisfy stability constraint want large possible. intuitively want convex hull contained polytope volume convex hull maximized. further infer constraints represent simplex rather convex hull reasons. first simplicity simplex makes easy user interpret; secondly relatively efﬁcient calculate volume simplex. procedure infersimplexcorrection implements process using greedy algorithm. brieﬂy ﬁrst randomly choose discovered region randomly sample simplex inside vertex move small distance random direction simplex still contained discovered regions volume increases. process stops volume cannot increased further. note approach sound optimal complete. words whenever algorithm ﬁnds symbolic correction correction veriﬁed stable guaranteed minimal. also approach fails stable symbolic correction mean corrections exist. however practice approach able stable corrections time distances discovered corrections small enough useful handling categorical features. assumed features reals. categorical features typically represented using one-hot encoding directly applying algorithm embedding result symbolic correction whose concrete corrections invalid. address issue enumerate embedding different values categorical features apply algorithm search symbolic corrections different embedding. example suppose features ﬁrst feature boolean value second feature real number interval symbolic corrections second feature. obtained assuming ﬁrst feature obtained assuming ﬁrst feature alse. extending non-relu-based neural networks. approach remains unchanged long activation functions continuous approximated using continuous piece-wise linear function. activation function continuous assumption test whether veriﬁed regions differ activation connected testing constraint corresponding activation breaks. activation functions continuous cannot approximated using piece-wise linear function aforementioned assumption hold need expressive constraints linear constraints represent veriﬁed regions. extending norms. assumed sizes vectors measured using norms. norms algorithm largely remains same except dise measures stability size inferred symbolic correction. norms applied evaluating dise requires solving non-linear optimization problems expensive number varied features large. avoiding adversarial corrections. adversarial inputs inputs generated existing input small perturbations indistinguishable users original input lead different classiﬁcations. adversarial inputs undesirable often considered bugs neural network. simplicity consider previous discussions. avoid corrections would result adversarial inputs rely user deﬁne threshold concrete correction considered adversarial. additional constraint region. implemented approach tool called polaris. polaris written three thousand lines python code. implement ﬁndminimumconcretecorrection used customized version cleverhans library implement isfeasible checks feasibility generated linear constraints applied commercial linear programming solver gurobi since publicly available neural networks mortgage underwriting ended building network. moreover datasets found provide application information performance information approved loans. datasets provide information rejected applications information often demographic factor decision making. result built network predicts whether applicant default mortgage based application information instead. consider application rejected network predicts applicant default. dataset. used single-family historical loan performance dataset published fannie largest publicly available single-family loan dataset. consists application information performance information million loans issued application loan consists features credit score debt-to-income ratio others. removed features whose values missing dataset results features. train neural network split dataset training validation test ratio evaluate approach randomly selected loan applications rejected network test set. algorithm conﬁguration. approach three parameters stability threshold number features allowed change simultaneously maximum number regions consider produce symbolic corrections easy understand moreover limit mutable features features consider useful providing users feedback loan applications described table slightly involved customized operator diste mortgage application. brieﬂy used weighted norm evaluate distance correction weighted norm evaluate stability. distance weight numeric feature. categorical feature property type charge distance minimum stable concrete correction symbolic correction would change otherwise. relatively large penalty changing property type requires applicant switch different property. stability deﬁne stability radius array weight feature category feature involved require symbolic corrections least contain categories feature. table deﬁnes range radius feature. deﬁne diste follow ﬁrst discuss quantitatively often polaris generates stable corrections away corrections original input. inspect generated corrections detail discuss whether indeed useful users. finally talk long takes polaris generate correction. stability minimality. polaris successfully generated symbolic corrections applications selected loan applications rejected neural network. rest applications either case corrections found polaris discarded unstable case polaris failed initial concrete correction incompleteness applied adversarial example generation algorithm. results show polaris effective ﬁnding symbolic corrections stable veriﬁed. next discuss similar corrections original input. figure lists sorted distances aforementioned symbolic corrections. recall distance deﬁned using weighted norm weight numeric feature charge categorical feature property type needs changed. average distance largest distance smallest distance small moreover corrections distances average distance already small distances symbolic corrections fact even much smaller. conclusion although polaris guarantee minimality corrections found figure shows symbolic correction generated along loan-to-value ratio property type minimum correction application. cross shows projection original application features blue lines represent corrected applications symbolic correction would lead first observe correction small. applicant loan approved reduce loan-to-value ratio correction also stable. applicant decides stick single-family home properties loan approved long reduction loan-to-value ration greater moreover similar results switch cooperative share properties condominiums. correction also makes much sense since reducing loan-to-value ratio often means reduce loan value. practice smaller loans easier approve. also perspective training data smaller loans less likely default. figure shows symbolic correction generated along debt-to-income ratio interest rate numeric features. similar figure cross represents projection original application blue triangle represents symbolic correction. addition polytope enclosed dotted yellow lines represent veriﬁed linear regions collected algorithm observations regions. first polytope highly irregular reﬂects highly nonlinear nature neural network. however polaris still able generate symbolic corrections efﬁciently. secondly ﬁnal correction inferred approach covers area regions shows effectiveness greedy algorithm applied infersimplexcorrection. correction also small stable distance larger previous correction along loan-to-value ratio property type. correction also makes sense training data perspective. obvious applicants smaller debt-to-income ratios less likely default. interest rate correction leans towards increasing might fact subprime mortgage crisis loans approved irrationally interest rate many went default later. figure shows correction generated along debt-toincome ratio loan-to-value ratio. compared previous corrections distance small highly unstable fact discarded polaris this. comparison corrections generated previous application figure shows ﬁnal correction generated application corresponds rightmost figure words ﬁnal correction qualitative study. previous discussion gives high-level idea effectiveness approach look individual generated symbolic corrections closely. interested answering questions figure shows symbolic corrections generated application minimum judgment interpretation among applications. application corresponds leftmost figure since polaris conﬁgured generate corrections involving features feamuch work interpretability gone analyzing results produced convolutional network image classiﬁcation. activation maximization approach follow-ups visualize learnt high-level features ﬁnding inputs maximize activations given neurons zeiler fergus uses deconvolution visualize network learnt. limited image domains recent works build interpretability part network also works explain neural network learning interpretable model know problem deﬁnition judgement interpretation none existing approaches directly solve moreover approaches typically generate single input prototype relevant features result corrections space inputs would lead prediction move undesirable class desirable class. adversarial examples ﬁrst introduced szegedy box-constrained l-bfgs applied generate them. various approaches proposed later. fast gradient sign method calculate adversarial perturbation taking sign gradient. jacobian-based saliency attack applies greedy algorithm based saliency models impact pixel resulting classiﬁcation. deepfool untargeted attack optimized norm. bastani applies linear programming adversarial example activations. techniques similar sense also minimum corrections produced corrections concrete symbolic. proposed approach interpret neural network generating minimal stable symbolic corrections would change output. interpretation useful provide feedback user neural network fails produce desirable output. designed implemented ﬁrst algorithm generating corrections demonstrated effectiveness neural network mortgage underwriting. largest distance among ﬁnal corrections generated applications. ﬁgure shows large distance makes hard applicant adopt. categories property type applicant needs raise credit score even cases easy practice. result polaris assigns high distance correction. efﬁciency. figure shows sorted running time polaris loan application. ranges seconds seconds balanced distribution. average polaris takes around minutes generate ﬁnal correction loan application. given reality mortgage underwriting usually takes days running time moderate. manual inspection found majority time spent invocations linear programming solver checkregionboundary. although invocation takes fraction second invocations many number neurons region added. future plan number invocation investigating sampling-based approaches. bach sebastian binder alexander montavon gr´egoire klauschen frederick m¨uller klaus-robert samek wojciech. pixel-wise explanations non-linear classiﬁer decisions layer-wise relevance propagation. plos bastani osbert ioannou yani lampropoulos leonidas vytiniotis dimitrios nori aditya criminisi antonio. measuring neural robustness constraints. advances neural information processing systems annual conference neural information processing systems december barcelona spain kaiming zhang xiangyu shaoqing jian. deep residual learning image recognition. ieee conference computer vision pattern recognition cvpr vegas june honglak grosse roger ranganath rajesh andrew convolutional deep belief networks scalable unsupervised learning hierarchical representations. proceedings annual international conference machine learning icml montreal quebec canada june barzilay regina jaakkola tommi rationalizing neural predictions. proceedings conference empirical methods natural language processing emnlp austin texas november moosavi-dezfooli seyed-mohsen fawzi alhussein frossard pascal. deepfool simple accurate ieee method fool deep neural networks. conference computer vision pattern recognition cvpr vegas june nguyen dosovitskiy alexey yosinski jason brox thomas clune jeff. synthesizing preferred inputs neurons neural networks deep generator networks. advances neural information processing systems nicolas papernot nicholas carlini goodfellow reuben feinman fartash faghri alexander matyasko karen hambardzumyan yi-lin juang alexey kurakin ryan sheatsley abhibhav garg yen-chen lin. cleverhans adversarial machine learning library. arxiv preprint arxiv. pinheiro pedro collobert ronan. imagelevel pixel-level labeling convolutional networks. ieee conference computer vision pattern recognition cvpr boston june ribeiro marco t´ulio singh sameer guestrin carlos. trust you? explaining predictions classiﬁer. proceedings sigkdd international conference knowledge discovery data mining francisco august szegedy christian zaremba wojciech sutskever ilya bruna joan erhan dumitru goodfellow fergus rob. intriguing properties neural networks. corr abs/. improving interpretability deep neural networks ieee workshop austimulated learning. tomatic speech recognition understanding asru scottsdale december chunyang karanasou penny gales mark chai. stimulated deep neural network speech recognition. interspeech annual conference international speech communication association francisco september zeiler matthew fergus rob. visualizing uncomputer viderstanding convolutional networks. sion eccv european conference zurich switzerland september proceedings part", "year": 2018}