{"title": "The Lottery Ticket Hypothesis: Training Pruned Neural Networks", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "Recent work on neural network pruning indicates that, at training time, neural networks need to be significantly larger in size than is necessary to represent the eventual functions that they learn. This paper articulates a new hypothesis to explain this phenomenon. This conjecture, which we term the \"lottery ticket hypothesis,\" proposes that successful training depends on lucky random initialization of a smaller subcomponent of the network. Larger networks have more of these \"lottery tickets,\" meaning they are more likely to luck out with a subcomponent initialized in a configuration amenable to successful optimization.  This paper conducts a series of experiments with XOR and MNIST that support the lottery ticket hypothesis. In particular, we identify these fortuitously-initialized subcomponents by pruning low-magnitude weights from trained networks. We then demonstrate that these subcomponents can be successfully retrained in isolation so long as the subnetworks are given the same initializations as they had at the beginning of the training process. Initialized as such, these small networks reliably converge successfully, often faster than the original network at the same level of accuracy. However, when these subcomponents are randomly reinitialized or rearranged, they perform worse than the original network. In other words, large networks that train successfully contain small subnetworks with initializations conducive to optimization.  The lottery ticket hypothesis and its connection to pruning are a step toward developing architectures, initializations, and training strategies that make it possible to solve the same problems with much smaller networks.", "text": "recent work neural network pruning indicates that training time neural networks need signiﬁcantly larger size necessary represent eventual functions learn. paper articulates hypothesis explain phenomenon. conjecture term lottery ticket hypothesis proposes successful training depends lucky random initialization smaller subcomponent network. larger networks lottery tickets meaning likely luck subcomponent initialized conﬁguration amenable successful optimization. paper conducts series experiments mnist support lottery ticket hypothesis. particular identify fortuitously-initialized subcomponents pruning low-magnitude weights trained networks. demonstrate subcomponents successfully retrained isolation long subnetworks given initializations beginning training process. initialized such small networks reliably converge successfully often faster original network level accuracy. however subcomponents randomly reinitialized rearranged perform worse original network. words large networks train successfully contain small subnetworks initializations conducive optimization. lottery ticket hypothesis connection pruning step toward developing architectures initializations training strategies make possible solve problems much smaller networks. recent work neural network pruning indicates neural networks dramatically simpliﬁed trained. training complete upwards weights pruned without reducing accuracy. network pruned function learned could represented smaller network used training. however researchers believe smaller networks cannot trained readily larger counterparts spite fact demonstrably capable representing desired functions. paper contend indeed possible train smaller networks directly. fact small trainable networks embedded within larger models typically train. paper articulates possible explanation disconnect neural network’s representation capacity trainability. conjecture term lottery ticket hypothesis mentions cnns contain fragile co-adapted features gradient descent able good solution network initially trained re-initializing layers retraining them...when retrain pruned layers keep surviving parameters instead re-initializing them. states training succeeds given network subnetworks randomly initialized could trained isolation—independent rest network—to high accuracy number iterations necessary train original network. refer fortuitously-initialized networks winning tickets. subnetworks winning tickets. perspective lottery ticket hypothesis network’s initialization procedure thought drawing many samples distribution initialized subnetworks. ideally procedure manages draw subnetwork right architecture weight initializations optimization succeed network size trained solve problem network size sufﬁcient represent function learned lottery ticket hypothesis train successfully subnetworks lucked initialization amenable optimization. metaphorically training network larger necessary represent function learned like buying many lottery tickets. larger networks combinatorially subcomponents could facilitate successful training initialization strategy determines subcomponents well-situated optimization succeed subcomponent initialized favorably training succeeds. identifying winning tickets. paper demonstrate possible automatically identify winning tickets making small critical modiﬁcation experiment prune trained neural network’s smallest weights manner al.; connections survives pruning process architecture winning ticket anticipated lottery ticket hypothesis. unique work winning ticket’s weights values connections initialized training began. aimed compress networks training process goal small networks trained independently start. show winning ticket extracted fashion initialized original weights training trained successfully isolation least fast full network. methodology. empirically assess lottery ticket hypothesis following procedure extract winning tickets fully-connected networks variety sizes mnist illustrative example small networks xor. procedure identical al.’s pruning process addition crucial last step resetting weights original values training. randomly initialize neural network. train network converges. prune fraction network. extract winning ticket reset weights remaining portion network successful training really rely fortuitous initialization subcomponent network pruning really reveal winning ticket lottery ticket hypothesis predicts pruned network—when reset original initializations training—will train successfully sizes small randomly-initialized randomly-conﬁgured network research questions. test lottery ticket hypothesis evaluate following research questions effectively winning tickets train comparison original network randomly sampled networks similar size? variety network conﬁgurations perturbations winning tickets measure convergence times test accuracy network converged. winning tickets relative size original network? training networks various sizes explore whether size winning ticket remains constant particular learning problem grows proportion size larger network derived. figure success rates random networks speciﬁed number hidden units. percent trials found correct decision boundary. percent trials reached zero loss. sensitive results particular pruning strategies? test broad classes strategies pruning hidden units lowest-magnitude incoming and/or outgoing weights individually pruning lowest-magnitude weights also study whether networks pruned single step whether must repeatedly pruned retrained reset iterative process. results. experimental results support lottery ticket hypothesis. xor. trained simple network hidden layer learn xor. minimal architecture capable representing hidden layer randomly-initialized units reached zero loss time. contrast network hidden units reached zero loss iteratively pruned two-unit winning ticket winning ticket reached zero loss time trained original initializations. mnist. certain point winning tickets derived pruning converged faster least accurately original network; point convergence times accuracy gradually rapidly dropped off. single step could prune networks still ﬁnding winning tickets that average converged faster original network matched accuracy. pruning iteratively time winning tickets smaller original network converged average faster. networks iteratively pruned average still converged fast original network maintaining accuracy. winning tickets randomly reinitialized weights randomly rearranged convergence times increased accuracy decreased compared original network. depending metric winning ticket size winning tickets grew either gradually marginally network size. apply algorithm empirically evaluate conjectures small networks. evidence supports lottery ticket hypothesis contention pruning extract winning tickets. although paper focuses mainly measurement important implications understanding training. increased representation power large networks necessarily required gradient descent learn functions small representations. lurking within large networks small fortuitously-initialized winning tickets efﬁcient train faster converge examining initalizations architectures successful winning tickets might ways designing networks smaller equally-capable function among simplest examples distinguish neural networks linear classiﬁers. presenting results mnist summarize lottery ticket hypothesis applies simple computation. function four data points coordinates ﬁrst last points placed class middle points class geometrically problem requires nonlinear decision boundary. experiment consider family fully connected networks input units hidden layer output unit figure success rates different pruning strategies trials each. deﬁned figure pruned columns include runs original ten-unit network pruned winning ticket found right decision boundary reached zero loss. ﬁrst table obtained pruning shot; subsequent rows involved pruning iteratively although network form hidden units sufﬁcient perfectly represent function probability standard training approach—one randomly initializes network’s weights applies gradient descent—correctly learns network hidden units relative larger network. figure contains overall success rates training runs network hidden units learned correct decision boundary trials. cross-entropy loss reached trials. meanwhile otherwise identical network outﬁtted hidden units learned decision boundary trials reached loss trials. figure charts loss hidden layer sizes. central question paper concrete terms problem need start neural network hidden units ensure training succeeds much smaller neural network hidden units represent function perfectly? propose lottery ticket hypothesis explanation phenomenon. lottery ticket hypothesis. training succeeds given network subnetworks randomly initialized trained isolation high accuracy number iterations necessary train original network. according lottery ticket hypothesis successful networks large number parameters contain winning tickets comprising small number fortuitously-initialized weights training still succeed. randomly initialize network hidden units. train iterations entire training set. prune certain number hidden units according particular pruning heuristic. extract winning ticket reset pruned network original initializations. ﬁrst three steps extract architecture winning ticket; crucial ﬁnal step extracts corresponding initializations. experiment different classes pruning strategies. one-shot pruning involves pruning network single pass. example one-shot pruning network would involve removing units trained. contrast iterative pruning involves repeating steps several times removing small portion weights sampled normal distribution centered standard deviation values standard deviations mean discarded resampled. biases initialized network trained iterations. units iteration. iterative pruning effective extracting smaller winning tickets; found compressing large networks maintaining accuracy. consider three different heuristics determining hidden units pruned input magnitude remove hidden unit smallest average input weight magnitudes. output magnitude remove hidden unit smallest output weight magnitude. magnitude product remove hidden unit smallest product magnitude one-shot pruning. generated networks hidden units pruned four hidden units using magnitude product heuristic. results appear ﬁrst figure winning tickets hidden units found correct decision boundary time reached zero loss time iterative pruning. conducted iterative version pruning experiment times starting networks containing hidden units eventually pruned networks containing candidate winning ticket hidden units. hidden unit networks reached zero loss hidden unit winning ticket also reached zero loss likewise hidden unit networks found correct decision boundary two-unit winning ticket four hidden unit winning tickets almost identically mirror performance original hidden unit network. found correct decision boundary reached zero loss respectively cases hidden unit network pruned trials appear figure experiments indicate that although iterative pruning computationally demanding one-shot pruning ﬁnds winning tickets higher rate one-shot pruning. importantly also conﬁrm networks hidden units pruned winning tickets hidden units that initialized values original network succeed training frequently randomly initialized network hidden units. winning tickets four hidden units succeed nearly frequently unit networks derive. results support lottery ticket hypothesis—that large networks contain smaller fortuitously-initialized winning tickets amenable successful optimization. addition magnitude-product pruning heuristic also tested input magnitude output magnitude heuristics. results appear figure magnitude product heuristic outperformed both. posit success fact that case input values either product input output weight magnitudes mimic activation unit section follow explore lottery ticket hypothesis applied mnist dataset. here analyze behavior one-shot pruning; following section show additional power iterative pruning offers. these numbers derived last figure networks hidden units reached zero loss. networks started units reached zero loss pruned two-unit networks also reached zero loss. trained pruned network fully-connected layers. used lenet-- architecture input units fully-connected hidden layer units fully-connected hidden layer units fully-connected output units hidden units relu activation functions output units softmax activation functions. default biases initialized weights randomly sampled normal distribution mean standard deviation networks optimized using stochastic gradient descent learning rate section follows experimental template section randomly initialize network. train iterations -example mini-batches training data prune certain percentage weights within hidden layer removing pruning strategy follow mnist removes individual weights rather entire units. preliminary experiments found strategy effective simplest weight-by-weight pruning heuristic possible remove weights lowest magnitudes within hidden layer weights connecting output layer pruned half percentage rest network pruned avoid severing connectivity output units. figure test accuracy mnist training proceeds. charts zoomed highest levels accuracy. curve shows average progression trials training speciﬁed pruning level. percents percent weights layer remain pruning. error bars show minimum maximum values trials. dots signify moment corresponding colored line converged error bars showing earliest latest convergence times amongst trials. pruning’s substantial impact convergence times. pruned size original network winning tickets converged average least faster accuracy remained average within original network’s accuracy. winning ticket pruned original size converged average faster original network. pruning caused convergence times slowly rise accuracy drop. figure shows test accuracy convergence behavior winning tickets pruned different levels trained. curve average different runs starting distinct deﬁne convergence moment -iteration moving average test accuracy changed less consecutive iterations. measured test accuracy every iterations. according deﬁnition convergence one-shot-pruned winning tickets improved test accuracy average convergence. acknowledge determining convergence times imprecise metric seems adequately characterize behavior convergence purposes. randomly initialized networks; error bars indicate minimum maximum value took point training process. dots indicate average convergence times curve corresponding color; error bars indicate minimum maximum convergence times. left graph figure shows that ﬁrst pruning levels convergence times decrease accuracy increases. winning ticket comprising weights original network converges slightly faster original network slower winning ticket original weights. pattern continues network pruned original size convergence times ﬂatten then increase. winning ticket original size network returns performance unpruned network. terms lottery ticket hypothesis attribute improving convergence times removal unnecessary noisy parts network pruning hones winning ticket. convergence times reach tipping point pruning begins remove weights essential winning ticket convergence times increase accuracy decreases. lottery ticket hypothesis also predicts behavior largely attributable conﬂuence initialization architecture. test conjecture control experiments retain winning ticket’s architecture randomize weights retain winning ticket’s weights randomize architecture. control experiment experiment evaluates extent initialization necessary component winning ticket. figure shows experiment. curves original network winning tickets original network’s size figure curves added control experiments. control experiment entailed training network used winning ticket’s architecture randomly reinitialized weights original initialization distribution trained three control experiments winning ticket control curves average experiments. unlike winning tickets control experiments converged average slowly original network simultaneously achieving lower levels accuracy. differences substantial average winning tickets converged times fast corresponding average controls. error bars convergence times reﬂect control trials exhibited much wider variance behavior. example earliest-converging control trials converged faster average unpruned network; however average control trial convergence time converged slower average original network. experiment supports lottery ticket hypothesis’ emphasis fortuitous initialization. using pruned architecture original initialization withstood beneﬁted pruning performance reinitialized network immediately suffered steadily diminished network pruned. outcome mirrors larger scale result experiment networks many hidden units could pruned smaller winning tickets found right decision boundary much higher rate randomly-initialized small networks. figure provides broader perspective patterns across levels pruned. left graph shows convergence time relation percentage network remaining pruning. blue line average winning ticket trials level. convergence figure convergence times accuracies running mnist pruning experiment various degrees pruning. blue line average trials different starting initializations prune reuse original initialization. multicolored lines represents three randomly reinitialized control trials error bars minimum maximum value trial takes interval. figure convergence times accuracy winning tickets level pruning trials winning ticket weights reinitialized trials winning ticket weights maintained shufﬂed within layer time initially decreases leveling slowly climbing again. contrast multicolored lines represent groups control trials winning ticket steadly require longer converge network pruned. control experiment error bars much larger suggesting wider variation convergence times compared consistent convergence times winning tickets. right graph figure provides important context accurate networks moment converge? average trial used original initialization maintans accuracy within original network pruned accuracy drops off. contrast accuracy average control trial drops level network pruned falling precipitously pruned experiment supports lottery ticket hypothesis’ prediction fortuitous initialization necessary ingredient make winning ticket. winning ticket’s structure alone insufﬁcient explain success. control experiment experiment evaluates extent architecture necessary component winning ticket. winning ticket level pruning randomly shufﬂed locations weights hidden layer maintaining original initializations. results appear figure figure blue line traces winning tickets pruned various sizes. orange line average trials control experiment green line average trials control experiment convergence times control experiments similar start increasing immediately increase rapidly network gets smaller. accuracy control experiment drops slightly earlier control experiment dropped winning ticket did. experiment supports lottery ticket hypothesis’ prediction winning tickets emerge combination initialization structure. neither initialization structure alone sufﬁcient explain better performance winning tickets. figure convergence times accuracy winning tickets extracted fully-connected networks mnist using one-shot pruning iterative pruning note x-axis logarithmic. summary. ﬁrst notable result experiments that even pruned sizes much smaller original network winning tickets still able converge all. supports core prediction lottery ticket hypothesis pruning reveals smaller subcomponents originally initialized train successfully isolation. networks train successfully converge faster maintain accuracy networks derive. furthermore winning tickets emerge conﬂuence fortuitous initalization structure. experiment section iterative pruning —repeatedly training pruning reinitializing pruning again—arrived winning tickets likely train successfully section iterative pruning makes possible extract winning tickets mnist network smaller generated one-shot pruning. iteratively prune incoming weights ﬁrst second layers network weights output layer start network fully-connected hidden layers hidden units prune network original weights remained. comparison one-shot pruning. figure shows difference convergence times accuracy one-shot pruning iterative pruning average iteratively pruned winning tickets reach initially reach lower convergence times. convergence times ﬂatten original network pruned original network size compared one-shot pruning. average iteratively pruned network returns original convergence time pruned likewise accuracy actually increases slightly many winning tickets returning original network’s accuracy winning ticket size average. contrast one-shot pruning begins drop winning ticket size original network. although iterative pruning extract much smaller winning tickets one-shot pruning costly winning tickets. extracting winning ticket one-shot pruning requires training original network single time regardless much network pruned. contrast iteratively pruning network iteration original network’s size requires training network times. however since goal understand behavior winning tickets rather efﬁciently iterative pruning’s compelling advantage able extract smaller winning tickets maintain convergence accuracy performance placing tighter upper-bound size network’s winning ticket. section re-run control experiments section before explore extent architecture initialization responsible winning ticket’s ability continue converge small sizes. figure contains average results performing control experiment orange control experiment green. comparison curve performance one-shot pruning. control experiment one-shot pruning average convergence times control experiment begin increasing soon network pruned continue grow steady rate. error bars figure reﬂect convergence times vary widely pruned networks reinitialized. average control trial’s accuracy begins dropping network pruned whereas average iteratively pruned network drops level pruned one-shot experiment control trial indicates initialization plays critical role making winning ticket. control experiment average convergence times control trial increase steadily pattern similar control trial error bars indicate convergence times similarly vary widely. accuracy begins dropping earlier steeply potentially suggesting architecture might important initialization. summary. control experiments iterative pruning results section sharper relief. iterative pruning makes possible extract smaller winning tickets one-shot pruning reach lower convergence times original network maintaining exceeding level accuracy. control experiments show initialization network architecture figure distributions initializations weights survived iterative pruning across iterative pruning runs. graphs contain initializations network pruned blue orange green lines distributions initial weights ﬁrst hidden layer second hidden layer output layer respectively. play factor creating winning ticket control trial suggesting network architecture might slightly important. experiments mnist support lottery ticket hypothesis. embedded within larger networks small subcomponents fortuitously initialized manner conducive successful training. extracted winning ticket architectures pruning determined corresponding initializations resetting winning ticket’s connections original training. networks trained successfully case iteratively-pruning mnist network converged faster accurately. meanwhile neither architecture initialization alone could entirely account result. next investigate architecture initializations small winning tickets behavior winning tickets subjected wider variety parameters section brieﬂy explore internal structure winning tickets result iterativelypruning mnist network. already found evidence support claim winning tickets arise conﬂuence architecture initialization. exactly architectures initializations look like? initializations. figure shows initialization distributions winning tickets four different levels pruning. note values winning ticket’s weights training. graph upper left contains initial weights entire network initialized according normal distribution mean standard deviation graph upper right contains weights iteratively pruning network original size. blue orange green lines distriutions initial weights ﬁrst hidden layer second hidden layer output layer respectively. remaining weights already show impact pruning. ﬁrst second hidden layer’s distributions bimodal peaks mirrored opposite since distributions plot original initializations weights survive pruning process distributions created removing samples formerly normal distribution. peaks graph appear left right tails original normal distribution. missing weights pruned. interestingly pruning occurs training graphs weights training. words distributions emerge small weights training must remained small training. second hidden layer retains center ﬁrst hidden layer indicating weights likely moved training. output distribution closely resembles original normal distribution indicating weights probably moved signiﬁcantly training. contributing factor output distribution prune slower rate meaning effects pruning make take longer appear. figure unit current layer many units previous layer connect left graph ﬁrst hidden layer. middle graph second hidden layer. right graph output layer. blue orange green lines winning tickets iteratively pruned respectively. point line represents single unit; units sorted descending order number connections have. data points collected trials. pattern pruning plays extreme form middles ﬁrst second hidden layer distributions continue hollowed happens output distribution. even extreme-looking input distributions corresponding networks converged faster original network retained accuracy. considering extent particular pruning strategy pursued left imprint winning tickets worth considering impact pruning strategies would broadly whether winning tickets found product pruning strategy pursued whether pruning strategy pursued happens exploit deeper reality neural networks behave. architecture. network pruned becomes sparser. figure shows distributions surviving connections aggregated across trials unit layer units layer network pruned original size. left middle right graphs show ﬁrst hidden layer second hidden layer output layer. pruned network remains almost fully-connected slight differences units least connections. network pruned units ﬁrst hidden layer continue roughly equal number connections units input layer. even network pruned small fraction hidden units ﬁrst layer eliminated entirely. second hidden layer becomes less evenly connected weights pruned. time network pruned nearly third units second hidden layer fully disconnected steep decline best-connected units worst-connected. output layer shows less severe slope likely every output unit serves clear function prune output layer slower rate. winning tickets quite sparse. even network pruned nearly fraction units eliminated entirely. units maintain large number connections pruning; instead nearly units retain proportionally small number connections. section explores sensitivity mnist results parameters lottery ticket experiment. namely explore role initialization network size play properties winning tickets emerge. although default network initialized normal distribution mean standard deviation experimented several standard deviations explore effect larger smaller weights behavior winning tickets. might expect pruning strategy would especially vulnerable initializing network weights large selecting highestmagnitude weights might exacerbate exploding gradients. likewise might resilient initializing network weights small since select largest weights training. section present results using one-shot pruning strategy. results iterative pruning similar. figure shows convergence times accuracy winning tickets networks initialized standard deviations larger expected convergence times increase accuracy decreases standard deviations increase. explore whether extent behavior resulted exploding gradients weaknesses pruning strategy. figure contains information winning tickets networks initialized standard deviations smaller standard deviation produces fastest convergence times cedes certain amount accuracy contrast standard deviation causes winning tickets converge slowly higher-accuracy optima. behavior suggests sweet spots convergence times accuracy tradeoff-space between. experimented increasing size default network order determine whether ﬁxed winning ticket size particular learning problem whether larger networks naturally beget larger winning tickets. consider possible deﬁnitions size network’s winning ticket winning ticket minimal network minimizes convergence time. since convergence times initially decrease pruning heuristic looks winning ticket lowest possible convergence time. figure convergence times accuracy groups winning tickets extracted networks various sizes one-shot pruning strategy. error bars elided improve readability. legend contains size network networks initialized standard deviation figure convergence times accuracy groups winning tickets extracted iteratively networks various sizes. error bars elided improve readability. networks initialized standard deviation one-shot pruning. trained networks whose sizes multiples original network size. results applying one-shot pruning strategy appear figure plots convergence times accuracy according number weights winning ticket. according convergence-based deﬁnition winning ticket winning ticket sizes increase gradually size network. lenet-- architecture appears reach point weights lenet-- weights. pattern holds larger architectures. larger networks capable representing sophisticated functions pruning larger networks produce different network architectures exploit additional representation capacity converge faster. indeed larger network lower convergence times winning tickets able achieve larger size reached them. accuracy-based deﬁnition winning ticket agreed. bottom graph figure illustrates accuracy larger networks dropped steeply slightly earlier times accuracy smaller networks. however differences quite small—on order tens thousands weights. although winning ticket size seem increase network size deﬁnition changes slight winning ticket sizes close uniform. iterative pruning. figure reﬂects convergence accuracy trends iteratively pruning larger networks remains one-shot case. larger networks reach minimum convergence times gradually larger sizes accuracy plummets unison. differences worth noting iterative case. figure convergence times accuracy winning tickets extracted iteratively pruning different rates iteration. error bars elided readability. note x-axis logarithmic. first minimum convergence times accuracy dropoffs occur much smaller network sizes one-shot experiments. result coincides iterative experiments demonstrate iterative pruning creates winning tickets pruned much smaller sizes convergence times increase accuracy diminishes. whereas accuracy dropoff took place networks weights one-shot experiments occurs iteratively-derived winning tickets tens thousands weights. second accuracy graphs small bulge upwards dropping indicating accuracy actually increases slightly winning tickets smallest. bulges occur winning ticket size cases regardless initial size network. summary. analysis subsection leaves many open questions future research. although undertake extensive analysis internal structure winning tickets study comparing equally-sized winning tickets derived networks different sizes would shed light extent winning tickets similar different various initial network sizes. choosing exact rate prune iteration iterative pruning entails balancing performance resulting winning ticket number iterations necessary extract winning ticket. figure shows convergence times accuracy lenet-- architecture iteratively pruned different rates iteration. experiment thought exploring middle grounds one-shot pruning iteratively pruning small rate. although pruning larger percentage iteration reaches smaller winning tickets faster winning tickets pruned aggressively fail match convergence times accuracy winning tickets pruned slowly. spectrum iteratively pruning appears achieve best convergence times accuracy would require training network times extract winning ticket original network’s size. experiments prune balances performance amount training required. training iteration iterative pruning approach reset weights unpruned connections original values training. part experiment evaluate lottery ticket hypothesis exploring well winning tickets obtained pruning train isolation. conjecture resetting training iteration makes easier small winning tickets. effect iteration recursive pruning problem subnetwork trains effectively starting original initializations must pruned slightly smaller network same. contrast interleave training pruning without ever resetting weights. round training low-magnitude weights pruned training continues based trained figure convergence times accuracy winning tickets extracted iteratively pruning using weight resetting iterations continuing trained weights pruning strategy orange). weights. differences approaches reﬂect different goals want produce smallest possible trained network wish pruned network trains successfully start. figure shows convergence times accuracy achieved winning tickets extracted using pruning strategies. simulate al.’s strategy iteratively trained network pruned low-magnitude weights continued training using trained weights. iteration copied resulting network reset weights original initializations trained network obtain results figure figure shows al.’s pruning strategy quite effective ﬁnding small networks rain successfully although strategy resetting weights iteration maintains lower convergence times higher accuracy slightly longer. however since figure logarithmic scale differences appear small network sizes. pruning. lecun ﬁrst explored pruning reduce size neural networks pruned based second derivative loss function respect weight. hassibi build approach. recently showed techniques could used substantially reduce size modern image-recognition networks. since then rich variety neural network pruning approaches emerged pruning units bayesian fashion pruning entire convolutional ﬁlters fusing redundant units increase network diversity goal literature pruning compress trained neural networks reducing size large model efﬁciently restricted computational platform without sacriﬁcing accuracy. contrast make possible train small neural networks start. follow-up work network compression takes place three iterative steps. first large network trained. second weights units pruned according heuristic. third network trained using already-trained weights. that without third retraining step network performance drops much earlier pruning process. also caution pruned network re-initialized training consider reusing values surviving weights initialized original network work builds literature pruning shedding light mechanisms make pruning possible. fact networks pruned maintaining accuracy indicates function learned represented much smaller network used training. understand pruning possible investigate whether small networks trained directly lottery ticket hypothesis posits large networks small fortuitously-initialized subnetworks facilitate successful training. point view neural network pruning ﬁnds winning tickets. evaluate lottery ticket hypothesis small fully-connected networks leverage al.’s experimental approach except make crucial modiﬁcation pruning reset weight original value. results explain complement lottery ticket hypothesis offers insight able prune networks. many trends parallel continuing train pruned networks based trained weights. dropout. dropout creates smaller subnetwork training iteration randomly removing subset units inference-time unit’s activation reduced probability dropped out. intuitively dropout intended reduce overﬁtting improve generalization forcing units remain robust changes network. follow-up work dropout characterized training dropout perform gradient descent...with respect to...the ensemble possible subnetworks inference dropout approximately computing average ensemble. terminology dropout experiment aims discover single particularly successful member ensemble subnetworks. dropout heuristic that training network without dropout drop lowest weights probability weights probability words perform extremely aggressive coarse-grained form dropout based examining results training network without dropout. however goal different. dropout designed regularize network training process used produce sparse networks. directly small networks trained start ﬁnish without removing weights. broader formulation lottery ticket hypothesis closely relate dropout’s notion ensemble learning. lottery ticket hypothesis views randomly-initialized large network collection combinatorial number small networks must initialized fortuitously enable training succeed. point view large network begins possibility coalescing toward exponential number subnetworks gradient descent drives toward subnetwork comprising winning ticket ﬁnd. work limited several ways. examine fully-connected networks smallest possible examples consider convolutional networks larger networks better reﬂect real-world examples. evidence lottery ticket hypothesis purely experimental; offer theoretical analysis formally support claim. finally although analyze structure initialization distributions winning tickets mnist devise turn observations useful strategies training smaller networks. anticipate exploring avenues future work updating paper paper proposes hypothesis explain large neural networks amenable substantial pruning pruned networks cannot trained effectively scratch. conjecture known lottery ticket hypothesis holds training succeeds subcomponent larger network randomly initialized fashion suitable optimization. furthermore conjectures pruning uncovers winning tickets. empirically evaluate hypothesis devised experiment based work where pruning trained network remaining weights reset original initializations. lottery ticket hypothesis holds pruning uncovers winning tickets pruned networks train successfully isolation reset original initializations. found winning tickets derived larger networks able learn decision boundary reach zero loss frequently randomly initialized. mnist winning tickets converged quickly reached higher accuracy original network. control experiments supported claim winning tickets represent conﬂuence fortuitious initialization network architecture. paper articulates perspective neural network training supports view empirically. foundation laid numerous research directions evaluate lottery ticket hypothesis exploit perspective improve network design training. larger examples. largest network examine fully-connected network mnist. repeating experiments outlined paper convolutional network larger networks harder learning tasks would make possible understand whether lottery ticket hypothesis holds generally manifests settings. understanding winning tickets. paper focuses mainly behavioral properties lottery ticket hypothesis pruning winning tickets. logical next step systematically analyze architectures initializations lottery tickets. extent winning tickets unique artifacts created randomly initializing large networks getting lucky? extent common structure multiple winning tickets task? winning tickets tell functions neural networks learn particular tasks? lottery ticket networks. lottery ticket hypothesis existence winning tickets demonstrate small networks trained start ﬁnish. concrete follow-up work would exploit lessons learned leveraging winning tickets develop network architectures initialization regimes allow smaller networks trained wider variety learning tasks. could reduce amount computation needed train neural networks. song huizi william dally. deep compression compressing deep neural network pruning trained quantization huffman coding. corr abs/. arxiv. http//arxiv.org/abs/. song jeff pool john tran william dally. learning weights connections efﬁcient neural network. advances neural information processing systems. nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research", "year": 2018}