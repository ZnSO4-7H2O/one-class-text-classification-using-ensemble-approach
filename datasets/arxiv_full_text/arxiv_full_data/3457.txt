{"title": "Low-shot learning with large-scale diffusion", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "This paper considers the problem of inferring image labels for which only a few labelled examples are available at training time. This setup is often referred to as low-shot learning in the literature, where a standard approach is to re-train the last few layers of a convolutional neural network learned on separate classes. We consider a semi-supervised setting in which we exploit a large collection of images to support label propagation. This is made possible by leveraging the recent advances on large-scale similarity graph construction. We show that despite its conceptual simplicity, scaling up label propagation to up hundred millions of images leads to state of the art accuracy in the low-shot learning regime.", "text": "paper considers problem inferring image labels labelled examples available training time. setup often referred low-shot learning literature standard approach re-train last layers convolutional neural network learned separate classes. consider semi-supervised setting exploit large collection images support label propagation. made possible leveraging recent advances large-scale similarity graph construction. show despite conceptual simplicity scaling label propagation hundred millions images leads state accuracy low-shot learning regime. large diverse collections images commonplace; often contain long tail visual concepts. concepts like person appear many images vast majority visual classes occur frequently. even though total number images large hard collect enough labeled data visual concepts. thus want learn them must labeled examples. task named low-shot learning literature. order learn classes little supervision standard approach leverage classiﬁers already learned frequent classes employing so-called transfer learning strategy. instance classes labels last layers convolutional neural network re-trained. limits number parameters need learned limits over-ﬁtting. paper consider low-shot learning problem described above goal learn detect visual classes annotated images class also assume many unlabelled images. called semi-supervised learning motivation work threefold. first want show modern computational tools classical semi-supervised learning methods scale gracefully hundreds millions unlabeled points. limiting factor previous evaluations cost construction similarity graph supporting diffusion. longer bottleneck thanks advances computing architectures algorithms routinely compute similarity graph millions images hours second want answer question exploiting large number images help semi-supervised learning? finally comparing results methods imagenet yfccm dataset highlight methods exhibit artiﬁcial aspects imagenet inﬂuence performance shot learning algorithms. summary contribution paper study semi-supervised learning considered scenario large number unlabeled images. main results setting semi-supervised learning leads state low-shot learning performance. detail make following contributions carry large-scale evaluation diffusion methods semi-supervised learning compare recent low-shot learning papers. experiments carried public benchmark imagenet yfcm dataset show approach efﬁcient diffusion process scales hundreds millions images order magnitude larger aware literature image-based diffusion made possible leveraging recent state efﬁcient k-nearest neighbor graph construction evaluate several variants hypotheses involved diffusion methods exploiting class frequency priors scenario realistic situations statistic known priori. propose simple estimate exploit without prior knowledge extend assumption multiclass setting introducing probabilistic projection step derived sinkhorn-knopp algorithm. experimental study shows simple propagation process carried large many unlabelled images signiﬁcantly outperforms state-of-the-art approaches low-shot visual learning number annotated images class small number unlabeled images large. paper organized follows. section reviews related works section describes label propagation methods variants. experimental study presented section conclusion section summarizes ﬁndings. low-shot learning recently renewed interest low-shot learning i.e. learning examples exploiting prior learning classes. works include metric learning learning regularization feature hallucination predicting parameters network ravi larochelle introduce meta-learner learn optimization parameters invovled low-shot learning regime works consider small datasets like omniglot cifar small subset imagenet. paper focus solely large datasets particular imagenet collection associated ilsvrc challenge. diffusion methods refer reader review diffusion processes matrix normalization options. methods efﬁcient clustering images given matrix input similarity graph successfully used semi-supervised discovery setup share connections spectral clustering graph clustered spectral clustering amounts computing eigenvectors associated largest eigenvalues graph clustering eigenvectors. since eigenvalues obtained lanczos iterations basic operation similar diffusion process. also related power iteration clustering work clusters. semi-supervised learning graph used transductive semi-supervised learning introduction). transductive learning relatively small number labels used augment large unlabeled data goal extend labeling unlabeled data semi-supervised learning similar except separate test points seen train time. work consider simple proposal powers graph used smooth functions graph desired values labeled points. many variations algorithms e.g. zhou weight edges based distances introduce loss trading classiﬁcation ﬁtting constraint smoothness term enforcing consistency neighboring nodes. label propagation transductive method. order evaluate data i.e. classes need extend smooth functions training data. standard method weighted nearest neighbors training data instead deep convolutional network trained similarly used build features. deep networks used sample extension e.g. unsuccessfully successfully speech domain. furthermore addition regressing actual predicted labels label propagation also show results regressing distributions similar efﬁcient knn-graph construction diffusion methods matrix input containing similarity images dataset. considering images e.g. possible store matrix size however image pairs related similarity close therefore diffusion methods usually implemented sparse matrices. means compute graph connecting image neighbors determined similarity metric image representations. particular consider k-nearest neighbor graph vectors. several approximate algorithms proposed efﬁciently produce graph used input iterative/diffusion methods since operation figure diffusion setup. arrows indicate direction diffusion. diffusion performed test rest graph edges bidirectional except mentioned otherwise edges weights. section describes initial stage proposal estimates class unlabelled images diffusion process. includes image description step construction graph connecting similar images label diffusion algorithm. meaningful semantic image representation associated metric required match instances classes seen beforehand. early works semi-supervised labelling using ad-hoc semantic global descriptors like gist classiﬁcation performance substantially improved last years deep architectures compelling choice purpose. therefore image description extract activation maps trained base classes independent novel classes evaluation performed. experimental section details training process descriptors. mean class classiﬁer introduced low-shot learning another perform dimensionality reduction improving accuracy thanks better comparison metric. consider approach since seen part descriptor learning. discussed related work diffusion processes input graph representing sparse similarity matrix denoted connects images collection. build graph using approximate k-nearest neighbor search. thanks recent advances efﬁcient similarity search trading accuracy efﬁciency drastically improves graph construction time. example faiss library building graph associated images takes minutes gpu. preliminary experiments approximation knn-graph construction induce sub-optimality possibly diffusion process compensates artifacts induced approximation. different strategies exist weights afﬁnity matrix choose search nearest neighbors image neighbors corresponding sparse matrix symmetrize matrix adding transpose. subsequently -normalize rows produce sparse stochastic matrix diagonal matrix sums. handling test points different test points participate label propagation classify independently others. therefore outgoing edges test points incoming edges nearest neighbors. give details diffusion process itself summarized figure build straightforward label propagation algorithm images perform diffusion composed labelled seed images unlabelled background images deﬁne matrix number classes want diffuse labels i.e. classes seen training set. associated given image represents probabilities class image. given column corresponds given class gives probabilities image. method initializes one-hot vector seeds. background images initialized probabilities classes. diffusing known labels method iterates wlt. optionally reset rows corresponding seeds -hot ground-truth. iterating convergence would eventually converge eigenvector largest eigenvalue harmonic function respect boundary conditions given seeds empirically low-shot learning observe resetting negative impact accuracy. also early stopping performs better cases cross-validate number diffusion iterations. classiﬁcation decision combination logistic regression predict class test example column maximizes score similar zhou also optimized loss balancing ﬁtting constraint diffusion smoothing term. however found simple late fusion scores produced diffusion logistic regression achieves better results. exploiting priors label propagation take account several priors depending assumptions problem integrated deﬁning normalization operator modifying update equation multiclass assumption. instance ilsvrc challenge built upon imagenet dataset label class therefore deﬁne function -normalizes provide distribution labels convention effect -norm class frequency priors. additionally point labels evenly distributed imagenet. translate setup semi-unsupervised setting would mean assume distribution unlabelled images uniform labels. assumption taken account deﬁning function performing normalization columns could argue realistic general realistic scenario consider know marginal distribution labels proposed show prior simply enforced arises situations prediction empirically measure relative probabilities tags possibly regularized lowest values. combined multiclass assumption class frequency priors. propose variant exploit multiclass setting prior class probabilities enforcing matrix jointly satisfy following properties prior distribution labels. purpose adopt strategy similar cuturi work optimal transport shows sinkhorn-knopp algorithm provides efﬁcient theoretically grounded project matrix satisﬁes marginals. sinkhorn-knopp algorithm iterates alternately enforcing marginal conditions convergence. assume algorithm operates rows columns whose strictly positive. discussed knight convergence algorithm fast. therefore stop iterations. projection performed update eqn. note solely considered second constraint eqn. obtained enforcing prior discussed bengio evaluate variants experimental section non-linear updates. markov clustering another diffusion algorithm nonlinear updates originally proposed clustering. contrast previous algorithm iterates directly similarity matrix element-wise raising power matrix followed column-wise normalization power bandwidth parameter high small edges quickly vanish along iterations. smaller preserves edges longer. clustering performed extracting connected components ﬁnal matrix. section evaluate role non-linear update introducing non-linearity diffusion procedure. precisely modify equation complexity evaluation distinguish stages. off-line stage trained base classes descriptors extracted background images knn-graph computed background images. on-line stage receive training test images novel classes compute features them complement knn-graph matrix include training test images perform diffusion iterations. assume graph matrix decomposed four blocks largest matrix }nb×nb computed off-line. on-line compute three matrices. combine merging similarity search result lists hence contains exactly non-zero elements. requires store distances along wbb. mostly interested complexity on-line phase. therefore exclude descriptor extraction independent classiﬁcation cost cost handling test images negligible compared training operations. consider logistic regression baseline complexity comparison logistic regression training cost multiply-adds denotes descriptor dimensionality number classes. number iterations batch size ilogreg diffusion cost decomposed into computing matrices involves multiply-adds using brute-force distance computations; performing idif iterations sparse-dense matrix multiplications incurs multiply-adds therefore diffusion cost linear number background images appendix details. memory usage. important bottleneck algorithm memory usage. sparse matrix occupies bytes almost twice amount nearest neighbors reciprocal; matrix bytes. fortunately iterations performed column time reducing bytes columns time). imagenet follow recent setup previously introduced low-shot learning. imagenet classes split randomly groups containing base novel classes. group used hyper-parameter tuning group testing ﬁxed hyper-parameters. assume full imagenet training data available base classes. novel classes images class available training. similar subset images drawn randomly random selection performed times different random seeds. large source unlabelled images yfccm dataset consists million representative images flickr photo sharing site. note works used dataset tags metadata weak supervision learning image descriptors. similarly train -layer resnet base classes ensure description calculation never seen image novel classes. images extract -dim vector layer last fully connected layer. descriptor used directly input logistic regression. diffusion pca-reduce feature vector dimensions l-normalize standardly done prior works unsupervised image matching pre-learned image representations performance measure baseline given group classify imagenet validation images base novel classes measure top- accuracy. therefore class distribution heavily unbalanced. since seed images drawn randomly repeat random draws times different random seeds average obtained top- accuracy baseline logistic regression applied labelled points. employ per-class image sampling strategy circumvent unbalanced number examples class. optimize learning rate batch size regularization factor logistic regression group images. none diffusion directly seed images test images; in-domain setting background images imagenet training image novel classes without labels. corresponds case images known belong classes subset labelled; out-of-domain setting background images taken yfccm. denote setting depending number images corresponds challenging setting prior knowledge image used diffusion. compare settings diffusion algorithm interest. cases number nearest neighbors evaluate nearest neighbors computed faiss using ivfflat algorithm computes exact distances occasionally miss neighbors graph edge weighting. experimented different edge weightings proposed literature. compared constant weight gaussian weighting weighting based meaningful neighbors theory table shows results remarkably independent choice edge weightings best normalization applied matrix simple column-wise normalization. thanks linear iteration formula applied iterations. class-speciﬁc weighting out-of-domain diffusion many background images distribution classes determined seed test images predominantly background images. therefore class prior applied images adjusted match background. this classify yfccm images using logistic regression classiﬁer resulting class distribution prior per-class normalization. since estimated class distribution relatively make peaky using power transform gives trade-off uniform distribution. single hyper-parameter cross-validated. table variations weighting edges normalization steps iterates tests performed runs group validation images. variants require parameter indicated case report best result appendix material full results. rest paper variants indicated bold since simple parameter. figure classiﬁcation performance left accuracy function iteration right various settings ordered total number edges appendix material provides complementary analysis. figure reports experiments varying number background images number neighbors appendix also show fast ﬁlls maximum accuracy also reached quickly. maximum accuracy occurs later larger smaller. plot also shows important early stopping. plot right reports large-scale behavior diffusion. curves optimal point terms accuracy computational cost intrinsic property descriptor manifold. worthwhile note starting diffusion iterations background images obtain accuracy knn-classiﬁer fastest setting knn-graph need constructed. another advantage require store graph. table comparison classiﬁers different values diffusion results. out-of-domain setup none column indicates diffusion solely relies labelled images. in-domain diffusion corresponds column imagenet. results most-right column state-of-the-art benchmark knowledge generally outperforming results matching networks model regression setting. graph construction time linear thanks pre-computation graph matrix background images comparison training logistic regression takes depending cross-validated parameters. terms memory usage biggest experiments need simultaneously keep matrix billion non-zero values columns). main drawback using diffusion. however table shows restricting diffusion million images already provides gain dividing order magnitude memory computation complexity. compare performance diffusion baseline classiﬁers table low-shot learning in-domain diffusion outperforms methods large margin. stated section include test points diffusion standard classiﬁcation setting. however allow this fully transductive setting obtain top- accuracy .%±. diffusion i.e. diffusion following comment out-of-domain setting. diffusion outperforms logistic regression classiﬁer combination. experimented simple late fusion combine scores classiﬁers simply take weighted average predictions cross validate weight factor. table shows result signiﬁcantly best input classiﬁers. shows logistic regression classiﬁer diffusion classiﬁer access different aspects image collection. also experimented complicated combination methods like using graph edges regularizer logistic regression improve result. combination outperforms state-of-the-art result setting). remarkable method complementary combination speciﬁc loss learned data augmentation procedure speciﬁcally tailored experimental setup base novel classes. contrast diffusion procedure generic parameters conclusion experimented large-scale label propagation low-shot learning. unsurprisingly found performing diffusion images domain works much better images different domain. clearly observe that number images diffuse grows accuracy steadily improve. main performance factor total number edges also reasonably reﬂects complexity. also report neutral results sophisticated variants instance show edge weights useful. furthermore labeled images included diffusion process used sources i.e. enforced keep label. main outcome study show diffusion large image superior state-ofthe-art methods low-shot learning labels available. interestingly late-fusion standard classiﬁer’s result effective shows complementary approaches.", "year": 2017}