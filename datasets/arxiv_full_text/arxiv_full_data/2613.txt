{"title": "Recursive Decomposition for Nonconvex Optimization", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Continuous optimization is an important problem in many areas of AI, including vision, robotics, probabilistic inference, and machine learning. Unfortunately, most real-world optimization problems are nonconvex, causing standard convex techniques to find only local optima, even with extensions like random restarts and simulated annealing. We observe that, in many cases, the local modes of the objective function have combinatorial structure, and thus ideas from combinatorial optimization can be brought to bear. Based on this, we propose a problem-decomposition approach to nonconvex optimization. Similarly to DPLL-style SAT solvers and recursive conditioning in probabilistic inference, our algorithm, RDIS, recursively sets variables so as to simplify and decompose the objective function into approximately independent sub-functions, until the remaining functions are simple enough to be optimized by standard techniques like gradient descent. The variables to set are chosen by graph partitioning, ensuring decomposition whenever possible. We show analytically that RDIS can solve a broad class of nonconvex optimization problems exponentially faster than gradient descent with random restarts. Experimentally, RDIS outperforms standard techniques on problems like structure from motion and protein folding.", "text": "continuous optimization important problem many areas including vision robotics probabilistic inference machine learning. unfortunately real-world optimization problems nonconvex causing standard convex techniques local optima even extensions like random restarts simulated annealing. observe that many cases local modes objective function combinatorial structure thus ideas combinatorial optimization brought bear. based this propose problem-decomposition approach nonconvex optimization. similarly dpll-style solvers recursive conditioning probabilistic inference algorithm rdis recursively sets variables simplify decompose objective function approximately independent subfunctions remaining functions simple enough optimized standard techniques like gradient descent. variables chosen graph partitioning ensuring decomposition whenever possible. show analytically rdis solve broad class nonconvex optimization problems exponentially faster gradient descent random restarts. experimentally rdis outperforms standard techniques problems like structure motion protein folding. introduction systems interact real world often solve continuous optimization problems. convex problems local optima many sophisticated algorithms exist. however continuous optimization problems related ﬁelds nonconvex often exponential number local optima. problems standard solution apply convex optimizers multistart randomization techniques problems exponential number optima typically fail global optimum reasonable amount time. branch bound methods also used scale poorly curse dimensionality paper propose problems instead approached using problem decomposition techniques long successful history solving discrete problems repeatedly decomposing problem independently solvable subproblems algorithms often solve polynomial time problems would otherwise take exponential time. main difﬁculty nonconvex optimization combinatorial structure modes convex optimization randomization illequipped deal with problem decomposition techniques well suited thus propose novel nonconvex optimization algorithm uses recursive decomposition handle hard combinatorial core problem leaving simpler subproblems solved using standard continuous optimizers. main challenges applying problem decomposition continuous problems extending handle continuous values deﬁning appropriate notion local structure. former embedding continuous optimizers within problem decomposition search manner reminiscent satisﬁability modulo theory solvers continuous optimization decision problems. latter observing many continuous objective functions approximately locally decomposable sense setting subset variables causes rest break subsets optimized nearly independently. particularly true objective function terms subsets variables typically case. number continuous optimization techniques employ static global decomposition partially separable methods many problems decompose locally dynamically algorithm accomplishes. example consider protein folding process protein consisting chain amino acids assumes functional shape. computational problem predict ﬁnal conformation minimizing highly nonconvex energy function consisting mainly pairwise distance-based terms representing chemical bonds electrostatic forces etc. physically conformation atom near small number atoms must rest; thus many terms negligible speciﬁc conformation term non-negligible conformation. suggests sections protein could optimized independently terms connecting negligible that global level never true. however positions atoms appropriately certain amino acids never interact making possible decompose problem multiple independent subproblems solve separately. local recursive decomposition algorithm continuous problems exactly this. ﬁrst deﬁne local structure present algorithm rdis ﬁnds global optimum nonconvex function ecursively ecomposing function locally ndependent ubspaces. analysis show rdis achieves exponential speedup versus traditional techniques nonconvex optimization gradient descent restarts grid search result supported empirically rdis signiﬁcantly outperforms standard nonconvex optimization algorithms three challenging domains structure motion highly multimodal test functions protein folding. section presents nonconvex optimization algorithm rdis. ﬁrst present notation deﬁne local structure method realizing describe rdis provide pseudocode. unconstrained optimization goal minimize objective function variables focus functions continuously differentiable nonempty optimal optimal value indices r|c| restriction indices domain partial assignment variables corresponding indices assigned values. deﬁne x|ρc∈ rn−|c| subspace variables indices values given partial assignment then slight abuse notation deﬁne restriction function domain x|ρc f|ρc following directly partition instead discussing partition induces local structure function fully decomposable exi= functions easy optimize since decompose respect minimization; minxi conversely decomposable nonconvex functions optimized without ﬁrst decomposing require exponentially exploration global optimum decomposed version. example modes modes knowing decomposable allows i=|mi| modes explore. however instead optimized dii=|mi| modes exponential unfortunately fully decomposable functions like rare variables generally appear multiple terms many different variables thus minimization trivially distribute. however decomposition still achieved function exhibits global local structure deﬁne here. deﬁnition globally decomposable exists partition that every partial assignment f|ρc f|ρc f|ρc space x|ρc f|ρc f|ρc f|ρc subspace x|ρc global structure easiest exploit also least prevalent. local structure initially appear limited subsumes global structure also allowing different decompositions throughout space making strictly general. similarly approximate local structure subsumes local structure. protein folding example amino acids pushed either close together apart different conﬁgurations amino acids. latter case optimized independently because terms connecting negligible. thus different partial conﬁgurations protein different approximate decompositions possible. independent subspaces result local decomposition exhibit local structure allowing decomposed turn. algorithm exploits local structure effectively never perform full combinatorial optimization. local structure need exist everywhere space regions explored. convenience refer local structure below unless distinction global local decomposition relevant. method achieving local decomposition simpliﬁcation. simpliﬁable subspace x|ρc deﬁned partial assignment given fi|ρc fi|ρc refer upper lower bounds respectively. similarly simpliﬁed subspace x|ρc deﬁned partial assignment given simpliﬁable terms fi|ρc replaced fi|ρc function constant terms local decomposition occurs terms simplify minimization distribute independent groups terms variables protein folding example above). given terms function maximum possible error simpliﬁed function versus true function however would require terms simpliﬁed true values bounds extremely unlikely; rather errors different terms often cancel simpliﬁed function tends remain accurate. note induces tradeoff acceptable error function evaluation computational cost optimization since simpliﬁed function fewer terms thus evaluating computing gradient cheaper. deﬁnition sums terms mechanism applies functions products factors although error grows multiplicatively here. rdis algorithm rdis optimization method explicitly ﬁnds exploits local decomposition. pseudocode shown algorithm subroutines explained text. level recursion rdis chooses subset variables assigns values simpliﬁed objective function f|ρc decomposes multiple independent subfunctions fi|ρc xuk} partition rdis recurses subfunction globally optimizing conditioned assignment recursion completes rdis uses returned optimal values choose values simpliﬁes decomposes optimizes function again. repeats heuristic stopping criterion satisﬁed. algorithm recursive decomposition locally independent subspaces input function variables initial state subspace opoutput global minimum state function rdis choosevars x\\xc repeat rdis selects variables heuristically goal choosing variables enables largest amount decomposition provides largest computational gains. speciﬁcally rdis uses hypergraph partitioning algorithm determine small cutset decompose figure visualization rdis decomposing objective function. vertices represent variables edges connect pair variables term. left rdis selects middle function simpliﬁcation. thick edges indicate simpliﬁable terms. assigned variables constant removed. right function decomposition. graph; cutset becomes selected variables values determined calling nonconvex subspace optimizer remaining variables ﬁxed current values. subspace optimizer speciﬁed user customizable problem solved. experiments used multi-start versions conjugate gradient descent levenberg-marquardt restarts occur within line converges without making progress restarts point runs reaches local minimum. simplify objective function rdis determines terms simpliﬁable simpliﬁes replacing constant. terms passed recursive calls. variables assigned function simpliﬁed rdis locally decomposes simpliﬁed function independent sub-functions overlapping terms variables thus optimized independently done recursively calling rdis each. figure visualization process. recursion halts choosevars selects occurs small enough subspace optimizer optimize directly. point rdis repeatedly calls subspace optimizer stopping criterion ﬁnds global optimum f|σ∗ since nonconvex optimizer. stopping criterion userspeciﬁed depends subspace optimizer. multistart descent method used termination occurs speciﬁed number restarts corresponding certain probability global optimum found. subspace optimizer grid search loop terminates values assigned. subroutine details provided section analysis present analytical results demonstrating beneﬁts rdis versus standard algorithms nonconvex optimization. formally show rdis explores state space exponentially less time subspace optimizer class functions locally decomposable converge global optimum. space limitations proofs presented appendix number variables number values assigned rdis size form depends subspace optimizer roughly interpreted number modes subfunction f|σ∗ proposition level rdis chooses size |xc|= that selected value simpliﬁed function ˆf|ρc locally decomposes independent sub-functions ˆfi} equal-sized domains time complexity rdis bounded domain width spacing dimension. complexity grid search simply proposition subspace optimizer grid search complexity rdisgs exponentially worse complexity rdisgs decomposition occurs. consider descent method random restarts subspace optimizer. volume basin attraction global minimum volume space probability randomly restarting global basin since restart behavior bernoulli process expected number restarts reach global basin shifted geometric distribution. number iterations needed reach stationary point current basin expected complexity used within rdis obtain following result. proposition subspace optimizer expected value expected complexity rdisdr on/d) shows rdisdr exponentially efﬁcient regarding convergence rdis converges global minimum given certain conditions subspace optimizer. grid search rdisgs returns global minimum grid ﬁnite sufﬁciently spacing. gradient descent restarts rdisdr converge stationary points long steps subspace optimizer satisfy technical conditions. ﬁrst armijo rule guaranteeing sufﬁcient decrease second guarantees sufﬁcient decrease norm gradient appendix a.). conditions necessary show rdisdr behaves like inexact gaussseidel method thus limit point generated sequence stationary point given this state probability rdisdr converge global minimum. proposition non-restart steps rdis satisfy number variables volume global basin volume entire space rdisdr returns global minimum restarts probability proof convergence even convex case since preliminary analysis indicates rare corner cases alternating aspect rdis combined simpliﬁcation error potentially result non-converging sequence values; however experienced practice. furthermore experiments clearly show extremely beneﬁcial especially large highly-connected problems. beyond discrete counterparts rdis related many well-known continuous optimization algorithms. variables chosen level recursion rdis simply reduces executing subspace optimizer. level recursion occurs rdis behaves similarly alternating minimization algorithms multiple levels recursion rdis similarities block coordinate descent algorithms references therein). however sets rdis apart decomposition rdis determined locally dynamically recursively. analysis experiments show exploiting lead substantial performance improvements. rdis subroutines section present speciﬁc choices we’ve made subroutines rdis note others possible intend investigate future work. variable selection. many possible methods exist choosing variables. example heuristics satisﬁability applicable however rdis uses hypergraph partitioning order ensure decomposition whenever possible. hypergraph partitioning splits graph components approximately equal size minimizing number hyperedges cut. maximize decomposition rdis choose smallest block variables that assigned decomposes remaining variables. corresponds exactly edges hypergraph partitioning hypergraph vertex term hyperedge variable connects terms variable rdis maintains hypergraph uses patoh hypergraph partitioning library quickly good approximate partitions. similar idea used darwiche hopkins construct d-trees recursive conditioning; however apply hypergraph partitioning beginning whereas rdis performs level recursion. variable selection could placed inside loop would repeatedly choose variables hypergraph partitioning based graph structure. however rdis still exploits local decomposition variables terms level recursion vary based local structure. addition edge vertex weights could based current bounds local information. value selection. rdis nonconvex optimization subroutine choose values allowing user pick optimizer appropriate domain. experiments multi-start versions conjugate gradient descent levenberg-marquardt possibilities include monte carlo search quasi-newton methods simulated annealing. experimented grid search branch bound found practical easy problems. experiments found helpful stop subspace optimizer early values likely change next iteration making quick approximate improvement effective slow exact improvement. simpliﬁcation decomposition. simpliﬁcation performed checking whether term simpliﬁable setting constant removing function. rdis knows analytical form function uses interval arithmetic general method computing maintaining bounds terms determine simpliﬁability. rdis maintains connected components dynamic graph variables terms components rdis correspond exactly connected components graph. assigned variables simpliﬁed terms removed graph potentially inducing local decomposition. caching branch bound. rdis’ similarity model counting algorithms suggests component caching branch bound experimented found effective used grid search; however beneﬁcial used descentbased subspace optimizers dominate grid-searchbased rdis non-trivial problems. caching because components almost never seen again reencountering variable values even approximately. interval arithmetic bounds tended overly loose bounding occurred. experience suggests because descent-based optimizer effectively focuses exploration minima space typically close value current optimum. however believe future work caching better bounds would beneﬁcial. experimental results evaluated rdis three difﬁcult nonconvex optimization problems hundreds thousands variables structure motion high-dimensional sinusoid protein folding. structure motion important problem vision protein folding core problem computational biology. rdis ﬁxed number restarts level thus guaranteeing found global minimum. structure motion compared rdis domain’s standard technique levenbergmarquardt using levmar library well block-coordinate descent version protein folding gradientbased methods commonly used determine lowest energy conﬁguration protein compared rdis conjugate gradient descent block-coordinate descent version bcd-cgd also used high-dimensional sinusoid. blocks formed grouping contextually-relevant variables together also compared ablated versions rdis. rdis-rnd uses random variable selection heuristic rdis-nrr internal random restarts top-level restarts. domain optimizer compare also used subspace optimizer rdis. experiments cluster. computer cluster identical .ghz quad core intel xeon processors ram. algorithm limited single thread. further details found appendix structure motion. structure motion problem reconstructing geometry scene images scene. consists ﬁrst determining initial estimate parameters performing nonlinear optimization minimize squared error image points projection points onto camera models latter known bundle adjustment task focus here. global structure exists since cameras interact explicitly points creating bipartite graph structure rdis decompose local structure exist bounds term wide tend include dataset used -camera -point data ladybug dataset figure shows performance bundle adjustment function size problem scale y-axis. point minimum error found running algorithm hours. algorithm given restart states algorithms converge faster these. since local structure exploited figure effectively demonstrates beneﬁts using recursive decomposition intelligent variable selection nonconvex optimization. decomposing optimization across independent subspaces allows subspace optimizer move faster further consistently allowing rdis dominate algorithms. missing points algorithms returning results allotted time. high-dimensional sinusoid. second domain highlymultimodal test function deﬁned multidimensional sinusoid placed basin quadratic small slope make global minimum unique. arity function controlled parametrically. functions larger arities contain terms dependencies thus challenging. small amount local structure exists problem. figure show current best value found versus time. datapoint single algorithm using top-level restarts although again algorithms converge faster these. rdis outperforms algorithms including rdis-nrr. nested restart behavior afforded recursive decomposition allows rdis effectively explore subspace escape local minima. poor initial performance rdis arities trapped local minimum early variable assignment performing optimizations lower recursion. however low-level recursions ﬁnish escapes ﬁnds best minimum without ever performing level restart protein folding. ﬁnal domain sidechain placement protein folding continuous angles atoms. amino acids composed backbone segment sidechain. sidechain placement requires setting sidechain angles backbone atoms ﬁxed. equivalent ﬁnding assignment continuous pairwise markov random ﬁeld signiﬁcant local structure present domain. test proteins selected protein data bank sequence length sequences overlap selection internal restarts local structure difﬁcult problem signiﬁcant local structure. algorithm hours proteins varying sizes. rdis results shown ﬁgure. rdis outperforms bcd-cgd proteins often large amount. figure demonstrates effect rdis protein folding. shows performance rdis-nrr function performance measured minimum energy found time taken. rdis-nrr used order remove randomness associated internal restarts rdis resulting accurate comparison across multiple runs. point energy curve minimum energy found restarts. point time curve total time taken restarts. increases time decreases local structure exploited. addition minimum energy actually decreases initially. attribute smoothing caused increased simpliﬁcation allowing rdis avoid minor local minima objective function. conclusion paper proposed approach solving hard nonconvex optimization problems based recursive decomposition. rdis decomposes function approximately locally independent sub-functions optimizes separately recursing them. results exponential reduction time required global optimum. experiments show problem decomposition enables rdis systematically outperform comparable methods. directions future research include applying rdis wide variety nonconvex optimization problems analyzing theoretical properties developing variable value selection methods extending rdis handle hard constraints incorporating discrete variables using similar ideas high-dimensional integration. acknowledgments research partly funded grant wnf-- grants n--- afrl contract fa---. views conclusions contained document authors interpreted necessarily representing ofﬁcial policies either expressed implied afrl united states government. references sameer agarwal noah snavely steven seitz richard szeliski. bundle adjustment large. kostas daniilidis petros maragos nikos paragios editors computer vision eccv volume lecture notes computer science pages springer berlin heidelberg roberto bayardo joseph daniel pehoushek. counting models using connected components. proceedings seventeenth national conference artiﬁcial intelligence pages helen berman john westbrook zukang feng gary gilliland bhat helge weissig ilya shindyalov philip bourne. protein data bank. nucleic acids research adnan darwiche mark hopkins. using recursive decomposition construct elimination orders jointrees dtrees. symbolic quantitative approaches reasoning uncertainty pages springer jacob holm kristian lichtenberg mikkel thorup. poly-logarithmic deterministic fullydynamic algorithms connectivity minimum spanning journal tree -edge biconnectivity. andrew leaver-fay michael tyka steven lewis oliver lange james thompson jacak kristian kaufman douglas renfrew colin smith shefﬂer rosetta object-oriented software suite simulation design macromolecules. methods enzymology matthew moskewicz conor madigan ying zhao lintao zhang sharad malik. chaff engineering efﬁcient solver. proceedings annual design automation conference pages tian sang fahiem bacchus paul beame henry kautz toniann pitassi. combining component caching clause learning effective model counting. seventh international conference theory applications satisﬁability testing tian sang paul beame henry kautz. performing bayesian inference weighted model counting. proceedings twentieth national conference artiﬁcial intelligence volume pages bill triggs philip mclauchlan richard hartley andrew fitzgibbon. bundle adjustment modern synthesis. vision algorithms theory practice pages springer appendix analysis details complexity rdis begins choosing block variables assuming choice made heuristically using patoh library hypergraph partitioning multi-level technique complexity choosing variables linear within loop rdis chooses values simpliﬁes decomposes function ﬁnally recurses. complexity choosing values using subspace optimizer |xc|= call subspace optimizer cheap relative simpliﬁcation requires iterating terms computing bounds linear number terms connected components maintained dynamic graph algorithm amortized complexity operation number vertices graph. finally number iterations loop function dimension since dimensions generally require restarts. proposition level rdis chooses size |xc|= that selected value simpliﬁed function ˆf|ρc locally decomposes independent sub-functions ˆfi} equal-sized domains time complexity rdis recursively calling rdis repeats. non-restart steps subspace optimizer satisfy practical conditions sufﬁcient decrease objective function gradient norm successive partial updates bonettini process equivalent -block inexact gauss-seidel method described bonettini cassioli limit point sequence generated rdis stationary point global minimum reachable restarts. formally superscript indicate recursion level recall decomposition. following proofs focus no-decomposition case clarity; however extension decomposable case trivial since sub-function decomposition independent. denote applying subspace optimizer stopping criterion reached single call subspace optimizer note deﬁnition returns global minimum repeatedly calling equivalent calling forcing parameter. bonettini details. inexact gauss-seidel method deﬁned every method generates sequence conditions hold guaranteed converge critical point rdisdr refer rdis. proposition non-restart steps rdis satisfy number variables volume global basin volume entire space rdisdr returns global minimum restarts probability proof. step given ﬁnite number restarts starts global basin rdisdr recursion returns global minimum satisﬁes seen follows. repeatedly calls equivalent calling returns global minimum thus rdisdr returns global minimum. returning global minimum corresponds step exact gauss-seidel algorithm special case algorithm deﬁnition satisﬁes base case. step rdisdr returns global minimum satisﬁes since rdisdr recurse beyond level. induction step. assume rdisdr show returns global minimum. rdisdr rdisdr ﬁrst partitions blocks iteratively takes folu lowing steps rdisdr ﬁrst simply calls subspace optimizer second recursive call equivalent inductive rdisdr which assumption returns global minimum rdisdr never restart subspace optimizer unless sequence generating converges. thus restart since blocks non-restart steps steps satisfy rdisdr b-igs method generated sequence converges stationary point current basin. level converging rdisdr restart iterate convergence repeat ﬁnite number restarts start global basin thus converge global minimum returned. step finally since probability rdisdr starting global basin probability starting global basin restarts above rdisdr return global minimum starts global basin thus rdisdr return global minimum restarts probability hypergraph partitioning goal split graph components approximately equal size minimizing number hyperedges cut. similarly order maximize decomposition rdis choose smallest block variables that assigned decomposes remaining variables. accordingly rdis constructs hypergraph vertex term hyperedge variable hyperedge connects vertices corresponding term contains variable partitioning resulting cutset smallest variables need removed order decompose hypergraph. since assigning variable constant effectively removes optimization cutset exactly rdis chooses line variable selection typically occupies tiny fraction runtime rdis vast majority rdis’ execution time spent computing gradients subspace optimizer. small non-negligible amount time spent maintaining component graph much efﬁcient recompute connected components time exponential gains decomposition well worth small upkeep costs. structure motion structure motion task goal minimize error dataset points image projection ﬁtted points representing scene’s geometry onto ﬁtted camera models. variables parameters cameras positions points cameras. problem highly-structured global sense cameras interact explicitly points creating bipartite graph structure rdis able exploit. dataset used -camera -point data ladybug dataset number points scaled proportionally number cameras used variables camera variables point. test function deﬁned follows. given height branching factor maximum arity deﬁne complete k-ary tree variables speciﬁed height root. paths tree length sin. resulting function multidimensional nusoid placed basin quadratic function parameterized linear slope deﬁned constant controls amplitude sinusoids. tests used example function shown figure used tree height branching factor resulting function variables. evaluated algorithms functions terms arity larger arity deﬁnes complex dependencies variables well terms function. functions three different arity levels terms respectively. jones potential function speciﬁed rosetta protein folding library basic form function distance atoms constants vary different types atoms. lennard-jones potential rosetta modiﬁed slightly behaves better large small. full energy funcφ amino acid protein torsion angles angles residue zero four torsion angles deﬁne conformation sidechain depending type amino acid. terms compute energy pairs ak)) refer positions atoms residues respectively distance atoms. torsion angles deﬁne positions atoms series kinematic relations detail here. smallest protein residues terms variables largest residues terms variables. average number residues terms variables respectively. proteins paper follows kma. figure trajectories test function data figure main paper. sharp rises show restarts. notably rdis-nrr restarts much often algorithms decomposition allows move space much efﬁciently. without internal restarting gets stuck local minima bcd-cgd cgd. arity rdis never performs full restart still ﬁnds best minimum despite using initial point algorithms. protein folding problem details protein folding process protein consisting long chain amino acids assumes functional shape. computational problem predict ﬁnal conformation given known sequence amino acids. requires minimizing energy function consisting mainly pairwise distance-based terms representing chemical bonds hydrophobic interactions electrostatic forces etc. where simplest case variables relative angles atoms. optimal state typically quite compact amino acids atoms bonded tightly another volume protein minimized. amino acid composed backbone segment sidechain backbone segment amino acid connects neighbors chain sidechains branch backbone segment form bonds distant neighbors. sidechain placement task predict conformation sidechains backbone atoms ﬁxed place.", "year": 2016}