{"title": "Robust Combining of Disparate Classifiers through Order Statistics", "tag": ["cs.LG", "cs.CV", "cs.NE", "I.5.1 ; G.3"], "abstract": "Integrating the outputs of multiple classifiers via combiners or meta-learners has led to substantial improvements in several difficult pattern recognition problems. In the typical setting investigated till now, each classifier is trained on data taken or resampled from a common data set, or (almost) randomly selected subsets thereof, and thus experiences similar quality of training data. However, in certain situations where data is acquired and analyzed on-line at several geographically distributed locations, the quality of data may vary substantially, leading to large discrepancies in performance of individual classifiers. In this article we introduce and investigate a family of classifiers based on order statistics, for robust handling of such cases. Based on a mathematical modeling of how the decision boundaries are affected by order statistic combiners, we derive expressions for the reductions in error expected when such combiners are used. We show analytically that the selection of the median, the maximum and in general, the $i^{th}$ order statistic improves classification performance. Furthermore, we introduce the trim and spread combiners, both based on linear combinations of the ordered classifier outputs, and show that they are quite beneficial in presence of outliers or uneven classifier performance. Experimental results on several public domain data sets corroborate these findings.", "text": "integrating outputs multiple classiﬁers combiners meta-learners substantial improvements several diﬃcult pattern recognition problems. typical setting investigated till classiﬁer trained data taken resampled common data randomly selected subsets thereof thus experiences similar quality training data. however certain situations data acquired analyzed on-line several geographically distributed locations quality data vary substantially leading large discrepancies performance individual classiﬁers. article introduce investigate family classiﬁers based order statistics robust handling cases. based mathematical modeling decision boundaries aﬀected order statistic combiners derive expressions reductions error expected combiners used. show analytically selection median maximum general order statistic improves classiﬁcation performance. furthermore introduce trim spread combiners based linear combinations ordered classiﬁer outputs show quite beneﬁcial presence outliers uneven classiﬁer performance. experimental results several public domain data sets corroborate ﬁndings. since diﬀerent types classiﬁers diﬀerent inductive bias expect generalization performance classiﬁers identical diﬃcult pattern recognition problems even trained data set. best classiﬁer selected based estimation true generalization performance using ﬁnite test valuable information contained results discarded classiﬁers lost. potential loss information avoided outputs available classiﬁers used ﬁnal classiﬁcation decision. concept received great deal attention recently many methods combining classiﬁer outputs proposed furthermore diversity among classiﬁers actively promoted strategies bagging arcing boosting correlation control prelude combining. approaches pooling classiﬁers separated main categories simple combiners e.g. voting bayesian based weighted product rule averaging meta-learners arbitration stacking simple combining methods best suited problems individual classiﬁers perform task comparable success. however combiners susceptible outliers unevenly performing classiﬁers. second category either sets combining rules full ﬂedged classiﬁers acting outputs individual classiﬁers constructed type combining general vulnerable problems associated added learning implicit assumption combining schemes classiﬁer sees training data resampled versions data. individual classiﬁers appropriately chosen trained properly performances comparable region problem space. gains combining derived diversity among classiﬁers rather compensating weak members pool. however real life situations individual classiﬁers access data. conditions arise certain data mining sensor fusion electrical logging problems large variabilities data acquired locally needs processed real time geographically separated places conditions create pool classiﬁers signiﬁcant variations overall performance. moreover lead conditions individual classiﬁers similar average performance substantially diﬀerent performance diﬀerent parts input space. cases combining still desirable neither simple combiners metalearners particularly well-suited type problems arise. example simplicity averaging classiﬁer outputs appealing prospect poor classiﬁer corrupting combiner makes risky choice. weighted averaging classiﬁer outputs appears provide ﬂexibility unfortunately weights still assigned classiﬁer basis rather sample class basis. classiﬁer accurate certain areas input space scheme fails take advantage variable accuracy classiﬁer question. using meta learner provides diﬀerent weights diﬀerent patterns potentially solve problem considerable cost. particular oﬀ-line training meta-learner using substantial amount data outputted geographically distributed classiﬁers feasible. addition providing robustness order statistic combiners presented work also bridging simplicity generality allowing ﬂexible selection classiﬁers without associated cost training meta-classiﬁers. section summarizes relationship classiﬁer errors decision boundaries provides necessary background mathematically analyzing order statistic combiners section introduces simple order statistic combiners. based concepts section propose powerful combiners trim spread derive amount error reduction associated each. section present performance order statistic combiners proben/uci benchmarks section section summarize approach results quantify eﬀect inaccuracies estimating posterior class probabilities classiﬁcation error single classiﬁer. background needed characterize understand impact order statistics combiners described sections well known that given one-of-l desired outputs suﬃcient training samples reﬂecting class priors outputs certain classiﬁers trained minimize mean square cross-entropy error criteria approximate posteriori probability densities corresponding classes based result model output classiﬁer ﬁrst component vary input provides oﬀset systematic error class. second component gives variability systematic error class zero mean variance components error similar bias variance decomposition quadratic loss function given although individual input level. therefore refer classiﬁers biased unbiased implying error selection particular decision boundary. general possible obtain density function boundary oﬀset without making assumptions distributions errors. however ﬁrst order approximation derived leads emphasize distinction biased unbiased classiﬁers model error given function biased classiﬁers. detailed derivation class boundaries error regions presented analyzing error regions combining comparing single classiﬁer case needs determine ﬁrst second moments boundary distributions aﬀected combining. following sections focus obtaining values various combiners. section brieﬂy discuss basic concepts properties order statistics. random variable probability density function cumulative distribution function random sample drawn distribution. arrange non-decreasing order providing corresponding probability density functions obtained equations. general order statistic cumulative distribution function gives probability exactly chosen less equal probability density function given general form however cannot always computed closed form. therefore obtaining expected value function using equation always possible. however ﬁrst moments density function widely available variety distributions moments used compute expected values certain speciﬁc functions e.g. polynomials order less two. three combiners relevant represent important qualitative interpretations output space. selecting maximum combiner equivalent selecting class highest posterior. indeed since network outputs approximate class posteriori distributions selecting maximum reduces selecting classiﬁer certain decision. drawback method however compromised single classiﬁer repeatedly provides high values. selection minimum combiner follows similar logic focuses classes unlikely correct rather correct class. thus combiner eliminates less likely classes basing decision lowest value given class. combiner suﬀers ills combiner. however less dependent single error since performs min-max operation rather max-max. median classiﬁer hand considers typical representation class. highly noisy data combiner desirable either combiners since decision compromised much single large error. i.i.d. ηk’s ﬁrst moments identical class. moreover taking order statistic shift mean amount leaving mean diﬀerence unaﬀected. therefore zero mean variance reduction factor depends order statistic distribution distributions found tabulated form example table provides values order statistic combiners classiﬁers gaussian distribution equation shows reduction error using combiner instead classiﬁer directly related reduction variance boundary oﬀset since means variances order statistics variety distributions widely available tabular form reductions readily quantiﬁed. section analyze error regions biased classiﬁers. return attention bos. first note error terms longer studied separately since general bos. therefore need specify mean variance result operation. equation becomes taking speciﬁc order statistic expression modify moments. ﬁrst moment given shift depends order statistic chosen class. then ﬁrst moment given note bias term represents average bias since contributions order statistic removed. therefore reductions bias cannot obtained table similar table analyzing error reduction general case requires knowledge bias introduced classiﬁer. unlike regression problems bias variance contributions error additive well-understood classiﬁcation problems interaction complex indeed observed ensemble methods simply reduce variance balances contributions error. small value reduce ﬁrst component error leaving second term untouched. eﬀect similar results obtained regression problems. case important reduce classiﬁer bias combining error reduction signiﬁcant second term small negative. fact variation among biases small relative magnitude error reduced unbiased cases. however variation large compared magnitude error reduction minimal. furthermore large biases small highly varied possible combiner worse individual classiﬁers danger present regression problems. observation closely parallels results reported previous section derived error reductions class posteriors directly estimated ordered classiﬁer outputs. since simple averaging also shown provide beneﬁts section investigate combinations averaging order statistics pooling classiﬁer outputs. ﬁrst linear combination ordered classiﬁer outputs study focuses extrema. discussed section maximum minimum classiﬁer outputs carry speciﬁc meanings. indeed maximum viewed class evidence. similarly minimum deletes classes little evidence. order avoid single classiﬁer large impact eventual output values averaged yield spread combiner. combiner strikes balance positive negative evidence leading robust combiner either them. represents covariance variables note ordering variances ﬁrst terms equation expressed terms individual classiﬁer variances. furthermore covariance order statistics also determined tabulated form given distributions. table provides values gaussian distribution based expression simpliﬁed symmetric distributions leads ﬁrst moment bspr obtained analyzing term equation fact oﬀset introduced ﬁrst order statistic classes cancel leaving average bias components error given βspr instead actively using extreme values case spread combiner base posterior estimate around median values. however instead selecting classiﬁer output done multiple classiﬁers whose outputs typical. scheme certain fraction available classiﬁers used given pattern. main advantage method weighted averaging classiﬁers contribute combiner vary pattern pattern. furthermore need determined externally function current pattern classiﬁer responses pattern. again using factors tables equation simpliﬁed. note gaussian distribution symmetric covariance ordered samples ordered samples. therefore equation leads variance ordered sample bmln covariance ordered samples given initial samples unit variance using theory highlighted section equation obtain following model error reduction based equation tables generated sample trim combiner reduction table. many possibilities table exhaustively provides reduction values practical. sample table selected averaging lowest highest values removed. comparison purposes reduction factors averaging combiner classiﬁers also provided similar results obtained regression problems numbers demonstrate although classiﬁers used trim combiner selectively weeding undesirable classiﬁers provides reduction factors signiﬁcantly better simply averaging arbitrary classiﬁers. trim combiner provides reduction factors comparable classiﬁer combiner without susceptible corruption particularly faulty classiﬁer. ﬁrst moment btrim obtained manner similar spread combiner. indeed mean oﬀset introduced speciﬁc order statistic class oﬀset introduced class trimmed mean biases remain giving ﬁrst moment btrim deriving variance btrim follow steps sections resulting boundary variance similar equation since reduction linear combination multiple ordered outputs replaced where need look interaction parts error reduction. ﬁrst term provides error reduction compared model error individual classiﬁer. smaller error reduction second term hand small value useful variability individual biases higher biases seen data schlumberger austin nasa houston unfortunately data sets standard public domain. article restrict public domain datasets simulate variability using early stopping i.e. prematurely terminating training individual classiﬁers. thus combining results ﬁrst reported case half classiﬁers ﬁnely tuned. procedure produces artiﬁcially created quality variation pool classiﬁers. experiments reported below used multi-layer perceptron single hidden layer whose weights randomly initialized run. classiﬁcation results reported article test error rates averaged runs along conﬁdence intervals. several types simple combiners averaging weighted averaging voting median products weighted products using dempster-schafer theory evidence entropy-based averaging proposed literature. however wide variety data sets observed simple averaging usually provides results comparable techniques reason study average combiner representative simple combiners comparison purposes. next data sets selected proben/uci benchmarks proben benchmarks particular training validation test splits data sets available http//www.ics.uci.edu/˜mlearn/mlrepository.html. results presented article based ﬁrst training validation test partition discussed half data used training quarter validation testing purposes. brieﬂy data sets corresponding single layer feed-forward neural network architectures diabetes -dimensional data classes based personal data pima indians obtained national institute diabetes digestive kidney diseases hidden units; highly variable. misclassiﬁcation percentage individual classiﬁers reported ﬁrst column. trimmed mean combiner also provide upper lower cutting points ordered average used equation obtained validation set. sonar data results indicate individual classiﬁer performance highly variable order statistics-based combiners provide better classiﬁcation results simple combiners. performance improvement obtained without sacriﬁcing simplicity combiner. uci/proben benchmarks order statistics based combiners provide better classiﬁcation performance three sets studied important thing note however eight data sets studied order statistics based combiners performed least well simple combiner implying risk taken using method. close inspection results reveals using either combiner provide better classiﬁcation rates diﬃcult determine successful given data set. validation used select other case potentially precious training data used solely determining combiner use. spread combiner removes dilemma consistently providing results comparable better than best max-min duo. important note combiner performs poorly soybean data. data outputs posterior estimates unlikely classes become extremely small highly inaccurate. basing decisions spurious values compromises combiner’s performance. notice however spread combiner adversely aﬀected phenomenon. ample data classiﬁers ﬁnely tuned simple combiners expected adequate. however always possible determine whether conditions lead ideal situation satisﬁed. therefore important know whether trimmed mean spread combiners presented article perform worse simple combiners conditions. combined ﬁnely tuned feed forward neural networks using methods proposed article compared results traditional averaging method. experiments conditions favor averaging combiner results displayed tables indicate that even circumstances spread trim combiners provide results comparable obtained article present analyze combiners based order statistics. combiners blend simplicity averaging generality meta-learners. particularly eﬀective signiﬁcant variations among component classiﬁers least parts joint input-output space. variations arise individual training sets cannot considered random samples common universal data set. examples cases include real-time data acquisition classiﬁcation geographically distributed sources data mining problems large databases random subsampling computationally expensive practical methods lead non-random subsamples furthermore robustness order statistics combiners also helpful certain individual classiﬁers experience catastrophic failures analytical framework provided paper quantiﬁes reductions error achieved order statistics based ensemble used. also shows methods linear combination order statistics introduced paper provide reliable estimates true posteriors individual order statistic combiners. experimental results section indicate high variability among classiﬁers order statistics-based combiners signiﬁcantly outperform simple combiners whereas absence variability combiners perform worse. thus family order statistic combiners able extract appropriate amount information individual classiﬁer outputs without requiring tuning additional parameters meta-learners without substantially aﬀected outliers. future endeavor helpful work well study classiﬁcation based large datasets general obtain suite public domain datasets intrinsically partitioned segments varying quality. though situations sometimes occur practice mortgage scoring data sets proprietary) represented standard venerable databases elena statlog typically used academic community. perhaps recent cross-industry standard process data mining initiative provide satisfactory solution problem near future.", "year": 1999}