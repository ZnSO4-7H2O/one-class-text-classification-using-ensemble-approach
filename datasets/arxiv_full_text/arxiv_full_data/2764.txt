{"title": "Scalable Log Determinants for Gaussian Process Kernel Learning", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "For applications as varied as Bayesian neural networks, determinantal point processes, elliptical graphical models, and kernel learning for Gaussian processes (GPs), one must compute a log determinant of an $n \\times n$ positive definite matrix, and its derivatives - leading to prohibitive $\\mathcal{O}(n^3)$ computations. We propose novel $\\mathcal{O}(n)$ approaches to estimating these quantities from only fast matrix vector multiplications (MVMs). These stochastic approximations are based on Chebyshev, Lanczos, and surrogate models, and converge quickly even for kernel matrices that have challenging spectra. We leverage these approximations to develop a scalable Gaussian process approach to kernel learning. We find that Lanczos is generally superior to Chebyshev for kernel learning, and that a surrogate approach can be highly efficient and accurate with popular kernels.", "text": "applications varied bayesian neural networks determinantal point processes elliptical graphical models kernel learning gaussian processes must compute determinant positive deﬁnite matrix derivatives leading prohibitive computations. propose novel approaches estimating quantities fast matrix vector multiplications stochastic approximations based chebyshev lanczos surrogate models converge quickly even kernel matrices challenging spectra. leverage approximations develop scalable gaussian process approach kernel learning. lanczos generally superior chebyshev kernel learning surrogate approach highly efﬁcient accurate popular kernels. pressing need scalable machine learning approaches extract rich statistical structure large datasets. common bottleneck arising determinantal point processes bayesian neural networks model comparison graphical models gaussian process kernel learning computing determinant large positive deﬁnite matrix. approximate determinants existing stochastic expansions relying matrix vector multiplications approaches make assumptions near-uniform eigenspectra unsuitable machine learning contexts. example popular kernel gives rise rapidly decaying eigenvalues. moreover standard approaches stochastic power series reasonable asymptotic complexity rank matrix require many terms precision necessary machine learning applications. gaussian processes provide principled probabilistic kernel learning framework determinant foundational importance. speciﬁcally marginal likelihood gaussian process probability data given kernel hyper-parameters. utility function kernel learning compartmentalizes automatically calibrated model complexity terms called automatic occam’s razor simplest models explain data automatically favoured without need approaches cross-validation regularization costly heuristic involve substantial hand-tuning human intervention. automatic complexity penalty called occam’s factor determinant kernel matrix related volume solutions expressed gaussian process. many current approaches scalable gaussian processes focus inference assuming ﬁxed kernel approximations allow ﬂexible kernel learning poor scaling number basis functions inducing points. alternatively approaches exploit algebraic structure kernel matrices provide highly expressive kernel learning essentially limited grid structured data. recently wilson nickisch proposed structured kernel interpolation framework generalizes structuring exploiting methods arbitrarily located data. works providing accurate fast matrix vector multiplies kernel matrices used iterative solvers linear conjugate gradients scalable inference. however evaluating marginal likelihood derivatives kernel learning followed scaled eigenvalue approach instead iterative approaches. approach inaccurate relies fast eigendecomposition structured matrix available many consequential situations fast mvms available including additive covariance functions multi-task learning change-points diagonal corrections kernel approximations fiedler weyl bounds used extend scaled eigenvalue approach similarly limited. extensions often approximate apply beyond sums three matrices matrix must fast eigendecomposition. machine learning recently renewed interest based approaches approximating determinants chebyshev lanczos based methods although approaches back least decades quantum chemistry computations independently several authors proposed various methods compute derivatives determinants determinant derivatives needed efﬁcient marginal likelihood learning derivatives required gradient-based optimization determinant needed model comparison comparisons likelihoods local maximizers fast effective choices starting points step sizes gradient-based optimization algorithm. paper develop novel scalable general purpose chebyshev lanczos surrogate approaches efﬁciently accurately computing determinant derivatives simultaneously. methods fast mvms re-use mvms computations. particular derive fast methods simultaneously computing determinant derivatives stochastic chebyshev stochastic lanczos surrogate models mvms alone. also perform error analysis extend approaches higher order derivatives. methods enable fast kernel learning whenever fast mvms possible including applications alternatives scaled eigenvalue methods diagonal corrections better kernel approximations additive covariances multi-task approaches non-gaussian likelihoods. illustrate performance approach several large multi-dimensional datasets including consequential crime prediction problem precipitation problem training points. consider variety kernels including deep kernels diagonal corrections gaussian non-gaussian likelihoods. released code tutorials extension gpml library https //github.com/kd/gpml_sld. python implementation approach also available gpytorch library https//github.com/jrg/gpytorch. using approach conjunction fast mvms kernel learning inducing points training points algebraic approaches also need worry quadratic storage inducing points since symmetric toeplitz kronecker matrices stored linear cost without needing explicitly construct matrix. although fast mvms emphasize proposed iterative approaches generally applicable easily used conjunction method admits fast mvms including classical inducing point methods ﬁnite basis expansions popular stochastic variational approaches moreover stochastic variational approaches naturally combined accelerate mvms start introduction kernel approximations. introduce stochastic trace estimation chebyshev lanczos approximations. describe different sources error approximations. consider experiments several large real-world data sets. conclude supplementary materials also contain several additional experiments details. gaussian process collection random variables ﬁnite number joint gaussian distribution used deﬁne distribution functions function value random variable indexed mean covariance functions process. covariance function often chosen matérn kernel denote kernel hyperparameters vector concise generally explicitly denote dependence associated matrices locations represent vectors function values evaluated matrix whose entry suppose vector corresponding function values entry contaminated independent gaussian noise variance gaussian process prior depending covariance hyperparameters marginal likelihood given ˜kxx optimization expensive since cheapest evaluating ˜kxx| derivatives without taking advantage structure involves computing cholesky factorization ˜kxx. computations expensive inference learning beyond even thousand points. popular approach scalability replace exact kernel approximate kernel admits fast computations several methods approximate inducing points {uj}m rn×n rank low-rank approximation matrix compute allowing solve linear systems involving time. another popular kernel approximation fully independent training conditional diagonal correction diagonal original kernel thus kernel matrices fitc low-rank plus diagonal structure. modiﬁcation exceptional practical signiﬁcance leading improved point predictions much realistic predictive uncertainty making fitc arguably popular approach scalable gaussian processes. wilson nickisch provides mechanism fast mvms proposing structured kernel interpolation approximation n-by-m matrix interpolation weights; authors local cubic interpolation sparse. sparsity makes possible naturally exploit algebraic structure inducing points grid extremely fast matrix vector multiplications approximate even data inputs arbitrarily located. instance toeplitz approximate costs contrast placing inducing points grid classical inducing point methods fitc result substantial performance gains costly cross-covariance matrices matrix logarithm compute traces involved determinant derivative stochastic trace estimators approximate trace matrix using matrix vector products. idea given matrix random probe vector independent entries mean zero variance common choice entries probe vectors rademacher random variables. practice estimate trace sample mean independent probe vectors. often surprisingly probe vectors sufﬁce. estimate need multiply probe vectors. consider ways estimate logz polynomial approximation using connection gaussian quadrature rule lanczos method cases show re-use probe vectors inexpensive coupled estimator derivatives. addition standard radial basis function interpolation determinant evaluated systematically chosen points hyperparameter space inexpensive surrogate determinant. also approximate logz lanczos decomposition; discussion lanczos-based computation stochastic lanczos estimation determinants. steps lanczos algorithm computes decomposition ﬁrst column identity. lanczos algorithm numerically unstable. several practical implementations resolve issue approximation corresponds gauss quadrature rule riemann-stieltjes integral measure associated eigenvalue distribution exact polynomial degree approximation also exact distinct eigenvalues particularly relevant gaussian process regression since frequently kernel matrices small number eigenvalues close zero. lanczos decomposition also allows estimate derivatives determinant minimal cost. lanczos decomposition approximation requires additional matrix vector multiplications beyond used compute lanczos decomposition already used estimate logz; exact arithmetic equivalent steps computing takes additional time; subsequently need matrix-vector multiply ˜k/∂θi probe vector estimate approximation provide poor estimate diagonal entries original kernel matrix kernels limited smoothness matérn kernel. general diagonal corrections scalable kernel approximations lead great performance gains. indeed popular fitc method exactly diagonal correction subset regressors thus modify approximation diagonal matrix diagonal approximated exact. words substracts diagonal adds true diagonal kxx. modiﬁcation possible scaled eigenvalue method approximating determinants since adding diagonal matrix makes impossible approximate eigenvalues eigenvalues however still admits fast mvms thus works approach estimating determinant derivatives. computing costs ﬂops since sparse local cubic interpolation. therefore compute ﬂops. already described stochastic estimators compute marginal likelihood ﬁrst derivatives. approach applies computing higher-order derivatives newton-like iteration understand sensitivity maximum likelihood parameters similar tasks. ﬁrst derivatives full marginal likelihood superﬁcially evaluating second derivatives would appear require several additional solves beyond used estimate ﬁrst derivatives determinant. fact unbiased estimator second derivatives additional solves fast products derivatives kernel matrices. independent probe vectors deﬁne ˜k−z ˜k−w. hence stochastic lanczos method compute determinant derivatives additional work required obtain second derivative estimate second partial kernel probe vector ﬁrst partial kernel products. another deal determinant derivatives evaluate determinant term systematically chosen points space hyperparameters interpolation approximation values. particularly useful kernel depends modest number hyperparameters thus number points need precompute relatively small. refer method surrogate since provides inexpensive substitute determinant derivatives. surrogate approach radial basis function interpolation cubic kernel linear tail. e.g. supplementary material details interpolation. addition usual errors sources solver termination criteria ﬂoating point arithmetic approach kernel learning involves several additional sources error approximate true kernel enables fast mvms approximate traces using stochastic estimation approximate actions probe vectors. compute ﬁrst-order estimates sensitivity likelihood perturbations kernel using stochastic estimators derivatives respect hyperparameters. example lref likelihood reference kernel norms ∂e/∂θi similarly estimate changes gradient likelihood allowing bound marginal likelihood hyperparameter estimates depend kernel approximations. eigenvalues kernel matrix without noise decay rapidly enough compared variance log. hence need fewer probe vectors obtain reasonable accuracy would expect bounds blind matrix structure. experiments typically probes sample variance across probes estimate posteriori stochastic component error likelihood computation. willing estimate hessian likelihood increase rates convergence ﬁnding kernel hyperparameters. chebyshev approximation scheme requires steps obtain approximation error computing logz λmax/λmin condition number behavior independent distribution eigenvalues within interval close optimal eigenvalues spread quasi-uniformly across interval. nonetheless condition number large convergence quite slow. lanczos approach converges least twice fast chebyshev general converges much rapidly eigenvalues uniform within interval case determinants many kernel matrices. hence recommend lanczos approach chebyshev approach general. experiments error associated approximating logz lanczos dominated sources error. test stochastic trace estimator chebyshev lanczos approximation schemes sound time series missing data using kernel; three-dimensional space-time precipitation data half million training points using kernel; two-dimensional tree growth data using log-gaussian process model kernel; three-dimensional space-time crime datasets log-gaussian model matérn spectral mixture kernels; high-dimensional feature space using deep kernel learning framework supplementary material also include several additional experiments illustrate particular aspects approach including kernel hyperparameter recovery diagonal corrections surrogate methods throughout method fast mvms. lanczos surrogate methods able kernel recovery inference signiﬁcantly faster accurately competing methods. consider natural sound benchmark shown figure goal recover contiguous missing regions waveform training points. exploit toeplitz structure matrix approximate kernel accelerated mvms. experiment considered scalable inference prediction hyperparameter learning since scaled eigenvalue approach requires eigenvalues toeplitz matrix computationally prohibitive cost however evaluating marginal likelihood training obstacle lanczos chebyshev since fast mvms approximation cost figure show lanczos chebyshev surrogate approaches scale number inducing points compared scaled eigenvalue method fitc. probe vectors iterations lanczos building surrogate hyperparameter learning lanczos. also probe vectors chebyshev moments. figure shows runtime hyperparameter learning phase different numbers inducing points lanczos surrogate clearly efﬁcient scaled eigenvalues chebyshev. hyperparameter learning fitc took several hours compared minutes alternatives; therefore exclude fitc figure figure shows time inference test points shows standardized mean absolute error test points. expected lanczos surrogate make accurate predictions much faster chebyshev scaled eigenvalues fitc. short lanczos surrogate approach much faster alternatives hyperparameter learning large number inducing points training points. figure sound modeling using training points test points. intensity time series seen train time kernel hyperparameters time inference standardized mean absolute error function time evaluation marginal likelihood derivatives shown surrogate lanczos chebyshev scaled eigenvalues fitc experiment involves precipitation data year collected around weather stations hourly precipitation data preprocessed daily data full information available. dataset entries terms precipitation given date longitude latitude. randomly select data points test points remaining points training. perform hyperparameter learning prediction kernel using lanczos scaled eigenvalues exact methods. lanczos scaled eigenvalues optimize hyperparameters subset data january induced grid points spatial dimension temporal dimension. memory constraints subset entries training exact method. scaled eigenvalues perform well fast eigendecompositions possible experiment lanczos nonetheless still runs faster slightly lower mse. table prediction comparison daily precipitation data showing number training points number induced grid points mean squared error inference time. incidentally able million inducing points lanczos scaled eigenvalues enabled representation covariance matrices accurate approximation. number inducing points unprecedented typical alternatives scale experiment apply lanczos log-gaussian process model laplace approximation posterior distribution. kernel poisson likelihood model. scaled eigenvalue method apply directly non-gaussian likelihoods; thus applied scaled eigenvalue method conjunction fiedler bound scaled eigenvalue comparison. indeed advantage lanczos approach applied whenever fast mvms available means additional approximations fiedler bound required non-gaussian likelihoods. dataset comes package spatstat point pattern hickory trees forest michigan. discretize area grid model exact scaled eigenvalues lanczos. table lanczos recovers hyperparameters much closer exact values scaled eigenvalue approach. figure shows predictions lanczos also indistinguishable exact computation. experiment apply lanczos spectral mixture kernel crime forecasting problem considered dataset consists incidents assault chicago january december ﬁrst years training attempt predict crime rate last years. spatial dimensions log-gaussian process model matérn-/ kernel negative binomial likelihood laplace approximation posterior. spectral mixture kernel components extra constant component temporal dimension. discretize data spatial grid corresponding -by- mile grid cells. temporal dimension data weeks total weeks. removing cells outside chicago total observations. results lanczos scaled eigenvalues seen table lanczos method used hutchinson probe vectors lanczos steps. methods allow iterations lbfgs recover hyperparameters often observe early convergence. rmse lanczos scaled eigenvalues happen close example recovered hyperparameters using scaled eigenvalues different lanczos. example scaled eigenvalue method learns much larger lanczos indicating model misspeciﬁcation. general data become increasingly non-gaussian fiedler bound become increasingly misspeciﬁed lanczos unaffected. table hyperparameters recovered recovery time rmse lanczos scaled eigenvalues chicago assault data. length scales spatial dimensions noise level. trecovery time recovering hyperparameters. tprediction time prediction observations handle high-dimensional datasets bring methods deep kernel learning framework replacing ﬁnal layer pre-trained deep neural network experiment uses sensor dataset machine learning repository. instances dimensions. pre-train attach gaussian process kernels two-dimensional output second-to-last layer. train parameters resulting kernel including weights marginal likelihood. example lanczos scaled eigenvalue approach perform similarly well. nonetheless lanczos effectively used high dimensional problem train hundreds thousands kernel parameters. many cases fast mvms achieved difﬁcult impossible efﬁciently compute determinant. developed framework scalable accurate estimates determinant derivatives relying mvms. particularly consider scalable kernel learning showing promise stochastic lanczos estimation combined pre-computed surrogate model. shown scalability ﬂexibility approach experiments kernel learning several real-world data sets using gaussian non-gaussian likelihoods highly parametrized deep kernels. iterative approaches great promise future exploration. begun explore signiﬁcant generality. addition determinants methods presented could adapted fast posterior sampling diagonal estimation matrix square roots many standard operations. proposed methods depend fast mvms—and structure necessary fast mvms often exists readily created. made create structure. approaches stochastic variational methods could used combined fast mvms moreover iterative methods naturally harmonize acceleration therefore likely increase future applicability popularity. finally could explore ideas presented scalable higher order derivatives making hessian methods greater convergence rates. christos boutsidis petros drineas prabhanjan kambadur eugenia-maria kontopoulou anastasios zouzias. randomized algorithm approximating determinant symmetric positive deﬁnite matrix. arxiv preprint arxiv. joaquin quiñonero-candela carl edward rasmussen. unifying view sparse approximate gaussian process regression. journal machine learning research andrew gordon wilson elad gilboa nehorai arye john cunningham. fast kernel learning multidimensional pattern extrapolation. advances neural information processing systems pages andrew gordon wilson hannes nickisch. kernel interpolation scalable structured gaussian processes international conference machine learning william herlands andrew wilson hannes nickisch seth flaxman daniel neill wilbert panhuis eric xing. scalable gaussian processes characterizing multidimensional change surfaces. artiﬁcial intelligence statistics edward snelson zoubin ghahramani. sparse gaussian processes using pseudo-inputs. advances neural information processing systems volume page press seth flaxman andrew wilson daniel neill hannes nickisch alex smola. fast kronecker inference gaussian processes non-gaussian likelihoods. international conference machine learning pages zhaojun mark fahey gene golub menon richter. computing partial eigenvalue sums electronic structure calculations. technical report tech. report sccm- stanford university andrew gordon wilson zhiting ruslan salakhutdinov eric xing. deep kernel learning. proceedings international conference artiﬁcial intelligence statistics pages carl edward rasmussen hannes nickisch. gaussian processes machine learning toolbox. journal machine learning research andrew wilson zhiting ruslan salakhutdinov eric xing. stochastic variational deep kernel learning. advances neural information processing systems pages bernhard silverman. aspects spline smoothing approach non-parametric regression curve ﬁtting. journal royal statistical society. series pages joaquin quinonero-candela carl edward rasmussen christopher williams. approximation methods gaussian process regression. large-scale kernel machines pages nicholas higham. functions matrices theory computation. siam michael hutchinson. stochastic estimator trace inﬂuence matrix laplacian smoothing splines. communications statistics-simulation computation holger wendland. scattered data approximation volume cambridge university press haim avron sivan toledo. randomized algorithms estimating trace implicit symmetric positive semi-deﬁnite matrix. http//dx.doi.org/./.. popular choices model heavy-tailed correlations function values. spectral behavior kernels well-studied years recommend recent results. particularly relevant discussion theorem weyl says symmetric kernel continuous derivatives eigenvalues associated integral operator decay like |λn| hence eigenvalues kernel matrices smooth kernel tend decay much rapidly less smooth matérn kernel derivatives zero derivative zero derivatives zero matters relative performance chebyshev lanczos approximations determinant large values small values exact approximate kernel. scaled eigenvalue method scaled eigenvalue method introduced estimate |kxx +σi| consists points. eigenvalues {λi}n approximated using largest eigenvalues covariance matrix full grid points speciﬁcally induced kernel plays role scaled eigenvalue method applied eigenvalues efﬁciently computed. assuming eigenvalues computed efﬁciently much stronger assumption fast based approach. radial basis function interpolation popular approaches approximating scattered data general number dimensions given distinct interpolation points {θi}n kernel one-dimensional function space polynomials variables degree many possible choices cubic kernel thin-plate spline kernel log. coefﬁcients determined imposing interpolation conditions discrete orthogonality condition willing price mvms expressions improve maximum likelihood estimate. independent probe vectors ˜k−z ˜k−w. estimate trace derivative computation standard stochastic trace estimation approach together observation linearization used directly alternately estimates ∂e/∂θi substitute order estimated bounds magnitude derivatives. coupled similar estimator hessian likelihood function method compute maximum likelihood parameters fast kernel compute correction −h−∇θlref estimate maximum likelihood parameters reference kernel. experiment compare accuracy lanczos chebyshev -dimensional perturbations true hyper-parameters demonstrate critical diagonal replacement approximate kernels. choose true hyper-parameters consider different types datasets. ﬁrst dataset consists equally spaced points interval case kernel matrix stationary kernel toeplitz make fast matrix-vector multiplication. second dataset consists data points drawn independently distribution. cubic interpolation construct approximate kernel based equally spaced points. function values drawn true hyper-parameters true approximate kernel. iterations lanczos chebyshev moments order assure convergence methods. results ﬁrst dataset matérn kernels seen figure results second dataset kernel seen figure lanczos yields excellent approximation determinant derivatives exact approximate kernels chebyshev struggles large values small values exact approximate kernel. expected since chebyshev issues singularity zero lanczos large quadrature weights close zero compensate singularity. scaled eigenvalue method issues approximate matérn kernel. experiment study performance advantage lanczos chebyshev. figure shows ritz values lanczos quickly converge spectrum kernel thanks absence interior eigenvalues. chebyshev approximation shows expected equioscillation behavior. importantly chebyshev approximation logarithms greatest error near zero majority eigenvalues also heaviest weight determinant. another advantage lanczos requires minimal knowledge spectrum chebyshev needs extremal eigenvalues rescaling. addition lanczos derivatives hyper-parameter chebyshev requires iteration leading extra computation memory usage. figure -dimensional perturbations exact matérn kernel data equally spaced points interval exact values lanczos chebyshev error bars lanczos chebyshev standard deviation computed runs different probe vectors method hurt predictive performance. experiment similar generate uniformly distributed points interval choose small number inducing points large chunk interval inducing point. interested behavior predictive uncertainties subinterval. function values given normally distributed noise standard deviation added function values. optimal hyper-parameters matérn using exact method hyper-parameters make predictions lanczos chebyshev fitc scaled eigenvalue method. consider lanczos without diagonal correction order affects predictions. results seen figure clear lanczos chebyshev conﬁdent predictive mean diagonal correction used predictive uncertainties agree well fitc diagonal correction used. scaled eigenvalue method cannot used efﬁciently diagonal correction leads predictions similar lanczos chebyshev without diagonal correction. ﬂexibility able diagonal correction lanczos chebyshev makes approaches appealing. point experiment illustrate accurate level-curves surrogate model compared level-curves true determinant. consider matérn kernels datasets considered study level curves compare vary building surrogate three hyper-parameters produces similar results requires design points. design points construct cubic linear tail. values determinant derivatives computed lanczos. clear figure surrogate model good approximating determinant kernels. figure -dimensional perturbations approximations matérn kernel data points drawn exact values lanczos diagonal replacement chebyshev diagonal replacement lanczos without diagonal replacement chebyshev without diagonal replacement scaled eigenvalues diagonal replacement makes perceptual difference kernel lines overlapping case. error bars lanczos chebyshev standard deviation computed runs different probe vectors experiments tests well recover hyper-parameters data generated compare chebyshev lanczos surrogate scaled eigenvalue method fitc. consider dataset points generated distribution. cubic interpolation total inducing points lanczos chebyshev scaled eigenvalue method. fitc used equally spaced points longer runtime function number inducing points. consider kernel matérn kernel sample ground truth parameters recover hyper-parameters generated original kernel. important emphasize sources errors present error kernel approximation errors stochastic error lanczos chebyshev. figure stochastic error lanczos relatively small follow-up experiment helps understand lanczos inﬂuenced error incurred approximate kernel. show true marginal likelihood recovered hyper-parameters run-time table clear table methods able recover parameters close ground truth kernel. results interesting matérn kernel fitc struggles parameters recovered fitc value marginal likelihood much worse methods. figure comparison true spectrum lanczos weights chebyshev weights kernel weights counts log-scale easier compare. blue bars correspond positive weights bars correspond negative weights. table hyper-parameter recovery matérn kernels. data generated normally distributed points. lanczos surrogate scaled eigenvalues used inducing points fitc used numbers chosen make times close equal. diagonal correction applied matérn approximate kernel. value marginal likelihood computed exact kernel shows value hyper-parameters recovered method. lanczos times averaged values. figure example shows important diagonal correction kernels. matérn kernel used data given black dots. data generated function added normally distributed noise standard deviation used exact method optimal hyper-parameters used hyper-parameters study different behavior predictive uncertainties inducing points given green crosses. solid blue line predictive mean dotted lines shows conﬁdence interval standard deviations. figure level curves exact surrogate approximation determinant function matérn kernels. used dataset consisted equally spaced points interval surrogate model constructed points shown determinant values computed using stochastic lanczos.", "year": 2017}