{"title": "Non-parametric Bayesian Learning with Deep Learning Structure and Its  Applications in Wireless Networks", "tag": ["cs.LG", "cs.NE", "cs.NI", "stat.ML"], "abstract": "In this paper, we present an infinite hierarchical non-parametric Bayesian model to extract the hidden factors over observed data, where the number of hidden factors for each layer is unknown and can be potentially infinite. Moreover, the number of layers can also be infinite. We construct the model structure that allows continuous values for the hidden factors and weights, which makes the model suitable for various applications. We use the Metropolis-Hastings method to infer the model structure. Then the performance of the algorithm is evaluated by the experiments. Simulation results show that the model fits the underlying structure of simulated data.", "text": "posterior distributions factor numbers. authors developed hierarchical model based beta process convolutional factor analysis deep learning. proposed linear model connecting weights hidden factors real values binary ones. model used multi-level analysis image-processing data sets. another approach constructed build prior distribution nonparametric bayesian factor regression. kingman’s coalescent chosen prior achieves good results gene-expression data analysis. indian buffet process introduced factor analysis therefore enabled model handling inﬁnite case. addition method allows real-valued weights factors. application realm non-parametric bayesian model explored solve various classiﬁcation clustering problems. deep belief networks applied unlabeled auditory data achieved good performance unsupervised classiﬁcation task. although nonparametric bayesian technique advanced researchers recently challenges still problem constructing real-valued non-linear models numbers hidden layers hidden factors inﬁnite. paper investigate nonparametric bayesian graphical model inﬁnite hierarchical hidden layers inﬁnite number hidden factors layer. main contributions include proposition inﬁnity structure latently hierarchically; linking weights extended binary values real values; proposed model constructed non-linear fashion like works employment metropolis-hastings algorithm enables alternative update values hidden factors layer layer making inference procedure recursively. phantom data simulated according inﬁnite generative model. inferring algorithm applied simulated data extract data structure. stated before considering wireless security circumstance applications mainly focus turn clustering problems. therefore interest lies number hidden factors indicates number clusters different hierarchical levels. simulation results show greedy algorithm accomplishes objective discovering number hidden factors accurately. nonparametric bayesian generative model introduced generate data. inference algorithm given section iii. simulation results presented section section abstract—in paper present inﬁnite hierarchical non-parametric bayesian model extract hidden factors observed data number hidden factors layer unknown potentially inﬁnite. moreover number layers also inﬁnite. previous non-parametric bayesian methods assume binary values hidden factors weights. contrast construct model structure allows continuous values hidden factors weights makes model suitable applications. metropolis-hastings method infer model structure. performance algorithm evaluated experiments. simulation results show model underlying structure simulated data. statistical models applied classiﬁcation prediction problems machine learning data analysis statistical methods make hypothesis mathematical models controlled certain parameters latent structure observed data observed data assumed generated complex structures hierarchical layers hidden causes challenge faced modeling data structure thus determination numbers layers hidden variables. however sometimes impractical challenging choose ﬁxed number model structure making hypothesis. therefore need ﬂexible non-parametric models make fewer assumptions capable unlimited amount latent structures. hierarchical nonparametric bayesian model assumes unspeciﬁed number latent variables produces rich kinds probabilistic structures constructing cascading layers. hence considered powerful technique cope challenge. two-layer non-parametric bayesian model proposed hidden factors linking weights binary. model accommodates potentially inﬁnite number hidden factors performs well inferring stroke localizations. works built deep cascading graphical model permits number hidden layers inﬁnite. technique used inference structures images. however proposed model infers priors number hidden factors layer ignores inﬂuence factor values layer strategy constructing matrix result indian buffet process number variables approaches inﬁnity matrix column variance conforms inverse gamma prior inversegamma. matrix imposes selection effect variables layers matrix indicates much inﬂuence variable receive higher level variables ancestors. obtained hidden vector weight matrix variables conditionally independently generated assume follow gaussian given parameter σyi− speciﬁed σyi− veriﬁed element weight matrix follows distribution generation variables ﬁrst hidden layer procesimilar dure above except parameterizations assume bernoulli beta matrix matrix inversegamma. hence distribution observed data vector expressed generative model employed many applications since able extract features data points also higher level hyper-features extracted features. instances found applications human face recognition input data images human faces ﬁrst level features curves edges second level features organs like eyes nose moreover allow variable possess hidden causes makes model robust. addition assume real weight matrix instead binary ones bring model closer practice since different hidden causes reasonably weighted. objective construct hierarchical bayesian framework based generative model allows inﬁnite layers inﬁnite components layer. better explain proposed model describe ﬁnite generative model ﬁrst. inﬁnite model obtained extending number hidden factors number layers inﬁnity. finite generative model used model causal effects among factors layers described construct model observation layer hidden layers. deﬁne matrix data data points vector dimensions. accordingly deﬁne matrix hidden factors ﬁrst hidden layer vector dimensions. similarly deﬁnition matrix hidden factors second hidden layer. express dependency successive layers weight matrix weight matrix respectively. instance exists connection inﬂuence generation data component rest hidden vectors {yi} weight matrices {wi} derived similar way. fig. illustrates proposed inﬁnite generative model structure particular instance note weight matrices remain instances data sets instances generated independently. input data points perform bayesian inference second hidden layer forth. since prior changed inference second hidden layer need re-infer ﬁrst hidden layer using updated upper hidden layers. iteratively perform inference layer value ...} converges. proved layer-wise inferring strategy efﬁcient different metropolis-hastings algorithm applied perform inference instead contrastive divergence method. metropolis-hastings algorithm ﬁrst introduced classic paper metropolis rosenbluth etc. extensively applied statistical problems. deﬁnes markov chain allows change dimensionality different states model. state generated previous state ﬁrst generating candidate state using speciﬁed proposal distribution. decision made accept candidate state based probability density relative previous state respect desired invariant distribution candidate state adopted evolves next state markov chain; otherwise state model stays same. better explain inference algorithm specify problem hidden layer inference. generalized inﬁnite case derived similar fashion. problem settings represent values weight matrix connecting data matrix hidden factors matrix dimension hidden factor change different states model adopted probability change dimensionality completed iteratively pick hidden factor corresponding column check number linked edges remove hidden factor together corresponding column decrease otherwise propose hidden factor linked edges sample values proposed state accepted probability probability adding hidden factor probability generating approximated speciﬁed normal distribution. obtained multiply probabilities. return previous conﬁguration delete hidden factor values proposed probability choosing hidden factor approximated moreover inﬁnite number components layer obtained taking limit demonstrate distributions selection matrices correspond ibp. example assumptions have number ﬁrst hidden layer factors selected n-th variable data point number ﬁrst hidden layer factors selecting components data point. distribution corresponds stochastic process analog dishes selecting customers indian buffet restaurant. restaurant provides customers inﬁnite array dishes corresponds inﬁnite components ﬁrst hidden layer factors. ﬁrst customer tries oisson dishes. succeeding customers select dishes ﬁrstly select previously selected dishes probability m−ik/i m−ik number customers chosen k-th dish except i-th customer himself. i-th customer selects next oisson constructed inﬁnite generative model goal infer number hidden layers well number hidden factors hidden layer based bayesian inference. task done obtain inference ...} given observed data however direct estimation intractable. inspired perform inference layer time. ﬁrst initialize weights matrices ...} well hidden layer ...}. value ...} ...} leading fact prior distribution known expressed terms ...} ...}. based scenario metropolis-hastings algorithm approximate method infer ﬁrst hidden layer inferring ﬁrst hidden layer matrix analyzed performance proposed modiﬁed metropolis-hastings algorithm inferring true number hidden factors ﬁrst hidden layer. first dimension observed data points vary number hidden factors integer value generate dataset containing data instances using proposed generative model. within instance sampled according gaussian prior. weight matrix drawn distribution speciﬁed finally data point generated gaussian distribution parameters expressed terms rest model parameters ﬁxed beta distribution; inversegamma distribution. modiﬁed metropolishastings algorithm initialized three choices random positive integer runs iterations. dataset estimated times inference procedure described previously. record expectation estimated number hidden factors variance result. accomplish algorithm need sample using gibbs sampling individually infer variable matrices turn distribukt|x y−kt tions jordan hierarchical bayesian nonparametric models applications bayesian nonparametrics principles practice hjort holmes m¨uller walker eds. cambridge university press thibaux jordan hierarchical beta processes indian buffet process. volume practical nonparametric semiparametric bayesian statistics tech. rep. wood non-parametric bayesian method inferring hidden causes proceedings twenty-second conference uncertainty artiﬁcial intelligence ryan adams ghahramani learning structure deep sparse graphical models international conference artiﬁcial intelligence statistics chia laguna sardinia italy chen polatkan sapiro carin dunson hierarchical beta process convolutional factor analysis deep learning proceedings international conference machine learning getoor scheffer eds. york knowles ghahramani inﬁnite sparse factor analysis inﬁnite independent components analysis independent component analysis signal separation ser. lecture notes computer science davies james abdallah plumbley eds. springer berlin heidelberg vol. pham largman unsupervised feature learning audio classiﬁcation using convolutional deep belief networks advances neural information processing systems annual conference neural information processing systems vancouver canada december nguyen zheng nonparametric bayesian approach opportunistic data transfer cellular networks proceedings international conference wireless algorithms systems applications yellow mountain china august wang xiao zhou sparse representation face recognition based discriminative low-rank dictionary learning computer vision pattern recognition ieee conference providence june nguyen zheng identifying primary user emulation attacks cognitive radio systems using nonparametric bayesian classiﬁcation signal processing ieee transactions vol. march plotted results fig. modiﬁed metropolishastings algorithm inﬂuence initialization. initializing much greater dimensions underlying model inferred values generally much larger true values. however initializing randomly results correspondingly show randomness. another observation method tends over-estimate number hidden factors. proposal hidden factor preferred accepted. according nominator usually larger denominator denominator composed multiplication probability terms. hence likely accepted. proposed model utilized unsupervised nonparametric clustering problem wireless networks. estimated number hidden factors solves challenge clustering problem determination number clusters. wireless security setting proposed model suitable solution identify attack devices communication system ﬁeld data analysis wireless networks proposed model serve feature extraction approach moreover proposed model contribute location estimation task wireless networks many wireless networking applications explored using proposed framework. paper developed deep hierarchical nonparametric bayesian model represent underlying structure observed data. correspondingly proposed modiﬁed metropolis-hastings algorithm recover number hidden factors. simulation results hidden layer show algorithm discovers model structure estimation errors. however shown results approach capable inferring increasing dimensions", "year": 2014}