{"title": "Condition Monitoring of HV Bushings in the Presence of Missing Data  Using Evolutionary Computing", "tag": ["cs.NE", "cs.AI"], "abstract": "The work proposes the application of neural networks with particle swarm optimisation (PSO) and genetic algorithms (GA) to compensate for missing data in classifying high voltage bushings. The classification is done using DGA data from 60966 bushings based on IEEEc57.104, IEC599 and IEEE production rates methods for oil impregnated paper (OIP) bushings. PSO and GA were compared in terms of accuracy and computational efficiency. Both GA and PSO simulations were able to estimate missing data values to an average 95% accuracy when only one variable was missing. However PSO rapidly deteriorated to 66% accuracy with two variables missing simultaneously, compared to 84% for GA. The data estimated using GA was found to classify the conditions of bushings than the PSO.", "text": "abstract work proposes application neural networks particle swarm optimisation genetic algorithms compensate missing data classifying high voltage bushings. classification done using data bushings based ieeec. ieee production rates methods impregnated paper bushings. compared terms accuracy computational efficiency. simulations able estimate missing data values average accuracy variable missing. however rapidly deteriorated accuracy variables missing simultaneously compared data estimated using found classify conditions bushings pso. methods used previously account missing data include regression techniques jackson well madow used principal component analysis. abdella marwala proposed method accurately compensating missing data using autoencoder. markey patel chose zeros missing data. work ghahramani tresp also used regression address problem missing data. accuracy solution derived replacing missing data averages zeros iterated number depends whether final diagnosis decision based missing variable alone decision depends collectively variables. accuracy approximated variable considered paper. important stages evaluating data note data measurements numbers little meaning historically human intervention required decisions made executed. artificial intelligence gives option autonomously transform correlate interpret data becomes valuable knowledge. work investigates tools compensate sensor failure systems used condition monitoring high voltage bushings. sensor failure concern needs taken account designing automated system bushing condition monitoring. sensor fails online bushing monitoring system system trips transformer financial legal consequences serious. failed sensor take hours months repair need answer implementing online diagnostics system. happens sensors fail? many sensors fail online system rendered ineffective? recognising value artificial intelligence risks associated usage cigre formed study committee improving applications data mining techniques within power systems mcgrail identified several areas reliability centred maintenance improved using artificial intelligence online offline. artificial intelligence significant value power systems operations maintenance control providing single engine execute critical role data-fusion however also clear systems need group variables fewer variables called principal components. principal component linear combination original variables. principal components orthogonal redundant information. everitt dunn found works well finding correlations among data contrast summarises data using fewer dimensions. detect underlying dimensions allows explain observed similarities dissimilarities investigated objects. comparable difference tends extract factors forcing user manually interpret results; result often yields useful solutions. welling webber established searches directions data-space independent across statistical orders. related principal component analysis factor analysis much powerful technique capable finding underlying factors sources classic methods fail completely. although finds minimum number components best represents data best representation least square sense guarantee usefulness discrimination. needs reduce dimensionality constraint maximizing discrimination. maximizing discrimination achieved increasing inter-cluster distances reducing intra-cluster distances. dimension reduction original complete data used generate smaller data enlarged approximation original data using prediction neural network missing data large data ignore missing variable dimension reduction technique available variables output etc. neural network regenerate original approximated variable. approximation process many steps errors introduced. reason paper evaluate autoencoders only. first reason concern sensor fails information available particular measured parameter missing variable might important. second reason concerned processor online diagnostics tool identify undefined value missing data sensor failure. third reason concern data sensor become corrupted loss calibration excessive noise. problem analytical tools neural network fuzzy theory principal component analysis etc. cannot process undefined values. currently method specifying missing data within neural network fuzzy toolbox missing data must approximated prior processing. multivariate data five methods available address missing data namely average values previous values variable; average values training data variable; using zero missing data; deleting variables undefined; finding correlation missing variable remaining variables. using constant averages zeros deleting variable completely provides incorrect solutions options cannot applied generic solution missing data problems. iterative methods reduce dimensionality data used look correlation measured data. work approximates data missing random missing completely random non-ignorable data described little rubin missing data many methods data fusion extracting features among variables highly dimensional data among dimension reduction techniques principal component fisher linear discriminant analysis multi-dimensional independent component analysis factor analysis auto associative neural network encoder methods except autoencoders dimension-reduction techniques sense used replace large observed variables smaller variables whilst preserving original information. takes advantage redundancy information simplifies data replacing desired output neuron output output. within neural network several methods optimisation used minimise errors weights. optimisation methods tested within neural networks included gradient methods conjugate gradient scaled conjugate gradient quasi-newton batch gradient descent stage optimisation error highlighted dhlamini marwala squared output error used prevent error negative number i.e. target greater output. squared error given autoencoder autoencoders principle contractive mapping locate point convergence given known sensorsâ€™ data unknown sensor data. contractive mapping occurs output distance points less input distance points. mathematically mapping complete metric space which space )yxdk banach fixed-point theorem states that contractive mapping exists unique fixed point moreover exists sequence {xn} element converges convergent point xo.. fig. shows structure autoencoder. thompson successfully applied autoncoders sensors minimising error missing sensor inputs outputs also minimising error entire input pattern output pattern using missing known sensors achieve final answer. input network vectors whose total dimension definition input dimension output dimension autoencoder hidden layer lower dimensionality input/output. rule ratio input hidden layer neurons input neural network given first vector known sensor values given input pattern. second vector missing sensor values. autoencoder used paper feedforward multilayered perceptron parameter wijk matrix weights whose element weight connecting known sensor value neuron hidden layer; wijm matrix missing sensors vector bias weights first layer; wkjk wkjm weights output. within input layer input transformed using weights baises genetic algorithms search solution space function simulating survival fittest strategy similar evolution. fittest individuals population reproduce survive next generation. attempting simulate evolution uses components stages include chromosomes selection functions genetic functions reproduction functions random initial population terminating criteria evaluation function five stages optimisation cycle create random initial state evaluate fitness select fittest population undergo crossover undergo mutation repeat successful. selection methods include roulette wheel techniques tournament elitist ranking methods. work generations population size used. roulette wheel selection method arithmetic crossover used nonuniform mutation used. table compares results simulations done using particle swarm optimisation takes origins form social behaviour bird flocking. global optimisation method. many similarities unlike evolution operators crossover mutation. potential solutions called particles problem space following current optimum particle. particle defined variables velocity position shown respectively. particle velocity present current particle current solution pbest best solution fitness achieved gbest best value global particles rand random number well learning factors defined eberhart particle keeps track coordinates problem space associated best solution fitness achieved far. fitness value also stored pbest. best value obtained neighbourhood particle also tracked. subscript stands missing stands known. evolutionary algorithms used optimise error function particle swarm optimisation genetic algorithms seeks maximise error function negative inserted error equation obtain minimum value. error function given process diagram evolutionary neural network shown fig. approximating missing optimisation techniques expectation maximisation type approach. first step called expectation step computes expected value missing variable. second step called maximization step substitutes expected values missing data obtained step maximizes likelihood function data missing obtain parameter estimates. cycle expectation maximisation repeated error within tolerance number cycles exceeded. approximated value closer actual missing value. large approximated values target value even approximation within standard deviation. based approximated missing variables bushings evaluated according ieeec. criteria classified acceptable unusable. table shows average values three simulations accuracy approximated values calculated using well influence estimations accuracy classification process. found times faster achieve level accuracy missing data point. argued depends number iteration swarm size. setting high values e.g. swarm size iterations average accuracy results improved approximate missing value known data guessed unknown/missing value trained autoencoder. average error input output values known values calculated approximated missing variable. error known variables greater approximated missing value recalculated pso. process continued number iterations swarm generations exceeded error became less results obtained shows autoencoder reliably trace correlation missing data known data. results show data available network could approximate missing variables within standard deviation. location called lbest. particle takes population topological neighbours best value global best called gbest. thus accelerating particle toward pbest lbest locations. acceleration weighted random term separate random numbers generated acceleration toward pbest lbest locations. clearly randomisation position particles start optimisation i.e. first particle values truly random. subsequent particles move towards best particle pbest located lbest. initial pbest local minimum lbest entire swarm search converge within local minimum. contrast entire search process random. starting positions chromosomes random mutation chromosome random randomised crossover mutated chromosomes. fittest chromosome selected mutated reproduced search surface less likely local minimum offspring completely unexplored surface search surface once. remembering best result total number generations able obtain global optimum. performs exhaustive search optimum pso. common implementation evolutionary techniques like following procedures random generation initial population; calculating fitness value subject directly dependant distance optimum; reproduction population based fitness values. using evolutionary algorithms together neural networks work able minimise error function number hidden neurons well number cycles iteration. missing data approximation simulations done bushings variables. criterion correct approximation missing value within standard deviation variable e.g. positive. standard deviations calculated gave values ranging abdella marwala genetic algorithms approximate missing data database proceedings ieee international conference computational cybernetics mauritius ghahramani m.i. jordan mixture models learning incomplete data chapter greiner petsche s.j. hanson computational learning theory natural making learning systems volume learning systems practical cambridge press tresp ahmad neuneier training neural networks deficient data chapter j.d. cowan tesauro j.alspector advances neural information processing systems mateo morgan kaufman b.s. everitt dunn applied multivariate data analysis edward arnold london independent component analysis incomplete data proceedings joint symposium neural computation ucsd b.b. thompson r.j. marks m.a. elcontractive nature sharkawi autoencoders sensor application restoration proceedings ieee joint conference neural networks july portland. b.b. thompson r.j. marks m.a. elsharkawi m.y. huang bunje implicit learning autoencoder novelty assessment proceedings international joint conference ieee world neural networks intelligence congress computational honolulu number iteration generations well swarm size population size increased achieve desired accuracy. parameters adjust optimisation. furthermore figure clearly indicates number missing values classification accuracy decreases. furthermore indicates particular application possible estimate missing value still achieve classification accuracy. without missing value classification accuracy however results also indicate particle swarm optimization applied paper loses accuracy number missing values increase. work finds autoencoder trace correlation missing data known data data missing could produce average accuracy approximation missing data within standard deviation also achieved data missing. percentage missing data less total number variables autoencoder approximated missing data average accuracy found times faster achieve level accuracy missing data point. argued depends number iteration swarm size. setting high values e.g. swarm size iterations accuracy results improve time simulate increased found perform exhaustive search optimum pso. missing data estimated found performs better classifying condition bushings pso. a.j. mcgrail gulski e.r.s. groot allan birtwhistle t.r. blackburn data mining techniques assess condition high voltage electrical plant proceedings cigre conference paris", "year": 2007}