{"title": "Semi-supervised Vocabulary-informed Learning", "tag": ["cs.CV", "cs.AI", "cs.LG", "stat.AP", "stat.ML"], "abstract": "Despite significant progress in object categorization, in recent years, a number of important challenges remain, mainly, ability to learn from limited labeled data and ability to recognize object classes within large, potentially open, set of labels. Zero-shot learning is one way of addressing these challenges, but it has only been shown to work with limited sized class vocabularies and typically requires separation between supervised and unsupervised classes, allowing former to inform the latter but not vice versa. We propose the notion of semi-supervised vocabulary-informed learning to alleviate the above mentioned challenges and address problems of supervised, zero-shot and open set recognition using a unified framework. Specifically, we propose a maximum margin framework for semantic manifold-based recognition that incorporates distance constraints from (both supervised and unsupervised) vocabulary atoms, ensuring that labeled samples are projected closest to their correct prototypes, in the embedding space, than to others. We show that resulting model shows improvements in supervised, zero-shot, and large open set recognition, with up to 310K class vocabulary on AwA and ImageNet datasets.", "text": "figure illustration semantic embeddings learned using support vector regression using proposed semi-supervised vocabulary-informed approach. cases t-sne visualization used illustrate samples source/auxiliary classes target/zeroshot classed imagenet dataset. decision boundaries illustrated dashed lines drawn hand visualization. note large margin constraints ss-voc among source/target classes external vocabulary atoms ﬁne-tuning semantic word space lead better embedding compact separated classes zero-shot learning widely studied variety research areas including neural decoding fmri images character recognition face veriﬁcation object recognition video understanding typically zero-shot learning approaches recognize instances unseen unknown testing target categories transferring information intermediate-level semantic representations known observed source categories many labeled instances exist. words supervised classes/instances used context recognition classes contain visual instances training time correspondence supervised classes/instances. such general experimental setting classes target source despite signiﬁcant progress object categorization recent years number important challenges remain; mainly ability learn limited labeled data ability recognize object classes within large potentially open labels. zero-shot learning addressing challenges shown work limited sized class vocabularies typically requires separation supervised unsupervised classes allowing former inform latter vice versa. propose notion semi-supervised vocabulary-informed learning alleviate mentioned challenges address problems supervised zero-shot open recognition using uniﬁed framework. speciﬁcally propose maximum margin framework semantic manifoldbased recognition incorporates distance constraints vocabulary atoms ensuring labeled samples projected closest correct prototypes embedding space others. show resulting model shows improvements supervised zero-shot large open recognition class vocabulary imagenet datasets. object recognition speciﬁcally object categorization seen unprecedented advances recent years development convolutional neural networks however successful recognition models date formulated supervised learning problems many cases requiring hundreds thousands labeled instances learn given concept class exuberant need large labeled datasets limited recognition models domains classes. humans hand able distinguish beyond basic level categories impressive fact humans learn examples effectively leveraging information object category classes even recognize objects without ever seeing ability spawned research few-shot zero-shot learning. setting important drawbacks assumes target classes cannot mis-classiﬁed source classes vice versa; greatly unrealistically simpliﬁes problem; target label often relatively small several thousand unknown labels compared least entry level categories humans distinguish; large amounts data source classes required problematic shown object classes instances vast open vocabulary semantic knowledge deﬁned part leveraged inform learning source class recognition. works recently looked resolving class-incremental learning designed distinguish seen unseen classes testing time apply appropriate model supervised former latter. however remain largely unresolved. particular artifacts setting fundamental. example consider learning looking image instances fig.. knowing motor vehicles exist world tempted call anything -wheels car. result zero-shot class truck large overlap class however imagine knowing also exist many motor vehicles even without visually seen objects basic knowledge exist world closely related should principal alter criterion recognizing instance encoding model results better separation among classes. tackle limitations towards goal generic open recognition propose idea semisupervised vocabulary-informed learning. speciﬁcally assuming labeled training instances large open vocabulary/semantic dictionary task semi-supervised vocabulary-informed learning learn model utilizes semantic dictionary help train better classiﬁers observed classes unobserved classes supervised zero-shot open image recognition settings. different standard semi-supervised learning assume unlabeled data available help train classiﬁer vocabulary target classes known. capable utilizing vocabulary unsupervised items training improve recognition. uniﬁed maximum margin framework used encode idea practice. particularly classiﬁcation done nearestneighbor distance class prototypes semantic embedding space encode constraints ensuring labeled images project semantic space closer correct class prototypes incorrect ones show word embedding used effectively initialize semantic space. experimentally illustrate paradigm achieve competitive supervised performance well open image recognition performance large number unobserved vocabulary entities effective learning samples also illustrated. related work one-shot learning machine learningbased object recognition algorithms require large amount training data one-shot learning aims learn object classiﬁers examples. compensate lack training instances enable oneshot learning knowledge much transferred sources example sharing features semantic attributes contextual information however none previous works used open vocabulary help learn object classiﬁers. zero-shot learning aims recognize novel classes training instance transferring knowledge source classes. ﬁrst explored attributebased semantic representations required pre-deﬁned attribute vector prototypes class costly large-scale dataset. recently semantic word vectors proposed embed class name without human annotation effort; therefore serve alternative semantic representation zsl. semantic word vectors learned large-scale text corpus language models wordvec glovec however previous work word vectors semantic representations setting neither utilized semantic word vectors explicitly learning better classiﬁers; extending setting towards open image recognition. notable exception aims recognize zero-shot classes given modest vocabulary source classes; explore vocabularies order magnitude larger open-set recognition term open recognition initially deﬁned formalized mainly aims identifying whether image belongs also known classseen unseen classes. incremental learning. however none identify classes unseen instances. exception augments zero-shot class labels source labels experimental settings. similarly deﬁne open image recognition problems recognizing class name image potentially large open vocabulary note methods like orthogonal potentially useful still worth identifying seen unseen instances recognized different label sets shown experiments. conceptually similar different formulation task open-vocabulary object retrieval focused retrieving objects using natural language open-vocabulary queries. visual-semantic embedding mapping visual features semantic entities explored ways directly learning embedding regressing visual features semantic space using support vector regressors neural network projecting visual features semantic entities common space wsabie devise contrast model trains better visual-semantic embedding training instances help large amount open vocabulary items formulation inspired uniﬁed semantic embedding model however unlike formulation built word vector representation contains data term incorporates constraints unlabeled vocabulary prototypes. samples image feature representation image class label taken english words phrases consequently |ws| number source classes. further suppose another class labels target classes labeled samples available. note potentially |wt| |ws|. given test image feature vector goal learn function using available information predicts class label note form problem changes drastically depending label assumed supervised learning zero-shot learning open recognition {wswt} generally posit single uniﬁed learned three cases. formalize deﬁnition semi-supervised vocabulary-informed learning follows traditional typically makes vocabulary test time ss-voc utilizes exactly data training. notably ss-voc requires additional annotations semantic knowledge; simply shifts burden testing training leveraging vocabulary learn better model. vocabulary come semantic embedding space learned wordvec glovec largescale corpus; vocabulary entity represented distributed semantic vector semantics embedding space help knowledge transfer among classes allow open image recognition. note semantic embedding spaces equivalent semantic knowledge base deﬁned hence make appropriate ss-voc setting. assuming learn mapping image features semantic space recognition carried using simple nearest neighbor distance e.g. closer ucar word vector; context interpreted prototype class thus core question learn mapping form inference optimal semantic space. learning propose discriminative maximum margin criterion ensures labeled samples project closer corresponding class prototypes prototype open vocabulary learning embedding maximum margin vocabulary-informed embedding learns mapping low-level features semantic word space utilizing maximum margin strategy. speciﬁcally consider where rp×d. ideally want estimate labeled instances data term easiest enforce objective minimize euclidian distance sample projections appropriate prototypes embedding space selected source/auxiliary dataset vocabulary. term enforces smaller distance facilitate computation similarly closest prototypes closest prototype source classes. complete pairwise maximum margin term note form rank hinge loss similar devise devise considers loss respect source/auxiliary data prototypes. vocabulary-informed embedding complete combined objective written ratio coefﬁcient terms. practical advantage objective function unconstrained minimization problem differentiable solved l-bfgs. initialized zeros converges iterations. fine-tuning word vector space formulation works well assuming semantic space well laid linear mapping sufﬁcient. however posit word vector space necessarily optimal visual discrimination. consider following case visually similar categories appear away semantic space. case would difﬁcult learn linear mapping matches instances category prototypes properly. inspired intuition also expressed natural language models propose ﬁne-tune word vector representation better visual discriminability. potentially ﬁne-tune representation optimizing directly alternating optimization however possible source/auxiliary class prototypes would break regularities semantic space reducing ability transfer knowledge source/auxilary target classes. alternatively propose optimizing global warping word vector space regularization coefﬁcient. still solved using l-bfgs initialized using identity matrix. algorithm ﬁrst updates typically step updating converge within iterations corresponding class prototypes used ﬁnal classiﬁcation updated nevertheless make embedding comparable support vector regression employ maximal margin strategy \u0001−insensitive smooth replace least square term eq.. regularization coef |ξ|\u0001 ﬁcient. indicates j-th value corresponding vector. j-th column conventional \u0001−svr formulated constrained minimization problem i.e. convex quadratic programming problem \u0001−ssvr employs quadratic smoothing make differentiable everywhere thus \u0001−ssvr solved unconstrained minimization problem directly. pairwise term data term ensures labelled samples project close correct prototypes. however since many samples number classes unlikely data constraints satisﬁed exactly. speciﬁcally consider following case part semantic space entities live projecting away asymptomatic i.e. result misclassiﬁcation. however close prototypes minor error regression result misclassiﬁcation. embed intuition learning enforce discriminative constraints learned semantic embedding space. speciﬁcally distance close possible also smaller distance formally deﬁne vocabulary pairwise maximal margin term selected open vocabulary; margin constant. here indicates quadratically smooth hinge loss convex gradient every point. speedup computation closest target prototypes source/auxiliary prototype semantic space. also deﬁne similar constraints source prototype pairs nearest neighbor classiﬁer directly measures distance predicted semantic vectors prototypes semantic space i.e. employ k-nearest neighbors testing instances average predictions i.e. averaging instances predicted semantic vectors. datasets. conduct experiments animals attributes dataset imagenet dataset. consists classes animals standard split source/auxiliary classes target/test classes introduced. follow split supervised zeroshot learning. overfeat features make results easily comparable state-of-the-art. imagenet dataset large-scale dataset. classes ilsvrc source/auxiliary classes classes ilsvrc used ilsvrc target data. pre-trained model extract deep features imagenet. dataset instances source dataset mimic human performance learning examples ability generalize. recognition tasks. consider three different settings variety experiments this strategy known rocchio algorithm information retrieval. rocchio algorithm method relevance feedback using relevant instances update query instances better recall possibly precision vector space ﬁrst suggested sophisticated algorithms also possible. classiﬁer trained directly training instances source data without semantic embedding. standard learning setting learned classiﬁer predict labels testing data source classes. devise conse compare state-of-the-art large-scale zero-shot learning approaches implement devise conse conse uses multi-class logistic regression classiﬁer predicting class probabilities source instances; parameter selected gives best results. conse method supervised setting works svr. code provided author webpage closed variant maximum margin leaning vocabulary-informed constraints known classes corresponds full model maximum margin constraints coming compute using without optimizing open vocabulary. google wordvec learn open vocabulary large text corpus around billion words umbc webbase latest wikipedia articles documents rare words high frequency stopping words pruned vocabulary remove words frequency million times. result vocabulary around words/phrases openness deﬁned computational parameters selection scalability. experiments repeated times avoid noise small training size report average across runs. experiments mean accuracy reported i.e. mean diagonal confusion matrix prediction testing data. parameters experiments training instances available imagenet varying values leads variances variances imagenet dataset; experimental conclusions still hold. cross-validation conducted solve scale stochastic gradient descent makes great progress initially often slow approaches solution. contrast l-bfgs method mentioned achieve steady convergence cost computing full objective gradient iteration. l-bfgs usually achieve better results good initialization however computationally expensive. leverage beneﬁts methods utilize hybrid method solve large-scale datasets solver initialized instances approximate gradients using ﬁrst gradually instances used switch l-bfgs made iterations. solver motivated friedlander theoretically analyzed proved convergence hybrid optimization methods. practice l-bfgs hybrid algorithms imagenet respectively. hybrid algorithm save training time compared l-bfgs. report experimental results tab. uses /-dimensional wordvec representation highlight following observations ss-voc variants better classiﬁcation accuracy svr. validates effectiveness model. particularly results ss-vocfull higher svr/svm supervised zero-shot recognition respectively. note though results svm/svr good supervised recognition tasks improve them attribute discriminative classiﬁcation boundary informed vocabulary. ss-vocw signiﬁcantly improves zero-shot recognition results ss-vocclosed. validates importance information open vocabulary. ss-voc beneﬁts open vocabulary compared word vector space ﬁne-tuneing. results supervised zero-shot recognition ss-vocfull ./.% higher ss-vocclosed. comparing state-of-the-art compare results state-of-the-art results dataset tab. compare ss-vocfull trained source instances instances including word vector attribute dimension wordvec dictionary used ss-voc. different types hand-crafted low-level feature used different methods. except ss-voc instances source data used training. general reference classiﬁcation accuracy imagenet ndecaf noverfeat nvgg ngooglelenet. stances class). model achieves accuracy remarkably higher previous methods. particularly impressive taking account fact semantic space additional attribute representations many competitor methods utilize. further results training instances small fraction instances used train methods already outperform approaches. argue much success improvement comes discriminative information obtained using open vocabulary corresponding large margin constraints rather features since method improved compared uses overfeat features. note ss-vocfull result higher closest competitor improvement statistically signiﬁcant. comparing work powerful visual features also employed semantic embeddings large-scale open recognition focus open-setk setting large vocabulary approximately entities; chance performance task much much lower. addition study effect performance function open vocabulary also conduct additional experiments different label sets open-setk−n labels nearest neighbor ground-truth class prototypes selected complete dictionary labels. corresponds open grained recognition; open-setk−rn label names randomly sampled set. results shown fig. also note ﬁne-tune word vector space open-setk setting since optimize better visual discriminability relative small subset compared vocabulary. open-set variants assume test data comes either source/auxiliary domain target domain split cases mimic supervised zero-shot scenarios easier analysis. supervised-like setting fig. accuracy better svr-map three different label sets rates. better results largely better embedding matrix learned enforcing maximum margins training class name open vocabulary source training data. zero shot-like setting method still notable advantage svr-map method top-k accuracy thanks better embedding learned however notice top- accuracy zero shot-like setting lower svrmap method. method tends label instances target data nearest classes within source label set. example humpback whale testing data likely labeled blue whale. however considering top-k accuracy method still advantages baselines. validate ﬁndings large-scale imagenet dataset; -dimensional wordvec representation used since dataset larger number classes awa. highlight results still better baselines svr-map settings respectively shown tab. open image recognition results shown fig. supervised-like zeroshot-like settings clearly framework still advantages baseline directly matches nearest neighbors vocabulary using predicted semantic word vectors testing instance. open recognition results dataset figure openness=.. chance=.e ground truth label extended variants. example count correct label ’pig’ image labeled ’pigs’. †different label settings. features. samples class used train models mimic human performance learning examples illustrate ability model learn little data. however semi-supervised vocabulary-informed learning improve recognition accuracy settings. open image recognition performance dropped around respectively drop caused intrinsic difﬁculty open image recognition task large-scale dataset. however performance still better svr-map baseline turn signiﬁcantly better chance-level. also evaluated model larger number training instances observe standard supervised learning setting improvements achieved using vocabulary-informed learning tend somewhat diminish number training instances substantially grows. large number training instances mapping between low-level image features semantic words becomes better behaved effect additional constraints open-vocabulary becomes less pronounced. comparing state-of-the-art zsl. compare results several state-of-the-art large-scale zero-shot recognition models. results ss-vocfull better conse devise metrics signiﬁcant margin poor results devise training instances largely inefﬁcient learning visual-semantic embedding matrix. algorithm also relies embedding matrix devise explains similar poor performance figure open recognition results imagenet dataset openness=.. chance=.e synsets class— synonymous terms ground truth names instance. training instances. contrast ss-vocfull leverage discriminative information open vocabulary max-margin constraints helps improve performance. devise imagenet instances conﬁrm observation results conse much better devise. results signiﬁcant improved conse. t-sne visualization target testing classes shown fig. compare ss-vocfull ssvocclosed svr. note distributions classes obtained using ss-voc centered separable data pairwise maximum margin terms help improve generalization learned; distribution different classes obtained using full model ss-vocfull also separable ss-vocclosed e.g. persian raccoon. attributed addition open-vocabularyinformed constraints learning improves generalization. example show open partial illustration embeddings learned imagenet/ dataset illustrated figure source/auxiliary target/zero-shot classes shown. better separation among classes largely attributed open-set max-margin constraints introduced ssvocfull model. additional examples miss-classiﬁed instances available supplemental material. conclusion future work paper introduces problem semi-supervised vocabulary-informed learning utilizing open semantic vocabulary help train better classiﬁers observed unobserved classes supervised learning open image recognition settings. formulate semisupervised vocabulary-informed learning maximum margin framework. extensive experimental results illustrate efﬁcacy learning paradigm. strikingly achieves competitive performance training instances relatively robust large open vocabulary class labels. rely wordvec transfer information observed unobserved classes. future linguistic visual semantic embeddings could explored instead combination part vocabulary-informed learning. references akata perronnin harchaoui schmid. labelembedding attribute-based classiﬁcation. cvpr akata reed walter schiele. evaluation output embeddings ﬁne-grained image classiﬁcation. cvpr xiang kodirov gong. zero-shot object recognition semantic manifold distance. cvpr guadarrama rodner saenko zhang farrell donahue darrell. open-vocabulary object retrieval. robotics science systems manning raghavan schutze. introduction information retrieval. cambridge university press mikolov sutskever chen corrado dean. distributed representations words phrases compositionality. neural information processing systems", "year": 2016}