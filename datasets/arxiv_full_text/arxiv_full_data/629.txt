{"title": "A Novel Representation of Neural Networks", "tag": ["stat.ML", "cs.AI", "cs.NE", "I.5.1; I.2.6"], "abstract": "Deep Neural Networks (DNNs) have become very popular for prediction in many areas. Their strength is in representation with a high number of parameters that are commonly learned via gradient descent or similar optimization methods. However, the representation is non-standardized, and the gradient calculation methods are often performed using component-based approaches that break parameters down into scalar units, instead of considering the parameters as whole entities. In this work, these problems are addressed. Standard notation is used to represent DNNs in a compact framework. Gradients of DNN loss functions are calculated directly over the inner product space on which the parameters are defined. This framework is general and is applied to two common network types: the Multilayer Perceptron and the Deep Autoencoder.", "text": "deep neural networks become popular prediction many areas. strength representation high number parameters commonly learned gradient descent similar optimization methods. however representation non-standardized gradient calculation methods often performed using component-based approaches break parameters scalar units instead considering parameters whole entities. work problems addressed. standard notation used represent dnns compact framework. gradients loss functions calculated directly inner product space parameters deﬁned. framework general applied common network types multilayer perceptron deep autoencoder. deep neural networks grown increasingly popular last years astounding results variety tasks. strength derives expressiveness grows network depth. however traditional approaches representing dnns suffers number network layers increases. often rely confusing diagrams provide incomplete description mechanics network leads complexity number layers increases. furthermore dnns inconsistently formulated mathematical problem throughout research ﬁeld especially notationally impedes efﬁciency results combined expanded upon. clear concise framework underpinning dnns must developed work endeavours address issue. work novel mathematical framework dnns created. formed employing carefully selected standard notions notation represent general dnn. common mathematical tools inner product adjoint operation maps deﬁned generic inner product spaces utilized throughout work. well-established mathematical objects treated as-is framework; longer necessary convert matrix column vector decompose collection components example purposes derivative calculation. work presents comprehensive mathematical standard upon dnns formulated. speciﬁc layout paper follows. mathematical preliminaries generic formulated abstract inner product space. chain rule used demonstrate concise coordinate-free approach backpropagation. standard loss functions explicitly considered shown handle variations within learning algorithm. then framework applied multilayer perceptron speciﬁcs previous approach become clear shown create gradient descent algorithm learn parameters mlp. theory developed section applied deep autoencoder demonstrates ﬂexibility approach. type framework extended types networks including convolutional neural networks recurrent neural networks omitted sake brevity. product denoted space linear maps denotedl. note denotes operating i.e. simply similarly space bilinear maps denoted denotes operating i.e. bilinear linear deﬁned follows similarly linear deﬁned follows operators⌟ and⌞ referred left hook right hook operators adjoint linear linear deﬁned adjoint derivative well deﬁned respect inner products denoted instead sake notational convenience. then denotes adjoint maps point consider maps another inner product space. derivative composition calculated using well-known chain rule. lemma suppose i.e. also inner product space. variable said state variable whereas parameter. notation presented used denote derivative respect state variable i.e. known elementwise operation associated deﬁnes operation elementwise function components{⟨v ek⟩}k vector operator basis-dependent {ek}n derivative elementwise function deﬁne symmetric bilinear operator⊙ basis vectors{ek}n δkk′ kronecker delta. standard hadamard product {ek}n standard basis seen generalization hadamard product referred vectors written proposition elementwise function deﬁned inner product space dimension basis{ek}n hadamard product⊙ deﬁned elementwise ﬁrst derivative deﬁned furthermore self-adjoint i.e. since⟨y =⟨dψ⋅ self-adjoint. proposition elementwise function deﬁned inner product space dimension basis{ek}n hadamard product⊙ deﬁned elementwise second derivative deﬁned furthermore \u0001dψ⌞ self-adjoint linear maps section coordinate-free backpropagation derived generic layered neural network. network formulated gradient descent algorithm given types loss functions. neural networks layered models actions layer denoted inner product spaces. words neural network layers state variable abstract representation input data layer parameters layer must learned often form gradient descent. note explicit dependence parameter suppressed notation throughout section. deﬁned depends then network prediction written composition suppressed dependence parameter represents parameter set{θ θl}. parameter independent parameters{θj}j≠i maps introduced assist derivative calculation. head level known response data. gradient descent used optimize loss function thus gradient respect parameters must calculated. done preliminary results introduced. section always assumed batch updates multiple data points. notice gradient descent performed directly inner product space layer contrasts standard approach performing descent individual component seen coordinate-free gradient descent algorithm. remark difﬁcult incorporate standard ℓ-regularizing term framework. regularization dαi⋅ dfi⋅ dαi−⋅ lemma theorem provides recursive update formula dωi\u0001∗ backpropagates error tangent network multiplication d∗fi adding another term. recall d∗ωi+ calculated recursively using theorem main result calculating∇θi presented. theorem algorithm presents single iteration gradient descent algorithm minimize directly parameter θl}. formula extends linearly batch updates several data points. extend deﬁned multiple pairs calculated pair; algorithm must vl+} calculated. ﬁrst network considered standard multilayer perceptron input data output rnl+ assumed layers. single-layer function rni× rni+ takes data layer along parameters rni+×ni rni+ outputs data theth layer i.e. corollary matrices vectors inner product⟨a valid. every vector treated matrix default. explicit action layer-wise function described elementwise function rni+ rni+ associated elementwise operation rni+ rni+ deﬁned operation nonlinear known elementwise nonlinear function elementwise nonlinearity. derivative maps calculated using propositions respectively. remark maps clearly depend choice nonlinearity common choices derivatives given table note heaviside step function sinh cosh hyperbolic sine cosine functions respectively. table complete description possible nonlinearities. calculated separately layer since independent independent layers first derivatives adjoints computed lemma consider function deﬁned then rni+×ni given results gradient descent algorithm developed minimize respect given data point learning rate iteration given algorithm output algorithm updated version process extended additively batch updates summing individual contributions note ﬁrst term tangent backpropagation expression theorem recursive part second term calculated stage d∗ωi+ calculated. maps calculated next theorem ﬁnal step gradient descent puzzle. theorem rnl+. then deﬁned respectively weight-sharing differernt layers network. introduce function network representation deﬁned follows function property then layerwise function rni+ represented following manner input layer rni+×ni weight matrix bias vector layer rni+ rni+ elementwise nonlinearity corresponding elementwise operation ∈l+×nξ rnξ×nξ+) governs weights shared layer structure autoencoder encode ﬁrst layers decode similar examples matrix transpose operator layer although kept general paper. however particular case adjoint calculated according following lemma. section single-layer derivatives backpropagation presented. strong correspondence section section similarity layerwise-deﬁning function exploited whenever possible. proceeding gradient calculation however particular instance chain rule introduced parameter-dependent maps. theorem generic inner product spaces. consider linear calculated rm×n. then equations consequences lemma equations also follow derivatives calculated lemmas along chain rule. equations follow reversing property adjoint iteration gradient descent algorithm minimize respect parameters given algorithm before output algorithm parameter taken step direction negative gradient work concise complete mathematical framework dnns formulated. generic multivariate functions deﬁned operation network layer composition deﬁned overall mechanics network. coordinate-free gradient descent algorithm relied heavily derivatives vector-valued functions presented applied speciﬁc examples. shown calculate gradients network loss functions inner product space parameters reside opposed individually respect component. simple loss function higher-order loss function considered also shown extend framework types loss functions. approach considered paper generic ﬂexible extended types networks besides ones considered here. immediate direction future work would represent parameters sort lower-dimensional subspace promote sparsity network. finding meaningful basis representations parameters could help limit amount overﬁtting still maintaining predictive power model. also sophisticated optimization methods become tractable number dimensions sufﬁciently reduced would interesting apply neural networks. another direction future work exploit discrete-time dynamical system structure presented layerwise network consider control dynamical systems theory improve network training output.", "year": 2016}