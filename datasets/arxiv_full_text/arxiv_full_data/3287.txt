{"title": "Semi-Orthogonal Multilinear PCA with Relaxed Start", "tag": ["stat.ML", "cs.CV", "cs.LG", "I.2.6"], "abstract": "Principal component analysis (PCA) is an unsupervised method for learning low-dimensional features with orthogonal projections. Multilinear PCA methods extend PCA to deal with multidimensional data (tensors) directly via tensor-to-tensor projection or tensor-to-vector projection (TVP). However, under the TVP setting, it is difficult to develop an effective multilinear PCA method with the orthogonality constraint. This paper tackles this problem by proposing a novel Semi-Orthogonal Multilinear PCA (SO-MPCA) approach. SO-MPCA learns low-dimensional features directly from tensors via TVP by imposing the orthogonality constraint in only one mode. This formulation results in more captured variance and more learned features than full orthogonality. For better generalization, we further introduce a relaxed start (RS) strategy to get SO-MPCA-RS by fixing the starting projection vectors, which increases the bias and reduces the variance of the learning model. Experiments on both face (2D) and gait (3D) data demonstrate that SO-MPCA-RS outperforms other competing algorithms on the whole, and the relaxed start strategy is also effective for other TVP-based PCA methods.", "text": "principal component analysis unsupervised method learning low-dimensional features orthogonal projections. multilinear methods extend deal multidimensional data directly tensor-to-tensor projection tensor-to-vector projection however setting difﬁcult develop effective multilinear method orthogonality constraint. paper tackles problem proposing novel semi-orthogonal multilinear approach. so-mpca learns low-dimensional features directly tensors imposing orthogonality constraint mode. formulation results captured variance learned features full orthogonality. better generalization introduce relaxed start strategy so-mpca-rs ﬁxing starting projection vectors increases bias reduces variance learning model. experiments face gait data demonstrate so-mpca-rs outperforms competing algorithms whole relaxed start strategy also effective tvp-based methods. introduction principal component analysis classical unsupervised dimensionality reduction method transforms input data feature space lower dimension orthogonal projections keeping variance original data. widely used areas data compression computer vision pattern recognition many real-world data multi-dimensional form tensors rather vectors number dimensions tensor order ∗this paper appear proceedings international joint conference artiﬁcial intelligence copyright association advancement artiﬁcial intelligence rights reserved. dimension mode example gray images second-order tensors video sequences third-order tensors tensor data also common applications data center monitoring social network analysis network forensics however multi-dimensional data requires reshaping tensors vectors ﬁrst. vectorization often leads breaking original data structures complex model lots parameters high computational memory demands many researchers address problem multilinear extensions deal tensors directly main approaches. approach based tensor-to-tensor projection learns low-dimensional tensors highdimensional tensors. two-dimensional probably ﬁrst extension deal images without vectorization. generalized rank approximation matrices generalized generalize dpca single-sided projections two-sided projections reconstruction error minimization variance maximization respectively. concurrent subspace analysis multilinear extend glram gpca general higher-order tensors respectively. another approach based tensor-to-vector projection learns low-dimensional vectors highdimensional tensors successive way. tensor rankone decomposition minimizes reconstruction error successive residue calculation. uncorrelated multilinear maximizes variance zero-correlation constraint following successive derivation pca. however number features extracted umpca upper-bounded lowest mode dimension. example tensor size umpca extract three features limited usage. orthogonality constraint popular feature extraction tensor decomposition low-rank tensor approximation also obtains orthogonal projections ttp-based methods produce orthogonal projection vectors mode. however none existing tvp-based so-mpca relaxed start section presents proposed so-mpca-rs ﬁrst formulating so-mpca problem deriving solutions successive conditional approach ﬁnally introducing relaxed start strategy better generalization. formulation semi-orthogonal mpca deﬁne so-mpca problem orthogonality constraint mode i.e. semi-orthogonality follows so-mpca problem tensor data samples x··· available training. sample ri×i×···×in viewed point objective maximize variance projected samples projection direction subject orthogonality constraint mode denoted ν-mode. variance measured total scatter deﬁned methods derive orthogonal projections. study found indeed ineffective impose full orthogonality modes tvp-based captured variance limited number extracted features. propose novel so-mpca approach maximize captured variance orthogonality constraint mode called semiorthogonality according semi-orthogonality results captured variance learned features full-orthogonality. tensor size discussed earlier so-mpca extract features fullorthogonal multilinear extract three features introduce relaxed start strategy sompca-rs ﬁxing starting projection vectors better generalization strategy constrains hypothesis space smaller leading increased bias reduced variance learning model. experimental results sec. show so-mpca-rs outperforms competing pcabased methods whole. addition strategy tensor-based algorithms show effectiveness tvp-based methods. background notations basic operations follow notations denote vectors lowercase boldface letters e.g. matrices uppercase boldface letters e.g. tensors calligraphic letters e.g. denote elements indices parentheses indices lowercase letters spanning range uppercase letter index e.g. nthorder tensor ri×···×in addressed indices {in}. addresses n-mode n-mode product nth-order tensor vector denoted tensor entries tensor-to-vector projection elementary multilinear projections building blocks tvp. denote u··· consisting unit projection vector mode i.e. euclidean norm vectors. projects tensor ri×i×···×in scalar unit projection vectors assop need deal ν-mode modes differently. modes solution obtained unit eigenvector constrained optimization ν-mode need determine solving following constrained optimization problem selection mode although free choose mode impose orthogonality constraint often good features practice. thus paper choose mode highest dimension similar umpca number features extracted fo-mpca upper-bounded lowest mode dimension minn quite limited. instance fo-mpca extract three features tensor size so-mpca extract features choosing tensor. observed fig. well. conditional subproblem order obtain need determine vectors. follow approach alternating least squares thus obtain locally optimal solutions many tensor-based methods. parameters n-mode projection vector estimated mode mode separately conditioned projection vectors modes. assuming projection vectors n-mode given project input tensor samples modes obtain partial multilinear projections relaxed start better generalization so-mpca features classiﬁcation performance limited. therefore introduce simple relaxed start strategy so-mpca-rs jection vectors) without variance maximization. starting normalized uniform vector simplicity. idea motivated theoretical studies chapter showing constraining learning model could lead better generalization. ﬁxing ﬁrst simple vectors following emps less freedom imposed semi-orthogonality increases bias reduces variance learning model. thus so-mpca-rs model smaller hypothesis so-mpca model. algorithms differ determine ﬁrst though following emps different dependency ﬁrst emp. relaxed start strategy speciﬁc so-mpca generally applicable tvp-based subspace learning algorithm. controlled experiments sec. show improve performance so-mpca also trod umpca. experiments section evaluates proposed methods secondorder third-order tensor data terms recognition rate number extracted features captured variance convergence. addition also study effectiveness relaxed start strategy tvp-based algorithms. data second-order tensors subset feret database face images subjects. face image normalized graylevel pixels. thirdorder tensors subset humanid gait challenge database gallery probe also test probe probe gait sample silhouette sequence size experiment setup face recognition experiments randomly select samples subject training data rest testing. repeat random splits times report mean correct recognition rates. gait recognition experiments follow standard setting gallery training data probes test data nearest neighbor classiﬁer feret subset. results highlighted bold fonts indicates enough features extracted. sorial features need vectorized. mpca uses full projection. trod umpca uniform initialization so-mpca sompca-rs selected mode maximum number features. iterative algorithms number iterations features sorted according scatters descending order classiﬁcation. nearest neighbor classiﬁer euclidean distance measure classify features. test features face recognition features gait recognition. performance fompca much worse so-mpca included comparisons save space. face recognition results table shows face recognition results including mean standard deviation repetitions. highlight results bold fonts easy comparison. sompca-rs consistently achieves results cases. compared existing methods so-mpca-rs outperforms best performing existing algorithm average. furthermore larger so-mpca-rs outperforms methods least average. smaller so-mpca-rs achieves greater improvement least existing methods indicating so-mpca-rs superior dealing small sample size problem. gait recognition results similarly gait recognition results reported table results highlighted. again so-mpca-rs consistently achieves results cases. rank rate best performing existing algorithm outperforms so-mpcars average. rank rate so-mpcars outperforms best performing existing algorithm average. number features tables indicate enough features. features face data since samples training. umpca extract features face gait data respectively. contrast sompca so-mpca-rs learn features face data features gait data. feature variance illustrate variance captured umpca fo-mpca so-mpca so-mpca-rs fig. face data figure shows sorted variance. clear semi-orthogonality captures variance full-orthogonality discussed sec. moreover so-mpca so-mpca-rs capture variance umpca less though capturing less variance so-mpca-rs achieves better overall classiﬁcation performance pca-based methods results consistently experiments. tables show relaxed start help trod umpca achieve better recognition rates. table trod-rs improves trod umpcars improves umpca average. table trod-rs achieves improvement trod umpca-rs achieves improvement umpca rank rate. rank rate trod-rs improves trod umpca-rs improves umpca. relaxed start effective so-mpca. so-mpcars improvement face data sompca. gait data so-mpca-rs outperforms so-mpca rank rate rank rate average. addition so-mpca-rs better face recognition performance trod-rs umpca-rs improvetable rank rank gait recognition rates percentage nearest neighbor classiﬁer subset. results highlighted bold fonts indicates enough features extracted. ment respectively. gait data sompca-rs improves rank recognition rate trod-rs umpca-rs average so-mpca-rs improves rank recognition rate trod-rs umpca-rs. controlled experiments show effectiveness relaxed start so-mpca tvp-based multilinear methods possible explanation increases bias reduces variance learning model investigation needed. conclusion paper proposes novel multilinear algorithm setting named semi-orthogonal multilinear relaxed start proposed so-mpca approach learns features directly tensors maximize captured variance orthogonality constraint imposed mode. semi-orthogonality capture variance learn features fullorthogonality. furthermore introduced relaxed start strategy achieve better generalization ﬁxing starting projection vectors uniform vectors increase bias reduce variance learning model. experiments face gait recognition show sompca-rs achieves best overall performance compared competing algorithms. addition relaxed start also deng guo. transform-invariant uniﬁed approach fully automatic face alignment representation recognition. ieee trans. pattern. anal. mach. intell. sarkar p.j. phillips vega grother w.bowyer. human gait challenge problem data sets performance analysis. ieee trans. pattern. anal. mach. intell. shashua levin. linear image coding regression classiﬁcation using tensor-rank principle. proc. ieee int. conf. computer vision pattern recognition volume pages p.a. viola s.m. drucker. face recognition using discriminatively trained orthogonal rank tensor projections. proc. ieee int. conf. computer vision pattern recognition pages sukthankar. pcasift distinctive representation local image descriptors. proc. ieee int. conf. computer vision pattern recognition volume pages kokiopoulou saad. orthogonal neighborhood preserving projections projection-based dimensionality reduction technique. ieee trans. pattern. anal. mach. intell. plataniotis venetsanopoulos. uncorrelated multilinear principal component analysis unsupervised multilinear subspace learning. ieee trans. neural networks", "year": 2015}