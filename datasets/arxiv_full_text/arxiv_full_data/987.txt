{"title": "On the Expressive Power of Deep Learning: A Tensor Analysis", "tag": ["cs.NE", "cs.LG", "cs.NA", "stat.ML"], "abstract": "It has long been conjectured that hypotheses spaces suitable for data that is compositional in nature, such as text or images, may be more efficiently represented with deep hierarchical networks than with shallow ones. Despite the vast empirical evidence supporting this belief, theoretical justifications to date are limited. In particular, they do not account for the locality, sharing and pooling constructs of convolutional networks, the most successful deep learning architecture to date. In this work we derive a deep network architecture based on arithmetic circuits that inherently employs locality, sharing and pooling. An equivalence between the networks and hierarchical tensor factorizations is established. We show that a shallow network corresponds to CP (rank-1) decomposition, whereas a deep network corresponds to Hierarchical Tucker decomposition. Using tools from measure theory and matrix algebra, we prove that besides a negligible set, all functions that can be implemented by a deep network of polynomial size, require exponential size in order to be realized (or even approximated) by a shallow network. Since log-space computation transforms our networks into SimNets, the result applies directly to a deep learning architecture demonstrating promising empirical performance. The construction and theory developed in this paper shed new light on various practices and ideas employed by the deep learning community.", "text": "long conjectured hypotheses spaces suitable data compositional nature text images efﬁciently represented deep hierarchical networks shallow ones. despite vast empirical evidence supporting belief theoretical justiﬁcations date limited. particular account locality sharing pooling constructs convolutional networks successful deep learning architecture date. work derive deep network architecture based arithmetic circuits inherently employs locality sharing pooling. equivalence networks hierarchical tensor factorizations established. show shallow network corresponds decomposition whereas deep network corresponds hierarchical tucker decomposition. using tools measure theory matrix algebra prove besides negligible functions implemented deep network polynomial size require exponential size order realized shallow network. since log-space computation transforms networks simnets result applies directly deep learning architecture demonstrating promising empirical performance. construction theory developed paper shed light various practices ideas employed deep learning community. expressive power neural networks achieved depth. mounting empirical evidence given budget resources deeper goes better eventual performance however existing theoretical arguments support empirical ﬁnding limited. many attempts theoretically analyze function spaces generated network architectures dependency network depth size. prominent approach justifying power depth show deep networks efﬁciently express functions would require shallow networks super-polynomial size. refer scenarios instances depth efﬁciency. unfortunately existing results dealing depth efﬁciency h˚astad goldmann delalleau bengio martens medabalimi typically apply speciﬁc network architectures resemble ones commonly used practice. particular none results apply convolutional networks represent empirically successful widely used deep learning architecture date. limitation current results merely show existence depth efﬁciency without providing information frequent property shortcomings current theory ones motivated work. architectural features specialize convolutional networks compared classic feedforward fully-connected networks threefold. ﬁrst feature locality refers connection neuron neighboring neurons preceding layer opposed entire layer drive context image processing locality believed reﬂect inherent compositional structure data closer pixels image likely correlated. second architectural feature convolutional networks sharing means different neurons layer connected different neighborhoods preceding layer share weights. sharing together locality gives rise convolution motivated fact natural images semantic meaning pattern often depend location finally third architectural idea convolutional networks pooling essentially operator decimates layers replacing neural activations spatial window single value context images pooling induces invariance translations addition believed create hierarchy abstraction patterns neurons respond three architectural elements locality sharing pooling facilitated great success convolutional networks lacking existing theoretical studies depth efﬁciency. paper introduce convolutional arithmetic circuit architecture incorporates locality sharing pooling. arithmetic circuits networks types nodes nodes compute weighted inputs product nodes computing product inputs. nodes implement convolutions product nodes realize pooling. models arrive viewed convolutional networks product pooling linear point-wise activation. attractive three accounts. first discussed app. convolutional arithmetic circuits equivalent simnets deep learning architecture recently demonstrated promising empirical results various image recognition benchmarks second show sec. convolutional arithmetic circuits realizations hierarchical tensor decompositions opening door various mathematical algorithmic tools analysis implementation. third depth efﬁciency convolutional arithmetic circuits analyze sec. shown subsequent work cohen shashua superior depth efﬁciency popular convolutional rectiﬁer networks namely convolutional networks rectiﬁed linear activation average pooling. employing machinery measure theory matrix algebra made available connection hierarchical tensor decompositions prove number fundamental results concerning depth efﬁciency convolutional arithmetic circuits. main theoretical result states besides negligible functions realized deep network polynomial size require exponential size order realized even approximated shallow network. translated viewpoint tensor decompositions implies almost tensors realized hierarchical tucker decomposition cannot efﬁciently realized classic decomposition. best knowledge result unknown tensor analysis community advantage typically demonstrated speciﬁc examples tensors efﬁciently realized former latter. following main result present generalization compares networks arbitrary depths showing amount resources order maintain representational power trimming layers network grows double exponentially w.r.t. number layers off. also characterize cases dropping single layer bears exponential price. remainder paper organized follows. sec. brieﬂy review notations mathematical background required order follow work. followed sec. presents convolutional arithmetic circuits establishes equivalence tensor decompositions. theoretical analysis covered sec. finally sec. concludes. order keep manuscript reasonable length defer detailed survey related work app. covering works depth efﬁciency boolean circuits arithmetic circuits neural networks well different applications tensor analysis ﬁeld deep learning. begin establishing notational conventions used throughout paper. denote vectors using bold typeface e.g. coordinates vector referenced regular typeface subscript e.g. confused bold typeface subscript e.g. represents vector belongs sequence. tensors denoted letters calligraphic typeface e.g. rm×···×mn speciﬁc entry tensor referenced subscripts e.g. ad...dn superscripts used denote individual objects within collection. example stands vector stands tensor cases collection interest indexed multiple coordinates multiple superscripts referencing individual objects e.g. aljγ stand vector shorthand cartesian product euclidean space times notation finally positive integer shorthand denote turn establish baseline i.e. present basic deﬁnitions results broad comprehensive ﬁeld tensor analysis. list essentials required order follow paper referring interested reader hackbusch complete introduction ﬁeld straightforward view tensor simply multi-dimensional array ad...dn number indexing entries array also called modes referred order tensor. term dimension stands number values index take particular mode. example tensor appearing order dimension mode space possible conﬁgurations take called tensor space denoted quite naturally rm×···×mn central operator tensor analysis tensor product denoted operator intakes tensors orders respectively returns tensor order deﬁned d...dp ad...dp +...dp notice case tensor product reduces outer product vectors. speciﬁcally tensor product rank- matrix rm×m. denote joint tensor product v⊗···⊗v. context often shorthand called pure elementary regarded rank- ∀i). difﬁcult tensor expressed rank- representation called candecomp/parafac decomposition short decomposition cp-rank deﬁned minimum number terms decomposition i.e. minimal hold. notice tensor order i.e. matrix deﬁnition cp-rank coincides standard matrix rank. symmetric tensor invariant permutations indices. formally tensor order symmetric equal dimension modes every permutation indices following equality hold adπ...dπ ad...dn note vector tensor rm×···×m symmetric. moreover every symmetric tensor expressed linear combination referred symmetric decomposition symmetric cp-rank minimal decomposition exists. since symmetric decomposition particular standard decomposition symmetric cp-rank symmetric tensor always greater equal standard cp-rank. note case symmetric matrices symmetric cp-rank original cp-rank always equal. repeating concept paper measure zero. broadly analysis framed measure theoretical terms. introduction ﬁeld beyond scope paper possible intuitively grasp ideas form basis claims. dealing subsets euclidean space standard natural measure sense called lebesgue measure. measure consider analysis. measure zero thought zero volume space interest. example interval zero measure subset plane positive measure subset x-axis. alternative view zero measure follows property draws random point space continuous distribution probability point hitting necessarily zero. related term used throughout paper almost everywhere refers entire space excluding most zero measure. convolutional arithmetic circuits consider task classifying instance categories representing instances collections vectors natural many applications. case image processing example correspond image correspond vector arrangements patches around pixels. customary classiﬁcation carried maximization per-label score functions {hy}y∈y i.e. predicted label instance index score value maximal. attention thus directed functions instance space deﬁne hypotheses space following .fθm referred representation functions selected parametric family r}θ∈θ. natural choices family wavelets radial basis functions afﬁne functions followed point-wise activation coefﬁcient tensor order dimension mode. entries correspond basis point-wise }d...dn∈. often consider ﬁxed linearly independent representation functions .fθm case point-wise product functions linearly independent well correspondence score functions coefﬁcient tensors. keep manuscript concise defer derivation hypotheses space app. noting arises naturally notion tensor products spaces. eventual realize score functions layered network architecture. ﬁrst step along path notice fully determined activations representation functions .fθm input vectors words given {fθd}d∈i∈ score independent input. thus natural consider computation numbers ﬁrst layer networks. layer referred representation layer conceived convolutional operator channels corresponding different function applied input vectors constrained score functions structure depicted learning classiﬁer reduces estimation parameters coefﬁcient tensors computational challenge latter tensors order exponential number entries next subsections utilize tensor decompositions address computational challenge show naturally realized convolutional arithmetic circuits. conclude network illustrated implements classiﬁer decomposition refer network model. network consists representation layer followed single hidden layer turn followed output. hidden layer begins conv operator simply convolution channels receptive ﬁeld convolution operate without coefﬁcient sharing i.e. ﬁlters generate feature maps sliding across previous layer different coefﬁcients different spatial locations. often referred deep learning community locallyconnected operator obtain standard convolutional operator simply enforce coefﬁcient sharing constraining vectors decomposition equal different values following conv operator hidden layer includes global product pooling. feature maps generated conv reduced singletons multiplication entries creating vector dimension vector mapped network outputs ﬁnal dense linear layer. recap model shallow convolutional arithmetic circuit realizes decomposition universal i.e. realize coefﬁcient tensors large enough size unfortunately since cp-rank generic tensor exponential order size required model universal exponential deep network hierarchical decomposition subsection present deep network corresponds recently introduced hierarchical tucker tensor decomposition refer short decomposition. network dubbed model universal. speciﬁcally tensors represented model represented model polynomial penalty terms resources. advantage model show sec. almost cases generates tensors require exponential size order realized even approximated model. differently draws weights model continuous distribution probability resulting tensors cannot approximated polynomial model. informally implies model exponentially expressive model. model based hierarchical tensor decomposition special case decomposition presented hackbusch k¨uhn construction theoretical results apply general decomposition well specialization done merely bring forth network resembles current convolutional networks index stands level decomposition represents location within level corresponds individual tensor level location referred level-l rank deﬁned number tensors location level tensor φljγ order assume simplicity order power limit generality analysis). parameters decomposition ﬁnal level weights {aly rrl−}y∈ intermediate levels’ weights {aljγ rrl−}l∈j∈γ∈ ﬁrst level vectors {ajγ individual paramehierarchical decomposition universal i.e. large enough ranks represent tensors. moreover super-set decomposition tensors representable decomposition components also representable hierarchical decomposition ranks note comes polynomial penalty number parameters increases decomposition hierarchical decomposition. however show sec. gain expressive power exponential. plugging expression hierarchical decomposition score function given obtain network displayed model. network includes representation layer followed hidden layers turn followed output. shallow model hidden layers consist conv operators followed product pooling. difference instead single hidden layer collapsing entire spatial structure global pooling hidden layers pool size- windows decimating feature maps factor layers feature maps reduced singletons arrive structure nodes. mapped network outputs ﬁnal dense linear layer. note network’s size- pooling windows correspond fact hierarchical decomposition based full binary tree modes i.e. combines tensors time. focus setting solely simplicity presentation since presented hackbusch k¨uhn analysis could easily adapted hierarchical decompositions based trees would correspond networks different pooling window sizes resulting depths. model conceptually divided parts. ﬁrst representation layer transforming input vectors real-valued scalars {fθd}i∈d∈. second main part network view inference engine convolutional arithmetic circuit takes measurements produced representation layer accordingly computes class scores output layer. recap deep network refer model computes score functions coefﬁcient tensors hierarchically decomposed network universal sense enough channels tensors represented. moreover model super-set shallow model presented sec. question depth efﬁciency naturally arises. particular would like know functions represented polynomially sized deep model require exponential size shallow model. answer described sec. almost functions realizable model meet property. words functions realizable polynomial model measure zero space functions realizable given polynomial model. shared coefﬁcients convolution conv operator networks implements local linear transformation coefﬁcients generally location-dependent. special case coefﬁcients depend location i.e. remain ﬁxed across space local linear transformation becomes standard convolution. refer setting coefﬁcient sharing. sharing widely used structural constraint pillars behind successful convolutional network architecture. context image processing sharing motivated observation natural images semantic content pattern often depend location. subsection explore effect sharing expressiveness networks speciﬁcally coefﬁcient tensors represent. model sharing universal represent symmetric tensors. case model sharing amounts applying following constraints hierarchical decomposition aln/lγ every .rl. note case universality lost well nonetheless generated tensors limited symmetric already demonstrating expressive advantage deep models shallow ones. sec. take showing shared model exponentially expressive model even latter constrained sharing. ﬁrst contribution paper presented sec. equivalence deep learning architectures successfully employed practice tensor decompositions. namely showed convolutional arithmetic circuits fact simnets demonstrated promising empirical performance formulated hierarchical tensor decompositions. second contribution make established link arithmetic circuits tensor decompositions combining theoretical tools worlds prove results interest deep learning tensor analysis communities. focus current section. theorem tensor order dimension mode generated recursive formulas deﬁne min{r consider space possible conﬁgurations parameters composition {aljγ}ljγ. space generated tensor cp-rank least almost everywhere differently conﬁgurations cp-rank less form measure zero. exact result holds constrain composition shared i.e. aljγ consider space {alγ}lγ conﬁgurations. corollary given linearly independent representation functions {fθd}d∈ randomizing weights model continuous distribution induces score functions probability cannot approximated arbitrarily well model less min{r m}n/ hidden channels. result holds even constrain model weight sharing leaving model general form. besides negligible functions realized polynomially sized model require exponential size order realized even approximated model. previous works relating depth efﬁciency merely show existence functions separate depths corollary hand establishes depth efﬁciency almost functions deep network implement. equally importantly applies deep learning architectures successfully employed practice adopting viewpoint tensor analysis thm. states besides negligible tensors realized decomposition cannot represented classic decomposition latter less exponential number terms best knowledge result never proved tensor analysis community. original paper introducing decomposition motivating example authors present speciﬁc tensor efﬁciently realizable decomposition requiring exponential number terms decomposition result strengthens motivation considerably showing speciﬁc tensor favors rather almost tensors realizable exhibit preference. taking account tensor realized also realized polynomial penalty number parameters implies asymptotic sense decomposition exponentially efﬁcient decomposition. prove thm. combine approaches worlds circuit complexity tensor decompositions. ﬁrst class machinery employ matrix algebra proven powerful source tools analyzing complexity circuits. example arithmetic circuits analyzed called partial derivative matrix boolean circuits widely used tool communication matrix gain access matrix algebra arranging tensors take part decompositions matrices process often referred matricization. matricization tensor product translates kronecker product properties latter become readily available. second tool-set make measure theory prevails study tensor decompositions much less frequent analyses circuit complexity. order frame stated sec. decomposition thm. applies actually special case decomposition introduced hackbusch k¨uhn however theorem proof easily adapted account general case. focus special case merely corresponds convolutional arithmetic circuit architectures used practice. problem measure theoretical terms obviously needs deﬁne measure space interest. tensor decompositions straightforward space focus decomposition variables. general circuits hand often unclear deﬁning measure space appropriate. however circuits considered context machine learning usually parameterized deﬁning measure space parameters effective approach studying prevalence various properties hypotheses spaces. proof thm. traverses following path. begin showing matricizing rank- tensor produces rank- matrix. implies matricization tensor generated decomposition terms rank turn show matricization tensor generated decomposition rank least min{r m}n/ almost everywhere. done induction levels decomposition ﬁrst level combination measure theoretical linear algebraic arguments show generated matrices maximal rank almost everywhere. induction step facts matricization tensor product translates kronecker product latter increases ranks multiplicatively imply matricization ranks current level generally equal previous level squared. measure theoretical claims made ensure indeed takes place almost everywhere. prove corollary based thm. need show inability model realize tensor generated model implies former cannot approximate score functions produced latter. general tensors expressible decomposition topologically closed implies a-priori model approximate tensors generated model even though cannot realize them. however since proof thm. achieved separation matrix rank distances indeed positive model cannot approximate model’s tensors almost always. translate tensors score functions simply note ﬁnite-dimensional hilbert space convergence norm implies convergence coefﬁcients basis. therefore space score functions convergence norm implies }d...dn∈. thm. corollary compare expressive power deep model shallow model argue analysis lacking convey information regarding importance individual layer. particular shed light advantage deep networks present provide state recognition accuracy compared networks moderate depth. purpose present generalization specifying amount resources order maintain representational power layers incrementally deep network. conciseness defer analysis app. merely state ﬁnal conclusions. representational penalty double exponential w.r.t. number layers removed. addition certain cases removal even single layer leads exponential inﬂation falling line suggestion bengio denotes kronecker product matrices rankb) rank·rank. hence deﬁnition border rank hackbusch work address fundamental issue deep learning expressive efﬁciency depth. many attempts theoretically analyze question practical machine learning perspective existing results limited. results apply speciﬁc types networks resemble ones used practice none results account localitysharing-pooling paradigm forms basis convolutional networks successful deep learning architecture date. addition current analyses merely show existence depth efﬁciency i.e. functions efﬁciently realizable deep networks shallow ones. practical implications ﬁndings arguably slight a-priori small fraction functions realizable deep networks enjoy depth efﬁciency rest shallow networks sufﬁce. paper develop theory facilitates analysis depth efﬁciency networks incorporate widely used structural ingredients locality sharing pooling. consider task classiﬁcation ﬁnite categories instance space deﬁned cartesian product vector spaces compliance common practice representing natural data ordered local structures vectors compose instance represented descriptor length generated running vector representation functions. customary classiﬁcation achieved maximization score functions every category score function linear combination possible products formed taking descriptor entry every input vector. coefﬁcients linear combinations conveniently reside tensors order dimension along axis. construct networks compute score functions decomposing coefﬁcient tensors resulting networks convolutional arithmetic circuits incorporate locality sharing pooling operate descriptor entries generated input. show shallow network realizes classic tensor decomposition whereas deep network hidden layers realizes recently introduced hierarchical tucker decomposition fundamental result presented thm. corollary states randomizing weights deep network continuous distribution lead probability score functions cannot approximated shallow network latter’s size exponential extend result deriving analogous claims compare networks depths deep shallow. highlight connection networks ones used practice show translating convolution product pooling computations log-space gives rise simnets recently proposed deep learning architecture shown produce state accuracy computationally limited settings besides central line work discussed above construction theory presented paper shed light various conjectures practices employed deep learning community. first respect pooling operation analysis points possibility perhaps factorization computed functions translation invariance. serve explanation fact pooling windows state convolutional networks typically small often much smaller radius translation would like invariant indeed framework second point theory sheds light sharing. discussed sec. introducing weight sharing shallow network considerably limits expressive power. network represent symmetric tensors turn means location invariant w.r.t. input vectors case deep network limitation posed sharing strict. generated tensors need symmetric implying network capable modeling location crucial ability almost real-world task. ﬁndings suggest sharing constraint increasingly limiting network gets shallower point causes complete ignorance location. could serve argument supporting empirical success deep convolutional networks bind together statistical computational advantages sharing many layers mitigate expressive limitations. lastly construction advocates locality speciﬁcally receptive ﬁelds. recent convolutional networks providing state recognition performance szegedy make extensive linear transformations proving successful practice. view model operators factorize tensors providing universality minimal number parameters. seems reasonable conjecture task factorizing coefﬁcient tensors larger receptive ﬁelds signiﬁcantly helpful lead redundancy deteriorate performance presence limited training data. investigation conjecture left future work. amnon shashua would like thank tomaso poggio shai shwartz illuminating discussions preparation manuscript. would also like thank tomer galanti tamir hazan lior wolf commenting draft versions paper. work partly funded intel grant icri-ci center grant nadav cohen supported google fellowship machine learning. references animashree anandkumar rong daniel sham kakade matus telgarsky. tensor decompositions learning latent variable models. journal machine learning research monica bianchini franco scarselli. complexity neural network classiﬁers comparison shallow deep architectures. neural networks learning systems ieee transactions andr´as hajnal wolfgang maass pavel pudl´ak m´arl´o szegedy gy¨orgy tur´an. threshold circuits bounded depth. foundations computer science annual symposium pages ieee vadim lebedev yaroslav ganin maksim rakhuba ivan oseledets victor lempitsky. speeding-up convolutional neural networks using fine-tuned cp-decomposition. corr abs/. cs.cv james martens arkadev chattopadhya toni pitassi richard zemel. representational efﬁciency restricted boltzmann machines. advances neural information processing systems pages guido montufar razvan pascanu kyunghyun yoshua bengio. number linear regions deep neural networks. advances neural information processing systems pages hendra setiawan zhongqiang huang jacob devlin thomas lamar rabih zbib richard schwartz john makhoul. statistical machine translation features multitask tensor networks. proceedings annual meeting association computational linguistics international joint conference natural language processing asian federation natural language processing cs.cl richard socher danqi chen christopher manning andrew reasoning neural tensor networks knowledge base completion. advances neural information processing systems pages maxwell stinchcombe halbert white. universal approximation using feedforward networks noninternational joint conference neural networks pages sec. presented fundamental theorem network capacity showing besides negligible functions realized polynomially sized model require exponential size order realized even approximated model. terms network depth models represent extremes former single hidden layer achieved global pooling whereas latter hidden layers achieved minimal pooling windows. interest generalize fundamental result establishing comparison networks intermediate depths. focus current appendix. difference decomposition original instead completing full process levels stop lc≤l. point remaining tensors binded together form ﬁnal order-n tensor. corresponding network simply include premature global pooling stage shrinks feature maps ﬁnal linear layer performs classiﬁcation. before consider shared version decomposition aljγ alγ. notice construction realizes continuum models correspond extreme cases respectively. following theorem generalization thm. compares truncated decomposition levels levels implements tensor quantifying penalty terms parameters theorem tensors order dimension mode generated truncated recursive formulas levels respectively. denote composition ranks respectively. assuming w.l.o.g. deﬁne consider space possible conﬁgurations parameters min{r composition {aljγ}ljγ. space almost everywhere generated tensor requires wishes equal differently conﬁgurations realized form measure zero. exact result holds constrain composition shared i.e. aljγ consider space {alγ}lγ conﬁgurations. corollary suppose given linearly independent representation functions .fθm consider networks correspond truncated hierarchical tensor decomposition hidden layers respectively. assume w.l.o.g. i.e. network deeper network deﬁne minimal number channels across representation layer ﬁrst hidden layers network then randomize weights network continuous distribution obtain probability score functions cannot approximated arbitrarily well network latter less channels last hidden layer. result holds even constrain network weight sharing leaving network general form. proofs thm. corollary given app. hereafter brieﬂy discuss implications. first notice indeed obtain generalization fundamental theorem network capacity corresponds extreme case second note baseline case i.e. full-depth network generated target score function approximating truncated network draws price grows double exponentially w.r.t. number missing layers. third intriguingly considerably smaller i.e. signiﬁcantly truncated network sufﬁcient model problem cutting even single layer leads exponential price price independent scenarios exponential penalty trimming single layer discussed bengio context speciﬁc functions realized networks resemble ones used practice example result). prove much broader practical setting showing convolutional arithmetic circuit architectures almost function realized signiﬁcantly truncated network exhibit behavior. issue relates empirical practice supporting common methodology designing networks deep possible. speciﬁcally encourages extending network depth pooling small regions avoiding signiﬁcant spatial decimation brings network termination closer. conclude appendix stressing construction theoretical approach limited models covered theorems merely exemplars deemed appropriate initial analysis. fundamental generalized theorems network capacity similar spirit analogous theorems networks different pooling window sizes depths easily derived. proof theorems proof thm. relies basic knowledge measure theory speciﬁcally lebesgue measure spaces. provide comprehensive background ﬁeld rather supplement brief discussion given sec. list facts using necessarily intuitive above entirety paper measure spaces consider euclidean spaces equipped lebesgue measure. thus d-dimensional points zero measure mean lebesgue measure d-dimensional euclidean space zero. moving preliminaries matrix tensor theory denote matricization order-n tensor rows correspond modes columns correspond even modes. namely rm×···×mn matrix m·m· .·mn− rows m·m· .·mn columns rearranging entries tensor ad...dn stored j=i+ distinguish tensor product operation denote kronecker product matrices speciﬁcally matrices rm×m rn×n matrix rmn×mn holds aijbkl index column index basic relation binds together tensor product matricization kronecker product tensors even orders. additional facts make matricization linear operator less trivially matrices rank equal rank rank proof). facts along basic relation laid above lead conclusion that words order-l tensor given cp-decomposition terms matricization rank thus prove certain order-l tensor cp-rank least sufﬁces show matricization rank least lemma deﬁne following mapping taking three matrices rm×n rm×n rn×n simply holds ﬁrst elements holds following elements diagonal matrix holds last elements diagonal. deﬁne product matrix rm×m consider points rank different min{m points zero measure. result also hold points reside elements used assign proof obviously rank) remains show rank) zero measure. top-left sub-matrix non-singular course rank) required. thus sufﬁces show points zero measure. polynomial entries either vanishes zero measure zero polynomial left disqualify latter option done ﬁnding speciﬁc point indeed choose identity matrix hold main diagonal otherwise. selection implies identity matrix particular lemma assume continuous mappings rm×n taking point matrices .ap. assume mappings points every satisﬁes rank) form zero measure. deﬁne mapping rm×n given then points rank) form zero measure. proof denote rank) would like show zero measure. ﬁrst note since continuous mapping matrices rm×n rank less closed closed particular measurable. strategy computing measure follows. every deﬁne marginal rank) show every zero measure measure zero. application fubini’s theorem prove desired result. points rank) assumption zero measure. show measure zero. deﬁnition exists rank) w.l.o.g. assume top-left sub-matrix non-singular. regarding ﬁxed determinant top-left sub-matrix polynomial elements zero polynomial setting yields determinant latter’s top-left sub-matrix non-zero. non-zero polynomial determinant top-left sub-matrix vanishes zero measure implies indeed measure zero. introduce notations towards application fubini’s theorem. first symbol used represent indicator functions e.g. function receives elsewhere. second subscript indicate corresponding intersected hyper-rectangle radius example stands intersection stands intersection sets consider measurable subscript ﬁnite measure. thus apply fubini’s theorem decomposition decomposition proof begin case unshared composition i.e. given denoting convenience show induction almost everywhere w.r.t. {aljγ}ljγ cp-ranks tensors {φljγ}j∈γ∈ least rl/. accordance discussion beginning subsection sufﬁces consider matricizations show ranks greater equal almost everywhere. denote rm×r matrix columns {aj−α}r rm×r matrix columns rr×r diagonal matrix diagonal. then write {ajα}r according lemma rank equals min{r almost everywhere merely recall dimensions zero measure subset rd+d. ﬁnite union zero measure sets zero measure thus fact rank holds almost everywhere individually implies holds almost everywhere jointly proves inductive hypothesis denote .rl−. inductive assumption general property rank rank·rank almost everywhere ranks matrices noticing {mα} depend aljγ turn attention lemma lemma tells rank almost everywhere. since ﬁnite union zero measure sets zero measure conclude almost everywhere rank holds jointly completes proof theorem unshared case. head prove thm. generalization thm. proof similar nature thm. slightly technical. short idea show generic case expressing tensor products tensors order requires least rn/l terms. since expressed terms demanding implies rn/l gain technical advantage utilize known results matrix theory introduce tensor squeezing operator operator receives tensor order divisible returns tensor obtained merging together latter’s modes groups size speciﬁcally applied tensor rm×···×mc·q returns tensor order holds ad...dc·q location deﬁned following index every mode j=i+ mj+q. notice applied tensor order returns vector. also note tensors orders divisible scalar desirable properties tensors order implies rn/l applying sides vectors. thus prove thm. sufﬁces show generic case cp-rank least rn/l alternatively rank matricization least rn/l strategy following proof proof accordance discussion sufﬁces show generic case rank)] rn/l ease path reader reformulate problem using slightly simpler notations. order-n tensor dimension mode generated follows positive integer smaller tensor squeezing operator merges groups modes. deﬁne min{r matricization operator deﬁned beginning appendix task prove rank] rn/l almost everywhere w.r.t. {aljγ}ljγ. also consider case shared parameters aljγ would like show condition holds almost everywhere w.r.t. {alγ}lγ. strategy proving claim inductive. show almost everywhere holds rank] rl−l treat special case showing indeed rank] rn/l begin setting unshared parameters afterwards attend scenario shared parameters well. ﬁrst task treat case i.e. show rank] almost everywhere jointly here already matrices). since union ﬁnitely many zero measure sets zero measure sufﬁces show condition holds almost everywhere speciﬁc chosen. denote vector holding entry elsewhere vector zeros vector ones. suppose every assign otherwise. suppose also aljγ otherwise. finally assume aljγ settings imply every φl−jγ i.e. tensor φl−jγ holds location elsewhere. φl−jγ zero tensor. conclude indices thus write since different other matrix rank however prove inductive hypothesis merely showed speciﬁc parameter assignment holds need show almost everywhere. consider sub-matrix non-singular speciﬁc parameter assignment deﬁned. determinant sub-matrix polynomial elements {aljγ}ljγ know vanish speciﬁc assignments deﬁned. thus polynomial vanishes subset {aljγ}ljγ zero measure sub-matrix rank almost everywhere thus rank least almost everywhere. completes treatment case turn prove propagation inductive hypothesis. assume inductive hypothesis holds speciﬁcally assume almost everywhere w.r.t. {aljγ}ljγ rank] rl−−l jointly would like show almost everywhere rank] rl−l jointly again fact ﬁnite union zero measure sets zero measure implies prove condition speciﬁc applying squeezing operator followed .rl− denote matrix fact kronecker product multiplies ranks along inductive assumption imply almost everywhere rank rl−−l rl−−l rl−l noting matrices depend aljγ apply lemma conclude almost everywhere rank] rl−l completes prove inductive propagation. next treat special case assume almost everywhere rank] rl−−l jointly again apply squeezing operator followed matricization time sides expression before denote l−l+ .rl−. using multiplicative rank property kronecker product along inductive assumption almost everywhere rl−−l rl−l. noticing {mα}α∈ depend apply lemma last time almost everywhere rank least rl−l. completes proof case unshared parameters. proving theorem case shared parameters done exact above. fact omit references proof apply. notice particular speciﬁc parameter assignment deﬁned handle completely symmetric i.e. include dependence proof corollaries corollaries direct continuation thm. respectively. theorems shown almost coefﬁcient tensors generated deep network cannot realized shallow network latter meet certain minimal size requirement. corollaries take further stating given linearly independent representation functions .fθm efﬁcient realization coefﬁcient tensors generally impossible also efﬁcient approximation score functions. prove extra step recall proofs thm. order show separation coefﬁcient tensor deep network shallow network relied matricization rank. speciﬁcally derived constants matricization deep network’s coefﬁcient tensor rank greater equal whereas matricization shallow network’s coefﬁcient tensor rank smaller equal given observation corollaries readily follow lemma lemma .fθm∈l linearly independent functions denote space tensors order dimension mode. given tensor denote means particular tensor space lies closure {a}∞ accordingly order show distance {aλ}λ∈λ strictly positive equivalently distance matrix family matrices {}λ∈λ strictly positive. however direct implication assumption rank) rank). order keep body paper reasonable length presentation hypotheses space sec. provide grounds deﬁnition. appendix derive hypotheses space step step. establishing basic preliminaries topic spaces utilize notion tensor products spaces reach universal representation make empirical studies characterizing statistics natural images argue practice moderate value sufﬁces. preliminaries spaces dealing functions scalars vectors collections vectors consider spaces formally hilbert spaces lebesgue measurable square-integrable real functions equipped standard addition scalar multiplication well inner-product deﬁned integral pointwise multiplication. topic function spaces lies heart functional analysis requires basic knowledge measure theory. present bare necessities required follow appendix referring interested reader rudin comprehensive introduction. vector space inﬁnite dimensional functions referred total closure span covers entire space i.e. function exist functions regarded linearly independent ﬁnite subsets linearly independent i.e. non-trivial result states spaces general must contain total linearly independent sets moreover contains countable type. seems reasonable draw analogy total linearly independent sets space bases ﬁnite dimensional vector space. analogy indeed appropriate perspective total linearly independent sets confused bases spaces typically deﬁned orthonormal. shown natural numbers {fd}d∈n fdi}d...dn∈n induced point-wise product functions form total linearly independent respectively products hilbert spaces. deﬁnitions given sec. tensor tensor space tensor product actually concrete special cases much deeper abstract algebraic concepts. formal line presentation considers multiple vector spaces deﬁnes tensor product space v⊗···⊗vn speciﬁc quotient space space freely generated cartesian product set. every combination vectors exists corresponding element v⊗···⊗v tensor product space moreover elements form span entire space. hilbert spaces possible equip v⊗···⊗vn natural inner-product operation thereby turning hilbert space. total linearly independent elements form shown sets total linearly independent respectively v⊗···⊗vn finally underα construction recall sec. instance space deﬁned accordance common practice representing natural data ordered local structures classify instances categories maximization per-label score functions r}y∈y. hypotheses space deﬁned app. stated {fd}d∈n total i.e. every function arbitrarily well approximated linear combination ﬁnite subset {fd}d∈n pointsubset score functions taken. fdi}d...dn∈n form total accordingly wise products universal hypotheses space score function arbitrarily well approximated ﬁnite linear combinations point-wise products. possible formulation would follows. assume interested \u0001-approximation score function consider formal tensor modes countable inﬁnite dimension mode indexed then exists tensor ﬁnite number entries zero which given functions {fd}d∈n⊂l total deﬁnes universal hypotheses space. many possibilities choosing total functions. wavelets perhaps obvious choice indeed used deep network setting bruna mallat special case gabor wavelets claimed induce features resemble representations visual cortex options special attention importance practice cases underlying parametric family functions r}θ∈θ countable total subset chosen. fact gaussians total proven girosi poggio direct corollary stone-weierstrass theorem. achieve countability simply consider gaussians rational parameters practice choice gaussians give rises similarity operator described simnet architecture cohen case neurons must restrict domain bounded otherwise functions integrable. however limitation practice indeed neurons widely used across many application domains. fact neurons total proven cybenko hornik threshold sigmoid activations. generally proven stinchcombe white wide class activation functions including linear combinations relu. pinkus survey results. countability restrict parameters rational. possible choose .fθm sufﬁce order represent score functions required natural tasks. moreover claim need large argument relies statistical properties natural images fully detailed app. implies proper choice }d...dn∈ spans score functions interest deﬁne label tensor order dimension mode that exactly hypotheses space presented sec. notice {fθd }d∈⊂l linearly independent product functions coefﬁcient tensor words score functions identical coefﬁcient tensors same. finite function bases classiﬁcation natural data app. laid framework classifying instances space labels maximization per-label score functions {fθ}d∈ selected parametric family functions r}θ∈θ. universality i.e. ability score functions approximate function required possible choose countable subset total noted families gaussians neurons meet requirement. subsection formalize argument ﬁnite value sufﬁcient represents natural data particular natural images. based empirical studies characterizing statistical properties natural images compliance number channels typical convolutional network layer order typically sufﬁces. distribution labeled instances induced marginal distribution instances would like show given particular assumptions exist functions tensors order dimension mode score functions deﬁned achieve classiﬁcation error words classiﬁcation error score functions bounded optimal expected hinge-loss plus term equal twice score approximation error. recall constrain optimal score functions way. thus assuming label deterministic given instance optimal expected hinge-loss essentially zero classiﬁcation error dominated score approximation error problem thus translates showing selected small. point introduce main assumption distribution speciﬁcally marginal distribution instances according various studies natural settings marginal distribution individual vectors e.g. small patches images relatively well captured gaussian mixture model moderate number distinct components. example shown zoran weiss natural image patches size essentially modeled gmms components complies common belief moderate number low-level templates sufﬁces order model vast majority local image patches. following line model marginal distribution components means assume components well localized i.e. standard deviations small compared distances means also compared variation target functions context images example latter assumptions imply local patch unambiguously assigned template assignment patches templates determines class image. returning general instances probability mass concentrated distinct regions space every vector lies near score functions approximately constant region. important stress assume statistical independence xi’s possible values quantized templates idealized assumptions expectation score approximation error turn show .fθm chosen separate components i.e. every functions gaussians simply mean standard deviations enough function effectively vanishes neurons additional requirement needed namely component means linearly separable. words require every exist positive negative otherwise. seem like strict assumption ﬁrst glance notice dimension often large even larger number components addition input vectors normalized unit length also normalized thus linear separability trivially met. assuming linear separability threshold relu activations indeed sigmoid activations need scale given following convenient form every plugging score approximation error zero. recap shown parametric functions gaussians neurons score functions given universal also achieve zero classiﬁcation error moderate value underlying data distribution natural. context regarded natural satisﬁes conditions. ﬁrst rather mild requires label completely determined instance. example image belong category probability rest categories probability zero. second condition restrictive states input vectors composing instance quantized moderate number templates. assumption natural images exhibit property based various empirical studies shown hold approximately. since hold exactly analysis approximate implication practice classiﬁcation error introduced constraining score functions format given negligible compared sources error classic approach theoretically analyzing power depth focused investigation computational complexity boolean circuits. early result known exponential efﬁciency depth summarized follows every integer boolean functions computed circuit comprising alternating layers gates depth polynomial size limits depth less exponentially large circuit required. sipser formal statement classic result. recently rossman established somewhat stronger result showing cases polynomially wide shallow boolean circuits incapable exact realization also approximation classical results related threshold circuits class models similar contemporary neural networks boolean circuits. namely viewed neural networks neuron computes weighted inputs followed threshold activation threshold circuits main known result context existence functions separate depth depth statement relating exact realization techniques maass martens extension approximation). recent studies focus arithmetic circuits whose nodes typically compute either weighted product inputs special case sum-product networks presented poon domingos spns class deep generative models designed efﬁciently compute probability density functions. summation weights typically constrained non-negative addition order valid additional architectural constraints needed widely known theoretical arguments regarding efﬁciency depth spns given delalleau bengio work speciﬁc families spns considered comprising alternating product layers family whose nodes form full binary tree family nodes layer connected nodes preceding layer. authors show functions implemented networks require exponential number nodes order realized shallow arithmetic circuit directed acyclic graph nodes incoming edges correspond inputs nodes outgoing edges correspond outputs remaining nodes either labeled product. product node computes product child nodes. node computes weighted child nodes weights parameters linked incoming edges. hidden-layer networks). limitations work twofold. first authors note themselves analyzes ability shallow networks realize exactly functions generated deep networks provide result relating approximation. second speciﬁc families considered work universal hypothesis classes resemble networks used practice. recently martens medabalimi proved exist functions efﬁciently computed decomposable complete spns depth require depth less super-polynomial size exact realization. analysis treats approximation limited case separating depth depth spns. additionally deals speciﬁc separating functions convey information regarding frequent are. words according analysis almost functions generated deep networks efﬁciently realized shallow networks pathological functions hold. limitation analysis general separation depths based multilinear circuit result yehudayoff translates network follow common practices deep learning. recent attempts analyze efﬁciency network depth settings well. commonly used type neural networks days includes neurons compute weighted inputs followed rectiﬁed linear unit activation max{ z}). pascanu montufar study number linear regions expressed networks function depth width thereby showing existence functions separating deep shallow networks. telgarsky shows simple construction depth width relu network operates one-dimensional inputs realizing function cannot approximated relu networks depth width polynomial eldan shamir provides functions expressible relu networks depth polynomial width approximated depth network latter’s width exponential. result paper applies relu activation also standard sigmoid generally universal activation bianchini scarselli also considers different types activations studying topological complexity decision regions function network depth width activation type. results paper establish existence deep shallow separating functions case polynomial activation. works address conventional neural networks account structure convolutional networks successful deep learning architectures date importantly prove existence separating functions without providing insight frequent are. ﬁrst incorporate ideas ﬁeld tensor analysis deep learning. socher setiawan hutchinson proposed different neural network architectures include tensor-based elements exhibit various advantages terms expressiveness and/or ease training. janzamin alternative algorithm training neural networks proposed based tensor decomposition fourier analysis proven generalization bounds. novikov anandkumar yang dunson song algorithms tensor decompositions used estimate parameters different graphical models. notably song uses relatively hierarchical tucker decomposition employ work certain similarities formulations. works differ considerably objectives though song focuses proposal training algorithm purpose work analyze expressive efﬁciency networks depends depth. recently lebedev modeled ﬁlters convolutional network four dimensional tensors used decomposition construct efﬁcient accurate approximation. another work draws connection tensor analysis deep learning recent study presented haeffele vidal work shows sufﬁciently large neural networks matter training initialized exists local optimum accessible gradient descent local optimum approximately equivalent global optimum terms objective value. practical issue faces implementing arithmetic circuits numerical instability product operation product node large number inputs easily susceptible numerical overﬂow underﬂow. common solution perform computations log-space i.e. instead computing activations compute log. requires activations non-negative begin with alters turning networks requirement activations non-negative limit universality. reason functions non-negative cases interest gaussians neurons addition always common offset coefﬁcient tensors ensuring positive without affecting classiﬁcation. non-negative decompositions found leading network activations non-negative. general non-negative tensor decompositions less efﬁcient unconstrained decompositions cases non-negative tensor supports unconstrained decomposition smaller minimal non-negative decomposition. nevertheless shall soon non-negative decompositions translate proven architecture demonstrated achieve comparable performance state convolutional networks thus practice deterioration efﬁciency seem signiﬁcant. na¨ıvely implementing model log-space translates activation following locally connected linear transformations product pooling turning pooling activation following pooling. however applying activations described without proper handling inputs computational layer would result numerically stable computation simnet architecture cohen naturally brings forth numerically stable implementation networks. architecture based ingredients ﬂexible similarity measure operator similarity layer capable computing common convolutional operator well weighted norm realize representation computing whereas naturally implement log-sum-exp sum-pooling meanj{xj}) numerically stable manner. simnets capable correctly efﬁciently implementing networks already demonstrated perform well state convolutional networks several image recognition benchmarks outperform computational resources limited. na¨ıve implementation softmax numerically stable involves storing directly. result identical exponentiate negative numbers least numbers equal zero", "year": 2015}