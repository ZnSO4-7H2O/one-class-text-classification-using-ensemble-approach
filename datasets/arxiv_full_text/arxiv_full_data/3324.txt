{"title": "Top-k Multiclass SVM", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Class ambiguity is typical in image classification problems with a large number of classes. When classes are difficult to discriminate, it makes sense to allow k guesses and evaluate classifiers based on the top-k error instead of the standard zero-one loss. We propose top-k multiclass SVM as a direct method to optimize for top-k performance. Our generalization of the well-known multiclass SVM is based on a tight convex upper bound of the top-k error. We propose a fast optimization scheme based on an efficient projection onto the top-k simplex, which is of its own interest. Experiments on five datasets show consistent improvements in top-k accuracy compared to various baselines.", "text": "class ambiguity typical image classiﬁcation problems large number classes. classes difﬁcult discriminate makes sense allow guesses evaluate classiﬁers based top-k error instead standard zero-one loss. propose top-k multiclass direct method optimize top-k performance. generalization well-known multiclass based tight convex upper bound top-k error. propose fast optimization scheme based efﬁcient projection onto top-k simplex interest. experiments datasets show consistent improvements top-k accuracy compared various baselines. number classes increases important issues emerge class overlap multilabel nature examples phenomenon asks adjustments evaluation metrics well loss functions employed. predictor allowed guesses penalized mistakes evaluation measure known top-k error. argue important metric inevitably receive attention future illustration figure indicates. obvious figure shows examples different classes? imagine human predict correctly ﬁrst attempt? even make sense penalize learning system mistakes? problem class ambiguity apparent computer vision similar problems arise domains number classes becomes large. propose top-k multiclass generalization well-known multiclass based tight convex upper bound top-k zero-one loss call top-k hinge loss. turns similar top-k version ranking based loss proposed show top-k hinge loss lower bound version thus tighter bound top-k zero-one loss. propose efﬁcient implementation based stochastic dual coordinate ascent ingredient optimization projection onto top-k simplex. projection turns tricky generalization continuous quadratic knapsack problem respectively projection onto standard simplex. proposed algorithm solving complexity implementation top-k multiclass scales large datasets like places million examples classes finally extensive experiments several challenging computer vision problems show top-k multiclass consistently improves top-k error multiclass one-vs-all methods based different ranking losses top-k loss multiclass classiﬁcation multiclass classiﬁcation given training examples along corresponding labels feature space labels. task learn linear predictors risk classiﬁer maxy∈y minimized given loss function usually chosen convex upper bound zero-one loss. generalization nonlinear predictors using kernels discussed below. classiﬁcation problem becomes extremely challenging presence large number ambiguous classes. natural case extend evaluation protocol allow guesses leads popular top-k error top-k accuracy performance measures. formally consider ranking labels induced prediction scores bracket denote permutation labels index j-th largest score i.e. section review multiclass crammer singer extended top-k multiclass following. mainly follow notation given training pair multiclass loss example deﬁned since optimization scheme based fenchel duality also require convex conjugate primal loss function −eyi ones vector j-th standard basis vector deﬁned componentwise directly corresponds top-k zero-one loss errk margin note function ignores values ﬁrst scores could quite large highly similar classes. would model long correct prediction within ﬁrst guesses. however function unfortunately nonconvex since function returning k-th largest coordinate nonconvex therefore ﬁnding globally optimal solution computationally intractable. instead propose following convex upper bound call top-k hinge loss also deﬁne arises naturally effective domain conjugate analogy call top-k simplex reduces standard simplex inequality constraint deﬁnition top-k simplex convex polytope deﬁned crucial difference standard simplex upper bound xi’s limits maximal contribution total figure illustration. ﬁrst technical contribution work follows. proposition primal-conjugate pair top-k hinge loss given follows bound holds equality otherwise top-k loss strictly better upper bound actual top-k zero-one loss. employed larank optimized approximation show loss function optimized exactly efﬁciently within prox-sdca framework. multiclass binary reduction. also possible compare directly ranking based methods solve binary problem using following reduction. employ experiments evaluate ranking based methods svmperf toppush trick augment training embedding using feature mapping places y-th position puts zeros everywhere else. example labeled labeled therefore training examples dimensional features. moreover establishes relation original multiclass problem. another approach general performance measures given turns using reduction show certain constraints classiﬁer recallk equivalent top-k error. convex upper bound recallk optimized structured svm. convex upper bound recallk decomposable instance based loss directly comparable loss. theoretically elegant approach scale large datasets. begin general -regularized multiclass classiﬁcation problem notational convenience keep loss function unspeciﬁed. multiclass top-k multiclass obtained plugging corresponding loss function fenchel duality -regularized multiclass classiﬁcation problems rd×n matrix training examples rd×m matrix primal variables obtained stacking vectors rm×n matrix dual variables. prove main result section ﬁrst impose technical constraint loss function compatible choice ground truth coordinate. top-k hinge loss section satisﬁes requirement show proposition also prove auxiliary lemma used theorem deﬁnition convex function j-compatible {hjx| rm}. using rank- update moore-penrose pseudoinverse since ker⊥ last term compute y−yjej. finally fact j-compatible prove zero satisﬁed equality. therefore sup{y yjej lemma compute convex conjugates loss functions. theorem yi-compatible regularization parameter gram matrix. primal fenchel dual objective functions given finally show theorem applies loss functions consider. proposition top-k hinge loss function section yi-compatible. proof. consider loss proposition optimization scheme employ proximal stochastic dual coordinate ascent framework shalev-shwartz zhang strong convergence guarantees easy adapt problem. particular iteratively update batch dual variables corresponding training pair maximize dual objective theorem also maintain primal variables stop relative duality procedure summarized algorithm make comments advantages proposed method. first apart update step discuss below main operations computed using blas library makes overall implementation efﬁcient. second update step line optimal sense yields maximal dual objective increase jointly variables. opposed updates data-independent step sizes well maximal scalar updates sdca variants. finally well-deﬁned stopping criterion compute duality latter especially attractive time budget learning. algorithm also easily kernelized since proposed top-k hinge loss section optimization dual objective given variables ﬁxed instance regularized projection problem onto top-k simplex respectively biased projection introduced proposition optimization problem proposition reduces euclidean projection onto biases solution orthogonal highlight substantially different standard simplex none existing methods used discuss below. finding euclidean projection onto simplex instance general optimization problem minx{a known continuous quadratic knapsack problem example project onto simplex well examined problem several highly efﬁcient algorithms available ﬁrst main difference upper bound xi’s. existing algorithms expect ﬁxed allows consider decompositions minxi{ introduces coupling across solved closed-form. case upper bound variables makes existing algorithms applicable. second main difference bias term added objective. additional difﬁculty introduced term relatively minor. thus solve problem general even though need proposition case problem reduces cqkp constraint satisﬁed equality. case algorithm knapsack problem. choose since easy implement require sorting scales linearly practice. bias projection problem reduces constant case therefore effect. projection onto top-k cone constraint satisﬁed equality optimum essentially inﬂuence projection problem removed. case left problem projection onto top-k cone address following lemma. lemma solution following optimization problem remaining case solving system exactly system constraints follow deﬁnition sets ensure computed thresholds compatible corresponding partitioning index set. proof. top-k cone. known euclidean projection onto i.e. normal cone therefore obtain equivalent condition maxx∈k take least components must positive. maximize would exactly positive corresponding largest components would result objective function additional term vanishes therefore optimal euclidean projection must also optimal biased projection. projection. lemmas suggest simple algorithm projection onto topk cone. first check projection constant case compute check compatible corresponding sets general case suggest simple exhaustive search strategy. sort loop feasible partitions solution satisﬁes since know +|m| limit search iterations worst case iteration requires constant number operations. biased projection leave fallback case lemma gives sufﬁcient condition. yields runtime complexity comparable simplex projection algorithms based sorting. argued projection onto top-k simplex becomes either knapsack problem projection onto top-k cone depending constraint optimum. following lemma provides check cases apply. lemma solution following optimization problem projection. lemma compute projection onto follows. first check special cases zero constant projections before. fails proceed knapsack problem since faster solve. thresholds partitioning sets compute value given lemma done. otherwise know directly general case lemma section show usunier version top-k hinge loss optimized using prox-sdca framework main ingredients discuss conjugate loss projection. turns difference conjugate top-k hinge loss introduced conjugate effective domains. proposition primal-conjugate pair top-k usunier loss note upper bounds xi’s ﬁxed means euclidean projection onto instance continuous quadratic knapsack problem unfortunately proximal step sdca framework corresponds biased projection additional regularizer coming regularizer training objective. address issue follow derivation given proofs lemmas projection. compute projection follow steps first solve knapsack problem using algorithm also computes dual variable done; otherwise sort loop feasible index sets stop satisﬁes compatible corresponding index sets. table top-k accuracy section state art. middle section baseline methods. preck recallk svmperf wsabie++ embedding dimension queue size ﬁrst part caltech indoor. bottom section top-k svms top-k svmα loss top-k svmβ loss main goals experiments. first show projection onto top-k simplex scalable comparable efﬁcient algorithm simplex projection. second show top-k multiclass using versions top-k hinge loss denoted top-k svmα top-k svmβ respectively leads improvements top-k accuracy consistently datasets choices particular note improvements compared multiclass crammer singer corresponds top- svmα/top- svmβ. release implementation projection procedures sdca solvers library matlab interface. table top-k accuracy section state art. middle section baseline methods. bottom section top-k svms top-k svmα loss top-k svmβ loss results places imagenet computed validation set. follow experimental setup sample points normal distribution solve projection problems using algorithm using proposed method projecting onto different values report total time taken single intel xeon .ghz processor. scaling linear problem dimension times essentially same. evaluate method image classiﬁcation datasets different scale complexity caltech silhouettes indoor places imagenet caltech others results large scale datasets supplement. cross-validate hyper-parameters range extending optimal value boundary. liblinear svmova svmperf corresponding loss function recallk code provided toppush. ranking method like recallk toppush scale particular dataset using reduction multiclass binary problem discussed one-vs-all version corresponding method. implemented wsabie++ based pseudo-code table among baseline methods tried toppushova scaled places imagenet datasets time memory-wise. caltech features provided datasets extract features pre-trained scene recognition datasets places ilsvrc caffe reference model experimental results given tables first note method scalable large datasets millions training examples places ilsvrc second observe optimizing top-k hinge loss yields consistently better top-k performance. might come cost decreased top- accuracy interestingly also result noticeable increase top- accuracy larger datasets like caltech silhouettes resonates argumentation optimizing top-k often appropriate datasets large number classes. overall systematic increase top-k accuracy datasets examined. example following improvements top- accuracy top- svmα compared top- svmα caltech indoor demonstrated scalability effectiveness proposed top-k multiclass image recognition datasets leading consistent improvements top-k performance. future could study top-k hinge loss generalized family ranking losses similar top-k loss could lead tighter convex upper bounds corresponding discrete losses.", "year": 2015}