{"title": "VQS: Linking Segmentations to Questions and Answers for Supervised  Attention in VQA and Question-Focused Semantic Segmentation", "tag": ["cs.CV", "cs.CL", "cs.LG"], "abstract": "Rich and dense human labeled datasets are among the main enabling factors for the recent advance on vision-language understanding. Many seemingly distant annotations (e.g., semantic segmentation and visual question answering (VQA)) are inherently connected in that they reveal different levels and perspectives of human understandings about the same visual scenes --- and even the same set of images (e.g., of COCO). The popularity of COCO correlates those annotations and tasks. Explicitly linking them up may significantly benefit both individual tasks and the unified vision and language modeling. We present the preliminary work of linking the instance segmentations provided by COCO to the questions and answers (QAs) in the VQA dataset, and name the collected links visual questions and segmentation answers (VQS). They transfer human supervision between the previously separate tasks, offer more effective leverage to existing problems, and also open the door for new research problems and models. We study two applications of the VQS data in this paper: supervised attention for VQA and a novel question-focused semantic segmentation task. For the former, we obtain state-of-the-art results on the VQA real multiple-choice task by simply augmenting the multilayer perceptrons with some attention features that are learned using the segmentation-QA links as explicit supervision. To put the latter in perspective, we study two plausible methods and compare them to an oracle method assuming that the instance segmentations are given at the test stage.", "text": "figure taking input image question image algorithm question-focused semantic segmentation desired generate segmentation mask entities image visually answer question. jective hard evaluate quality captions generated different algorithms tend miss subtle details training models capturing scene-level gist rather ﬁne-grained entities. light premises demerits image captioning visual question answering visual grounding proposed parallel accommodate automatic evaluation multiple levels focus visual entities rich dense human annotated datasets arguably main enabler among others line exciting works vision-language understanding. coco especially noticeable among them. contains mainly classical labels image captions. many research groups collected additional labels coco images variety tasks. agrawal crowdsourced questions answers subset coco images abstract scenes collected seven types object mentions asrich dense human labeled datasets among main enabling factors recent advance visionlanguage understanding. many seemingly distant annotations inherently connected reveal different levels perspectives human understandings visual scenes even images popularity coco correlates annotations tasks. explicitly linking signiﬁcantly beneﬁt individual tasks uniﬁed vision language modeling. present preliminary work linking instance segmentations provided coco questions answers dataset name collected links visual questions segmentation answers transfer human supervision previously separate tasks offer effective leverage existing problems also open door research problems models. study applications data paper supervised attention novel question-focused semantic segmentation task. former obtain state-of-the-art results real multiple-choice task simply augmenting multilayer perceptrons attention features learned using segmentation-qa links explicit supervision. latter perspective study plausible methods compare oracle method assuming instance segmentations given test stage. connecting visual understanding natural language received extensive attentions recent years. witnessed resurgence image captioning often addressed jointly modeling visual textual content deep neural networks. however image captions tend diverse subsociated bounding boxes images users give referring expressions pinpoints unique object image. visual genome dataset also intersects coco terms images provides dense human annotations especially scene graphs. seemingly distant annotations inherently connected sense reveal different perspectives human understandings coco images. popularity coco could strongly correlate annotations even tasks. explicitly linking envision signiﬁcantly beneﬁt individual tasks uniﬁed vision-language understanding well corresponding approaches models. contributions paper initiate preliminary work this. particular focus linking segmentations provided coco dataset displaying image pair image participant choose segmentation image order visually answer question. figure illustrates collected visual answers. question what next dog? output supposed segmentation mask man. question what time clock segmented out. another intriguing example cars desired segmentations answer street empty? providing essential visual evidence simple text answer note many visual entities could mentioned question participants choose target segmentation visually answer question. simpliﬁes annotation task results higher agreement participants. section details annotation collection process statistics. related datasets. collected human attention maps task blur images users scratch seek visual cues help answer questions. obtained attention maps often small revealing meaningful parts rather complete objects. object parts also mixed background areas other. result human attention maps likely less accurate supervision attention based approaches links built segmentations qas. experiments verify hypothesis bounding boxes provided visualw object mentions serve purpose directly answering questions except pointing type questions. contrast provide direct visual answers form segmentations question types. applications segmentation-qa links tions segmentation answers links transfer human supervision previously separate tasks i.e. semantic segmentation vqa. enable tackle existing problems effective leverage also open door research problems models vision-language understanding. study applications dataset paper supervised attention novel question-focused semantic segmentation task. former obtain state-of-the-art results real multiplechoice task simply augmenting multilayer perceptrons attention features. designed answer natural language questions images form short texts. attention scheme often found useful either attending particular image regions modeling object relationships however lacking explicit attention annotations existing methods latent variables indirect cues inference. result machine-generated attention maps poorly correlated human attention maps surprising since latent variables hardly match semantic interpretations lack explicit training signals; similar observations exist studies e.g. object detection video recognition text processing phenomena highlight need explicit links visual text answers realized work vqs. show that supervised learning attend different image regions using collected segmentationqa links boost simple model compelling performance real multi-choice task. since desires text answers exist potential shortcuts learning agent e.g. generate correct answers without accurately reasoning locations relations different visual entities. visual grounding avoids caveat placing bounding boxes segmentations target visual entities scope text expressions existing works often limited visual entities present image. order bring together best propose qfss task whose objective produce pixel-wise segmentations order visually answer questions images. effectively borrows versatile questions meanwhile resembles design terms pixel-wise segmentations desired output. figure typical examples dataset. left right underlying tasks respectively object localization semantic segmentation understanding object relationships ﬁne-grained activity localization commonsense reasoning. given image question image propose mask aggregation approach generating segmentation mask visual answer. since qfss task perspective compare proposed approach competing baselines also study upperbound method assuming instance segmentations given oracles test stage. al.’s work related qfss. learn ground text expressions form image segmentations. unlike questions used work ﬂexible incorporate commonsense knowledge bases expressive scope text phrases often limited visual entities associated images. rest paper organized follows. section details collection process analyses data. section show collected segmentation-qa links learn supervised attention features augement existing methods. section study potential frameworks address question-focused semantic segmentation task. section concludes paper. section describe detail collect links semantic image segmentations text questions answers build work upon images instance segmentation masks coco dataset coco images mainly everyday scenes contain common objects natural contexts accommodating complex interactions relationships different visual entities. avoid trivial links segmentations pairs keep images contain least three instance segmentations work. questions diverse comprehensively cover various parts image different levels semantic interpretations well commonsense knowledge bases. display annotators image instance segmentations coco dataset pair image dataset. textual answer given addition question facilitate participants choose right segmentations visual answer. instructions give annotators please tick black button question think question answered full image. please tick gray button question feel question ambiguous sure segment/region select answer question. occasionally visual answer supposed part instance segment given coco. instance mcdonald logo answers what fast food restaurant seen? figure corresponding segmentation logo coco. another example region ring answers woman wearing ring? cases participants draw tight bounding boxes around them. segment instead learning agent qfss never able produce right segmentation unless include training images future since regions ﬁne-grained visual entities show times data collection process. quality control. tried amturk collect annotations beginning. inter-annotator agreement high questions objects people many inconsistent annotations questions referring activities besides amturk workers tend frequently tick black button says full image visual answer gray button tells question ambiguous. obtain higher-quality annotations instead invited undergraduate graduate volunteers trained person control annotation quality annotator asked ﬁnish assignment images look annotations together volunteers asked participate discussion jointly decide expected annotations every question. also gradually increased hourly payment rate incentives high-quality work. thanks rich questions collected agrawal complex visual scenes coco participants parse question understand visual scene context infer interactions visual entities pick segmentations answer questions. many vision tasks play roles process. figure shows typical examples facilitate following discussion. object detection. many questions directly properties objects images. figure participants supposed identify cluttered scene question what color coffee cup?. semantic segmentation. questions visual evidence answers best represented semantic segmentations. take figures instance. simply detecting rider and/or bike would inadequate expressing spatial interactions. spatial relationship reasoning. question like what bench next woman? poses challenge participants spatial relationship objects including bench woman answer purse. figure another example realm. fine-grained activity recognition. question activity participants label visual entities involved. words expected spot ﬁnegrained details activity. commonsense reasoning. commonsense knowledge help participants signiﬁcantly reduce search space visual answers e.g. clock answer what time figure mcdonald logo answer what fast food restaurant seen? figure figure supervised attention vqa. learn attention features question-image pair corresponding segmentation mask supervision train attention network. that augment model attention features. collecting annotations remove question-image pairs users selected black buttons gray buttons avoid trivial ambiguous segmentation-qa links respectively. total keep images questions instance segmentations bounding boxes. following differentiate segmentations bounding boxes ease presentation also sake bounding boxes tight small much fewer segmentations. figure counts distribution possible number instance segmentations selected image response question. questions answered segmentation. average question-image pair candidate segmentations among selected annotators visual answers. figure visualize distribution question types. popular type what questions is/are does/do questions note although textual answers simply participants explicitly demonstrate understanding visual content producing semantic segmentation masks. third column table show average number segmentations chosen users average number candidates question types. user linked visual questions segmentations latter visually answers former quite versatile. offer better leverage least problems i.e. supervised attention questionfocused semantic segmentation designed answer natural language questions image form short texts. conjecture learning agent produce accurate text answers given privileged access segmentations user linked training. verify point design simple experiment augment model augmented signiﬁcantly improves upon plain version gives rise state-of-the-art results real multiple-choice task experiment setup. conduct experiments real multiple choices dataset contains questions training validation testing. question candidate answer choices learning agent required ﬁgure correct answer among them. evaluate results following metric suggested multiple choice. since multiple-choice task supplies candidate answers question jabri propose transform problem stack binary classiﬁcation problems solve multilayer perceptrons model xiqa concatenation feature representations image question image candidate answer sigmoid function. hidden layer units relu activation. model competitive albeit simple. image features extract types features input image resnet pool activation attribute features latter attribute detection scores. implement attribute detector revising output layer resnet. particularly given attributes impose sigmoid function attribute train network using binary cross-entropy loss. training data obtained coco image captions keep frequent words attributes removing stop words. attention features xatt. concatenate attention features xatt original input xiqa. attention features motivated weighted combination image regional features question features non-negative weight image region function question regional features {ri}. borrow network architecture well code implementation yang function except train network cross-entropy loss match weights {pi} groundtruth attentions derived segmentations dataset. particular down-sample segmentation associated question-image pair size number image regions normalize valid probability distribution. training network match weights toward attentions enforce larger weights regions correspond user selected segmentations. upper panel figure illustrates process extracting attention features bottom panel shows model augmented attention features real multiple-choice task. table reports comparison results attention features augmented several state-of-the-art methods real multiple-choice task. mainly test comparison. determining best single ensemble models also submit evaluation server acquire results test standard. first note absolute improvement plain model simply augmenting using learned attention features second attribute features images actually quite effective. gain improvement plain replacing resnet image features attribute features nonetheless appending attention features attri. still observe absolute gain. finally ensemble resnet atten. models attri. atten. models submission evaluation server ranked second test standard real multiplechoice task paper submission date. good supervision attention vqa? section contrast data human attention maps bounding boxes placed tightly around segmentations vqs. comparison results reported table evaluated testdev dataset real multiple choice. segmentaitons linked give rise little better results bounding boxes outperform hat. conﬁrm conjecture might suboptimal supervised learning attentions since reveal usually small parts objects contain large proportions background. however believe remains interesting examine generic attention-based models supplementary materials describe detailed implementation ensemble model. also present additional results studying different resolutions segmentation masks inﬂuence results. image qfss expects learning agent output visual answer semantically segment right visual entities image. designed similarly segmentation natural language expressions possible applications robot vision photo editing etc. order task perspective propose mask aggregation approach qfss study baseline also investigate upper bound method assuming instance segmentations given oracles test stage. propose mask aggregation approach tackling qfss. modeling hypothesis desired output segmentation mask composed high-quality segmentation proposals. particular segmentation proposals e··· generated sharpmask given image. proposal binary segmentation mask size image. image pair i-th combination coefﬁcient determined question features representations i-th segmentation proposal softmax function i.e. softmax. learn model parameters minimizing loss user selected segmentations model generated segmentation mask current model shallow straightforward make deep e.g. stacking output original input following prior practice stacked attention network coco assuming available oracles testing using binary classiﬁer determine whether instance segmentation included visual answer. results considered upper bound approach segmentations certainly accurate machine generated proposals binary classiﬁcation arguably easier solve aggregating multiple masks. re-train binary classiﬁer here; takes input concatenated features segmentation question. bound method concrete question-image example. baseline using deconvolutional network. finally study competitive baseline motivated textconditioned figure shows contains three components convolutional neural network deconvolutional neural network question embedding attend feature maps cnn. images resized convolutional deconvolutional nets follow speciﬁcations namely vgg- trimmed till last convolutional layer followed fully connected layers mirrored deconvnet. input question embedding matrix size feature last convolutional layer. question embedding element-wsie multiplied feature map. train network loss output mask groundtruth segmentation mask. experiments qfss features. addition representing questions using word embedding features section also test bag-of-words features. instance segmentation proposal mask pixels image extract features last pooling layer resnet- dataset split. sharpmask learned training coco. hence split data test intersect training sharpmask. particularly images correspondingly questions training set. split remaining images questions parts images associated questions validation images questions test set. results. table reports comparison results qfss evaluated intersection-over-union addition ﬁrst three columns number different types questions average numbers user selected segmentations question type. average segmentations selected question types. first note proposed mask aggregation outperforms baseline deconvnet signiﬁcantly worse upper bound method. mask aggregation superior deconvnet partially actually used extra supervised information beyond data; namely sharpmask trained using instance segmentations training coco. upper bound results indicate still large room mask aggregation framework improve; possibility make deep future work. besides question representations bag-of-wrods word embedding give rise distinguishable results either mask aggregation deconvnet. observation intriguing since implies qfss task responsive question representation schemes. thus reasonable expect qfss beneﬁt advance progress joint vision paper propose link instance segmentations provided coco questions answers collected links named visual questions segmentation answers transfer human supervision individual tasks semantic segmentation thus enabling study least problems better leverage before supervised attention novel question-focused semantic segmentation task. former obtain state-of-the-art results real multiple-choice task simply augmenting multilayer perceptrons attention features. latter propose approach based mask aggregation. perspective study baseline method upper-bound method assuming instance segmentations given oracles. work inspired upon observing popularity coco suspect existing seemingly distinct annotations mscoco images inherently connected. reveal different levels perspectives human understandings visual scenes. explicitly linking signiﬁcantly beneﬁt individual tasks also overarching goal uniﬁed vision-language understanding. paper scratches surface. explore types annotations richer models future work. acknowledgement work supported part award gift adobe systems inc. nvidia. partially supported national basic research program china national natural science foundation china", "year": 2017}