{"title": "Exponential Family Graph Matching and Ranking", "tag": ["cs.LG", "cs.AI"], "abstract": "We present a method for learning max-weight matching predictors in bipartite graphs. The method consists of performing maximum a posteriori estimation in exponential families with sufficient statistics that encode permutations and data features. Although inference is in general hard, we show that for one very relevant application - web page ranking - exact inference is efficient. For general model instances, an appropriate sampler is readily available. Contrary to existing max-margin matching models, our approach is statistically consistent and, in addition, experiments with increasing sample sizes indicate superior improvement over such models. We apply the method to graph matching in computer vision as well as to a standard benchmark dataset for learning web page ranking, in which we obtain state-of-the-art results, in particular improving on max-margin variants. The drawback of this method with respect to max-margin alternatives is its runtime for large graphs, which is comparatively high.", "text": "present method learning max-weight matching predictors bipartite graphs. method consists performing maximum posteriori estimation exponential families sufﬁcient statistics encode permutations data features. although inference general hard show relevant application–web page ranking–exact inference efﬁcient. general model instances appropriate sampler readily available. contrary existing maxmargin matching models approach statistically consistent addition experiments increasing sample sizes indicate superior improvement models. apply method graph matching computer vision well standard benchmark dataset learning page ranking obtain state-of-the-art results particular improving max-margin variants. drawback method respect max-margin alternatives runtime large graphs comparatively high. maximum-weight bipartite matching problem fundamental problem combinatorial optimization problem ﬁnding ‘heaviest’ perfect match weighted bipartite graph. exact optimal solution found cubic time standard methods hungarian algorithm. problem practical interest nicely model real-world applications. example computer vision crucial problem ﬁnding correspondence sets image features often modeled matching problem ranking algorithms based matching framework clustering algorithms modeling problem matching central question choice weight matrix. problem real applications typically observe edge feature vectors edge weights. consider concrete example computer vision ∗nicta’s statistical machine learning program locked australia research school information sciences engineering australian national university australia. nicta funded australian government’s backing australia’s ability initiative australian research council’s centre excellence program. e-mails first.lastnicta.com.au setting natural whether could parameterize features labeled matches order estimate parameters that given graphs ‘similar’ features resulting max-weight matches also ‘similar’. idea ‘parameterizing algorithms’ optimizing agreement data called structured estimation describe max-margin structured estimation formalisms problem. max-margin structured estimators appealing minimize loss really cares however structured losses typically piecewise constant parameters eliminates hope using smooth optimization directly. max-margin estimators instead minimize surrogate loss easier optimize namely convex upper bound structured loss practice results often good known convex relaxations produce estimators statistically inconsistent i.e. algorithm general fails obtain best attainable model limit inﬁnite training data. inconsistency multiclass support vector machines well-known issue literature received careful examination recently motivated inconsistency issues max-margin structured estimators well well-known beneﬁts full probabilistic model paper present maximum posteriori estimator matching problem. observed data edge feature vectors labeled matches provided training. maximize conditional posterior likelihood matches given observed data. build exponential family model sufﬁcient statistics mode distribution solution max-weight matching problem. resulting partition function p-complete compute exactly. however show learning rank applications model instance tractable. compare performance model instance large number state-of-theart ranking methods including dorm approach differs model instance using max-margin instead formulation. show competitive results standard webpage ranking datasets particular show model performs better dorm. intractable model instances show problem approximately solved using sampling provide experiments computer vision domain. however fastest suitable sampler still quite slow large models case max-margin matching estimators like likely preferable even spite potential inferior accuracy. arbitrary input space arbitrary discrete space typically exponentially large. example space matrices trees graphs sequences strings matches etc. structured nature structured prediction refers setting paper vector-weighted bipartite graphs perfect matches induced graphs available along corresponding annotated matches task estimate apply predictor graph produces match similar matches similar graphs annotated set. structured learning structured estimation refers process estimating vector predictor data available. structured prediction input means computing using estimated generic estimation strategies popular producing structured predictors. based max-margin estimators maximumlikelihood estimators exponential family models ﬁrst approach generalization support vector machines case structured. however resulting estimators known inconsistent general limit inﬁnite training data algorithm fails recover best model model class mcallester recently provided interesting analysis issue proposed upper bounds whose minimization results consistent estimators bounds convex approach uses estimation conditional exponential families ‘structured’ sufﬁcient statistics probabilistic graphical models decomposed cliques graph case tractable graphical models dynamic programming used efﬁciently perform inference. tractable models type include models predict spanning trees models predict binary labelings planar graphs estimators exponential families amount solving unconstrained convex optimization problem; addition statistically consistent. main problem types models often partition function intractable. motivated max-margin methods many scenarios intractability arises. figure left illustration input vector-weighted bipartite graph edges. vector associated edge right weighted bipartite graph obtained evaluating learned vector well-studied problem; tractable solved time model used match features images improve classiﬁcation algorithms rank webpages cite applications. typical setting consists engineering score matrix according domain knowledge subsequently solving combinatorial problem. basic goal paper assume weights instead estimated training data. precisely weight associated edge graph result appropriate composition feature vector parameter vector therefore practice input vector. basics need solve argminθ convex differentiable function therefore gradient descent global optimum. order compute need compute ∇θg. standard result exponential families gradient log-partition function expectation sufﬁcient statistics note expression permanent matrix permanent similar deﬁnition determinant difference latter comes product. however unlike determinant computable efﬁciently exactly standard linear algebra manipulations computing permanent p-complete problem therefore realistic hope computing exactly general problems. exact expectation exact partition function efﬁciently computed using algorithm ryser however arbitrary expectations aware exact algorithm efﬁcient full enumeration however even case small graphs important application learning rank. experiments successfully apply tractable instance model benchmark webpage ranking datasets obtaining competitive results. larger graphs alternative options indicated below. approximate expectation situation feasible permutations large fully enumerated efﬁciently need resort approximation expectation sufﬁcient statistics. best solution aware huber recently presented algorithm approximate permanent dense non-negative matrices algorithm works producing exact samples distribution perfect matches weighted bipartite graphs. precisely form distribution here algorithm applications involve larger graphs. generate samples distribution directly approximate monte carlo estimate ranking apply general matching model introduced previous sections task learning rank. ranking fundamental problem applications diverse areas document retrieval recommender systems product rating others. focus page ranking. given queries {qk} query list documents measuring relevance degree document respect query rating relevance degree usually nominal value list typically also given every retrieved document joint feature vector documents retrieved query nodes side correspond possible ranking positions documents subset chosen randomly provided least exemplar document every rating present. therefore must process repeated bootstrap manner resample documents time documents fact least exemplar every rating present otherwise randomly). effectively boosts number training examples since query ends selected many times time different subset documents original documents. following drop query index examine single query. follow construction used matching problems ranking problems max-margin estimator exponential family.) edge feature vector product feature vector associated document scalar associated ranking position notice scalar non-increasing function rank position solved simply sorting values decreasing order. words matching problem becomes ranking values setting makes sense interpret quantity score document query leaves open question non-increasing function used. solve problem paper instead choose ﬁxed theory possible optimize learning case optimization problem would longer convex. describe results method letor publicly available benchmark data collection comparing learning rank algorithms. comprised three data sets ohsumed data sets ohsumed contains features extracted query-document pairs ohsumed collection subset medline database medical publications. contains queries. query number associated documents relevance degrees judged humans three levels deﬁnitely possibly denotes vector ranks entries vector maximized permutation relevant. query-document pair associated dimensional feature vector total number query-document pairs contain features extracted topic distillation tasks trec trec queries respectively. again query number associated documents relevance degrees judged humans case levels provided relevant relevant. query-document pair associated dimensional feature vector total number query-document pairs datasets already partitioned -fold cross-validation. details. external parameters regularization constant chosen -fold crossvalidation partition provided letor package. experiments repeated times account randomness sampling training data. experiments. backtracking line search described number samples linearly also trained possible subsets plots). figure plot results method compared achieved number state-of-the-art methods published ndcg scores least datasets rankboost ranksvm frank listnet adarank qbrank isorank sortnet structrank c-crf also included plot implementation dorm using precisely resampling methodology data fair comparison. rankmatch performs among best methods ohsumed performs poorly fairly well notice four methods report results three datasets sortnet versions reported structrank c-crf reported ohsumed. rankmatch compares similarly sortnet structrank similarly c-crf structrank ohsumed similarly versions sortnet exhausts comparisons methods results reported datasets. fairer comparison could made methods performance published respective missing dataset. results interpreted cautiously; presents interesting discussion issues datasets. also benchmarking ranking algorithms still infancy don’t publicly available code competitive methods. expect situation change near future able compare fair transparent basis. consistency second experiment trained rankmatch different training subset sizes starting going again repeated experiments dorm using precisely training subsets. purpose whether observe practical advantage method increasing sample size since statistical consistency provides asymptotic indication. results plotted figure -right that training data available rankmatch improves saliently dorm. runtime runtime algorithm competitive max-margin small graphs arise ranking application. larger graphs sampling algorithm result much slower runtimes typically obtained max-margin framework. certainly beneﬁt max-margin matching formulations much faster large graphs. table shows runtimes graphs different sizes exponential family max-margin matching models. table training times exponential model max-margin. runtimes ranking experiments computed full enumeration; corresponds image matching experiments sampler problem size cannot practically solved full enumeration. computer vision application used silhouette image mythological creatures database. randomly selected points silhouette interest points applied shear image creating different images. randomly selected pairs images training validation testing trained model match interest points pairs. setup denotes elementwise difference shape context feature vector point graph size computing exact expectation feasible used sampling method described section again regularization constant chosen cross-validation. given fact estimator consistent max-margin estimator tempted investigate practical performance estimators sample size grows. however since consistency asymptotic property also since hamming loss criterion optimized either estimator imply better large-sample performance real experiments. case present results varying training sizes figure -left. max-margin method sufﬁciently large training size model seems enjoy slight advantage. presented method learning max-weight bipartite matching predictors applied extensively well-known webpage ranking datasets obtaining state-of-the-art results. also illustrated–with image matching application–that larger problems also solved albeit slowly recently developed sampler. method number convenient features. first consists performing maximum-a-posteriori estimation exponential family model results simple unconstrained convex optimization problem solvable standard algorithms bfgs. second figure performance increasing sample size. left hamming loss different numbers training pairs image matching problem right results ndcg ranking dataset ohsumed. evidence agreement fact estimator consistent max-margin not. estimator statistically consistent also practice seems beneﬁt increasing sample sizes max-margin alternative. finally fully probabilistic model easily integrated module bayesian framework example. main direction future research consists ﬁnding efﬁcient ways solve large problems. likely arise appropriate exploitation data sparsity permutation group.", "year": 2009}