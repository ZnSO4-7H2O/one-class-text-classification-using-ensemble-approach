{"title": "Towards cross-lingual distributed representations without parallel text  trained with adversarial autoencoders", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "Current approaches to learning vector representations of text that are compatible between different languages usually require some amount of parallel text, aligned at word, sentence or at least document level. We hypothesize however, that different natural languages share enough semantic structure that it should be possible, in principle, to learn compatible vector representations just by analyzing the monolingual distribution of words.  In order to evaluate this hypothesis, we propose a scheme to map word vectors trained on a source language to vectors semantically compatible with word vectors trained on a target language using an adversarial autoencoder.  We present preliminary qualitative results and discuss possible future developments of this technique, such as applications to cross-lingual sentence representations.", "text": "current approaches learning vector representations text compatible between different languages usually require amount parallel text aligned word sentence least document level. hypothesize however different natural languages share enough semantic structure possible principle learn compatible vector representations analyzing monolingual distribution words. order evaluate hypothesis propose scheme word vectors trained source language vectors semantically compatible word vectors trained target language using adversarial autoencoder. distributed representations words sentences paragraphs documents vectors real numbers proven extremely useful variety natural language processing tasks provide effective inject machine learning models general prior knowledge language automatically obtained inexpensive unannotated corpora. based assumption different languages share similar semantic structure various approaches succeeded obtain distributed representations compatible across multiple languages either learning mappings different embedding spaces jointly training cross-lingual representations approaches require amount parallel text aligned word level sentence level least document level kind parallel resources dictionaries work explore whether assumption shared semantic structure languages strong enough allows induce compatible distributed representations without using parallel resource. require monolingual corpora thematically similar languages general sense. hypothesize exist suitable vectorial space language viewed random process produces vectors level granularity encoded discrete surface forms hypothesize that languages used convey thematically similar information similar contexts random processes approximately isomorphic between languages isomorphism learned statistics realizations processes monolingual corpora principle without form explicit alignment. motivate hypothesis observing humans especially young children acquire multiple languages often relatively little exposure explicitly aligned parallel linguistic information best access distant noisy alignment information form multisensorial environmental clues. nevertheless multilingual speakers always automatically able translate languages speak suggests brain either uses shared conceptual representations different surface features language uses distinct near-isomorphic representations easily transformed other. problem learning transformations probability distributions real vectors studied context generative neural network models approaches generative moment matching networks generative adversarial networks work consider gans since effectiveness demonstrated literature thoroughly gmmns. typical wish train generator model usually neural network transform samples known easy sample uninformative distribution samples distributed according target distribution deﬁned implicitly training set. order iteratively alternate training differentiable discriminator model also neural network distinguish training samples artiﬁcial samples produced generator training generator fool discriminator misclassifying artiﬁcial examples training examples. done conventional gradient-based optimization discriminator differentiable thus backpropagate gradients generator. proven that sufﬁcient model capacity optimization power sufﬁcient entropy generator input distribution limit inﬁnite training size generator learns produce samples correct distribution. intuitively computable test allows distinguish artiﬁcial samples training samples better random guessing probability sufﬁciently powerful discriminator eventually learn exploit sufﬁciently powerful generator eventually learn counter generator output distribution becomes undistinguishable true training distribution. practice actual models ﬁnite capacity gradient-based optimization algorithms become unstable stuck applied multi-objective optimization problem though successfully used generate fairly realistic-looking images preliminary experiments attempted adapt gans problem training generator learn transformation word embeddings trained different languages embedding dimensionality generator parametrized discriminator parametrized unfortunately found setup even different network architectures hyperparameters model quickly converges pathological solution generator always emits constant near-constant samples somehow fool discriminator. appears extreme case know mode-seeking issue gans probably exacerbated settings point-mass nature probability distributions word embedding mode own. order avoid pathological solutions needed penalize generator destroying much information input. therefore turned attention adversarial autoencoders generator called encoder paired another model decoder parametrized attempts transform artiﬁcial samples emitted encoder back input samples. encoder decoder jointly trained minimize combination average reconstruction loss lr)) adversarial loss deﬁned above. discriminator trained above. original formulation discriminator used enforce known prior intermediate latent representation setting instead match latent representation target embedding distribution encoder used transform source embeddings target ones experiments cosine dissimilarity reconstruction loss penalty also include pairwise cosine dissimilarity generated latent samples {ˆe} true target samples {e}n. therefore total loss incurred encoder-decoder step train english italian word embeddings randomly subsampled wikipedia corpora consisting million sentences language. wordvec skipgram mode generate embeddings dimension encoder decoder linear models tied matrices initialized random orthogonal matrices discriminator residual network without convolutions leaky relu non-linearity block non-linearities passthrough path batch normalization dropout block equation leaky relu k-dimensional block state network blocks followed -dimensional output layer logistic sigmoid activation. found using residual network discriminator rather standard multi-layer perceptron yields larger gradients backpropagated generator facilitating training. actually train discriminators experiment identical structure different random initializations train generator monitoring order help determine whether overﬁtting underﬁtting occurs. step word embeddings sampled according frequency original corpora adjusted subsample frequent words wordvec. updates performed using adam optimizer learning rate encoder-decoder discriminator. code implemented python theano lasagne. qualitatively analyzed quality embeddings considering closest italian embeddings sample transformed english embeddings. notice cases closest nearly closest embedding true translation instance ’computer’ ->’computer’ cases closest terms translations subjectively appear semantically related instance ’rain’ ->’gelo’ ’intensissimo’ ’galleggiava’ ’rigidissimo’ ’arida’ ’semi-desertico’ ’fortunale’ ’gelata’ ’piovosa’ ’comics’ ->’kadath’ ’microciccio’’cugel’’promethea’’ﬂashback’’episodio’ ’morimura’ ’chatwin’ ’romanzato’’deedlit’ ’anime’ ->’zatanna’ ’alita’ ’yuriko’ ’batwoman’ ’leery’ ’aquarion’ ’vampirella’ ’minaccia’ terms names places tend transformed incorrectly however ->’radiomobile’ ’cornhole’ ’cartubi’ ’internazione’ ’ueci’ ’rientro’ concatenation reuters corpora news commentary corpora embedding dimension discriminator depth qualitative analysis notice similar partial semantic similarity patterns. however cross-lingual document classiﬁcation task able improve baseline smallest training size. qualitative analysis word embedding mappings appears model learn transfer semantic information although it’s competitive cross-lingual representation approaches. possibly issue hyperparameter choice architectural details since knowledge ﬁrst work apply adversarial training techniques point-mass distribution arising tasks. experimentation needed determine whether model improved whether already fundamental limit much semantic transfer performed monolingual distribution matching alone. additional experimentation help test strongly initial hypothesis semantic isomorphism between languages holds particular across languages different linguistic families. even hypothesis hold strong sense semantic transfer monolingual text alone turns infeasible technique might help conjunction training parallel data. instance neural machine translation sequencesequence transducers without attention could useful train usual parallel sentences train autoencoder mode monolingual sentences using adversarial loss computed discriminator intermediate latent representations push isomorphic languages. modiﬁcation technique allows latent representation variable-sized could also applied attentive sequencesequence transducers alternative addition monolingual dataset augmentation backtranslation conclusion believe work initiates potentially promising line research natural language processing consisting applying distribution matching techniques adversarial training learn isomorphisms languages. references waleed ammar george mulcaire yulia tsvetkov guillaume lample chris dyer noah smith. massively multilingual word embeddings. arxiv preprint arxiv.. marco baroni georgiana dinu germ´an kruszewski. don’t count predict systematic comparison context-counting context-predicting semantic vectors. pages yoshua bengio holger schwenk jean-s´ebastien sen´ecal fr´ederic morin jeanluc gauvain. neural probabilistic language models. innovations machine learning pages springer. sarath chandar stanislas lauly hugo larochelle mitesh khapra balaraman ravindran vikas raykar amrita saha. autoencoder approach learning bilingual word representations. advances neural information processing systems pages ronan collobert jason weston. uniﬁed architecture natural language processing deep neural networks proceedings inmultitask learning. ternational conference machine learning pages acm. emily denton soumith chintala fergus deep generative image models using laplacian pyramid adversarial networks. advances neural information processing systems pages manaal faruqui chris dyer. improving vector space word representations using multilingual correlation. association computational linguistics. goodfellow jean pougetabadie mehdi mirza bing david wardefarley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems pages sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. arxiv preprint arxiv.. ivan titov binod bhattarai. inducing crosslingual distributed representations words. proceedings international conference computational linguistics bombay india december. omer levy yoav goldberg improving distributional similarity dagan. lessons learned word embeddings. transactions association computational linguistics andrew maas raymond daly peter pham huang andrew christopher potts. learning word vectors sentiment analysis. proceedings annual meeting association computational linguistics human language technologies-volume pages association computational linguistics. joseph turian ratinov yoshua bengio. word representations simple general method semi-supervised learning. proceedings annual meeting association computational linguistics pages association computational linguistics. tomas mikolov ilya sutskever chen greg corrado jeff dean. distributed representations words phrases advances neural compositionality. information processing systems pages srivastava nitish geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research alec radford luke metz soumith chintala. unsupervised representation learning deep convolutional genarxiv preprint erative adversarial networks. arxiv..", "year": 2016}