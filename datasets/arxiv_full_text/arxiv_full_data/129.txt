{"title": "Multilingual Image Description with Neural Sequence Models", "tag": ["cs.CL", "cs.CV", "cs.LG", "cs.NE"], "abstract": "In this paper we present an approach to multi-language image description bringing together insights from neural machine translation and neural image description. To create a description of an image for a given target language, our sequence generation models condition on feature vectors from the image, the description from the source language, and/or a multimodal vector computed over the image and a description in the source language. In image description experiments on the IAPR-TC12 dataset of images aligned with English and German sentences, we find significant and substantial improvements in BLEU4 and Meteor scores for models trained over multiple languages, compared to a monolingual baseline.", "text": "introduce multilingual image description task generating descriptions images given data multiple languages. viewed visuallygrounded machine translation allowing image play role disambiguating language. present models task inspired neural models image description machine translation. multilingual image description models generate target-language sentences using features transferred separate models multimodal features monolingual source-language image description model visual features object recognition model. experiments dataset images paired english german sentences using bleu meteor metric models substantially improve upon existing monolingual image description models. automatic image description task generating natural language sentences image thus exclusively performed english availability english datasets. however applications automatic image description text-based image search providing image alt-texts visually impaired also relevant languages. current image description models inherently english-language speciﬁc simple approach generating descriptions another language would collect annotations train model language. nonetheless wealth image description resources english suggest cross-language resource transfer approach explore here. words best resources language generating descriptions language introduce multilingual image description present multilingual multimodal image description model task. multilingual image description form visually-grounded machine translation parallel sentences grounded features image. grounding particularly useful source sentence contains ambiguities need resolved target sentence. example german sentence steht neben haus could refer either bicycle wheel visual context intended meaning easily translated english. cases source language features precise noisy image features e.g. identifying difference river harbour. figure illustration multilingual multimodal language model. descriptions generated combining features sourcetarget-language multimodal language models. dashed lines denote variants model removing features source model would create language-only conditioning vectors; whereas removing input decoder assumes source feature vectors know enough image generate good description. picts overall approach illustrating transfer feature representations models. image description models generally ﬁxed representation visual input taken object detection model work ﬁxed features extracted source language model image description model. distinct neural machine translation models train source language feature representations speciﬁcally target decoding joint model composite model pipeline ﬂexible joint model allowing reuse models tasks requiring retraining different language pair. show representations extracted source language models despite trained translate languages nevertheless highly successful transferring additional informative features target language image description model. series experiments iapr-tc dataset images described english german models incorporate source language features substantially outperform target monolingual image description models. best english-language model improves upon state-of-theart bleu points dataset. ﬁrst results reported german image description model achieves meteor point improvement compared monolingual image description baseline. implication linguistic visual features offer orthogonal improvements multimodal modelling kiela bottou models include visual features also improve translation baselines although lesser extent; attribute dataset exact translations rather independently elicited descriptions leading high performance translation baseline. analyses show additional features improve mainly lower-quality sentences indicating best models successfully combine multiple noisy input modalities. multilingual image description models neural sequence generation models additional inputs either visual linguistic modalities both. present family models sequence increasing complexity make compositional character clear beginning neural sequence model words concluding full model using image source features. figure depiction model architecture. core model recurrent neural network model word sequences i.e. neural language model model trained predict next word sequence given current sequence seen far. timestep input sequence w...n input word represented one-hot vector vocabulary embedded highdimensional continuous vector using learned embedding matrix nonlinear function applied embedding combined previous hidden state generate hidden state output layer next word predicted softmax function figure multilingual multimodal model predicts next word description given current word hidden state source language image features additional input model source features shown rolled-up save space transferred multimodal language model language model; section details. simple rnns tanh sigmoid function. here lstm avoid problems longer sequences sentences buffered timestep special beginning-of-sentence marker end-of-sequence marker timestep initial hidden state values learned together weight matrices recurrent language model generates sequences words conditioned previously seen words thus cannot visual input image description. multimodal language model however sequence generation additionally conditioned image features resulting model generates word sequences corresponding image. image features input model ﬁrst timestep translation model analogous multimodal language model above instead adding image features target language model features source language model. feature vector ﬁnal hidden state extracted sequence model source language source-lm. initial state target-lm thus deﬁned follow recent work sequence-to-sequence architectures neural machine translation calling source language model ‘encoder’ target language model ‘decoder’. however important note source encoder lstm produced better validation performance gated recurrent unit adding image features every timestep reportedly results overﬁtting source model weights updated based local gradients. despite optimised translation source features turn effective initialising target language model indicating useful semantic information captured ﬁnal hidden state. finally image source language features combined multimodal translation model. image features input source target side results doubly multimodal multilingual model alternative formulations image features input source target model initial state target-mlm regardless source model type description generation process model. first model initialised special beginning-of-sentence token image source features. timestep generated output maximum probability word softmax layer subsequently used input token timestep process continues model generates endof-sentence token pre-deﬁned number timesteps iapr-tc dataset originally introduced imageclef shared task object segmentation later expanded complete image descriptions dataset contains images multiple descriptions english german. sentence corresponds different aspect image salient objects likely described ﬁrst description ﬁrst description image. note english descriptions originals; german data professionally translated english. figure shows example image-bitext tuple dataset. perform experiments using standard splits images training reserve hyperparameter estimation evaluation. leaves total training tokens english vocabulary types; tokens german types. compared flickrk flickrk coco datasets english descriptions iapr-tc dataset long average length words. extract image features pre-trained vgg- object recognition model speciﬁcally image features extracted ﬁxed representations penultimate layer line recent work area. source-lm target-lm second baseline translation model trained source target descriptions without visual features. ﬁnal hidden state source-lm generated source sentence input target-lm. source-mlm target-mlm model translation baseline replaced multimodal language models. source features input target model thus multimodal i.e. word image features captured source-language sentence. target decoder also conditioned image features directly. note source target matrices parameterised separately. source-mlm target-lm visual input given source-mlm target-lm uses single input vector source-mlm. source encoder combines linguistic visual cues extent visual features represented source-mlm feature vector. lstm recurrent language model. hidden layer size dimensions. word embeddings -dimensional learned along model parameters. also experimented larger hidden layers result improvements also took longer train. image features -dimension penultimate layer vgg- object recognition network applied image. models trained mini-batches examples towards objective function using adam optimiser early stopping model selection based bleu validation bleu increased epochs validation language model perplexity stopped decreasing training halted. apply dropout image features source features word representations discourage overﬁtting objective function includes regularisation term λ=e−. results reported averages three runs different glorot-style uniform weight initialisations report image description quality using bleu meteor language-model perplexity. meteor shown correlate better human judgements bleu image description bleu meteor scores calculated using multeval this difference length resulted difﬁculties initial experiments preco-training using datasets. plan pursuing future work since independence source encoder model makes kind transfer learning natural. results image description german english presented tables generation examples seen figures appendix knowledge ﬁrst published results german image description. overall found english image description easier german description measured bleu meteor scores. caused complex german morphology results larger vocabulary hence model parameters. english monolingual image description model comparable state-of-the-art models typically report results flickrk flickrk dataset. en-mlm achieves bleu score flickrk dataset nearly matching score karpathy fei-fei uses ensemble models beam search decoding. iapr-tc dataset en-mlm baseline outperforms kiros report higher performance evaluate reference descriptions making ﬁgures incomparable. multilingual models beat monolingual image description baseline bleu meteor points best models. clearly features transferred source models useful target-lm target-mlm description generator despite switch languages. translation baseline without visual features performs well. indicates effectiveness translation model even without joint training also artifact dataset. different dataset independently elicited descriptions result worse performance translation system visually grounded target descriptions would comparable source descriptions. overall multilingual models encode source using outperform source-lm models. target side simple decoders perform better decoders. explained extent smaller number parameters models input visual features twice. incorporating image features source side seems effective possibly source constrained gold description test time leading figure t-sne embeddings illustrate positive effect conditioning image description models multiple language data. model image people climbing snowy cliff closer images depicting people snow ﬁelds. coherent match visual linguistic features. conversely target-mlm variants tend worse sentence generators models indicating visual features lead useful hidden state values room improving role generation. source features beyond image features? source features useful baseline successfully separate related images. image description models compress image feature vector number dimensions hidden layer recurrent network effectively distilling image features correspond words description. step model prone mistakes resulting descriptions poor quality. however best multilingual models initialised features transferred image description models different language. cases source language features already compressed image features source language image description task. qualitatively illustrate effect using barnes-hut t-sne projections initial hidden representations models figure shows t-sne projection example figure using initial hidden state target side monolingual example nearest neighbours target image desert scenes groups people. adding transferred source features results representation places importance background fact consistently mentioned descriptions. nearest neighbours images mountainous snow regions groups people. descriptions improved source image features? figure shows distribution sentence-level meteor scores baseline models average per-sentence change moving best performing multilingual multimodal model additional source language features additional modality result similar patterns quality descriptions improved high quality descriptions deteriorate. adding image features seems riskier adding source language features unsurprising given larger distance visual linguistic space versus moving language another. also consistent lower performance baseline models compared lm→lm models. figure effect adding multimodal source features monolingual english image description model german-english translation model plots show baseline sentence-level meteor score distributions bottom plots show difference score compared multilingual multimodal sentences baseline score improved adding multimodal source features. analysis lm→mlm model shows similar behaviour mlm→lm model above. however model decreasing performance starts earlier lm→mlm model improves lm→lm baseline lowest score bin. adding image features source side rather target side seems ﬁlter noise complexity image features essential source language features retained. conversely merging source language features image features target side target-mlm models leads less helpful entangling linguistic noisier image input maybe many sources information combined time past years seen numerous results showing relatively standard neural network model architectures applied variety tasks. ﬂexibility application architectures seen strong point indicating representations learned general models sufﬁciently powerful lead good performance. another advantage exploited work presented here becomes relatively straightforward make connections models different tasks case image description machine translation. automatic image description received great deal attention recent years detailed overview task datasets models evaluation issues). deep neural networks image description typically estimate joint image-sentence representation multimodal recurrent neural network main difference models discrete tuple-based representations image description necessary explicitly deﬁne joint representation; structure neural network used estimate optimal joint representation description task. image–sentence representation multimodal initialised image features ﬁnal fully-connected layer convolutional neural network trained multi-class object recognition alternative formulations input image features model timestep ﬁrst detect words image generate sentences using maximum-entropy language model domain machine translation greater variety neural models used subtasks within pipeline neural network language models joint translation language models re-ranking phrase-based translation models directly decoding recently end-to-end neural systems using long short-term memory networks gated recurrent units proposed encoder-decoder models translation proven highly effective multimodal modelling literature related approaches using visual textual information build representations word similarity categorization tasks silberer lapata combine textual visual modalities jointly training stacked autoencoders kiela bottou construct multi-modal representations concatenating distributed linguistic visual feature vectors. recently kiela induced bilingual lexicon grounding lexical entries features. cases results show bimodal representations superior unimodal counterparts. introduced multilingual image description task generating descriptions image given corpus descriptions multiple languages. task expands range output languages image description also raises questions integrate features multiple languages well multiple modalities effective generation model. multilingual multimodal model loosely inspired encoder-decoder approach neural machine translation. encoder captures multimodal representation image sourcelanguage words used additional conditioning vector decoder produces descriptions target language. conditioning vector originally trained towards objective image features transferred object recognition model source features transferred source-language image description model. model substantially improves quality descriptions directions compared monolingual baselines. dataset used paper consists translated descriptions leading high performance translation baseline. however believe multilingual image description based independently elicited descriptions multiple languages rather literal translations. linguistic cultural differences lead different descriptions appropriate different languages cases image features essential. open question whether beneﬁts multiple monolingual references extend multiple multilingual references. image description datasets typically include multiple reference sentences essential capturing linguistic diversity within single language experiments found useful image description diversity also found languages instead multiple monolingual references. future would like explore attention-based recurrent neural networks used machine translation image description also plan apply models language pairs recently released pascal japanese translations dataset lastly apply types models multilingual video description dataset elliott supported alain bensoussain career development fellowship. frank supported funding european union’s horizon research innovation programme grant agreement thank philip schulz khalil sima’an arjen vries lynda hardman richard glassey wilker aziz joost bastings ´akos k´ad´ar discussions feedback work. built auli michael galley michel quirk chris zweig geoffrey. joint language translation modeling recurrent neural networks. proceedings conference empirical methods natural language processing association computational linguistics bernardi raffaella cakici ruken elliott desmond erdem aykut erdem erkut ikizler-cinbis nazli keller frank muscat adrian plank barbara. automatic description generation images survey models datasets evaluation measures. appear jair bojar ondˇrej chatterjee rajen federmann christian haddow barry huck matthias hokamp chris koehn philipp logacheva varvara monz christof negri matteo post matt scarton carolina specia lucia turchi marco. findings workshop statistical machine translation. proceedings tenth workshop statistical machine translation lisbon portugal september association computational linguistics. chen david dolan william building persistent workforce mechanical turk multilingual data collection. proceedings human computation workshop august chen xinlei fang tsung-yi vedantam ramakrishna gupta saurabh doll´ar piotr zitnick lawrence. microsoft coco captions data collection evaluation server. corr abs/. kyunghyun merrienboer bart gulcehre caglar bahdanau dzmitry bougares fethi schwenk holger bengio yoshua. learning phrase representations using encoder– decoder statistical machine translation. proceedings conference empirical methods natural language processing association computational linguistics clark jonathan dyer chris lavie alon smith noah better hypothesis testing statistical machine translation controlling optimizer instability. proceedings annual meeting association computational linguistics human language technologies short papers volume devlin jacob zbib rabih huang zhongqiang lamar thomas schwartz richard makhoul john. fast robust neural network joint models statistical machine translation. proceedings annual meeting association computational linguistics association computational linguistics donahue hendricks guadarrama rohrbach venugopalan saenko darrell long-term recurrent convolutional networks visual recognition description. corr abs/. fang gupta saurabh iandola forrest srivastava rupesh deng dollar piotr jianfeng xiaodong mitchell margaret platt john lawrence zitnick zweig geoffrey. captions visual concepts back. ieee conference computer vision pattern recognition june funaki ruka nakayama hideki. image-mediated learning zero-shot cross-lingual document retrieval. empirical methods natural language processing glorot xavier bengio yoshua. understanding difﬁculty training deep feedforward neural networks. international conference artiﬁcial intelligence statistics hodosh micah young hockenmaier framing image description ranking task data models evaluation metrics. journal artiﬁcial intelligence research jean s´ebastien firat orhan kyunghyun memisevic roland bengio yoshua. montreal neural machine translation systems wmt’. proceedings tenth workshop statistical machine translation lisbon portugal september association computational linguistics. kiela douwe bottou l´eon. learning image embeddings using convolutional neural networks improved multi-modal semantics. proceedings conference empirical methods natural language processing kiela douwe vuli´c ivan clark stephen. visual bilingual lexicon induction transferred convnet features. proceedings conference empirical methods natural language processing krizhevsky alex sutskever ilya hinton geoffrey imagenet classiﬁcation deep convolutional neural networks. pereira burges c.j.c. bottou weinberger k.q. advances neural information processing systems curran associates inc. mitchell margaret xufeng dodge jesse mensch alyssa goyal amit berg yamaguchi kota berg stratos karl daume iii. midge generating image descriptions computer vision detections. eacl rashtchian young hodosh hockenmaier collecting image annotations using amazon’s mechanical turk. naaclhlt workshop creating speech language data amazon’s mechanical turk srivastava nitish hinton geoffrey krizhevsky alex sutskever ilya salakhutdinov ruslan. dropout simple prevent neural networks overﬁtting. journal machine learning research sutskever ilya vinyals oriol quoc sequence sequence learning neural networks. ghahramani welling cortes lawrence n.d. weinberger k.q. advances neural information processing systems curran associates inc. vinyals oriol toshev alexander bengio samy erhan dumitru. show tell neural image caption generator. ieee conference computer vision pattern recognition june kelvin jimmy kiros ryan kyunghyun courville aaron salakhutdinov ruslan zemel richard bengio yoshua. show attend tell neural image caption generation visual attention. corr abs/. always helps condition table image description performance validation data set. features different language english german german english. sections detailed explanations model variants. report mean standard deviation calculated three runs random weight initialisation. present examples descriptions generated models studied paper. figure monolingual generates best descriptions. however figures best descriptions generated transferring source features target target", "year": 2015}