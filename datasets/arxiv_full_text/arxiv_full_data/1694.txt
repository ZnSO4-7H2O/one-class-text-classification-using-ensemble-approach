{"title": "word2vec Explained: deriving Mikolov et al.'s negative-sampling  word-embedding method", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "The word2vec software of Tomas Mikolov and colleagues (https://code.google.com/p/word2vec/ ) has gained a lot of traction lately, and provides state-of-the-art word embeddings. The learning models behind the software are described in two research papers. We found the description of the models in these papers to be somewhat cryptic and hard to follow. While the motivations and presentation may be obvious to the neural-networks language-modeling crowd, we had to struggle quite a bit to figure out the rationale behind the equations.  This note is an attempt to explain equation (4) (negative sampling) in \"Distributed Representations of Words and Phrases and their Compositionality\" by Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean.", "text": "wordvec software tomas mikolov colleagues gained traction lately provides state-of-the-art word embeddings. learning models behind software described research papers found description models papers somewhat cryptic hard follow. motivations presentation obvious neural-networks language-modeling crowd struggle quite ﬁgure rationale behind equations. note attempt explain equation distributed representations words phrases compositionality tomas mikolov ilya sutskever chen greg corrado jeﬀrey dean departure point paper skip-gram model. model given corpus words contexts consider conditional probabilities given corpus goal parameters maximize corpus probability mikolov present negative-sampling approach eﬃcient deriving word embeddings. negative-sampling based skip-gram model fact optimizing diﬀerent objective. follows derivation negative-sampling objective. consider pair word context. pair come training data? let’s denote probability came corpus data. correspondingly probability come corpus data. before assume parameters controlling distribution throughout note assume words contexts come distinct vocabularies that example vector associated word diﬀerent vector associated context dog. assumption follows literature motivated. motivation making assumption following consider case word context share vector words hardly appear contexts themselves model assign probability entails assigning value impossible. need mechanism prevents vectors value disallowing combinations. present model pairs must i.e. pairs data. achieved generating random pairs assuming incorrect optimization objective becomes speciﬁcally negative sampling mikolov al.’s constructed times larger construct samples drawn according unigram distribution raised power. equivalent drawing samples distribution pwords pcontexts pwords pcontexts unigram distributions words contexts respectively normalization constant. work mikolov context word pcontext pwords count words representation learn contexts representation contexts representation learn word representations model reduces logistic regression convex. however model words contexts representations learned jointly making model non-convex. section lists peculiarities contexts used wordvec software reﬂected code. generally speaking sentence words contexts word comes window size around word wi−k wi+k parameter. however subtleties parameters discarding input words words appearing less min-count times considered either words contexts addition frequent words down-sampled. importantly words removed text generating contexts. eﬀect increasing eﬀective window size certain words. according mikolov sub-sampling frequent words improves quality resulting embedding benchmarks. original motivation sub-sampling frequent words less informative. another explanation eﬀectiveness eﬀective window size grows including context-words content-full linearly away focus word thus making similarities topical. distributional hypothesis states words similar contexts similar meanings. objective clearly tries increase quantity good word-context pairs decrease ones. intuitively means words share many contexts similar however hand-wavy.", "year": 2014}