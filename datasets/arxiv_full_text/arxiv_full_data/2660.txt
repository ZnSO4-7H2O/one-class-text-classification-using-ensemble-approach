{"title": "DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "A key enabler for optimizing business processes is accurately estimating the probability distribution of a time series future given its past. Such probabilistic forecasts are crucial for example for reducing excess inventory in supply chains. In this paper we propose DeepAR, a novel methodology for producing accurate probabilistic forecasts, based on training an auto-regressive recurrent network model on a large number of related time series. We show through extensive empirical evaluation on several real-world forecasting data sets that our methodology is more accurate than state-of-the-art models, while requiring minimal feature engineering.", "text": "probabilistic forecasting i.e. estimating probability distribution time series’ future given past enabler optimizing business processes. retail businesses example forecasting demand crucial right inventory available right time right place. paper propose deepar methodology producing accurate probabilistic forecasts based training auto-regressive recurrent network model large number related time series. demonstrate applying deep learning techniques forecasting overcome many challenges faced widely-used classical approaches problem. show extensive empirical evaluation several real-world forecasting data sets methodology produces accurate forecasts state-of-the-art methods requiring minimal manual work. forecasting plays role automating optimizing operational processes businesses enables data driven decision making. retail example probabilistic forecasts product supply demand used optimal inventory management staff scheduling topology planning generally crucial technology aspects supply chain optimization. prevalent forecasting methods today developed setting forecasting individual small groups time series. approach model parameters given time series independently estimated past observations. model typically manually selected account different factors autocorrelation structure trend seasonality explanatory variables. ﬁtted model used forecast time series future according model dynamics possibly admitting probabilistic forecasts simulation closed-form expressions predictive distributions. many methods class based classical box-jenkins methodology exponential smoothing techniques state space models recent years type forecasting problem become increasingly important many applications. instead needing predict individual small number time series faced forecasting thousands millions related time series. examples include forecasting energy consumption individual households forecasting load servers data center forecasting demand products large retailer offers. scenarios substantial amount data past behavior similar related time series leveraged making forecast individual time series. using data related time series allows ﬁtting complex models without overﬁtting also alleviate time labor intensive manual feature engineering model selection steps required classical techniques. work present deepar forecasting method based autoregressive recurrent networks learns global model historical data time series data set. method builds upon previous work deep learning time series data tailors similar lstm-based recurrent neural network architecture probabilistic forecasting problem. challenge often encountered attempting jointly learn multiple time series realworld forecasting problems magnitudes time series differ widely distribution magnitudes strongly skewed. issue illustrated fig. shows distribution sales velocity across millions items sold amazon. distribution orders magnitude approximate power-law. observation best knowledge fundamental implications forecasting methods attempt learn global models datasets. scale-free nature distribution makes difﬁcult divide data sub-groups time series certain velocity band learn separate models them velocity sub-group would similar skew. further group-based regularization schemes proposed chapados fail velocities vastly different within group. finally skewed distributions make certain commonly employed normalization techniques input standardization batch normalization less effective. main contributions paper twofold propose architecture probabilistic forecasting incorporating negative binomial likelihood count data well special treatment case magnitudes time series vary widely; demonstrate empirically several real-world data sets model produces accurate probabilistic forecasts across range input characteristics thus showing modern deep learning-based approaches effective address probabilistic forecasting problem contrast common belief ﬁeld mixed results reported addition providing better forecast accuracy previous methods approach number advantages compared classical approaches global methods model learns seasonal behavior dependencies given covariates across time series minimal manual feature engineering needed capture complex group-dependent behavior; deepar makes probabilistic forecasts form monte carlo samples used compute consistent quantile estimates sub-ranges prediction horizon; learning similar items method able provide forecasts items little history case traditional single-item forecasting methods fail; approach assume gaussian noise incorporate wide range likelihood functions allowing user choose appropriate statistical properties data. points deepar apart classical forecasting approaches pertain producing accurate calibrated forecast distributions learned historical behavior time series jointly addressed global methods probabilistic forecasts crucial importance many applications they—in contrast point forecasts—enable optimal decision making uncertainty minimizing risk functions i.e. expectations loss function forecast distribution. immense practical importance forecasting vast variety different forecasting methods developed. prominent examples methods forecasting individual time series include arima models exponential smoothing methods; hyndman provide unifying review related techniques. figure summary model. training time step inputs network covariates target value previous time step zit− well previous network output hit−. network output used compute parameters likelihood used training model parameters. prediction history time series prediction range sample ˆzit drawn back next point prediction range generating sample trace. repeating prediction process yields many traces representing joint predicted distribution. especially demand forecasting domain often faced highly erratic intermittent bursty data violate core assumptions many classical techniques gaussian errors stationarity homoscedasticity time series. since data preprocessing methods often alleviate conditions forecasting methods also incorporated suitable likelihood functions zero-inﬂated poisson distribution negative binomial distribution combination tailored multi-stage likelihood sharing information across time series improve forecast accuracy difﬁcult accomplish practice often heterogeneous nature data. matrix factorization methods well bayesian methods share information hierarchical priors proposed mechanisms learning across multiple related time series leveraging hierarchical structure neural networks investigated context forecasting long time recent work considering lstm cells). recently kourentzes applied neural networks speciﬁcally intermittent data obtained mixed results. neural networks forecasting typically applied individual time series i.e. different model ﬁtted time series independently hand outside forecasting community time series models based recurrent neural networks successfully applied applications natural language processing audio modeling image generation main characteristics make forecasting setting consider different first probabilistic forecasting interested full predictive distribution single best realization used downstream decision making systems. second obtain accurate distributions count data negative binomial likelihood improves accuracy precludes directly applying standard data normalization techniques. past zit− denotes time point assume unknown prediction time covariates assumed known time points. prevent confusion avoid ambiguous terms past future refer time ranges conditioning range prediction range respectively. training ranges past observed prediction available conditioning range. note time index relative i.e. correspond different actual time period function implemented multi-layer recurrent neural network lstm cells. model autoregressive sense consumes observation last time step zit− input well recurrent i.e. previous output network hit− back input next time step. likelihood ﬁxed distribution whose parameters given function network output information observations conditioning range zit− transferred prediction range initial state hit−. sequence-to-sequence setup initial state output encoder network. general encoder network different architecture experiments using architecture model conditioning range prediction range further share weights them initial state decoder hit− obtained computing required quantities observed. initial state encoder well initialized zero. given model parameters ancestral sampling first obtain hit− computing sample ˜zit initialized ˜hit− hit− ˜zit− zit−. samples model obtained used compute quantities interest e.g. quantiles distribution values time range future. likelihood model likelihood determines noise model chosen match statistical properties data. approach network directly predicts parameters probability distribution next time point. experiments paper consider choices gaussian likelihood real-valued data negative-binomial likelihood positive count data. likelihood models also readily used e.g. beta likelihood data unit interval bernoulli likelihood binary data mixtures order handle complex marginal distributions long samples distribution cheaply obtained log-likelihood gradients wrt. parameters evaluated. parametrize gaussian likelihood using mean standard deviation mean given afﬁne function network output standard deviation obtained applying afﬁne transformation followed softplus activation order ensure modeling time series positive count data negative binomial distribution commonly used choice parameterize negative binomial distribution mean shape parameter parameters obtained network output fully-connected layer softplus activation ensure positivity. parameterization negative binomial distribution shape parameter scales variance relative mean i.e. parameterizations possible found particular especially conducive fast convergence preliminary experiments. training given data time series {zit}i=...n associated covariates obtained choosing time range prediction range known parameters model consisting parameters well parameters learned maximizing log-likelihood deterministic function input quantities required compute observed that—in contrast state space models latent variables—no inference required optimized directly stochastic gradient descent computing gradients respect experiments encoder model decoder distinction encoder decoder somewhat artiﬁcial training also include likelihood terms time series dataset generate multiple training instances selecting windows different starting points original time series. practice keep total length well relative length conditioning prediction ranges ﬁxed training examples. example total available range given time series ranges create training examples corresponding choosing windows ensure entire prediction range always covered available ground truth data chose start time series e.g. example above padding unobserved target zeros. allows model learn behavior time series taking account available features. augmenting data using windowing procedure ensure information absolute time available model covariates relative position time series. bengio noted that autoregressive nature models optimizing directly causes discrepancy model used training obtaining predictions model training values known prediction range used compute hit; prediction however unknown single sample model distribution used computation according instead. shown disconnect poses severe problem e.g. tasks observed adverse effects forecasting setting. preliminary experiments variants scheduled sampling show signiﬁcant accuracy improvements applying model data exhibits power-law scales depicted fig. presents challenges. firstly autoregressive nature model autoregressive input zit− well output network directly scale observations nonlinearities network limited operating range. without modiﬁcations network thus learn scale input appropriate range input layer invert scaling output. address issue dividing autoregressive inputs item-dependent scale factor conversely multiplying scale-dependent likelihood parameters factor. instance negative binomial likelihood log) log)/ outputs network parameters. note real-valued data could alternatively scale input preprocessing step possible count distributions. choosing appropriate scale factor might challenging conﬁdence interval model learns accurate seasonality patterns uncertainty estimates items different velocity age. item variances). however scaling average value experiments heuristic works well practice. secondly imbalance data stochastic optimization procedure picks training instances uniformly random visit small number time series large scale infrequently result underﬁtting time series. could especially problematic demand forecasting setting high-velocity items exhibit qualitatively different behavior low-velocity items accurate forecast high-velocity items might important meeting certain business objectives. counteract effect sample examples non-uniformly training. particular weighted sampling scheme probability selecting window example scale proportional sampling scheme simple effectively compensates skew fig. covariates item-dependent time-dependent both. used provide additional information item time point model. also used include covariates expects inﬂuence outcome long features’ values available also prediction range. experiments feature i.e. distance ﬁrst observation time series. also day-of-the-week hour-of-the-day hourly data week-of-year weekly data month-of-year monthly data. further include single categorical item feature embedding learned model. retail demand forecasting data sets item feature corresponds product category smaller data sets corresponds item’s identity allowing model learn item-speciﬁc behavior. standardize covariates zero mean unit variance. implement model using mxnet single p.xlarge instance containing cpus experiments. hardware full training prediction large dataset containing time series completed less hours. prediction already fast easily parallelized necessary. description hyper-parameter tuning procedure obtained hyper-parameter values well statistics datasets running time given supplementary material. datasets datasets evaluations. ﬁrst three–parts electricity traffic–are public datasets; parts consists aligned time series time steps each representing monthly sales different items automobile company electricity contains hourly time series electricity consumption customers traffic also used contains hourly occupancy rate lanes francisco area freeways. parts dataset ﬁrst months training data report error remaining results electricity traffic computed using rolling covariates depend time handled repeating along time dimension. instead using dummy variables encode these simply encode increasing numeric values. figure uncertainty growth time issm deepar models. unlike issm postulates linear growth uncertainty behavior uncertainty learned data resulting non-linear growth higher uncertainty around aggregate calculated entire dataset. figure coverage spans ec-sub dataset. left panel shows coverage single time-step interval right panel shows metrics larger time interval time-steps. correlation prediction sample paths destroyed shufﬂing samples time step correlation destroyed forecast becomes less calibrated. shufﬂed prediction also higher .-risk. window predictions described retrain model window single model trained data ﬁrst prediction window. remaining datasets ec-sub weekly item sales amazon used predict weeks evaluation done year following time series datasets diverse erratic ranging fast slow moving items contains products introduced weeks forecast time fig. further item velocities data power-law distribution shown fig. parts ec/ec-sub datasets compare negative-binomial autoregressive method snyder issm method strongest published results datasets. addition compare baseline models rnn-gaussian rnn-negbin. rnn-gaussian uses architecture deepar gaussian likelihood; however uses uniform sampling simpler scaling mechanism time series divided outputs multiplied rnn-negbin uses negative binomial distribution scale inputs outputs training instances drawn uniformly rather using weighted sampling. ρ-risk metrics quantify accuracy quantile predictive distribution; exact deﬁnition metric given supplementary material. metrics evaluated certain spans prediction range lead time forecast start point. table shows .-risk .-risk different lead times spans. denotes average risk marginals normalize reported metrics respect strongest previously published method deepar performs signiﬁcantly better methods datasets. results also show importance modeling data sets count distribution rnn-gaussian performs signiﬁcantly worse. ec-sub data sets exhibit power behavior discussed above without scaling weighted sampling accuracy decreased parts data exhibit power-law behavior rnn-negbin performs similar deepar. table compare point forecast accuracy electricity traffic datasets matrix factorization technique proposed consider metrics namely normalized deviation normalized rmse whose deﬁnition given supplementary material. results show deepar signiﬁcantly outperforms matfact electricity matfact performs better traffic. suspect fact time series traffic highly correlated matfact leverage improved accuracy. figure shows example predictions data set. fig. show aggregate sums different quantiles marginal predictive distribution deepar issm dataset. contrast issm models linear growth uncertainty part modeling assumptions uncertainty growth pattern learned data. case model learn overall growth uncertainty time. however simply linear growth uncertainty increases decreases shortly afterwards. calibration forecast distribution depicted fig. show percentile coverage deﬁned fraction time series dataset ppercentile predictive distribution larger true target. perfectly calibrated prediction holds coverage corresponds diagonal. compared issm model calibration improved overall. assess effect modeling correlations output i.e. much differ independent distributions time-point plot calibration curves shufﬂed forecast time point realizations original forecast shufﬂed destroying correlation time steps. short lead-time span consists time-point impact marginal distribution. longer lead-time span however destroying correlation leads worse calibration showing important temporal correlations captured time steps. shown forecasting approaches based modern deep learning techniques drastically improve forecast accuracy state forecasting methods wide variety data sets. proposed deepar model effectively learns global model related time series handles widely-varying scales rescaling velocity-based sampling generates calibrated probabilistic forecasts high accuracy able learn complex patterns seasonality uncertainty growth time data. interestingly method works little hyperparameter tuning wide variety datasets applicable medium-size datasets containing hundred time series. t=t+l zit. given quantile denote predicted ρ-quantile obtain quantile prediction sample paths realization ﬁrst summed given span. samples sums represent estimated distribution take ρ-quantile empirical distribution. ρ-quantile loss deﬁned mxnet neural network framework experiments laptop parts single p.xlarge instance datasets. note predictions done datasets matter hours even single machine. adam optimizer early stopping standard lstm cells forget bias experiment samples drawn decoder generate predictions. parts dataset ﬁrst months training data report error remaining datasets electricity traffic ec-sub possible training instances sub-sampled number indicated table scores electricity traffic reported using rolling window operation described note retrain model reuse predicting across different time windows instead. running times measures evaluation e.g. processing features training neural network drawing samples evaluating produced distributions. dataset grid-search used best value hyper-parameters item output embedding dimension lstm nodes data forecast start time used training split partitions. hyper-parameter candidate model ﬁrst partition training containing data pick minimal negative log-likelihood remaining best hyper-parameters found evaluation metrics evaluated test e.g. data coming forecast start time. note procedure could lead over-ﬁtting hyper-parameters training would also degrades metric report. better procedure would parameters evaluate negative log-likelihood different windows also non-overlapping time intervals. learning rate tuned manually every dataset kept ﬁxed hyper-parameter tuning. parameters encoder length decoder length item input embedding considered domain dependent ﬁtted. batch size increased larger datasets beneﬁt gpu’s parallelization. finally running times measures evaluation e.g. processing features training neural network drawing samples evaluating produced distributions. forecasting settings target values might missing subset time points. instance context demand forecasting item out-of-stock certain time case demand item cannot observed. explicitly modeling missing observations best case lead systematic forecast underbias worst case larger supply chain context lead disastrous downward spiral out-ofstock situation leads lower demand forecast lower re-ordering out-of-stock-situations. model missing observations easily handled principled replacing unobserved value sample ˜zit conditional predictive distribution computing excluding likelihood term corresponding missing observation omitted experimental results setting paper proper evaluation light missing data prediction range requires non-standard adjusted metrics hard compare across studies", "year": 2017}