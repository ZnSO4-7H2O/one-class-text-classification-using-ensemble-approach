{"title": "Experimental results : Reinforcement Learning of POMDPs using Spectral  Methods", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "We propose a new reinforcement learning algorithm for partially observable Markov decision processes (POMDP) based on spectral decomposition methods. While spectral methods have been previously employed for consistent learning of (passive) latent variable models such as hidden Markov models, POMDPs are more challenging since the learner interacts with the environment and possibly changes the future observations in the process. We devise a learning algorithm running through epochs, in each epoch we employ spectral techniques to learn the POMDP parameters from a trajectory generated by a fixed policy. At the end of the epoch, an optimization oracle returns the optimal memoryless planning policy which maximizes the expected reward based on the estimated POMDP model. We prove an order-optimal regret bound with respect to the optimal memoryless policy and efficient scaling with respect to the dimensionality of observation and action spaces.", "text": "propose reinforcement learning algorithm partially observable markov decision processes based spectral decomposition methods. spectral methods previously employed consistent learning latent variable models hidden markov models pomdps challenging since learner interacts environment possibly changes future observations process. devise learning algorithm running epochs epoch employ spectral techniques learn pomdp parameters trajectory generated ﬁxed policy. epoch optimization oracle returns optimal memoryless planning policy maximizes expected reward based estimated pomdp model. prove order-optimal regret bound respect optimal memoryless policy efﬁcient scaling respect dimensionality observation action spaces. reinforcement learning effective approach solve problem sequential decision– making uncertainty. agents learn maximize long-term reward using experience obtained direct interaction stochastic environment since environment initially unknown agent balance exploring environment estimate structure exploiting estimates compute policy maximizes long-term reward. result designing algorithm requires three different elements estimator environment’s structure planning algorithm compute optimal policy estimated environment strategy make trade exploration exploitation minimize regret i.e. difference performance exact optimal policy rewards accumulated agent time. literature assumes environment modeled markov decision process markovian state evolution fully observed. number exploration– exploitation strategies shown strong performance guarantees mdps either terms regret sample complexity however assumption full observability state evolution often violated practice agent noisy observations true state environment case appropriate partially-observable pomdp model. many challenges arise designing algorithms pomdps. unlike mdps estimation problem involves identifying parameters latent variable model agent directly observes state transitions estimation generative model straightforward empirical estimators. hand pomdp transition reward models must inferred noisy observations markovian state evolution hidden. planning problem i.e. computing optimal policy pomdp known parameters pspace-complete requires solving augmented built continuous belief space finally integrating estimation planning exploration–exploitation strategy guarantees non-trivial no-regret strategies currently known handle challenges build results paper previous paper azizzadenesheli pomdps. main contributions paper follows propose algorithm pomdps incorporates spectral parameter estimation within exploration-exploitation framework. apply algorithm grid world atari game compare performance state deep learning mnih show underlying model model based algorithms learn wrong model representation model free algorithms learn q-function observation carry markovian property anymore. furthermore non-markovianity observation current observation sufﬁcient statistic policy sort memory required. assume game green apples. beginning game emulator reveals shows apple positive reward negative reward. case based learner forgets always suffers regret linear regret. paper estimation pomdp carried spectral methods involve decomposition certain moment tensors computed data. learning algorithm interleaved optimization planning policy using exploration–exploitation strategy inspired ucrl method mdps resulting algorithm called sm-ucrl runs epochs variable length agent follows ﬁxed policy enough data collected updates current policy according estimates pomdp parameters accuracy. derive regret bound respect best memoryless policy given pomdp. indeed general pomdp optimal policy need memoryless. however ﬁnding optimal policy uncomputable inﬁnite horizon regret minimization instead memoryless policies shown good performance practice moreover class so-called contextual special class pomdps optimal policy also memoryless brafman tennenholtz jaksch different setting. mdps widely studied design effective exploration–exploration strategies pomdps still relatively unexplored. ross poupart vlassis propose integrate problem estimating belief state model-based bayesian approach distribution possible mdps updated time. alternative model-based approaches adapt model-free algorithms q-learning case pomdps. perkins proposes monte-carlo approach action-value estimation shows convergence locally optimal memoryless policies. alternative approach solve pomdps policy search methods avoid estimating value functions directly optimize performance searching given policy space usually contains memoryless policies beside practical success ofﬂine problems policy search successfully integrated efﬁcient exploration–exploitation techniques shown achieve small regret nonetheless performance methods severely constrained choice policy space contain policies good performance. matrix decomposition methods previously used general setting predictive state representation reconstruct structure dynamical system. despite generality psrs proposed model relies strong assumptions dynamics system theoretical guarantee performance. recently introduced compressed method reduce computation cost exploiting advantages dimensionality reduction incremental matrix decomposition compressed sensing. work take ideas considering powerful tensor decomposition techniques. last decades latent variable models become popular model problems partially observable variables. traditional methods expectation-maximization variational methods used learn hidden structure model usually consistency guarantees computationally massive mostly converge local optimum arbitrarily bad. come drawbacks spectral methods used consistent estimation wide class lvms anandkumar anandkumar theoretical guarantee computation complexity using robust tensor power method well studied song wang today spectral methods tensor decomposition methods well known credible alternative variational methods inferring latent structure model. method shown efﬁcient learning gaussian mixture models topic modeling latent dirichlet allocation hidden markov model etc. pomdp tuple ﬁnite state space cardinality ﬁnite action space cardinality ﬁnite observation space cardinality ﬁnite reward space cardinality largest reward rmax. finally denotes transition density probability transition given state-action pair mean reward state action furthermore observation density probability receiving observation corresponding indicator vector given state whenever convenient tensor forms density functions rx×x×a also denote ﬁber obtained ﬁxing arrival state action rx×x transition matrix states using action graphical model associated pomdp illustrated fig. policy stochastic mapping observations actions policy denote density function. denote stochastic memoryless policies. acting according policy pomdp deﬁnes markov chain characterized transition density section introduce novel spectral method estimate pomdp parameters stochastic policy used generate trajectory steps. similar case hmms element apply spectral methods construct multi-view model hidden states. despite similarity spectral method developed anandkumar cannot directly employed here. fact hmms state transition observations depend current state. hand pomdps probability transition state depends also action since action chosen according memoryless policy based current observation creates indirect dependency observation makes model intricate. estimate pomdp parameters action separately. step construct three views contain observable elements. seen fig. three views provide information hidden state careful analysis graph dependencies shows conditionally views independent. instance consider ~yt+. random variables clearly dependent since inﬂuences action triggers transition emits observation yt+. nonetheless sufﬁcient condition action break dependency make ~yt+ independent. similar arguments hold elements views used recover latent variable formally encode triple vector whenever ~yt− suitable mapping index indices action observation reward. similarly proceed introduce three view matrices ra·y ·r×x empirical estimates pomdp parameters. available need estimated samples. given trajectory steps obtained executing policy steps action played collect triples construct corresponding views apply spectral tensor decomposition method recover empirical estimates second third views thereafter simple manipulation results model parameters. overall method summarized alg. empirical estimates pomdp parameters enjoy following guarantee. edly tried time estimates converge true parameters pomdp. contrast em-based methods typically stuck local maxima return biased estimators thus preventing deriving conﬁdence intervals. interesting aspect estimation process illustrated previous section applied samples collected using policy result integrated exploration-exploitation strategy policy changes time attempt minimizing regret. algorithm. sm-ucrl algorithm illustrated alg. result integration spectral method structure similar ucrl designed optimize exploration-exploitation trade-off. learning process split episodes increasing length. beginning episode computed using spectral method bled i.e. stopping criterion avoids switching policies often guarantees epoch terminated enough samples collected compute policy. process repeated epochs expect optimistic policy progressively closer best policy estimates pomdp accurate. random reward obtained time states traversed policies performed epochs actual pomdp. restate complexity learning pomdp partially determined diameter deﬁned corresponds expected passing time state state starting action terminating action following effective memoryless policy result following theorem. subsection illustrate performance method simple synthetic environment follows pomdp structure rmax spectral learning method quickly learn model parameters fig. estimation transition tensor takes effort compared estimation observation matrix reward matrix fact transition tensor estimated given estimated matrix adds error. planning given pomdp model parameters alternating maximization method memoryless policy. method iteratively alternates updates policy stationary distribution ends stationary point optimization problem. that practice method converges reasonably good solution shows planing np-hard general). resulting regret bounds shown fig. compare following algorithm baseline random policies simply selects random actions without looking observed data ucrl-mdp auer attempts model observed data runs ucrl policy q-learning watkins dayan model-free method updates policy based q-function. method converges much faster. addition show converges much better policy note mdp-based policies ucrl-mdp q-learning perform poorly even worse random policy sm-ucrl policy. model misspeciﬁcation dealing larger state space. following provide empirical guarantees sm-ucrl environment simple computer game. game figs. environment grid world sweet apple poisonous apples environment uniformly spread apples. addition apples lasts uniformly time steps disappear eaten agent. game agent interacts environment. study settings possible actions actions time step agent chooses action deterministically moves step direction. sweet apple location agent score score poisonous one. game time step agent partially observes environment fig. single agent visible fig. three boxes observable. randomness rewarding process partial observability bring notion hidden structure pushes environment pompds models rather mdps. addition apply mnih three hidden layers hidden units hyperbolic tangent activation functions hidden layer back propagation rmsprop method shown robust stable. figs. shows performance sm-ucrl case action show sm-ucrl captures environment behavior faster also reaches better long term average reward. couple times represent average performance shown figs. times traps local minima results performance degrades average performance. setting three boxes observable fig. observation cardinality tune sm-ucrl apply environment. again show sm-ucrl outperforms structure except hidden units hidden layer. implementation observed sm-ucrl need estimate model parameter well reasonable policy. comes stochastic reasonably good policy even beginning. hand observed policy makes balance moving upward downward makes good balance moving rightward leftward order keep agent away walls. helps agent collect reward move around area walls. conclusion introduced novel algorithm pomdps relies spectral method consistently identify parameters pomdp optimistic approach solution exploration– exploitation problem. resulting algorithm derive conﬁdence intervals parameters minimax optimal bound regret. work opens several interesting directions future development. pomdp special case predictive state representation model littman allows representing sophisticated dynamical systems. given spectral method developed paper natural extension apply general model integrate exploration– exploitation algorithm achieve bounded regret. long pomdps suitable models real world applications compared experimental analyses interesting.", "year": 2017}