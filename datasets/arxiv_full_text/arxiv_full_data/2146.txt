{"title": "Efficient Hyperparameter Optimization of Deep Learning Algorithms Using  Deterministic RBF Surrogates", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Automatically searching for optimal hyperparameter configurations is of crucial importance for applying deep learning algorithms in practice. Recently, Bayesian optimization has been proposed for optimizing hyperparameters of various machine learning algorithms. Those methods adopt probabilistic surrogate models like Gaussian processes to approximate and minimize the validation error function of hyperparameter values. However, probabilistic surrogates require accurate estimates of sufficient statistics (e.g., covariance) of the error distribution and thus need many function evaluations with a sizeable number of hyperparameters. This makes them inefficient for optimizing hyperparameters of deep learning algorithms, which are highly expensive to evaluate. In this work, we propose a new deterministic and efficient hyperparameter optimization method that employs radial basis functions as error surrogates. The proposed mixed integer algorithm, called HORD, searches the surrogate for the most promising hyperparameter values through dynamic coordinate search and requires many fewer function evaluations. HORD does well in low dimensions but it is exceptionally better in higher dimensions. Extensive evaluations on MNIST and CIFAR-10 for four deep neural networks demonstrate HORD significantly outperforms the well-established Bayesian optimization methods such as GP, SMAC, and TPE. For instance, on average, HORD is more than 6 times faster than GP-EI in obtaining the best configuration of 19 hyperparameters.", "text": "automatically searching optimal hyperparameter conﬁgurations crucial importance applying deep learning algorithms practice. recently bayesian optimization proposed optimizing hyperparameters various machine learning algorithms. methods adopt probabilistic surrogate models like gaussian processes approximate minimize validation error function hyperparameter values. however probabilistic surrogates require accurate estimates sufﬁcient statistics error distribution thus need many function evaluations sizeable number hyperparameters. makes inefﬁcient optimizing hyperparameters deep learning algorithms highly expensive evaluate. work propose deterministic efﬁcient hyperparameter optimization method employs radial basis functions error surrogates. proposed mixed integer algorithm called hord searches surrogate promising hyperparameter values dynamic coordinate search requires many fewer function evaluations. hord well dimensions exceptionally better higher dimensions. extensive evaluations mnist cifar- four deep neural networks demonstrate hord signiﬁcantly outperforms well-established bayesian optimization methods smac tpe. instance average hord times faster gp-ei obtaining best conﬁguration hyperparameters. deep learning algorithms extensively used various artiﬁcial intelligence related problems recent years. however adoption severely hampered many hyperparameter choices must make e.g. architectures deep neural networks forms activation functions learning rates. determining appropriate values hyperparameters crucial importance copyright association advancement artiﬁcial intelligence rights reserved. hyperparameter optimization global optimization black-box error function whose evaluation expensive function maps hyperparameter choice conﬁgurable hyperparameters validation error deep learning algorithm learned parameters optimizing follows gives automatically search optimal hyperparameters ztrain zval denote training validation datasets respectively learned minimizing training error bounded set. following omit dependency denote short. solving problem challenging high complexity function popular solution employ bayesian optimization algorithms cheap probabilistic surrogate model approximate expensive error function gaussian process tree-structured parzen estimator work propose deterministic-surrogate based hyperparameter optimization method show requires considerably fewer function evaluations optimization. proposed algorithm uses radial basis function surrogate approximate error function hyperparameters avoids expensive computation covariance statistics needed methods. proposed algorithm searches surrogate dynamic hyperparameter scales cubically number observations. address scalability issue based methods proposed non-standard bayesian-based optimization algorithm uses treestructured parzen estimators model error distribution non-parametric way. smac another tree-based algorithm uses random forests estimate error density. eggensperger empirically showed spearmints gp-based approaches reach state-of-the-art performance optimizing hyperparameters tpe’s smac’s tree-structured approaches achieve best results high-dimensional spaces. thus compare algorithm four methods. authors propose interesting using surrogate based models hyperparameter optimization. propose build surrogate validation error given hyperparameter conﬁguration evaluate compare different hyperparameter optimization algorithms. however unify surrogate models optimization algorithm. authors address scalability issue gb-based models explore neural networks alternative surrogate model. neural network based hyperparameter optimization algorithm uses fraction computational resources gp-based algorithms especially number function evaluation grows however opposed proposed method approach outperform gp-based algorithms tree-based algorithms smac. description proposed method hord introduce novel hyperparameter optimization called hyperparameter optimization using dynamic coordinate search hord short. surrogate model radial basis function interpolation model surrogate model searching optimal hyperparameters. given number hyperparameters hyperparameter conﬁgurations corresponding validation errors deﬁne interpolation model denotes cubic spline euclidean norm bx+a polynomial tail parameters interpolation model parameters determined solving linear system equations coordinate search able near optimal hyperparameter conﬁguration expensive function evaluations either integer continuous valued hyperparameters. name algorithm hyperparameter optimization dynamic coordinate search hord short. compare method well-established gp-based algorithms tree-based algorithms optimizing hyperparameters types commonly used neural networks applied mnist cifar- benchmark datasets. provide mixed integer deterministic-surrogate optimization algorithm optimizing hyperparameters. algorithm capable optimizing continuous integer hyperparameters deep neural networks performs equally well low-dimensional hyperparameter spaces exceptionally well higher dimensions. extensive evaluations demonstrate superiority proposed algorithm state-of-the-art ﬁnding near-optimal conﬁguration fewer function evaluations achieving lower ﬁnal validation error. discussed later hord obtains speedups fold average computation time algorithms -dimensional problem faster average dimensions problems tested. surrogate-based optimization strategy global optimization expensive black-box functions constrained domain. goal obtain near optimal solution possible function evaluations. main idea utilize surrogate model expensive function inexpensively evaluated determine next promising point evaluation. surrogate-based optimization methods differ aspects type model used surrogate model used determine next promising point expensive evaluation. bayesian optimization type surrogate-based optimization surrogate probabilistic model compute posterior estimate distribution expensive error function values. next promising point determined optimizing acquisition function choice given distribution estimation. various bayesian optimizations methods mainly differ estimate error distribution deﬁnition acquisition function. gp-ei gppes methods gaussian processes estimate distribution validation error given hyperparameter conﬁguration using history observations. methods expected improvement predictive entropy search respectively acquisition function. although simple ﬂexible gp-based optimization involves inverting expensive covariance matrix thus algorithm hyperparameter optimization using rbfbased surrogate dycors input nmax. output optimal hyperparameters xbest. latin hypercube sampling sample points evaluating points gives update surrogate model xbest min{f compute probability perturbing coordinate. populate candidate points candidate xbest select coordinates perturbed probability sampled coordinates selected round nearest integer required. calculate ﬁnal weighted score min{wn}. evaluate adjust variance update average number coordinates perturbed always less hord combines features dycors so-mi generate candidates. dycors continuous optimization surrogate method uses controlling perturbations. so-mi ﬁrst mixed integer implementation surrogate method. initially tfail variance consecutive iterations improvement current xbest variance doubled consecutive iterations improvement. adjustment variance place facilitate convergence optimization algorithm. candidate points populated points estimate corresponding validation errors computing surrogate values denotes candidate point. compute distances previously evaluated points mint here euclidean norm. also compute ∆max max{∆} ∆min min{∆}. searching hyperparameter space training evaluation deep neural network regarded function maps hyperparameter conﬁguration used train network validation error obtained end. optimizing respect hyperparameter conﬁguration global optimization problem since function typically highly multimodal. employ interpolation approximate expensive function build mixed integer global optimization hord algorithm features continuous global dynamic coordinate search search surrogate model promising hyperparameter conﬁgurations. lmsrbf dycors-lmsrbf algorithm starts ﬁtting initial surrogate model using {)}n latin hypercube sampling method sample hyperparameter conﬁgurations obtain respective validation errors next nmax user-deﬁned maximal evaluation number candidate point generation algorithm populates candidate point candidate points selects evaluation promising point xn+. nmax iterations algorithm returns hyperparameter conﬁguration xbest yielded lowest validation error formal algorithm description given alg. candidate hyperparameters iteration populated points generated adding noise coordinates current xbest. hord expected number coordinates perturbed monotonically decreasing since perturbing coordinates current xbest result point much xbest signiﬁcant problem dimension becomes large e.g. note that iteration candidate point generated perturbing potentially different subset coordinates. compute ﬁnal weighted score ﬁnal weighted score acquisition function used hord select evaluation point. here cyclic weight balancing global local search. cycle following weights sequential manner. select point lowest weighted score next evaluated point xn+. muller shoemaker compare candidate approach instead searching genetic algorithm. krityakierne akhtar shoemaker multi-objective methods surrogate select next point evaluation single objective optimization problems test hord four hyperparameter optimization problems hyperparameters. ﬁrst problem consists continuous integer hyperparameters multi-layer perceptron network applied classifying grayscale images handwritten digits popular benchmark dataset mnist. problem also referred -mlp subsequent discussions. network consists hidden layers relu activation softmax end. learning algorithm stochastic gradient descent compute hyperparameters optimize hord algorithms include hyperparameters learning algorithm layer weight initialization hyperparameters network structure hyperparameters. full details used datasets hyperparameters optimized respective value ranges well values used initial starting point provided supplementary materials. second problem continuous integer hyperparameters complex convolutional neural network consists convolutional blocks containing convolutional layer batch normalization followed relu activation max-pooling. following convolutional blocks fully-connected layers leakyrelu activation softmax layer end. mnist dataset follow leakyrelu deﬁned third problem incorporates higher number hyperparameters. increase number hyperparameters continuous integer setup second experiment. test problem referred -cnn subsequent discussion. finally design fourth problem challenging dataset cifar- even higher dimensional hyperparameter space optimizing hyperparameters continuous integer. hyperparameter optimization problem referred -cnn subsequent discussions. optimize hyperparameters network problem -cnn except include four dropout layers convolutional block fully-connected layer. dropout rate four layers hyperparameters optimized. baseline algorithms compare hord gaussian processes expected improvement gaussian processes predictive entropy search tree parzen estimator sequential modelbased algorithm conﬁguration algorithms. evaluation budget trials initial points hyperparameter evaluation typically computationally expensive desirable good hyperparameter values within limited evaluation budget. accordingly limit number optimization iterations function evaluations function evaluations involves full training evaluation dnn. least trials experiment using different random seeds. smac algorithm starts optimization manually initial starting point advantage using manually oppose random samples becomes evident optimizing high number hyperparameters. unfortunately gp-based algorithms well cannot directly employ manually guide search thus slight disadvantage. hand hord initial surrogate model latin hypercube samples also include manually added points guide search better region hyperparameter space. denote variant hord hord-isp test hord-isp -cnn problems only. following common guidelines setting hyperparameters network supply hord-isp smac -cnn problems. hord-isp performed hord hyperparameter cases. pysot hord number algorithm parameters including number points initial latin hypercube samples number candidate points cycling weights used computing candidate point score parameters controlling changes perturbation variances improvement. used default values algorithm parameters given pysot software toolbox attempt adjust improve performance. spearmint library obtain results gpbased algorithms. hyperopt library obtain results method. public implementation smac. hord implementation networks optimized code necessary reproducing experiments available bit.ly/hord-aaai. algorithm comparison methodology purpose algorithm comparison done assess algorithms likely efﬁcient efﬁciency vary number hyperparameters across different problems. also would like identify possible causes variation. table shows mean standard deviation best test error obtained methods hundred evaluations test problems. problems hord performs better algorithms test error. however analyze detail comparative performance algorithms subsequent discussions validation error since objective function figure plots mean value best solution found function number expensive evaluations average trials. since minimizing lowest curves better. horizontal dotted line figure -cnn problem hord reaches evaluations mean best validation error achieved smac evaluations hord required evaluations required smac answer; shown lower right corner table table reports percentage hord comparison algorithms hyperparameter optimization problems tested. percentages signiﬁcantly less implying every problem hord obtained equivalent mean answer methods less time. since hord baseline algorithms stochastic also important stringent test statistical test understand many function evaluations hord achieves best validation error statistically better best validation error achieved algorithms evaluations. employ rank test analysis report percentages table figure efﬁciency comparison hord hord-isp baselines optimizing hyperparameters cifar- dataset plot validation error curves compared methods number function evaluations hord hord-isp show signiﬁcantly efﬁcient methods. hord-isp takes function evaluations achieve lowest validation error best baseline achieves evaluations. evaluate performance deep learning algorithms optimized hyperparameters report mean standard deviation best test error algorithms problems evaluations table results indicate hord hord-isp achieve best results average problems. results optimization network hyperparameters -mlp problem average performance hord better algorithms evaluations furthermore table clearly indicates hord found signiﬁcantly better hyperparameters tree-based algorithms smac gp-pes within evaluations required algorithms evaluations. performances hord gp-ei however statistically distinguishable performance gp-ei algorithm expected since gp-based methods well suited low-dimensional optimization. however hord also performs almost exactly same using times less computing resources propose hyperparameter conﬁgurations. supplementary comparison algorithm running times. table report mean standard deviation test error obtained best found hyperparameter conﬁguration iterations least trials different random seeds. algorithm lowest test error shown bold. function evaluations furthermore performance gp-ei statistically comparable hord function evaluations. however hord statistically signiﬁcantly outperforms tree-based algorithms gp-pes shows algorithm perform well complex network slightly higher number integer hyperparameters. results optimization hyperparameters mnist cifar- -cnn -cnn experiments conﬁrm higher dimensional spaces hord-isp truly shines expected performance gp-based algorithms degrades signiﬁcantly high dimensional search space. while hord-isp continues perform well even outperforms tree-based algorithms smac designed speciﬁcally optimizing high number hyperparameters. analysis figure indicates hord hord-isp ﬁnding better solutions iterations algorithms -cnn. observe points used build surrogate hord hord-isp propose hyperparameter values result validation errors mostly range while algorithms essentially performing random search whole hyperparameter space. furthermore hord hord-isp function evaluations reach validation errors lower ﬁnal lowest validation errors algorithms. compare hord-isp figure mean validation error v.s. number function evaluations different methods optimizing hyperparameters cifar-. represents validation error algorithm corresponding evaluation instance. evaluations searching hord hordisp starts focus hyperparameters smaller validation error stark contrast methods. discussion differences results among methods compared hod-isp dramatically different dimensional hyperparameter problems. example problem hord-isp obtains function evaluations average validation answer gp-ei required evaluations obtain meaning hord-isp times faster gp-ei. even compared smac designed work better higher dimensions hord times faster based average times stringent statistical test given table example hord-isp times faster best algorithms smac hord without initial guess also much better gp-ei gp-pes higher dimensions neither algorithm initial guess terms speed quality answer hord hord-isp obtained evaluations. lowest dimension problems shown table speed hord compared methods equal greater comparison methods. table shows comparisons hord methods lower table report minimum percentage function evaluations required hord hord-isp obtain signiﬁcantly better best validation error best error achieved algorithms function evaluations. difference algorithms insigniﬁcant function evaluations report statistical tests performed percent signiﬁcance level. dimensional problems statistically signiﬁcant level variability among trials methods. however hord hord-isp best average answer dimensions algorithm performed well dimensions. understand differences performance algorithms consider inﬂuence surrogate type procedure selecting next point algorithm evaluates expensive function regard surrogate type comparisons made widely used algorithm gaussian process surrogate uses maximizing expected improvement search surrogate. functions poorly higher dimensions deterministic surrogate optimization method dycors best three multimodal problems paper. previous comparisons based numbers function evaluation. current study non-evaluation time also seen much longer gaussian process based methods presumably require relatively large amount computing kriging surface covariance matrix amount computing increases rapidly dimension increases. however also major differences among algorithms procedure selecting next expensive evaluation point based surrogate possibly important type surrogate used. algorithms compared randomly generated candidate points select point expensively evaluate sorting criterion based surrogate location previously evaluated points. gp-ei gp-pes smac distribute candidate points uniformly domain. contrast hord uses strategy lmsrbf create candidate points generating perturbations around current best solution. uniform distribution distribution normally random perturbations around current best solution compared surrogate optimization lmsrbf gave better results range problems. hord uses dycors strategy perturb faction dimensions strategy used baseline methods. strategy greatly improves efﬁciency higher dimensional problems applications shown earlier papers indicate combination methods used hord generate trial points search weighted average surrogate value distance successful method kinds continuous global optimization problems. surprising methods also work well hord mixed integer problems multimodal machine learning. introduce hyperparameter optimization algorithm hord paper. results show hord signiﬁcantly faster higher dimensional problems. hord efﬁcient previous algorithms uses deterministic radial basis surrogate iteration generates candidate points different places closer current best solution reduces expected number dimensions perturbed number iterations increase. also present test hord-isp variant hord initial guess solution. demonstrated method effective optimizing hyperparameters deep learning algorithms. future plan extend hord incorporate parallelization mixture surrogate models improving algorithm efﬁciency. potential improving efﬁciency surrogate based algorithms parallelization depicted mixture surrogate models depicted work jiashi feng partially supported national university singapore startup grant r---c ministry education singapore acrf tier grant r---c-. shoemaker akhtar supported part startup funds create program sponsored singapore.", "year": 2016}