{"title": "A Compact DNN: Approaching GoogLeNet-Level Accuracy of Classification  and Domain Adaptation", "tag": ["cs.CV", "cs.AI", "cs.NE"], "abstract": "Recently, DNN model compression based on network architecture design, e.g., SqueezeNet, attracted a lot attention. No accuracy drop on image classification is observed on these extremely compact networks, compared to well-known models. An emerging question, however, is whether these model compression techniques hurt DNN's learning ability other than classifying images on a single dataset. Our preliminary experiment shows that these compression methods could degrade domain adaptation (DA) ability, though the classification performance is preserved. Therefore, we propose a new compact network architecture and unsupervised DA method in this paper. The DNN is built on a new basic module Conv-M which provides more diverse feature extractors without significantly increasing parameters. The unified framework of our DA method will simultaneously learn invariance across domains, reduce divergence of feature representations, and adapt label prediction. Our DNN has 4.1M parameters, which is only 6.7% of AlexNet or 59% of GoogLeNet. Experiments show that our DNN obtains GoogLeNet-level accuracy both on classification and DA, and our DA method slightly outperforms previous competitive ones. Put all together, our DA strategy based on our DNN achieves state-of-the-art on sixteen of total eighteen DA tasks on popular Office-31 and Office-Caltech datasets.", "text": "recently model compression based network architecture design e.g. squeezenet attracted attention. compared well-known models extremely compact networks don’t show accuracy drop image classiﬁcation. emerging question however whether compression techniques hurt dnn’s learning ability classifying images single dataset. preliminary experiment shows compression methods could degrade domain adaptation ability though classiﬁcation performance preserved. work propose compact network architecture unsupervised method. built basic module conv-m provides diverse feature extractors withsigniﬁcantly increasing parameters. uniﬁed framework method simultaneously learn invariance across domains reduce divergence feature representations adapt label prediction. parameters—only alexnet googlenet. experiments show obtains googlenet-level accuracy classiﬁcation method slightly outperforms previous competitive ones. together strategy based achieves stateof-the-art sixteen total eighteen tasks popular ofﬁce- ofﬁce-caltech datasets. success deep neural networks encourages extensive applications various types platforms e.g. self-driving cars headsets. overcome hardware constraints model compression techniques learning based network architecture de∗part work done intern jose intersign recently attracted attention. estingly extremely compact models show accuracy drop image classiﬁcation. critical question emerges however classifying images single dataset whether compression methods hurt dnn’s learning ability. work attempt bridge compressed architecture domain adaptation ability. ability evaluate whether machine learning model capture covariate shift source target domains adapt remove divergence. model outstanding semi-supervised unsupervised ability greatly reduce requirement manually labeled examples real-world applications. observe accuracy degradation model compression methods based architecture design e.g. googlenet-level classiﬁcation accuracy obtains alexnet-level accuracy. table shows experimental results. squeezenet faconvnet used compare alexnet respectively smallest model achieving alexnet-level googlenet-level accuracy image classiﬁcation best knowledge. popular dataset imagenet’ adopted image classiﬁcation benchmark. three standard tasks ofﬁce- dataset adopted unsupervised method used dnns table dnns pre-trained imagenet’ ﬁne-tuned tasks. accuracy difference alexnet squeezenet though networks almost classiﬁcation accuracy. faconvnet outperforms alexnet classiﬁcation also slightly lags behind alexnet intuitively increasing parameters lead better accuracy. following experiment shows accuracy squeezenet faconvnet improved reach level classiﬁcation solely method consists three components learning invariance across domains reducing discrepancy feature representations predicting labels. third experiments show obtains googlenet-level accuracy classiﬁcation accuracy googlenet compact dnns much larger. fourth uniﬁed framework method slightly outperforms previous competitive methods method based network achieves state-of-the-art sixteen total eighteen tasks popular ofﬁce- ofﬁcecaltech datasets. model compression little accuracy drop image classiﬁcation traditionally learning based. zero alexnet’s parameters using sparse decomposition regularize model structured sparsity based group lasso. prune small-weight connections retrain remaining connections. recent research began shrink model directly based network architecture design. squeezenet built module feeds squeeze layer expand layer basic structure faconvnet convolutional layer stacked single basis layer. popular design methodology compact architectures extensively uses small convolutional kernels especially linear projection conv layer shown bold figure based preliminary experimental result table argue necessary redesign basic module extremely shrunk dnns e.g. faconvnet squeezenet introducing diverse operations feature extraction order achieve high accuracy classiﬁcation challenge lies complex feature extraction methods e.g. multi-scale convolution often result steep increase parameters basic module used reapeatedly. shortcut connection used resnet instance underboosting parameter numbers. speciﬁcally without changing structure models increase parameters faconvnet squeezenet. basic modules respectively adopted faconvnet squeezenet ﬁrst compared shown figure shared feature modules bottleneck layer conv denoted bold. hence gradually increase parameters bottleneck layers faconvnet squeezenet accuracy beneﬁt could obtained. parameters layers increased accuracy gain. ﬁnal accuracy adapted models revfaconvnet rev-squeezenet respectively shown table expectation rev-faconvnet’s accuracy much higher alexnet. rev-faconvnet however slightly outperforms alexnet almost parameters. objective work develop compact architecture achieve level accuracy classiﬁcation solution offers four important features. first parameters alexnet googlenet. compactness network attributed module conv-m parameter-saving module extract details based multi-scale convolution deconvolution inspired googlenet’s inception. secunsupervised following early attempt reweighting samples source domain shekhar learn dictionary based representations minimizing divergence source target domains. subspace based methods hand evaluate distance domains low-dimensional manifold terms frobenius norm based methods proposed recently. glorot chopra learn cross-domain features using auto-encoders followed label prediction. popular strategy combine feature adaptation label prediction uniﬁed framework. introduces adaptation layers domain confusion metric architecture combines classiﬁers label domain using gradient reverse layer. focus effectively measuring feature representations kernel spaces. transduction jointly optimizes target label domain transformation parameters. method adopts uniﬁed framework simultaneously learn invariance across domains reduce divergence feature representations adapt label prediction. based image segmentation. dnns segmentation classiﬁcation mainly differ upsampling layers recover resolution. various up-scaling methods proposed adopted straightforward bicubic interpolation learning based deconvolution unpooling improve deconvolution remove artifacts described section type shape feature extractor basic module dnn. consideration training convergence speed unpooling fewer parameters better choice compared deconvolution especially small-scale medium-scale problems. adopt unpooling sample reconstruction method. addition different strategies presented train segmentation networks. segnetbasic directly trained whole. long hand adapt popular classiﬁcation network fully convolutional network ﬁne-tune segmentation tasks. show accuracy improved plugging context module existing segmentation model. decoder design sample reconstruction inspired structure simpler multi-stream structure fcn. figure shows conv-m module used dnn. according preliminary experiment analysis section design idea capture diverse details different levels using fewer parameters. achieve goal dilated convolution multi-resolution deconvolution introduced. dilated convolution extract features larger receptive ﬁeld withincreasing kernel size e.g. extracting features window kernel. deconvolution reconstruct shapes input providing distinct features regular convolution. addition decrease redundant parameters implement separable convolution figure uniﬁed framework method. simultaneously adapts feature representations source label prediction sampling ratio target domain gradually increased training. visualize activations convolution deconvolution conv-m module network figure appearance details extracted convolution deconvolution tends describe completed shapes. therefore features extracted convolution deconvolution complementary beneﬁt addition shapes captured deconvolution generic class object compared appearance details extracted convolution facilitates strategy explore divergence classes knowledge transfer. input feature maps previous layer respectively processed regular convolution dilated convolution deconvolution three branches. outputs concatenated together. pipelines three branches c-c-c-dropout c-dic-dic-dropout c-dec-dec-dropout. three branches start convolution linear projection. parameters kernel size stride. dilation factor indicates receptive ﬁeld group number separable convolution indicates feature maps adjacent layers separated groups. dropout ratio ﬁxed output deconvolution cropped input size. relu adopted nine convolutions shown figure parameter number architecture shown table generally consists convolution alternating max-pooling conv-m avg-pooling linear listed second column types/module. note last linear layer image classiﬁcation removed conducting tasks. fairly compare methods section include layer estimation total parameters shown table. output size third column multiplication height width number feature maps layer. speciﬁc parameters conv-m layer listed fourth column filter size/stride conv-m ﬁfth column feature maps basic settings conv-m represented figure ﬁfth column shows feature number nine convolutions dec. nine convolutions feature numbers max-pooling layers same generally increased model depth. pixels input images processed regular convolution kernel size much larger kernels used conv-m. preliminary experiment shows input image data convolution smaller kernel degrade classiﬁcation accuracy .%∼.%. conv-m hand using larger kernels improve performance slightly .%∼.%. ﬁnal column parameters table lists parameter numbers layer. dominant parameter consumers conv-m modules fourth maxpooling avg-pooling. total number parameters given input data sampled source target domains. sampling ratio target domain gradually increased training. formally three terms minimized uniﬁed framework reconstruction error source target samples invariance learning discrepancy hidden representations layers between domains prediction error source labels shown table last linear layer neurons removed tasks. extra layers shown orange blue figure added domain alignment training layers related label prediction kept testing. invariance learning. error minimization reconstructing input source target samples force learn cross-domain features. asymmetrical encoder-decoder architecture adopted sample reconstruction shown figure encoder pretrained without avg-pooling last linear layers decoder fewer layers consists alternating un-pooling regular convolution. un-pooling decoder up-sample input feature maps using indexes obtained corresponding max-pooling layer encoder. encoder responsible feature extraction decoder restoring resolution. preliminary experiment shows asymmetrical structure slightly decreases ﬁnal accuracy signiﬁcantly accelerates training speed compared symmetrical design. addition decoders different scales introduced. representation discrepancy reduction. instead using parametric criteria kullback-leibler divergence reduce cross-domain divergence adopt non-parametric method estimate feature distribution distance domains. speciﬁcally minimize maximum mean discrepancies gretton deﬁned respectively input source target denote corresponding sample numbers. function non-linear feature mapping. universal reproducing kernel hilbert space. criteria denoted g-mmd method adopt gaussian kernel. shown figure g-mmd loss added last three conv-m layers dnn. source label prediction. shown figure linear layers neuron numbers second speciﬁed dataset. signiﬁcant accuracy beneﬁt observed adding linear layers preliminary experiment. trained benchmark dataset imagenet’ compared well-known models total parameter numbers classiﬁcation accuracy. following standard pipeline ﬁne-tune trained model unsupervised tasks popular datasets according method. accuracy compared competitive methods. train imagenet’ dataset parameters training solver according quick solver.prototxt caffe batch size table compares classiﬁcation accuracy parameter numbers alexnet googlenet alexnet googlenet directly trained models provided caffe. vgg’s result obtained original paper achieves googlenetlevel accuracy total parameter numbers googlenet. ofﬁce-. standard benchmark consists images categories collected three distinct domains amazon webcam dslr samples three domains respectively downloaded amazon.com taken camera taken digital camera ofﬁce environment different photographic settings. tasks three domains adopted completeness d→a. ofﬁce-caltech. popular dataset composed overlapping categories ofﬁce- caltech datasets. twelve tasks used c→d. ofﬁce- dataset challenging categories images ofﬁce-caltech provides tasks observe dataset bias networks. five dnns used experiments alexnet rev-faconvnet googlenet faconvnet methods transduction originally pre-trained alexnet according papers. revfaconvnet achieves much better accuracy compared squeezenet rev-squeezenet faconvnet shown preliminary experiments table faconvnet revfaconvnet reach googlenet-level classiﬁcation accuracy. work googlenet faconvnet baselines comparison. experiments. besides running previous methods alexnet also following eight experiments quantize contribution method running revfaconvnet; running dnn; running revfaconvnet; running dnn; running method rev-faconvnet; running method faconvnet result used baseline; running method googlenet result used baseline; running method ﬁnal result. parameter settings. follow speciﬁc description previous methods papers. hyperparameter selected based cross-validation consistent papers method based pre-trained network imagenet’ convolution ﬁrst three conv-m shown table frozen ofﬁce- ofﬁce-caltech datasets rather small-scale. newly added layers shown orange blue figure trained scratch learning rate times higher. learning rate policy adopt poly described caffe initial value power ﬁxed batch size sampling ratio target domains uniformly increased training. testing stage layers sample reconstruction removed aforementioned section remaining layers label prediction figure neuron numbers ﬁrst linear layer second ofﬁce- dataset ofﬁce-caltech dataset. g-mmd loss added last three conv-m layers dnn. regularization dlid transduction baseline baseline methods remove last linear layer pre-trained network extra layers according section smaller change. size models also slightly different actual size reported hence directly report total parameter numbers pre-trained network fair comparison. based nvidia titan inference speed squeezenet rev-squeezenet faster faconvnet rev-faconvnet network though cannot obtain googlenet-level classiﬁcation speciﬁcally rev-squeezenet slower squeezenet rev-faconvnet decreases speed faconvnet network consumes less time compared faconvnet. table table respectively summarize accuracy ofﬁce- ofﬁce-caltech datasets. tables separated four groups rows. ﬁrst group previous methods based alexnet. second group compares previous methods revfaconvnet third group compares methods dnn. fourth group provides result first approaches googlenet’s accuracy method googlenet previous compact dnns much larger according four observations table table though faconvnet rev-faconvnet obtain googlenet-level classiﬁcation accuracy matched accuracy classiﬁcation moreover smaller revfaconvnet also outperforms alexnet using method comparison table shows. third together method based achieves state-of-the-art sixteen total eighteen tasks datasets shown last tables table table boost accuracy task compared transduction shown table ofﬁce- dataset accuracy tasks greatly increases indicating larger appearance difference domains domain difference also larger words ofﬁce- dataset transfer relatively easier method difﬁcult consistent results previous methods. ofﬁce-caltech dataset bilateral transfer gets largest accuracy method shown table convolution conv-m. validate contribution non-regular convolution conv-m module replace nonregular convolution regular ones keep kernel size unchanged. ﬁrst table shows result second original solution. signiﬁcant accuracy drop observed classiﬁcation almost tasks. comparison table indicates importance features extracted dilated convolution improved deconvolution conv-m. reconstrution g-mmd. based table table respectively show contribution components methods ofﬁce- ofﬁce-caltech datasets. g-mmd tables shows result obtained removing g-mmd method recons. corresponds method without including sample reconstruction. rows lower accuracy indicates contribution component. regular result without removing component respective table table ofﬁce- dataset shown table reconstruction important transfers rely g-mmd. table demonstrates contributions reconstruction g-mmd almost same. paper present compact architecture unsupervised method based observation current small dnns unmatched accuracy classiﬁcation e.g. googlenet-level classiﬁcation accuracy obtains alexnet-level accuracy. basic module used conv-m introduces multi-scale convolution deconvolution without using kernels larger uniﬁed framework method learns crossdomain features sample reconstruction g-mmd simultaneously tunes label prediction. parameter numbers googlenet experiments show obtains googlenet-level accuracy classiﬁcation method slightly outperforms previous competitive addition method based achieves state-of-the-art sixteen total eighteen tasks popular ofﬁce- ofﬁce-caltech datasets. acknowledgments. work part supported ccf- opinions ﬁndings conclusions recommendations expressed material authors necessarily reﬂect views grant agencies contractors.", "year": 2017}