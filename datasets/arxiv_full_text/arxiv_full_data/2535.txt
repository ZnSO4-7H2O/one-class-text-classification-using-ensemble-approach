{"title": "A Novel Regularized Principal Graph Learning Framework on Explicit Graph  Representation", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Many scientific datasets are of high dimension, and the analysis usually requires visual manipulation by retaining the most important structures of data. Principal curve is a widely used approach for this purpose. However, many existing methods work only for data with structures that are not self-intersected, which is quite restrictive for real applications. A few methods can overcome the above problem, but they either require complicated human-made rules for a specific task with lack of convergence guarantee and adaption flexibility to different tasks, or cannot obtain explicit structures of data. To address these issues, we develop a new regularized principal graph learning framework that captures the local information of the underlying graph structure based on reversed graph embedding. As showcases, models that can learn a spanning tree or a weighted undirected $\\ell_1$ graph are proposed, and a new learning algorithm is developed that learns a set of principal points and a graph structure from data, simultaneously. The new algorithm is simple with guaranteed convergence. We then extend the proposed framework to deal with large-scale data. Experimental results on various synthetic and six real world datasets show that the proposed method compares favorably with baselines and can uncover the underlying structure correctly.", "text": "abstract. many scientiﬁc datasets high dimension analysis usually requires visual manipulation retaining important structures data. principal curve widely used approach purpose. however many existing methods work data structures self-intersected quite restrictive real applications. methods overcome problem either require complicated human-made rules speciﬁc task lack convergence guarantee adaption ﬂexibility different tasks cannot obtain explicit structures data. address issues develop regularized principal graph learning framework captures local information underlying graph structure based reversed graph embedding. showcases models learn spanning tree weighted undirected graph proposed learning algorithm developed learns principal points graph structure data simultaneously. algorithm simple guaranteed convergence. extend proposed framework deal largescale data. experimental results various synthetic real world datasets show proposed method compares favorably baselines uncover underlying structure correctly. many ﬁelds science often encounters observations represented high-dimensional vectors sampled unknown distributions. sometimes difﬁcult directly analyze data original space desirable perform data dimensionality reduction associate data structured objects exploratory analysis. example study human cancer dynamic disease develops extended time period accumulation series genetic alterations delineation dynamic process would provide critical insights molecular mechanisms underlying disease process inform development diagnostics prognostics targeted therapeutics. recently developed high-throughput genomics technology made possible measure expression levels genes tissues thousands tumor samples simultaneously. however delineation cancer progression path embedded high-dimensional genomics space remains challenging problem principal component analysis commonly used methods visualize data low-dimensional space linear assumption limits general applications. several nonlinear approaches based kernel trick proposed e-mail maoqgmail.com wang department mathematics statistics computer science university illinois chicago chicago usa. e-mail liwanguic.edu i.w. tsang centre quantum computation intelligent systems university technology sydney australia. buffalo usa.. remain sub-optimal detecting complex structures. alternatively data dimensionality high manifold learning based local information data effective. examples include locally linear embedding laplacian eigenmaps however methods generally require construct carefully tuned neighborhood graph performance heavily depends quality constructed graphs. another approach principal curve initially proposed nonlinear generalization ﬁrst principal component line informally principal curve inﬁnitely differentiable curve ﬁnite length passes middle data. several principal curve approaches proposed including minimize certain types risk functions quantization error negative log-likelihood function overcome over-ﬁtting issue regularization generally required. k´egl bounded total length principal curve proved principal curve bounded length always exists data distribution ﬁnite second moment. similar results obtained bounding turns principal curve recently elastic maps approach proposed regularize elastic energy membrane. alternative deﬁnition principal curve based mixture model considered model parameters learned maximum likelihood estimation regularization achieved using smoothness coordinate functions. generative topographic mapping proposed maximize posterior probability data generated low-dimensional discrete grid mapped original space corrupted additive gaussian noise. provides principled alternative self-organizing impossible deﬁne optimality criterion methods learning principal curve widely studied generally limited learn structure intersect methods handle complex principal objects. k´egl krzyzak extended polygonal line method skeletonization handwritten digits. principal manifold approach extends elastic maps approach learn graph structure generated graph grammar. major drawback methods require either predeﬁned rules grammars many parameters difﬁcult tuned makes implementations complicated adaptations datasets difﬁcult. importantly convergences guaranteed. recently subspace constrained mean shift method proposed obtain principal points given second-order differentiable density function still trivial transform principal points explicit structure. many real applications explicit structure uncovered given data helpful downstream analysis. critical point illustrated section section detail various real world datasets. another line research relevant work structure learning made great success constructing learning explicit structures data. graph structures commonly used graph-based clustering semi-supervised learning knearest neighbor graph \u0001-neighborhood graph dramatic inﬂuences graphs clustering techniques studied since \u0001-neighborhood graph could result disconnected components subgraphs dataset even isolated singleton vertices b-matching method applied learn better b-nearest neighbor graph loopy belief propagation however improper ﬁxed neighborhood size since curvature manifold density data points different different regions manifold order alleviate issues method simultaneous clustering embedding data lying multiple manifolds proposed using norm errors measure linear representation every data point using neighborhood information. similarly graph learned image analysis using norm errors enhancing robustness learned graph instead learning directed graphs using methods integrated model undirected graph imposing sparsity penalty symmetric similarity matrix positive semi-deﬁnite constraint laplacian matrix proposed although methods demonstrated effectiveness various problems challenging apply moderate-size data alone large-scale data high computational complexity. moreover might suboptimal heuristically transforming directed graph undirected graph clustering dimensionality reduction figure real world problems exhibiting variety graph structures. rotation teapot images forming circle. optical character templates different digits containing loops bifurcations multiple disconnected components. branching architecture cancer evolution. selective pressures allow tumor clones expand others become extinct remain dormant. paper extension preliminary work demonstrated effectiveness learning minimum-cost spanning tree showcase datasets tree structures certain applications. move forward take account complicated structures might exist real world datasets. moreover propose strategies speciﬁcally overcome issue high-complexity proposed method large-scale data. main contributions paper summarized follows propose novel regularized principal graph learning framework addresses aforementioned limitations learning principal points explicit graph structure data simultaneously. best knowledge ﬁrst work represent principal objects using explicit graph learned systematic guaranteed convergence practical large-scale data. novel formulation called reversed graph embedding representation principal graph facilitates learning graph structures generalizes several existing methods possesses many advantageous properties. addition spanning trees weighted undirected graph proposed modeling various types graph structures including curves bifurcations selfintersections loops multiple disconnected components. facilitates proposed learning framework datasets complicated graphs. propose strategies deal large-scale data. side-information priori reduce complexity graph learning learn representative graph landmarks. strategies simultaneously incorporated framework efﬁciently handling largescale data. extensive experiments conducted unveiling underlying graph structures variety datasets including various synthetic datasets real world applications consisting various structures hierarchy facial expression images progression path breast cancer microarray data rotation circle teapot images smoothing skeleton optical characters similarity patterns digits large-scale handwritten digits databases. rest paper organized follows. ﬁrst illustrate learning problem using various real world datasets section section present three building blocks representation principal graphs. based building blocks propose regularized principal graph learning framework section section incorporate strategies proposed framework deal largescale datasets. extensive experiments conducted section finally conclude work section many ﬁelds science experimental data resides high-dimensional space. however distance data points directly reﬂect distance measured intrinsic structure data. hence desirable uncover intrinsic structure data conducting analysis. demonstrated many high-dimensional datasets generated realworld problems contain special structures embedded intrinsic dimensional spaces. example collection teapot images viewed different angles image contains pixels pixel space dimensionality intrinsic structure degree freedom angle rotation. shown fig. distances computed following intrinsic circle meaningful distances computed original space. another example given demonstrated collection facial expression images contains two-layer hierarchical structure images three facial expressions subject grouped together form ﬁrst layer images three subjects form second layer. words images subject distant images subjects images three expressions subject close. complicated structures including loops bifurcations multiple disconnected components shown fig. observed optical character templates identifying smoothing skeleton character examples speciﬁc intrinsic structures also discussed particularly interested studying human cancer dynamic disease develops extended time period. initiated normal cell advance malignancy extent considered darwinian process multistep evolutionary process responds selective pressure disease progresses series clonal expansions result tumor persistence growth ultimately ability invade surrounding tissues metastasize distant organs. shown fig. evolution trajectories inherent cancer progression complex branching obvious necessity timely treatment typically feasible collect time series data study human cancer progression however massive molecular proﬁle data excised tumor tissues accumulates becomes possible design integrative computation analyses approximate disease progression provide insights molecular mechanisms cancer. previously shown indeed possible derive evolutionary trajectories static molecular data breast cancer progression represented high-dimensional manifold multiple branches concrete examples convince many datasets high-dimensional space represented certain intrinsic structure dimensional space. existing methods exploit intrinsic structure data implicitly either learning parametric mapping function approximating manifold local geometric information observed data. however methods directly learn explicit form intrinsic structure low-dimensional space. contrast proposed method designed purpose express intrinsic structure data explicit graph representation discussed following sections. input space observed data points. consider learning projection function latent points d-dimensional space trustfully represent functions deﬁned graph function maps weighted undirected graph vertices edges rk×k edge weight matrix element denoted weight edge suppose every vertex one-to-one correspondence latent point resides manifold intrinsic dimension next introduce three components form building blocks proposed framework. reversed graph embedding. important component proposed framework model relationship graph latent points given weight measures similarity latent points intrinsic space since corresponding latent points generally unknown advance coin graph-based representation namely reversed graph embedding order bridge connection graph structure corresponding data points input space. speciﬁcally intuition latent variables neighbors high similarity corresponding points input space close another. capture intuition propose minimize following objective function explicit representation principal graphs given variables optimized given moment cannot obtain interesting solutions directly solving problem respect multiple trivial solutions leading zero objective value. however objective function possesses many interesting properties used graph-based regularization unveiling intrinsic structure given data detailed below. relationship laplacian eigenmap. objective function interpreted reverse perspective well-known laplacian eigenmap denote rd×n laplacian eigenmap solves following optimization problem similarity computed input space order capture locality information manifold modeled neighbors input data points e.g. using heat kernel bandwidth parameter neighbor also neighbor otherwise vij]) rn×n diagonal matrix. euclidean distance i.e. ||yi−yj|| computed latent space consequently multiplication terms objective functions reverse graph embedding laplacian eigenmap calculated different space weights computed respectively; distances ||hg hg)|| ||yi yj|| computed respectively. formulation directly model intrinsic structure data laplacian eigenmap approximates intrinsic structure using neighborhood information observed data points. words proposed reversed graph embedding representation facilitates learning graph structure data. weight encodes similarity connectivity vertices example means edge absent cases dataset given graph unknown. cases necessary automatically learn data. objective function linear respect weight matrix linearity property facilitates learning graph structure. discuss different representations graphs based linearity property section another important difference number latent variables necessary equal number input data points i.e. useful obtain representative graph landmark points compressing large amount input data points representative graph still trustfully represent whole data. explore property large-scale principal graph learning section harmonic function general graph. discussed properties reversed graph embedding treating variables single integrated variable hg∀k. actually optimal function obtained solving related harmonic pluriharmonic functions. illustrated following observations. neighbors point yk∀k. given problem rewritten equalities hold function harmoinc function since value nonterminal vertex mean values closest neighbors vertex plurihamonic graphs deﬁned impose penalty subset k-stars |nm| k∀m. contrast formulation allows ﬂexibly incorporate neighborhood structure existing connection harmonic pluriharmonic functions enriches target function previously discussed extension principal curves general graphs. equation generalizes various existing methods. worth noting quantity considered length principal graph terms square norm. case linear chain structure length polygonal line deﬁned however general graphs trees ﬂexible principal curves since graph structure allows self-intersection. principal graph learning elastic also deﬁnes penalty based given graph. however based discussion difﬁcult solve problem respect function graph weights within elastic-maps framework. contrast proposed reversed graph embedding leads simple efﬁcient algorithm learn principal tree weighted undirected graph guaranteed convergence. clariﬁed section data grouping. second important component proposed framework measure ﬁtness latent variables given data terms given graph projection function number latent variables necessarily equal number input data points assume projected point centroid cluster input data points high similarity form cluster. empirical quantization error widely used ﬁtting criterion minimized optimal cluster centroids also frequently employed principal curve learning methods given formulation interpreted negative log-likelihood data i.i.d drawn multivariate normal distribution mean covariance matrix k-means introduce indicator matrix }n×k element assigned cluster centroid otherwise. consequently following equivalent optimization problem variant equivalent representation indicator matrix integer solution obtained. relatively large k-means minimizes might generate many empty clusters. avoid issue introduce soft assignment strategy adding negative entropy regularization paper since generalized version cases also close relationship mean shift clustering method gaussian mixture model k-means method. below discuss interesting properties detail. proofs proposition proposition given appendix. proof. mean shift clustering non-parametric model locating modes density function. modes ﬁnd. kernel density estimator used estimate density function given ﬁxed objective function equivalent maximum likelihood function i.i.d. drawn mixture gaussian distributions consisting discrete uniform components gaussian distribution distribution component i.e. minimizing equivalent minimizing minimizing equivalent minimizing according aforementioned results brieﬂy summarize merits function first empty clusters never created according proposition second loss function takes density input data account. words centroids obtained minimizing loss function reside high density region data. third loss function makes learning graph structures computationally feasible large-scale data case discussed section latent sparse graph learning. third important component proposed framework learn latent graph data. achieve goal investigate special graph structures learned data formulating learning problems linear programming. tree structure represented minimum-cost spanning tree weighted undirected graph assumed sparse. dataset. task construct weighted undirected graph cost associated edge optimizing similarity matrix rk×k based assumption speciﬁc graph structure. minimum cost spanning tree. tree minimum total cost edges forming tree. order represent learn tree structure consider binary indicator matrix otherwise. integer linear programming formulation minimum spanning tree wkkφkk }n×n}∩w written minw∈w ﬁrst constraint enforces symmetric connection undirected graph e.g. wkk. second constraint states spanning tree contains edges. third constraint imposes acyclicity connectivity properties tree. difﬁcult solve integer programming problem directly. thus resort relaxed problem letting weighted undirected graph. complete graph imposed sparse constraints edge weights called graph considered learning sparse graph. graph motivated intuition data point usually represented linear combination data points similar data point. words data points similar edge weight positive value treated similarity measurement data points. consequently coefﬁcient linear combination coincident edge weight graph. system linear wkkzk∀k vector approximated edge weight matrix overcomplete dictionary bases. practice exist noises certain elements natural estimate edge weights tolerating errors. introduce error linear equation k=k=k wkkzk ξk∀k. order learn similarity measurement undirected graph impose nonnegative symmetric constraints i.e. wt}. generally sparse solution robust facilitates consequent identiﬁcation test sample following recent results sparse representation problem convex -norm minimization effectively recover sparse solution rd×k. propose learn solving following linear programming problem figure cartoon illustrating proposed formulation teapot images. circle marker represents teapot image. assumption data generation process graph exists latent space image mapped point input space maintaining graph structure reversed graph embedding ﬁnally image observed conditioned according certain noise model. problem different methods learn diξk graphs different method learns undirected graphs using probabilistic model gaussian markov random ﬁelds. moreover data points different data points used compute costs. generalized graph structure learning. ease representation uniﬁed formulation learning similarity graph given dataset similarity matrix graph given dataset cost matrix entry φkk. feasible space denoted speciﬁcally solve following generalized graph structure learning problem graph learning problem formulated linear programming constraints represented used following proposed framework given combining three building blocks discussed section ready propose uniﬁed framework learning principal graph. alternate convex search algorithm solve proposed formulations simultaneously learning principal points undirected graph guaranteed convergence. proposed formulation. given input dataset uncover underlying graph structure generates since input data drawn noise model improper learn directly unveil hidden structure assume underlying graph recovered learning model represented form e.g. graph undirected weighted graph speciﬁc structures; graph satisﬁes reversed graph embedding assumption cost deﬁned ||hg hg|| vertices close high similarity points close other; data points {hg}n considered denoised points movement denoised point measured data grouping properly. clarity illustrate relations various variables fig. based parameter. worth noting component learning graph given also contains reversed graph embedding objective crucially important element special graph learning formulations addition measure noise input data euclidean distance input data point denoised counterpart. instead manually setting parameters \u0001i∀i assume total noise input data points minimal. optimization problem reformulated penalize total noise tradeoff parameter graph-based regularizer total noises input data. rewrite problem following optimization problem second term objective function equivalent discussed section loss function ﬂexible consequently propose regularized principal graph learning framework simultaneously optimizing graph weight matrix latent variables projection function soft-assignment matrix optimization algorithm. focus learning underlying graph structure data. instead learning separately optimize joint learn deﬁne properly e.g. linear projection function might possible studied case learning feasible space objective function similar removing second term objective function. simplicity explicitly show formulation learning spanning tree. problem bi-convex optimization problem ﬁxed optimizing convex optimization problem; ﬁxed optimizing also convex. propose solve problem using alternate convex search minimization method solve biconvex problem variable divided disjoint blocks blocks variables deﬁned convex subproblems solved cyclically optimizing variables block ﬁxing variables blocks. convex subproblem solved efﬁciently using convex minimization method. below discuss subproblem details pseudo-code proposed method given algorithm solve given problem respect solved independently. decoupled variables rows solve independently. according proposition analytic solution given discussed section problem solved kruskal’s algorithm. problem linear programming problem solved efﬁciently off-the-shelf linear programming solver smallmoderate-sized datasets mosek convergence complexity analysis. since problem non-convex exist many local optimal solutions. following initialization strategy mean shift clustering initialize original input data shown algorithm theoretical convergence analysis algorithm presented following theorem. theorem solution problem iteration corresponding objective function value have sequence monotonically decreasing. furthermore function lower-bounded monotone convergence theorem exists −γσn converges next prove sequence generated algorithm also converges. compactness feasible sets sequence converges since converges xp∗− diag diag. according theorem deﬁne stopping criterion algorithm terms relative increase function value compared last iteration experiments. empirical convergence results shown section time complexity algorithm learning tree structure determined three individual parts. ﬁrst part complexity running kruskal’s algorithm construct minimum spanning tree. requires computing fully connected graph ﬁnding spanning tree. second part dominated computing soft assignments samples complexity third part dominated inverse matrix size takes operations matrix multiplication takes operations. therefore total complexity iteration learning undirected weighted graph solving problem difference complexity linear programming solved efﬁciently mosek section extend proposed algorithm dealing large-scale data. order reduce high computational complexity algorithm large-scale data propose incorporate strategies proposed model fast learning using landmarks side information. graph learning side information. instance-level constraints useful express priori knowledge instances grouped together successfully applied many learning tasks clustering metric learning kernel learning area clustering prevalent form advice conjunctions pairwise instance level-constraints form mustlink cannot-link state pairs instances different clusters respectively given points cluster constraints clustering constraints constraints improve clustering results. consider certain structure-speciﬁc side information guidance learning similarity matrix computational reduction instead optimizing full matrix. purpose take account constraints. data points might linked data point contrary data points belong set. incorporating side information proposed framework come following optimization problem learning graph representation given landmark-based regularized principal graph. order handle large-scale data extend proposed regularized principal graph learning framework using landmarkbased technique. landmarks widely used clustering methods deal large-scale data random landmark selection method widely used spectral clustering well known selecting landmarks improve clustering performance certain adaptive strategy k-means variety ﬁxed adaptive sampling schemes family ensemble-based sampling algorithms regularized principal graph learning framework inherently take landmark data points account data grouping. taking weighted undirected graph based framework example optimization problem considering landmarks given landmarks size moreover proposed methods automatically adjust centroids landmarks optimization procedure. non-convexity objective function properly initialize landmarks. shown literature k-means generally obtain better clustering performance spectral clustering methods. paper follow idea centroids obtained k-means initialize pseudo-code shown algorithm following analysis procedure section computational complexity algorithm signiﬁcantly smaller algorithm hence algorithm practical large-scale data large section conduct extensive experiments evaluate proposed models learning either spanning trees weighted undirected graphs various synthetic datasets real world applications. convergence sensitivity analysis. perform convergence analysis algorithm using synthetic tree dataset. fig. shows empirical convergence results obtained learning different graph structures well intermediate results. panel fig. shows empirical convergence results illustrating relative difference objective function value terms number iterations algorithm observe proposed algorithm converges less iterations. consistent result theoretical analysis section bottom panel fig. algorithm continues iterations tree perform parameter sensitivity analysis using distorted s-shape dataset demonstrate algorithm behaves respect parameters appears algorithm learning graph investigate sensitivity parameters terms learning graph. results obtained learning graph similarly applied algorithm learning spanning tree. fig. shows graphs constructed varying parameter ﬁxing others. following observations. first according fig. clear smaller shorter length s-shape curve contrast larger faithful curve passes middle data. therefore important parameter controls trade-off curve ﬁtting error length principal graph. second shown fig. graph structure becomes smoother increasing also stated propositions explore relationships proposed formulation mean-shift clustering. words large encourages data points merge together. data movement represented variable matrix algorithm also restricted graph length graph minimized data points smoother reach goal. choice bandwidth parameter algorithm similar mean-shift clustering. suggested best explore range bandwidths instead estimating data minimizing loss function heuristic rules since clustering nature exploratory learning structure paper. third larger edges learned graph contains shown figs. reason noise term becomes large less data points required represent current data point large value. exploratory nature learning graph structure data lack evaluation metrics graph structure learning unsupervised setting automation setting parameters using provided data generally difﬁcult. however aforementioned parameter sensitivity analysis suggests pragmatic tune parameters learning expected graph structure. recommended procedure given follows ﬁrst setting small value tuning large value small data properly ﬁnally increasing obtain reasonable structure. order learn graph data initial graph structure constructed solving problem using proper weight matrix captures structure input data. then recommended procedure tuning applied. tuning automatically data studied previous work using leave-one-out-maximum likelihood criterion described statistics respectively. discussed natural tune parameter different range. following experiments take tuning strategy setting parameters large range parameters used reported reproducibility experiments. synthetic data. evaluate performance algorithm learning either spanning tree graph comparing polygonal line method scms synthetic datasets. among them ﬁrst datasets also used case learning graph incorporate neighbors data point side information i.e. data points neighborhood data point considered cannot-links data point. paper take nn-nearest neighbor showcase side-information. experiments conducted settings. ﬁrst setting evaluate four methods learning curves second setting investigate structures datasets including loops self-intersection disconnected components. experiments section parameters compared methods shown table worth noting tuning parameters proposed method relatively easier tuning scms needs ﬁner tunning order achieve comparable results. ﬁrst rows fig. show results principal curve learning setting. following observations proposed methods obtain smoothing curves tested methods. polygonal line method fails spiral data. also scms cannot obtain curve structure many projected points ordering information points scattered shown spiral data. leaves non-trivial problem learn underlying structure using scms. proposed method problems. following experiments demonstrate importance learning explicit structure various real-world applications. last four rows fig. show results obtained datasets containing loop self-intersection disconnected components. polygonal line method fails four datasets improper assumption principal curves. figure results parameter sensitivity analysis algorithm learning graph performed distorted s-shape dataset varying different parameters varying varying varying table parameters used proposed algorithm synthetic datasets. proposed method learning graph spanning tree share parameters scms tuned ﬁne-grid achieve reasonable principal points. point tree structure naturally used learning tree structure proposed method learning tree structure outperforms proposed learning graph shown results tree data. however latter outperforms former datasets multiple disconnected components loops. results second setting consistent obtained ﬁrst setting. results suggest methods obtain results least good baselines properly tuned parameters using recommended strategy outperform polygonal line method fail datasets; methods handle complicated data structures polygonal line method provide graph structures cannot easily obtained scms. figure results four principal curve methods performed synthetic datasets containing various situations including curves loops self-intersections multiple disconnected components. ﬁrst second columns show results proposed methods learning either graph spanning tree. third forth columns report results generated polygonal line method scms respectively. rotation teapot images. collection teapot images used. images taken successively teapot rotated goal construct principal curve organizes images. image consists pixels represented vector. data dimension normalized zero mean unit standard deviation. similar kernel matrix generated exp. proposed method using kernel matrix input. ﬁrst perform kernel matrix project -dimensional space keeps total energy. experimental results proposed method learning spanning tree graph shown fig. principal curves shown terms ﬁrst columns learned projection matrix coordinates represents image sampled images intervals plotted purpose visualization. fig. show linear chain dependency among teapot images following consecutive rotation process. curve generated method agreement rotating process consecutive teapot images. data graph reasonable spanning tree since underlying manifold forms loop. clearly demonstrated fig. similar result also recovered cluhsic assumes label kernel matrix ring structure however main differences. first principal curves generated methods much smoother obtained cluhsic second method learns adjacency matrix given dataset cluhsic requires label matrix priori. two-dimensional representation teapot images given maximum variance unfolding used arranges images circle attempted keeping energy i.e. however storage allocation fails large memory requirement solving semi-deﬁnite programming problem mvu. hence cannot applied learn relatively large intrinsic dimensionality. however method suffer issue. hierarchical tree facial images. facial expression data used hierarchical clustering takes account identities individuals emotion expressed data contains face images three types facial expressions taken three subjects alternating order around repetitions each. eyes facial images aligned average pixel intensities adjusted. teapot data image represented vector normalized dimension zero mean unit standard deviation. kernel matrix used input algorithm learning spanning tree setting experimental results shown fig. clearly three subjects connected different branches tree. take black circle fig. root hierarchy tree forms two-level hierarchical structure. shown fig. three facial expressions three subjects also clearly separated. similar two-level hierarchy also recovered cluhsic however advantages using proposed method discussed teapot images also applied here. addition observe figure experimental results proposed method applied teapot images. principal circle generated proposed method learning graph. represents teapot image. images following principal curve plotted intervals visualization. adjacency matrix circle follows ordering consecutive teapot images rotation. principle curve adjacency matrix obtained proposed method learning spanning tree. detailed information tree structure. example leso junction subjects i.e. arso chso observed adjacency matrix shown fig. observation suggests shock similar facial expression among three subjects. however cluhsic able obtain information. breast cancer progression path. section demonstrate utility proposed method unveiling cancer progression path microarray samples. interrogated large-scale publicly available breast cancer dataset dataset contains expression levels gene transcripts data gene copy number genes. molecular data obtained normal breast tissue samples tumor samples survival data patients figure experimental results proposed method learning spanning tree performed facial expression images. generated hierarchical tree. represents face image. images three types facial expressions three subjects plotted visualization. black circle considered root hierarchical structure achieving layers hierarchy nine subjects; adjacency matrix tree nine blocks indicates block corresponds facial expression subject. disease available. total genes associated cancer progression selected using non-linear regression method clustering analysis identiﬁed genetically homogeneous tumor groups visually demonstrate cancer progression path learned principal points projected three-dimensional space ﬁrst three principal components data tree structure retains learned highdimensional space. polygonal line method work well data report result. apply proposed method visualize underlying data structure case represents progression path breast cancer towards malignancy revealed data manifold indicates linear bifurcating progression path breast cancer progression conﬁrming previous studies solely assist visualization color-coded tumor samples according molecular subtype system system categorizes tumor tissue samples normal-like luminal luminal her+ basal subtypes based expression gene transcripts. used normal breast tissue samples dataset baseline. evident linear progression path tracks logically normal tissue samples luminal subtypes splits either her+ tumor samples basal tumor samples. latter subtypes known aggressive breast tumor types. data human tissue molecular proﬁling accumulates advanced computational analyses provide insights disease progression become increasingly important. implementation proposed method facilitate derivation interactive progression models enable identiﬁcation molecular events associated carcinogenesis malignancy. demonstrate importance learning explicit tree structure extract longest path traversing learned tree figure progression path breast cancer data. tree structure learned proposed method learning spanning tree; principal points learned scms; ﬁtted curves gene expression gene respect samples path normal basel using quadratic function; ﬁtted curves gene expression gene respect samples path normal her+ using quadratic function. leaf node labeled normal sample leaf node labeled either basal her+ respectively. path normal basal consists samples path normal her+ consists samples. important genes used subtypes plot variation gene expression terms samples given path. fig. show results selected genes different paths demonstrate different trend gene expression variation. genes might causes cancer progression evolved luminal either basal her+. example expression level gene actrb increases suddenly patient might cancer state basal. however principal points returned scms shown fig. clear progression structure cannot obtain similar implications proposed method. figure results proposed rpg- ﬁnding smoothing skeletons optical characters learned smoothing skeletons optical characters different writing styles; learned smoothing skeletons optical characters noises; adjusted smoothing skeletons optical characters enlarging properly. skeletons optical characters. proposed algorithm learning graph smooth skeletons handwritten character templates used recover trajectory penstroke. optical characters generally contains loops self intersections bifurcation points. principal curves applied similar purposes kegl extends polygonal line algorithm handle loops self intersections modifying table rules adding preprocessing postprocessing steps. however table rules specially designed self intersections difﬁcult extend data sets dimensionality larger two. ozertem proposed kde-scms give satisfactory results without rule model based special treatment self intersections. kde-scms obtain figure experiments large-scale datasets. adjacency matrices centroids reshufﬂe according labels pendigits mnist respectively. learned tree structure pendigits centroids label colored visualized ﬁrst three principal components. bunch principal points describe skeletons handwritten characters explicit structures templates. proposed method learning graph effectively handle loops self intersections disconnected components moreover require rules simultaneously return smooth skeletons templates form graphs. handwritten digits data provided kegl downloaded website dataset reported following preprocessing work ﬁrst transform black-and-white character templates two-dimensional data sets unit length coordinate system width height pixel pixel integer coordinates. then pixel value data set. finally scale data square area unit width height. proposed method learning weighted undirected graph parameters handwritten characters fig. shows results smooth skeletons different character templates various written styles. skeletons correctly identiﬁed shown fig. also observed shown fig. noises characters also tolerated distinguished skeleton proposed method. small percentage characters achieve smoothing effect shown fig. problem effectively solved increasing shown last fig. observations imply proposed method learning graph effectively deal loops bifurcations self-intersections multiple disconnected components. hence powerful tool various problems scientiﬁc exploratory research. experiments large-scale datasets. tested proposed algorithm large-scale datasets pendigits mnist. pendigits database handwritten digit dataset samples writers consists total images represented features using sampled coordination information. mnist database handwritten digits total samples. digits size-normalized https//www.lri.fr/∼kegl/researchudem/research/pcurves/implementations/skeletonizationtemplates/ https//www.csie.ntu.edu.tw/∼cjlin/libsvmtools/datasets/ centered ﬁxed-size image. image size represented features. scale gray pixel dividing handwritten digit datasets labels order visualize learned graph apply keep total energy obtain pendigits mnist respectively. rescale projected low-dimensional sample dividing maximum absolute value dimension independently. test algorithm learning tree structure. parameters i.e. ﬁxed datasets. moreover apply k-means method obtain centroids centroids initialize algorithm label centroid label using majority voting method labels data points assigned cluster. fig. shows results obtained proposed method datasets. learned adjacency matrix reported fig. pendigits mnist respectively data points reshufﬂed according labels implemented method matlab empirical times spent learning seconds pendigits mnist respectively. following observations first learned adjacency matrices demonstrate good shape diagonal matrix. words data points labels sticked together useful clustering. addition explicit tree structure shown fig. drawn ﬁrst three principal components demonstrate relations among different labels. fig. clear path started digits passed ﬁnally interesting since digits similar bottom half images digits similar terms half images. path tree represents certain types manifold digits. furthermore proposed algorithm used exploratory analysis large amount data points. paper proposed simple principal graph learning framework used obtain principal points graph structure simultaneously. experimental results demonstrated effectiveness proposed method various real world datasets different graph structures. since principal graph model formulated general graph development principal graph methods speciﬁc structure also possible. future work explore principal graph learning graphs k-nearest neighbor graphs apply real-world datasets. erling andersen knud andersen. mosek interior point optimizer linear programming implementation homogeneous algorithm. high performance optimization pages springer ehsan elhamifar ren´e vidal. sparse manifold clustering embedding. nips erwin obermayer schulten. self-organizing maps ordering convergence properties greaves carlo maley. clonal evolution cancer. nature hastie stuetzle. principal curves. jasa jebara wang s.f. chang. graph construction b-matching semi-supervised learning. jolliffe. principal component analysis. springer-verlag berlin k´egl kryzak. piecewise linear skeletonization using principal curves. ieee t-pami ozertem erdogmus. locally deﬁned principal curves surfaces. jmlr j.s. parker mullins m.c. cheang supervised risk predictor breast cancer based intrinsic huang m.i. jordan. fast approximation spectral clustering. jinfeng zhuang ivor tsang steven hoi. family simple non-parametric kernel learning", "year": 2015}