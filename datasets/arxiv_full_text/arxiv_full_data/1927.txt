{"title": "Training and Evaluating Multimodal Word Embeddings with Large-scale Web  Annotated Images", "tag": ["cs.LG", "cs.CL", "cs.CV", "I.2.6; I.2.7; I.2.10"], "abstract": "In this paper, we focus on training and evaluating effective word embeddings with both text and visual information. More specifically, we introduce a large-scale dataset with 300 million sentences describing over 40 million images crawled and downloaded from publicly available Pins (i.e. an image with sentence descriptions uploaded by users) on Pinterest. This dataset is more than 200 times larger than MS COCO, the standard large-scale image dataset with sentence descriptions. In addition, we construct an evaluation dataset to directly assess the effectiveness of word embeddings in terms of finding semantically similar or related words and phrases. The word/phrase pairs in this evaluation dataset are collected from the click data with millions of users in an image search system, thus contain rich semantic relationships. Based on these datasets, we propose and compare several Recurrent Neural Networks (RNNs) based multimodal (text and image) models. Experiments show that our model benefits from incorporating the visual information into the word embeddings, and a weight sharing strategy is crucial for learning such multimodal embeddings. The project page is: http://www.stat.ucla.edu/~junhua.mao/multimodal_embedding.html", "text": "paper focus training evaluating effective word embeddings text visual information. speciﬁcally introduce large-scale dataset million sentences describing million images crawled downloaded publicly available pins pinterest dataset times larger coco standard large-scale image dataset sentence descriptions. addition construct evaluation dataset directly assess effectiveness word embeddings terms ﬁnding semantically similar related words phrases. word/phrase pairs evaluation dataset collected click data millions users image search system thus contain rich semantic relationships. based datasets propose compare several recurrent neural networks based multimodal models. experiments show model beneﬁts incorporating visual information word embeddings weight sharing strategy crucial learning multimodal embeddings. project page http//www. stat.ucla.edu/~junhua.mao/multimodal_embedding.html. word embeddings dense vector representations words semantic relational information. vector space semantically related similar words close other. large-scale training dataset billions words crucial train effective word embedding models. trained word embeddings useful various tasks real-world applications involve searching semantically similar related words phrases. large proportion state-of-the-art word embedding models trained pure text data only. since important functions language describe visual world argue effective word embeddings contain rich visual semantics. previous work shown visual information important training effective embedding models. however lack large training datasets scale pure text dataset models either trained relatively small datasets visual contraints applied limited number pre-deﬁned visual concepts therefore work fully explore potential visual information learning word embeddings. paper introduce large-scale dataset text descriptions images crawled collected pinterest largest database annotated images. pinterest users save images onto boards supply descriptions images. descriptions collected images saved commented users. compared coco dataset much larger scale standard pure text training datasets sample images descriptions challenge word embeddings learning directly evaluate quality model respect tasks state-ofthe-art neural language models often negative log-likelihood predicted words training loss always correlated effectiveness learned embedding. current evaluation datasets word similarity relatedness contain less thousand word pairs cannot comprehensively evaluate embeddings words appearing training set. challenge constructing large-scale evaluation datasets partly difﬁculty ﬁnding large number semantically similar related word/phrase pairs. paper utilize user click information collected pinterest’s image search system generate millions candidate word/phrase pairs. user click data somewhat noisy removed inaccurate entries dataset using crowdsourcing human annotations. ﬁnal gold standard evaluation dataset consists entries. equipped datasets propose train evaluate several recurrent neural network based models input text descriptions images. models directly minimize euclidean distance visual features word embeddings states similar previous work best performing model inspired recent image captioning models additional weight-sharing strategy originally proposed learn novel visual concepts. strategy imposes soft constraints visual features related words sentences. experiments validate effectiveness importance incorporating visual information learned word embeddings. make three major contributions firstly constructed large-scale multimodal dataset text descriptions images scale pure text training set. secondly collected labeled large-scale evaluation dataset word phrase similarity relatedness evaluation. finally proposed compared several based models learning multimodal word embeddings effectively. facilitate research area gradually release datasets proposed paper project page. related work image-sentence description datasets image descriptions datasets flickrk flickrk iapr-tc coco greatly facilitated development models language vision tasks image captioning. takes lots resources label images sentences descriptions scale datasets relatively small addition language used describe images datasets relatively simple imtext dataset proposed adopts similar data collection process using million images million user annotated captions flickr. scale still much smaller pinterestm dataset. recently proposed released yfccm dataset large-scale multimedia dataset contains metadata million flickr images. provides rich information images tags titles locations taken. users’ comments obtained querying flickr api. different functionality user groups flickr pinterest users’ comments flickr images quite different pinterest dataset provides complementary information pinterestm dataset. word similarity-relatedness evaluation standard benchmarks wordsim-/wssim simlex- consist couple hundreds word pairs similarity relatedness scores. word pairs composed asking human subjects write ﬁrst related similar word comes mind presented concept word randomly selecting frequent words large text corpus manually searching useful pairs work able collect large number word/phrase pairs good quality mining click data pinterest’s image search system used millions users. addition dataset collected visual search system suitable evaluate multimodal embedding models. another related evaluation analogy task proposed model questions like woman equal king what? evaluation. questions directly measure word similarity relatedness cannot cover semantic relationships million words dictionary. language vision models inspired recent rnn-cnn based image captioning models viewed special case sequence-tosequence learning framework adopt gated recurrent units variation simple model. multimodal word embedding models pure text effective approaches learn word embeddings train neural network models predict word given context words sentence predict context words given current word large literature word embedding models utilize visual information. type methods takes two-step strategy ﬁrst extracts text image features separately fuses together using singular value decomposition stacked autoencoders even simple concatenation learn text image features jointly fusing visual perceptual information skip-gram model however lack large-scale multimodal datasets associate visual content pre-deﬁned nouns perception domains sentences focus abstract scenes contrast best performing model places soft constraint visual features words sentences weight sharing strategy shown section training dataset pinterest largest repository images. users commonly images short descriptions share images others. since given image shared tagged multiple sometimes thousands users many images rich descriptions making source data ideal training model text image inputs. dataset prepared following ﬁrst crawled public available data pinterest construct training dataset million images. image associated average sentences removed duplicated short sentences less words. duplication figure illustration positive word/phrase pairs generation. calculate score annotation aggregating click frequency items belongs rank according score. ﬁnal list positive phrases generated ranked phrases removing phrases containing overlapping words user query phrase. text details. dataset contains million images million sentences much larger previous image description datasets addition descriptions annotated users expressed interest images descriptions dataset natural richer annotated image description datasets. dataset unique words minimum number occurence compared words appearing least times coco imtext dataset respectively. best knowledge previous paper trains multimodal model dataset scale. evaluation datasets work proposes labeled phrase triplets triplet three-phrase tuple containing phrase phrase phrase considered semantically closer testing time compute distance word embedding space consider test triplet positive relative comparison approach commonly used evaluate compare different word embedding models order generate large number phrase triplets rely user-click data collected pinterest image search system. construct large-scale evaluation dataset million triplets cleaned gold standard version thousand triplets evaluation dataset user clickthrough data hard obtain large number semantically similar related word phrase pairs. challenges constructing large-scale word/phrase similarity relatedness evaluation dataset. address challenge utilizing user clickthrough data pinterest image search system figure illustration. speciﬁcally given query user search system returns list items item composed image annotations please note annotation appear multiple items e.g. hair tutorial describe items related prom hair styles ponytails. derive matching score annotation aggregating click frequency items containing annotation. annotations ranked according matching scores ranked annotations considered positive phrases words respect user query. increase difﬁculty dataset remove phrases share common words user query initial list positive phrases. e.g. hair tutorials removed word hair contained query phrase hair styles. stemmer python’s stemmer package also adopted words root pruning step also prevents giving bias methods measure similarity positive phrase query phrase counting number overlapping words them. collected semantically similar phrase pairs. previous word similarity/relatedness datasets manually annotated word pair absolute score reﬂecting much words pair semantically related. testing stage predicted similarity score list word pairs generated model dataset compared groundtruth score list. spearman’s rank correlation lists calculated score model. however often hard expensive label absolute related score maintain consistency across pairs large-scale dataset even average scores several annotators. adopt simple strategy composing triplets phrase pairs. speciﬁcally randomly sample negative phrases pool billion phrases. negative phrase contain overlapping word phrases original phrase pair. construct triplets format evaluation model able distinguish positive phrase negative phrase calculating similarities base phrase embedding space. denote dataset related phrase dataset. cleaned-up gold standard dataset related query dataset built upon user click information contains noisy triplets create gold standard dataset conduct clean step using crowdsourcing platform crowdflower remove inaccurate triplets. sample question choices crowdsourcing annotators shown figure positive negative phrases triplet randomly given choice annotators required choose phrase related base phrase related unrelated. help annotators understand meaning phrases click phrases google search results. annotate triplets randomly sampled related query dataset. three annotators assigned question. triplet accepted added ﬁnal cleaned dataset annotators agree original positive negative label queries practice selected phrases triplets annotators agree. leads figure illustration structures model extract visual representations model sentences. numbers bottom right corner layers indicate dimensions. sampled softmax layer negative words accelerate training. model differ fuse visual representation rnn. text details. multimodal word embedding models propose three rnn-cnn based models learn multimodal word embeddings illustrated figure models parts common convolutional neural network extract visual representations recurrent neural network model sentences. part resize images adopt -layer vggnet visual feature extractor. binarized activation layer softmax layer used image features mapped space state word embeddings depends structure model fully connected layer rectiﬁed linear unit function relu max). represents element-wise product sigmoid function denotes word embedding word reset gate update gate respectively. inputs words sentence trained predict next words given previous words. words appear times pinterestm dataset dictionary. ﬁnal vocabulary size vocabulary size huge adopt sampled softmax loss accelerate training. training step sample negative words according frequency training data calculate sampled softmax loss positive word. sampled softmax loss function part adopted model minimizing loss function considered approximately maximizing probability sentences training set. illustrated figure model different ways fuse visual information word embeddings. model inspired cnn-rnn based image captioning models visual representation space states initialize since visual information embedding layer usually hard ensure information fused learned embeddings. adopt transposed weight sharing strategy proposed originally used enhance models’ ability learn novel visual concepts. speciﬁcally share weight matrix softmax layer matrix word embedding layer transposed manner. learned decode visual information enforced incorporate information word embedding matrix experiments show strategy signiﬁcantly improve performance trained embeddings. model trained maximizing likelihood next words given previous words conditioned visual representations similar image captioning models. compared model adopt direct utilize visual information model model direct supervisions ﬁnal state word embeddings adding loss terms addition negative log-likelihood loss sampled softmax layer length sentence mini-batch sentences eqn. eqn. denote additional losses model respectively. added loss term balanced weight hyperparameter negative log-likehood loss sampled softmax layer. experiments training details convert words sentences pinterestm dataset lower cases. nonalphanumeric characters removed. start sign sign added beginning sentences respectively. stochastic gradient descent method mini-batch size sentences learning rate gradient clipped train models loss decrease small validation images descriptions. models scan dataset roughly epochs. bias terms gates layer initialized evaluation details trained embedding models extract embeddings words phrase aggregate average pooling phrase representation. check whether cosine distance pair smaller pair. average precision triplets related phrases dataset gold standard related phrases dataset reported. results gold datasets evaluate compare model variants several strong baselines gold datasets. results shown table pure text denotes baseline model without input visual features trained pinterestm. model structure model except initialize hidden state zero vector. model without weight sharing denotes variant model weight matrix word embedding layer shared weight matrix sampled softmax layer wordvec-googlenews denotes state-of-the-art off-the-shelf word embedding models wordvec trained google-news data glove-twitter denotes glove model trained twitter data pure text models trained large dataset comparing models draw following conclusions evaluation criteria visual information signiﬁcantly helps learning word embeddings model successfully fuses visual text information together. e.g. model outperforms wordvec model gold datasets respectively. model also outperforms pure text baselines. weight sharing strategy crucial enhance ability model fuse visual information learned embeddings. e.g. model outperforms baseline without sharing strategy gold respectively. model performs best among three models. shows soft supervision imposed weight-sharing strategy effective direct supervision. surprising since words semantically related content image direct hard constraint might hinder learning embeddings words. discussion paper investigate task training evaluating word embedding models. introduce pinterestm largest image dataset sentence descriptions best knowledge construct evaluation dataset word/phrase similarity relatedness evaluation. based datasets propose several cnn-rnn based multimodal models learn effective word embeddings. experiments show visual information signiﬁcantly helps training word embeddings proposed model successfully incorporates information learned embeddings. lots possible extensions proposed model dataset. e.g. plan separate semantically similar related phrase pairs gold dataset better understand performance methods similar also give relatedness similarity scores pairs enable evaluation strategy previous datasets finally plan propose better models phrase representations. acknowledgement grateful james rubinstein setting crowdsourcing experiments dataset cleanup. thank veronica mapes pawel garbacki leon wong discussions support. appreciate comments suggestions anonymous reviewers nips work partly supported center brains minds machines award ccf- army research ofﬁce -cs.", "year": 2016}