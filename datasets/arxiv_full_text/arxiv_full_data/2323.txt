{"title": "Efficient Optimal Learning for Contextual Bandits", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We address the problem of learning in an online setting where the learner repeatedly observes features, selects among a set of actions, and receives reward for the action taken. We provide the first efficient algorithm with an optimal regret. Our algorithm uses a cost sensitive classification learner as an oracle and has a running time $\\mathrm{polylog}(N)$, where $N$ is the number of classification rules among which the oracle might choose. This is exponentially faster than all previous algorithms that achieve optimal regret in this setting. Our formulation also enables us to create an algorithm with regret that is additive rather than multiplicative in feedback delay as in all previous work.", "text": "address problem learning online setting learner repeatedly observes features selects among actions receives reward action taken. provide ﬁrst eﬃcient algorithm optimal regret. algorithm uses cost sensitive classiﬁcation learner oracle running time polylog number classiﬁcation rules among oracle might choose. exponentially faster previous algorithms achieve optimal regret setting. formulation also enables create algorithm regret additive rather multiplicative feedback delay previous work. diﬀerence contextual bandit setting standard supervised learning reward chosen action revealed. example always choosing action several times feedback given provides almost basis prefer chosen action another action. essence contextual bandit setting captures diﬃculty exploration avoiding diﬃculty contextual bandit setting half-way point between standard supervised learning full-scale reinforcement learning appears possible construct algorithms convergence rate guarantees similar supervised learning. many natural settings satisfy half-way point motivating investigation contextual bandit learning. example problem choosing interesting news articles users internet companies naturally modeled contextual bandit setting. medical domain discrete treatments tested approval process deciding patients eligible treatment takes contexts account. generally imagine future personalized medicine treatments essentially equivalent actions contextual bandit setting. create algorithm step competes policies. measure success comparing algorithm’s cumulative reward expected cumulative reward best policy set. diﬀerence called regret. existing algorithms setting either achieve suboptimal regret require computation linear number policies unstructured policy spaces computational complexity best hope for. hand case rewards actions revealed problem equivalent cost-sensitive classiﬁcation know algorithms eﬃciently search space policies cost-sensitive logistic regression support vector machines. cases space classiﬁcagoal eﬃciently solve contextual bandit problems similarly large policy spaces. reducing contextual bandit problem cost-sensitive classiﬁcation. given supervised cost-sensitive learning algorithm oracle algorithm runs time polylog achieving regret previous regret-optimal approaches measure based—they work updating measure policies operation linear number policies. contrast regret guarantees scale logarithmically number policies. computational bottleneck regret guarantees imply could dramatically increase performance contextual bandit settings using expressive policies. overcome computational bottleneck using algorithm works creating cost-sensitive classiﬁcation instances calling oracle choose optimal policies. actions chosen based policies returned oracle rather according measure policies. reminiscent adaboost creates weighted binary classiﬁcation instances calls weak learner oracle obtain classiﬁcation rules. classiﬁcation rules combined ﬁnal classiﬁer boosted accuracy. similarly adaboost converts weak learner strong learner approach converts cost-sensitive classiﬁcation learner algorithm solves contextual bandit problem. diﬃcult version contextual bandits adversary chooses given knowledge learning algorithm known regret-optimal solutions adversarial setting variants algorithm achieves regret rate algorithm succeed high probability also classes adversary constrained i.i.d. sampling. central beneﬁts hope realize directly assuming i.i.d. contexts reward vectors. attempt around follow-the-perturbed-leader algorithm provides computationally tractable solution certain special-case structures. algorithm mechanism eﬃcient application arbitrary policy spaces even given eﬃcient cost-sensitive classiﬁcation oracle. eﬃcient cost-sensitive classiﬁcation oracle shown eﬀective transductive settings aside drawback requiring transductive setting regret achieved substantially worse exp. improved rates. world completely adversarial possible achieve substantially lower regrets possible algorithms optimized adversarial setting. example supervised learning possible obtain regrets scaling problem dependent constant feedback delayed rounds lower bounds imply regret adversarial also many special-case analyses. example theory context-free setting well understood similarly good algorithms exist rewards linear functions features actions continuous space reward function sampled according gaussian process element algorithm identiﬁcation distribution actions simultaneously achieves small expected regret allows estimating value every policy small variance. existence distribution shown nonconstructively minimax argument. policyelimination computationally intractable also requires exact knowledge context distribution show address issues section using algorithm call randomizeducb. namely prove following theorem. randomizeducb’s analysis substantially complex subroutine application ellipsoid algorithm costsensitive classiﬁcation oracle randomizeducb assume knowledge context distribution instead works history contexts observed. modifying proof empirical distribution requires covering argument distributions policies uses probabilistic method. result algorithm similar top-level analysis policyelimination running time poly-logarithmic number policies given costsensitive classiﬁcation oracle. i.i.d. setting round world chooses i.i.d. according reveals learner. learner access chooses action world reveals reward learner interaction proceeds next round. consider modes accessing policies ﬁrst option enumeration policies. impractical general suﬃces illustrative purpose ﬁrst algorithm. second option oracle access argmax oracle corresponding cost-sensitive learner apart tractable algorithm analysis used derive tighter regrets would possible adversarial setting. example section consider common setting reward feedback delayed rounds. straightforward modiﬁcation algorithms work choosing distributions policies turn induce distributions actions. distribution policies denote induced conditional distribution actions given context step step ﬁnds distribution policies induces variance estimate value policies. minimax theorem show distribution always exists. distribution speciﬁed here section develop method based ellipsoid algorithm. step projects distribution onto distribution actions applies smoothing. finally step eliminates policies determined suboptimal non-empty. recast feasibility problem step game players prover trying produce falsiﬁer trying violating constraints. give power falsiﬁer allow choose distribution would violate constraints. note policy corresponds point space randomized policies distribution policies induced randomized policy corresponds point convex hull πt−. denoting convex hull prover’s choice falsiﬁer’s choice diﬃculties addressed randomizeducb algorithm present analyze section. approach reminiscent algorithm developed context-free setting keeps upperconﬁdence bound expected reward action. however instead choosing highest upper conﬁdence bound randomize choices according value empirical performance. algorithm following properties suboptimal policies implicitly used decreasing frequency using non-uniform variance constraint depends policy’s estimated regret. consequence bound value optimization stated lemma below. lemma implies algorithm always able select distribution policies focuses mostly policies estimated regret. moreover variance constraints ensure good policies never appear policies allowed incur high variance reward estimates. hence minimizing objective eﬀective surrogate minimizing regret. bulk analysis consists analyzing variance importance-weighted reward estimates showing relate actual expected rewards details deferred appendix algorithm relies ellipsoid method. ellipsoid method general technique solving convex programs equipped separation oracle. separation oracle deﬁned follows convex empty not. given non-empty point ellipsoid algorithm decides correctly empty executing iterations involving call separation oracle additional processing time. algorithm complicated fact need ensure feasible region non-empty non-negligible volume necessitates small error satisfying constraints program. leave details appendix modulo details construction separation oracle essen. first constraint note linear compute maxπ lemma compute check constraint satisﬁed. constraint linear automatically yields separating hyperplane. this apply ellipsoid method. this need separation oracle program. separation oracle constraints constructed step above. constraints candidate solution construct separating hyperplane lemma itly compute approximate convex combination policies yields done running perceptron algorithm stopping bound number iterations reached. collect policies found perceptron algorithm guaranteed close distance convex hull. closest point convex hull policies solving simple quadratic program. theorem iterative algorithm iterations involving call processing time either declares correctly infeasible outputs distribution policies satisﬁes eyal even-dar shie mannor yishay mansour. action elimination stopping conditions multi-armed bandit reinforcement learning problems. journal machine learning research following immediate corollary theorem viewed version freedman’s inequality sequence real-valued random variables. denote conditional expectation conditional variance. theorem", "year": 2011}