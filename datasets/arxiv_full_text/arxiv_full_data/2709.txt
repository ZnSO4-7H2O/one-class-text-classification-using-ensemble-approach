{"title": "Large-Scale Low-Rank Matrix Learning with Nonconvex Regularizers", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Low-rank modeling has many important applications in computer vision and machine learning. While the matrix rank is often approximated by the convex nuclear norm, the use of nonconvex low-rank regularizers has demonstrated better empirical performance. However, the resulting optimization problem is much more challenging. Recent state-of-the-art requires an expensive full SVD in each iteration. In this paper, we show that for many commonly-used nonconvex low-rank regularizers, a cutoff can be derived to automatically threshold the singular values obtained from the proximal operator. This allows such operator being efficiently approximated by power method. Based on it, we develop a proximal gradient algorithm (and its accelerated variant) with inexact proximal splitting and prove that a convergence rate of O(1/T) where T is the number of iterations is guaranteed. Furthermore, we show the proposed algorithm can be well parallelized, which achieves nearly linear speedup w.r.t the number of threads. Extensive experiments are performed on matrix completion and robust principal component analysis, which shows a significant speedup over the state-of-the-art. Moreover, the matrix solution obtained is more accurate and has a lower rank than that of the nuclear norm regularizer.", "text": "abstract—low-rank modeling many important applications computer vision machine learning. matrix rank often approximated convex nuclear norm nonconvex low-rank regularizers demonstrated better empirical performance. however resulting optimization problem much challenging. recent state-of-the-art requires expensive full iteration. paper show many commonly-used nonconvex low-rank regularizers singular values obtained proximal operator automatically threshold. allows proximal operator efﬁciently approximated power method. develop fast proximal algorithm accelerated variant inexact proximal step. convergence rate number iterations guaranteed. furthermore show proposed algorithm parallelized resultant algorithm achieves nearly linear speedup w.r.t. number threads. extensive experiments performed matrix completion robust principal component analysis. signiﬁcant speedup state-of-the-art observed. index terms—low-rank matrix learning nonconvex regularization proximal algorithm parallel algorithm matrix completion robust principle component analysis machine learning computer vision problems. example matrix completion successful approaches collaborative ﬁltering assumes target rating matrix low-rank. besides collaborative ﬁltering matrix completion also used tasks video image processing another important low-rank matrix learning robust principal component analysis assumes target matrix low-rank also corrupted sparse noise. rpca popularly used computer vision applications shadow removal background modeling robust photometric stereo besides low-rank matrix learning also used face recognition subspace clustering however minimization matrix rank np-hard alleviate problem common approach convex surrogate nuclear norm known nuclear norm tightest convex lower bound rank. though nuclear norm non-smooth resultant optimization problem solved efﬁciently using modern tools proximal algorithm frank-wolfe algorithm active subspace selection method kwok department computer science engineering hong kong university science technology clear water hong kong. e-mails {qyaoaa jamesk}cse.ust.hk wang microsoft research asia beijing china e-mails {taifengw tyliu}microsoft.com larger thus informative singular values less penalized. example nonconvex low-rank regularizers include capped- penalty log-sum penalty truncated nuclear norm smoothly clipped absolute deviation minimax concave penalty applied various computer vision tasks image denoising background modeling empirically nonconvex regularizers achieve better recovery performance convex nuclear norm regularizer. recently theoretical results also established however resultant nonconvex optimization problem much challenging. existing optimization algorithms work nuclear norm cannot applied. general approach still used concave-convex procedure decomposes nonconvex regularizer difference convex functions however sequence relaxed optimization problems solved computationally expensive efﬁcient approach recently proposed iteratively re-weighted nuclear norm algorithm based observation existing nonconvex regularizers concave non-increasing super-gradients. irnn iteration involves computing supergradient regularizer singular value decomposition however performing matrix takes time expensive large matrices. recently proximal algorithm used nonconvex low-rank matrix learning however requires full solve proximal operator expensive. paper observe commonly-used nonconvex low-rank regularizers singular values obtained corresponding proximal operator automatically convex proximal algorithm converges optimal solution rate number iterations. accelerated rate replacing proper linear combination recently accelerated proximal algorithm extended problems nonconvex state-ofthe-art nonmonotone accelerated proximal gradient algorithm iteration perform proximal steps acceleration performed step objective checked determine whether accepted problem nonconvex convergence rate still open. however empirically much faster. thresholded. needs leading singular values/vectors order generate next iterate. moreover instead computing proximal operator large matrix needs matrix projected onto leading subspace. matrix size signiﬁcantly reduced proximal operator made much efﬁcient. besides using power method good approximation subspace efﬁciently obtained. proposed procedure readily used standard proximal algorithm convergence properties directly applicable proximal step approximately solved. sequel show inexactness proximal step controlled convergence rate still guaranteed. moreover algorithm speeded using acceleration. effectiveness proposed algorithms demonstrated popular low-rank matrix learning applications namely matrix completion robust principal component analysis matrix completion show additional speedup possible exploring problem’s sparse plus low-rank structure; whereas rpca extend proposed algorithm handle parameter blocks involved rpca formulation. popularity multicore shared-memory platforms parallelize proposed algorithms handle much larger data sets. show achieve almost linear speedup w.r.t. number threads. experiments performed synthetic realworld data sets. results show proposed nonconvex low-rank matrix learning algorithms several orders faster state-of-the-art outperform approaches including factorization nuclear norm regularization. preliminary results paper reported full version speed algorithm acceleration demonstrate applied important instances low-rank matrix learning problems namely matrix completion rpca. besides show proposed algorithms parallelized. extensive empirical evaluations also performed sequential parallel versions algorithms. notation sequel vectors denoted lowercase boldface matrices uppercase boldface transpose superscript square matrix trace. rectangle matrix leading singular value nuclear norm. given diag constructs diagonal matrix whose diagonal element denotes identity matrix. differentiable function gradient. nonsmooth function subdifferential i.e. nuclear norm makes low-rank optimization easier good enough approximation matrix rank mentioned section number nonconvex surrogates recently proposed. paper make following assumption low-rank regularizer satisﬁed nonconvex low-rank regularizers table automatic thresholding singular values following proposition shows becomes zero smaller regularizer-speciﬁc threshold. proof found appendix proposition exists threshold together proposition solving proximal operator needs leading singular values/vectors nonconvex regularizers table simple closed-form solutions obtained examining optimality conditions proof found appendix reducing size assume singular values larger need rank-k rank-ˆk uˆkσˆkv following proposition shows proxµr obtained proximal operator smaller matrix. proof found appendix proposition assume rm×k orthogonal span span. then proxµr proxµrz). obtaining approximate gsvt obtain power method recently used approximate nuclear norm minimization number power iterations warm-start used matrix algorithm particularly useful iterative nature proximal algorithm. obtaining approximate using algorithm takes time. propack recently iteratively reweighted nuclear norm algorithm proposed handle nonconvex low-rank matrix optimization problem. iteration solves subproblem original nonconvex regularizer approximated weighted version nuclear subproblem closed-form solution needed takes time. solvers designed speciﬁc nonconvex low-rank regularizers include perform iteration takes time slow. proximal algorithm mostly used convex problems recently also applied nonconvex problems generalized proximal gradient algorithm ﬁrst proximal algorithm handle nonconvex regularizers. particular proximal operator computed follows. problem solved ﬁxed-point iteration. however closed-form solutions indeed exist regularizers table nevertheless proposition still involves takes time. algorithm also used obtain time. however ﬁnds exactly cannot beneﬁt warm-start. hence though time complexity power method empirically much less efﬁcient shown algorithm step uses power method efﬁciently obtain orthogonal matrix approximates span. step performs small svd. though still exact much smaller svdz) takes time. step singular values σii’s thresholded using corollary steps obtains proxµr using proposition time complexity gsvt reduced algorithm approximate gsvt approxgsvt. input rm×n rn×k warm-start; powermethod; svdz); number σii’s corollary leading columns leading columns return diag however assumed convex attouch considered nonconvex require difﬁcult expensive condition control inexactness holds accept ˜xp; otherwise improve using ˜vp− warm-start next iterate. following proposition shows convergence algorithm proof found appendix proposition number singular values larger limp→∞ prox complete procedure complete procedure solving shown algorithm called fancl similar perform warm-start using column spaces previous iterates speedup employ continuation strategy step speciﬁcally initialized large value decreases gradually. assume evaluations take time valid many applications matrix completion rpca. rank iteration rt−. algorithm step takes time; step takes time columns. iteration time complexity thus experiment enough guarantee empirically. iteration time complexity algorithm thus reduces contrast exact gsvt takes time much slower besides space complexity algorithm convergence analysis inexact proximal algorithm ﬁrst considered assumes convex. nonconvex extension considered however discussed section expensive condition control inexactness proximal step. thus analysis cannot applied here. known assumption decomposed difference convex functions following proposition shows also admits decomposition. proof appendix proposition decomposed convex. known proximal mapping critical point motivates measure convergence however cannot used nonconvex proximal step inexact. proposition guarantees existence limit points instead measure convergence. proximal step exact following corollary shows convergence algorithm proof found appendix corollary mint=...t acceleration convex optimization acceleration commonly used speed convergence proximal algorithms recently also extended nonconvex optimization state-of-the-art algorithm nmapg section integrate nmapg fancl. whole procedure shown algorithm accelerated iterate obtained step resultant inexact proximal step solution achieve sufﬁcient decrease iterate accepted otherwise choose inexact proximal step solution obtained nonaccelerated iterate note step step algorithm thus iteration time complexity algorithm twice algorithm still several major differences algorithm nmapg. first proximal step algorithm inexact. make algorithm robust allow nonmonotonous update cannot larger moreover simpler acceleration scheme involved. matrix completion problems allows using sparse plus low-rank structure greatly reduce iteration complexity finally require extra comparison objective step reduces iteration complexity. used measure progress proximal step. algorithm proximal step accelerated iterate non-accelerated iterate hence step performed otherwise. similar corollary following shows convergence rate. proof found appendix corollary algorithm mint=...t xt+−ct −inf mint nonconvex optimization problems optimal convergence rate ﬁrst-order methods thus convergence rate algorithm cannot improve algorithm however practice acceleration still signiﬁcantly reduce number iterations nonconvex problems hand algorithm need second proximal step iteration time complexity higher algorithm however much compensated speedup convergence. demonstrated section empirically algorithm much faster. take oktk time step algorithm columns +ωkt) call approximate gsvt takes time finally step algorithm ωkt) time. result step also takes +ωkt) time. algorithm takes total step slightly cheaper time complexity ortkt +ωrt). summarizing iteration time complexity algorithm space complexity also reduced. need store low-rank factorizations sparse matrices take total okt+ω) space section techniques also used algorithm easily shown iteration time complexity ortkt +ωrt) space complexity comparison existing algorithms table compares convergence rates iteration time complexities space complexities various matrix completion algorithms empirically compared section overall proposed algorithms enjoy fast convergence cheap iteration complexity memory cost. algorithms convergence rate section algorithm signiﬁcantly faster. robust principal component analysis given noisy data matrix rm×n rpca assumes approximated low-rank matrix plus sparse noise optimization problem low-rank regularizer sparsity-inducing regularizer. here allow nonconvex nonsmooth. thus seen nonconvex extension rpca -regularizer examples nonconvex shown table examples nonconvex include -norm capped-norm log-sum-penalty following theorem shows limit point iterates algorithm also critical point. proof found appendix theorem {xtj} subsequence {xt} generated algorithm limtj→∞ assumption theorem critical point section consider important instances problem namely matrix completion robust principal component analysis accelerated fancl algorithm usually faster nonaccelerated variant consider accelerated variant here. matrix completion show algorithm made even faster require much less memory using sparse plus low-rank structure problem. section show algorithm extended deal parameter blocks rpca. matrix completion attempts recover low-rank matrix rm×n observing elements observed positions indicated }m×n observed otherwise. matrix completion formulated optimization problem utilizing problem structure first consider step checks objectives. computing relies observed positions singular values hence instead explicitly constructing maintain utσtv sparse matrix computing takes oωrt) takes oωkt) time time. computing time. thus step takes oωkt comparison iteration time complexities convergence rates space complexity various matrix completion solvers. here integer constants. active subspace selection method number inner iterations required. many popular sparsity-inducing regularizers computing proxυg time sign sign sign however directly computing proxλr requires time expensive. alleviate problem algorithm easily extended algorithm iteration time complexity dominated inexact proximal steps steps reduced convergence results section easily extended rpca problem. proofs following found appendices proposition sequence generated algorithm bounded least limit point. parallel fancl matrix completion section show proposed algorithms parallelized. consider matrix completion problem extension problems rpca section similarly performed. moreover simplicity discussion focus simpler fancl algorithm accelerated variant similarly parallelized shown appendix proposed algorithm operations matrix often form multiplications element-wise operation popular scheme parallel linear algebra block distribution assume threads parallelization. block distribution partitions rows columns parts leading total blocks. figure shows computations element-wise operation easily parallelized. algorithm important variables low-rank factorized form utσtv sparse matrices pωpω. using block distribution thus partitioned figure resultant parallelized version fancl shown algorithm steps parallelized marked subroutines introduced namely indespan-pl replaces factorization approxgsvt-pl parallelized version algorithm discussed detail following sections. note algorithm equivalent algorithm except parallelized. thus convergence results section still hold. identifying span step algorithm factorization used span matrix parallelized householder transformation gaussian elimination however typically complex. following resultant parallel algorithm shown algorithm time complexity algorithm calls algorithm input thus takes time rt−. parallelize steps matrices involved small. approximate gsvt steps approximate gsvt power method svd. power method parallelized straightforwardly algorithm also replace subroutine algorithm multiple factorizations usually needed parallelization complex discussed section following proposition performs simpler manner. proof found appendix proposition given matrix rn×k rn×k orthogonal equals span uσv. then resultant parallelized procedure approximate gsvt shown algorithm step small performed matrix step algorithm returned algorithm keep low-rank factorized form. besides algorithm called sparse plus low-rank structure mentioned earlier. hence used speed matrix multiplications. columns algorithm active subspace selection adds/removes rank-one subspaces active iteration. nuclear norm optimization problem reduced smaller problem deﬁned active set. algorithm approximate gsvt parallel approxgsvtpl. input partitioned matrix rm×n rn×k; powermethod-pl; rn×k iden-span; svd; rk×k number σii’s corollary leading columns leading columns return diag shown figures computation directly parallelized takes evaluate thus computing takes time. time. similarly computing takes lowrank factorized forms utilized. based figures performed time. thus time complexity steps algorithm experiments performed windows server system intel xeon e--v memory. algorithms sections implemented matlab. section intel-mkl package matrix operations standard thread library multi-thread programming. matrix completion compare number low-rank matrix completion solvers including models based commonly used nuclear norm regularizer; ﬁxed-rank factorization models decompose observed synthetic data observed matrix generated elements rm×k rk×m sampled i.i.d. standard normal distribution elements sampled total random elements observed. half used training rest validation parameter tuning. testing performed unobserved elements. normalized mean squared error nmse /pω⊥ recovered matrix denotes unobserved positions; rank training time. vary range experiment repeated times. results shown table seen nonconvex regularization tnn) leads much lower nmse’s convex nuclear norm regularization ﬁxed-rank factorization. moreover nuclear norm ermp output much higher ranks. terms speed among nonconvex low-rank solvers fancl fast fanclacc fastest. larger matrix higher speedup fancl fancl-acc irnn. movielens experiment performed popular movielens data contain ratings different users movies. follow setup observed ratings training validation rest testing. performance evaluation root mean squared error test rmse recovered matrix. experiment repeated times. results shown table again nonconvex regularizers lead lowest rmse’s. moreover fanclacc also fastest among nonconvex low-rank solvers even faster state-of-the-art gpg. particular fancl accelerated variant fancl-acc solvers movielens-m data sets. figure compares objectives time nonconvex regularization solvers movielens-k. seen fancl fancl-acc decrease objective rmse much faster others. figure shows testing rmses movielens-m. again fancl-acc fastest. results shown table irnn cannot data large. ais-impute similar running time lmafit inferior performance thus compared. again nonconvex regularizers converge faster yield lower rmse’s solutions much lower ranks. figure shows objectives rmse time fancl-acc fastest. robust principal component analysis synthetic data section ﬁrst perform experiments synthetic data set. observed matrix generated ˜s+g elements rm×k rk×m sampled i.i.d. elements sampled matrix sparse elements randomly −uv∞ equal probabilities. whole data randomly split training test sets equal size. standard regularizer used sparsity regularizer different convex/nonconvex lowrank regularizers used hyperparameters tuned using training set. performance evaluation nmse recovered low-rank sparse components respectively; results shown table seen nonconvex regularizers lead better psnr’s convex nuclear norm. moreover fancl-acc much faster gpg. figure shows psnr time bootstrap campus data sets. again fancl-acc converges higher psnr much faster. results hall escalator similar. used here. objectives form smooth function plus low-rank regularizer rpca also nonsmooth regularizer. similarly ais-impute matrix completion. moreover fancl shown slower fancl-acc compared. results shown table accuracies locating sparse support always methods thus shown. moreover convex nonconvex regularizers perfectly recover matrix rank sparse locations nonconvex regularizers lower nmse’s. matrix completion fancl-acc much faster; larger matrix higher speedup. background removal videos section rpca background removal videos. four benchmark videos used example frames shown figure image background considered low-rank foreground moving objects contribute sparse component. given video image frames frame ﬁrst reshaped m-dimensional column vector frames stacked together form matrix. pixel values normalized gaussian noise added. experiment repeated times. performance evaluation commonly used peak signal-to-noise ratio algorithms inferior performance machine cores thread core. suggested randomly shufﬂe matrix columns rows partitioning. penalty total number iterations hyperparameters section experiments repeated times. convergence objective typical shown figure multiple threads running single report clock time instead time. seen accelerated algorithms much faster non-accelerated ones parallelization provides speedup. figure shows speedup different numbers threads. seen parallelized variants scale well number threads. particular scaling better yahoo. observed entries partitioned data submatrices distributed evenly improves performance parallel algorithms another observation speedup larger one. discussed performing multiplications large sparse matrix signiﬁcant amount time spent indexing nonzero elements. matrix partitioned submatrix becomes smaller easier indexed. thus memory cache also becomes effective. conclusion paper considered challenging problem nonconvex low-rank matrix optimization. observations popular low-rank regularizers singular values obtained proximal operator automatically thresholded proximal operator computed smaller matrix. allows proximal operator efﬁciently approximated power method. extended proximal algorithm nonconvex optimization setting acceleration inexact proximal step. parallelized proposed algorithm scales well w.r.t. number threads. extensive experiments matrix completion rpca show proposed algorithm much faster state-of-the-art. also demonstrates nonconvex lowrank regularizers outperform standard nuclear norm regularizer. parallel setting typically observed entries non-uniformly distributed partitioned matrices workloads different threads well balanced. future direction allow asynchronized updates parallel algorithm. help reduce waiting time threads light workloads makes efﬁcient cpu. moreover parallel algorithms multicore machines easier implement communication issues less scalable distributed algorithms allow scaleup massive data sets consider extending proposed algorithms distributed computing environment. zhang fast accurate matrix completion truncated nuclear norm regularization ieee transactions pattern analysis machine intelligence vol. musialski wonka tensor completion estimating missing values visual data ieee transactions pattern analysis machine intelligence vol. tang nonconvex nonsmooth rank minimization iteratively reweighted nuclear norm ieee transactions image processing vol. meng feng zhang weighted nuclear norm minimization applications level vision international journal computer vision vol. t.-h. bazin kweon partial minimization singular values robust algorithm applications ieee transactions pattern analysis machine intelligence vol. yang qian zhang nuclear norm based matrix regression applications face recognition occlusion illumination changes ieee transactions pattern analysis machine intelligence vol. robust recovery subspace structures low-rank representation ieee transactions pattern analysis machine intelligence vol. kwok accelerated inexact soft-impute fast large-scale matrix completion proceedings international joint conference artiﬁcial intelligence zhang schuurmans y.-l. accelerated training matrix-norm regularization boosting approach advances neural information processing systems gong zhang huang general iterative shrinkage thresholding algorithm non-convex regularized optimization problems proceedings international conference machine learning halko p.-g. martinsson tropp finding structure randomness probabilistic algorithms constructing approximate matrix decompositions siam review vol. matsushita kweon fast randomized singular value thresholding nuclear norm minimization proceedings conference computer vision pattern recognition attouch bolte svaiter convergence descent methods semi-algebraic tame problems proximal algorithms forward-backward splitting regularized gauss-seidel methods mathematical programming vol. schmidt roux bach convergence rates inexact proximal-gradient methods convex optimization advances neural information processing systems hiriart-urruty generalized differentiability duality optimization problems dealing differences convex functions convexity duality optimization zhang solving low-rank factorization model matrix completion nonlinear successive over-relaxation algorithm mathematical programming computation vol. wang davulcu orthogonal rank-one matrix pursuit rank matrix completion siam journal scientiﬁc computing vol. gemulla nijkamp haas sismanis large-scale matrix factorization distributed stochastic gradient descent proceedings international conference knowledge discovery data mining h.-f. c.-j. hsieh dhillon scalable coordinate descent approaches parallel matrix factorization recommender systems proceedings international conference data mining avron kale sindhwani kasiviswanathan efﬁcient practical stochastic subgradient descent nuclear norm regularization proceedings international conference machine learning goumas kourtis anastopoulos karakasis koziris understanding performance sparse matrixvector multiplication proceedings euromicro conference parallel distributed network-based processing bertsekas nonlinear programming. athena scientiﬁc separation theorems singular values matrices applications multivariate analysis journal multivariate analysis vol. algorithm shows parallel version fancl-acc. acceleration performed step ﬁrst inexact proximal step performed steps step checks whether accelerated iterate accepted. condition fails second inexact proximal step performed steps note algorithm equivalent algorithm thus convergence analysis section still holds. algorithm fancl-acc parallel fancl-acc-pl. input choose initialize random gaussian matrices partition start threads parallelization; αt−− indespan-pl return however becomes smaller reach reaches zero. comes facts. first becomes smaller gets smaller become smaller second limy→+ illustration relationships among shown following ﬁgure. thus exists becomes span span. theorem substituting σiz) combining obtain optimal solution thus rank-ˆk uˆk)σˆkv corresponding left right singular vectors contained quˆk respectively. thus maxtj =...∞ sequence bounded least limit point |ω∞| inﬁnite |ω∞| ﬁnite bounded sequence must bounded. besides assumption indicates sequence must bounded least limit point |ω∞| |ω∞| inﬁnite cases bounded least limit point |ω∞| |ω∞| inﬁnite.", "year": 2017}