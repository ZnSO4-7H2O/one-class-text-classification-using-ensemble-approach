{"title": "Complexity of stochastic branch and bound methods for belief tree search  in Bayesian reinforcement learning", "tag": ["cs.LG", "cs.AI"], "abstract": "There has been a lot of recent work on Bayesian methods for reinforcement learning exhibiting near-optimal online performance. The main obstacle facing such methods is that in most problems of interest, the optimal solution involves planning in an infinitely large tree. However, it is possible to obtain stochastic lower and upper bounds on the value of each tree node. This enables us to use stochastic branch and bound algorithms to search the tree efficiently. This paper proposes two such algorithms and examines their complexity in this setting.", "text": "recent work bayesian methods reinforcement learning exhibiting near-optimal online performance. main obstacle facing methods problems interest optimal solution involves planning inﬁnitely large tree. however possible obtain stochastic lower upper bounds value tree node. enables stochastic branch bound algorithms search tree efﬁciently. paper proposes algorithms examines complexity setting. various bayesian methods exploration markov decision processes solving known partially-observable markov decision processes proposed previously however methods often suffer computational tractability problems. optimal bayesian exploration requires creation augmented model form tree root node current belief-state pair children possible subsequent belief-state pairs. size belief tree increases exponentially horizon branching factor inﬁnite case continuous observations actions. work examine complexity efﬁcient algorithms expanding tree. particular propose analyse stochastic search methods similar ones proposed related methods previously examined experimentally context bayesian reinforcement learning remainder section summarises bayesian planning framework. main results presented sect. section concludes discussion related work. technical proofs related results presented appendix. reinforcement learning discrete-time sequential decision making problem wish maximise expected krt+k stochastic reward time discounted future rewards interested rewards time plays role deﬁnition markov decision process discrete-time stochastic process with state time reward generated process action chosen decision maker. denote distribution next states depends furthermore reward distribution conditioned states actions. finally above throughout text usually take mean distribution process compactness. frequently notation imply marginalisation. example shall write mean decision maker takes actions according policy deﬁnes distribution conditioned state i.e. probability measures indexed policy stationary tt′. expected utility policy selecting actions time written value function denotes expectation markov chain arising acting policy whenever clear context superscripts subscripts shall omitted brevity. optimal value function denoted maxp known evaluate optimal value function policy time polynomial sizes state action sets backwards induction unknown bayesian framework represent uncertainty requires maintaining belief corresponds reality. precisely deﬁne measurable space mdps suitable -algebra. appropriate initial density obtain sequence densities representing subjective belief time conditioning order optimally select actions framework necessary explicitly take account future changes belief planning idea combine original mdp’s state belief state deﬁnition belief-augmented hyper-states appropriate probability measures state action sets time agent observes hyper-state takes action write transition distribution horizon ﬁnite need require expand tree depth thus backwards induction starting terminal hyper-states proceeding backwards provides solution bounds value function shall relate optimal value function bamdp value functions mdps optimal policy denoted mean resulting belief denoted properties ﬁnite cannot calculate upper bound closed form. however monte carlo sampling given hyper-state draw mdps belief estimate value function then limm→¥ discrete case sample multinomial distribution dirichlet densities independently transitions. rewards draw independent bernoulli distributions beta state-action pair. present main results. detailed proofs given appendix. search trees arise context planning uncertainty mdps using bamdp framework. value function bounds leaf nodes partially expanded bamdp tree obtain bounds inner nodes backwards induction. bounds used action selection tree expansion. however bounds estimated monte carlo sampling something necessitates stochastic branch bound technique expand tree. analyse algorithms. ﬁrst search ﬁxed depth employs exact lower bounds. show stochastic bounds available complexity ﬁxed depth search increases logarithmically. present stochastic branch bound algorithms whose complexity dependent number near-optimal branches. ﬁrst uses bound samples leaf nodes only second uses samples obtained last half parents leaf nodes thus using collected samples efﬁciently. present main assumptions concerning tree search pointing relations bayesian symbols overloaded make correspondence apparent. tree branching factor branching action choices random outcomes thus nodes depth correspond hyper-states t+k} bamdp. abusing notation also refer components node deﬁne branch policies value branch maxp root branch policies value hyper-state b-reachable .any branch partitioned b-reachable branches possible partition maxa simplify considering deterministic policies. denote k-horizon value function maxp tree node fully expanding tree depth performing backwards induction using either value leaf nodes obtain respectively upper lower stochastic branch bound algorithm similar examined originally developed norkin optimisation problems. stage takes additional sample leaf node improve upper bound estimates expands node highest mean upper bound. algorithm uses basic idea averaging value function samples every leaf node. order bound complexity need bound time required discover nearly optimal branch. calculate number times suboptimal branch expanded suboptimality discovered. similarly calculate number times shall sample optimal node mean upper bound becomes dominant. results cover time spent sampling upper bounds nodes optimal branch without expanding time spent expanding nodes sub-optimal branch. setting difference optimal second optimal branch lemma bound number times leaf nodes optimal branch sampled without expanded. converse problem bounding number times suboptimal branch expanded. degeneracy main problem alg. alg. propagates upper bounds multiple leaf nodes root also re-uses upper bound samples inner nodes order handle degenerate case path non-zero probability. longer operating leaf nodes take advantage upper bound samples collected along given trajectory. however upper bounds along branch early samples bias estimates lot. reason leaf depth average upper bounds along branch depth complexity approach given following lemma proof degenerate case sub-branch non-zero probability. however re-use samples obtained previous expansions allows tighter thus allowing upper bound bias hoeffding bound obtain desired outcome. bound decreases faster furthermore dependence initial transitory period however long. gain fact re-using upper bounds previously obtained inner nodes. thus algorithm particularly suitable stochastic problems. lower bounds bayesian reduce branching factor employing sparse sampling methods o{|a|exp]}. essentially approach employed however main focus reduce depth branch searched. results help obtain better lower bounds ways. first note initially converges faster large thus able expand less deeply. later large sample even sparely. search depth rewards then naively error however mean mdps close bounded mean lem. means signiﬁcantly smaller n/n. undiscounted fact total error bounded problems error bounded original case taking account smoothness. much recent work bayesian focused myopic estimates full expansion belief tree certain depth. exceptions include uses analytical bound based sampling small beliefs uses kearn’s sparse sampling algorithm expand tree. methods complexity exponential horizon something improve smoothness properties induced bayesian updating. also connections work pomdps problems however setting though equivalent abstract sense sufﬁciently close consider. results bandit problems employing value function bounds used herein reported experimentally compared algorithms operating leaf nodes only. interestingly alg. resembles search adapted stochasverses tree major differences. trees. means samples upper bounds rather upper bounds sample means. reasons unable simply restate arguments presented complexity results counting arguments number tree search algorithms trees stochastic upper lower bounds satisfying smoothness property exist. ﬁrst results type partially extend results provided asymptotic convergence proof similar smoothness conditions stochastic branch bound algorithm. addition introduce mechanism utilise samples obtained inner nodes calculating mean upper bounds leaf nodes. finally relate complexity results whose lower bound provide small improvement. plan address online sample complexity proposed algorithms well practical performance future work. work part icis project supported dutch ministry economic affairs grant bsik. would also like thank anonymous reviewers detailed reviews earlier versions paper; peter auer peter gr¨unwald ronald ortner remi munos extensive discussions; ﬁnally shimon whiteson frans groen comments corrections. lemma similarly previous lemma degenerate case sub-branch non-zero probability. however algorithm re-uses samples obtained previous expansions. depth average bounds", "year": 2009}