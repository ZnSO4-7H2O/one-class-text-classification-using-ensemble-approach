{"title": "Stochastic Variance Reduction for Nonconvex Optimization", "tag": ["math.OC", "cs.LG", "cs.NE", "stat.ML"], "abstract": "We study nonconvex finite-sum problems and analyze stochastic variance reduced gradient (SVRG) methods for them. SVRG and related methods have recently surged into prominence for convex optimization given their edge over stochastic gradient descent (SGD); but their theoretical analysis almost exclusively assumes convexity. In contrast, we prove non-asymptotic rates of convergence (to stationary points) of SVRG for nonconvex optimization, and show that it is provably faster than SGD and gradient descent. We also analyze a subclass of nonconvex problems on which SVRG attains linear convergence to the global optimum. We extend our analysis to mini-batch variants of SVRG, showing (theoretical) linear speedup due to mini-batching in parallel settings.", "text": "study nonconvex ﬁnite-sum problems analyze stochastic variance reduced gradient methods them. svrg related methods recently surged prominence convex optimization given edge stochastic gradient descent theoretical analysis almost exclusively assumes convexity. contrast prove non-asymptotic rates convergence svrg nonconvex optimization show provably faster gradient descent. also analyze subclass nonconvex problems svrg attains linear convergence global optimum. extend analysis mini-batch variants svrg showing linear speedup mini-batching parallel settings. table table comparing complexity diﬀerent algorithms discussed paper. complexity measured terms number oracle calls required achieve \u0001-accurate solution here ﬁxed step size mean step size algorithm ﬁxed dependent complexity gradient dominated functions refers number calls required obtain \u0001-accurate solution -gradient dominated function aware speciﬁc results gradient dominated functions. also empirical risk minimization typically uses convex ﬁnite-sum models; deep learning uses nonconvex ones. prototypical algorithm stochastic gradient descent witnessed tremendous progress recent years. variety accelerated parallel faster converging versions known. among these particular importance variance reduced stochastic methods delivered exciting progress linear convergence rates opposed sublinear rates ordinary similar beneﬁts methods also seen smooth convex functions. svrg algorithm particularly attractive storage requirement comparison algorithms convex provide compelling experimental results however problems encountered practice typically even locally convex alone strongly convex. current analysis svrg extend nonconvex functions relies heavily convexity controlling variance. given dominance stochastic gradient methods optimizing deep neural nets large nonconvex models theoretical investigation faster nonconvex stochastic methods much needed. convex methods known enjoy faster convergence rate gradientdescent much weaker dependence without compromising rate like sgd. however clear beneﬁts carry beyond convex problems prompting central question paper analyze nonconvex stochastic variance reduced gradient prove faster rates convergence gradientdescent ordinary sgd. show svrg faster gradientdescent factor nesterov polyak propose variant svrg attains global linear rate convergence. improve upon many prior results subclass functions best knowledge ﬁrst work shows stochastic method linear convergence gradient dominated functions. speciﬁcally show theoretical linear speedups parallel settings large mini-batch sizes. using mini-batch size show mini-batch nonconvex svrg faster factor aware prior work mini-batch ﬁrst-order stochastic methods shows linear speedup parallel settings nonconvex optimization. convex. bertsekas surveys several incremental gradient methods convex problems. reference stochastic convex optimization faster rates convergence attained problems methods e.g. asynchronous frameworks developed agarwal bottou zhou study lower-bounds convex ﬁnite-sum problems. shalevshwartz prove linear convergence stochastic dual coordinate ascent individual nonconvex strongly convex. study general nonconvex case. nonconvex. dates least seminal work since developed several directions ﬁnite-sum setting considers proximal splitting methods analyzes asymptotic convergence nonvanishing gradient errors. hong studies distributed nonconvex incremental admm algorithm. works however prove expected convergence stationary points often lack analysis rates. ﬁrst nonasymptotic convergence rate analysis show ensures iterations. similar rate parallel distributed shown recently gradientdescent known ensure iterations ﬁrst analysis nonconvex svrg seems shamir considers special problem computing leading eigenvectors also follow work finally note another interesting example stochastic optimization locally quasi-convex functions wherein actually convergence function value shown. general typically assumed similar magnitude. throughout analysis assume constant report dependence results. analysis need following deﬁnition. uses unbiased estimate gradient iteration. appropriate conditions ghadimi establish convergence rate stationary point results include following theorem. turn focus variance reduced methods. svrg algorithm recently shown eﬀective reducing variance convex problems. result gained considerable interest machine learning optimization communities. seek understand beneﬁts nonconvex optimization. reference algorithm presents svrg’s pseudocode. observe algorithm operates epochs. epoch full gradient calculated point requiring calls ifo. within inner loop svrg performs stochastic updates. total number calls epoch thus algorithm reduces classic gradientdescent algorithm. suppose chosen total calls epoch enable fair comparison assume total number inner iterations across epochs algorithm also note simple important implementation detail written algorithm requires storing storage avoided keeping running average respect iterates probability distribution {pi}m algorithm attains linear convergence strongly convex nonstrongly convex functions rates faster shown using indirect perturbation argument—see e.g. furthermore also show nonconvex svrg exhibits expected descent every epoch. condition multiple solely convenience removed slight modiﬁcation theorem statement. note value depend obtain explicit dependence simplify using speciﬁc choices formalized below. ending discussion convergence nonconvex svrg prove linear convergence rate class -gradient dominated functions ease exposition assume property analogous high condition number regime strongly convex functions typical machine learning. note gradient dominated functions nonconvex. note gradientdescent also achieve linear convergence rate gradient dominated functions however gradientdescent requires calls obtain \u0001-accurate solution opposed svrg. similar gains seen svrg strongly convex functions also notice assume anything except smoothness individual functions results. particular following corollary also immediate consequence. recall denotes condition number λ-strongly convex function. corollary follows corollary upon noting λ-strongly convex function /λ-gradient dominated. theorem generalizes linear convergence result since allows learning. precisely l-regularized empirical loss minimization shalev-shwartz show sdca requires iterations fi’s possibly nonconvex strongly convex. comparison show algorithm requires log) iterations improvement sdca previous section showed nonconvex svrg converges stationary point rate natural question whether rate improved assume convexity? provide aﬃrmative answer. non-strongly convex functions yields direct analysis svrg. state results terms stationarity ease comparison analysis also provides rates respect optimality theorem convex results follow corollary corollary noting total calls made algorithm instructive quantitatively compare corollary corollary. step size independent convergence rate svrg dependence order dependence reduced either carefully selecting step size diminishes using good initial point obtained running iterations sgd. emphasize convergence rate convex case improved signiﬁcantly slightly modifying algorithm using choice changes epoch however clear strategies provide theoretical gains general nonconvex case. section study mini-batch version algorithm mini-batching popular strategy especially multicore distributed settings greatly helps exploit parallelism reduce communication costs. pseudocode mini-batch nonconvex svrg provided supplement lack space. diﬀerence mini-batch svrg algorithm lies lines mini-batches replace line sampling mini-batch size lines replaced following updates reduces algorithm mini-batch typically used reduce variance stochastic gradient increase parallelism. lemma shows reduction variance stochastic gradients mini-batch size using lemma derive mini-batch equivalents lemma theorem theorem however sake brevity directly state following main result mini-batch svrg. important compare result mini-batched sgd. batch size obtains rate calls achieving \u0001-accurate solution thus performance degrades mini-batching. section give comprehensive comparison results obtained paper. particular compare aspects convergence rates gradientdescent svrg. comparison based complexity achieve \u0001-accurate solution. dependence number calls svrg gradientdescent depend explicitly contrast number oracle calls independent however comes expense worse dependence number calls gradientdescent proportional svrg dependence reduces convex nonconvex problems. whether diﬀerence dependence nonconvexity artifact analysis interesting open problem. dependence dependence follows convergence rates algorithms. seen depend regardless convexity nonconvexity. contrast convex nonconvex settings svrg gradientdescent converge furthermore gradient dominated functions svrg gradientdescent global linear convergence. speedup convergence especially signiﬁcant medium high accuracy solutions required assumptions used analysis important understand assumptions used deriving convergence rates. algorithms assume lipschitz continuous gradients. however requires additional subtle important assumptions σ-bounded gradients advance knowledge hand svrg gradientdescent require assumptions thus ﬂexible. step size learning rates valuable compare step sizes used algorithms. step sizes shrink number iterations increases—an undesirable property. hand step sizes svrg gradientdescent independent hence algorithms executed ﬁxed step size. however svrg uses step sizes depend step size independent used svrg convex albeit cost worse dependence gradientdescent issue step size independent dependence initial point mini-batch svrg sensitive initial point comparison sgd. seen comparing corollary theorem hence important good initial point svrg. similarly good mini-batch beneﬁcial svrg. moreover mini-batches provides parallelism also good theoretical guarantees contrast performance gain mini-batches pronounced seen previous section svrg combines beneﬁts gradientdescent sgd. show beneﬁts svrg made pronounced appropriate step size additional assumptions. case complexity svrg lower gradientdescent. variant svrg chooses step size based total number iterations discussion below assume figure neural network results cifar- mnist stl- datasets. represents results cifar- dataset. bottom left middle ﬁgures represent results mnist dataset. bottom right ﬁgure represents result stl-. msvrg convergence rate faster svrg though beneﬁt without cost. msvrg contrast svrg uses additional assumption σ-bounded gradients. furthermore step size ﬁxed since depends number iterations often diﬃcult practice compute step size msvrg typical multiple step sizes choose best results. present empirical results section. experiments study problem multiclass classiﬁcation using neural networks. typical nonconvex problem encountered machine learning. experimental setup. train neural networks fully-connected hidden layer nodes softmax output nodes. -regularization training. cifar- mnist stl- datasets experiments. datasets standard neural networks literature. regularization cifar- mnist stl-. features datasets normalized interval datasets come predeﬁned split training test datasets. performance training loss. experiments also results ﬁxed step size sgd. svrg ﬁxed step size suggested analysis. again step size chosen svrg gives best performance training loss. svrg iterations cifar- minst iterations stl- running algorithm initialization standard variance reduced schemes even convex problems noted earlier section svrg sensitive initial point initialization typically helpful. mini-batches size experiments. mini-batches common training neural networks. note mini-batch training especially beneﬁcial svrg shown analysis section along lines theoretical analysis provided theorem epoch size experiments. algorithms compare criteria number eﬀective passes data i.e. calls divided includes cost calculating full gradient epoch svrg. initialization svrg mini-batching svrg plots start x-axis value cifar- mnist stl-. figure shows results experiment. seen svrg lower compared suggesting faster convergence stationary point. furthermore training loss also lower compared datasets. notably test error cifar- lower svrg indicating better generalization; notice substantial diﬀerence test error mnist stl- overall results network hidden layer promising; interesting study svrg deep neural networks future. paper examined scheme nonconvex optimization. showed employing stochastic methods perform better gradientdescent context nonconvex optimization. function gradient dominated proposed variant svrg linear convergence global minimum. analysis shows svrg number interesting properties include convergence ﬁxed step size descent property every epoch; property need hold sgd. also showed svrg contrast enjoys eﬃcient mini-batching attaining speedups linear size minibatches parallel settings. analysis also reveals initial point mini-batches important svrg. concluding paper would like discuss implications work caveats. exercise caution interpreting results paper. theoretical results based stationarity gap. general necessarily translate optimality training loss test error. criticism schemes nonconvex optimization general wisdom variance stochastic gradients actually help escape local minimum saddle points. fact additional noise stochastic gradient order escape saddle points. however reap beneﬁt schemes even scenarios. example envision algorithm uses exploration tool obtain good initial point uses algorithm exploitation tool quickly converge good local minimum. either case believe variance reduction used important tool alongside tools like momentum adaptive learning rates faster better nonconvex optimization.", "year": 2016}