{"title": "Multimodal sparse representation learning and applications", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Unsupervised methods have proven effective for discriminative tasks in a single-modality scenario. In this paper, we present a multimodal framework for learning sparse representations that can capture semantic correlation between modalities. The framework can model relationships at a higher level by forcing the shared sparse representation. In particular, we propose the use of joint dictionary learning technique for sparse coding and formulate the joint representation for concision, cross-modal representations (in case of a missing modality), and union of the cross-modal representations. Given the accelerated growth of multimodal data posted on the Web such as YouTube, Wikipedia, and Twitter, learning good multimodal features is becoming increasingly important. We show that the shared representations enabled by our framework substantially improve the classification performance under both unimodal and multimodal settings. We further show how deep architectures built on the proposed framework are effective for the case of highly nonlinear correlations between modalities. The effectiveness of our approach is demonstrated experimentally in image denoising, multimedia event detection and retrieval on the TRECVID dataset (audio-video), category classification on the Wikipedia dataset (image-text), and sentiment classification on PhotoTweet (image-text).", "text": "unsupervised methods proven effective discriminative tasks singlemodality scenario. paper present multimodal framework learning sparse representations capture semantic correlation modalities. framework model relationships higher level forcing shared sparse representation. particular propose joint dictionary learning technique sparse coding formulate joint representation concision cross-modal representations union cross-modal representations. given accelerated growth multimodal data posted youtube wikipedia twitter learning good multimodal features becoming increasingly important. show shared representations enabled framework substantially improve classiﬁcation performance unimodal multimodal settings. show deep architectures built proposed framework effective case highly nonlinear correlations modalities. effectiveness approach demonstrated experimentally image denoising multimedia event detection retrieval trecvid dataset category classiﬁcation wikipedia dataset sentiment classiﬁcation phototweet human perception works integrating multiple sensory inputs. processing different sensory modalities correlating improve perceptual abilities numerous ways different phenomena cause ambiguity activating similar features modality features modalities examined. modality impaired becomes corrupted modalities help missing information robustness. finally consensus among modalities taken reinforcing factor. multiple modalities believed beneﬁt discriminative machine learning tasks. using different sensors simultaneously scene event described multiple data modalities. example consider multimedia event detection problem class names show firework playing fetch dogs shooting gun. judging based video modality only show playing fetch dogs come close featuring people dogs coordinated actions. however classes easier discriminate incorporating audio modality show characterized crowd noise microphone announcement absent playing fetch dog. hand firework shooting hard discriminate audio visual differences useful. multimodal feature learning wishes learn good shared representations across heterogeneous data modalities. form union unimodal features learning features modality separately. approach however drawback unable learn patterns occur jointly selectively across modalities since unimodal learning emphasizes relating information within modality. paper present sparse coding framework model correlation modalities. framework aims learn relationships higher level forcing share sparse representation. sparse coding recognized widely machine learning applications classiﬁcation denoising recognition particular known multimedia data well-represented sparse linear combination basis vectors. example abundance unlabeled photos makes good large dictionaries readily available sparse coding. similar learning methods sparse coding primarily applied unimodal settings. however recognize numerous multimodal approaches sparse coding recent literature. approaches common learn shared sparse representation different modalities. exploit structured sparsity learn shared latent space multi-view data monaci propose sparse coding-based scheme learns bimodal structure audio-visual speech data. additionally zhuang describe supervised sparse coding scheme cross-modal retrieval contributions two-fold. first experimental deep architecture built multiple layers sparse coding pooling units. this report promising results classiﬁcation multimodal datasets. secondly demonstrate performance multimodal sparse coding comprehensive applications. particular include result trecvid tasks detecting high-level complex events user-generated videos. examine various settings multimodal sparse coding using several multimodal datasets semantically correlated pairs semantic correlation reveals shared statistical association modalities thus provide complementary information other. existing multimodal learning schemes sparse coding-based. include audio-visual speech recognition sentiment recognition image-text retrieval ngiam applied deep stacked autoencoder learn representations speech audio coupled video lips. poirson idrees used denoising autoencoder flickr photos associated text. following sections cover basic principle sparse coding extend build multimodal framework shallow deep architectures. demonstrate effectiveness approach experimentally multimedia applications include image denoising categorical classiﬁcation images text wikipedia sentiment classiﬁcation using phototweet trecvid med. originated explain neuronal activations encode sensory information sparse coding unsupervised method learn efﬁcient representation data using small number basis vectors. used discover higher-level features data unlabeled examples. given data input sparse coding solves representation simultaneously updating dictionary rn×k basis vectors dictionary atom regularization parameter penalizes norm induces sparse solution. sparse coding typically trains overcomplete dictionary. makes sparse code higher dimension elements nonzero. sparse coding alternatively regularize pseudo-norm. finding sparsest solution general however known intractable. although greedy- methods orthogonal matching pursuit used consider equation choice sparse coding throughout paper. least angle regression dictionary learning algorithm mairal spams toolbox section describes multimodal feature learning schemes sparse coding. schemes general readily extended modalities. clarity explanation modalities throughout section. straightforward approach sparse coding heterogeneous modalities learn separate dictionary basis vectors modality. figure depicts unimodal sparse coding schemes modalities learn dictionaries parallel unimodal sparse coding takes unlabeled examples computing corresponding sparse code training example.) similarly modality train unlabeled examples computing union unimodal sparse codes ya+b simple encapsulate features modalities. however unimodal training model ﬂawed since cannot capture correlations modalities could beneﬁcial inference tasks. remedy lack joint learning propose multimodal sparse coding scheme illustrated figure joint principle joint sparse coding equation combines objectives equations ideally forcing sparse codes could ab-b although empirical values determined three different optimizations would differ reality. according yang highly correlated high resolution images originated source. however come different modalities correlation present semantics. thus equality assumption among ab-b even less likely met. reason introduce cross-modal sparse coding captures weak correlation heterogeneous modalities resulting sparse code discriminative. cross-modal sparse coding ﬁrst trains joint dictionary dab. test time cross-modal sparse codes computed using sub-dictionaries dab-a dab-b. various feature formation possibilities multimodal sparse coding—joint cross-modal union cross-modal sparse codes explained figure considered shallow learning architectures using single layer sparse coding dictionary learning. shallow architecture capable learning features jointly sufﬁcient capture complex semantic correlations modalities fully. expect modalities high semantic correlation stable hierarchical architecture utilize large number hidden layers parameters extract meaningful highlevel representation modalities. composing higher-level representation using low-level features advantageous contextual data human language speech audio sequence image patterns. hierarchical composition sparse codes shown help unveil separability data invisible lower-level features unimodal settings therefore consider deep architectures multimodal sparse coding. figure propose possible architectures. ngiam report rbm-based approaches beneﬁcial applying deep learning modality joint training. adopting conﬁguration layers sparse coding modality followed joint sparse coding illustrated figure write two-layer sparse coding modality denote daii dictionaries learned sparse-coding layers unpooled sparse codes yaii hidden activations haii pooling sparse codes. pooling factors refer number sparse codes aggregated pooled representation. determined empirically. since sparse coding takes dense input vectors produces sparse output vectors poor multilayering. hence interlace nonlinear pooling units sparse coding layers aggregate multiple sparse vectors nonlinearly dense vector passing onto next layer. similar modality work dbii ybii hbii modality ultimately joint feature representation figure learned moreover consider additional joint sparse coding layer illustrated figure again form dense hidden activations habi pooling multiple vectors. compute deeper representation ﬁrst experiment evaluate image denoising problem zero-mean gaussian noise removed given image. proposed learning algorithm used jointly learn associations clean noisy images. consider clean noisy counterpart input modalities recover clean image noisy one. randomly select images cifar- zero-mean gaussian noise range standard deviation generate noisy images. pairs clean noisy images training joint dictionary pairs testing. test input noisy image compute cross-modal sparse code recover clean estimate. compare performance joint multimodal sparse coding denoising autoencoder table summarizes denoising results joint sparse coding dae. experiment dictionaries used size designed handle clean noisy image patches size pixels every result reported average experiments. different realization noise joint sparse coding improves quality noisy images often achieves comparable results. figure visualize generated clean images noisy images notice joint sparse coding learn shared association clean noisy images achieve comparable results. section apply multimodal sparse coding schemes multimedia event detection tasks. aims identify complex activities encompassing various human actions objects interactions different places time. considered difﬁcult concept analysis action recognition received signiﬁcant attention computer vision machine learning research. dataset tasks metrics. trecvid dataset evaluate schemes. consider event detection retrieval tasks using data scenarios gives multimedia examples event examples event event classes event names bike trick show marriage proposal. evaluate peformance trained features learned unimodal multimodal sparse coding schemes. particular compute classiﬁcation accuracy mean average precision metrics according nist standard following experiments cross-validation train using test data processing. processing efﬁciency scalability keyframe-based feature extraction audio-video data. apply simple two-pass algorithm computes color histogram difference successive frames determines keyframe candidate based threshold calculated mean standard deviation histogram differences. examine number different colors present keyframe candidates discard ones less colors ensure non-blank keyframes. around keyframe extract -sec audio data additional uniformly sampled video frames within duration illustrated figure extracted audio stereo take left channel. audio waveform resampled regularized time-frequency automatic gain control balance energy sub-bands. form audio frames using -msec hann window overlap successive frames smoothing. frame compute mel-frequency cepstral coefﬁcients low-level audio feature. addition append delta cepstral delta-delta cepstral coefﬁcients make lowlevel audio feature vectors dimensional. apply whitening unsupervised learning. complete audio preprocessing steps described figure video preprocessing tried pretrained convolutional neural network models ended choosing ilsvrc layers university oxford’s visual geometry group imagenet large-scale visual recognition challenge depicted figure feedforward passes extracted video frames take -dimensional hidden activation highest hidden layer ﬁnal relu. whitening reduce dimensionality feature learning med. build feature vectors sparse coding preprocessed audio video data. number basis vectors dictionaries unimodal multimodal sparse coding schemes. aggregate sparse codes around keyframe training example pooling form feature vectors classiﬁcation. train linear -vs-all classiﬁers event whose hyper-parameters determined -fold cross-validation feature learning schemes comparison. consider unsupervised methods learn audio-video features comparison. report results gaussian mixture model restricted boltzmann machine similar unimodal multimodal settings. expectation-maximization preprocessed input vectors audio-only videoonly concatenated audio-video mixtures form supervectors containing posterior probabilities respect gaussian. max-pooled supervectors used train svms. adopt shallow bimodal pretraining model ngiam rbm. fairness hidden layer size max-pooled activations used train svms. target sparsity applied rbm. results. table presents classiﬁcation accuracy performance unimodal multimodal sparse coding schemes. ex/ex experiment used best parameter setting cross-validation test examples. general observe union unimodal audio video feature vectors perform better using unimodal cross-modal features. multimodal union scheme performs better joint schemes. union schemes however double feature dimensionality since union operation concatenates feature vectors. joint feature vector economical combining audio video features keeping dimension audio-only video-only. table report mean accuracy union joint feature learning schemes ex/ex experiment. results show sparse coding better accuracy map. however performance sparse coding. leaves good next step develop joint feature learning scheme rbm. article wikipedia dataset contains paragraphs text subject corresponding picture. text-image pairs annotated label categories biology geography history literature media music royalty sport warfare. corpus contains total documents. split documents folds. along dataset authors supplied features images text. image represented histogram codeword sift features text represented histogram -topic latent dirichlet allocation model sift features images features text input sparse coding algorithms train -vs-all svms. that predict labels test image-text pairs report accuracy. experiment used table reports results various feature representations. comparison unimodal sparse coding images text alone text features outperform image features. wikipedia category membership mostly driven text. categorization solely based image ambiguous difﬁcult even human. thus expected image features would lower accuracy text features. union unimodal image text features improves accuracy. joint sparse coding able learn multimodal features beyond simply concatenating unimodal features. notice learning shared association image text improve classiﬁcation accuracy even single modality available training testing. cross-modal images improves accuracy compared unimodal sparse coding images cross-modal text achieves higher accuracy unimodal sparse coding text cross-modal features images text concatenated outperforms feature combinations. feature representations multimodal features signiﬁcantly outperform unimodal features. shows learned shared association multiple modalities useful cases single multiple modalities available training testing. figure compares classiﬁcation accuracies joint sparse coding cross-modal text multimodal feature union categories. although multimodal feature feature representation sparse coding images sparse coding text unimodal feature union joint sparse coding cross-modal images cross-modal text multimodal feature union phototweet dataset includes twitter messages associated photos. benchmark designed twitter user’s sentiment prediction using visual textual features. phototweet collected november peoplebrowsr api. ground truths labels obtained amazon mechanic turk annotation resulting positive negative sentiments. authors dataset partitioned dataset folds cross-validation. represent textual data using binary bag-of-words embedding producing dimensions vector. process embedded tweet data consecutive patches conﬁgurable size dimension resulting patches reduced dimensions whitening. images consider per-image feature vector non-overlapping patches drawn receptive ﬁeld width pixels. thus patch size unsupervised learning precondition visual textual patches mean removal whitening sparse coding. used dictionary size unimodal multimodal settings. pooling pooling factors table present sentiment classiﬁcation performances using linear -vs-all svm. compare performances sparse coding methods discussed section twitter sentiment classiﬁcation task image text features alone equally useful. thus image features complement text features unimodal feature union achieving shared representation learned unsupervised learning stage helps increase classiﬁcation performance cross-modal features compared unimodal features. best feature shallow model multimodal feature union finally improve performance building deep representations model correlation across learned shallow representations. tried architectures figures obtained result summarized better table feature representation sparse coding images sparse coding text unimodal feature union joint sparse coding cross-modal images cross-modal text multimodal feature union deep multimodal sparse coding table presents summary compares sentiment classiﬁcation performances approach previous work. authors phototweet dataset combination sentistrength textual features sentibank mid-level visual features. combined method simply concatenates features sentistrength sentibank learn shared association modalities. notice multimodal feature union outperforms sentistrength+sentibank emphasizing importance shared learning across multiple modalities. baecchi extension continuous bag-of-words model text denoising autoencoder images. again textual visual features concatenated. compare method hierarchical learning deep multimodal sparse coding show method yields better classiﬁcation result. presented multimodal sparse coding algorithms model semantic correlation between modalities. shown multimodal features signiﬁcantly outperform unimodal features. experimental results also indicate multimodal features learned algorithms discriminative feature formed concatenating multiple unimodal features. addition cross-modal features computed using modality also lead better performance unimodal features. suggests learn better features modality joint learning multiple modalities. effectiveness approach demonstrated various multimedia applications image denoising category sentiment classiﬁcation. papandreou katsamanis pitsikalis maragos multimodal fusion learning uncertain features applied audiovisual speech recognition. ieee workshop multimedia signal processing", "year": 2015}