{"title": "Learning from Scarce Experience", "tag": ["cs.AI", "cs.LG", "cs.NE", "cs.RO", "I.2; I.2.8; I.2.11; I.2.6; G.1.6"], "abstract": "Searching the space of policies directly for the optimal policy has been one popular method for solving partially observable reinforcement learning problems. Typically, with each change of the target policy, its value is estimated from the results of following that very policy. This requires a large number of interactions with the environment as different polices are considered. We present a family of algorithms based on likelihood ratio estimation that use data gathered when executing one policy (or collection of policies) to estimate the value of a different policy. The algorithms combine estimation and optimization stages. The former utilizes experience to build a non-parametric representation of an optimized function. The latter performs optimization on this estimate. We show positive empirical results and provide the sample complexity bound.", "text": "dient descent considered variety policy classes commonly recognized shortcoming variations gradient descent policy search require large number samples converge. inefﬁciency arises value policy estimated sampling returns obtained following policy. thus policy evaluated proposed samples taken policy must discarded. step policy search algorithm requires samples. solving inefﬁciency data gathered using policy estimate value another policy. method known likelihood ratio estimation enables data reuse. stochastic gradient methods likelihood ratios long used optimization problems recently stochastic gradient descent methods particular reinforce used conjunction policy classes constrained various ways external memory ﬁnite state controllers multi-agent settings idea using likelihood ratios reinforcement learning suggested developed solving mdps function approximation gradient descent ﬁnite state controllers however on-line optimization consid developed greedy algorithm combining ered. samples multiple policies normalized estimators demonstrated dramatic improvement performance. showed likelihood-ratio estimation enables application methods statistical learning theory derive bounds sample complexity. provide method estimating return every policy simultaneously using data gathered executing ﬁxed policy without likelihood ratios. domains natural distance observations actions also allows re-use exsearching space policies directly optimal policy popular method solving partially observable reinforcement learning problems. typically change target policy value estimated results following policy. requires large number interactions environment different polices considered. present family algorithms based likelihood ratio estimation data gathered executing policy estimate value different policy. algorithms combine estimation optimization stages. former utilizes experience build non-parametric representation optimized function. latter performs optimization estimate. show positive empirical results provide sample complexity bound. research reinforcement learning focuses designing algorithms agent interacting environment adjust behavior optimize long-term return. environments fully-observable problem often solved using one-step look ahead analysis formulate solution dynamic programming problem. however case partially observable domains perceptual aliasing observations makes methods infeasible. function parameterized vector policy class policies realizable parameter settings. assume probability elementary event bounded away zero history includes several immediate rewards typically summed form return results independent method used compute return. together distributions deﬁned pomdp policy deﬁnes conditional distribution class histories value policy expected return according probability induced policy history space assume policy values non-negative bounded vmax. objective agent policy optimal value argmaxθv because agent model environment’s dynamics reward function calculate must estimate sampling. wish estimate value policy draw sample histories distribution induced policy executing policy multiple times environment. taking samples {hi} unbiased estimator imagine however unable sample policy directly instead samples another policy intuition knew similar policies another could samples drawn according distribution make adjustment proportional similarity policies. formally have paper extends previous work presenting generalized method using likelihood ratio estimation policy search investigating performance method different conditions illustrative examples. publication hope stimulate dialog communities reinforcement learning computational learning theory. present clear outline algorithms hope attract wider research community applying algorithms various domains. also present bounds sample complexity algorithms making attempt relate results empirical results. begin paper brief deﬁnition reinforcement learning sampling order clarify notation. present algorithm consider question sample. finally consider question much sample present pac-style bound quantitative answer. introduce environment model importance sampling single mathematical notation. particular keep standard notation partially observable markov decision processes modify sampling notation consistent. class problems consider described partially observable markov decision process model. pomdp sequence events occur time step agent observes observation dependent state environment performs action according policy inducing state transition environment; receives reward based action taken environment’s state. pomdp deﬁned four probability distributions distribution starting states distribution observations conditioned state distribution next states conditioned current state agent’s action distribution rewards given state action. distributions specifying dynamics environment unknown agent along state space process denote possible experiences sequences length generally speaking pomdp policy function specifying action perform time step function whole previous history table.the sample routine accepts policy parameter setting outputs return history sample policy. note many cases entire history need returned. note estimators contain quantity ratio likelihoods. observation remainder paper agent assumed model environment therefore able calculate able calculate likelihood ratio policies written product pr|o contribution agent’s actions likelihood history contribution environmental events. component independent policy cancels ratio depends agent observations actions known agent computed differentiated. allows construct efﬁcient learning algorithms take advantage past experience. finally sampling distribution constant single unbiased importance sampling estimator constructed using samples assumed single sampling distribution mixture true sampling distributions. thus samples taken according policies replaced tance sampling estimators independent identically drawn samples. using estimator allows change policies sampling. consider constructing proxy environment contains non-parametric model values policies illustrated figure model result trying several policies given arbitrary policy proxy environment returns estimate value policy tried real environment. assuming obtaining sample environment costly want construct proxy module based small number queries policies {θi} return values queries implemented sample routine getting samples requires memory size store data length trial sets possible observations action respectively. however many policy classes memory requirement reduced. example policy class reactive history summarized sufﬁciently counts number times action chosen observation. requires memory size o|o||a|). figure.a diagram policy evaluation process. sampling process costly therefore performed limited number times. proxy collects samples environment constructs agent-centric model predicts effects hypothetical agent policies. agent learns interacting proxy. proxy queried learning algorithm shown table response policy parameter settings routine evaluate returns estimate expected return derivative. algorithm shown table computes weighted importance sampling estimate. simplicity inner loop shown. practice computations loop need redone every evaluation. using memory size values computed ahead time thus reducing evaluation time. evaluate routine relies routines calculate calculate derivative recall policy’s factor probability history example assume policy reactive parameter probability selecting action observing count number times action chosen observing history policy search algorithm combined proxy environment learn scarce experience. table shows general reinforcement learning algorithm family using proxy. deﬁnitions pick sample data optimize crucial behavior algorithm. reinforce algorithm particular instantiation learn routine pick sample returns without consulting data table.the learn routine accepts number trials allowed returns guess optimal policy. relies four external routines pick sample selects policy sample given data current best guess sample shown table data adds data point data collected optimize performs form optimization proxy evaluation function. data forgets previous data replaces recent sample optimize performs step gradient descent exploration extension reinforce proposed exactly except pick sample routine returns policy mixture random policy. order make effective data deﬁne data append data sample collection data. allows algorithm remember previous experience. additionally optimize routine performs full optimization reinforce policy search methods current policy guess embodies known information past samples. therefore important take small steps decreasing size insure algorithm converges. remember previous samples restraint policy must next sample search true optimum estimator every step. routine pick sample. routine represents balance between exploration exploitation. paper consider simple possibility illustrate trade-off. pick sample routine single parameter pick sample stochastic probability returns remainder time returns random policy chosen uniformly space thus larger value exploitative algorithm learn algorithm table different settings parameters figure shows true value resulting policies averaged runs algorithm. plots look discouraging remember problems ways worse-case situation. true value actions becomes apparent sampling order times. plots support hypothesis relative success exploitation. however although acting greedily somewhat better much worse illustrates withprior knowledge domain given limited number samples important guide sampling much optimization. wish guarantee probability error estimate value function less derive bounds necessary sample size depend vmax complexity hypothesis class expressed covering number result extension sample complexity bound estimator estimator. quote results here. point derivation fact gives sample complexity bound similar obtained well known weak dependence horizon interesting accordance empirical ﬁndings. covering number deﬁned value describes complexity policy class consider problems called expected returns actions always returns returns probability probability always returns returns probability probability would expect greedy learning algorithm sample near policies look better scarce information tending choose sub-optimal problem. strategy inferior blind sampling samples uniformly policy space discover hidden treasure faster. contrast figure.diagram load-unload world. agent observes position whether cart loaded cart loads left-most state. reaches right-most position loaded unloads gets unit reward. agent choice moving left right position. trial begins load state lasts steps. optimal controller requires memory. ﬁnite-state controller class policies ﬁxed memory size. controller internal memory state restricted ﬁnite number values. time step selects action take also memory state next time step. controller’s choice action next memory state independent past given current observation memory state. model extension reactive policy class allow controller remember small amount past. finite-state controllers capability remembering information arbitrarily long period time. figure demonstrates effect policy complexity performance algorithm. plot ones ﬁgure except exploitation probability ﬁxed four lines depict results different policy classes solid line reactive policies whereas dashed dotted lines ﬁnite-state controllers varying amounts memory. memory required perform optimally environment. using states memory superﬂuous. simpler policy class quickly algorithm converges. however simple policy class convergence suboptimal policy. comparison thin dashed line presents bepioneering work considers issue generating enough information determine near-best policy. compare sample complexity results similar result reusable trajectories algorithm. using random policy reusable trajectories generates history trees. information used deﬁne estimates uniformly converge true values. algorithm relies generative model environment allows simulation reset environment state execution action sample immediate reward. reuse information partial estimate policy value built subset experiences consistent estimated policy. substituting expression compare bounds bound presented table metric entropy takes place dimension terms policy class complexity. metric entropy reﬁned measure capacity dimension; dimension upper bound growth function upper bound metric entropy complexity problem measured covering number encodes complexity combination pomdp policy class. loadunload problem ﬁgure illustrate effect policy complexity. agent cart designed shuttle loads back forth end-points line. cart sensors indicate whether loaded unloaded determine position line. optimal policy cart moves back forth leftmost rightmost states moving many measure complexity policy space. estimating covering number challenging problem itself. however would desirable constructive solution covering problem sense universal prediction theory obviously given covering number might several ways cover space. finding covering would equivalent reducing global optimization problem evaluation several representative policies. another sample complexity results minimal experience necessary able provide estimate policy class given conﬁdence. would similar structural risk minimization principal vapnik intuition given limited data might prefer search primitive class hypotheses high conﬁdence rather lost sophisticated class hypotheses conﬁdence. authors would like thank leslie kaelbling helpful discussions comments manuscript. c.s. supported grants contracts nos. n--- contracts nos. dms- contract n--- muri program decision making uncertainty stanford. l.p. supported grant daa---.", "year": 2002}