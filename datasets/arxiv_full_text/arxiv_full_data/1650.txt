{"title": "Deep Transfer in Reinforcement Learning by Language Grounding", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "In this paper, we explore the utilization of natural language to drive transfer for reinforcement learning (RL). Despite the wide-spread application of deep RL techniques, learning generalized policy representations that work across domains remains a challenging problem. We demonstrate that textual descriptions of environments provide a compact intermediate channel to facilitate effective policy transfer. We employ a model-based RL approach consisting of a differentiable planning module, a model-free component and a factorized representation to effectively utilize entity descriptions. Our model outperforms prior work on both transfer and multi-task scenarios in a variety of different environments.", "text": "example consider game environments figure games boulderchase bomberman differ layouts entity types. however high-level behavior entities games similar. instance scorpion boulderchase moving entity agent avoid similar spider bomberman though similarity clearly reﬂected text descriptions figure take multiple environment interactions discover. therefore exploiting textual clues could help autonomous agent understand connection effectively leading faster policy learning. paper explore utilization natural language drive transfer reinforcement learning despite wide-spread application deep techniques learning generalized policy representations work across domains remains challenging problem. demonstrate textual descriptions environments provide compact intermediate channel facilitate effective policy transfer. employ model-based approach consisting differentiable planning module model-free component factorized representation effectively utilize entity descriptions. model outperforms prior work transfer multi-task scenarios variety different environments. learning emerged deep reinforcement method choice many control applications ranging computer games robotics however success approach depends substantial number interactions environment training easily reaching millions steps moreover given task even related training process performed scratch. inefﬁciency motivated recent work learning universal policies generalize across related tasks well transfer approaches paper explore transfer methods vironment. describe control strategies commonly used prior work grounding instead specify dynamics environments conducive crossdomain transfer. order effectively utilize type information employ model-based reinforcement learning approach. typically representations environment learned approaches inherently domain speciﬁc. address issue using natural language implicit intermediate channel transfer. speciﬁcally model learns text descriptions transitions rewards environment capability speeds learning unseen domains. induce two-part representation input state generalizes domains incorporating domain-speciﬁc information textual knowledge. representation utilized action-value function parametrized single deep neural network differentiable value iteration module entire model trained end-to-end using rewards environment. evaluate model several game worlds gvgai framework ﬁrst evaluation scenario transfer learning agent trained source tasks learning performance evaluated different target tasks. across multiple evaluation metrics method consistently outperforms several baselines existing transfer approach called actor mimic instance model achieves higher average reward higher jumpstart reward. also evaluate multi-task setting learning simultaneously performed multiple environments. case obtain gains average asymptotic reward respectively. grounding language interactive environments recent years increasing interest systems utilize textual knowledge learn control policies. applications include interpreting help documentation inwork represents departures prior work grounding. first rather optimizing control performance single domain interested multi-domain transfer scenario language descriptions drive generalization. second prior work uses text form strategy advice directly learn policy. since policies typically optimized speciﬁc task harder transfer across domains. instead utilize text bootstrap induction environment dynamics moving beyond task-speciﬁc strategies. another related line work consists systems learn spatial topographical maps environment robot navigation using natural language descriptions approaches text mainly containing appearance positional information integrate semantic sources obtain accurate maps. contrast work uses language describing dynamics environment entity movements interactions complementary positional information received state observations. further goal help agent learn policies generalize different stochastic domains work considers single domain. transfer reinforcement learning transferring policies across domains challenging problem reinforcement learning main hurdle lies learning good mapping state action spaces different domains enable effective transfer. previous approaches either explored skill transfer value function transfer attempts model-based transfer methods either rely hand-coded inter-task mappings state actions spaces require signiﬁcant interactions q-function predicts expected future reward choosing action state straightforward policy simply choose action maximizes q-value current state maxa also make descriptions text-conditioned policy maxa successful control policy environment contain knowledge environment dynamics capability identify goal states. latter task-speciﬁc former characteristic useful learning general policy transfers different domains. based hypothesis employ model-aware approach learn dynamics world estimating optimal speciﬁcally make value iteration algorithm based dynamic programming. update equations follows text descriptions estimating dynamics environment interactive experience require signiﬁcant number samples. main hypothesis agent derive information dynamics text descriptions determine faster accurately. instance consider sentence moves horizontally left right.. talks movement third-party entity independent agent’s goal. provided agent learn interpret sentence infer direction movement different entity train deep network mimic pretrained experts source tasks using policy distillation. learned parameters used initialize network target task perform transfer. rusu perform transfer freezing parameters learned source tasks adding parameters every target task using sets learn policy. work uses attention networks selectively transfer expert policies task. approach orthogonal since text bootstrap transfer potentially combined methods achieve effective transfer. environment setup model single environment markov decision process represented here state space actions available agent. work consider every state -dimensional grid size cell containing entity symbol transition distribution possible next states conditioned agent choosing action state determines reward provided agent time step. agent access true environment. domain also goal state determines episode terminates. finally complete text descriptions provided agent particular environment. reinforcement learning goal autonomous agent maximize cumulative reward obtained environment. traditional achieve learning action value function reinforcement. experiments relax assumption allow multiple entities cell ease description shall assume single entity. assumption worlds also easily relaxed generalize model situations. predict transitions rewards completely world without requiring substantial interaction. propose neural architecture consisting main components representation generator value iteration network representation generator takes state observation text descriptions produce tensor capturing essential information decision making. module implicitly encodes value iteration computation recurrent network convolutional modules producing action-value function using tensor representation input. together modules form end-to-end differentiable network trained using simple back-propagation. representation generator main purpose module fuse together information inputs state text speciﬁcations. important consideration however ability handle partial incomplete text descriptions contain particulars entity. thus would like incorporate useful information text rely completely. motivates utilize factorized representation input modalities. mind provide agent text descriptions collectively portray characteristics world. single description talks particular entity world. text contains information entity’s movement interaction player avatar. description also aligned corresponding entity environment. figure provides samples; details data collection statistics section transfer natural scenario test grounding hypothesis consider learning across multiple environments. agent learn ground language semantics environment test understanding capability placing unseen domain agent allowed unlimited experience convergence policy allowed interact learn policy provide agent mapping entities goals across domains either directly text. agent’s goal re-utilize information obtained learn efﬁciently grounding language policy transfer across domains requires model meets needs. first must allow ﬂexible representation fuses information state observations text descriptions. representation capture compositional nature language mapping linguistic semantics characteristics world. second model must capability learn accurate prototype environment using interactive feedback. overall model must enable agent text descriptions environment dynamics; allows formally given state matrix text descriptions module produces tensor consider cell containing entity corresponding description cell converted vector consisting parts concatenated together decomposition allows learn policies based object described behavior text. environment previously seen entities reuse learned representations directly based symbols. completely entities model form useful representations text descriptions. value iteration network model-based approach task require means estimate environment. achieve explicitly using predictive models functions learning transitions experienced agent. models used estimate optimal equation however pipelined approach would result errors propagating different stages predictions. directly predicting outcome value iteration thereby avoiding aforementioned error propagation. computation mimicked recurrent network operations step. first compute functions reward predictor operates utilizes output previous predict functions parametrized convolutional neural networks suit tensor representation second network employs pooling action channels q-value produced obtain value iteration computation thus approximated dimension m×n×|a|. point note model produces values cell input state matrix assuming agent’s position particular cell. convolution ﬁlters help capture information neighboring cells state matrix approximations parameters cnns approximate respectively. tamar detailed discussion. recursive computation traditional value iteration captured employing cnns recurrent fashion steps. intuitively larger values imply larger ﬁeld neighbors inﬂuencing q-value prediction particular cell information propagates longer. note output recurrent computation tensor. however since need policy agent’s current location appropriate selection function reduces q-value single action values agent’s location final prediction games follow complex dynamics challenging capture precisely especially longer term. vins approximate dynamics implicitly learned convolutional operations. thus likely estimated qvin values helpful short-term planning corresponds limited number iterations need complement local q-values estimates based global view. following speciﬁcation architecture also contains modelfree component implemented deep q-network network also provides prediction q-value combined qvin using composition function descriptions represented vectors environment’s transitions rewards encoded parameters domain model produce reasonable policy using corresponding text even receiving interactive feedback. maxa θi−) target q-value parameters ﬁxed previous iteration. employ experience replay memory store transitions periodically perform updates random samples memory. \u0001-greedy policy exploration. transfer procedure transfer learning scenario considers single task source target environments. better test generalization robustness methods consider transfer multiple source tasks multiple target tasks. ﬁrst train model achieve optimal performance source tasks. model parameters shared tasks. agent receives episode time environment round-robin fashion along corresponding text descriptions. algorithm details multi-task training procedure. training converges learned parameters initialize model target tasks. parameters replicated weights representation generator reused. speciﬁcally previously seen objects words obtain learned entity-speciﬁc embeddings whereas vectors objects unseen words target tasks initialized randomly. parameters ﬁne-tuned target tasks episodes sampled round-robin fashion. designed larger variation entity types. game total twenty different non-player entities different types movement interaction player’s avatar. instance entities move random chase avatar shoot bullets avatar must avoid. objective player meet friendly entities avoiding enemies. game instance four non-player entities sampled pool randomly placed grid. makes instances signiﬁcantly varied previous three games. created versions game f&e- f&e- sprites f&e- moving faster making harder environment. table shows different transfer scenarios consider experiments. environments perform experiments series environments within gvgai framework used annual video game competition. addition pre-speciﬁed games framework supports creation games using py-vgdl description language four different games evaluate transfer multitask learning freeway bomberman boulderchase friends enemies. certain similarities games. game consists grid player controlling movable avatar degrees freedom. also domain contains entities stationary moving interact avatar. however game also distinct freeway goal cross characteristics. multi-lane freeway avoiding cars lanes. cars move various paces either horizontal direction. bomberman boulderchase involve player seeking exit door avoiding enemies either chase player away move random. agent also collect resources like diamonds place bombs clear paths. three games level variants different layouts starting entity placements. friends enemies environment text descriptions collect textual descriptions using amazon mechanical turk provide annotators sample gameplay videos game describe speciﬁc entities terms movement interactions avatar. since users provide independent account entity obtain descriptive sentences opposed toinstructive ones inform optimal course action avatar’s viewpoint. manual veriﬁcation less obtained annotations instructive. aggregated together four sets descriptions different annotator every environment. description environment aligned constituent entity. also make sure entity names repeated across games table provides corpus-level statistics collected data figure sample descriptions. baselines explore several baselines transfer deep q-network initialized randomly trained scratch target tasks. case parameters transferred source tasks. implementation details models used adam optimization scheme learning rate annealed linearly minibatch size annealed source tasks evaluate transfer since perform online multitask learning directly comparable. target tasks. value iteration module experimented different levels recurrence found work best. used convolutional layers followed single fully connected layer relu non-linearities.the cnns ﬁlters strides length cnns modelfree component used ﬁlters sizes corresponding strides size embeddings initialized randomly. transfer performance table demonstrates transferring policies positively assists learning domains. model text-vin performs better baselines across different metrics. ﬁrst metric average reward text-vin achieves gain f&e- f&e- text-vin achieves gain text-dqn f&e- freeway. also evident sample reward curve shown figure transfer approaches outperform transfer baseline except bomberman boulderchase. domains text-dqn model obtains higher scores already demonstrating advantage using text transfer. text-vin achieves highest numbers transfer settings demonstrating effective utilization text descriptions bootstrap environment. ﬁnal metric asymptotic performance text-vin achieves highest convergence scores except f&e- f&e- obtains score partly smoother convergence; improving stability model training could boost asymptotic performance. negative transfer also observe challenging nature policy transfer scenarios. example bomberman boulderchase textdqn achieve lower average reward lower asymptotic reward transfer model exhibiting negative transfer numbers parentheses text-vin indicate value. textmodels make textual descriptions. reward attainable target environments least freeway boulderchase respectively. higher scores better; bold indicates best numbers. figure reward curves transfer condition f&e- freeway multitask learning f&e-. numbers parentheses text-vin indicate value. graphs averaged runs different seeds; shaded areas represent bootstrapped conﬁdence intervals. reward asymptotic reward learning across twenty variants f&e- domain simultaneously. model utilizes text learn faster achieve higher optimum scores text-vin showing gains text-dqn average asymptotic rewards respectively. figure shows corresponding reward curves. effect factorized representation investigate usefulness factorized representation training variant model using textbased vector representation entity. consider different transfer scenarios source target instances domain source/target instances different domains cases figure value maps produced module seen entity unseen entity description unseen entity ’friendly’ description unseen entity ’enemy’ description. agent non-player entity two-part representation results faster learning effective transfer obtaining higher average reward asymptotic reward f&e- f&e- transfer. representation able transfer prior knowledge textbased component retaining ability learn entity-speciﬁc representations quickly. text representation lstm also consider different variant model uses simple word embeddings description instead lstm. table provides comparison transfer performance conditions. observe across models lstm representation provides greater gains. fact representation worse vanilla cases. underscores importance good text representation model. value analysis finally provide qualitative evidence demonstrate generalization capacity text-vin. figure shows visualizations four value maps produced module trained model agent’s avatar position single entity map. ﬁrst entity known friendly leads high values surrounding areas expected. second entity unseen without descriptions; hence values uninformed. third fourth maps however contain unseen entities descriptions. cases module predicts higher lower values around entity depending whether text portrays friend enemy. thus even single interaction domain model utilize text generate good value maps. bootstraps learning process making efﬁcient. proposed novel method utilizing natural language drive transfer reinforcement learning show textual descriptions environments provide compact intermediate channel facilitate effective policy transfer. using model-aware approach design consists differentiable planning module model-free component two-part representation effectively utilize entity descriptions. demonstrate effectiveness approach transfer multi-task scenarios variety environments.", "year": 2017}