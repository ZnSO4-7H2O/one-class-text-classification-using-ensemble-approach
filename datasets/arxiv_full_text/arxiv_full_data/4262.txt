{"title": "Divide, Denoise, and Defend against Adversarial Attacks", "tag": ["cs.CV", "cs.AI"], "abstract": "Deep neural networks, although shown to be a successful class of machine learning algorithms, are known to be extremely unstable to adversarial perturbations. Improving the robustness of neural networks against these attacks is important, especially for security-critical applications. To defend against such attacks, we propose dividing the input image into multiple patches, denoising each patch independently, and reconstructing the image, without losing significant image content. This proposed defense mechanism is non-differentiable which makes it non-trivial for an adversary to apply gradient-based attacks. Moreover, we do not fine-tune the network with adversarial examples, making it more robust against unknown attacks. We present a thorough analysis of the tradeoff between accuracy and robustness against adversarial attacks. We evaluate our method under black-box, grey-box, and white-box settings. The proposed method outperforms the state-of-the-art by a significant margin on the ImageNet dataset under grey-box attacks while maintaining good accuracy on clean images. We also establish a strong baseline for a novel white-box attack.", "text": "figure proposed method transforms input image using non-differentiable algorithm. transformation removes adversarial noise improve robustness attacks. kanbak moreover attacks transferrable meaning adversary perturbations without access network. example successfully attacked image classiﬁers used commercial applications. observations highlight need improve robustness deep networks especially deployed hostile security-critical environment. many existing defense methods either show results smaller datasets mnist cifar- adversarial examples training data since attacks constantly proposed ideal defense attack-agnostic make robust unknown attack. defense non-differentiable allow adversary back-propagate defense mechanism. defense mechanism maps input image space based following observations increased dimensionality adverse effect robustness deep networks mapping reduce accuracy clean data mapping stable output minimally sensitive input perturbations. deep neural networks although shown successful class machine learning algorithms known extremely unstable adversarial perturbations. improving robustness neural networks attacks important especially security-critical applications. defend attacks propose dividing input image multiple patches denoising patch independently reconstructing image without losing signiﬁcant image content. proposed defense mechanism nondifferentiable makes non-trivial adversary apply gradient-based attacks. moreover ﬁne-tune network adversarial examples making robust unknown attacks. present thorough analysis tradeoff accuracy robustness adversarial attacks. evaluate method under black-box grey-box white-box settings. proposed method outperforms state-ofthe-art signiﬁcant margin imagenet dataset grey-box attacks maintaining good accuracy clean images. also establish strong baseline novel white-box attack. deep neural networks produced valuable results many practical applications vulnerable even small adversarial perturbations particular perturbations change decision dnn-based image classiﬁers. vulnerability deep networks adversarial manipulations input goes beyond classiﬁcation tasks additive perturbations algorithm algorithm denoising fast non-differentiable. propose novel patch-selection algorithm construct dictionary selected patches similar represent salient parts image. selection process mitigates effect adversarial perturbations maintaining good accuracy clean images. evaluate algorithm using imagenet dataset three settings black-box attacks adversary know network defense method grey-box attacks adversary knows network parameters defense mechanism white-box attacks adversary knows network parameters defense algorithm. algorithm performs comparably state-of-the-art methods black-box setting performs signiﬁcantly better grey-box attacks. also propose simple effective attack white-box setting establish strong baseline performance. addition show task complexity decreases remove information content image denoising without losing much accuracy robustness. propose novel framework defending adversarial attacks dividing images overlapping patches denoising independently using nondifferentiable attack-agnostic algorithm. seminal work highlights vulnerability deep neural networks adversarial examples. since then many methods proposed assess vulnerability developing various adversarial attacks. authors proposed fast gradient sign method attacks classiﬁer computing sign gradient loss w.r.t. input images. assess robustness deep networks accurately iterative algorithms deepfool later introduced. also possible generative models like generative also shown adversarial attacks transferrable used black-box attacks. attacks adversary access neither weights network architecture. recently shown exists image-agnostic attacks universal adversarial perturbation added image fool given network. even worse perturbations computed without dataset used training network ﬁrst defense adversarial perturbations proposed used stacked denoising auto-encoders mitigate perturbations. distillation suggested defense; however masks gradient still vulnerable black-box settings demonstrated recently adversarial training applied large-scale datasets e.g. imagenet using ensemble networks main drawback adversarial training usually overﬁts perturbation generated speciﬁc attack generalize unknown attack. recently several strategies defend attacks explored using various image transformations different approach detecting malicious samples instead improving robustness sought demonstrated deep networks augmented network detect adversarial examples. approach also suffers overﬁtting speciﬁc types perturbations. recent work successfully attacked different defense strategies emphasizing difﬁculty problem. theoretical works studying robustness deep networks. fawzi showed tradeoff accuracy robustness kernel classiﬁers. authors established bound robustness certain type classiﬁers adversary restricted dimensional space. lower bounds derived robustness simple neural networks. problem formulation classiﬁer parameterized computes class input image natural numbers denoting class labels. adversary perturb image noise norm noise kept small corrupted figure reducing dimensionality improves robustness classiﬁer. union subspaces illustrated blue hyperplanes. here image projected nearest subspace avoid clutter showing original image adversarial noise smallest distance classiﬁer’s decision surface adversary restricted smaller dimensional subspace norm noise much bigger norm cross decision boundary. noise scaled make attack stronger. improve robustnesss learning stable transformation label transformed image change corrupted noise i.e. fθ). moreover want maintain accuracy clean images transformed ensuring equal ground truth label attacks rely computing gradients classiﬁcation function w.r.t. input. hence desirable make transformation non-differentiable gradients cannot pass defense block. create defense mechanism robust unknown future attack want keep defense algorithm attack-agnostic want ﬁne-tune network simulated adversarial images. regularity conditions restricting adversary dimensional subspace improve robustness simple dimensionality reduction methods studied improve defense adversarial perturbations. however solutions work simpler tasks signiﬁcantly decrease discriminative performance network. transformations usually remove high frequency information required complex tasks -class classiﬁcation imagenet dataset therefore better information preserving dimensionality reduction method required limit space adversarial noise keeping important details. assume operator projects input image closest subspace union dimensional subspaces. operation linear projection operator onto dimensional subspace local neighborhood additive perturbations adversary limited locally seeking noise dimensional subspace robustness ideally improved factor propose patch-based denoising method defense. divide input image multiple patches denoise independently sparse reconstruction using dictionary patches. assume sparsity patch matrix containing columns dimension here sparsity. ﬁrst dictionary used select ﬁrst atom reconstructing given patch then residual computed image patch selected atom standard matching pursuit residual used select next atom. unlike standard different dictionary select sparsity level. summarize approach algorithm scale dictionary learning task large dataset imagenet propose efﬁcient greedy patchselection algorithm. mentioned earlier compute multiple dictionaries different sparsity levels algorithm. build dictionaries greedy manner selecting important diverse patches. algorithm takes account saliency information images. norm gradient classiﬁcation function w.r.t. input image used saliency map. importance sampling among patches w.r.t. saliency map. patch dictionary reconstruction patch using existing dictionary greater threshold angular distance patch. saliency helps preserving details important classiﬁcation task whereas cutoff angular distance ensures dictionary diverse. experiments using pre-tained network imagenet dataset improvement classiﬁcation accuracy saliency compared randomly selecting patch whole image. diversity among dictionary atoms encourages mapping clean corresponding noisy image patch dictionary atom. ensuring patches dictionary certain threshold apart also improves robustness classiﬁer. pixels. then non-overlapping patches local dimensionality projection operator would according dimensionality reduction would ideally improve robustness factor sparse reconstruction dictionary-based methods widely used enhance quality images computational efﬁciency sampled image patches dictionary. novel patch selection algorithm optimized improve robustness classiﬁer. sparse reconstruction efﬁcient greedy algorithm variant matching pursuit method summarized figure computing performance algorithm different hyper-parameters requires training algorithm classiﬁcation network computationally expensive. therefore efﬁciently study effect hyper-parameters algorithm compute following proxy metrics matching-rate fraction patches identical denoised image clean image ˆpn} patches extracted respectively. matching-rate deﬁned ex∈d) indicator function. here slight abuse notation assume applied patches. higher corresponds robust attacks. reconstruction-error average distance clean image transformed image experiments show proxy metrics highly correlated accuracy robustness classiﬁer. small randomly chosen images dataset quickly compute values. next describe patch denoising algorithm patch-selection algorithm construct dictionaries ﬁrst dictionary constructed reconstruct image patches using dictionary compute residuals. next dictionary constructed residual images instead original images. process repeated remaining dictionaries described algorithm found higher used different dictionary different sparsity using residual images compared using common dictionary sparsity levels constructed original images. example found re=. using separate dictionaries second dictionary contained residuals instead image patches. comparing re=. using dictionary constructed using original image patches shows using multiple dictionaries gives better matching-rate reconstruction quality. proposed defense algorithm divides input image overlapping patches denoises patch using constructed dictionaries reconstructs denoised image averaging pixels overlapping patches. experiments amount overlap patch size. randomization experiments observe withgiving access patch dictionaries algorithm successful defending adversarial attack. however defense weaker adversary access dictionary. observation motivates randomization transformation function adversary full access algorithm adding randomization even though adversary access dictionary exact atoms used reconstruction available. following efﬁcient randomization schemes defense randomize columns dictionaries randomly selecting ﬁfth atoms patch dictionaries denoising patch. conduct experiments -class imagenet dataset resnet- evaluate defense following diverse attack algorithms fgsm one-step attack gradient direction multiplied norm noise deepfool iterative attack ensuring network output changed transferable attack computes universal noise images. compute adversarial perturbation ﬂoating point scale ﬁxed norm i.e. noise added image converted ﬂoating point normalized. experimental setup described compare algorithm several image transformation based defenses proposed show signiﬁcant improvement classiﬁcation accuracy grey-box settin. furthermore analyze tradeoff clean image accuracy robustness attacks. study effect hyper-parameter values show tuned improve accuracy robustness. pretrained network original image space ﬁne-tune transformed images dictionary reconstruction. hyper-parameters algorithm used control quality image reconstruction. example increase sparsity details preserved accuracy reconstructed clean images figure tradeoff clean image accuracy robustness classiﬁer increasing sparsity. matchine rate reconstruction quality sparsity values. ﬁgures clean image accuracy reconstruction quality increase sparsity. decreases sparsity causing classiﬁer less robust. classiﬁer performance adversarially perturbed images small sparsity reconstruction quality also large sparsity matching-rate. optimal performance achieved intermediate value computational constraints reduce patch overlap pixels analysis improves reconstruction speed plot accuracy clean noisy images greybox attack figure plots classiﬁcation accuracy clean images consistently improves sparsity. accuracy noisy images small sparsity increases beginning reconstruction quality improves. increasing sparsity large value noise also starts getting reconstructed classiﬁcation accuracy drops. hence sweet spot optimal defense attack sparsity used tradeoff parameter. figure shows matching-rate reconstruction-quality sparsity values. reconstruction quality improves sparsity correlated classiﬁcation accuracy clean images. however matching-rate decreases sparsity increased causing network less robust adversarial noise. figure shows example images different sparsity values. experiments dictionary size also plays role accuracy robustness tradeoff. larger dictionary improves accuracy clean images images better reconstructed. smaller dictionary generally improves robustness dictionary atoms average farther apart. correlation classiﬁer’s performance enables efﬁciently study effects hyper-parameters. based analysis pick three settings evaluate algorithm. ﬁrst setting high sparsity choose large dictionary setting encourages high accuracy clean images less robust adversarial attacks. second setting reduce dictionary size keeping sparsity third setting dictionary size reduce sparsity improving robustness. next describe three types attacks compare results state-of-the-art. perturbations using original images mentioned earlier norm noise added ﬂoating point image. without defense classiﬁcation accuracies reduce deepfool fgsm universal perturbations. shown table methods robust attacks attack easiest defend. note assume adversary knows network architecture exact parameters. setting adversary access defense mechanism knows network weights. compute noise patterns using gradients ﬁne-tuned network evaluated original images setting white-box setting compare results grey-box setting white-box setting. without defense mechanism attacks successful setting fgsm reduces classiﬁcation accuracy deepfool reduces reduces results table show perform signiﬁcantly better state-ofthe-art. also note decease dictionary size sparsity robustness classiﬁer improves. example deepfool accuracy improves reducing sparsity before adversarial examples ﬁne-tuning network. ensemble training better fgsm attack ﬁne-tuning network fgsm adversarial examples. trained model performs poorly deepfool attack resulting accuracy. propose novel attack adversary knows weights ﬁned-tuned network well defense since transformation function differentiable fool network using gradient-based attacks compute adversarial noise using ﬁne-tuned network weights gradients computed transformed image noise added image best knowledge defense algorithm evaluated kind white-box attack. challenging setting deepfool attack reduces classiﬁcation accuracy fgsm attack reduces make robust adding randomization prevent attacker access exact atoms used reconstruction described section classiﬁcation accuracies using randomization shown table shows signiﬁcant improvement deterministic figure effect patch size matching-rate reconstruction-quality increasing improves matching-rate reduces reconstruction-quality decreasing classiﬁer performance clean image. simpler tasks algorithm denoise images without losing classiﬁcation accuracy time providing robustness. evaluate defense mechanism lower complexity task randomly select classes imagenet dataset learn -class classiﬁer. proposed defense successful lower complexity task. shown tables black grey-box attacks lower top- classiﬁcation accuracy classiﬁer less challenging white-box setting deepfool attack top- classiﬁcation accuracy noisy images drops clean images. drop smaller loss accuracy class classiﬁer setting table proposed defense following hyperparameters patch-size sparsity dictionary size minimum distance dictionary atoms analyze effect accuracy figure show accuracy robustness correlated matching-rate reconstruction-quality show effect different patch sizes figure matching-rate reconstruction-error. increase patch size matching-rate increases improving robustness algorithm. however reconstruction-quality decreases making classiﬁer less accurate. next study effect figure expected increasing minimum angle atoms increases matchingrate improve robustness. also reconstruction-quality decreases thus decreasing classiﬁer accuracy. recommend keeping high because improves robustness hurts accuracy clean images. described novel patch-based denoising algorithm improve robustness classiﬁers adversarial perturbations. developed proxy metrics help guide designing algorithm. design denoising patch selection algorithms encourages high matching-rate clean patch corresponding noisy patch. provided thorough analysis tradeoff clean image accuracy robustness attacks. proposed novel white-box attack efﬁcient randomization schemes improve robustness. evaluated proposed method imagenet showed defense mechanism provides state-of-the-art results. references abu-el-haija sami kothari nisarg joonseok natsev paul toderici george varadarajan balakrishnan vijayanarasimhan sudheendra. youtube-m largescale video classiﬁcation benchmark. arxiv preprint arxiv. ciss´e moustapha yossi neverova natalia keshet joseph. houdini fooling deep structured visual speech recognition models adversarial examples. proc. nips krasin ivan duerig alldrin neil veit andreas abu-el-haija sami belongie serge david feng zheyun ferrari vittorio gomes victor gupta abhinav narayanan dhyanesh chen chechik murphy kevin. openimages public dataset largescale multi-label multi-class image classiﬁcation. dataset available https//github.com/openimages tsung-yi maire michael belongie serge hays james perona pietro ramanan deva doll´ar piotr zitnick lawrence. microsoft coco common objects context. proc. eccv szegedy christian google zaremba wojciech sutskever ilya google bruna joan erhan dumitru google goodfellow fergus rob. intriguing properties neural networks. proc. iclr madry aleksander makelov aleksandar schmidt ludwig tsipras dimitris vladu adrian. towards deep learning models resistant adversarial attacks. arxiv preprint arxiv. mopuri konda reddy garg utsav babu venkatesh. fast feature fool data independent approach arxiv preprint universal adversarial perturbations. arxiv. papernot nicolas mcdaniel patrick somesh swami ananthram. distillation defense adversarial perturbations deep neural networks. ieee symposium security privacy pati rezaiifar krishnaprasad orthogonal matching pursuit recursive function approximation applications wavelet decomposition. asilomar conference signals systems computers shrivastava ashish patel vishal pillai jaishanker chellappa rama. generalized dictionaries multiple instance learning. international journal computer vision", "year": 2018}