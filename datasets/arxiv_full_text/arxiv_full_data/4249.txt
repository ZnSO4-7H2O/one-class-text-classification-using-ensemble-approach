{"title": "Learning Binary Residual Representations for Domain-specific Video  Streaming", "tag": ["cs.CV", "cs.AI"], "abstract": "We study domain-specific video streaming. Specifically, we target a streaming setting where the videos to be streamed from a server to a client are all in the same domain and they have to be compressed to a small size for low-latency transmission. Several popular video streaming services, such as the video game streaming services of GeForce Now and Twitch, fall in this category. While conventional video compression standards such as H.264 are commonly used for this task, we hypothesize that one can leverage the property that the videos are all in the same domain to achieve better video quality. Based on this hypothesis, we propose a novel video compression pipeline. Specifically, we first apply H.264 to compress domain-specific videos. We then train a novel binary autoencoder to encode the leftover domain-specific residual information frame-by-frame into binary representations. These binary representations are then compressed and sent to the client together with the H.264 stream. In our experiments, we show that our pipeline yields consistent gains over standard H.264 compression across several benchmark datasets while using the same channel bandwidth.", "text": "videos difﬁcult compress contains highly non-linear patterns. neither linear predictive coding linear transform coding effectively compress residual. paper hypothesize effectively compress residual information willing limit video compression method speciﬁc domain. words longer wish video compression method works universally well videos. wish method works particularly well speciﬁc domain. although setting appear inconvenient ﬁrst glance needs video compressor domain well several major video streaming applications video game streaming sports streaming. example video game streaming services geforce twitch gamer chooses game title play video content rendered server delivered client’s mobile console. game playing period video content stream video game domain domain-speciﬁc video compression method entirely appropriate. setting also user cases streaming sports often limited particular discipline well things like compressing dash videos videos street scenes. verify hypothesis leverage deep learning models established powerful non-linear function approximators encode highly nonlinear residual information. video compression pipeline ﬁrst apply compress videos speciﬁc domain train novel binary autoencoder encode resulting residual information frame-by-frame binary representation. apply huffman coding compress binary representations lossless manner. compressed binary representations sent client meta data ﬁeld streaming packet. method integrated existing video streaming standard. illustrate method figure show proposed binary residual representation achieve better video quality sending video stream using smaller bandwidth utilizing saved bandwidth transmit learned binary residual representation. study domain-speciﬁc video streaming. speciﬁcally target streaming setting videos streamed server client domain compressed small size low-latency transmission. several popular video streaming services video game streaming services geforce twitch fall category. conventional video compression standards commonly used task hypothesize leverage property videos domain achieve better video quality. based hypothesis propose novel video compression pipeline. speciﬁcally ﬁrst apply compress domain-speciﬁc videos. train novel binary autoencoder encode leftover domain-speciﬁc residual information frame-by-frame binary representations. binary representations compressed sent client together stream. experiments show pipeline yields consistent gains standard compression across several benchmark datasets using channel bandwidth. video streaming services netﬂix youtube popular methods viewing entertainment content nowadays. large video sizes limited network bandwidth video compression required streaming video content server client. video compression reduce size video often comes undesired compression artifacts image blocking effects blurry effects. decades efforts made towards delivering best possible video quality bandwidth constraint. stateof-the-art video compression methods mpeg- hevc combine various classical techniques including image transform coding predictive coding source coding motion estimation carefully-engineered framework. methods general applied various video domains effectively compressing information video. however residual information difference uncompressed compressed copyright association advancement artiﬁcial intelligence rights reserved. methods learning encode domain-speciﬁc residual information binary form. also compare various ways training proposed binary autoencoder encoding residual information. kitti three games video datasets show method consistently outperforms quantitatively qualitatively. example psnr score better average bandwidth mbps. image/video compression transform coding using discrete/integer cosine transform widely used image video coding standards jpeg mpeg- hevc encoder divides image/video frame non-overlapping blocks applies dct/ict individual block quantizes coefﬁcients. energy concentration ability dct/ict compressed images good quality moderate high rates. however real-time video stream requires bit-rate compression. result compressed images often suffer blocking artifacts block-wise processing. another problem existing coding standards designed universal coders cannot tailored speciﬁc domains. machine learning-based techniques developed compression standards. colorization model utilized learn representative color pixels store better reconstruction. another work adopts dictionary-based approach learn sparse representations image compression. method also based learning leverage state-of-the-art deep learning models compress residual information. recently reinforcement learning scheme adopted adaptively select rate video streaming minimizing latency deep learning-based image compression instead engineering every step coding system numerous learning-based approaches discussed jiang developed compress data holistic manner. recently autoencoders widely used extracting abstract representations images learning reconstruct input signals methods bottleneck layer learn compact representation dimension compact representation continuous needs quantized compression. various approaches utilized bottleneck layer compress data. quantization methods estimate entropy rate quantized data optimizing used number bits. addition adversarial training scheme developed produce sharper visually pleasing results. hand variants autoencoder architecture recurrent networks show ability directly learn binary representations bottleneck layer theses methods focus still image compression algorithm designed improve video quality streaming applications especially domain-speciﬁc scenarios game street view videos. integrate existing video compression standards effectively exploit temporal information learning-based methods efﬁciently transmit binary residual representations. result integration system adaptively applied existing video compression platforms improve performance. post-processing remove compression artifacts post-processing decoder aims reducing coding artifacts without introducing additional rates encoder end. earlier approaches manually smoothing ﬁlters reduce blocking effects caused expense blurry image outputs. recently learningbased methods developed model different compression artifacts. example arcnn method uses end-to-end training removing various jpeg artifacts. furthermore scheme employs jpeg priors improve reconstruction results. deeper models ddcn also developed eliminate artifacts recent work combines perceptual loss generate visually pleasing outputs. note post-processing methods require extensive computation client side well-suited embedded devices. contrast method encodes residual information binary form server side send client. utilizing encoded residual information client better recover original video using much less computational power. here ﬁrst introduce video compression pipeline streaming domain-speciﬁc videos. present details autoencoder encoding residual information binary forms training methods. video streaming pipeline proposed pipeline consists modules video compression module deep learning-based autoencoder shown figure video compression module adopts standard demonstrated good performance compressing temporally smooth visual signals residual autoencoder assists recover lost information compression client side leveraging property videos compressed domain. using hybrid approach figure overview proposed video streaming pipeline. consists modules conventional module proposed residual autoencoder. input residual module difference original compressed videos. difference encoded binarized generate binary representations. utilize huffman coding compress binary representations stream lossless manner. client side reconstruct output video adding back decoded difference compressed video. improve output quality spending small amount effort also system adapt existing compression platforms train speciﬁc domains exploiting large-scale data. note that although pipeline video compression standards mpeg hevc used well. given input video obtain compressed video applying difference videos called residual information larger residual information poorer compressed video quality. also note included because consists highly non-linear patterns compressed effectively conventional approaches. argue limiting video domain could leverage novel autoencoder effectively compress residual information. autoencoder consists pair functions encoder maps binary decoder recovers binary client side. recovered residual information referred ﬁnal output video better visual quality note binary mapped stream using huffman coding algorithm asymptotically optimal reduce bandwidth usage. sending stream residual information requires additional bandwidth. however train autoencoder requires much smaller bandwidth compress residual information therefore standard higher compression rate uses smaller bandwidth results larger residual signal. apply autoencoder compress residual signal small stream. considering scenario bandwidth video stream mbps apply proposed pipeline compress video mbps using utilize remaining mbps sending residual signal. autoencoder efﬁcient wonder completely replacing standard proposed residual autoencoder. argue residual autoencoder efﬁcient compressing residual signal. carefullyengineered efﬁcient compressing core video. marrying strength proposed autoencoder hybrid system achieves better performance. moreover pipeline easily integrated existing standard since residual information attached meta ﬁeld streaming packet. hence enjoy popularity standard various hardware accelerators implemented proposed domain-speciﬁc video streaming pipeline needs send parameters huffman decoder table client streaming service requires additional bandwidth. however parameters sent streaming starts. parameters sent user enjoy latency high video quality features proposed video streaming pipeline. binary residual autoencoder design autoencoder consists three components encoder binarizer decoder encoder goal learn extract compact feature representations following binarizer. convolutional layers encoder layer channel number stride down-samples feature maps. binarizer converts output last convolutional layer binary map. decoder up-sample binary back original input. decoder convolutional layers. convolutional layer sub-pixel layer element encoder output interval discretize given activation discretization functions respectively. following discuss different functions activation various methodologies binarization gumbel noise tanh/hardtanh activation. tanh common activation project feature values approximation version hardtanh function. here deﬁne binarized output tanh hardtanh function. however since binarization differentiable function train proposed autoencoder using backpropagation. avoid issue inspired recent binarization work adopt piecewise function back-propagation sigmoid activation. sigmoid function outputs value convert output applying approach discussed previous paragraph binarization training. stochastic regularization. incorporate stochastic process using tanh activation used up-sampling. since upsampling resolution feature spatial dimension channel number convolutional layer decoder subpixel layer. facilitate learning process batch normalization relu layers used. architecture autoencoder given figure encode decode residual signal frame frame. {ri} residual frames computed applying target rate. train autoencoder solving following optimization problem care bandwidth required transmitting binary representations determined factors number layers encoder number channels width height input image binary size given c×w×h large number smaller number would result smaller size encoder output hence smaller binary map. however smaller encoder output makes training autoencoder difﬁcult. experiments discuss results different numbers training binary residual autoencoder binarizing feature maps neural networks studied several earlier works purpose reducing memory footprint mobile devices. also used image compression work different binarize feature maps video streaming. discuss several binarization methods compare advantages drawbacks used pipeline experiment section. formulation. output feature binarizer aims produce binary output generate binary outputs process consists back-propagation similarly pass unchanged gradients gumbel noise. gumbel-softmax distributions utilized learning categorical/discrete outputs adopt similar approach adding gumbel noise sigmoid function refer gumbel-sigmoid. since sigmoid viewed special -class case softmax derive gumbel-sigmoid sigm sigmoid function. since still range adopt approach introduced shift value following start high temperature gradually anneal small non-zero temperature. lossless compression. generate binary feature huffman coding reduce size binary representation. coding schemes asymptotically optimal means number bits group together symbol better compression ratio. behavior illustrated table using kitti dataset. note source coding methods arithmetic coding used well. evaluate pipeline using kitti dataset consists various driving sequences street scenes three popular video games assassins creed skyrim borderlands. details datasets shown table tracking benchmark kitti dataset contains street-view videos. randomly select videos training videos testing. images videos resized game video resolutions report psnr ssim scores different rates quantitative comparisons. metrics popular performance metrics benchmarking video compression algorithms ﬁrst conduct ablation study using kitti dataset discuss impact layer channel numbers design. compare various methods training autoencoder. finally report performance comparisons deep learning baseline. implementation details. throughout paper adam train binary residual autoencoder. learning rate decreased half every epochs. momentums batch size train model epochs. implementation based pytorch. runtime analysis. server side encoder binarizer takes seconds compress residual image resolution using titan gpu. decoder client side takes seconds reconstruct residual image. ablation study depth breadth model. analyze impact layer numbers channel numbers video compression performance pipeline. study bandwidth mbps generating training testing videos. train residual autoencoder different utilizing hardtanh activation binarization. parameters determine size binary impacts compression rate well training difﬁculty. intuitively smaller binary easier compression task harder training task. results shown table based rate divide settings three groups. ones group similar rate applying huffman coding. best setting rate group rest experiments. binarization. compare various methods discussed earlier training binary residual autoencoder. train model different figure example results kitti video game datasets. compare pipeline artifact-removal method. corresponding rate psnr shown next images. best viewed enlarged images. compression rates. results reported table models trained using hardtanh tanh activations consistently outperform others. model trained stochastic regularization perform well possibly training difﬁculty. known stochastic noise increases gradient variance. addition empirically model using gumbel-sigmoid performs much worse residual signal compression task. uses gumbel noise reconstructing discrete/categorical signals continuous signals images task.). hence hardtanh activations rest experiments superior performance. video compression performance compare video compression pipeline standard artifact removal network popular approach reduce distortions. following recent deep learning works image enhancement denoising utilize neural network convolutional layers remove artifacts. network takes compressed images inputs frame-by-frame fashion outputs enhanced images. train artifact removal network video domain fair comparisons. note proposed pipeline requires bandwidth binary map. account rate calculation fair comparisons baseline methods. words rate proposed pipeline rate stream huffman code binary map. again take transmitting network parameters account since sent streaming starts. streaming services main goal high quality videos latency. report psnr ssim scores kitti benchmark video games figures pipeline consistently outperforms baseline methods rates. pipeline achieves psnr mbps averaged four datasets better better artifact-removal network. similarly pipeline performs better ssim e.g. improvements artifactremoval network mbps respectively. figure present qualitative results showing method preserves details textured contents reconstructed images using smaller bandwidth. validate importance modeling residual also carry experiments directly compressing video using autoencoder architecture. however results worse performance compared since method leverage motion information video compression. overall results show propagating extracted binary residual representations server client quality reconstructed videos largely improved. outperforms artifact removal network aims solving challenging inverse problem leverage prior knowledge compression process. addition runtime binary residual autoencoder times faster artifact removal network. paper propose video streaming system integrates binary residual autoencoder encode non-linear compression errors domain-speciﬁc video streaming. analyze various network design choices methods obtaining binary representations residual information. binary representations compressed transmitted server client. kitti benchmark dataset three popular video game datasets proposed algorithm generates better reconstructed videos artifact-removal methods using smaller bandwidth. acknowledgment authors would like thank azar zheng yuan helpful discussions.", "year": 2017}