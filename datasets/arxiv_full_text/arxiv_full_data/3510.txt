{"title": "Learning Wasserstein Embeddings", "tag": ["stat.ML", "cs.CV", "cs.LG", "stat.CO"], "abstract": "The Wasserstein distance received a lot of attention recently in the community of machine learning, especially for its principled way of comparing distributions. It has found numerous applications in several hard problems, such as domain adaptation, dimensionality reduction or generative models. However, its use is still limited by a heavy computational cost. Our goal is to alleviate this problem by providing an approximation mechanism that allows to break its inherent complexity. It relies on the search of an embedding where the Euclidean distance mimics the Wasserstein distance. We show that such an embedding can be found with a siamese architecture associated with a decoder network that allows to move from the embedding space back to the original input space. Once this embedding has been found, computing optimization problems in the Wasserstein space (e.g. barycenters, principal directions or even archetypes) can be conducted extremely fast. Numerical experiments supporting this idea are conducted on image datasets, and show the wide potential benefits of our method.", "text": "wasserstein distance received attention recently community machine learning especially principled comparing distributions. found numerous applications several hard problems domain adaptation dimensionality reduction generative models. however still limited heavy computational cost. goal alleviate problem providing approximation mechanism allows break inherent complexity. relies search embedding euclidean distance mimics wasserstein distance. show embedding found siamese architecture associated decoder network allows move embedding space back original input space. embedding found computing optimization problems wasserstein space conducted extremely fast. numerical experiments supporting idea conducted image datasets show wide potential beneﬁts method. wasserstein distance powerful tool based theory optimal transport compare data distributions wide applications image processing computer vision machine learning context machine learning recently found numerous applications e.g. domain adaptation word embedding generative models power comes major reasons allows operate empirical data distributions non-parametric geometry underlying space leveraged compare distributions geometrically sound way. space probability measures equipped wasserstein distance used construct objects interest barycenters geodesics used data analysis mining tasks. formally metric space endowed metric space borel probability measures ﬁnite moments order i.e. p-wasserstein distance deﬁned here probabilistic couplings such every borel subsets well known deﬁnes metric long deﬁnition also known earth mover’s distance monge-kantorovich distance. geometry thoroughly studied exists several works computing point sets however number applications natural distance arising computer vision computer graphics machine learning discussion quality comparison deployment wasserstein distances wide class applications somehow limited especially heavy computational burden. discrete version optimisation problem number variables scale quadratically number samples distributions solving associated linear program network algorithms known cubical complexity. recent strategies implying slicing technique entropic regularization involving stochastic optimization emerged cost computing pairwise wasserstein distances large number distributions prohibitive. true considers problem computing barycenters population means. recent attempt staib colleagues distributed computing solving problem scalable way. propose work learn euclidean embedding distributions euclidean norm approximates wasserstein distances. finding embedding enables standard euclidean methods embedded space signiﬁcant speedup pairwise wasserstein distance computation construction objects interests barycenters. embedding expressed deep neural network learnt strategy similar siamese networks also show simultaneously learning inverse embedding function possible allows reconstruction probability distribution embedding. ﬁrst start describing existing works wasserstein space embedding. proceed presenting learning framework give proof concepts empirical results existing datasets. metric embedding question metric embedding usually arises context approximation algorithms. generally speaking seeks representation data hand space distances original space preserved. representation should positive side effect offers computational ease time-consuming task interpretation facilities formally given metrics spaces mapping embedding distortion exists coefﬁcient dαdx here parameter understood global scaling coefﬁcient. distortion mapping inﬁmum possible previous relation holds. obviously lower better quality embedding noted existence exact embedding always guaranteed sometimes possible. finally embeddability metric space another possible exists mapping constant distortion. good introduction metric embedding found theoretical results wasserstein space embedding embedding wasserstein space normed metric space still theoretical open questions theoretical guarantees obtained simple case exists isometric embedding absolutely continuous probability measures given |fµ− fν|dx. fact exploited computation sliced wasserstein distance conversely known isometric embedding pointsets i.e. regularly sampled grids best known distortions regarding recent results shown exist meaningful embedding constant approximation. results show notably embedding pointsets size must incur distortion also considers riemannian structure wasserstein space provide meaningful linearization projecting onto tangent figure architecture wasserstein deep learning samples drawn data distribution input network computes embedding. embedding learnt squared euclidean distance embedding mimics wasserstein distance. embedded representation data decoded different network trained kullback-leibler divergence loss. space. notably allows faster computation pairwise wasserstein distances number samples dataset) allow statistical analysis embedded data. proceed specifying template element compute particle approximations data linear transport plans template element allow derive embedding used analysis. seguy cuturi also proposed similar pipeline based velocity ﬁeld without relying implicit embedding. noted data images cumulative radon transform also allows embedding used interpolation analysis exploiting exact solution optimal transport cumulative distribution functions. work ﬁrst propose learn generic embedding rather constructing explicit approximations/transformations data analytical operators riemannian logarithm maps. such formulation generic adapts type data. finally since mapping embedded space constructed explicitly handling unseen data require compute optimal transport plans optimization yielding extremely fast computation performances similar approximation performances. discuss method coined deep wasserstein embedding learns supervised representation data. need pre-computed dataset consists pairs histograms i}i∈...n dimensionality corresponding )}i∈...n. immediate solve problem would wasserstein distance originally designed metric learning purpose similarity learning type architecture usually deﬁned replicating network takes input samples learning learns mapping space contrastive loss. mainly used computer vision successful applications face recognition one-shot learning example though capacity learn meaningful embeddings highlighted never used best knowledge mimicking speciﬁc distance exhibits computation challenges. precisely objective here. propose learn embedding network takes input histogram project given euclidean space practice embedding mirror geometrical property wasserstein space. also propose regularize computation embedding adding reconstruction loss based decoding network important impacts first observed empirically eases learning embedding improves generalization performance network forcing embedded representation catch sufﬁcient information input data allow good reconstruction. type autoencoder regularization loss discussed different context embedding learning. second disposing of.a decoder network allows interpretation results prime importance several data-mining tasks overall picture depicting whole process given figure global objective function reads functions learned several data mining tasks operated wasserstein space. discuss potential applications computational scheme wide range applications problems wasserstein distance plays important role. though method exact wasserstein estimator empirically show numerical experiments performs well competes favorably classical computation strategies. wasserstein barycenters barycenters wasserstein space ﬁrst discussed agueh carlier designed analogy barycenters euclidean space wasserstein barycenters family measures deﬁned minimizers weighted squared wasserstein distances. framework barycenters obtained note samples barycenter corresponds wasserstein interpolation distributions weights uniform whole data collection considered barycenter wasserstein population mean also known fréchet mean principal geodesic analysis wasserstein space principal geodesic analysis ﬁrst introduced fletcher seen generalization general riemannian manifolds. goal directions called geodesic directions principal geodesics best encode statistical variability data. possible deﬁne making analogy pca. elements classical amounts mean data subtract samples build recursively subspace span solving following maximization problem fletcher gives generalization problem complete geodesic spaces extending three important concepts variance expected value squared riemannian distance mean geodesic subspaces portion manifold generated principal directions projection operator onto geodesic submanifold. space probability distribution equipped wasserstein metric deﬁnes geodesic space riemannian structure application appealing tool analyzing distributional data. however noted direct application fletcher’s original algorithm intractable inﬁnite dimensional analytical expression exponential logarithmic maps allowing travel corresponding wasserstein tangent space. propose novel approximation following procedure approximate fréchet mean strictly equivalent perform embedded space. reconstruction corresponding subspace original space conducted postpone detailed analytical study approximation subsequent works beyond goals paper. possible methods. matter facts several methods operate distributions beneﬁt approximation scheme. methods transposition euclidian counterparts embedding space. among them clustering methods wasserstein k-means readily adaptable framework. recent works also highlighted success using wasserstein distance dictionary learning archetypal analysis section evaluate performances method grayscale images normalized histograms. images offering nice testbed dimensionality large datasets frequently available computer vision. framework approach shown consists encoder decoder composed cascade. encoder produces representation input images architecture used embedding consists convolutional layers relu activations ﬁrst convolutional layer ﬁlters kernel size convolutional layer ﬁlters size convolutional layers followed linear dense layers respectively size ﬁnal layer size architecture reconstruction consists dense layer output relu activation followed dense layer output reshape layer input convolutional layer reshape output vector d-tensor. eventually invert convolutional layers convolutional layers ﬁrst convolutional layer ﬁlters relu activation kernel size followed second layer ﬁlter kernel size eventually decoder outputs reconstruction image shape work consider grayscale images normalized represent probability distributions. hence image depicted histogram. order normalize decoder reconstruction softmax activation last layer. dataset considered handwritten data hence holds inherent sparsity. case cannot promote output sparsity convex regularization softmax outputs positive values forces output instead apply pseudo -norm regularization reconstructed image promotes sparse output allows sharper reconstruction images dataset training. ﬁrst numerical experiment performed well known mnist digits dataset. dataset contains images digit classes order create training dataset draw randomly million pairs indexes training samples compute exact wasserstein distance quadratic ground metric using toolbox pairwise distances computed embarrassingly parallel scheme among million used learning neural network used validation pairs used testing purposes. model learnt standard nvidia node takes around stopping criterion computed validation set. figure prediction performance mnist dataset. test performance follows mse=. relative mse=. correlation=.. computational performance given average number relative correlation quantiles show small uncertainty slight bias large values small number samples available. results show good approximation performed approach investigate ability approach compute efﬁciently. compute average speed wasserstein distance computation test dataset estimate number computations second table fig. note ways compute approach denoted indep pairwise. comes fact computation basically squared euclidean norm embedding space. ﬁrst computation measures independent samples projecting embedding time compute computing distance. second computation aims computing pairwise sets samples time needs project samples compute pairwise distances making efﬁcient. note second approach would used retrieval problem would embed query compute distance selection dataset wasserstein nearest neighbor instance. speedup achieved method impressive even speedup respectively indep pairwise. allows even larger speedup respectively respect state-of-the-art compiled network flow solver toolbox course speed-up comes price time-consuming learning phase makes method better suited mining large scale datasets online applications. wasserstein barycenters next evaluate embedding task computing wasserstein barycenters class mnist dataset. take samples class test dataset compute uniform weight wasserstein barycenter using resulting barycenters euclidean means reported fig. note barycenters sensible also conserve sharpness problem occurs regularized barycenters computation barycenters also efﬁcient since requires barycenter complexity scales linearly number samples. figure principal geodesic analysis classes mnist dataset squared euclidean distance wasserstein deep learning class method show variation barycenter along ﬁrst principal modes variation. principal geodesic analysis report figure principal component analysis principal geodesic analysis classes mnist dataset. using wasserstein encode displacement mass leads semantic nonlinear subspaces rotation/width stroke global sizes digits. well known illustrated nevertheless method allows estimating principal component even large scale datasets reconstruction seems detailed compared maybe approach large number samples subspace estimation. datasets google doodle dataset crowd sourced dataset freely available contains million drawings. data collected asking users hand draw mouse given object animal less seconds. lead large number examples class also noise sens people often stopped drawing used numpy bitmaps format proposed quick draw github account. made simpliﬁed drawings rendered grayscale images. images aligned center drawing’s bounding box. paper downloaded classes crab faces tried learn wasserstein embedding classes architecture used mnist. order create training dataset draw randomly million pairs indexes training samples categories compute exact wasserstein distance quadratic ground metric using toolbox mnist used learning neural network used validation pars used testing purposes. three categories holds respectively training samples. numerical precision cross dataset comparison numerical performances learned models doodle dataset reported diagonal table datasets much difﬁcult mnist curated contain large variance numerous unﬁnished doodles. interesting comparison cross comparison datasets embedding learned dataset compute another. cross performances given table shows deﬁnitively loss accuracy prediction loss limited doodle datasets important variety. performance loss across doodle mnist dataset larger latter highly structured needs representative dataset generalize well case mnist. wasserstein interpolation ﬁrst compute wasserstein interpolation four samples datasets figure note interpolation might optimal w.r.t. objects clearly continuous displacement mass characteristic optimal transport. leads surprising artefacts example face fuse border nose turns eye. also note reason wasserstein barycenter realistic sample. next qualitatively evaluate subspace learned comparing wasserstein interpolation approach true wasserstein interpolation estimated solving linear program using regularized bregman projections interpolation results methods euclidean interpolation available fig. solver takes long time leads noisy interpolation already explained regularized wasserstein barycenter obtained rapidly also smooth risk loosing details despite choosing small regularization prevents numerical problems. reconstruction also looses details auto-encoder error fast done real time work presented computational approximation wasserstein distance suitable large scale data mining tasks. method ﬁnds embedding samples space euclidean distance emulates behavior wasserstein distance. thanks embedding numerous data analysis tasks conducted cheap computational price. forecast strategy help generalizing wasserstein distance numerous applications. however method appealing practice still raises questions theoretical guarantees approximation quality. first difﬁcult foresee given network architecture sufﬁciently complex ﬁnding successful embedding. conjectured dependent complexity data hand also locality manifold data live second theoretical existence results wasserstein embedding constant distortion still lacking. future works consider questions well applications approximation strategy wider range ground loss data mining tasks. also study transferability database another diminish computational burden computing wasserstein distances numerous pairs learning process.", "year": 2017}