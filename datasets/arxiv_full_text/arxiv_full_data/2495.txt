{"title": "Trend Filtering on Graphs", "tag": ["stat.ML", "cs.AI", "cs.LG", "stat.ME", "62G05"], "abstract": "We introduce a family of adaptive estimators on graphs, based on penalizing the $\\ell_1$ norm of discrete graph differences. This generalizes the idea of trend filtering [Kim et al. (2009), Tibshirani (2014)], used for univariate nonparametric regression, to graphs. Analogous to the univariate case, graph trend filtering exhibits a level of local adaptivity unmatched by the usual $\\ell_2$-based graph smoothers. It is also defined by a convex minimization problem that is readily solved (e.g., by fast ADMM or Newton algorithms). We demonstrate the merits of graph trend filtering through examples and theory.", "text": "introduce family adaptive estimators graphs based penalizing norm discrete graph differences. generalizes idea trend ﬁltering used univariate nonparametric regression graphs. analogous univariate case graph trend ﬁltering exhibits level local adaptivity unmatched usual -based graph smoothers. also deﬁned convex minimization problem readily solved demonstrate merits graph trend ﬁltering examples theory. nonparametric regression rich history statistics carrying well years associated literature. goal paper port successful idea univariate nonparametric regression trend ﬁltering setting estimation graphs. proposed estimator graph trend ﬁltering shares three properties trend ﬁltering univariate setting. local adaptivity graph trend ﬁltering adapt inhomogeneity level smoothness observed signal across nodes. stands contrast usual -based methods e.g. laplacian regularization enforce smoothness globally much heavier hand tends yield estimates either smooth else wiggly throughout. computational efﬁciency graph trend ﬁltering deﬁned regularized least squares problem penalty term nonsmooth convex structured enough permit efﬁcient large-scale computation. analysis regularization graph trend ﬁltering problem directly penalizes differences ﬁtted signal across nodes. therefore graph trend ﬁltering falls called analysis framework deﬁning estimators. alternatively synthesis framework would ﬁrst construct suitable basis graph regress observed signal basis; e.g. shuman survey number approaches using wavelets; likewise kernel methods regularize terms eigenfunctions graph laplacian advantage analysis regularization easily yields complex extensions basic estimator mixing penalties. motivating example consider denoising problem census tracts allegheny county arranged graph vertices edges obtained connecting spatially adjacent tracts. illustrate adaptive property graph trend ﬁltering generated artiﬁcial signal inhomogeneous smoothness across nodes sharp peaks near center graph seen left panel figure drew noisy observations around signal shown right panel ﬁgure graph trend ﬁltering graph laplacian smoothing wavelet smoothing observations. graph trend ﬁltering deﬁned section latter recall deﬁned optimization problems vector observations measured nodes graph rn×n graph laplacian matrix rn×n wavelet basis built graph. wavelet smoothing problem displayed really oversimpliﬁed representation class wavelets methods since encapsulates estimators employ orthogonal wavelet basis present experiment constructed according spanning tree wavelet design sharpnack found construction performed best among graph wavelet designs considered data hand. completeness results alternative wavelet designs given appendix. graph trend ﬁltering laplacian smoothing wavelet smoothing regularization parameters parameters generally scale. therefore comparisons effective degrees freedom common measure complexities ﬁtted models. right panel figure shows graph trend ﬁltering estimate adaptively sharp peaks center graph smooths surrounding regions appropriately. graph laplacian estimate substantially oversmooths high peaks center begins detect high peaks center undersmooths neighboring regions. wavelet smoothing performs quite poorly across values—it appears affected level noise observations. quantitative assessment figure shows mean squared errors estimates true underlying signal. differences performance analogous univariate case comparing trend ﬁltering smoothing splines smaller values laplacian smoothing global considerations fails adapt local differences across nodes. trend ﬁltering performs much better values matches laplacian smoothing sufﬁciently complex i.e. overﬁtting regime. demonstrates local ﬂexibility trend ﬁltering estimates attribute. outline rest article. section deﬁnes graph trend ﬁltering gives underlying motivation intuition. section covers basic properties extensions graph trend ﬁltering estimator. section examines computational approaches section looks number real simulated data examples. section presents asymptotic error bounds graph trend ﬁltering. section concludes discussion. notation write extract rows matrix rm×n correspond subset extract complementary rows. similar convention vectors denote components vector correspond complement respectively. write null null spaces respectively pseudoinverse x)†x rectangular. begin reviewing trend ﬁltering univariate setting discrete difference operators play central role. suppose observe across input locations simplicity suppose inputs evenly spaced given integer order trend ﬁltering estimate deﬁned |βi+ order trend ﬁltering estimate reduces dimensional fused lasso estimator also called -dimensional total variation denoising operator deﬁned recursively denoting version ﬁrst difference operator words given taking ﬁrst differences differences. interpretation hence problem penalizes changes discrete differences ﬁtted trend. estimated components exhibit form order piecewise polynomial function evaluated input locations formally veriﬁed examining continuous-time analog trend filtering graphs graph vertices undirected edges suppose observe nodes. following univariate deﬁnition deﬁne order graph trend ﬁltering estimate broad terms problem type generalized lasso problem penalty matrix suitably deﬁned graph difference operator order fact novelty proposal lies entirely within deﬁnition operator. penalty term sums absolute differences across connected nodes achieve this }m×n oriented incidence matrix graph containing edge graph; speciﬁcally recursion deﬁne higher order graph difference operators manner similar univariate case. recursion alternates multiplying ﬁrst difference operator transpose important point deﬁned graph difference operators reduce univariate ones case chain graph n−}) modulo boundary terms. even removes ﬁrst rows last rows chain graph recovers removes ﬁrst last rows chain graph recovers intuition graph difference operators given next. give insight deﬁnition graph difference operators based idea piecewise polynomials graphs. univariate case described section sparsity difference operator implies speciﬁc order piecewise polynomial structure components since components correspond input locations interpretation piecewise polynomial unambiguous. graph might sparsity mean components piecewise polynomial? latter even mean components deﬁned nodes? address questions intuitively deﬁne piecewise polynomial graph show implies sparsity constructed graph difference operators. piecewise constant signal piecewise constant graph many differences zero across edges note exactly property associated sparsity since oriented incidence matrix graph. quite natural notion linearity requiring equal average neighboring values would enforce linearity appropriate embedding points euclidean space. again precisely requiring sparse since graph laplacian. piecewise polynomial piecewise quadratic structure ﬁrst differences second differences mostly zero edges likewise piecewise cubic structure second differences second differences mostly zero nodes argument extends alternating leading ﬁrst second differences even sparsity either case exactly corresponds many differences zero construction. figure illustrate graph trend ﬁltering estimator grid graph dimension i.e. grid graph nodes edges. cases generated synthetic measurements grid computed estimate corresponding order. chose grid setting piecewise polynomial nature estimates could visualized. plot utilized graph trend ﬁltering penalty displayed explicit detail. instructive compare graph trend ﬁltering estimator deﬁned laplacian smoothing standard laplacian smoothing uses least squares loss replaces penalty term βlβ. natural generalization would allow power laplacian matrix deﬁne order graph laplacian smoothing according critical difference graph laplacian smoothing graph trend ﬁltering lies choice penalty norm former latter. effect penalty program many graph differences zero exactly leave others large nonzero values; i.e. estimate simultaneously smooth parts graph wiggly others. hand penalty graph laplacian smoother cannot graph differences zero exactly roughly speaking must choose making graph differences small large. relevant analogy comparison lasso ridge regression univariate trend ﬁltering smoothing splines suggestion adapt proper local degree smoothness laplacian smoothing cannot. strongly supported examples given throughout paper. authors signal processing community e.g. bredies setzer studied total generalized variation higher order variant total variation regularization. moreover several discrete versions operators proposed. often similar construction have. however focus works mostly well discrete functional approximates continuous counterpart. quite different concern signal graph meaningful continuous-space embedding all. addition aware study statistical properties regularizers. fact theoretical analysis section extended methods too. describe basic structure graph trend ﬁltering estimates present unbiased estimate degrees freedom. tuning parameter arbitrary ﬁxed. virtue penalty solution satisﬁes supp active null−a therefore basic structure trivially reexpress estimates revealed analyzing null space suboperator lemma assume without loss generality connected oriented incidence matrix laplacian matrix even denote subgraph induced removing edges indexed connected components g−a. proof lemma appears appendix. lemma useful reasons. first motivated above describes coarse structure solutions. indeed piecewise constant groups nodes structure smoothed multiplying piecewise constant levels meanwhile structure estimate based assigning nonzero values subset nodes smoothing multiplication smoothing operations depend interesting interpretations terms electrical network perspective graphs. developed next subsection. second lemma leads simple expression degrees freedom i.e. effective number parameters estimate results generalized lasso problems denoting support nullity dimension null space matrix applying lemma gives following. result lemma form simple unbiased estimate max{|a| even number connected components support reporting degrees freedom graph trend ﬁltering unbiased estimates. electrical network interpretation lemma reveals mathematical structure estimates satisfy null−a interesting interpret results using electrical network perspective graphs perspective imagine replacing edge graph resistor value describes much current going node graph describes induced voltage node. provided means total accumulation current network solve current values voltage values l†v. says estimates formed assigning sparse number nodes graph nonzero voltage solving induced current assign sparse number nodes nonzero voltage solve induced current repeat this relabel induced current input voltages nodes compute induced current. process iterated result says estimates given choosing partition nodes assigning constant input voltage element partition. solve induced current process iterated relabeling induced current input voltage. comparison structure estimates informative sense tells order estimates smoother order estimates sparse input voltage vector need induce current piecewise constant nodes graph. example input voltage vector nodes large nonzero values induce current peaked around nodes piecewise constant. several extensions proposed graph trend ﬁltering model possible. trend ﬁltering weighted graph example could performed using properly weighted version edge incidence matrix carrying forward recursion higher order difference operators. another example gaussian regression loss could changed another suitable likelihood-derived losses order accommodate different data type logistic regression loss binary data poisson regression loss count data. section explore modest extension strongly convex prior term criterion assist performing graph-based imputation partially observed data nodes. section investigate modiﬁcation proposed regularization scheme pure penalty hence forming sparse variant gtf. potentially interesting penalty extensions include mixing graph difference penalties various orders tying together several denoising tasks group penalty. extensions easily built recall result analysis framework used program wherein estimate deﬁned direct regularization analyzing operator -based graph difference penalty graph trend ﬁltering deﬁned convex optimization problem principle means that least small moderately sized problems solutions reliably computed using variety standard algorithms. order handle larger scale problems describe specialized algorithms improve generic procedures taking advantage structure reparametrize introducing auxiliary variables apply admm. even special transformation critical fast computation univariate trend ﬁltering); possible. reparametrizations even linear system well-conditioned sparse solved efﬁciently using preconditioned conjugate gradient method. involves multiplication laplacian matrices. small enough choices system diagonally dominant special laplacian/sdd solvers applied almost linear time update simply given soft-thresholding given graph denoising i.e. given solving graph fused lasso problem. note subproblem exact structure graph trend ﬁltering problem direct approach graph denoising available based parametric max-ﬂow algorithm empirically much faster worstcase complexity special case underlying graph grid promising alternative method employs proximal stacking techniques solution given )ˆv. adopt similar strategy instead interior point method.) projected newton method performs updates using reduced hessian abbreviating iteration boils indices linear system always sparse conditioning becomes issue grows addition identity matrix found empirically preconditioned conjugate gradient method works quite well struggles larger figure compares performances described algorithms moderately large simulated example using grid graph. projected newton method converges faster admm story reversed projected newton iterations quickly become stagnant admm enjoys better convergence. figure convergence plots projected newton method admm solving algorithms grid graph nodes edges. projected newton plot duality across iterations; admm plot average primal dual residuals loosely speaking order provides solutions exhibit different class structure gives piecewise constant solutions gives piecewise linear gives piecewise smooth. orders continue give piecewise smooth less less transparent differences since conditioning graph trend ﬁltering operator worsens increases makes computation difﬁcult makes practical sense simply choose whenever piecewise smooth desired. introduction examined denoising power graph trend ﬁltering spatial setting. examine behavior graph trend ﬁltering nonplanar graph facebook graph stanford network analysis project composed nodes representing facebook users edges representing friendships collected real survey participants; graph connected component observed degree sequence mixed ranging details). generated synthetic measurements facebook nodes based three different ground truth models precisely evaluate compare estimation accuracy laplacian smoothing wavelet smoothing. latter used spanning tree wavelet design sharpnack performed among best wavelets designs data settings considered here. results wavelet designs presented appendix. three ground truth models represent different scenarios underlying signal favorable different estimation methods. inhomogeneous random walk decaying random walks different starter nodes graph recorded total number visits node. speciﬁcally chose nodes starter nodes assigned starter node decay probability uniformly random starter node also sent varying number random walks chosen uniformly case synthetic measurements formed adding noise note model designed favorable laplace smoothing; model designed favorable gtf; inhomogeneity model designed challenging laplacian smoothing favorable adaptive wavelet methods. figure shows performance three estimation methods wide range noise levels synthetic measurements; performance measured best achieved mean squared error allowing method tuned optimally noise level. summary estimates superior laplacian-based sparsity pattern effect nonetheless highly competitive settings—the dense case laplacian smoothing thrives inhomogeneous random walk case wavelets thrive. graph trend ﬁltering used graph-based transductive learning motivated work talukdar crammer talukdar pereira rely laplacian regularization. consider semisupervised learning setting given small number seed labels nodes graph goal impute labels remaining nodes. write observed figure performance others three generative models facebook graph. x-axis shows negative logx underlying signal noise variance. hence noise level increasing left right. y-axis shows denoised negative logx denotes mean squared error achieved increasing bottom top. matrix rn×k indicator matrix observed described class observed otherwise. matrix rn×k contains ﬁtted probabilities giving probability node class write column hence middle term criterion encourages class probabilities behave smoothly graph. last term criterion ties ﬁtted probabilities given prior weights rn×k. principle could second tuning parameter simplicity take small ﬁxed guaranteeing criterion strictly convex thus unique solution entries need probabilites strict sense still interpret relative probabilities imputation performed assigning unobserved node class label ˆbij largest. speciﬁcation mad-gtf deviates proposal talukdar crammer place -based graph difference regularizer underlying class probabilities thought heterogeneous smoothness graph replacing laplacian regularizer gtf-designed might lead better performance. broad comparison methods popular classiﬁcation data sets machine learning repository data constructed -nearest-neighbor graph based euclidean distance provided features randomly selected seeds class serve observed class labels. used prior weights chose tuning parameter wide grid values table misclassiﬁcation rates mad-laplacian mad-gtf imputation data sets. also compute p-values repetitions data paired t-tests. cases mad-gtf achieves signiﬁcantly better misclassiﬁcation rate level highlighted green; cases mad-gtf achieves signiﬁcantly worse miclassiﬁcation rate level highlighted red. represent best achievable performance method experiment. figure table summarize misclassiﬁcation rates imputation using mad-laplacian mad-gtf averaged repetitions randomly selected seed labels. mad-gtf seem work well smoother alternatives. importantly mad-gtf perform least well sometimes better mad-laplacian data sets. recall data sets selected entirely based popularity belief might represent favorable scenarios therefore fact mad-gtf nonetheless performs competitively broad range experiments convincing evidence utility regularizer. call sparse graph trend ﬁltering tuning parameters proper tuning sparse estimate zero many nodes graph otherwise deviate smoothly zero. useful situations observed signal represents difference smooth processes mostly similar exhibit differences regions graph. apply problem detecting events based abnormalities number taxi trips different locations york city. data kindly provided authors doraiswamy obtained data taxi limosine commission. speciﬁcally consider graph road network manhattan contains nodes edges measurements nodes used number taxi pickups dropoffs particular time period interest june corresponding pride parade. pickups dropoffs generically occur road junctions used interpolation form counts graph nodes. baseline seasonal average calculated considering data note nonzero node estimates sparse applied proper tuning mark events interest convey substantial differences observed expected taxi counts. according descriptions news know pride parade giant march noon fifth ave. christopher greenwich village trafﬁc blocked entire route hours therefore hand-labeled route crude ground truth event interest illustrated left panel figure bottom panels figure compare sparse sparse variant laplacian smoothing obtained replacing ﬁrst regularization term βlβ. qualitative visual comparison smoothing parameter chosen methods degrees freedom sparsity parameter similar seen already able better localize estimates around strong inhomogenous spikes measurements able better capture event interest. result sparse laplacian smoothing localized around ground truth event displays many nonzero node estimates throughout distant regions graph. decrease ﬂexibility sparse laplacian output would display smoothness graph node estimates around ground truth region would also grossly shrunken. estimation error bounds section assume study asymptotic error rates graph trend ﬁltering. analysis actually focuses broadly generalized lasso problem rr×n arbitrary linear operator denotes number rows. throughout specialize derived results graph difference operator obtain concrete statements particular graphs. proofs deferred appendix. recall nullity denotes dimension null space operator nullity theorem says estimate converges average squared error rate probability. theorem quite general applies linear operator might therefore think cannot yield fast rates. still show next imply consistency graph trend ﬁltering certain cases. λmin smallest nonzero eigenvalue laplacian general λmin small leading loose error bounds particular graphs question well-controlled. bounded cases corollary show stronger assumption bounded. case corollary covers univariate trend ﬁltering result case based direct calculation using speciﬁc facts known univariate difference operators. natural univariate setting assume nkdβ bounded bounded). assumption corollary yields convergence rate trend ﬁltering estimator converge minimax optimal rate proved tibshirani using connection univariate trend ﬁltering locally adaptive regression splines relying sharp entropy-based rates locally adaptive regression splines mammen geer note pure graph-centric setting latter strategy generally applicable notion spline function obviously extend nodes arbitrary graph structure. next subsections develop advanced strategies deriving fast error rates based incoherence entropy. provide substantial improvements basic error bound established subsection applicable certain graph models. fortunately includes common graphs interest regular grids. verify sharpness alternative strategies show used recover optimal rates convergence trend ﬁltering setting. second bound holds standard result maxima gaussians norm columns ∆†). ﬁrst bound uses holder’s inequality; note applies i.e. information distribution properties next lemma reveals potential advantage gained replacing bound stemming holder’s inequality linearized bound. lemma denote assume right-hand side linear function indeed encompasses bound special case result lemma reduces theorem result lemma much stronger adjusted smaller also small. arrangement possible certain operators e.g. possible incoherence-type assumption theorem rank denote singular values increasing order. also corresponding left singular vectors. assume vectors incoherent theorem proved leveraging linearized bound holds incoherence condition singular vectors compared basic result theorem result theorem clearly stronger allows replace m—which grow like reciprocal minimum nonzero singular value ∆—with something akin average reciprocal larger singular values. does course also make stronger assumptions interesting note functional also determined play leading role error bounds graph fourier based singular vectors depend graph question. singular vectors eigenvectors laplacian even left singular vectors edge incidence matrix loosely speaking vectors incoherent neighborhoods different vertices look roughly same. social networks property bulk vertices grid graphs also property. first consider trend ﬁltering grid i.e. chain note corollary essentially recovers optimal rate convergence univariate trend ﬁltering estimator orders nk∆β assumed bounded natural assumption univariate setting corollary shows estimator converge rate n−//. ignoring factor matches minimax optimal rate established tibshirani wang importantly proof corollary unlike used previous works free dependence univariate spline functions; completely graph-theoretic uses incoherence properties grid graph. strength approach wider applicability demonstrate moving grids. result corollary written form mimics result corollary claim analog boundedness nk∆β boundedness nk/∆β thus appropriate boundedness condition rate shows improvement rate makes sense since regularization enforced richer manner. worthwhile highlighting result particular says that absolute discrete differences bounded grid fused lasso error rate n−/. faster rate fused lasso absolute differences bounded. rates higher dimensional grid graphs follow analogous arguments omit details. strong error bounds based entropy different fractional bound gaussian contrast provides alternate route deriving sharper rates. style bound inspired seminal work geer main motivation bounds form follow entropy bounds recall covering number fewest number balls radius cover respect norm covering entropy number next result make connection entropy fractional bounds precise; follows closely lemma geer make result theorem must obtain entropy bound literature entropy numbers rich various methods computing entropy bounds used purposes long bounds form required theorem. bounding entropy like common techniques characterization spectral decay analysis correlations columns nice review strategies applications refer reader section geer lederer section buhlmann geer pursue either strategies current paper. instead consider third somewhat transparent strategy based covering number bound columns lemma denote atoms associated operator i.e. columns {±gi denote symmetrized atoms. suppose exists constants following property arrangement balls radius entropy-based results subsection appear complex involving incoherence previous subsection indeed said proofs found appendix. entropy machinery established actually remarkably easy lemma produce sharp results. conclude giving example. corollary rederives optimal convergence rate univariate fused lasso assuming boundedness already shown mammen geer tibshirani like corollary proof rely special facts functions bounded variation. uses covering number bound columns operator strategy that principle extends many settings worth emphasizing simple covering number construction compared incoherence-based arguments lead result; invite curious reader compare proofs corollaries work proposed graph trend ﬁltering useful alternative laplacian wavelet smoothers graphs. analogous usefulness univariate trend ﬁltering nonparametric regression alternative smoothing splines wavelets documented empirical evidence superior local adaptivity -based -based graph laplacian smoother superior robustness wavelet smoothing high-noise scenarios. theoretical analysis provides basis deeper understanding estimation properties gtf. precise theoretical characterizations involving entropy topic future work comparisons error rates achieved common estimators laplacian smoothing. extensions many others well within reach. authors would like thank harish doraiswamy nivan ferreira theodoros damoulas claudio silva sharing pre-processed taxi data jeff irion naoki saito help implementation graph wavelet algorithms well associate editor anonymous reviewers valuable feedback. supported singapore national research foundation international research centre singapore funding initiative administered programme ofﬁce. supported grant dms-. supported google faculty research grant. supported grant dms-. addition considering wavelet design sharpnack allegheny county example also considered designs coiman maggioni classic method builds diffusion wavelets graph irion recent graph wavelet construction. contrast sharpnack produces single signal-independent orthogonal basis graph coiman maggioni irion build wavelet packets given graph structure. wavelet packet overcomplete basis indexed hierarchical data structure used generate exponential number orthogonal bases. construction computationally expensive typically involves computing eigendecompositions large matrices. wavelet packet constructed input signal observes graph question runs best basis algorithm choose particular orthogonal basis wavelet packet optimizing particular cost function eventual wavelet coefﬁcients. based message-passing-like dynamic programming algorithm quite efﬁcient. lastly denoising procedure deﬁned usual namely performs basis transformation soft-thresholds coefﬁcients reconstructs denoised signal. experiments used wavelet implementations released authors coiman maggioni irion default settings. particular former implementation coiman maggioni builds wavelets diffusion operator constructed adjacency matrix graph cost function best basis deﬁned norm wavelet coefﬁcients. latter implementation irion uses exhaustive search building wavelet packets hierarchical partitioning eigentransform three different laplacian matrices fourth generalized haar-walsh transform choosing best basis four collections optimizing meta cost function norm wavelet coefﬁcients cumulative relative error deﬁned equation irion left panel figure plot mean squared errors wavelet methods simulations allegheny county example figure section middle right panels ﬁgure show denoised signals methods data figure optimal degrees freedom values spanning tree wavelet design sharpnack best performer among three candidate wavelet designs. rough sense construction irion seems perform similarly sharpnack best larger values whereas construction coiman maggioni performs best smaller values provided authors wavelet methods; wavelet implementation coiman maggioni took power diffusion operator instead wavelet implementation irion used another best basis algorithm searches within basis collection ghwt eigendecomposition original algorithm slow larger scale considered example. chosen bases come ghwt eigendecomposition.) view changes minor changes applied methods coiman maggioni irion smaller allegheny county example obvious differences results. figure shows results wavelet methods facebook graph simulation using setup figure again spanning tree wavelets sharpnack perform better wavelet methods across essentially scenarios. case univariate trend ﬁltering estimator order considering penalty matrix univariate difference operator order note null space constant dimension show lemma appendix contains last columns order falling row) factorial basis matrix evaluated input points largest column norm cases ramanujan d-regular graph number edges graph operator number rows even; overall dimension null space constant erdos-renyi random graph bounds apply number rows dimension null space except bounds become probabilistic ones. remark. proof rely input points indeed result holds true sequence inputs used deﬁne discrete difference matrix falling factorial basis matrix. numerical methods differential equations matrix called ﬁnite difference operator laplace equation neumann boundary conditions known eigenvalues eigenvectors therefore eigenvectors incoherent constant course implies shares eigenvectors meanwhile eigenvalues given raising power compute partial squared reciprocals applying theorem gives result odd. case even. even instead dlk/ edge incidence matrix chain clear left singular vectors dlk/ simply left singular vectors equivalently eigenvectors explicit i.e. grid laplacian kronecker grid laplacian eigenvectors given pairwise kronecker products eigenvectors form moreover hard unit norm allows conclude eigenvectors obey incoherence property r−)× difference operator grid size identity matrix). sufﬁces check incoherence left singular vectors dlk/ since eigenvalues dlk/ raised power rest proof follows precisely case odd. left singular vectors dlk/ left singular vectors eigenvectors observe hence derived eigenvectors note vectors actually nonzero eigenvalues) work eigenvectors case recall unit vectors satisfying incoherence property means eigenvectors unit norm also incoherent constant modulo appropriate normalization derived remaining eigenvectors remains check incoherence normalized eigenvectors ﬁrst second expressions simply rearrangements other matter form study; consider second expression entrywise thus sufﬁces show proof follows closely lemma geer however author uses different problem scaling ours care must taken applying lemma. first abbreviate deﬁne recall maximum column norm hard proof corollary covered balls radius nj−/ζ respect norm covered balls radius cj−/ζ respect scaled norm theorem carl implies convex hull conv respect another constant covered balls radius converting back entropy bound original metric noting conv n/j. also balls radiusn/j. construction then ≤n/j ≤n/j .gjd ≤n/j means covered balls radiusn/j. balls require radius ofn/j words balls require radius ofn/j.", "year": 2014}