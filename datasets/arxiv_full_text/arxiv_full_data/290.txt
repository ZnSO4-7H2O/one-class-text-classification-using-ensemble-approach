{"title": "Multi-task Learning Of Deep Neural Networks For Audio Visual Automatic  Speech Recognition", "tag": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "abstract": "Multi-task learning (MTL) involves the simultaneous training of two or more related tasks over shared representations. In this work, we apply MTL to audio-visual automatic speech recognition(AV-ASR). Our primary task is to learn a mapping between audio-visual fused features and frame labels obtained from acoustic GMM/HMM model. This is combined with an auxiliary task which maps visual features to frame labels obtained from a separate visual GMM/HMM model. The MTL model is tested at various levels of babble noise and the results are compared with a base-line hybrid DNN-HMM AV-ASR model. Our results indicate that MTL is especially useful at higher level of noise. Compared to base-line, upto 7\\% relative improvement in WER is reported at -3 SNR dB", "text": "multi-task learning involves simultaneous training related tasks shared representations. work apply audio-visual automatic speech recognition. primary task learn mapping audio-visual fused features frame labels obtained acoustic gmm/hmm model. combined auxiliary task maps visual features frame labels obtained separate visual gmm/hmm model. model tested various levels babble noise results compared base-line hybrid dnn-hmm avasr model. results indicate especially useful higher level noise. compared base-line upto relative improvement reported build noise robust automatic speech recognition system incorporate complementary information different modality independent noise speech. example audio-visual system visual information speaker’s region combined audio inputs shown provide signiﬁcant reduction word error rates audio modality corrupted noise. inspired fact human perception speech dependent auditory visual senses demonstrated famous mcgurk effect. traditionally av-asr systems implemented using gmm/hmm models model modality separate gmm/hmm test time fuse decisions stream linearly combining log-likelihoods overall likelihood. feature fusion method audio visual features combined usually concatenation followed dimensionality reduction step. fused features modeled single gmm/hmm model. advancement deep learning based techniques speech recognition corresponding av-asr systems based deep learning proposed deep learning based systems tend outperform gmm/hmm models several reasons. firstly mixture model acts experts model whereas product experts. also gmm/hmm systems require uncorrelated inputs beneﬁt multiple frames input whereas case dnn.correspondingly deep learning based av-asr systems shown perform better gmm/hmm counterparts. similar gmm/hmm feature fusion decision fusion possible dnn-hmm systems. decision fusion modality modeled separate network. midlevel fusion features also possible fusing hidden layer outputs separate audio visual features deep networks. fusion models based deep learning single task learning methods fusion network predicts posterior probabilities labels state space common audio visual modalities. alternative approach would assume different label modalities train network tasks simultaneously using shared representation. called multi-task learning successfully applied various problems speech recognition speech synthesis multilingual acoustic modeling. tasks considered primary task auxiliary task. auxiliary task helps better feature estimation hidden layers proper generalization model. better generalization results improved robustness noise. model trained parameters corresponding auxiliary task usually discarded. work explore application av-asr systems. model visual stream separate gmm/hmm whose states used classes auxiliary task. compare method baseline dnn-hmm based model. show gives better performance especially higher noise levels. cross-entropy costs corresponds output state utterance time given modality correspond soft-max outputs task. corresponds input fused features utterance time mtl-dnn architecture used work shown figure dimensional audio features dimensional visual features fused simple concatenation. dimensional audio-visual features spliced frames provide contextual information. mtl-dnn trained follows primary task inputs audio-visual fused features audio visual modalities present. addition primary task also trained fused features either audio visual modality suppressed setting corresponding features small values. secondary task inputs fused features audio modality suppressed i.e. inputs correspond visual modality. system trained tested grid audio-visual corpus. corpus collection audio video recordings speakers uttering sentences. utterance approximately seconds length. syntactic structures sentences similar. simple language model following structure built work. dataset effect consisted utterances used training cross validation remaining data used test set. training test data contain utterances speakers. models trained tested using kaldi speech recognition tool kaldi+pdnn. base-line base-line system single task learning feature fusion model somewhat similar network input network dimensional vector. feature extraction described frame labels obtained primary task mtl-dnn described difference mtl-dnn stl-dnn absence secondary task stl-dnn. explained followed description primary auxiliary tasks av-asr. training procedure experimental results discussed section section discusses relationship work previous av-asr methods. finally summarize work section sampling rate audio data converted khz. frame speech signal duration ﬁlter-bank features dimensions extracted. mean variance normalization performed. video frame rate increased match rate audio frames interpolation. region interest corresponding area surrounding speaker’s mouth extracted follows frame converted gray scale face detection performed using viola-jones algorithm. region extracted detecting landmark points speakers face cropping surrounding speakers mouth chin. dimensional features extracted roi. similar audio features perform mean variance normalization. primary task along related secondary/auxiliary tasks learnt simultaneously shared representation data. secondary task acts type regularization results better generalization model. model trained parameters corresponding secondary task ignored. cost function network given work primary well secondary tasks hybrid dnn-hmm models. dnn-hmm hybrid systems frame level classiﬁers estimate posterior probabilities tied states given acoustic feature vector. frame labels obtained ﬁrst training bootstrap gmm/hmm model aligning data it.we train tri-phone gmm/hmm acoustic model linear discriminant analysis transformed audio features input. alignments obtained audio feature sequences acoustic model used labels primary task mtl-dnn correspondingly secondary task train another tri-phone gmm/hmm visual model levels noise base-line system gives slightly better performance compared mtldnn models.however noise increases difference performance improvement mtl-dnn stl-dnn becomes signiﬁcant. mtldnn gives nearly relative improvement stl-dnn gives relative improvement. among mtl-dnn models model gives better performance consistent idea regularization auxiliary task reduces over-ﬁtting results better performance corrupted inputs. however experiments found increasing further resulted degradation performance primary task. last table corresponds lip-reading audio modality suppressed. mtl-dnn gives better performance compared stl-dnn approximately relative gain relative gain multi-task learning successfully applied various problems speech synthesis multilingual acoustic modeling applications application av-asr suggested although results reported. training procedure extent similar like separate inputs outputs tasks. work inspired authors employ unsupervised learning methods obtain shared representations audio visual modalities used separate supervised training step. work learning shared representation well supervised training taken care multi-task learning. contrast employ low-level feature fusion. training suppress modalities ensure network overﬁt particular modality. addition results refig. mtl-dnn architecture. dimensional audio ﬁlterbank features concatenated dimensional visual features. fused features spliced context frames. hidden layer dimension soft-max layers correspond audio video tied states using mini-batch stochastic gradient descent frames shufﬂed randomly epoch. batch size initial learning rate learning schedule adopted i.e. improvement cross-validation accuracy successive epochs falls learning rate halved.the halving performed subsequent epoch increase frame level accuracy less point training halted. models tested four levels babble noise clean audio. noise added test data artiﬁcially mixing babble noise clean audio ﬁles. test time secondary task outputs ignored. decoding performed weighted finite state transducer built using gmm/hmm acoustic model grid corpus lexicon grid language model. model tested audio-visual features audio features i.e. visual features small values. posteriors converted log-likelihoods passed decoder outputs ﬁnal word sequences. results tabulated table line previous av-asr systems visual modality turned mtl-dnn stl-dnn provide signiﬁcant gains compared audio input. large gains case part small vocabulary simple language model grid corpus. work applied multi-task learning audio-visual speech recognition. described primary auxiliary tasks train model. proposed network architecture training protocol network. model compared baseline stl-dnn model various levels babble noise. results indicate results improvement baseline model especially higher levels noise. future work would like compare ensemble learning av-asr. jing huang gerasimos potamianos chalapathy neti improving audio-visual speech recognition infrared headset avsp -international conference audio-visual speech processing neﬁan luhong liang xiaobo xiaoxing kevin murphy dynamic bayesian networks audio-visual speech recognition eurasip journal advances signal processing vol. george dahl dong deng alex acero context-dependent pre-trained deep neural networks large-vocabulary speech recognition ieee transactions audio speech language processing vol. jing huang brian kingsbury audio-visual deep learning noise robust speech recognition ieee international conference acoustics speech signal processing. ieee kuniaki noda yuki yamaguchi kazuhiro nakadai hiroshi okuno tetsuya ogata audio-visual speech recognition using deep learning applied intelligence vol. geoffrey hinton deng dong george dahl abdel-rahman mohamed navdeep jaitly andrew senior vincent vanhoucke patrick nguyen tara sainath deep neural networks acoustic modeling speech recognition shared views four research groups ieee signal processing magazine vol. gerasimos potamianos chalapathy neti guillaume gravier ashutosh garg andrew senior recent advances automatic recognition audiovisual speech proceedings ieee vol. ronan collobert jason weston uniﬁed architecture natural language processing deep neural networks multitask learning proceedings international conference machine learning. georg heigold vincent vanhoucke alan senior patrick nguyen ranzato matthieu devin jeffrey dean multilingual acoustic models using distributed deep neural networks ieee international conference acoustics speech signal processing. ieee youssef mroueh etienne marcheret vaibhava goel deep multimodal learning audio-visual speech recognition ieee international conference acoustics speech signal processing ieee deng dong yi-fan gong alex acero chin-hui study multilingual acoustic modeling large vocabulary ieee international conference acoustics speech signal processing. ieee vahid kazemi josephine sullivan millisecond face alignment ensemble regression trees proceedings ieee conference computer vision pattern recognition martin cooke barker stuart cunningham shao audio-visual corpus speech perception automatic speech recognition journal acoustical society america vol. daniel povey arnab ghoshal gilles boulianne lukas burget ondrej glembek nagendra goel mirko hannemann petr motlicek yanmin qian petr schwarz kaldi speech recognition toolkit ieee workshop automatic speech recognition understanding. ieee signal processing society number epfl-conf-. zhizheng cassia valentini-botinhao oliver watts simon king deep neural networks employing multi-task learning stacked bottleneck features ieee international conspeech synthesis ference acoustics speech signal processing ieee", "year": 2017}