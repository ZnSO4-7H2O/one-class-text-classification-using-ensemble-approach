{"title": "Automatic Text Scoring Using Neural Networks", "tag": ["cs.CL", "cs.LG", "cs.NE", "I.5.1; I.2.6; I.2.7"], "abstract": "Automated Text Scoring (ATS) provides a cost-effective and consistent alternative to human marking. However, in order to achieve good performance, the predictive features of the system need to be manually engineered by human experts. We introduce a model that forms word representations by learning the extent to which specific words contribute to the text's score. Using Long-Short Term Memory networks to represent the meaning of texts, we demonstrate that a fully automated framework is able to achieve excellent results over similar approaches. In an attempt to make our results more interpretable, and inspired by recent advances in visualizing neural networks, we introduce a novel method for identifying the regions of the text that the model has found more discriminative.", "text": "automated text scoring provides cost-effective consistent alternative human marking. however order achieve good performance predictive features system need manually engineered human experts. introduce model forms word representations learning extent speciﬁc words contribute text’s score. using long-short term memory networks represent meaning texts demonstrate fully automated framework able achieve excellent results similar approaches. attempt make results interpretable inspired recent advances visualizing neural networks introduce novel method identifying regions text model found discriminative. automated text scoring refers statistical natural language processing techniques used automatically score text marking scale. advantages systems established since project essay grade earliest systems whose development largely motivated prospect reducing labour-intensive marking activities. addition providing cost-effective efﬁcient approach large-scale grading text systems ensure consistent application marking criteria therefore facilitating equity scoring. burstein rudner liang elliot landauer briscoe yannakoudakis sakaguchi among others) overviews found various studies implicitly explicitly previous work primarily treated text scoring supervised text classiﬁcation task utilized large selection techniques ranging syntactic parsers vectorial semantics combined dimensionality reduction generative discriminative machine learning. multiple factors inﬂuence quality texts systems typically exploit large range textual features correspond different properties text grammar vocabulary style topic relevance discourse coherence cohesion. addition lexical part-of-speech ngrams linguistically deeper features types syntactic constructions grammatical relations measures sentence complexity among properties form system’s internal marking criteria. ﬁnal representation text typically consists vector features manually selected tuned predict score marking scale. although current approaches scoring regression ranking shown achieve performance indistinguishable human examiners substantial manual effort involved reaching results different domains genres prompts forth. linguistic features intended capture aspects writing assessed hand-selected tuned speciﬁc domains. order perform well different data separate models distinct feature sets typically tuned. propose recurrent neural network models ats. multi-layer neural networks known automatically learning useful features data lower layers learning basic feature detectors upper levels learning high-level abstract features additionally recurrent neural networks well-suited modeling compositionality language shown perform well task language modeling therefore propose apply network structures task scoring order improve performance systems learn required feature representations dataset automatically without need manual tuning. speciﬁcally focus predicting holistic score extended-response writing items. however automated models panacea deployment depends largely ability examine characteristics whether measure intended measured whether internal marking criteria interpreted meaningful useful way. deep architecture neural network models however makes rather difﬁcult identify extract properties text network identiﬁed discriminative. therefore also describe preliminary method visualizing information model exploiting assigning speciﬁc score input text. section describe number inﬂuential and/or recent approaches automated text scoring non-native english-learner writing. project essay grade earliest automated scoring systems predicting score using linear regression vectors textual features considered proxies writing quality. intelligent essay assessor uses latent semantic analysis compute semantic similarity between texts speciﬁc grade points test text assigned score based ones training similar. lonsdale strong-krause link grammar parser analyse score texts based average sentence-level bayesian essay test scoring system investigates multinomial bernoulli naive bayes models classify texts based shallow content style features. erater developed educational testing service ﬁrst systems deployed operational scoring high-stakes assessments. model uses number different features including aspects grammar vocabulary style whose weights ﬁtted marking scheme regression. chen voting algorithm address text scoring within weakly supervised bag-of-words framework. yannakoudakis extract deep linguistic features employ discriminative learning-to-rank model outperforms regression. recently mcnamara used hierachical classiﬁcation approach scoring utilizing linguistic semantic rhetorical features among others. farra utilize variants logistic linear regression develop models score persuasive essays based features extracted opinion expressions topical elements. also attempts incorporate diverse features text scoring models. klebanov flor demonstrate essay scoring performance improved adding model information percentages highly associated mildly associated dis-associated pairs words co-exist given text. somasundaran exploit lexical chains interaction discourse elements evaluating quality persuasive essays respect discourse coherence. crossley identify student attributes standardized test scores predictive writing success conjunction textual features develop essay scoring models. kaggle sponsored hewlett foundation hosted automated student assessment prize contest aiming demonstrate capabilities automated text scoring systems dataset released consists around twenty thousand texts produced middle-school englishspeaking students part experiments develop models. model learns word embeddings ranking activation true sequence higher activation ‘noisy’ counterpart objective model becomes minimize hinge loss ensures activations original ‘noisy’ ngrams differ least following tang extend previous model capture local linguistic environment word also word contributes overall score essay. construct representations which along linguistic information given linear order words sentence able capture usage information. words appear essay score considered under-informative sense activate equally high scoring essays. informative words hand ones would impact essay score order capture score-speciﬁc word embeddings extend adding further linear unit output layer performs linear regression predicting essay score. using collobert weston collobert introduce neural network architecture learns distributed representation word corpus based local context. concretely suppose want learn representation target word found n-sized sequence words based words exist sequence wt). order derive representation model learns discriminate ‘noisy’ counterpart target word substituted randomly sampled word vocabulary every word predictive local context random word corpus. every word mapped real-valued vector mapping function rd×|v| embedding matrix column network takes input concatenating vectors words found rnd. similarly formed substituting reﬂect fact mis-spelled words tend appear lower scoring essays. using sswes correctly spelled words pulled apart vector space incorrectly spelled ones retaining however information labtop copmuter still contextually related long-short term memory network sswes obtained model derive continuous representations essay. treat essay sequence tokens explore unibi-directional longshort term memory networks order embed sequences vector ﬁxed size. unibi-directional lstms effectively used embedding long sequences lstms kind recurrent neural network architecture output time conditioned input time time hyper-parameter determining error functions weighted. values closer place weight scorespeciﬁc aspect embeddings whereas values closer favour contextual information. fig. shows advantage using sswes present setting. based solely information provided linguistic environment words computer laptop going placed together mis-spelled counterparts copmuter labtop this however figure comparison standard score-speciﬁc word embeddings. virtue appearing similar environments standard neural embeddings place correct incorrect spelling closer vector space. however since mistakes found lower scoring essays sswes able discriminate correct incorrect versions without loss contextual meaning. interpretation word point might different know word ti+. effective around issue train lstm bidirectional manner. requires forward backward pass sequence hidden layer element therefore re-written concatenation forward backward hidden vectors feed embedding word found essay lstm time zero-padding shorter sequences. form d-dimensional essay embeddings taking activation lstm layer timestep last word essay presented network. case bi-directional lstms independent passes essay concatenated together predict essay score. essay embeddings linear unit output layer predicts essay score mean square error predicted gold score loss function optimize rmsprop propagating errors back word embeddings. figure single-layer long short term memory network. word vectors enter input layer time. hidden layer formed last timestep used predict essay score using linear regression. also explore bi-directional lstms ‘deeper’ representations stack lstm layers hidden layer shown here. element-wise non-linear functions logistic sigmoid ez+); hadamard hyperbolic tangent widely used approaches text scoring. parse data using rasp parser extract number different features assessing quality essays. speciﬁcally character part-of-speech unigrams bigrams trigrams; word unigrams bigrams trigrams replace open-class words pos; distribution common nouns prepositions coordinators. additionally extract features rules phrasestructure tree based parse sentence well estimate error rate based manually-derived error rules. ngrams weighted using tf–idf rest count-based scaled features approximately order magnitude. ﬁnal input vectors unit-normalized account varying text-length biases. above also explore distributed memory model paragraph vectors proposed mikolov means directly obtain essay embeddings. pv-dm takes input word vectors make ngram sequences uses predict next word sequence. feature pv-dm however ‘paragraph’ assigned unique vector used prediction. vector therefore acts ‘memory’ retaining information contexts appeared paragraph. paragraph vectors linear regression model obtain essay scores kaggle dataset contains essays ranging words each marked raters essays written students ranging grade grade comprising eight distinct sets elicited eight different prompts distinct marking criteria score range. experiments resolved combined score raters calculated average raters’ scores determined third expert currently state-of-the-art dataset achieved cohen’s however test released withgold score annotations rendering comparisons futile therefore restricted splitting given training create test set. sets divided follows entire dataset reserved training/validation testing. training/validation subset used actual training remaining validation facilitate future work release validation test essays used experiments addition source code various hyperparameter values. results hyperparameters model follows sizes layers learning rate window size number ‘noisy’ sequences weighting factor also hyperparameters lstm size lstm layer dlst well dropout rate since search space would massive grid model docvec lstm blstm two-layer lstm two-layer blstm wordvec lstm wordvec blstm wordvec two-layer lstm wordvec two-layer blstm wordvecpre-trained two-layer blstm sswe lstm sswe blstm sswe two-layer lstm sswe two-layer blstm table results different models kaggle dataset. resulting vectors trained using linear regression. optimized parameters using separate validation report results test set. search best hyperparameters determined using bayesian optimization context performance models validation modeled sample gaussian process constructing probabilistic model error function exploiting model make decisions next evaluate function. hyperparameters baselines also determined using methodology. training except preﬁxed ‘wordvecpre-trained’ uses pre-trained embeddings google news corpus. report spearman’s rank correlation coefﬁcient pearson’s product-moment correlation coefﬁcient root mean square error predicted scores gold standard test considered appropriate metrics evaluating essay scoring systems however also report cohen’s quadratic weights evaluation metric used kaggle competition. performance models shown table terms correlation svms produce competitive results outperforming docvec lstm blstm well deep counterparts. described above model rich linguistic knowledge consists hand-picked features achieved excellent performance similar tasks however terms rmse among lowest performing models together ‘blstm’ ‘two-layer blstm’. deep models combination wordvec svms comparable terms though terms rmse former produce better results rmse improving half docvec also produces competitive rmse results though correlation much lower blstms trained wordvec embeddings among competitive models terms correlation outperform models except ones using pre-trained embeddings sswes. increasing number hidden layers and/or adding bi-directionality always improve performance clearly helps case performance improves compared uni-directional counterparts. using pre-trained word embeddings improves results further. speciﬁcally found ‘wordvecpre-trained two-layer blstm’ best conﬁguration increasing correlation reducing rmse note however entirely fair comparison trained much larger corpus training nevertheless sswes models able outperform ‘wordvecpre-trained two-layer blstm’ even though embeddings trained fewer data points. speciﬁcally best model improves correlation well rmse giving maximum increase around correlation. given results pre-trained model believe performance best sswe model improve training data given discussion sswe lstm approach prior knowledge grammar language domain text able score essays human-like outperforming state-ofthe-art systems. furthermore tuned models’ hyperparameters separate validation perform pre-processing text simple tokenization. essay scoring literature text length tends strong predictor overall score. order investigate possible effects essay length also calculate correlation gold scores length essays. correlations test relatively therefore conclude strong effects. described above used bayesian optimization optimal hyperparameter conﬁgurations fewer steps regular grid search. using approach optimization model showed clear preferences parameters associated better scoring models number ‘noisy’ sequences weighting factor size lstm layer dlst optimal value consistently shows sswe approach necessary capture usage words. performance dropped considerably increased using equivalent using basic model found performance considerably lower similar discussion) rather approach. finally optimal value dlst corpus-dependent. section inspired recent advances convolutional neural networks computer vision text summarization introduce novel method generating interpretable visualizations network’s performance. present context particularly important advantage manual methods discussed able know grounds model made decisions features discriminative. outset goal assess ‘quality’ word vectors. ‘quality’ mean level word appearing particular context would prove problematic network’s prediction. order identify ‘high’ ‘low’ quality vectors perform single pass essay left right lstm make score prediction. normally would provide gold scores adjust network weights based error gradients. instead provide network pseudo-score taking maximum score speciﬁc essay take provide ‘gold’ score. word vector ‘high’ quality going little adjustment weights order predict highest score possible. conversely providing minimum possible score assess ‘bad’ word vectors are. vectors require minimal adjustment reach lowest score considered ‘lower’ quality. note since complete pass network vector quality going essay dependent. loss induced feeding pseudo-scores taking magnitude error vector since limw→ magnitude tell much embedding needs change order achieve gold score case provide minimum pseudo-score value closer zero would indicate incorrectly used word. results reported here combine magnitudes produced giving maximum minimum pseudo-scores single score computed where show examples visualization procedure table model capable providing positive feedback. correctly placed punctuation long-distance dependencies particularly favoured model. conversely model deal well proper names able cope mistakes however seen sentence model perfect returns false negative case satisﬁed. sometimes incorrectly model would able distinguish them. possible solutions problem either provide gold score timestep results computationally expensive endeavour feed sentences phrases smaller size scoring would consistent. paper introduced deep neural network model capable representing local contextual usage information encapsulated essay scoring. model yields score-speciﬁc word embeddings used later recurrent neural network order form essay representations. shown kind architecture able surpass similar state-of-the-art systems well systems based manual feature engineering achieved results close upper bound past work. also introduced novel exploring basis network’s internal scoring criteria showed models interpretable exploited provide useful feedback author. note visualization technique used show ‘goodness’ phrases/sentences. within phrase setting feeding last word phrase network lstm layer contain phrase embedding. then assess ‘goodness’ embedding evaluating error gradients predicting highest/lowest score. briscoe medlock øistein andersen. automated assessment esol free text examinations. technical report ucam-cl-tr- university cambridge computer laboratory nov. ciprian chelba tom´aˇs mikolov mike schuster thorsten brants phillipp koehn tony robinson. billion word benchmark measuring progress statistical language modeling. arxiv preprint. ronan collobert uniﬁed architecture weston. natural language processing deep neural networks multitask learning. proceedings twentyfifth international conference machine learning pages july. scott crossley laura allen erica snow danielle mcnamara. pssst... textual features... automatic proceedings essay scoring fifth international conference learning analytics knowledge pages acm. misha denil alban demiraj kalchbrenner phil blunsom nando freitas. modelling visualising summarising documents single convolutional neural network. jun. noura farra swapna somasundaran jill burstein. scoring persuasive essays using opinions targets. proceedings tenth workshop innovative building educational applications pages michael gutmann aapo hyv¨arinen. noise-contrastive estimation unnormalized statistical models applications natural image statistics. mach. learn. res. february. beata beigman klebanov michael flor. word association proﬁles automated scoring essays. proceedings annual meeting association computational linguistics pages thomas landauer darrell laham peter foltz. automated scoring annotation essays intelligent essay assessor. m.d. shermis j.c. burstein editors automated essay scoring cross-disciplinary perspective pages honglak roger grosse rajesh ranganath andrew convolutional deep belief networks scalable unsupervised learning hierarchical representations. proceedings annual international conference machine learning icml lonsdale strong-krause. automated rating proceedings hlt-naacl essays. workshop building educational applications using natural language processing. danielle mcnamara scott crossley roscoe laura allen jianmin dai. hierarchical classiﬁcation approach automated essay scoring. assessing writing tom´aˇs mikolov stefan kombrink anoop deoras luk´aˇs burget ˇcernock´y. rnnlm-recurrent neural network language modeling toolkit. asru demo session. duyu tang. sentiment-speciﬁc representation learning document-level sentiment analysis. proceedings eighth international conference search data mining wsdm association computing machinery yannakoudakis ronan cummins. evaluating performance automated text scoring systems. proceedings tenth workshop innovative building educational applications. association computational linguistics helen yannakoudakis briscoe medlock. dataset method automatically grading esol texts. annual meeting association computational linguistics human language technologies proceedings conference june portland oregon pages sutskever chen corrado jeffrey dean. distributed representations words phrases advances neural compositionality. information processing systems pages keisuke sakaguchi michael heilman nitin madnani. effective feature integration automated short answer scoring. proceedings tenth workshop innovative building educational applications.", "year": 2016}