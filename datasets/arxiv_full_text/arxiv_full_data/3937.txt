{"title": "Instantaneously Trained Neural Networks", "tag": ["cs.NE", "cs.AI"], "abstract": "This paper presents a review of instantaneously trained neural networks (ITNNs). These networks trade learning time for size and, in the basic model, a new hidden node is created for each training sample. Various versions of the corner-classification family of ITNNs, which have found applications in artificial intelligence (AI), are described. Implementation issues are also considered.", "text": "networks trade learning time size basic model hidden node created training sample. various versions cornerclassification family itnns found applications artificial intelligence described. implementation issues also considered. cell neuron fundamental unit. number neurons connections neurons enormous; ensemble enables brain surpass computational capacity supercomputers existence today. artificial neural networks models brain implement mapping task completed certain sense. networks able learn training samples well generalize them makes interesting research area. unfortunately doubtful current models ever learning capacity match performance living systems. furthermore number iterations required generalization often excessive motivates look network designs learning fast. comparison biological systems noted memory stored single area brain distributed. also different parts brain required processing different kinds memories. hippocampus parahippocampal region areas cerebral cortex support declarative cognitive memory amygdala striatum cerebellum support nondeclarative behavioral memory. declarative knowledge classified working episodic semantic memories requires processing medieval temporal region parts thalamus non-declarative knowledge requires processing basil ganglia. semantic memory represents individual’s knowledge general whereas episodic memory structures individual’s experiences. step design connectionist intelligent machines done several models. however learning generalization time popular models large. ability store information quickly provides capacity common biological systems obvious applications computational systems. models deficient intelligence consequence quantum information processing brain current models quantum information processing suffer shortcomings consider paper trained neural networks matches biological capacity fast learning. paper organized follows section provides brief historical background research neural networks. itnns described section applications presented section hardware implementations specific type itnn presented section pitts showed network built sufficient number neurons proper weights capable feasible computation. network suffered limitation lacked capacity learn. d.hebb gave first neural network learning capability based correlation principle neuron repeatedly stimulated neuron times neuron active neuron become sensitive stimuli neuron lead notion adjustable synaptic weights incorporated neural networks know today. frank rosenblatt proposed class neural networks called perceptrons. believed neural networks could perform function minsky papert pointed certain computations limitations perceptrons specifically showed could perform tasks like exclusive-xor. field dormant backpropagation algorithm introduced. considered breakthrough area neural networks algorithm suffers drawback converge adaline invented widrow group. adaptive learning algorithm used adjust weights minimizing mean square error. madaline consisted many adaline networks parallel. output either depending output individual adaline units. techniques functional approximation mixture models layer neural network hidden unit implements radial activated function. output units implement weighted hidden unit outputs. rbfs possess nonlinear approximation properties hence model complex mappings. mapping input nonlinear linear output. commonly used rbfs gaussian piecewise-linear approximation cubic approximation multiquadratic function inverse-multiquadratic function. rbf’s convergence weights faster less sensitive interference compared network. task hand neural network learn model embedded; observations made pooled form training samples train network. observations pooled either labeled unlabeled depending weather sample input-output pair not. learning classified supervised unsupervised depending weather sample labeled unlabeled. wizard functions. training done updating contents chips. input output must digitized binary vectors hence back draw network cannot used time series prediction. schematic diagram node shown below. node performs logical functions returns value given input vector. discriminator individual nodes. network consists discriminators parallel discriminator trained recognize different class pattern. initially locations discriminator zero training done applying input pattern input nodes value responded noisy version presented output function number previously written locations accessed noisy input. proportional similarity input pattern without noise input pattern noise. probabilistic neural network viewed normalized network hidden unit training value hidden units called kernels typically probability density functions gaussian. hidden-to-output weights usually hidden unit weight used connection output true connections given weights possibility assign weights prior probabilities class. weights need learned widths units. principle operation based statistical technique combining bayes strategy nonparametric estimation technique. estimator gaussian probability distribution function continuous valued vectors normalized unit length. input units receive inputs feed pattern units form product weight vector perform nonlinear operation product feeds summation unit activation level. nonlinear operation performed exponential function form exp/σ]. called smoothing parameter. good performance takes values training connecting pattern unit output appropriate summation unit connection weight one. using bayes decision theory test pattern assigned category priori probability occurrence patterns category loss associated classifying test pattern category reality belongs output unit receives output summation multiplied network drawback cannot used applications involving function approximation. need generalized regression neural networks application involves function approximation. principal itnn corner classification neural network includes variant neural network networks attempt model biological memory. networks purely instantaneous need learning. however learning could done quickly certain conditions. generalization capacity seems almost good backpropagation networks memory divided three types sensory short-term long-term memory. duration information retained shortest sensory memory longest greatest long-term memory short-term memory stands sensory long-term memory. short-time memory also called working memory involves ruminate thoughts encountered information fades approximately twenty seconds renewed rehearsal. short-term memory needs protected overloading sensory stimulation cognitive processes help preventing overloading sensory gating selective attention. sensory gating process certain channels turned others turned off. selective attention process culling information received channel i.e. abate information received channel favor information entering channel. amount information short-term memory hold limited extended grouping information. corner classification network based idea phonological loop visio-spatial sketchpad proposed three variations advanced variants also known class neural networks. neural network overcame generalization problem plagued earlier network. hamming distance used classification binary vectors i.e. test vector whose hamming distance training vector smaller radius generalization network classified output class training vector. unique neuron associated training sample node network acts filter training sample. filter realized making hyper plane separate corner n-dimensional cube represented training vector hence name corner-classification technique. shown better networks category number input output neurons equal length input output patterns vectors. number hidden neurons equal number training samples network requires. last node input layer bias hidden layer. binary step function used activation function hidden output neurons. output function summation positive zero otherwise. weight link base node hidden neuron r–s+ radius generalization number ones input sequence. weights output layer equal output value output value amounts learning input class complement thus instantaneous. radius generalization seen considering all-zero input vectors choice depend nature generalization sought. network also suffers draw back input output data must digitized. interpret temperature express degree hotness coldness like truth statement might true false. might another might etc. principle fuzzy classification networks depend concept nearest neighbor consists three layers-an input layer hidden layer output layer. acronym seen either stand fuzzy classification fast classification represented weight vector elements represented output product vectors network trained passes samples first pass assigns synaptic weights second pass determines radius generalization training sample. fuzzification location training sampler assigning fuzzy membership functions output classes input vectors. network behaves classifier classifier according weather input vector falls within radius generalization training vector hence radius generalization acts switch classifier classifier. network meets specifications traditional function approximation every data point covered given training sample also cover’s theorem separability patterns. practical case values determined sample size fraction sample size. network operating classifier viewed network provided membership function chosen gaussian distributed moreover weighting function chosen membership function network considered kernel regression. time series prediction pattern classification data fusion metasearch engines. areas naturally covers many specific applications. example time series historical data time index network predicts future values based past values. time series examples investigated henon mackey-glass time series chaotic time series therefore statistical properties remain unchanged. patterns. neural networks trained order recognize patterns. term pattern classification refers process network learns given training values mapping given input-output pairs attempts assign input patterns output classes predefined training classes. studies show itnns almost well backpropagation networks basic pattern classification problems discarding redundant results. itnns used fuse data application. method proposed network built using keywords pages. individual words either depending weather bottom list search results. mplementation algorithm speaks success commercial arena. algorithm implemented using reconfigurable computing design optical neural network .the network implemented fpgas. sutton implemented kak’s network fpgas. celoxica board xilinx virtex-ii chip used jhdl hardware description language used design simulation hardware implementation. hidden neuron circuit based euclidian distance kak’s network implementation fpga would require computational complexity square operations number neurons number elements weight vector. hence different distance metrics proposed city block distance distance. noted general distance metric defined names shown case different distance metrics justified experiments done estlick gave acceptable results activation circuit implemented n-bit constant comparator radius generalization constant training. bitonic selection network used implement circuit. implementation fpga requires algorithm simple modular highly parallel networks possess characteristics. ease implementation networks fpga justifies claim itnns modeled hardware commercialized. reconfigurable computing using fine grained parallelism. shortt keating moulinier pannell made optical implementation neural network using bipolar matrix vector multiplier suitable modifications structure training algorithm required build optical neural network implementing n-parity. paper provides review neural networks emphasis itnns. corner classification network learns creating hidden node training sample. advantage speed consequence price paid size network. hidden neurons pruned size remain much larger backpropagation network. applications size matter networks attractive alternative backpropagation. although evaluated various kinds chaotic time series performance networks various benchmark data remains checked. estlick algorithmic transformations implementation kmeans clustering reconfigurable hardware proceedings ninth international symposium field-programmable gate arrays. sigda california. gazzaniga cognitive neurosciences press cambridge mass. hebb organization behavior. wiley york j.h. holland adaptation natural artificial systems. univ michigan press minsky papert perceptrons. press cambridge minsky society mind. simon schuster york e.a. patrick fundamentals pattern recognition. prentice hall englewood", "year": 2006}