{"title": "A general framework for the IT-based clustering methods", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Previously, we proposed a physically inspired rule to organize the data points in a sparse yet effective structure, called the in-tree (IT) graph, which is able to capture a wide class of underlying cluster structures in the datasets, especially for the density-based datasets. Although there are some redundant edges or lines between clusters requiring to be removed by computer, this IT graph has a big advantage compared with the k-nearest-neighborhood (k-NN) or the minimal spanning tree (MST) graph, in that the redundant edges in the IT graph are much more distinguishable and thus can be easily determined by several methods previously proposed by us.  In this paper, we propose a general framework to re-construct the IT graph, based on an initial neighborhood graph, such as the k-NN or MST, etc, and the corresponding graph distances. For this general framework, our previous way of constructing the IT graph turns out to be a special case of it. This general framework 1) can make the IT graph capture a wider class of underlying cluster structures in the datasets, especially for the manifolds, and 2) should be more effective to cluster the sparse or graph-based datasets.", "text": "laboratory neuroinformation ministry education china school life science technology university electronic science technology china chengdu china *corresponding author. abstract previously proposed physically inspired rule organize data points sparse effective structure called in-tree graph able capture wide class underlying cluster structures datasets especially density-based datasets. although redundant edges lines clusters requiring removed computer graph advantage compared k-nearest-neighborhood minimal spanning tree graph redundant edges graph much distinguishable thus easily determined several methods previously proposed paper propose general framework re-construct graph based initial neighborhood graph k-nn corresponding graph distances. general framework previous constructing graph turns special case general framework make graph capture wider class underlying cluster structures datasets especially manifolds effective cluster sparse graph-based datasets. alex alessandro published clustering method science magazine titled clustering fast search find density peaks theoretically simple sound technically fast effective reliable. method belong class density-based clustering methods cluster centers usually associated density peak points. previously density peak points searched based zero-gradient assumption density peak points zero gradient assumed density function. representative popular approach called mean-shift based assumption mean-shift provides simple iterative search density peaks. although mean-shift effective confronts well known problems applicable data points euclidean space; computationally costly sensitive density ripples undesired local namely density function fitted well density ripples noise extreme point ripples also zero gradient thus falsely treated cluster center. also last paragraph alex alessandro’ paper. also early months proposed serial it-based clustering methods start physical imagination space treated horizontal rubber sheet data points particles mass. intuitively swarm particles sheet particles curve sheet downward turn curved sheet force particles move higher lower potential areas. last particles cluster certain places locally lowest potentials. process actually clustering behavior swarm particles applicable high-dimensional space irrespective space euclidean space thus motivating devise similar approach cluster data points. process constructing curved rubber sheet equivalent constructing non-uniform space varying field potential based assumed rules like this particle assumed generate gaussian field magnitude exponentially declined distance; fields different particles additive. methods mimic particles’ clustering behavior non-uniform space different described previously kind moving behavior associated force concept since newton’s force theory well-known. force concept mathematically modeled gradient thus data points stop cluster places zero gradient. again assumption appears. similar case happens previous density-based clustering methods mentioned section force-based method also share problems mean-shift clustering method. contrast don’t seek analytic solution view particles’ clustering behavior analytically. instead view general abstract level. consequently force gradient concepts involved. instead simple general behavior rule introduced particles tendency moving higher lower potential areas descending tendency. moreover required particle move place another without process. here process continuous space smooth trajectory whereas discrete data’s space here approximated zigzag path along data points serve transfer stations. choose transfer station? proximity principle natural choice. according descending tendency proximity principle moving behavior data points last becomes concise rule call nearest neighbor descent rule. descent refers descending tendency high potential areas nearest neighbor corresponds proximity principle. shown fig. treat descending action rule directed edge fantasy graph structure appears. fantasy structure proves in-tree graph detailed also section definition graph. note that rule treat nodes equally general perspective. words particularities extreme points temporarily ignored left behind. local extreme problem gradient-based methods confront involved here. instead leaves redundant edges problem. clustering purpose redundant edges need determined removed computer consequently independent sub-graph represent cluster. however usually easy computer determine redundant edges since usually salient edges. fig. illustration previous based clustering framework input data points. graph. point root node. clustering purpose redundant undesired edge clusters needs removed. independent sub-graphs obtained removing purple edge cluster. node sub-graphs path reach corresponding root node process finding root nodes along edge directions. data points root nodes assigned clusters. order readers deeply realize salient feature redundant edges graph make comparison graph three common neighborhood graphs minimal spanning tree delaunay triangulation k-nearest-neighbor fig. that since elongated clusters close contaminated noise redundant edges clusters neighborhood graphs short thus hard computer distinguish edges. contrast redundant edge clusters graph looks much longer since doesn’t connect neighboring nodes instead usually starts node center cluster ends node another cluster also notice rest edges graph still connect neighboring nodes. makes redundant edge fig. distinguishable neighborhood graphs values assigned nodes graph. information helps devise easy effective rule redundant edges determined removed computer. fig. comparison show saliency redundant edge physically-inspired graph. colors nodes denote magnitudes potential values. redder nodes larger potential magnitudes are. physically inspired graph well captures underlying cluster structure data sets except imperfect place exist redundant edges clusters. hard computer determine redundant edges saliency feature them. fact made attempts edge-cutting issue. interactive semi-supervised methods proposed redundant edges. combined structure affinity propagation consequently automatic cutting method called g-ap proposed therein. interestingly g-ap turns powerful since g-ap discover non-spherical clusters cannot. combined structure isometric feature mapping consequently interactive method called it-map proposed therein. also interestingly it-map preserve clusters mapping data points low-dimensional euclidean space whereas hard isomap so-called crowding problem. moreover first paper serials also demonstrated details alex alessandro’ decision graph method actually viewed another simple interactive reliable method remove redundant edges. because simply using intermediate variables graph i.e. potential edge distance also derive similar pop-out points actually correspond start nodes redundant edges. methods constitute group clustering methods it-based clustering methods. fact admit it’s amazing coincidence rule turns similar alex alessandro’s clustering method. that hand method methods determine redundant edges graph; hand methods viewed complement theirs theory methodology levels theory physical background graph-based implementation first paper provides physical graphical explanation efficiency behind clustering method thus increasing understanding methodology compared graph perspective provides concrete problem i.e. redundant edge problem consequently motivated extend tree forest words concrete problem lead answers. work continue extend it-based clustering methods family. however rather propose methods remove redundant edges propose general construct graph. notice first paper that actually defaults consider nodes complete graph means node connected directly nodes edges thus complete graph actually ignores underlying structure information datasets. also notice fig. that although like graph ignore redundant undesired edges clusters however similarity word previous framework also deal sparse distance similarity matrix. neighborhood graphs also capture underlying structure data sparse form. fact neighborhood graphs capable that since rely pair-wise distance information construct graphs thus general meaning wider class datasets. reminiscent famous manifold method—isomap builds classic multidimensional scaling dimensionality reduction method k-nn graph consequently isomap able deal important class data manifolds. here also build previous framework neighborhood graphs instead complete graph make it-based clustering methods able deal larger range clustering problems. graphs graph theory structures data point viewed node relationship pair data points represented edge. edge length usually represents distance data points. note graphs imaginary auxiliary models help understand principles methods. following graph neighborhood graphs belong scope. in-tree graph also called in-arborescence in-branching graph directed graph meets following conditions node directed edge started node directed edge started cycle connected graph. brief graph connected directed acyclic graph together beautiful order neighborhood graphs either parametric non-parametric. typical parametric neighborhood graphs k-nearest-neighborhood ϵ-nearest-neighborhood graph. k-nn graph node selects neighbors nearest nodes. ϵ-nn graph node takes neighbors typical non-parametric neighborhood node within radius i.e. graphs mentioned fig. together relative neighborhood graph gabriel graph non-parametric graphs connected graphs. designed find underlying structure data relying pair-wise distances similarities different rules goals. instance graph aims connect nodes least edge lengths. interestingly namely sub-graph relationships sub-graph etc. removing edges mst. fig. illustration proposed method. k-nn graph respectively. corresponding global potential graphs shown corresponding graphs shown colors nodes denote magnitudes potentials. decision graphs corresponding graphs respectively. boxes drawn user computer judge points inside pop-out points. pop-out points correspond start nodes redundant edges clustering results. data points colors assigned clusters. computed pair nodes step graph distance graph here graph distance refers shortest path distance. therefore connected graph nodes different sub-graphs since path them. graph distance computed classic floyd dijkstra algorithm fast approach provided makes good sparseness graph. parameter adjusted users. note that compared previous work differences here graph distance used; distance squared here call graphs graph-distance-based potential values nodes global potential graphs based different initial graphs k-nn different global geometric potential graphs gpg-knn gpg-mst gpg-dt obtained shown fig. respectively potential values denoted different colors. node lowest potential. suggested data define index term bring advantages. moreover step actually approximation traditional gradient-based methods consequently local extreme problem gradient-based methods largely reduced. directed edge graph connect node node constructed shown fig. contains nodes directed points edge edges methods proposed previous works it-maps g-ap int-cut sup-cut here demonstrate effectiveness work show connection framework. mainly features denotes potential node length directed edge started node thus node similar decision graph coordinated variables shown fig. fig. start nodes magnitude undesired edges denoted high values thus fig. c~c. turn undesired redundant edges indirectly determined start nodes since node graph start node directed edge. effective interactive method help computer determine redundant edges graph since always start nodes redundant edges scatter plot irrespective whether graph redundant edges visualized not. last step computer determine members sub-graph. since sub-graph still graph root node computer first nodes find corresponding root nodes. since node graph directed path reach root node searching along directed edges node sure find roots finite steps. specific step know node first transfer node node descends next transfer point node? node words next transfer nodes node functions store node node serve index next transfer nodes. index updated time sure stop steps. proved ⌈log⌉steps need maximum number edges directed paths graph cutting. denotes stop criterion change happens updating. last root node sub-graph node resides. conclusion steps similar isomap except choices constructing initial graph step steps serve obtain graph steps almost distance first work except graph distance participates computation here. last steps previous works except fact methods used remove redundant edges step table overview general framework it-based clustering methods input distance output vector stores roots nodes. steps construct neighborhood graph k-nn etc. compute graph distance compute potential *words behind annotations *abbr. k-nearest neighborhood; minimal spanning tree; delaunay triangulation; relative neighborhood graph gabriel graph in-tree; decision graph; g-ap generalized affinity propagation; first tested two-gaussian dataset appeared fig. using three different neighborhood graphs k-nn respectively. fig. seen redundant edges neighborhood graphs non-salient whereas proposed method transfer salient graphs shown fig. respectively. fact results robust wide range choices parameter shown table instance used step arbitrarily chose different values among interval leading excellent results i.e. clustering results consistent visual perception graphs also salient. salient means redundant edges pop-out points distinguishable results fig. note that graphs here shown fig. little different alex alessandro’s paper here number pop-out points less number clusters obtained. also tested different shapes datasets shown table initial graphs k-nn graphs fixed values varying values also arbitrarily chose several values large range possible. instance spiral dataset small large although jain data range appropriate relatively narrow still progress since dataset easy clustered either previous framework alex alessandro’s decision graph. figure lists test result data set. fig. several results different datasets. shows decision graph clustering result data set. bottom data sets spiral flame aggragation jain compound datasets; parameters also tested dataset table varying values fixed value here refers parameter k-nn graph. shown table results quite robust choice details results fig. note fig. k-nn graph connected five independent sub-graphs. therefore pop-out points fig. corresponding undesired edges sub-graphs fig. proposed general framework makes effective combination bridge. consequently inherits advantages neighborhood graphs rule i.e. neighborhood graphs flexible capture underlying clustering structures datasets; rule construct graph whose redundant edges easy determined. effect fig. underlying structure first captured neighborhood graphs transformed rule salient graph salient refers redundant edges easy determined. fact fig. that edge length graphs serve suitable measure make redundant edges salient enough distinguished edges. progress structure since usually cannot rely edge length determine redundant edges. framework make graph capture wider class underlying clustering structures datasets. consequently shown fig. method able detect clustering structure jain compound datasets. framework becomes flexible powerful. shown table. multiple choices remove redundant edges also multiple choices constructing initial graph certain neighborhood graph superior graphs dealing specific problems. note graph used step corresponding graph distance also called minimum curvilinearity distance believe proposed general framework reveal meaningful underlying structures relationships datasets input distance matrixs sparse datasets graph-based. because previously distance points without direct connection considered contrast framework since points still indirectly connected path graph thus distance rodriguez laio clustering fast search find density peaks. science fukunaga hostetler estimation gradient density function applications pattern recognition. ieee trans. inf. theory cheng mean shift mode seeking clustering. ieee trans. pattern anal. mach. intell. hinneburg keim efficient approach clustering large multimedia databases noise. proceedings international conference knowledge discovery data mining agrawal p.e. stolorz piatetsky-shapiro eds. zhang extended fast search clustering algorithm widely density clusters density peaks. arxiv preprint arxiv.. yang physically inspired clustering algorithm evolve like particles. arxiv preprint arxiv.. generalized affinity propagation clustering algorithm nonspherical cluster discovery. arxiv preprint arxiv.. it-map effective nonlinear dimensionality preprint reduction method arxiv.. clustering descending nearest neighbor delaunay graph space. arxiv preprint arxiv.. ruta gabrys framework machine learning based dynamic physical fields. natural computing zahn graph-theoretical methods detecting describing gestalt clusters. ieee trans. comput. delaunay sphere vide. izv. akad. nauk sssr otdelenie matematicheskii estestvennyka nauk frey dueck clustering passing messages data points. science tenenbaum silva langford global geometric science framework gross yellen graph theory applications toussaint relative neighbourhood graph finite planar set. pattern recognit. gabriel sokal statistical approach geographic variation analysis. systematic biology toussaint proximity graphs nearest neighbor decision rules recent progress. interface floyd algorithm shortest path. commun. dijkstra note problems connexion graphs. numerische mathematik kumar grama gupta karypis introduction parallel computing design analysis algorithms chang yeung robust path-based spectral clustering. pattern recognit. medico flame novel fuzzy clustering method analysis microarray data. bioinf. knowl. discovery data jain data clustering user’s dilemma. pattern recognition machine intelligence biswas cannistraci ravasi montevecchi ideker alessio nonlinear dimension reduction clustering minimum curvilinearity unfold neuropathic pain tissue embryological classes. bioinformatics i-i. cannistraci alanis-lobato ravasi minimum curvilinearity enhance topological prediction protein interactions network embedding. bioinformatics i-i.", "year": 2015}