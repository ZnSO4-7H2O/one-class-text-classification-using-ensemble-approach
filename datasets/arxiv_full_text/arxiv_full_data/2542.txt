{"title": "Deep Exploration via Bootstrapped DQN", "tag": ["cs.LG", "cs.AI", "cs.SY", "stat.ML"], "abstract": "Efficient exploration in complex environments remains a major challenge for reinforcement learning. We propose bootstrapped DQN, a simple algorithm that explores in a computationally and statistically efficient manner through use of randomized value functions. Unlike dithering strategies such as epsilon-greedy exploration, bootstrapped DQN carries out temporally-extended (or deep) exploration; this can lead to exponentially faster learning. We demonstrate these benefits in complex stochastic MDPs and in the large-scale Arcade Learning Environment. Bootstrapped DQN substantially improves learning times and performance across most Atari games.", "text": "eﬃcient exploration remains major challenge reinforcement learning common dithering strategies exploration \u0001-greedy carry temporally-extended exploration; lead exponentially larger data requirements. however algorithms statistically eﬃcient computationally tractable complex environments. randomized value functions oﬀer promising approach eﬃcient exploration generalization existing algorithms compatible nonlinearly parameterized value functions. ﬁrst step towards addressing contexts develop bootstrapped dqn. demonstrate bootstrapped combine deep exploration deep neural networks exponentially faster learning dithering strategy. arcade learning environment bootstrapped substantially improves learning speed cumulative performance across games. study reinforcement learning problem agent interacts unknown environment. agent takes sequence actions order maximize cumulative rewards. unlike standard planning problems agent begin perfect knowledge environment learns experience. leads fundamental trade-oﬀ exploration versus exploitation; agent improve future rewards exploring poorly understood states actions require sacriﬁcing immediate rewards. learn eﬃciently agent explore valuable learning opportunities. further since action long term consequences agent reason informational value possible observation sequences. without sort temporally extended exploration learning times worsen exponential factor. theoretical literature oﬀers variety provably-eﬃcient approaches deep exploration however designed markov decision processes small ﬁnite state spaces others require solving computationally intractable planning tasks algorithms practical complex environments agent must generalize operate eﬀectively. reason large-scale applications relied upon statistically ineﬃcient strategies exploration even exploration review related literature detail section common dithering strategies \u0001-greedy approximate value action single number. time pick action highest estimate sometimes choose another action random. paper consider alternative approach eﬃcient exploration inspired thompson sampling. algorithms notion uncertainty instead maintain distribution possible values. explore randomly select policy according probability optimal policy. recent work shown randomized value functions implement something similar thompson sampling without need intractable exact posterior update. however work restricted linearly-parameterized value functions present natural extension approach enables complex non-linear generalization methods deep neural networks. show bootstrap random initialization produce reasonable uncertainty estimates neural networks computational cost. bootstrapped leverages uncertainty estimates eﬃcient exploration. demonstrate beneﬁts extend large scale problems designed highlight deep exploration. bootstrapped substantially reduces learning times improves performance across games. algorithm computationally eﬃcient parallelizable; single machine implementation runs roughly slower dqn. deep neural networks represent state many supervised reinforcement learning domains want exploration strategy statistically computationally eﬃcient together representation value function. explore eﬃciently ﬁrst step quantify uncertainty value estimates agent judge potential beneﬁts exploratory actions. neural network literature presents sizable body work uncertainty quantiﬁcation founded parametric bayesian inference actually found simple non-parametric bootstrap random initialization eﬀective experiments main ideas paper would apply approach uncertainty dnns. bootstrap princple approximate population distribution sample distribution common form bootstrap takes input data estimator generate sample bootstrapped distribution data cardinality equal sampled uniformly replacement bootstrap sample estimate taken bootstrap widely hailed great advance century applied statistics even comes theoretical guarantees figure present eﬃcient scalable method generating bootstrap samples large deep neural network. network consists shared architecture bootstrapped heads branching independently. head trained bootstrapped sub-sample data represents single bootstrap sample shared network learns joint feature representation across data provide signiﬁcant computational advantages cost lower diversity heads. type bootstrap trained eﬃciently single forward/backward pass; thought data-dependent dropout dropout mask head ﬁxed data point figure presents example uncertainty estimates bootstrapped neural networks regression task noisy data. trained fully-connected -layer neural networks rectiﬁed linear units layer bootstrapped samples data. standard initialize networks random parameter values induces important initial diversity models. unable generate eﬀective uncertainty estimates problem using dropout approach prior literature details provided appendix bootstrapped selected policy optimal value maxπ scale large problems learn parameterized estimate q-value function rather tabular encoding. neural network estimate value. q-learning update state action reward state given scalar learning rate target network parameters ﬁxed several important modiﬁcations q-learning update improve stability first algorithm learns sampled transitions experience buﬀer rather learning fully online. second algorithm uses target network parameters copied learning network every time steps kept ﬁxed updates. double modiﬁes target bootstrapped modiﬁes approximate distribution q-values bootstrap. start episode bootstrapped samples single q-value function approximate posterior. agent follows policy optimal sample duration episode. natural adaptation thompson sampling heuristic allows temporally extended exploration implement algorithm eﬃciently building bootstrapped estimates q-value function parallel figure importantly value function function heads trained target network means provide temporally extended estimate value uncertainty estimates. order keep track data belongs bootstrap head store ﬂags indicating heads privy data. approximate bootstrap sample selecting uniformly random following duration episode. present detailed algorithm implementation bootstrapped appendix related work observation temporally extended exploration necessary eﬃcient reinforcement learning new. prior distribution mdps optimal exploration strategy available dynamic programming bayesian belief state space. however exact solution intractable even simple systems. many successful applications focus generalization planning address exploration ineﬃcient exploration even none however exploration strategies highly ineﬃcient. many exploration strategies guided principle optimism face uncertainty algorithms exploration bonus values state-action pairs lead useful learning select actions maximize adjusted values. approach ﬁrst proposed ﬁnite-armed bandits principle extended successfully across bandits generalization tabular except particular deterministic contexts methods lead eﬃcient complex domains computationally intractable. work aims eﬀective bonus variation dqn. resulting algorithm relies large number hand-tuned parameters suitable application deterministic problems. compare results atari appendix bootstrapped oﬀers signiﬁcant improvement previous methods. perhaps oldest heuristic balancing exploration exploitation given thompson sampling bandit algorithm takes single sample posterior every time step chooses action optimal time step. apply thompson sampling principle agent sample value function posterior. naive applications thompson sampling resample every timestep extremely ineﬃcient. agent must also commit sample several time steps order achieve deep exploration algorithm psrl exactly this state guarantees however algorithm still requires solving single known usually intractable large systems. algorithm bootstrapped approximates approach exploration randomized value functions sampled approximate posterior. recently authors proposed rlsvi algorithm accomplishes linearly parameterized value functions. surprisingly rlsvi recovers state guarantees setting tabular basis functions performance crucially dependent upon suitable linear representation value function extend ideas produce algorithm simultaneously perform generalization exploration ﬂexible nonlinear value function representation. method simple general compatible almost advances deep computational cost tuning parameters. uncertainty estimates allow agent direct exploration potentially informative states actions. bandits choice directed exploration rather dithering generally categorizes eﬃcient algorithms. story simple directed exploration enough guarantee eﬃciency; exploration must also deep. deep exploration means exploration directed multiple time steps; also called planning learn far-sighted exploration. unlike bandit problems balance actions immediately rewarding immediately informative settings require planning several time steps exploitation means eﬃcient agent must consider future rewards several time steps simply myopic rewards. exactly eﬃcient exploration require taking actions neither immediately rewarding immediately informative. illustrate distinction consider simple deterministic chain three step horizon starting state known agent priori deterministic actions left right. states zero reward except leftmost state known reward rightmost state unknown. order reach either rewarding state informative state within three steps agent must plan consistent strategy several time steps. figure depicts planning look ahead trees several algorithmic approaches example mdp. action left gray action right black. rewarding states depicted informative states blue. dashed lines indicate agent plan ahead either rewards information. unlike bandit algorithms agent plan exploit future rewards. agent deep exploration plan learn. testing deep exploration present series didactic computational experiments designed highlight need deep exploration. environments described chains length figure episode interaction lasts steps point agent resets initial state problems intended expository rather entirely realistic. balancing well known mildly successful strategy versus unknown potentially rewarding approach emerge many practical applications. environments described ﬁnite tabular mdp. however consider algorithms interact pixel features. consider feature mappings φhot φtherm present results φtherm worked better variants better generalization diﬀerence relatively small appendix thompson bootstrapped resamples every timestep. ensemble uses architecture bootstrapped ensemble policy. algorithm successfully learned optimal policy successfully completed hundred episodes optimal reward chain length learning algorithm episodes across three seeds. plot median time learn figure together conservative lower bound expected time learn shallow exploration strategy bootstrapped demonstrates graceful scaling long chains require deep exploration. bootstrapped drive deep exploration? bootstrapped explores manner similar provably-eﬃcient algorithm psrl uses bootstrapped neural network approximate posterior sample value. unlike psrl bootstrapped directly samples value function require planning steps. algorithm similar rlsvi also provably-eﬃcient neural network instead linear value function bootstrap instead gaussian sampling. analysis linear setting suggests nonlinear approach work well long distribution remains stochastically optimistic least spread correct posterior. bootstrapped relies upon random initialization network weights prior induce diversity. surprisingly found initial diversity enough maintain diverse generalization unseen states large deep neural networks. eﬀective experimental setting work situations. general necessary maintain rigorous notion prior potentially artiﬁcial prior data maintain diversity potential explanation eﬃcacy simple random initialization unlike supervised learning bandits networks data heads unique target network. this together stochastic minibatch ﬂexible nonlinear representations means even small diﬀerences initialization become bigger reﬁt unique errors. bootstrapped require single network initialized correct policy right every step would exponentially unlikely large chains algorithm successful example require networks generalize diverse actions never chosen states visited often. imagine that example above network made state never observed action right long head imagines bootstrapping propagate signal back target network drive deep exploration. expected time estimates propagate least head grows gracefully even relatively small experiments show. expand upon intuition video designed highlight bootstrapped demonstrates deep exploration https//youtu.be/ekuv_demk. present evaluation diﬃcult stochastic appendix arcade learning environment evaluate algorithm across atari games arcade learning environment importantly unlike experiments section domains speciﬁcally designed showcase algorithm. fact many atari games structured small rewards always indicate part optimal policy. crucial strong performance observed dithering strategies. exploration bootstrapped produces signiﬁcant gains versus \u0001-greedy setting. bootstrapped reaches peak performance roughly similar dqn. however improved exploration mean reach human performance average faster across games. translates signiﬁcantly improved cumulative rewards learning. follow setup network architecture benchmark performance algorithm. network structure identical convolutional structure except split separate bootstrap heads convolutional layer figure recently several authors provided architectural algorithmic improvements ddqn compare results since advances orthogonal concern could easily incorporated bootstrapped design. full details experimental available appendix examine generate online bootstrap samples computationally eﬃcient manner. focus three questions many heads need pass gradients shared network bootstrap data online? make signiﬁcant compromises order maintain computational cost comparable dqn. figure presents cumulative reward bootstrapped game breakout diﬀerent number heads heads leads faster learning even small number heads captures beneﬁts bootstrapped dqn. choose shared network architecture allows train combined network backpropagation. feeding network heads shared convolutional network eﬀectively increases learning rate portion network. games leads premature sub-optimal convergence. found best ﬁnal scores normalizing gradients also leads slower early learning. appendix details. implement online bootstrap independent bernoulli mask w..wk∼ber head episode. ﬂags stored memory replay buﬀer identify heads trained data. however trained using shared minibatch algorithm also require eﬀective iterations; undesirable computationally. surprisingly found algorithm performed similarly irrespective outperformed shown figure strange discuss phenomenon appendix however light empirical observation atari chose save minibatch passes. result bootstrapped runs similar computational speed vanilla identical hardware. eﬃcient exploration atari bootstrapped drives eﬃcient exploration several atari games. amount game experience bootstrapped generally outperforms \u0001-greedy exploration. figure demonstrates eﬀect diverse selection games. games performs well bootstrapped typically performs better. bootstrapped reach human performance amidar beam rider battle zone summarize improvement learning time consider number frames required reach human performance. bootstrapped reaches human performance frames improved figure shows bootstrapped typically reaches human performance signiﬁcantly faster. games reach human performance bootstrapped solve problem itself. challenging atari games deep exploration conjectured important results entirely successful still promising. frostbite bootstrapped reaches second level much faster network instabilities cause performance crash. montezuma’s revenge bootstrapped reaches ﬁrst frames properly learn experience. results suggest improved exploration help solve remaining games also highlight importance problems like network instability reward clipping temporally extended rewards. double-or-nothing bootstrap ensemble bootstrapping all. implementation less increase wall-time versus dqn. improved training method prioritized replay help solve problem. overall performance bootstrapped able learn much faster dqn. figure shows bootstrapped also improves upon ﬁnal score across games. however real beneﬁts eﬃcient exploration mean bootstrapped outperforms orders magnitude terms cumulative rewards learning optimize auc- normalized version cumulative returns frames. according metric averaged across games consider improve upon base best method obtain bootstrapped dqn. present results together results tables across games appendix visualizing bootstrapped present insight bootstrapped drives deep exploration atari. game although head learns high scoring policy policies quite distinct. video https//youtu.be/zmkoto_m show evolution policies simultaneously several games. although head performs well follow unique policy. contrast \u0001-greedy strategies almost indistinguishable small values totally ineﬀectual larger values. believe deep exploration improved learning since diverse experiences allow better generalization. disregarding exploration bootstrapped beneﬁcial purely exploitative policy. combine heads single ensemble policy example choosing action votes across heads. approach might several beneﬁts. first ensemble policy often outperform individual policy. second distribution votes across heads give measure uncertainty optimal policy. unlike vanilla bootstrapped know doesn’t know. application executing poorly-understood action dangerous could crucial. video https//youtu.be/jveccjvgy visualize ensemble policy across several games. uncertainty policy surprisingly interpretable heads agree clearly crucial decision points remain diverse less important steps. closing remarks paper present bootstrapped algorithm eﬃcient reinforcement learning complex environments. demonstrate bootstrap produce useful uncertainty estimates deep neural networks. bootstrapped computationally tractable also naturally scalable massive parallel systems. believe that beyond speciﬁc implementation randomized value functions represent promising alternative dithering exploration. bootstrapped practically combines eﬃcient generalization exploration complex nonlinear value functions. references marc bellemare yavar naddaf joel veness michael bowling. arcade learning environment evaluation platform general agents. arxiv preprint arxiv. peter bickel david freedman. asymptotic theory bootstrap. annals arthur guez david silver peter dayan. eﬃcient bayes-adaptive reinforcement learning using sample-based search. advances neural information processing systems pages nitish srivastava geoﬀrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research appendix discuss experimental setup qualitatively evaluate uncertainty methods deep neural networks. this generated twenty noisy regression pairs with drawn uniformly none numerical choices important except represent highly nonlinear function lots noise several clear regions uncertain. present regression data together indication generating distribution figure results unsatisfactory several reasons. first network extrapolates mean posterior outside range actual data believe dropout perturbs locally single neural network unlike bootstrap. second posterior samples dropout approximation spiky look like sensible posterior sample. third network collapses almost zero uncertainty regions data. spent time altering dropout scheme eﬀect might undesirable stochastic domains believed might artefact implementation. however thought believe eﬀect would expect dropout posterior approximations. figure present didactic example taken author’s website right hand side plot generate noisy data wildly diﬀerent values. training neural network using criterion means network surely converge mean noisy data. dropout samples remain highly concentrated around mean. contrast bootstrapped neural networks include diﬀerent subsets noisy data produce intuitive uncertainty estimates settings. note isn’t necessarily failure dropout approximate gaussian process posterior artefact could shared homoskedastic posterior. authors propose heteroskedastic variant help address fundamental issue large networks trained convergence dropout samples converge every single datapoint... even outliers. paper focus bootstrap approach uncertainty neural networks. like simplicity connections established statistical methodology empirical good performance. however insights paper deep exploration randomized value functions. compatible approximate posterior estimator deep neural networks. believe area uncertainty estimates neural networks remains important area research right. bootstrapped uncertainty estimates q-value functions another crucial advantage dropout appear supervised problem. unlike random dropout masks trained random target networks implementation bootstrap trains temporally consistent target network. means bootstrap estimates able bootstrap estimates long value. important quantify long uncertainty drive deep exploration. algorithm gives full description bootstrapped dqn. captures modes operation either neural networks used estimate qk-value functions neural network heads used estimate q-value functions. cases largely parameterisation issue denote value function networks output network head. core idea full bootstrapped algorithm bootstrap mask mask decides value function whether train upon experience generated step simplest form binary vector length masking including value function training time step experience tuple). masking distribution responsible generating example yields whose components independently drawn bernoulli distribution parameter corresponds double-or-nothing bootstrap hand yields mask ones algorithm reduces ensemble method. poisson masks provides natural parallel standard non-parameteric boostrap since exponential masks closely resemble standard bayesian nonparametric posterior dirichlet process periodically replay buﬀer played back update parameters value function network gradients value function tuple replay buﬀer naive implementation bootstrapped builds complete networks distinct memory buﬀers. method parallelizable many machines however wanted produce algorithm eﬃcient even single machine. this implemented bootstrap heads single larger network like figure without shared network. implement bootstrap masking episode data according ber. figure demonstrate bootstrapped implement deep exploration even relatively small values however results robust scalable larger experiments example figure surprisingly method even eﬀective complete data sharing heads. degenerate full sharing information turns remarkably eﬃcient training large deep neural networks. discuss phenomenon appendix generating good estimates uncertainty enough eﬃcient exploration. figure methods trained network architecture totally ineﬀective implementing deep exploration. \u0001-greedy policy follows q-value estimate. allow policy evaluated without dithering. ensemble policy trained exactly bootstrapped except stage algorithm follows policy majority vote bootstrap heads. thompson sampling bootstrapped except head sampled every timestep rather every episode. bootstrapped demonstrates eﬃcient deep exploration domain. diﬃcult stochastic figure shows bootstrapped implement eﬀective exploration similar deep architectures fail. however since underlying system small ﬁnite several simpler strategies would also solve problem. consider diﬃcult variant chain system signiﬁcant stochastic noise transitions depicted figure action left deterministically moves agent left action right successful time otherwise also moves left. agent interacts episodes length begins episode optimal policy head right. bootstrapped unique amongst scalable approaches eﬃcient exploration deep stochastic domains. benchmark performance implement three algorithms which unlike bootstrapped receive true tabular representation mdp. algorithms based three state approaches exploration dithering optimism posterior sampling discuss choice benchmarks appendix figure present empirical regret algorithm averaged seeds ﬁrst thousand episodes. empirical regret cumulative diﬀerence expected rewards optimal policy realized rewards algorithm. bootstrapped achieves similar performance state eﬃcient exploration schemes psrl even without prior knowledge tabular structure noisy environments. telling much better bootstrapped state optimistic algorithm ucrl. although figure seems suggest ucrl incurs linear regret actually follows bounds number states number actions. example figure attempted display performance compared several benchmark tabula rasa approaches exploration. many algorithms could considered short paper chose focus common approach pre-eminent optimistic approach posterior sampling common heuristic approaches optimistic initialization q-learning tuned work well domain however precise parameters sensitive underlying mdp. make general-purpose version heuristic essentially leads optimistic algorithms. since ucrl originally designed inﬁnite-horizon mdps natural adaptation algorithm state guarantees ﬁnite horizon mdps well figure displays empirical regret algorithms together bootstrapped example figure somewhat disconcerting ucrl appears incur linear regret proven satisfy near-optimal regret bounds. actually show figure algorithm produces regret scales similarly established bounds similarly even tiny problem size recent analysis proves near optimal sample complexity ﬁxed horizon problems guarantees fewer suboptimal episodes. bounds acceptable worst case scaling much practical use. figure include mean performance bootstrapped one-hot feature encodings. found that using features bootstrapped learned optimal policy seeds somewhat less robust thermometer encoding. seeds failed learn optimal policy within episodes presented figure atari games experiments. step agent corresponds four steps emulator action repeated reward values agents clipped stability. evaluate agents report performance based upon scores. convolutional part network used identical used input network tensor rescaled grayscale version last four observations. further diﬃcult extend idea optimistic initialization function generalization ﬁrst convolutional layer ﬁlters size stride second conv layer ﬁlters size stride last conv layer ﬁlters size split network beyond ﬁnal layer distinct heads fully connected identical single head consists fully connected layer units followed another fully connected layer q-values action. fully connected layers rectiﬁed linear units non-linearity. normalize gradients head. trained networks rmsprop momentum learning rate discount number steps target updates steps. trained agents total steps game corresponds frames. agents every frames evaluation bootstrapped ensemble voting policy. experience replay contains recent transitions. update network every steps randomly sampling minibatch transitions replay buﬀer exact minibatch schedule dqn. training used \u0001-greedy policy annealed linearly ﬁrst timesteps. literature deep atari focuses learning best single evaluation policy particular attention whether human performance unusual literature typically focuses upon cumulative ﬁnal performance. bootstrapped makes signiﬁcant improvements cumulative rewards atari display figure peak performance much found using bootstrapped without gradient normalization head typically learned even faster implementation rescaling somewhat prone premature suboptimal convergence. present example phenomenon figure found that order better benchmark best policies reported helpful gradient normalization. however entirely clear whether represents improvement settings. figures present cumulative rewards algorithms beam rider. system deployed learn real interactions cumulative rewards present better measure performance. settings beneﬁts gradient normalization less clear. however even normalization bootstrapped signiﬁcantly outperforms terms cumulative rewards. reﬂected clearly figure table setting network heads share data actually traditional bootstrap all. diﬀerent regression task section bootstrapped data essential obtain meaningful uncertainty estimates. several theories networks maintain signiﬁcant diversity even without data bootstrapping setting. build upon intuition section first train diﬀerent target networks. means even facing datapoint still lead drastically diﬀerent q-value updates. second atari deterministic environment transition observation unique correct datapoint setting. third networks deep initialized diﬀerent random values likely quite diverse generalization even agree given data. finally since variants take many many frames update policy likely even using would still populate replay memory identical datapoints. means using save minibatch passes seems like reasonable compromise doesn’t seem negatively aﬀect performance much setting. research needed examine exactly where/when data sharing important. table average score achieved agents successful evaluation period compared human performance uniformly random policy. implementation hyperparameters speciﬁed above using double qlearning update.. peak ﬁnal performance similar bootstrapped previous benchmarks. compare beneﬁts exploration bootstrapped benchmark performance similar prior work incentivizing exploration atari this compute auc- measure speciﬁed work. present results table compare best performing strategy well implementation dqn. importantly bootstrapped outperforms prior work signiﬁcantly. alien amidar assault asterix asteroids atlantis bank heist battle zone beam rider bowling boxing breakout centipede chopper command crazy climber demon attack double dunk enduro fishing derby freeway frostbite gopher gravitar hero hockey jamesbond kangaroo krull kung master montezuma revenge pacman name game pong private qbert riverraid road runner robotank seaquest space invaders star gunner tennis time pilot tutankham venture video pinball wizard zaxxon compare method results paper introduce measure performance called auc- something similar normalized cumulative rewards million frames. table displays results reference bootstrapped boot-dqn. reproduce reference results that average bootstrapped implementations outperform dynamic best algorithm previous work. game dynamic produces best results bowling diﬀerence bowling dominated implementation dqn* dqn. bootstrapped still gives improvement relevant baseline. overall clear boot-dqn+ performs best terms auc- metric. averaged across games better next best competitor bootstrapped gradient normalization. however terms peak performance frames boot-dqn generally reached higher scores. boot-dqn+ sometimes plateaud early figure highlights important distinction evaluation based best learned policy versus cumulative rewards discuss appendix bootstrapped displays biggest improvements well learning important.", "year": 2016}