{"title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on  ImageNet Classification", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.", "text": "rectiﬁed activation units essential state-of-the-art neural networks. work study rectiﬁer neural networks image classiﬁcation aspects. first propose parametric rectiﬁed linear unit generalizes traditional rectiﬁed unit. prelu improves model ﬁtting nearly zero extra computational cost little overﬁtting risk. second derive robust initialization method particularly considers rectiﬁer nonlinearities. method enables train extremely deep rectiﬁed models directly scratch investigate deeper wider network architectures. based prelu networks achieve top- test error imagenet classiﬁcation dataset. relative improvement ilsvrc winner knowledge result ﬁrst surpass human-level performance visual recognition challenge. convolutional neural networks demonstrated recognition accuracy better comparable humans several visual recognition tasks including recognizing trafﬁc signs faces handwritten digits work present result surpasses human-level performance generic challenging recognition task classiﬁcation task -class imagenet dataset last years witnessed tremendous improvements recognition performance mainly advances technical directions building powerful models designing effective strategies overﬁtting. hand neural networks becoming capable ﬁtting training data increased complexity enlarged width smaller strides nonlinear activations sophisticated layer designs hand better generalization achieved effective regularization techniques aggressive data augmentation large-scale data among advances rectiﬁer neuron e.g. rectiﬁed linear unit several keys recent success deep networks expedites convergence training procedure leads better solutions conventional sigmoidlike units. despite prevalence rectiﬁer networks recent improvements models theoretical guidelines training rarely focused properties rectiﬁers. paper investigate neural networks aspects particularly driven rectiﬁers. first propose generalization relu call parametric rectiﬁed linear unit activation function adaptively learns parameters rectiﬁers improves accuracy negligible extra computational cost. second study difﬁculty training rectiﬁed models deep. explicitly modeling nonlinearity rectiﬁers derive theoretically sound initialization method helps convergence deep models trained directly scratch. gives ﬂexibility explore powerful network architectures. -class imagenet dataset prelu network leads single-model result top- error surpasses existing multi-model results. further multi-model result achieves top- error test relative improvement ilsvrc winner best knowledge result surpasses ﬁrst time reported human-level performance visual recognition challenge. runs positions feature map. channel-shared variant gradient sums channels layer. time complexity prelu negligible forward backward propagation. adopt momentum method updating momentum learning rate. worth noticing weight decay updating weight decay tends push zero thus biases prelu toward relu. even without regularization learned coefﬁcients rarely magnitude larger experiments. further constrain range activation function non-monotonic. initialization throughout paper. conducted comparisons deep efﬁcient model weight layers. model studied architecture described table choose model sufﬁcient representing category deep models well make experiments feasible. baseline train model relu applied convolutional layers ﬁrst fullyconnected layers. training implementation follows top- top- errors imagenet using -view testing input nonlinear activation channel coefﬁcient controlling slope negative part. subscript indicates allow nonlinear activation vary different channels. becomes relu; learnable parameter refer eqn. parametric relu figure shows shapes relu prelu. eqn. equivalent min. small ﬁxed value prelu becomes leaky relu motivation lrelu avoid zero gradients. experiments show lrelu negligible impact accuracy compared relu. contrary method adaptively learns prelu parameters jointly whole model. hope end-to-end training lead specialized activations. prelu introduces small number extra parameters. number extra parameters equal total number channels negligible considering total number weights. expect extra risk overﬁtting. also consider channel-shared variant table small deep -layer model ﬁlter size ﬁlter number layer listed. number indicates stride used. learned coefﬁcients prelu also shown. channel-wise case average {ai} channels shown layer. table comparisons relu prelu small model. error rates imagenet using -view testing. images resized shorter side training testing. view models trained using epochs. train architecture scratch relus replaced prelus top- error reduced gain relu baseline. table also shows channel-wise/channelshared prelus perform comparably. channelshared version prelu introduces extra free parameters compared relu counterpart. small number free parameters play critical roles evidenced gain baseline. implies importance adaptively learning shapes activation functions. table also shows learned coefﬁcients prelus layer. interesting phenomena table first ﬁrst conv layer coefﬁcients signiﬁcantly greater ﬁlters conv mostly gabor-like ﬁlters edge texture detectors learned results show positive negative responses ﬁlters respected. believe economical exploiting lowlevel information given limited number ﬁlters second channel-wise version deeper conv layers general smaller coefﬁcients. implies activations gradually become more nonlinear increasing depths. words learned model tends keep information earlier stages becomes discriminative deeper stages. rectiﬁer networks easier train compared traditional sigmoid-like activation networks. initialization still hamper learning highly non-linear system. subsection propose robust initialization method removes obstacle training extremely deep rectiﬁer networks. recent deep cnns mostly initialized random weights drawn gaussian distributions ﬁxed standard deviations deep models difﬁculties converge reported team also observed experiments. address issue pre-train model conv layers initialize deeper models. strategy requires training time also lead poorer local optimum. auxiliary classiﬁers added intermediate layers help convergence. glorot bengio proposed adopt properly scaled uniform distribution initialization. called xavier initialization derivation based assumption activations linear. assumption invalid relu prelu. following derive theoretically sound initialization taking relu/prelu account. experiments initialization method allows extremely deep models converge xavier method cannot. here kc-by- vector represents co-located pixels input channels. spatial ﬁlter size layer. denoting number connections response d-by-n matrix number ﬁlters represents weights ﬁlter. vector biases response pixel output map. index layer. activation. also dl−. reshaped kd-by- vector. denote note c-by-ˆn matrix ﬁlters rearranged back-propagation. note reshaped other. cby- vector representing gradient pixel layer. above assume independent other zero mean initialized symmetric distribution around zero. back-propagation also f∆xl+ derivative relu case zero probabilities equal. assume ∆xl+ independent other. thus also var. compute variance gradient eqn. ﬁrst layer need compute represents image domain. still adopt eqn. ﬁrst layer reason forward propagation case factor single layer make overall product exponentially large/small. note sufﬁcient either eqn. eqn. alone. example eqn. nlvar eqn. nl/ˆnl c/dl diminishing number common network designs. means initialization properly scales backward signal also case forward signal; vice versa. models paper forms make converge. initialized elements mutually independent share distribution. assume elements also mutually independent share distribution independent other. have product initialization design. proper initialization method avoid reducing magnifying magnitudes input signals exponentially. expect product take proper scalar sufﬁcient condition also worth noticing variance input signal roughly preserved ﬁrst layer last. cases input signal normalized magnitude large softmax operator overﬂow. solution normalize input signal impact hyper-parameters. another solution include small factor weights among layers e.g. main difference derivation xavier initialization address rectiﬁer nonlinearities. derivation considers linear case result given nlvar implemented zero-mean derived std. layers number however small enough completely stall convergence models actually used paper shown experiments. figure compares convergence -layer model. methods able make converge. starts reducing error earlier. also investigate possible impact accuracy. model table xavier initialization method leads top/top- error leads ./.. observed clear superiority accuracy. derived variance adopted uniform distributions forward backward cases averaged. straightforward adopt conclusion gaussian distributions forward backward case only. figure convergence -layer large model x-axis number training epochs. y-axis top- error random samples evaluated center crop. relu activation cases. initialization xavier lead convergence starts reducing error earlier. figure convergence -layer small model relu activation cases. initialization able make converge. xavier completely stalls also verify gradients diminishing. converge even given epochs. rescaled factor layers represent layers. large leads extremely ampliﬁed signals algorithm output inﬁnity; leads diminishing signals. either case algorithm converge diverges former case stalls latter. derivation also explains constant standard deviation makes deeper networks stall take model team’s paper example. model conv layers ﬁlters. ﬁlter numbers layers layers layers presence weight decay gradient contributed logistic loss function diminishing total gradient diminishing weight decay. diagnosing diminishing gradients check whether gradient modulated weight decay. table figure shows convergence -layer model. initialization able make extremely deep model converge. contrary xavier method completely stalls learning gradients diminishing monitored experiments. studies demonstrate ready investigate extremely deep rectiﬁed models using principled initialization method. current experiments imagenet observed beneﬁt training extremely deep models. example aforementioned -layer model top-/top- error clearly worse error -layer model table accuracy saturation degradation also observed study small models vgg’s large models speech recognition perhaps method increasing depth appropriate recognition task enough complex. though attempts extremely deep models shown beneﬁts initialization method paves foundation study increasing depth. hope helpful complex tasks. baseline -layer model table better comparison also list vgg- model model following modiﬁcations vgg- ﬁrst layer ﬁlter size stride move three conv layers largest feature maps smaller feature maps time complexity roughly unchanged deeper layers ﬁlters; spatial pyramid pooling ﬁrst layer. pyramid levels numbers bins total bins. worth noticing evidence model better architecture vgg- though model better results vgg-’s result reported earlier experiments less scale augmentation observed model reproduced vgg- comparable. main purpose using model faster running speed. actual running time conv layers larger feature maps slower smaller feature maps time complexity same. four-gpu implementation model takes mini-batch reproduced vgg- takes evaluated four nvidia gpus. choose increase model width instead depth deeper models diminishing improvement even degradation accuracy. recent experiments small models found aggressively increasing depth leads saturated degraded accuracy. paper -layer -layer models perform comparably. speech recognition research deep models degrade using hidden layers conjecture similar degradation also happen larger models imagenet. monitored training procedures extremely deep models found training testing error rates degraded ﬁrst epochs because possible degradation choose increase depth large models. hand recent research small datasets suggests accuracy improve increased number parameters conv layers. number depends depth width. choose increase width conv layers obtain highercapacity model. models table large observed severe overﬁtting. attribute aggressive data augmentation used throughout whole training procedure introduced below. training algorithm mostly follows resized image whose shorter side crop randomly sampled per-pixel mean subtracted. scale randomly jittered range following half random samples ﬂipped horizontally random color altering also used. unlike applies scale jittering ﬁnetuning apply beginning training. further unlike initializes deeper model using shallower directly train deep model using initialization described sec. endto-end training help improve accuracy avoid poorer local optima. ﬁrst apply convolutional layers resized full image obtain last convolutional feature map. feature window pooled using layer layers applied pooled features compute scores. also done horizontally ﬂipped images. scores dense sliding windows averaged combine results multiple scales adopt simple variant krizhevsky’s method parallel training multiple gpus. adopt data parallelism conv layers. gpus synchronized ﬁrst layer. forward/backward propagations layers performed single means parallelize computation layers. time cost layers necessary parallelize them. leads simpler implementation model parallelism besides model parallelism introduces overhead communication ﬁlter responses faster computing layers single gpu. implement algorithm modiﬁcation caffe library increase mini-batch size accuracy decreased large models paper observed speedup using gpus speedup using gpus. perform experiments -class imagenet dataset contains million training images validation images test images results measured top/top- error rates provided data training. results evaluated validation except ﬁnal results table evaluated test set. top- error rate metric ofﬁcially used rank methods classiﬁcation challenge table compare relu prelu large model channel-wise version prelu. fair comparisons relu/prelu models trained using total number epochs learning rates also switched running number epochs. table shows results three scales multiscale combination. best single scale possibly middle jittering range multi-scale combination prelu reduces error top- error compared relu. results table table consistently show prelu improves small large models. improvement obtained almost computational cost. next compare single-model results. ﬁrst show view testing results table here view -crop. -view results vgg- based testing using publicly released model reported best -view result models also outperform existing results. table shows comparisons single-model results obtained using multi-scale multi-view test. results denoted msra. baseline model already substantially better best existing single-model result reported vgg- latest update bemoreover best single model top- error. result even better previous multi-model results comparing a+prelu b+prelu -layer model -layer model perform comparably. hand increasing width still improve accuracy. indicates models deep enough width becomes essential factor accuracy. combine models including table time trained model architecture models accuracy inferior considerable margins. conjecture obtain better results using fewer stronger models. multi-model results table result top- error test set. number evaluated ilsvrc server labels test published. result better ilsvrc winner represents relative improvement. also relative improvement latest result figure shows example validation images successfully classiﬁed method. besides correctly predicted labels also attention four predictions top- results. four labels objects multi-object images e.g. horse-cart image contains mini-bus also recognized algorithm. four labels uncertainty among similar classes e.g. coucal image predicted labels bird species. figure shows per-class top- error result test displayed ascending order. result zero top- error classes images classes correctly classiﬁed. three classes highest top- error letter opener spotlight restaurant error existence multiple objects small objects large intra-class variance. figure shows example images misclassiﬁed method three classes. predicted labels still make sense. figure show per-class difference top- error rates result team’s in-competition result ilsvrc error rates reduced classes unchanged classes increased classes. russakovsky recently reported human performance yields top- error imagenet dataset. number achieved human annotator well trained validation images better aware existence relevant classes. annotating test images human annotator given special interface class title accompanied example training images. reported human performance estimated random subset test images. result exceeds reported human-level performance. knowledge result ﬁrst published instance surpassing humans visual recognition challenge. analysis reveals major types human errors come ﬁne-grained recognition class unawareness. investigation suggests algorithms better ﬁne-grained recognition second figure shows example ﬁne-grained objects successfully recognized method coucal komondor yellow lady’s slipper. humans easily recognize objects bird ﬂower nontrivial humans tell species. negative side algorithm still makes mistakes cases difﬁcult humans especially requiring context understanding high-level knowledge algorithm produces superior result particular dataset indicate machine vision outperforms human vision object recognition general. recognizing elementary object categories pascal figure example validation images incorrectly classiﬁed method three classes highest top- test error. letter opener middle spotlight bottom restaurant image ground-truth label top- labels predicted method listed.", "year": 2015}