{"title": "Where do goals come from? A Generic Approach to Autonomous Goal-System  Development", "tag": ["cs.LG", "cs.AI"], "abstract": "Goals express agents' intentions and allow them to organize their behavior based on low-dimensional abstractions of high-dimensional world states. How can agents develop such goals autonomously? This paper proposes a detailed conceptual and computational account to this longstanding problem. We argue to consider goals as high-level abstractions of lower-level intention mechanisms such as rewards and values, and point out that goals need to be considered alongside with a detection of the own actions' effects. We propose Latent Goal Analysis as a computational learning formulation thereof, and show constructively that any reward or value function can by explained by goals and such self-detection as latent mechanisms. We first show that learned goals provide a highly effective dimensionality reduction in a practical reinforcement learning problem. Then, we investigate a developmental scenario in which entirely task-unspecific rewards induced by visual saliency lead to self and goal representations that constitute goal-directed reaching.", "text": "ﬁrst place. recent theories however much suppose also contribute cortex-wide higherlevel cognitive processes engaged social behavior goals also seen major factor motivation psychology goal-setting considered essential long term behavior organization. goals considered likewise structuring element cognition perception agents’ behavior imitation learning teleological action understanding goals useful abstractions. come from? neither robotics machine learning neuroscience psychology provide conclusive even general hints biological artiﬁcial agent starts goals acquire them. already weng pointed developmental robots learn without particular task given ﬁrst. then could ever develop abstractions? consequent need consider learning goals explicitly pointed ﬁrstly prince essentially unsolved ever since. difﬁcult think heuristics agent acquire goals within isolated special scenarios could general mechanisms development goal systems? article seeks answers longstanding question. thereby focus learning agent’s goals contrast observational learning others’ goals imitation learning values inverse reinforcement learning fully autonomous learning without external supervision agent told paper makes four contributions along next four sections. firstly develop general conceptualization goals sec. introduce several novel arguments order deﬁne goals precisely possible argue consider goal systems abstractions reward systems. arguments meant stimulate wider discussion provide basis second contribution sec. introduce computational learning formulation latent goal analysis based given rewards show latent goals extracted sensory action information constructively prove universal existence transformation. thirdly show application oriented experiment sec. dimension reduction technique reinforcement learning. recommender scenario online news articles recommended readers. goals learned compact representations high-dimensional data outperform standard methods abstract—goals express agents’ intentions allow organize behavior based low-dimensional abstractions high-dimensional world states. agents develop goals autonomously? paper proposes detailed conceptual computational account longstanding problem. argue consider goals high-level abstractions lower-level intention mechanisms rewards values point goals need considered alongside detection actions’ effects. propose latent goal analysis computational learning formulation thereof show constructively reward value function explained goals self-detection latent mechanisms. ﬁrst show learned goals provide highly effective dimensionality reduction practical reinforcement learning problem. then investigate developmental scenario entirely taskunspeciﬁc rewards induced visual saliency lead self goal representations constitute goal-directed reaching. goals abstractions high-dimensional world states express intelligent agents’ intentions underlying actions. goals considered organize behavior humans robots. instance robot planning well motor control goals describe desired outcome future actions terms aspect variable world relevant desired value also robot learning goals proven useful scaffolding mechanism perform efﬁcient exploration also high dimensional sensorimotor spaces scenarios goals carefully handcrafted variable controlled well mechanism select particular goal agent time need speciﬁed designer. several formulations motor learning automatically choose internal goals purely sake training skill neither explain choose goals depending external stimuli determine variable controlled. goals fundamental concept also neuroscience psychology. entire formulation cerebellum providing internal forward inverse models makes sense goals already given input inverse models. conservative standpoint models concern motor control study supported jsps grant-in-aid specially promoted research research related european project codefror also would like thank yahoo webscope program providing yahoo front page today module user click dataset version experiments. room temperature) much higher-dimensional physical processes. also goals planning describe world variables desired value variables irrelevant. control planning goal achieved means action i.e. agent actions result observation desired variable values. similar aspects also found goal-setting psychology instance management psychology proposed goals speciﬁc measurable realistic points clearly distinguish goals kinds desires intentions optimization general improvement goals narrow sense. improvement speciﬁc sense particular achieved value speciﬁed. hence deﬁnite achievement possible deﬁned. wishes desired world states goals achievable means action ﬁrst place. refer equivalence states sense irrelevant variables matter goal. hence values equivalently acceptable. main point goals reﬂect particular desire. achievement value agent. stands affordances sahin formulated contrast e.g. affordances relation action agent object manipulation effect object objects similar action-effect relations summarized equivalence classes standonable. related formulation contingencies action-effect bindings describe general relations actions speciﬁc effects. goals affordances interactivist concepts deﬁned agent environment interplay. crucial difference affordances describe possible thing could done. affordances associated value desire desire something constituting aspect goals view. applied ﬁeld. lastly show experimental setup investigate human development goals case reaching sec. show task-unspeciﬁc information seeking reward based visual saliency leads representation to-be-reached objects goals self-detection hand. thereby learn abstractions already utilize applying goal babbling generate actions fully bootstrapping closed action-perception-learning loop. operationalize acquisition goals? order achieve general conceptualization start common sense dictionary deﬁnitions relate meaning usages various scientiﬁc ﬁelds. starting general deﬁnition distinguish goals several related terms optimization affordances. importantly point several vague entirely unspeciﬁed aspects common sense usage goals make propositions substantiate terminology. thereby conceptualization general possible concrete enough mathematical operationalization. dictionaries refer term goal desired result someone’s ambition effort goals precisely deﬁned computational domains them. motor coordination control goals typically lowdimensional abstractions task desired angular velocity electric motor desired position robot’s effector. thereby goals formulated values low-dimensional space abstract many task-irrelevant variables information seeking exemplary experiment sec. take latter perspective. abstraction process thereby could concern immediate rewards also expected future rewards expressed value systems fig. illustrate proposition saying achievement goal describes reward. second question must clarify conceptualize goals achievement means. natural language often expressed action achieves goal. however skips important aspect must considered profound conceptualization goals come alone always paired evaluation action’s effect. effect action action goal compared. robot reaching evaluation rather learning often referred self-detection robot’s hand needs detected instance camera image. case reaching notion related body schema describe localization body general. term self-detection also nonreaching non-spatial sense describes effect related goal. instance goal sandwich would accompanied detection actually sandwich not. evaluation action equally exists planning domains effect past action compared desired outcome goal-setting psychology major emphasis made measurability goals. domains effect action compared goal within common reference frame order assess achievement. need comparison forbids considering development goals self-detection separately other supervised action motor learning. self-detection goals available supervised learning signal becomes available learning processes. self-detection goals already available used numerous approaches motor learning already respect autonomous development goals already prince noted goals related selfsupervision missed point goals themselves rather self-detection enables supervision. finally need consider goals become active i.e. agent determines goal follow present moment. often considered agents multiple goals e.g. different timescales also parallel secondary goals long run. leaves play operationalization propose organize following restriction using notion cognitive processing systems internal agent hence different systems cognitive processing motor planning control present goal each. note goal gets triggered context sensory information internal state information processing systems. seems trivial makes important point needs goal-detection determine followed goal context parallels self-detection. reaching example might mechanism determine position relevant object camera images also hard-wired position selectors many robot setups. terms motivational psychology instance certain context conversation might trigger subsequent communicational goal convey information summary refer goal system joint apparatus goal-detection context self-detection compared within common reference frame achievement goal means action reﬂected reward value last section established conceptual framework goals argued learn together selfdetection abstractions reward value signals. order develop mathematical learning framework terms formal relation. best consider domain already formal relation least conceptual elements control coordination problems section develop computational learning framework goals introducing formal relations adding terms concept still missing establishing learning rule rewards mathematical terms corresponding conceptual terms order establish generic framework make assumptions source rewards show examples extrinsic intrinsic rewards experiments. fig. latent goal analysis identiﬁes project actions contexts common observation space. observed rewards thereby explained distance action-outcome goal reward problem turned control problem. argued starting particular domain motor control considerably narrows scope overall concept. however show section universality mathematical approach i.e. learn goals reward value function. speciﬁcally show equivalence relation motor control reinforcement learning reinforcement learning problem transformed motor control problem abstractions goals self-detection. hence learning framework general reinforcement learning often considered general learning formulation all. start establishing formal relation conceptual terms based motor control coordination problems used contexts robot control learning based internal models motor control problems shown fig. follow simple protocol world provides goal agent situated observation space x⊆rn. agent chooses action action space world provides causal outcome agent’s action situated agent’s task choose action outcome matches goal many coordination problems provide redundancy action space substantially higher dimensional observation space multiple actions outcome scenarios additional cost functions often considered select optimal action among fulﬁll case hand-eye coordination could correspond joint angles torques. position end-effector results applying ground truth function called forward function. function learned supervised examples called forward model. task identify called self-detection agent’s selection action addressed inverse model inverse function forward model problem often considered means overall cost-function distance goal outcome easily transformed reward semantics inverting sign still missing formulation goal-detection. mechanism usually part academic papers motor control always present goals control processes chosen vision processes identifying relevant objects manipulated planningprocesses usually hard-wired. hence determined larger internal external context. denote selection abstract level function refer goal-detection. further introduce sake symmetry virtual cost term depends context later need theoretical considerations. term inﬂuence optimal action selection given context introduces aspect optimally possible reward depends context. altogether gives reward transformation overall protocol corresponds one-step reinforcement problem shown fig. world provides context context space agent chooses action action space world provides reward based latent goals action outcomes equation established formal relation terms concept. formulation allows transform existing goals rewards. central idea computational learning goals invert process. given possible reward value function possible transform back motor control problem? fact possible according common beliefs reinforcement learning motor control comes highly informative usually low-dimensional abstractions used supervised learning goal actual values motor control deﬁne relation similar actual target outputs classical supervised learning setups providing directional information contrast mere magnitude error reinforcement learning given rich structure motor control reinforcement learning seems general setup work tackle temporal credit assignment problem estimate itself. however value system estimate future rewards already available decomposing either computationally equivalent since scalar functions major challenge identify forward model self-detection goal-detection ˆx∗. thereby goals outcomes considered latent variables reward function. abstractions constitute control problem describing interaction goals outcomes low-dimensional observation space cost terms depending context action considered remainders fact easy given ansatz finding functions formulated ﬁnding appropriate coefﬁcients parametrized functions. first consider features describe contexts actions. assuming ndimensional observation space denote function candidates coefﬁcients form symmetric coefﬁcient matrix universal approximator arbitrarily well approximate least continuous functions appropriate features chosen. instance features separate polynomial features polynomial degree subterm ·kca·ψa contain joint polynomial terms degree hence equation least describe functions described polynomials least continuous functions. transform rewards goals outcomes even under-determined. inﬁnitely many decompositions matrix kca. choice perfect match generated residual terms concrete decomposition consider singular value decomposition usvt orthonormal matrices positive diagonal matrix exemplary decomposition could still resulting observation space actions contexts high-dimensional dimensions since rp×m however dimension reduction straightforward based select diagonal matrix rn×n largest singular values respective singular vectors rp×n rm×n approximate usvt rn×p rn×m chosen within column space order project n-dimensional observation space. hence latent observation space uniquely determined number dimensions except multiple identical singular values. sufﬁciently large approximates reward function arbitrarily well. optimal selfgoal-detection selection observation space uniquely determined column space positioning goals outcomes inside space i.e. precise choice unique. choose matrices means transformation matrices rn×n using notation easy show irrelevant. insert deﬁnitions reward equation appears since orthonormal case right away. reﬂects rotation entire observation space change distances space therefore matter lga. remains choose orthonormal matrix rn×n positive diagonal matrix rn×n. together determine actions contexts precisely projected observation space goals action-outcomes interestingly thereby scaled against other increasing results scaling scaling appearance operation largely modiﬁes distance therefore value −||ˆx∗− ˆx||. nevertheless change overall reward function shifts parameter mass different terms −||ˆh ˆea. therefore also contribution rewards different terms changed choice course −||ˆx∗ ˆx|| relevant term since reﬂects relation goals outcomes main constituents control problems. therefore signiﬁcant contribution cost terms rather residuals. hence choose minimal formulate norms matrices currently aware closed form solution optimization problem. low-dimensional though. practice found solved efﬁciently effectively simple gradient descent. therefore initialize apply gradient descent step width numeric convergence. thereby diagonal values considered orthonormalized setting singular values update. altogether starts universal approximation reward value function function quadratic form shown equation mechanism corresponds value systems supposed exist midbrain structures whereas reward could reﬂect extrinsic intrinsic phenomenon. second step kca. select axis column space highest singular values. corresponds axis inside actioncontext space signiﬁcant reward/value function. hence step identiﬁes low-dimensional observation space goals action outcomes situated. third last step actually locate goals action outcomes inside observation space. therefore assemble matrices goalself-detection terms directly gives functions allows compute needed. idea. thereby goals outcomes considered latent variables reward function allows view compact low-dimensional representations otherwise highdimensional actions world states. representations thereby justice semantics desire intention expressing aspects relevant reward justice achievement semantics representing common space compared. following experiments exemplify aspects universality approach showing dimension reduction application based external rewards sec. developmental study generic intrinsic information seeking rewards sec. highly interpretable representations learned used bootstrap self-supervised motor learning. computational learning formulation shows goals learned abstractions rewards. algorithm ﬁrst place meant show feasibility approach. however since formulation based spectral decomposition immediately apply dimensionality reduction reward-based learning problems. ﬁrst experiment therefore investigates ability take practical scenario. here task fact already given means extrinsic rewards. show learned goals serve useful compact representations externally given tasks. contrast external speciﬁcation sec. investigate purely intrinsic rewards immediately describe task. experiment investigates lga’s capability dimension reduction one-step reinforcement learning problem. scenario considers website comprising certain news articles time. article featured i.e. recommended prominent position website. task select article’s teaser featured position. recommender system supposed select actions probability website visitor interacts maximized earning operating company rise end. order perform selection speciﬁc visitor many cases information like country previous click-history available ip-address cookies login website. information reward function estimated resembles click probability. dimensionality reduction however crucial domain context action typically high-dimensional recommender system must react extremely quickly thousands millions visits webpage. achieved dimension reduced allow efﬁcient evaluation dimension reduction reinforcement learning tedious issue. attempt learn reduced rank regression transition probabilities guide exploration directly consider features’ relevance reward achievement. purely unsupervised schemes like slowfeature analysis frequently applied state-space matrix-vector multiplications size; squaredistance exactly ﬂoating point operation expensive scalar product. baselines used reduce dimension context-space applying either quadratic bi-linear regression. condition dimension actions cannot reduced. order evaluate effectiveness technique need domain speciﬁc evaluation metric i.e. many visitors’ clicks approaches generate. method denote policy choose news-teaser based user information argmaxa∈a articles available time page visit. natural performance metric clickrate ctrπ total number number clicks generated page visits selecting teasers measure thoroughly measured policy online webpage. ofﬂine evaluation estimate performance counting often actually clicked teaser would also recommended policy performance ctr% baselined uniform random strategy. fig. shows achieves substantially better performance bi-linear model decomposition fact already components corresponds evaluating sufﬁce reach factor compared chance level. reﬂects simple fact articles interesting others. quickly improves nctr components improves nctr bi-linear model requires components reach nctr minimal improvement components. might considered unfair allow term whereas bi-linear models neither comprise require additional measure. recommender scenario however reasonable dimension reduction also cannot account actual reward-relevance. reward-modulated versions learning rules account reward-relevance least extent limited simple correlations. none methods effectively reduce dimension actions actions naturally observable probability distribution models consider states actions reward time estimate reward function based bi-linear regression reduce rank parameter matrix perspective dimension reduction only models similar approach although coming entirely different direction. show experimentally method yields signiﬁcantly compact representations practical scenario experiment yahoo front page today module user click dataset version comprises recordings click behavior yahoo.com’s front page consecutive days october utilize ﬁrst recordings only. recording contains total number events. event contains actually displayed news-teaser currently available news visitor features visitor’s decision click teaser different teasers shown period represent dimensional actions encoded -of-m scheme. events contain total number binary features visitor fully anonymized data i.e. individual meanings revealed yahoo. using data estimate using batch-gradient descent empirical error e−r)]. applied epochs training gradient step width starting zero initial parameters. parameters ﬁne-tuned applying whitening contexts continuing batch regression another epochs step width baseline applied bi-linear regression model trained procedure quadratic model. bi-linear regression models previously used reduce dimension recommender scenarios matrix decomposed ubsbvt singular value decomposition signiﬁcant dimensions kept. decomposition allows rewrite similar approach method interpreted projecting contexts actions common space referred partworth space space comparison done scalar product whereas measures direct distance. partworth space therefore encode goals cannot deﬁnitely achieved evaluation cost models same involve needs computed article published. cost evaluating policy page visit same since need multiply visitor context matrix. computes distance dimension scalar product dimensions also cost. however particular setup achieves equally high performance cost term omitted bi-linear decomposition outperform counterparts unsupervised states running bi-linear quadratic regression. interestingly pca&qr substantially outperforms standard decomposition approach shows high expressiveness quadratic regression general. pca&qr however increased cost evaluating context instead methods. still extracts compact representations quadratic regressor outperforms pca&qr. conclude allows effective dimensionality reduction online news recommender setup outperforms standard bi-linear model terms generated clicks. margin thereby numerically high range still highly signiﬁcant domain since clicks directly related website’s monetary income. studies recommender systems reported much higher absolute values benchmarks suggests data used rather hard. possible reason features actions identities. features relate similar articles could therefore increase absolute well margin different methods. goals seem useful abstractions setup. within framework goals action outcomes serve lowdimensional compact representations deﬁnes reward. particular case though impossible interpret goals’ semantics simply reason know meaning context features content articles previously anonymized. would know them however could interpret learned observation space space different topics interest users. webserver’s perspective user comes topic interest described goal space. webserver to-be-shown articles actions described topic space means action outcomes result choosing particular article certain point interest space. achievement semantics describe match user interested likely generate reward system. experiment goal development reaching last section shown extract goals useful abstractions extrinsic rewards already specify concrete task. could agent learn goals task explicitly given already beginning? section contrast previous experiment intrinsic rewards describe particular task. consider visual saliency reward simple robot implement information seeking behavior show leads meaningful goalselfrepresentations. saliency measures already shown permit self-detection end-effector simply looking hand interesting. extend ﬁnding considering object time. turns interesting looking hand object look closely together bottom). show thereby develops detection external object goal self-detection hand. representations thereby already utilized means goal babbling closed loop results emergence goal-directed reaching. basic scenario shown fig. consider simple robot three joints actions joint angles refer effector’s actual position cartesian coordinates xt∈r. salient object placed somewhere scene coordinates ot∈r. object together rendered pixel image. generically could think image context terms visual perception. however considering dimensional visual input learning neither computationally feasible biologically plausible. experiment assume certain extent image processing already identiﬁed object hand coordinates keypoints image. context learning comprises basic coordinates plus additional noise dimensions challenge learning. every timestep agent assumed still position object position hence learner’s context gaussian noise estimated functions locally-linear learning formulation identical receptive ﬁeld radii respectively. design choice selected components extracted dimensional context dimensional action. reward learner’s reward computed using simple saliency model based difference-of-gaussians agent received context selected action compute reward based after-action image containing object action then compute pyramid gaussians image smoothed gaussian kernel scaled factor procedure repeated times. saliency middle) computed images taking difference them adding amplitudes differences. considering salient point would mean look interesting pixel. however assume agent attend single visual receptor rather region visual scene. therefore smooth saliency large scale right) gaussian ﬁlter pixels width models total width agent’s visual ﬁeld. highest value smoothed saliency therefore measures much information agent visual ﬁeld. manually normalized scale values approximately range consider rewards online learning algorithm experiment developed online algorithm lga. previously introduced algorithm useful show theoretic feasibility batch processing suited closed loop processing intended experiment. simple gradient descent algorithm estimate goalself-detection directly model ﬁrst). consider agent observes samples along time agent perceives context executes action receives reward based hidden reward function agent supposed learn functions observed reward explained according eqn. done reducing reward-prediction error ||et|| ||rt ˆr|| denote learnable parameters respectively. initial symmetry-breaking necessary initialize small random values. point simple gradient descent succeed estimate functions. however need consider values kept small. purpose simple decay term similar weight-decay often used neural networks parameter values adapted error-reduction signal also decay last term formula depending function approximation method. decay term disabled formulas correspond ordinary gradient descent decay term balances contribution terms goalself-detection take dominating role explaining reward. term −||ˆh− ˆf|| however model arbitrary reward functions alone. particular negative distance account numerically negative rewards. modeling numerically positive rewards requires terms shift entire estimate constant. decay term used process never fully reach necessary shift. order still permit reasonable learning signal introduce purely scalar term reward estimation. term effected decay shift reward estimate −||ˆh− ˆf|| used model shape reward function −||ˆh experiment utilize whereas could potentially used select cost-optimal actions goal. term directly useful needs accompany estimation approximating possible reward function shown sec. iii. procedure conducted experiment independent trials samples each. continuous movement object visual scene performed continuous online learning learning rates entire procedure possible online fashion only decided perform additional consolidation phase speed learning. therefore generation samples interrupted every epoch samples last samples presented random ordering times. describes agent learn internal representations goals self. instantaneously describe strategy select actions. however goalself-detection perform self-supervised motor learning executed actions estimated outcomes allow generate supervised learning signal common methods motor learning used together estimated goal perform goal-directed motorcontrol. utilize previous algorithm goal babbling also utilizes goals order scaffold learning. fig. goalself-detection learn represent object hand position respectively. ﬁrst principal component internal coordinate system encodes top-down coordinate. goal babbling uses abstractions learn inverse model consistently makes contact object. algorithm learns inverse model selfgenerated examples actions selected trying accomplish goal means inverse model plus exploratory noise learning rate local model distances exploratory noise amplitude shown fig. world provides object gets encoded context together agent’s last action. agent’s saliency system generates intrinsic reward information seeking. latent goal analysis extracts reward-relevant information action disentangles goal information context order explain reward relation goal self. selfdetection estimated goals used self-supervised goal babbling closed loop. evaluation every samples consolidation steps. investigated three questions self-detection encode? goal-detection encode? behavior results abstractions? order investigate checked well internal representations outcomes goals describe values actual effector position object position even internal variables encode perfectly arbitrary shifts translations internal coordinate system. therefore computed best linear internal representations actual variables. assessed quality encoding normalized rootactual variable perfectly predicted internal internal representation encodes actual variable. value means prediction gives error range variable’s variance indicates internal variable encode actual all. results self-detection shown fig. learning representation robot’s hand requires strongly non-linear multi-dimensional mapping. results show already beginning seems certain extent encoding errors signiﬁcantly however results merely versatility actions beginning. goal babbling initially chooses actions close single posture since sufﬁciently trained yet. outcomes locally distributed postures limited extent predicted randomly initialized self-detection. approx. epochs values stabilize around means actual effector-position’s variance explained internal representations. later stages minimal increase error values goal babbling learns different widespread postures. hence population gets less local harder describe non-linearities. latent goal analysis succeeds learn robot arm’s forward function joint angles effector position using saliency reward. additionally investigated encoding checking different coordinate axes learned representation encode. blue line fig. shows prediction effector’s top/down coordinate highest variance principle component internal representation. errors indicate axis indeed encodes top/down movements. object location already directly encoded context however variable still needs identiﬁed relevant among entries context right relation self-detection. particular later stages learning experiment handposition strongly correlates object position goal-directed reaching keeping track right variable trivial. results fig. show time initialization object position encoded goal-detection. then strongest principal component quickly coincides top-down axis object position epochs later goals’ values largely encode actual object position errors around results show robot’s hand position object position indeed found representations self goals. order check well together checked behavior generated goal babbling result abstractions. order perform analysis excludes exploratory noise evaluate combination goal-detection inverse model function suggests actions context hence check actions whether correspond reaching towards object position encoded counted contexts within epoch often actions contact either hand object whole object results fig. show learning rapidly seeks contacts object ﬁrst shortly later establishes contact rate robot’s hand object. example performances shown fig. thereby give explanation perfectly accurate relation self-detection actual effector position despite contact rate need encode tool-center position hand usually done robotics lack adaptive representations. rather model learns contact objects close shoulder inner part effector away objects contacted outer part effector. latent goal analysis together goal babbling indeed produces representations well inverse models correspond goal directed reaching experiment. remarkably bootstrapped task-unspeciﬁc reward based visual saliency sole original learning signal. abstractions bootstrapped used selfsupervised learning signal goal babbling. experiment therefore dual ﬁnding firstly show saliency information seeking reward results goal-directed reaching behavior seen distal perspective. reported saliency account self-detection hand aware studies also showing goal-directed behavior direct consequence. secondly information seeking reward together latent goal analysis account emergence internal self goal representations typically preprogrammed robotics pre-supposed computational neuroscience internal models. autonomous development goals fundamental issue developmental robotics. paper proposed detailed conceptual framework mathematical operationalization agents learn goals themselves. main argument therefore consider goals high level abstractions lower level mechanisms intention reward value systems could origin either external rewards intrinsic motivation measures. emphasized need consider learn goals alongside self-detection actions’ outcomes. compared common space. suggest goals outcomes together learned considering latent variables explain observed expected future intrinsic external reward. computational approach latent goal analysis operationalizes thought. therefore shown universal existence transformation rewards goals. shown different experiments learning formulation hand concrete task already speciﬁed external rewards learn goals compact practically useful representations. hand shown considering mere visual saliency generic task-unspeciﬁc information seeking reward processed framework leads abstractions self goals ultimately lead goaldirected reaching behavior. shown abstractions encode already capitalized self-supervised motor learning goal babbling. think complementary cases highlight generality conceptual mathematical approach. course need examples future research substantiate claim. purpose think work indeed widely open door investigations instance social learning scenarios also measures intrinsic motivation particular case intrinsic rewards gives rise interesting questions general cognitive science side. case saliency experiment robot start particular task semantics. rather generic information seeking leads goal-directed behavior. novelty study thereby behavior generated accompanied also goal representations agent going often seen crucial feature intentional action goals robot’s goals? hope work stimulates discussion around issue provide detailed answers infants become intentional agents goals robots able theirs too. wolpert doya kawato unifying computational interaction philosophical framework motor control social transactions royal society biological sciences vol. wrede rohlﬁng steil wrede p.-y. oudeyer tani towards robots teleological action language understanding humanoids workshop developmental robotics developmental robotics yield human-like cognitive abilities? prince helder hollich ongoing emergence core concept epigenetic robotics int. conf. epigenetic robotics schaal imitation learning route humanoid robots? trends nagai nakatani fukuyama myowa-yamakoshi asada co-development information transfer within infant caregiver ieee int. conf. development learning epigenetic robotics s.-t. park beaupre motgi phadke chakraborty zachariah case study behavior-driven conjoint analysis yahoo front page today module int. conf. knowledge discovery data mining itti koch niebur model saliency-based visual attention rapid scene analysis ieee transactions pattern analysis machine intelligence vol. merrick comparative study value systems self-motivated exploration learning robots ieee trans. autonomous mental development vol.", "year": 2014}