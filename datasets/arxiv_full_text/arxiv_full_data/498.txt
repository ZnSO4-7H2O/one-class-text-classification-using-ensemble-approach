{"title": "Factorization tricks for LSTM networks", "tag": ["cs.CL", "cs.NE", "stat.ML"], "abstract": "We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is \"matrix factorization by design\" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the near state-of the art perplexity while using significantly less RNN parameters.", "text": "present simple ways reducing number parameters accelerating training large long short-term memory networks ﬁrst matrix factorization design lstm matrix product smaller matrices second partitioning lstm matrix inputs states independent groups. approaches allow train large lstm networks signiﬁcantly faster near state-of perplexity using signiﬁcantly less parameters. lstm networks successfully used language modeling speech recognition machine translation many tasks. however networks millions parameters require weeks training multi-gpu systems. introduce modiﬁcations lstm cell projection lstmp reduce number parameters speed-up training. ﬁrst method factorized lstm approximates lstm matrix product smaller matrices. second method group lstm partitions lstm cell independent groups. test f-lstm g-lstm architectures task language modeling using billion word benchmark baseline used biglstm model without inputs described jozefowicz train networks week station system tesla gpus biglstm’s evaluation perplexity g-lstm based model f-lstm based model using three times less parameters. learning long-range dependencies recurrent neural networks challenging vanishing exploding gradient problems address issue lstm cell introduced hochreiter schmidhuber following recurrent computations linear projection. major part lstmp cell computation computing afﬁne transform involves multiplication matrix thus focus reducing number parameters partition layer parallel groups introduced krizhevsky alexnet convolutional layers divided groups split model gpus. multi-group convnets widely used reduce network weights required compute example esser multi-group approach extended extreme xception architecture chollet idea factorization large convolutinal layer stack layers smaller ﬁlters used example networks resnet bottleneck design denil shown possible train several different deep architectures learning small number weights predicting rest. case lstm networks convlstm introduced better exploit possible spatiotemporal correlations conceptually similar grouping. factorized lstm replaces matrix product smaller matrices essentially approximate size assumption well approximated matrix rank approximation contains less lstmp parameters original model versus therefore computed faster synchronized faster case distributed training. figure language model using regular lstm layers f-lstm layers glstm layers group layer. equations inside cells show kind afﬁne transforms computed cells time step. models without groups model groups; time index dropped clarity. approach inspired groups alexnet postulate parts input hidden state thought independent feature groups. example groups effectively split vectors concatenated cell’s memory together might look similar ensemble multi-tower models differences input different groups different assumed independent instead computing ensemble output concatenated independent pieces. testing used task learning joint probabilities word sequences arbitrary real sentences high probabilities compared random sequences words. figure shows typical lstm-based model ﬁrst words embedded dimensional dense input context learned using rnns number steps ﬁnally softmax layer converts output probability distribution test following models train models station gpus ween using adagrad optimizer projection size cell size mini-batch sampled softmax samples learning rate. note projection crucial helps keep embedding softmax layer sizes. table summarizes experiments. judging training loss plots appendix clearly visible step count model parameters wins. however given amount time factorized models train faster. difference biglstm g-lstm-g clearly visible g-lstm-g contains almost times less parameters biglstm trains faster results achieves similar evaluation perplexity within training time budget might approximate transform using arbitrary feed forward neural network inputs outputs initial experiments immediate beneﬁts hence remains topic future research. might possible reduce number parameters even stacking g-lstm layers increasing group counts other. second smaller experiment replace second layer g-lstm-g network layer groups instead call g-lstm-g-g. g-lstm-g g-lstm-g-g week gpus achieved similar perplexities. hence model hierarchical groups lose much accuracy faster better perplexity. hierarchical group layers look intriguing might provide learning different levels abstractions remains topic future research. ciprian chelba tomas mikolov mike schuster thorsten brants phillipp koehn tony robinson. billion word benchmark measuring progress statistical language modeling. arxiv preprint arxiv. ciregan ueli meier j¨urgen schmidhuber. multi-column deep neural networks image classiﬁcation. computer vision pattern recognition ieee conference ieee steven esser paul merolla john arthur andrew cassidy rathinakumar appuswamy alexander andreopoulos david berg jeffrey mckinstry timothy melano davis barch convolutional networks fast energy-efﬁcient neuromorphic computing. proceedings national academy sciences alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems noam shazeer azalia mirhoseini krzysztof maziarz andy davis quoc geoffrey hinton jeff dean. outrageously large neural networks sparsely-gated mixture-of-experts layer. arxiv preprint arxiv. xingjian zhourong chen wang dit-yan yeung wai-kin wong wang-chun woo. convolutional lstm network machine learning approach precipitation nowcasting. proceedings international conference neural information processing systems nips’ cambridge press. http//dl.acm. org/citation.cfm?id=.. yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan klaus macherey google’s neural machine transarxiv preprint lation system bridging human machine translation. arxiv. wayne xiong jasha droppo xuedong huang frank seide mike seltzer andreas stolcke dong geoffrey zweig. achieving human parity conversational speech recognition. arxiv preprint arxiv. figure y-axis training loss log-scale x-axis step mini-batch count hours training. biglstm baseline g-lstm-g g-lstm-g f-lstm-f trained exactly week. clearly visible step count model parameters wins. hand factorized models signiﬁcantly iterations given amount time therefore better results given amount time. week).", "year": 2017}