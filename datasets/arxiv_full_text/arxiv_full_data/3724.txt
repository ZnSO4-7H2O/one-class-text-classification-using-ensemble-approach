{"title": "Information Theoretic Interpretation of Deep learning", "tag": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "stat.ML"], "abstract": "We interpret part of the experimental results of Shwartz-Ziv and Tishby [2017]. Inspired by these results, we established a conjecture of the dynamics of the machinary of deep neural network. This conjecture can be used to explain the counterpart result by Saxe et al. [2018].", "text": "interpret part experimental results shwartz-ziv tishby inspired results established conjecture dynamics machinary deep neural network. conjecture used explain counterpart result saxe consider deep learning problem training data {}i=n known {xi}i=n objects interest {yi}i=n corresponding labels sampled random variables unknown joint distribution. example could continuous random variable large integer representing image single digit discrete random variable integer ranging practice data deep neural network parametrized high dimension. object assigns probability prediction taken value highest probability. practice usually achieved using softmax function. goal optimization algorithm train parameter joint probability matches joint probability work primarily motivated zhang poggio underdeterministic problem number parameter larger amount samples typically inﬁnite many solutions according occam’s razor \"simple\" solutions usually desired. practice training deep neural network explicitly regularized there’s obvious guarantee solution trained \"simple\". however experimental results report model still steadly improving solutions already reached. it’s commonly believed optimization algorithm used named stochastic gradient descent improving among solutions underdetermined system. experiments performed shwartz-ziv tishby give answer phenomina information theoretic perspective. experiment designed binary classiﬁcation problem trained fully connected feed-forward neural network using cross-entropy loss function. epoch training discretized values last feature layer bins directly goal estimate dynamics mutual information interpreted measure much information retains similarly much information preserves encourage reader watch fantastic video optimization process information plane https//goo.gl/rygyit. paper interested behaviour last layer essentially behaviour beginning training increases meaning network \"memorizing\" data outputing meaningful information note process fast. starts decrease keeps increasing meaning network \"forgetting\" data memorized still improving give information note process slow. ﬁrst part named ﬁtting phase second part named compression phase. saxe started similar line work measuring dynamics mutual information. proposed following compression phase explicit network uses relu activation function instead throughout network. compression phase happening alongside ﬁtting phase input data gaussianlike labels assigned randomly. theoretically implausible measure information intermediate layer given mutual information continuous random variables general ill-deﬁned. propose following conjecture machinary deep neural network data transformed linearly seperable feature nonlinear relu network \"almost invertible\" explained detail below. following full connected layer sigmoid/softmax function together behave like multiclass giving best prediction among available classes. given structure give interpretation experimental results shwartz-ziv tishby ﬁtting phase corresponds ﬁnding parameters relu network organize features last layer linearly seperable fashion full connected parameters represent hyperplanes seperate features. compression phase corresponds ﬁnding parameters maximizing margin linear seperation essentially driven randomness algorithm. progresses slowly prove section note procedure margin maximization \"support feature\" \"forgetting\" features enabled sigmoid/softmax function. follows mutual information input prediction diminising phase. phenomena propose saxe therefore explained activation function used last layer replaced relu structure destroyed. network \"forgetting\" approximately half features compression phase explicit. gaussian input isotropic network transform data linearly seperable one. argue compression phase exists there’s margin data different classes. agree it’s often dangerous deﬁne mutual information continuous valued random variables. general joint probability continuous random variable degenerate open neighborhood exists invertible mapping needs inﬁnite amount information describe mutual relationship. fact believe neural network better performance intermediate layers fully preserve information case relu better sigmoid tanh functions matches experimental reports nair hinton last layer fully connected activated sigmoid function indicated krizhevsky however mutual information continuous valued random variable discrete valued random variable well deﬁned sense discretized measurement converges. argue expirical measurement shwartz-ziv tishby last layer remains theoretical valid supports hypothesis. throughout theoretical analysis below represent discretized prediction random variable comparing residual network winner ilsvrc best existing deep network structures. explain successful theory. it’s well known network deep working well. reason could perspective there’s much information loss ﬁrst part proposed structure res-net designed allow model \"learn\" identity easily. speciﬁcally kraskov mentioned appendix mutual information fully preserved homeomorphisms res-net building block input vector output vector related operator could composition activation functions convolution drop-out) batch normalization). section details. shown operator norm theoretically guaranteed inverse enables information preservation intermediate layers. section experimentally veriﬁed intermediate layers. main strength res-net understanding allows deep mapping figure invertible independent almost never invertible relu convolution drop-out singular operations. work jacobsen built deep invertible networks showed network could successful without losing information intermediate layers. zheng understood maximum entropy perspective. chaudhari used entropy detect wide valleys algorithm. shamir proved generalization property framework. achille soatto investigated amount information loss various operations deep network. troost proved upper bound prove takes time polynomial order grow ﬁtting phase time exponential order grow compression phase matches video information plane dynamics shwartz-ziv tishby assume figure almost invertible sense argue deep network treated abstract function describing relationship governed word \"parametrized\" proper quantities information theoretic problem considered positive constant. therefore abuse notation equivalent information theoretic setting. particular deﬁned figure prove there’s direct relationship problem problem linear case. conclude deep neural network information theoretic point view reduced hard-margin problem. discuss notion generalization context. history work towarding behaviour discrete sgd. particular roberts tweedie shows discrete langevin difussion converges target distribution exponentially fast time. general result applying markov chain monte carlo practical \"path-ﬁnder\" diminishing step length. raginsky tzen showed searching path needs time exponential order jump local another local intuitively compression phase slow. zhang showed path needs time polynomial order enter ﬁrst \"good\" local intuitively ﬁtting phase fast. work present similar result information theoretic perspective using analogy standard stochastic convex analysis bottou recall video shwartz-ziv tishby information plane https//goo.gl/rygyit. claim general form given assumption smooth respect infc{||x ||∇f m||x satisfying strongly convex respect infc{||x m||x satisfying suppose closed away element consider training process restricted polynomial time regime ||wt w∗|| following shows polynomial time regime converging local linearly fast. consider exponential time regime analysis invalid last term innegligible. there’s possibility jump another conclude lower bound quantity interest converges local polynomial time switch higher \"better\" local exponential time explains video tishby’s. second bounded zero deﬁnition; small guaranteed large number assumption number samples sufﬁciently large. constant depending hypothesis class bound supf∈h typically controlled theory literature vapnik pointed zhang number parameters much larger number samples form regularization needed ensure small generalization error. neyshabur mentioned sharper bound obtained making dependent choice would like provide insights generalization formally establishing relationship generalization deep learning margin. model last layer generalizes better margin larger. neyshabur carefully analysized notion normed based control margin relation generalization. bartlett also proved bound generalization margin using rademacher complexity. compared different structures building blocks report performance table give interpretation result according theory relu adding identity mapping makes whole mapping invertible; makes deﬁned nonnegative operator potentially enlarge operator norm theoretical guarantee performs relu directly input loses information applying batch normalization convolution also meaningless. full reference experimental results sagun generalization property poggio proved appendix converges optimal solution underdetermined least square problem proper initialization; soudry proved converges optimal solution hard margin slowly. work motivated corresponding result deep learning. difference theory seperable data deterministic it’s controlled feature learning figure network looking create larger margin data different category \"weighted\" linear seperator achieve maximum margin. back experiment trained neural network batch gradient descent ﬁrst steps switch stochastic gradient descent rest steps. notice there’s signiﬁcant jump step optimization algorithm changed. argue despite jump exists linear interpolation lead essentially \"basin\". pointed dauphin high dimensional problem it’s hard encounter strict local minima almost critical points saddle points could always exist direction \"going landscape. suspect almost \"basins\" connected together sense. work dinh argued universal deﬁnition ﬂatness error surface still unclear. particular proved geometry local minimum changed arbitrarily reparametrization without changing function represents. instead concerning notion ﬂatness talk margin. according theory seperates feature data maximize margin. step feature point sitting right boundary linear seperator break balance leads sudden jump training error. generalizes better linear seperator last layer seperates feature data larger margin. notion margin needs deﬁned carefully scales norm weights data. used tensorﬂow code uploaded github wenxinxu -layer residual network cifar- steps computed building blocks step. deﬁned square-root squares entries tensor. exported output building blocks training tensorboard. conclude operator norm every building block smaller meets hypothesis. figure stochastic training information plane. tanh network trained sgd. tanh network trained bgd. relu network trained sgd. relu network trained bgd. random non-random training procedures show similar information plane dynamics. although sigmoid tanh functions mathematically invertible push large amount information boundary range practice classiﬁed single making highly noninvertible. hand relu keeps least half information input. experimental result matches theory tanh function compresses information relu keeps fair amount information. would like emphasis network needs compressive activation function last layer playing role similar svm. full reference experimental results jacobsen projected feature data last layer network dimensional space them. good performance shows intrinsic dimensionality feature data last layer supports seperability assumptions. paper analyzed dynamics information plane proposed shwartz-ziv tishby importantly gave hypothesis learning structure deep neural network answered questions arised saxe references alessandro achille stefano soatto. information dropout learning optimal representations noise. corr abs/. http//dblp.uni-trier.de/db/journals/corr/ corr.htmlachilles. pratik chaudhari anna choromanska stefano soatto yann lecun carlo baldassi christian borgs jennifer chayes levent sagun riccardo zecchina. entropy-sgd biasing gradient descent wide valleys. corr abs/. http//arxiv.org/abs/.. yann dauphin razvan pascanu çaglar gülçehre kyunghyun surya ganguli yoshua bengio. identifying attacking saddle point problem high-dimensional non-convex optimization. corr abs/. http//arxiv.org/abs/.. sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. corr abs/. http//arxiv.org/abs/ jörn-henrik jacobsen arnold smeulders edouard oyallon. i-revnet deep invertible networks. iclr international conference learning representations vancouver canada april https//hal.archives-ouvertes.fr/hal-. alexander kraskov harald stögbauer peter grassberger. estimating mutual information. phys. rev. ./physreve... https//link.aps. org/doi/./physreve... imagenet classiﬁcation pereira burges bottou deep convolutional neural networks. weinberger editors advances neural information processing systems pages curran associates inc. http//papers.nips.cc/paper/ -imagenet-classification-with-deep-convolutional-neural-networks.pdf. vinod nair geoffrey hinton. rectiﬁed linear units improve restricted boltzmann machines. proceedings international conference international conference machine learning icml’ pages omnipress. isbn ----. http//dl.acm.org/citation.cfm?id=.. behnam neyshabur srinadh bhojanapalli david mcallester nathan srebro. exploring generalization deep learning. corr abs/. http//arxiv.org/abs/. tomaso poggio qianli liao brando miranda lorenzo rosasco xavier boix jack hidary hrushikesh mhaskar. theory deep learning explaining non-overﬁtting puzzle. maxim raginsky alexander rakhlin matus telgarsky. non-convex learning stochastic gradient langevin dynamics nonasymptotic analysis. satyen kale ohad shamir editors proceedings conference learning theory volume proceedings machine learning research pages amsterdam netherlands pmlr. http//proceedings.mlr.press/v/raginskya.html. gareth roberts richard tweedie. exponential convergence langevin distributions discrete approximations. bernoulli https//projecteuclid. org/euclid.bj/. levent sagun utku evci ugur güney yann dauphin léon bottou. empirical analysis hessian over-parametrized neural networks. corr abs/. http //arxiv.org/abs/.. andrew michael saxe yamini bansal joel dapello madhu advani artemy kolchinsky brendan daniel tracey david daniel cox. information bottleneck theory deep international conference learning representations https learning. //openreview.net/forum?id=ry_wpg-a-. ohad shamir sivan sabato naftali tishby. learning generalization information bottleneck. theoretical computer science issn https//doi.org/./j.tcs.... http//www.sciencedirect.com/science/ article/pii/sx. algorithmic learning theory daniel soudry elad hoffer nathan srebro. implicit bias gradient descent separable data. international conference learning representations https//openreview. net/forum?id=rqngab. nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research http//jmlr.org/papers/v/srivastavaa.html. belinda tzen tengyuan liang maxim raginsky. local optimality generalization guarantees langevin algorithm empirical metastability. corr abs/. http//arxiv.org/abs/.. roman vershynin. high-dimensional probability introduction applications data science. cambridge series statistical probabilistic mathematics. cambridge university press chiyuan zhang samy bengio moritz hardt benjamin recht oriol vinyals. understanding deep learning requires rethinking generalization. https//arxiv.org/abs/. make assumption models trained time it’s output approximately uniform distributed ﬁnite labels. treat constant also note given model prediction satisﬁes intuitively term blow practice people estimate mutual information doesn’t yield meaningful quantity. particular don’t know whether converges take point general random variables degenerate open interval inverse function theorem exists invertible relationship need inﬁnite amount information describe happening consider instead mutual information discrete random variable ﬁnite range continuous random variable continuous density estimation would take form analytical form ﬁnite know practical approximation meaningful. practice true quantity usually ﬁnite. example mnist image label essentially discrete mutual information deﬁned strict discrete sense. conclusion it’s always sensible deﬁne mutual information discrete ﬁnite random variable continuous random variable exact integral form.", "year": 2018}