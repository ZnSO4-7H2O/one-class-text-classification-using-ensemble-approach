{"title": "Exploiting Layerwise Convexity of Rectifier Networks with Sign  Constrained Weights", "tag": ["cs.LG", "cs.AI"], "abstract": "By introducing sign constraints on the weights, this paper proposes sign constrained rectifier networks (SCRNs), whose training can be solved efficiently by the well known majorization-minimization (MM) algorithms. We prove that the proposed two-hidden-layer SCRNs, which exhibit negative weights in the second hidden layer and negative weights in the output layer, are capable of separating any two (or more) disjoint pattern sets. Furthermore, the proposed two-hidden-layer SCRNs can decompose the patterns of each class into several clusters so that each cluster is convexly separable from all the patterns from the other classes. This provides a means to learn the pattern structures and analyse the discriminant factors between different classes of patterns.", "text": "introducing sign constraints weights paper proposes sign constrained rectiﬁer networks whose training solved efﬁciently well known majorization-minimization algorithms. prove proposed two-hiddenlayer scrns exhibit negative weights second hidden layer negative weights output layer capable separating disjoint pattern sets. furthermore proposed two-hidden-layer scrns decompose patterns class several clusters cluster convexly separable patterns classes. provides means learn pattern structures analyse discriminant factors different classes patterns. recent years deep rectiﬁer networks achieved outstanding performance various applications including object recognition face veriﬁcation speech recognition handwritten digit recognition however complex hierarchical structures deep rectiﬁer networks geometrically interpretable convergence training using stochastic gradient descent methods still well understood. efforts made visualize understand convolutional layers. visualization techniques used improve classiﬁcation performance however reveal general properties deep neural networks fact ﬁrst layers learn generic features images last layers learn classspeciﬁc features classiﬁcation problems. recent work proposes understand improve training rectiﬁer neural networks using piecewise convexity property objective functions. proved that objective functions convex functions outputs rectiﬁer neural network piecewise convex functions parameters layer parameters ﬁxed. however exponentially large number pieces objective function convex piece convex across pieces. paper propose sign constrained rectiﬁer networks show networks convexity properties used develop efﬁcient training algorithms learning geometrically interpretable classiﬁers. hinge loss convex regularisation term used objective function train proposed neural networks objective function minimized using well known majorization-minimization algorithms algorithm iterative optimization method exploiting partial convexities function order avoid local minima good one. algorithm operates ﬁnding convex surrogate function upperbounds objective function. optimizing surrogate function drives objective function downward local optimum reached. training scrns show that initialization parameters surrogate function convex function layer’s parameters parameters ﬁxed. hence layer’s weights biases learnt alternatively using algorithms. furthermore scrns also decompose pattern several clusters cluster convexly separable patterns classes thus used learn pattern structures analyse discriminant factors patterns different classes. techniques enable feature analysis knowledge discovery manual supervision improve efﬁciency performance training classiﬁers. typical applications include feature discovery–in health production management precision livestock farming needs identify features associated diseases commercial farms using routinely collected farm management data supervised shape-free clustering knowledge discovery–the proposed scrns used separate class patterns several clusters cluster patterns convexly separable classes patterns wherein clusters required particular shape convex polytopes; iii) humansupervised neural network training–the proposed hiddenlayer scrns transform input data convexly separable data using ﬁrst hidden layer. transform data linearly separable data using second hidden layer. decomposition properties scrns enable human visualize patterns identify outliers check separating boundaries supervise training removing outliers mislabelled data. introduction sign constraints weights neural networks order learn geometrically interpretable models sign constraints imposed weights proposed scrns ﬁrst hidden layer transforms data convexly separable second hidden layer transforms data linearly separable. consequently every node concave function input preceding hidden layer. since concave piecewise linear function minimum several linear functions learnt scrn models thus geometrically interpretable used analyse discriminant features different classes patterns. introduction algorithms training sign constrained rectiﬁer neural networks layerwise convexity/concavity properties proposed scrns result existence convex surrogate function upperbound non-convex hinge loss function efﬁcient algorithm used learn parameters neural networks. related works work related exploits piecewise convexity properties rectiﬁer neural networks overcome local minima problems. uses piecewise convexity general rectiﬁer neural networks work introduces layer-wise convexity/concavity properties imposing sign constraints weights networks exploits properties pattern decomposition efﬁcient training using algorithms reduce risk local minima. work universal classiﬁcation power related address universal approximation power deep neural networks functions probability distributions proves multiple pattern sets transformed linearly separable hidden layers additional distance preserving properties. paper prove number pattern sets separated three-layer neural network negative weights output layer negative weights second hidden layer. biases weights ﬁrst hidden layer either positive negative. signiﬁcance proposed scrns lies fact decompose class patterns several subsets subset convexly separable classes patterns. decomposition used analyse pattern sets identify discriminant features pattern recognition. preliminary results paper reported wherein sign constraints introduced data decomposition discussion limited case binary classiﬁcation. paper extends multi-category classiﬁcation presents mm-based efﬁcient training algorithms proposed scrns. notations. throughout paper capital letters denote matrices lower case letters scalar terms bold lower letters vectors. instance denote column matrix denote element vector integer denote integer i.e. denote identity matrix proper dimensions denote vector elements denote vector elements denote elements non-negative denote elements non-positive. given ﬁnite number points convex combination points linear combination points coefﬁcients non-negative convex hull denoted convex combinations points organization. rest paper organised follows. introduce rectiﬁer neural networks sign constrained weights section investigate capacity sign constrained single hidden layer rectiﬁer neural networks classiﬁcation pattern decomposition section section investigates universal classiﬁcation power pattern decomposition capacity hidden layer rectiﬁer neural networks sign constraints output layer last hidden layer. sign constraints used control strategy two-hidden-layer neural network achieve linear separability. section ﬁrst introduce general algorithm presents algorithms used train sign constrained neural networks. section concludes paper. particular paper considers special class rectiﬁer neural networks sign constraints weights output layer second hidden layer. sign constraints used decompose pattern sets discriminate factor analysis. non-negativeness output layer weights imposed sign-constrained rectiﬁer neural networks. paper non-positive constraints convenience presenting decomposition properties sign constrained renn multiple category classiﬁcation problems. hidden layer sign constrained renns impose non-negativeness weights output layer impose non-positiveness weights second hidden layer. hidden layer sign constrained renn described below. useful properties convex/concave functions. first classiﬁer convex/concave function separates domain regions convex convex convex function convex concave. second convex/concave function approximated series linear classiﬁers. properties make sign-constrained rectiﬁer networks geometrically interpretable. section section investigate properties decompose data discriminant factor analysis. complexity pattern recognition problems quite different practice. section ﬁrst examine different categories classiﬁcation problems based complexities patterns’ separating boundaries. then investigate capacity single hidden layer nets section linear-separability categories linearly-separable hyperplane vector space separate note that linearlyseparable also linearly-separable linear-separability mutual. also known that linearly-separable ch{x} ch{x} unidirectional convex-separability categories called convexly-separable convex region including points excluding points convexly-separable ch{x} mutual convex-separability categories called mutually convexly-separable convexly-separable other. mutually convexly-separable ch{x} ch{x} note mutual convex-separability weaker linear separability linearlyseparable pattern sets mutually convexly-separable mutually convexly-separable pattern sets linearlyseparable. pairwise convex-separability multiple categories pattern sets {xi}m pairwise convexly-separable every pattern sets mutually convexly-separable next establish connections sign constrained rectiﬁer networks convexly-separable pattern sets. pattern sets labelled positive negative respectively single-hidden-layer binary classiﬁer deﬁned called single hidden layer separator satisﬁes lemma pair ﬁnite pattern sets labelled positive negative respectively. separated sign-constrained single-hidden-layer classiﬁer deﬁned satisfying positive pattern convexly-separable negative pattern i.e. proving lemma give example explain subsets three subsets note decomposed subsets overlaps. number subsets determined number hidden nodes. compact decompositions meaningful discriminate feature analysis small numbers hidden nodes preferable. signiﬁcance lemma discovery single-hidden-layer screnn decompose convexly-separable pattern sets linearly-separable subsets discriminate features convexly-separable patterns analysed linear classiﬁers separating pattern subsets pattern section considers multiple category classiﬁcation problems. show multiple sets transformed linearly separable single hidden layer every pair classes mutually convexly-separable. true proves sufﬁciency mutual convex-separability multiple category pattern sets separable single-hidden-layer screnn. next prove necessity. suppose single-hidden-layer screnn exists separate category pattern sets {xi}m lemma convexly-separable union pattern sets therefore convexly-separable pattern note that true therefore every pair pattern sets mutually convexly-separable completes proof necessity pairwise mutual convex-separability. note dimension dimensional multiple category classiﬁer sign-constrained renn separate pattern others decomposition property sign-constrained multiple category classiﬁers derived directly lemma section ﬁrst investigate universal classiﬁcation power sign-constrained two-hidden-layer binary classiﬁers capacity decompose pattern smaller subsets subset convexly separable pattern set. extend result multiple category classiﬁcation problems. next investigate applications two-hidden-layer sign constrained renn classiﬁer decompose pattern several subsets subset convexly separable pattern set. lemma disjoint pattern sets deﬁned satisfying sign-constrained twohidden-layer binary separators hidden nodes satisfying denote column denote element deﬁne linearly-separable. hence investigate discriminant features patterns using linear classiﬁers subsets patterns. decomposed subsets investigate pattern structures. numbers subsets determined numbers hidden nodes hidden layers two-hidden-layer screnns numbers hidden nodes singlehidden-layer screnns. compact pattern structures meaningful discriminant features small number hidden nodes preferable. section extends results last section multiple category classiﬁcation problems. m-dimensional classiﬁer two-hidden-layer screnn namely described theorem disjoint pattern sets ﬁnite number points exists m-dimensional classiﬁer two-hidden-layer screnn deﬁned positive negative pattern sets. weights bias two-hidden-layer renn deﬁned chosen output network satisﬁes fk{gk}. note deﬁned renn two-hiddenlayer sign-constrained renn complete proof universal classiﬁcation power two-hidden-layer screnn. next address decomposition capacity two-hiddenlayer rectiﬁer neural networks. suppose two-hidden-layer screnn separator pattern sets output satisﬁes pattern sets. hence separable then subset namely union single-hidden-layer screnn separate decomposed several subsets linearly separable linear separators decomposed subsets analyse data discriminant factors. algorithm iterative algorithm minimization non-convex objective functions iterations consisting steps majorization step ﬁnds surrogate function upperbounds objective function minimization step minimizes surrogate function. suppose following optimization problem ﬁrst inequality last equality follow sandwiched inequality follows hence objective function decreases converges stationary point. moreover many local minima objective function avoided larger values minimum convex functions iterations algorithm. hence algorithm usually ﬁnds good solution even though cannot guarantee global minima. many local minima non-convex function avoided algorithm risk local minima problem greatly reduced. next consider training scrns hidden layers. ﬁrst layer weights biases ﬁxed learning parameters essentially training problem single hidden layer scrn thus optimized using algorithm presented section next consider optimization parameters ﬁxed. following lemma provides foundation algorithm presented. proof statement true max. prove convexity fact function max. therefore lemma convex function parameters ﬁxed. proves statement completes proof. based lemma optimized iteratively using algorithm follows. parameter step arbitrary initialization. activation patterns respectively step denote shown that sign constraints weights output second hidden layers two-hidden-layer scrns still universal classiﬁers capable decomposing class patterns several subsets subset convexly separable pattern set. addition single-hidden-layer scrns capable separating convexly separable pattern sets well decomposing several subsets subset linearly separable pattern set. proposed scrn enables pattern feature analysis model interpretability knowledge discovery also enables efﬁcient training well known algorithms reduce risks local minima.", "year": 2017}