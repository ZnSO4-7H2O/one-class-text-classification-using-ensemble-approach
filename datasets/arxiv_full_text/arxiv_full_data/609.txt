{"title": "Learning to Generate Reviews and Discovering Sentiment", "tag": ["cs.LG", "cs.CL", "cs.NE"], "abstract": "We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.", "text": "explore properties byte-level recurrent language models. given sufﬁcient amounts capacity training data compute time representations learned models include disentangled features corresponding high-level concepts. speciﬁcally single unit performs sentiment analysis. representations learned unsupervised manner achieve state binary subset stanford sentiment treebank. also data efﬁcient. using handful labeled examples approach matches performance strong baselines trained full datasets. also demonstrate sentiment unit direct inﬂuence generative process model. simply ﬁxing value positive negative generates samples corresponding positive negative sentiment. representation learning plays critical role many modern machine learning systems. representations data useful forms choice representation important component application. broadly speaking areas research emphasizing different details learn useful representations. supervised training high-capacity models large labeled datasets critical recent success deep learning techniques wide range applications image classiﬁcation speech recognition machine translation analysis task speciﬁc representations learned models reveals many fascinating properties image classiﬁers learn broadly useful hierarchy feature detectors rerepresenting pixels edges textures objects ﬁeld computer vision also long history unsupervised representation learning much early research modern deep learning developed validated approach unsupervised learning promising ability scale beyond subsets domains data cleaned labeled given resource privacy constraints. advantage also difﬁculty. supervised approaches clear objectives directly optimized unsupervised approaches rely proxy tasks reconstruction density estimation generation directly encourage useful representations speciﬁc tasks. result much work gone designing objectives priors architectures meant encourage learning useful representations. refer readers goodfellow detailed review. despite difﬁculties notable applications unsupervised learning. pre-trained word vectors vital part many modern systems representations learned modeling word co-occurrences increase data efﬁciency generalization capability systems topic modelling also discover factors within corpus text align human interpretable concepts education learn representations phrases sentences documents open area research. inspired success word vectors kiros propose skipthought vectors method training sentence encoder predicting preceding following sentence. representation learned objective performs competitively broad suite evaluated tasks. advanced training techniques layer normalization improve results. however skip-thought vectors still outperformed supervised models directly optimize desired performance metric speciﬁc dataset. case text classiﬁcation tasks measure whether speciﬁc concept well encoded representation general semantic similarity tasks. occurs even datasets relatively small modern standards often consisting thousand labeled examples. contrast learning generic representation large dataset evaluating tasks/datasets proposed using similar unsupervised objectives sequence autoencoding language modeling ﬁrst pretrain model dataset ﬁnetune given task. approach outperformed training model random initialization achieved state several text classiﬁcation datasets. combining language modelling topic modelling ﬁtting small supervised feature extractor also achieved strong results in-domain document level sentiment analysis considering this hypothesize effects combining result weaker performance purely unsupervised approaches. skip-thought vectors trained corpus books. classiﬁcation tasks evaluated sentiment analysis reviews consumer goods much overlap text novels. propose distributional issue combined limited capacity current models results representational underﬁtting. current generic distributed sentence representations lossy good capturing gist poor precise semantic syntactic details critical applications. experimental evaluation protocols underestimating quality unsupervised representation learning sentences documents certain seemingly insigniﬁcant design decisions. hill also raises concern current evaluation tasks recent work provides thorough survey architectures objectives learning unsupervised sentence representations including mentioned skip-thoughts. work test whether case. focus task sentiment analysis attempt learn unsupervised representation accurately contains concept. mikolov showed word-level recurrent language modelling supports learning useful word vectors interested pushing line work. approach consider popular research benchmark byte level language modelling simplicity generality. also interested evaluating approach immediately clear whether low-level training objective supports learning high-level representations. train large corpus picked similar distribution task interest. also benchmark wider range tasks quantify sensitivity learned represenmuch previous work language modeling evaluated relatively small competitive datasets penn treebank hutter prize wikipedia discussed jozefowicz performance datasets primarily dominated regularization. since interested high-quality sentiment representations chose amazon product review dataset introduced mcauley training corpus. de-duplicated form dataset contains million product reviews july amounting billion training bytes. size dataset ﬁrst split shards containing equal numbers reviews aside shard validation shard test. many potential recurrent architectures hyperparameter settings considered preliminary experiments dataset. given size dataset searching wide space possible conﬁgurations quite costly. help alleviate this evaluated generative performance smaller candidate models single pass dataset. model chosen large scale experiment single layer multiplicative lstm units. observed multiplicative lstms converge faster normal lstms hyperparameter settings explored terms data wall-clock time. model trained single epoch mini-batches subsequences length total million weight updates. states initialized zero beginning shard persisted across updates simulate full-backpropagation allow forward propagation information outside given subsequence. adam used accelerate learning initial learning rate decayed linearly zero course training. weight normalization applied lstm parameters. data-parallelism used across pascal titan gpus speed training increase effective memory size. training took approximately month. model compact containing approximately many parameters reviews training dataset. also high ratio compute total parameters compared large scale language models operating byte level. selected model reaches bits byte. figure performance binary version function labeled training examples. solid lines indicate average runs sharded regions indicate percentiles. previous results dataset plotted dashed lines numbers indicating amount examples required logistic regression byte mlstm representation match performance. rntn lstm ct-lstm model processes text sequence utf- encoded bytes byte model updates hidden state predicts probability distribution next possible byte. hidden state model serves online summary sequence encodes information model learned preserve relevant predicting future bytes sequence. interested understanding properties learned encoding. process extracting feature representation outlined follows leading whitespace removed replaced newline+space simulate start token. trailing whitespace removed replaced space simulate token. text encoded byte sequence. follow methodology established kiros training logistic regression classiﬁer model’s representation datasets tasks including semantic relatedness text classiﬁcation paraphrase detection. details comparison experiments refer reader work. exception penalty text classiﬁcation results instead found performed better data regime. sentiment analysis datasets improve state signiﬁcant margin. datasets sentences extracted rotten tomatoes movie review website amazon product reviews suggests model learned rich representation text similar domain. datasets subj’s subjectivity/objectivity detection mpqa’s opinion polarity model noticeable advantage unsupervised representation learning approaches still outperformed supervised approach. better quantify learned representation also test wider sentiment analysis datasets different properties. stanford sentiment treebank created speciﬁcally evaluate complex compositional models language. derived base dataset relabeled amazon mechanical includes dense labeling phrases parse trees computed sentences. binary subtask amounts total labels compared sentence level labels. demonstration capability unsupervised representation learning simplify data collection remove preprocessing steps reported results ignore dense labels computed parse trees using text sentence level labels. representation learned model achieves signiﬁcantly outperforming state model ensemble visualized figure model data efﬁcient. matches performance baselines using dozen labeled examples outperforms previous results hundred labeled examples. total sentences dataset. confusingly despite relative error reduction binary subtask reach state ﬁne-grained subtask achieving sentations model learned achieve observed data efﬁciency. beneﬁt penalty data regime clue. regularization known reduce sample complexity many irrelevant features likely case model since trained language model supervised feature extractor. inspecting relative contributions features various datasets discovered single unit within mlstm directly corresponds sentiment. figure show histogram ﬁnal activations unit processing imdb reviews shows bimodal distribution clear separation positive negative reviews. figure visualize activations unit randomly selected reviews high contrast reviews shows acts online estimate local sentiment review. fitting threshold single unit achieves test accuracy outperforms strong supervised results dataset nb-svm trigram still semi-supervised state using full unit representation achieves improvement sentiment unit suggesting almost information model retains relevant sentiment analysis represented compact form single scalar. table full list results imdb dataset. figure performance binary version yelp reviews dataset function labeled training examples. model’s performance plateaus labeled examples slow improves additional data. businesses details like hospitality location atmosphere important. ideas present reviews products. additionally notable drop relative performance approach transitioning sentence document datasets. likely model working byte level leads focusing content last sentences instead whole document. finally amount labeled data increases performance simple linear model train static representation eventually saturate. complex models explicitly trained task continue improve eventually outperform approach enough labeled data. figure visualizing value sentiment cell processes randomly selected high contrast imdb reviews. indicates negative sentiment green indicates positive sentiment. best seen color. challenge introduced zhang dataset contains examples order magnitude larger datasets tested visualizing performance function number training examples figure observe capacity ceiling test accuracy approach improves little across four order magnitude increase training data. using full dataset achieve test accuracy. better tfidf baseline slightly worse linear classiﬁer frequent n-grams length observed capacity ceiling interesting phenomena stumbling point scaling unsupervised representations. think variety factors contributing cause this. since model trained amazon reviews appear sensitive concepts speciﬁc domains. instance yelp reviews like cover fits good. however annoying rear piece like garbage one. bought hoping would help huge pull back black doesn’t stay. scrap everytime it.... disappointed. table random samples model generated value sentiment hidden state ﬁxed either steps. sentiment unit strong inﬂuence model’s generative process. small sentence level dataset known domain model sets state art. large document level dataset different domain competitive standard baselines. besides classiﬁcation also evaluate standard tasks semantic relatedness paraphrase detection. model performs competitively microsoft research paraphrase corpus table performs poorly sick semantic relatedness task table likely form content semantic relatedness task built descriptions images videos contains sentences turtle hunting effectively out-of-domain model trained text product reviews. although focus analysis properties model’s representation trained generative model also interested generative capabilities. dong designed conditional generative models disentangle content text various attributes like sentiment tense. curious whether similar result could achieved using sentiment unit. table show simply setting sentiment unit positive negative model generates corresponding positive negative reviews. sampled negative reviews contain sentences negative sentiment sometimes contain sentences positive sentiment well. might reﬂective bias training corpus contains many star reviews star reviews. nevertheless interesting simple manipulation model’s representation noticeable effect behavior. samples also high quality byte level language model often include valid sentences. open question model recovers concept sentiment precise disentangled interpretable manipulable way. possible sentiment conditioning feature strong predictive capability language modelling. likely since sentiment important component review. previous work analysing lstm language models showed existence interpretable units indicate position within line presence inside quotation many ways sentiment unit model scaled example phenomena. update equation lstm could play role. element-wise operation gates encourage axis-aligned representations. models wordvec also observed small subsets dimensions strongly associated speciﬁc tasks work highlights sensitivity learned representations data distribution trained results make clear unrealistic expect model trained corpus books common genres romance fantasy learn encoding preserves exact sentiment review. likewise unrealistic expect model trained amazon product reviews represent precise semantic content caption image video. several promising directions future work highlighted results. observed performance plateau even relatively similar domains suggests improving representation model terms architecture size. since model operates byte-level hierarchical/multi-timescale extensions could improve quality representations longer documents. sensitivity learned representations training domain could addressed training wider datasets better coverage target tasks. finally work encourages research language modelling demonstrates standard language modelling objective modiﬁcations sufﬁcient learn high-quality representations. bengio yoshua courville aaron vincent pascal. representation learning review perspectives. ieee transactions pattern analysis machine intelligence dolan bill quirk chris brockett chris. unsupervised construction large paraphrase corpora exploiting massively parallel news sources. proceedings international conference computational linguistics association computational linguistics dong huang shaohan furu lapata mirella zhou ming learning generate product reviews attributes. proceedings conference european chapter association computational linguistics association computational linguistics hinton geoffrey deng dong dahl george mohamed abdel-rahman jaitly navdeep senior andrew vanhoucke vincent nguyen patrick sainath tara deep neural networks acoustic modeling speech recognition shared views four research groups. ieee signal processing magazine huang boureau y-lan lecun yann unsupervised learning invariant feature hierarchies applications object recognition. computer vision pattern recognition cvpr’. ieee conference ieee kiros ryan yukun salakhutdinov ruslan zemel richard urtasun raquel torralba antonio fidler sanja. skip-thought vectors. advances neural information processing systems krizhevsky alex sutskever ilya hinton geoffrey imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems kumar ankit irsoy ozan jonathan bradbury james english robert pierce brian ondruska peter gulrajani ishaan socher richard. anything dynamic memory networks natural language processing. corr abs/. quoc building high-level features using large scale unsupervised learning. acoustics speech signal processing ieee international conference ieee maas andrew daly raymond pham peter huang andrew potts christopher. learning word vectors sentiment analysis. proceedings annual meeting association computational linguistics human language technologiesvolume association computational linguistics madnani nitin tetreault joel chodorow martin. reexamining machine translation metrics paraphrase identiﬁcation. proceedings conference north american chapter association computational linguistics human language technologies association computational linguistics marelli marco bentivogli luisa baroni marco bernardi raffaella menini stefano zamparelli roberto. semeval- task evaluation compositional distributional semantic models full sentences semantic relatedness textual entailment. semeval- mcauley julian pandey rahul leskovec jure. inferring networks substitutable complementary products. proceedings sigkdd international conference knowledge discovery data mining oquab maxime bottou leon laptev ivan sivic josef. learning transferring mid-level image representations using convolutional neural networks. proceedings ieee conference computer vision pattern recognition pang lillian. sentimental education sentiment analysis using subjectivity summarization based minimum cuts. proceedings annual meeting association computational linguistics association computational linguistics pang lillian. seeing stars exploiting class relationships sentiment categorization respect rating scales. proceedings annual meeting association computational linguistics association computational linguistics salimans kingma diederik weight normalization simple reparameterization accelerate training deep neural networks. advances neural information processing systems socher richard perelygin alex jean chuang jason manning christopher andrew potts christopher recursive deep models semantic compositionality sentiment treebank. citeseer vincent pascal larochelle hugo bengio yoshua manzagol pierre-antoine. extracting composing robust features denoising autoencoders. proceedings international conference machine learning wang sida manning christopher baselines bigrams simple good sentiment topic classiﬁcaproceedings annual meeting tion. association computational linguistics short papers-volume association computational linguistics yonghui schuster mike chen zhifeng quoc norouzi mohammad macherey wolfgang krikun maxim yuan macherey klaus google’s neural machine translation system bridging", "year": 2017}