{"title": "Large-Margin Learning of Submodular Summarization Methods", "tag": ["cs.AI", "cs.CL", "cs.LG"], "abstract": "In this paper, we present a supervised learning approach to training submodular scoring functions for extractive multi-document summarization. By taking a structured predicition approach, we provide a large-margin method that directly optimizes a convex relaxation of the desired performance measure. The learning method applies to all submodular summarization methods, and we demonstrate its effectiveness for both pairwise as well as coverage-based scoring functions on multiple datasets. Compared to state-of-the-art functions that were tuned manually, our method significantly improves performance and enables high-fidelity models with numbers of parameters well beyond what could reasonbly be tuned by hand.", "text": "paper present supervised learning approach training submodular scoring functions extractive multi-document summarization. taking structured predicition approach provide large-margin method directly optimizes convex relaxation desired performance measure. learning method applies submodular summarization methods demonstrate effectiveness pairwise well coverage-based scoring functions multiple datasets. compared state-of-the-art functions tuned manually method signiﬁcantly improves performance enables high-ﬁdelity models numbers parameters well beyond could reasonbly tuned hand. automatic document summarization problem constructing short text describing main points document. example applications range generating short summaries news articles presenting snippets urls web-search. paper focus extractive multi-document summarization ﬁnal summary subset sentences multiple input documents. extractive summarization avoids hard problem generating wellformed natural-language sentences since existing sentences input documents used. bilmes using submodular scoring function based inter-sentence similarity. hand scoring function rewards summaries similar many sentences original documents hand penalizes summaries contain sentences similar obtaining exact summary optimizes objective computationally hard show greedy algorithm guaranteed compute good approximation. however work address select good inter-sentence similarity measure leaving problem well selecting appropriate trade-off coverage redundancy manual tuning. overcome problem propose supervised learning method learn similarity measure well coverage/reduncancy trade-off training data. furthermore learning algorithm limited model bilmes applies submodular summarization models. diminishing-returns property submodular functions computational tractability class functions provides rich space designing summarization methods. illustrate point also provide experiments submodular coverage-based model originally developed diversiﬁed information retrieval summaries training examples formulate learning problem structured prediction problem derive maximum-margin algorithm structural framework. note that unlike learning approaches method require heuristic decomposition learning task binary classiﬁcation problems directly optimizes structured prediction. enables algorithm directly optimize desired performance measure training. furthermore method limited linear-chain dependencies like learn submodular scoring function. ability easily train summarization models makes possible efﬁciently tune models various types document collections. particular learning method reliably tune models hundreds parameters based training examples. increases ﬁdelity models compared hand-tuned counterparts showing signiﬁcantly improved empirical performance. provide detailed investigation sources improvements identifying directions research. work extractive summarization spans large range approaches. starting unsupervised methods widely known approaches uses greedy approach selection considers trade-off relevance redundancy. later extended support multi-document settings incorporating additional information available case. good results achieved reformulating knapsack packing problem solving using dynamic programing popular stohastic graph-based summarization method lexrank computes sentence importance based concept eigenvector centrality graph sentence similarities. similarly textrank also graph based ranking system identiﬁcation important sentences document using sentence similarity pagerank sentence extraction also implemented using graph based scoring approaches hits positional power functions. graph based methods also paired clustering collabsum approach ﬁrst uses clustering obtain document clusters uses graph based algorithm sentence selection includes inter intra-document sentence similarities. another clustering based algorithm diversity based extension ﬁnds diversity clustering proceeds reduce redundancy selecting representative cluster. manually tuned sentence pairwise model took inspiration based budgeted submodular optimization. summary produced maximizing objective function includes coverage redundancy terms. coverage deﬁned sentence similarities between selected summary rest sentences redundancy pairwise intra-summary sentence similarities. another approach based submodularity relying extracting important keyphrases citation sentences given paper using build summary. supervised setting early methods made independent binary decisions whether include particular sentence summary not. ignores dependencies sentences result high redundancy. problem arises using learning rank approaches ranking support vector machines support vector regression gradient boosted decision trees select relevant sentences summary introducing dependencies improve performance. limited introducing dependencies sentences using linearchain hmm. assumed produce summary chain transitioning summarization non-summarization states traversing sentences document. expressive approach using sequence labeling utilize larger necessarily independent feature spaces. disadvantage using linear chain models however represent summary sequence sentences. dependencies sentences away cannot modeled efﬁciently. contrast linear chain models closely related work diversiﬁed retrieval method proposed document summarization. moreover assume subtopic labels available additional constraints diversity coverage balance added structural learning problem. contrast approach require knowledge subtopics avoids adding additional constraints furthermore different submodular objective functions example word coverage sentence pairwise models described later paper. another closely related work also takes learning approach structural framework summarize documents. however consider submodular functions instead solve integer linear program approximation thereof. encodes compression model arbitrary parts parse trees sentences summary removed. allows select parts sentences preserve gramatical structure. work focuses learning particular compression model work explores learning general large class sentence selection models. section illustrate document summarization addressed using submodular functions. documents summarized split individual sentences sn}. summarization method selects subset sentences maximizes given scoring function subject budget constraint s.t. intuitively deﬁnition says adding subset increases least much adding using speciﬁc submodular functions examples following sections illustrate diminishing returns property naturally reﬂects trade-off maximizing coverage minimizing redundancy. ﬁrst submodular scoring function consider proposed based model pairwise sentence similarities. scores summary using following function shows submodular. denotes measure similarity between pairs sentences ﬁrst term measure similar sentences included summary sentences second term penalizes similar sentences other. scalar parameter trades terms. maximizing amounts increasing similarity summary excluded sentences minimizing repetitions summary. example illustrated figure simplest case tfidf cosine similarity show later learn sophisticated similariy functions. coverage scoring function second scoring function consider ﬁrst proposed diversiﬁed document retrieval naturally applies document summarization well based notion word coverage word importance weight summary covers word least sentences contains word. score summary simply word weights covers example summary scored illustrated figure analogous deﬁnition similarity pairwise model choice word importance function crucial coverage model. simple heuristic weigh words highly occur many sentences documents however show following learn training data. computing summary computing summary maximizes either scoring functions np-hard however known greedy algorithm shown figure achieve approximation optimum solution linear budget constraint even further algorithm starts empty summarization. step sentence added summary results maximum relative increase objective. increase relative amount budget used added sentence. algorithm terminates budget reached. note algorithm parameter denominator selection rule report impact performance. selecting less gives importance information density greedy approximation guarantee holds despite additional parameter details choice effects provided experiments section. section propose supervised learning method training submodular scoring function produce desirable summaries. particular pairwise coverage model show learn similarity function word importance weights respectively. particular parameterize using linear model allowing depends full input sentences formulation ensures scoring function target summary larger scoring function summary objective function learns large margin weight vector trading upper bound empirical loss. quantities traded parameter even though exponentially many constraints number sentences input documents solved polynomial time cutting plane algorithm steps algorithm shown figure iteration algorithm training document summary worst violates constraint found. done solving weight vector learned feature vectors. pairwise model include feature like tfidf cosine number words document titles share etc. coverage model include features like indicator whether occurs sentences whether occurs document title etc. predicts structured output given input called joint feature-map input output note submodular scoring function eqns. brought form linear parametrization learn weight vector structural svms require training examples input/output pairs. document summarization however correct extractive summary typically known. instead training documents typically annotated multiple manual summaries determine single extractive target summary training extractive summary optimizes rouge score loss function respect based rouge- score standard metric measuring quality document summarization. ensure loss function zero target label deﬁned normalized loss below loss used experiments. thus training structural loss maximizes rouge- score true manual summaries provided training examples trading margin. note could easily different loss function different target evaluation metric. finally obtained structural training prediction summary test document easily obtained section empirically evaluate approach proposed paper. following experiments conducted different datasets datasets contain document sets four manual summaries set. document concatenated articles split sentences using tool provided dataset. supervised setting used resamplings random train/test/validation split. determining best value using performance validation report average performence corresponding test sets. baseline performance computed using test sets single test set. experiments datasets used greedy algorithm recommended dataset. construction features learning organized word groups. trivial group simply words considering properties words themselves constructed several features properties capitalized words words certain length non-stop words obtained another features frequently occuring words articles also considered position sentence article another feature word groups reﬁned selecting different thresholds weighting schemes forming binned variants features. pairwise model cosine similarity sentences using words given word group computation. word coverage model create separate features covering words different groups. gives fairly comparable feature strength models. addition different word coverage levels coverage model. first consider well sentence cover word secondly look important cover word combining criteria using different thresholds features word. coverage features motivated approach contrast hand-tuned pairwise baseline uses tfidf weighted cosine similarity sentences using words following approach resulting summaries evaluated using rouge version selected rouge measure used commonly used performance scores recent work. however learning method applies performance measures well. note rouge- measure loss function learning well evalﬁrst experiment compare supervised learning approach hand-tuned approach. results experiment summarized figure first supervised training pairwise model resulted statistically signiﬁcant increase performance datasets compared reimplementation manually tuned pairwise model. note reimplementation approach resulted slightly different performance numbers reported better somewhat lower evaluated selection test examples theirs. conjecture small differences implementation and/or preprocessing dataset. furthermore authors note paper datasets behave quite differently. figure results obtained datasets using supervised models. increase performance hand-tuned statistically signiﬁcant pairwise model datasets coverage model. figure also reports performance coverage model trained algorithm. results compared pairwise model. since using features comparable strength approaches well greedy algorithm structural learning method comparison largely reﬂects quality models themselves. dataset models achieve performance pairwise model performs signiﬁcantly better coverage model. figure learning curve pairwise model dataset showing rouge- scores different numbers learning examples dashed line represents preformance handtuned model. fast algorithm learn? hand-tuned approaches limited ﬂexibility. whenever move signiﬁcantly different collection documents reinvest time retune learning make adaptation collection automatic faster especially since training data collected even manual tuning. figure evaluates effectively learning algorithm make given amount training data. particular ﬁgure shows learning curve approach. even training examples learning approach already outperforms baseline. furthermore maximum number training examples available curve still increases. therefore conjecture data would improve performance. room improvement? rough estimate actually achievable terms ﬁnal rouge- score looked different upper bounds various scenarios first rouge score computed using four manual summaries different assessors estimate inter-subject disagreement. computes rouge score held-out summary remaining three summaries resulting performance given second extractive summarization restrict summaries sentences documents themselves likely lead reduction rouge. estimate drop greedy algorithm select extractive summary maximizes rouge test documents. resulting performance given extractive figure dataset drop performance optimal extractive summary points rouge. third expect drop performance since model able optimal extractive summaries lack expressiveness. estimated looking training performance reported model figure datasets drop points rouge performance. adding better features might help model data better. finally last drop performance come overﬁtting. test rouge scores given prediction figure note drop training test performance rather small overﬁtting issue well controlled algorithm. therefore conclude increasing model ﬁdelity seems like promising direction improvements. features useful? understand features affected ﬁnal performance approach assessed strength features. particular looked ﬁnal test score changes removed certain features groups shown figure important group features basic features since removing results largest drop performance. however features play signiﬁcant role conﬁrms performance improved adding compared greedy algorithm exhaustive search three selected sentences half cases solution cases soultion average optimal conﬁrming greedy selection works quite well. figure upper bounds rouge- scores agreement manual summaries greedily computed best extractive summaries best model train test scores pairwise model. second important group features conlocation) sidering drop performance compared full pariwise model. important basic similarity features including words last feature group actually lowered score included model found later dataset. four manual summaries important computing reliable rouge score evaluation clear whether approach efﬁcient annotator resources training. ﬁnal experiment trained method using single manual summary documents. using single manual summary arbitrarily took ﬁrst provided four reference summaries used compute target label training otherwise experimental setup previous subsections using pairwise model. rouge- score obtained using single summary document slightly signiﬁcantly lower obtained four summaries similarly performance drop signiﬁcant well. based results conjecture having documents sets single manual summary useful training fewer training examples better labels cases spend approximately amount effort however training examples helps spending effort multiple summaries appears minor beneﬁt training. paper presented supervised learning approach extractive document summarization based structual svms. learning method applies submodular scoring functions ranging pairwise-similarity models coverage-based approaches. learning problem formulated convex quadratic program solved approximated using cutting-plane method. empirical evaluation structural approach signiﬁcantly outperforms conventional hand-tuned models datasets. advantage learning approach ability handle large numbers features providing substantial ﬂexibility building high-ﬁdelity summarization models. furthermore shows good control overﬁtting making possible train models even training examples.", "year": 2011}