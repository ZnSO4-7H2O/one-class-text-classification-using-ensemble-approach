{"title": "Conditions Under Which Conditional Independence and Scoring Methods Lead  to Identical Selection of Bayesian Network Models", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "It is often stated in papers tackling the task of inferring Bayesian network structures from data that there are these two distinct approaches: (i) Apply conditional independence tests when testing for the presence or otherwise of edges; (ii) Search the model space using a scoring metric. Here I argue that for complete data and a given node ordering this division is a myth, by showing that cross entropy methods for checking conditional independence are mathematically identical to methods based upon discriminating between models by their overall goodness-of-fit logarithmic scores.", "text": "argue complete data given node ordering division myth showing cross entropy methods checking conditional mathematically identical upon discriminating overall goodness-of-fit plan paper follows. introduces sults. section sections. known distribution infinite data set. section considers realistic data classical tive. paper consider learning bayesian network structures restrictions ordering. following typical statements novel algorithm reviewing learning bayesian networks. discrete random variables taking values state space xj=xi. denotes take values xaeaxa. quote convenient either introduc­ index particular using lower case letters paper consider search algorithms given node ordering; take node ordering denote directed acyclic graphs parent denote distributions rected markov respect means probability generally categories rithms uses heuristic construct model evaluates using scoring method. category algorithms bayesian networks analyzing dependency relationships nodes. rated model complete graph would data perfectly. scoring based search methods usually izing model's predictive model's complexity fitting. assumption made simplicity mations made account pattern missing data. also plies logarithmic additively function node depending upon node parents dag) thus making local enabling indepen­ dent optimizations markov respect minimum kullback-leibler obeys node graph. prin­ ciple search possible distributions vergence kullback-leibler among graphs fewest num­ edges. \\xpa· differing edges choosing maximizes equivalent minimizes adding parents divergence decrease kullback-leibler possible could thin parents node removing nodes remains zero. generally decision criterion search gorithm moves model submodel possible sets based upon considerati tests associated conditional independence could used give result based upon consideration changes divergence numeri­ kullback-leibler entering decision process upon quantities decision based identical search frameworks. another every search heuristic using conditional alent search heuristic kullback-leibler fundamental difference proaches p)/fi would either accept reject hypothesis x;jls; latter decide among candidate search next iteration algorithm. common decision heuristics value fixed threshold value accept conditional classical conditional independence multiple suitable heuristics directed markov property tests data allows conditional performed locally dependence tests employ mles. however sam­ pling variability requirement cross entropy expressions thus example suppose current model node parents data obtains mles furthermore suppose node nodes x/-l marginal likelihood factor­ izes terms node parents. before graph except difference require prior alternative ors) every node local prior graph form priors obtain ratio posterior prior space conditions ordering shown conditional tests searching lent local log-scoring interpreting possible sidering additions; terms pair nodes) combination would considered indepen­ dence tests. however latter case would extra option deciding conditional dependence properties nodes independently locally conditional fined using scoring metrics considering conditional reversals. inde­ pendence tests combined single test procedures decision rules.", "year": 2013}