{"title": "Falling Rule Lists", "tag": ["cs.AI", "cs.LG"], "abstract": "Falling rule lists are classification models consisting of an ordered list of if-then rules, where (i) the order of rules determines which example should be classified by each rule, and (ii) the estimated probability of success decreases monotonically down the list. These kinds of rule lists are inspired by healthcare applications where patients would be stratified into risk sets and the highest at-risk patients should be considered first. We provide a Bayesian framework for learning falling rule lists that does not rely on traditional greedy decision tree learning methods.", "text": "falling rule lists classiﬁcation models consisting ordered list if-then rules order rules determines example classiﬁed rule estimated probability success decreases monotonically list. kinds rule lists inspired healthcare applications patients would stratiﬁed risk sets highest at-risk patients considered ﬁrst. provide bayesian framework learning falling rule lists rely traditional greedy decision tree learning methods. healthcare patients actions need prioritized based risk. at-risk patients handled highest priority patients second at-risk receive second highest priority decision process perfectly natural human decision-maker instance physician might check patient symptoms high severity diseases ﬁrst check symptoms less serious diseases etc.; however traditional paradigm predictive modeling naturally contain type logic. clear logic wellfounded typical machine learning model would usually able discover machine learning methods produce highly complex models designed provide ability reason prediction. leaves predictive models directly aligned decisions need made them. problem could directly useful clinical practice. falling rule list ordered list if-then rules order rules determines example classiﬁed rule estimated probability success decreases monotonically list. thus falling rule list directly contains decision-making process whereby at-risk observations classiﬁed ﬁrst second falling rule list might instance patients history heart disease highest risk stroke risk patients high blood pressure second highest risk stroke risk patients neither conditions lowest risk stroke risk. table shows example decision lists constructed mammographic mass dataset part experimental study. took seconds construct model laptop. model states biopsy results show tumor irregular shape patient tumor highest risk being malignant next risk remaining tumors spiculated margins patients years right column table shows many patients rules risk probabilities directly calibrated data. falling rule lists serve dual purpose rank rules form predictive model stratify patients decreasing risk sets. saves work physician; sorting expensive mental operation model sorting naturally. standard decision tree decision list method instead identifying highest at-risk patients could much involved calculation number conditions at-risk patients need satisfy might diﬃcult physicians memorize. data-driven algorithmic approaches. manually-created risk assessment tools used possibly every hospital; e.g. timi scores chads score apache scores ranson score name models computed without calculator making practical decision aids. course level interpretability purely data-driven classiﬁers manual feature selection rounding coeﬃcients. algorithms discretize input space gained popularity purely yield interpretable models. decision trees well decision lists organize collection simple rules larger logical structure popular despite greedy. inductive logic programming returns unstructured conjunctive rules example classiﬁed positive satisﬁes rules set. extremely simple induce probabilistic model unordered rules given method place decision list ordering rules empirical risk. also done associative classiﬁcation however resulting model cannot expected exhibit good predictive performance constituent rules chosen diﬀerent objective. since possible decision tree methods produce results inconsistent monotonicity properties data subﬁeld dedicated altering greedy decision tree algorithms obey monotonicity properties studies showed many cases accuracy lost enforcing monotonicity constraints medical experts willing models monotonicity constraints accurate classiﬁers often large enough contains interpretable classiﬁers monotonicity properties enforce much stronger ben-david feelders pardoel altendorf accuracy sometimes sacriﬁced always generally much. hand possible method gains level practicality interpretability methods simply cannot. interpretability context dependent matter measures domain diﬀerent next domain. falling rule list used medical practice beneﬁt practice sparse desired. since automatically stratiﬁes patients risk order used decision making physicians choose look much list need make decision; list sparse requires physicians care high risk patients look rules check whether patient obeys clauses. algorithm provide falling rule lists aims best worlds accuracy interpretability computation. algorithm starts statistical assumption build accurate model pre-mined itemsets. helps tremendously computation restricts building models interpretable building blocks itemsets discovered bayesian modeling approach chooses subset permutation rules form decision list. user determines desired size rule list bayesian prior. generative model constructed monotonicity property fully enforced example might indicate presence disease would patient’s features. represent conditional distribution decision list ordered list if...then... rules. require special structure decision list probability associated rule decreasing moves decision list. discussed earlier help computation place positive prior probability {cl}l− lists consisting boolean clauses returned frequent itemset mining algorithm particular work used fpgrowth whose input binary dataset boolean vector whose output subsets features dataset. example might indicate presence diabetes might indicate presence hypertension boolean function returned fpgrowth might return patients diabetes hypertension. matter rule mining algorithm chosen perform breadth-ﬁrst searches return clauses suﬃcient support. here needs sufﬁciently large hypothesis space considered models small. viewed hyperparameter maximum length decision result draw discrete distribution l-th rule drawn probability proportional user designed weight example rule might chosen probability proportional number clauses allows user express preferences diﬀerent types clauses list. observed though note process speciﬁes joint distribution independently distributed truncated gamma variables permits posterior gibbs sampling enforcing monotonicity constraints still permitting diversity prior distributions. example could encourage near middle list large case risks would widely spaced middle list finally models risk patients satisfying rules gamma distributed. first describe approach ﬁnding decision list maximum posteriori probability. discuss approach perform monte carlo sampling posterior distribution decision furthermore solution subproblem ﬁnding ...l− approximated closely using simple procedure involves maximizing posterior probability decision list given rules optimizing equation lends better simulated annealing equation optimizing involves optimization discrete space given objective function discrete search space function specifying neighbors state temperature schedule function time steps simulated annealing procedure note approach optimizes full rule lists itemsets rely greedy splitting. even bayesian tree methods traverse wider search space greedy splitting local solutions e.g. chipman perform posterior sampling gibbs sampling steps {γl}l− made possible variable augmentation metropolis-hastings steps {cl}. describe variable augmentation given augmented model cycle following steps following deterministic order. discussed detail shortly. regarding notation θaug refer parameters augmented model mixing gibbs collapsed metropolis-hastings sampling steps requires special care ensure markov chain proper possesses desired stationary distribution. refer park details note collapsed metropolis-hastings step ﬁrst performing gibbs upn= separate gibbs update would proper. show simulated data generated known decision list simulated annealing procedure searches decision list high probability recovers true decision list. given observations arbitrary features collection rules features construct binary matrix rows represent observations columns represent rules entry rule applies observation otherwise. need simulate binary matrix represent observations without losing generality. simulations generated independent binary rule sets rules setting feature value independently probability generated random decision list size selecting rules random setting induced roughly evenly spaced unit interval performed following procedure times generate random rule matrix random decision list pendent dataset size generating labels decision list perform simulated annealing using procedure described section obtain point estimate decision list. calculate edit distance decision list returned simulated annealing true decision list. figure shows average distance replicates simulated annealing steps replicate used gradual cooling schedule. since placed extremely strong restriction characteristics predictive model expect lose predictive accuracy unrestricted methods. interpretability beneﬁt suﬃcient compensate this heavily application-dependent. found several cases loss performance step reason using collapsed rather regular metropolis-hastings step improve chain mixing. metropolis-hastings proposal distribul= exactly proposal distribution used generate successor state simulated annealing used decision list. diﬀerence operastep full conditional distribution pairs variables mutually independent conditioning. therefore sample suﬃcient independently sample shown following sampling scheme samples full conditional distribution sample measured out-of-sample performance using auroc -fold cross validation decision list training used predict test fold turn. compared regularized logistic regression cart random forests implemented python using scikit-learn package. logistic regression hyperparameters tuned grid search nested cross validation. discussed decision lists consisting rules inductive logic programming method expected exhibit strong performance. tested nfoil default settings obtain rules. rules ordered diﬀerent ways form additional comparison methods empirical risk rule using rules pre-mined rule accepts input note risk probabilities rule lists returned necessarily decreasing monotonically nfoil rules necessarily rule list returned since omission rule increase posterior. auroc’s diﬀerent methods table indicating loss accuracy using falling rule lists particular dataset. training folds decision lists length either sparse. figure shows curves test folds methods. mean curves bolded. particular dataset cart perform well. unclear perform well cross-validation performed svm; usually svm’s perform well cross-validated expected nfoil-based methods later section quantify loss predictive power falling rule lists methods using out-of-sample predictive performance evaluation. speciﬁcally compare several baseline methods standard publicly available datasets quantify possible loss predictive performance. applied falling rule lists preliminary readmissions data compiled collaboration major hospital u.s. goal predict whether patient readmitted hospital days using data prior release. dataset contains features binary readmissions outcomes approximately patients prior history readmissions. features detailed include aspects like impaired mental status diﬃcult behavior chronic pain feels unsafe features might predictive readmission. luckily physician required collect amount detailed information assess whether given patient high risk readmission. experiments experiments next section parameters tuned falling rule lists global hyperparameters chosen follows. mined rules support least cardinality conditions rule. assumed prior conditioned rule equal chance rule list. readmissions risk readmissions risk else poorprognosis maxcare readmissions risk else poorcondition noshow readmissions risk else bedsores else negativeideation noshow readmissions risk readmissions risk else maxcare readmissions risk else noshow readmissions risk else moodproblems else readmissions risk table displays results. discussed earlier observed even severe restrictions model performance levels still methods often substantially worse. likely beneﬁts using greedy splitting method restricting space mined rules careful formulation optimization. furthermore beats nfoil based methods performance public datasets. again reason rules nfoil found using diﬀerent objective intended predict well placed decision list. even subset nfoil rules could selected placed order performance poor showing nfoil rule contain rules useful falling rule list. fact nfoil rules always much smaller fpgrowth overly restricting hypothesis space. display table number rules mined fpgrowth nfoil datasets analyzed. table shows point estimate obtained training falling rule lists full dataset took seconds. probability column empirical probability readmission rule; support indicates number patients classiﬁed rule. model indicates patients sores skipped appointments likely readmitted. conditions used model include poorprognosis meaning patient need palliative care services poorcondition meaning patient exhibits frailty signs neglect malnutrition negativeideation means suicidal violent thoughts maxcare means patient needs maximum care model lends naturally decision-making need view list obtain characterization high risk patients. present class interpretable predictive models could potentially major beneﬁt decision-making domains. nicely stated director u.s. national institute justice interpretable model actually used better accurate sits shelf. envision models produced used instance physicians third world countries require models printed laminated cards. high stakes decisions important know whether trust model using make decisions; models like help tell trust.", "year": 2014}