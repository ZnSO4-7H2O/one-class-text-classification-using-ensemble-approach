{"title": "Dense Prediction on Sequences with Time-Dilated Convolutions for Speech  Recognition", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "In computer vision pixelwise dense prediction is the task of predicting a label for each pixel in the image. Convolutional neural networks achieve good performance on this task, while being computationally efficient. In this paper we carry these ideas over to the problem of assigning a sequence of labels to a set of speech frames, a task commonly known as framewise classification. We show that dense prediction view of framewise classification offers several advantages and insights, including computational efficiency and the ability to apply batch normalization. When doing dense prediction we pay specific attention to strided pooling in time and introduce an asymmetric dilated convolution, called time-dilated convolution, that allows for efficient and elegant implementation of pooling in time. We show results using time-dilated convolutions in a very deep VGG-style CNN with batch normalization on the Hub5 Switchboard-2000 benchmark task. With a big n-gram language model, we achieve 7.7% WER which is the best single model single-pass performance reported so far.", "text": "computer vision pixelwise dense prediction task predicting label pixel image. convolutional neural networks achieve good performance task computationally efﬁcient. paper carry ideas problem assigning sequence labels speech frames task commonly known framewise classiﬁcation. show dense prediction view framewise classiﬁcation offers several advantages insights including computational efﬁciency ability apply batch normalization. dense prediction speciﬁc attention strided pooling time introduce asymmetric dilated convolution called time-dilated convolution allows efﬁcient elegant implementation pooling time. show results using time-dilated convolutions deep vgg-style batch normalization switchboard- benchmark task. n-gram language model achieve best single model single-pass performance reported far. deep convolutional networks seen tremendous sucess computer vision speech recognition last years. many computer vision problems fall problem types ﬁrst classiﬁcation single label produced image second dense pixelwise prediction label produced pixel image. examples dense prediciton semantic segmentation depth prediction optical surface normal prediction etc. efﬁcient convolutional architectures allow produce full image sized output rather predicting values pixel separately small patch centered around pixel. paper argue look acoustic modeling speech dense prediction task sequences. contrast usual viewpoint framewise classiﬁcation indicating cross-entropy training stage context-window used input network predicts center frame. however stages want acoustic model applied sequence produce sequence predictions. case sequence training test time end-to-end training setting. similar convolutional architectures dense prediction computer vision focus efforts convolutional architectures process utterance produce sequence labels output rather splicing utterance i.e. labeling frame independently small window around four main advantages convolutional architectures allow efﬁcient evaluation full utterance dense prediction viewpoint main architectural novelty paper allow strided pooling time. next sections adopt recent technique dense prediction named dilated convolutions acoustic models enable strided pooling time. experiments results model section pooling stride essential ingredient classiﬁcation allowing access context higher feature maps reducing spatial resolution absorbed fully connected layers. however dense pixelwise prediction tasks less straightforward deal downsampling hand downsampling allows global view large receptive ﬁelds resolution hand also need detail small scale i.e. need high resolution information. incorporate global local information downsampling pooling incorporated dense prediction networks several ways. firstly many methods involve upsampling lower resolution feature maps usually combined higher resolution feature maps. image processed three different scales three different cnns output feature maps merged. fully convolutional networks classiﬁcation network basis introducing skip connections merge hi-res lower layers upsampled low-res layers deeper network. segnet uses encoder-decoder structure upsampling done max-unpooling i.e. remembering location encoder’s pooling layers. second using cnns strided pooling dense prediction proposed every pooling layer stride input duplicated times shifted offset convolutional stages output interleaved recover full resolution. third called spatial dilated convolutions keeps feature maps original resolution. idea replace pooling stride pooling stride dilate convolutions factor meaning values skipped. called ﬁlter rarefaction introduced d-regularly sparse kernels dubbed spatial dilated convolutions noted method equivalent shift-and-interleave though intuitive. recent wavenet work uses dilated convolutions generative model audio. previous work cnns acoustic modeling eliminated possibility strided pooling time downsampling effect. recent work shows signiﬁcant performance boost using pooling time cross-entropy training however sequence training prohibitively expensive since utterance spliced uttlen independent windows. adapting notion dense prediction propose allow pooling time maintaining efﬁcient full-utterance processing using asymmetric version spatial dilated convolution dilation time direction frequency direction appropriately call time-dilated convolutions. problem strided pooling time length output sequence shorter length input sequence factor assuming pooling layers stride recurrent end-to-end networks typically factor size reduction accepted limits number pooling layers hybrid nn/hmm framework pooling acceptable. essentially need strided pooling time keeping resolution. tackle problem version sparse kernels equivalently spatial dilated convolutions consider simple figure takes context window frames produces single output. let’s consider applying full utterance length ﬁgure blue outputs downsampled factor strided pooling output sequence length match number targets solution problem visualized figure first pool without stride preserves resolution pooling. however consecutive convolutional layer needs modiﬁed; speciﬁcally kernel skip every value order ignore values came values. dilation kernel factor time direction. formally discrete convolution dilation convolves signal kernel size deﬁned general procedure change time-pooling cross-entropy training dense prediction stage sequence training testing follows. change pooling layers stride stride multiply dilation factor following convolutions factor this convolution coming pooling layers original stride dilation factor fully connected layers equivalent trivially replaced convolutional layers kernel dilating procedure classiﬁcation network adapted semantic segmentation using time-dilated convolutions feature maps output keep full resolution input pooling stride. pooling receptive ﬁeld time larger network without pooling. allows combine performance gains pooling maintaining computational efﬁciency ability apply batch normalization trained style hybrid nn/hmm setting switchboard+fisher dataset. architecture training method similar earlier papers based setup described input features vtln-warped logmel outputs tied states forced alignment. table fully speciﬁes training windows predicting center frame. corresponding observations time though frequency direction. training followed standard two-stage scheme ﬁrst frames cross-entropy training followed frames sequence training training done nesterov acceleration learning rate decaying frames. data balancing exponent report results hub’ decoding using standard small n-gram language model word vocabulary. slight improvement results decoding exponent prior lower used training. mentioned section batch normalization network mean variance statistics accumulated feature maps frequency direction. selection models decoding prior acoustic weight happened decoding heldout set. result presented tables baseline personal communication authors. baseline means system combination. note baselines slightly smaller n-grams small n-grams note typically subsequent rescoring advanced language models like lstm lms; single model performance achieved starting n-gram decoding result knowledge best published single model. stacked bottleneck networks hierarchical bottleneck networks inﬂuential acoustic model hybrid nn/hmm speech recognition. sbns typically seen consecutive dnns stage separately trained discriminatively bottleneck ﬁrst sees input features second gets bottleneck features ﬁrst input. typically second gets bottleneck features stride i.e. features position relative center pointed convolutional backpropagate stages together. fact multi-stage architecture special case time-dilated convolution. speciﬁcally equivalent large ﬁrst kernel followed kernels. second exactly equivalent ﬁrst kernel size dilation factor time direction. layers bottleneck ﬁrst form auxilary classiﬁer. realization prompts number directions sbns extended. firstly avoiding large kernel ﬁrst convolutional layer possible keep time frequency structure internal representations future layers enabling increased depth. secondly rather increasing time-dilation factor once seems natural gradually increase time-dilation factor throughout depth network. convolutional networks also used end-to-end models speech recognition. cldnn architecture deep speech combine convolutional network ﬁrst stage lstm fully connected output layers. wavletter competitive end-to-end model presented fully convolutional. wavletter certain amount downsampling pooling striding accepted training criterion since doesn’t require output length input. however report degradation english work around using grapheme bigram targets. time-dilated convolutions introduced could improve models ways either could allow amount pooling keeping higher resolution. alternatively could keep resolution expand receptive ﬁeld adding time-dilated convolution layers gives access broader context layers. conclusion work relevant end-to-end models hybrid hmm/nn models. drew parallel dense prediction computer vision framewise sequence labeling hmm/nn end-to-end setting. provided tool adopt pooling time acoustic models maintaining efﬁcient processing batch normalization full utterances. hub’ brought previous work relative improvement. language model achieve best single model single-pass performance reported far. fisher vladlen koltun multi-scale context aggregation dilated convolutions proc iclr aaron oord sander dieleman heiga karen simonyan oriol vinyals alex graves kalchbrenner dario amodei rishita anubhai eric battenberg carl case jared casper bryan catanzaro jingdong chen mike chrzanowski adam coates greg diamos deep speech end-to-end speech recognition english mandarin corr arxiv.", "year": 2016}