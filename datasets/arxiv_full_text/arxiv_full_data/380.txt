{"title": "Using Parameterized Black-Box Priors to Scale Up Model-Based Policy  Search for Robotics", "tag": ["cs.RO", "cs.AI", "cs.LG", "cs.NE", "stat.ML"], "abstract": "The most data-efficient algorithms for reinforcement learning in robotics are model-based policy search algorithms, which alternate between learning a dynamical model of the robot and optimizing a policy to maximize the expected return given the model and its uncertainties. Among the few proposed approaches, the recently introduced Black-DROPS algorithm exploits a black-box optimization algorithm to achieve both high data-efficiency and good computation times when several cores are used; nevertheless, like all model-based policy search approaches, Black-DROPS does not scale to high dimensional state/action spaces. In this paper, we introduce a new model learning procedure in Black-DROPS that leverages parameterized black-box priors to (1) scale up to high-dimensional systems, and (2) be robust to large inaccuracies of the prior information. We demonstrate the effectiveness of our approach with the \"pendubot\" swing-up task in simulation and with a physical hexapod robot (48D state space, 18D action space) that has to walk forward as fast as possible. The results show that our new algorithm is more data-efficient than previous model-based policy search algorithms (with and without priors) and that it can allow a physical 6-legged robot to learn new gaits in only 16 to 30 seconds of interaction time.", "text": "recently introduced black-drops algorithm ﬁrst model-based policy search algorithms robotics purely black-box extensively take advantage parallel computations. black-drops achieves similar data-efﬁciency state-of-the-art approaches like pilco faster multi-core computers easier much less limiting however black-drops scales well number processors main challenge model-based policy search scaling complex problems algorithm models transition function full state/action spaces complexity model increases substantially degree freedom; unfortunately quantity data required learn good model scales time exponentially dimension state space consequence data-efﬁciency model-based approaches greatly suffers increase dimensionality model. practice model-based policy search algorithms currently employed simple systems state action space combined abstract— data-efﬁcient algorithms reinforcement learning robotics model-based policy search algorithms alternate learning dynamical model robot optimizing policy maximize expected return given model uncertainties. among proposed approaches recently introduced blackdrops algorithm exploits black-box optimization algorithm achieve high data-efﬁciency good computation times several cores used; nevertheless like modelbased policy search approaches black-drops scale high dimensional state/action spaces. paper introduce model learning procedure black-drops leverages parameterized black-box priors scale high-dimensional systems robust large inaccuracies prior information. demonstrate effectiveness approach pendubot swing-up task simulation physical hexapod robot walk forward fast possible. results show algorithm data-efﬁcient previous model-based policy search algorithms allow physical -legged robot learn gaits seconds interaction time. trying something might take seconds hours even days unfortunately current state-of-the-art learning algorithms either rely availability large data sets make sense simulated environments scarcity data calls algorithms highly dataefﬁcient minimize interaction time robot world even means considerable computation cost. dataefﬁcient algorithms model-based policy search algorithms episode algorithm updates model dynamics robot searches best policy according model. improve data-efﬁciency current algorithms take uncertainty model account order avoid overﬁtting model pilco algorithm implements *corresponding author jean-baptiste.mouretinria.fr authors following afﬁliations inria villers-l`es-nancy france cnrs loria vandœuvre-l`es-nancy france universit´e lorraine loria vandœuvre-l`es-nancy france work received funding european research council european union’s horizon research innovation programme european commission project andy dimensionality prior information system modeled; instance dynamic simulators robot effective priors often available. ideal model-based policy search algorithm priors robotics should therefore algorithms leverage prior information speedlearning real system none fulﬁlls properties. paper propose novel purely black-box ﬂexible data-efﬁcient model-based policy search algorithm combines ideas black-drops algorithm simulation-based priors recent model learning algorithms show approach capable learning policies seconds control damaged physical hexapod robot outperforms state-of-the-art model-based policy search algorithms without black-drops priors well prior-based bayesian optimization model-free policy search methods successful robotics easily applied highdimensional continuous state-action problems power algorithm uses probability-weighted averaging property following natural gradient without computing algorithm similar performance power puts constraint reward function. natural evolution strategies covariance matrix adaptation families algorithms population-based blackbox optimizers iteratively update search distribution calculating estimated gradient distribution parameters generation sample policy parameters rank based expected return. performs gradient ascent along natural gradient whereas cma-es updates distribution exploiting technique evolution paths. although model-free policy search methods promising require hundreds thousands episodes converge good solutions data-efﬁciency methods increased learning model system data inferring optimal policy model example state-of-the-art model-free policy gradient methods ddpg require interaction time solve cart-pole swing-up task whereas state-of-the-art model-based policy search algorithms require less probabilistic models successful deterministic ones provide estimate uncertainty approximation incorporated long-term planning black-drops pilco dataefﬁcient model-based policy search algorithms robot control. essentially differ uncertainty model optimize policy given model pilco uses moment matching analytical gradients whereas black-drops uses monte-carlo rollouts black-box optimizer. black-drops adds main beneﬁts pilco reward function policy parameterization used highly-parallel algorithm takes advantages multi-core computers. black-drops achieves similar dataefﬁciency pilco escapes local optima faster standard control benchmarks also able learn scratch high dimensional policy trials physical low-cost manipulator model-based policy search algorithms reduce required interaction time complex higher dimensional systems still require dozens even hundreds episodes working policy; systems might also fail good policy inevitable model errors biases reduce interaction time without learning models begin meaningful initial policy search locally improve usually done human demonstration movement primitives human either tele-operates moves robot hand trying achieve task model-free method applied improve initial policy however approaches still suffer data inefﬁciency model-free approaches require dozens hundreds episodes good policies. another reduce interaction time modelfree approaches pre-compute archives/libraries policies/controllers search online works best real system intelligent trial-and-error algorithm ﬁrst uses evolutionary algorithm called map-elites off-line create archive diverse locally high-performing behaviors utilizes modiﬁed version bayesian optimization quickly compensatory behavior. although it&e allow instance damaged -legged robot gait dozen trials robotic overcome several blocked joints minutes searching reducing interaction time model-based policy search achieved using priors models i.e. starting initial guess dynamics learning residual model. pilco priors pi-rem closely related policy search procedure pilco. pilco priors uses simulated data create gaussian process prior whereas pi-rem uses analytic equations prior model. main limitation pilco priors implicitly requires task solved prior model pilco gp-ilqg also learns residual model like pi-rem uses modiﬁed version ilqg policy given uncertainties model. gp-ilqg however requires prior model differentiable. traditional exploiting analytic equations model identiﬁcation approaches model identiﬁcation rely main ingredients proper excitation system parametric models. recently proposed method combines model identiﬁcation speciﬁcally approach relies model predictive control scheme optimistic exploration parametric model estimated collected data using least-squares. analytical equations fully capture system often case dealing unforeseen effects like example complex friction effects exists severe model mismatch like instance robot damaged. methods proposed combine model identiﬁcation model learning nevertheless methods based manipulator equation exploiting different ways straight-forward used complicated robots involve complex collisions contacts continuous-valued states controls i.i.d. gaussian system noise unknown transition dynamics assume initial guess dynamics function accurate either precise model system robot damaged unforeseen tunable parameters change behavior. examples parameters optimization parameters dynamic simulator involving contacts collisions internal parameters robot finally non-parametric model model whatever possible capture objective deterministic policy maximizes expected long-term reward following policy time steps immediate reward state assume function parameterized model-based policy search priors begin optimizing policy prior model applying real system gather initial data. afterwards loop iterated ﬁrst learn model using prior model collected data optimize policy given newly learned model finally policy applied real system data collected loop re-iterates task solved. would like model approximates accurately possible unknown dynamics system given initial guess rely gaussian processes successfully used many model-based reinforcement learning approaches extension multivariate gaussian distribution inﬁnite-dimension stochastic process ﬁnite combination dimensions gaussian distribution inputs tuples made state vector action vector re+u training targets difference current state vector next independent model dimension difference vector ∆xt. assuming observations simulator function query input point formulation allows combine observations simulator real-world smoothly. areas real-world data available simulator’s prediction corrected match real-world ones. contrary areas real-world data predictions resort simulator model learning procedure used several articles particular learn cumulative reward model procedure highlighted it&e approach gp-ilqg pi-rem formulate similar model learning procedure optimal control policy search respectively. gp-ilqg additionally assumes prior model differentiable always true might slow perform ﬁnite differences pilco priors utilizes similar scheme assumes prior model learned simulation data gathered running pilco prior system. exponential kernel automatic relevance determination searching best kernel hyper-parameters maximum likelihood estimation non-tunable mean function seek maximize gradients likelihood function analytically computed makes possible gradient based optimizer since independent independent optimizations. limbo library regression model system searching best matches observations seen model identiﬁcation procedure could solved minimizing mean squared error; nevertheless framework allows jointly optimize kernel hyper-parameters mean parameters allows modeling procedure balance non-parametric parametric modeling. easily extend include parameterized mean functions time even though independent need share mean parameters model robot consistent output dimensions. thus jointly optimize mean parameters kernel hyper-parameters gps. since dynamic simulators differentiable cannot resort gradient-based optimization optimize jointly gps. black-box optimizer like cma-es could employed instead optimization slow converge preliminary experiments. combine beneﬁts gradient-based gradient-free optimization gradient-based optimization kernel hyper-parameters black-box optimization mean parameters. conceptually would like optimize mean parameters given optimal kernel hyperparameters them. since know before-hand nested optimization loops outer loop gradient-free local optimizer searches best parameters provided nlopt continuous spaces exhaustive search discrete ones) inner optimization loop given mean parameter vector gradient-based optimizer searches best kernel hyper-parameters returns score corresponds optimal natural combining likelihoods independent form objective function outer loop take product would equivalent taking joint probability likelihoods independent however observed taking harmonic mean likelihoods instead yielded robust results. comes fact product dominated terms thus parameters explain output dimension perfectly others well would still chosen. addition practice observed taking likelihoods proved numerically stable harmonic mean. approaches usually either computationally expensive requires approximation made instead treats monte-carlo rollout noisy measurement function actual function perturbed noise tries maximize expectation assume therefore maximizing equivalent maximizing second main idea black-drops population-based black-box optimizer optimize noisy functions take advantage multi-core computers. bipop-cmaes learning approach call gp-mi combines nonparametric model learning parametric model identiﬁcation related approach differences them. firstly model learning procedure depends manipulator equation cannot easily used robots directly comply equation whereas gp-mi imposes structure prior model providing tunable parameters furthermore approach tied inverse dynamics models cannot used forward models general case contrary gp-mi used inverse forward dynamics models general blackbox tunable prior model. black-drops algorithm policy search allows type priors discussed section iv-b leverage speciﬁc policy parameterizations suitable different cases assume prior information policy parameters begin optimizing policy prior model. moreover took advantage multi-core architectures speedexperiments. contrary black-drops pilco cannot take advantage multiple cores need deriving gradients different policy/reward makes difﬁcult ideas/policies. reference pilco priors pendubot task took around hours modern computer cores whereas black-drops priors blackdrops gp-mi took around hours hours respectively. pi-rem close approach leverages priors learn residual model performs policy search model. however pi-rem assumes prior information ﬁxed cannot tuned whereas approach additional ﬂexibility able change behavior prior. addition pi-rem utilizes policy search procedure pilco limiting many cases already discussed. nevertheless blackdrops pilco shown perform similarly pilco’s limitations present include experiments variant approach resembles pi-rem ﬁrst evaluate approach simulation pendubot swing-up task. pendubot two-link underactuated robotic introduced inner joint exerts torque outer joint cannot system four continuous state variables joint angles joint angular velocities. angles joints measured anti-clockwise upright position. pendubot starts hanging goal policy pendubot swings balances upright position. episode lasts control rate distance based reward function chose task fairly difﬁcult problem forces slower convergence model-based techniques without priors hard fact allowed make rather extensive evaluation meaningful fig. results pendubot task lines median values shaded regions percentiles. table description priors. black-drops gp-mi always solves task achieves high rewards least fast approaches cases considered. black-drops achieves good rewards whenever parameters tune ones wrong rewards otherwise black-drops priors performs well whenever prior model away real well whenever prior misleading black-drops priors similar performance easily distinguishable. it&e pilco priors able reliably solve task across different prior models. comparisons assume priors available; tried capture easy difﬁcult cases cases wrong parameters tuned tunable useful fully tunable prior close actual one; tunable fully tunable prior close actual; tunable misleading prior fully tuned actual; partially tunable prior cannot fully tuned actual. black-drops gp-mi variant additionally assume parameters tuned parameters ﬁxed cannot changed. since adaptation part it&e deterministic algorithm system uncertainty prior generated algorithm speciﬁc form ﬁrst formulated paper discussed above close spirit gp-ilqg pi-rem therefore assume performance black-drops priors representative could achieved pi-rem gp-ilqg although black-drops priors effective performs global search archives different random seeds adaptation part it&e archive. used equally spread time end-effector positions behavior descriptor archive generation map-elites. black-drops variants it&e used neural network policy hidden layer hyperbolic tangent activation function. similarly it&e since pilco priors deterministic algorithm given prior prior pilco times different random seeds prior model pilco priors actual system different model. used priors policy dynamics model learning actual system also used policy pseudo-observations black-drops gp-mi always solves task achieves high rewards approaches cases considered blackdrops performs well parameters tune ones wrong badly otherwise black-drops priors performs well whenever prior model away real well whenever prior misleading black-drops pilco cannot solve task less interaction time blackdrops shows faster learning curve interestingly pilco priors able always achieve better results black-drops always worse black-drops priors. explained fact pilco without priors learns slower blackdrops local search algorithm needs interaction time achieve good results. contrary black-drops uses modiﬁed version cma-es easily escape local optima moreover initial prior model pilco priors approximated model whereas black-drops priors uses actual prior model begin with. lastly policy pilco mainly used with creates really high dimensional policy spaces compared simple neural network policy black-drops using causes policy search converge slower. it&e able reliably solve task achieve high rewards. it&e assumes system redundant enough task solved many different ways policy/controller pre-computed archive solve task obviously assumptions violated pendubot scenario system underactuated thus required redundancy system inherently unstable precise policy parameters needed also evaluate approach hexapod locomotion task introduced it&e paper physical robot scenario it&e excels achieves remarkable recovery capabilities assume simulator intact robot available gp-mi also assume alter simulator removing hexapod simulator accurate assume perfect velocity actuators inﬁnite torque. leading total dof. state robot consists joint angles joint velocities center mass pose velocities. policy open-loop controller parameters outputs joint angles every similar used episode lasts robot tracked motion capture system. task policy walk forward fast possible. complexity problem compare algorithms different conditions crossing reality-gap problem; case approach cannot mostly rely identiﬁcation part importance modeling highlighted rear removed; back removals especially difﬁcult effective gaits intact robot rely them. results show black-drops gp-mi able learn highly effective walking policies physical hexapod robot particular using dynamics simulator prior information black-drops gp-mi able achieve better walking speeds it&e intact physical hexapod moreover rear-leg removal damage case black-drops gp-mi allows damaged robot walk effectively seconds interaction time pilco used linear policy types dart simulator pilco black-drops could solution preliminary simulation experiments even several minutes interaction time black-drops priors worse black-drops gp-mi. fig. results physical hexapod locomotion task lines median values shaded regions percentiles. improving policy intact robot black-drops gp-mi ﬁnds highly-effective policy less seconds interaction time whereas it&e able substantially improve initial policy. rear-leg removal damage case black-drops gp-mi allows damaged robot walk effectively seconds interaction time ﬁnds higherperforming policies it&e overall black-drops gp-mi able successfully learn working policies even though dimensionality state action space hexapod robot respectively. addition rear damage case black-drops always tried safer policies it&e often executed policies would cause robot fall over. video algorithm running damaged hexapod available supplementary video black-drops gp-mi ﬁrst model-based policy search algorithms efﬁciently learn highdimensional physical robots. able learn walking policies physical hexapod less minute interaction time without prior policy parameters black-box nature approach along extra ﬂexibility tuning black-box prior model opens direction experimentation changing priors robots tasks requires minimum effort. compute long-term predictions requires predicted states back prior simulator. cause simulator crash guarantee predicted state possibly makes sense real world make sense prior model; especially models differ obstacles collisions involved. also holds prior-based methods easily seen simple systems. contrary observed phenomenon times hexapod experiments. using prior simulator reference mixing prior real data direction future work. finally black-drops gp-mi brings closer trialand-error diagnosis-based approaches robot damage recovery. successfully combines diagnosis prior knowledge possible damages/different conditions robot face trial-and-error learning. bischoff nguyen-tuong hoof mchutchon rasmussen knoll peters deisenroth policy search learning robot control using sparse data proc. icra cully clune tarapore j.-b. mouret robots marco berkenkamp hennig schoellig krause schaal trimpe virtual real trading simulations physical experiments reinforcement learning bayesian optimization proc. icra majumdar tedrake funnel libraries real-time robust feedback motion planning ijrr vol. antonova atkeson sample efﬁcient optimization learning controllers bipedal locomotion proc. humanoids vassiliades chatzilygeroudis j.-b. mouret using centroidal voronoi tessellations scale multi-dimensional archive phenotypic elites algorithm ieee trans. evolutionary computation johnson steven nlopt nonlinear-optimization package. kupcsik deisenroth peters vadakkepat neumann model-based contextual policy search data-efﬁcient generalization robot skills artiﬁcial intelligence", "year": 2017}