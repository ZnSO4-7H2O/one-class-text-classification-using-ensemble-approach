{"title": "Gaussian meta-embeddings for efficient scoring of a heavy-tailed PLDA  model", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "Embeddings in machine learning are low-dimensional representations of complex input patterns, with the property that simple geometric operations like Euclidean distances and dot products can be used for classification and comparison tasks. The proposed meta-embeddings are special embeddings that live in more general inner product spaces. They are designed to propagate uncertainty to the final output in speaker recognition and similar applications. The familiar Gaussian PLDA model (GPLDA) can be re-formulated as an extractor for Gaussian meta-embeddings (GMEs), such that likelihood ratio scores are given by Hilbert space inner products between Gaussian likelihood functions. GMEs extracted by the GPLDA model have fixed precisions and do not propagate uncertainty. We show that a generalization to heavy-tailed PLDA gives GMEs with variable precisions, which do propagate uncertainty. Experiments on NIST SRE 2010 and 2016 show that the proposed method applied to i-vectors without length normalization is up to 20% more accurate than GPLDA applied to length-normalized ivectors.", "text": "johns hopkins hltcoe scale workshop ongoing research embeddings speaker recognition inspired generalization meta-embeddings. bulk work meta-embeddings remains unpublished current draft work followed github traditional embeddings interpreted point estimates hidden variables interest typically live low-dimensional euclidean spaces comparisons based ordinary products. metaembeddings likelihood functions hidden variables. typically live inﬁnite-dimensional hilbert function spaces comparisons based generally deﬁned inner products. considerable generalization provides many opportunities also complex challenges theoretical computational. work restrict attention multivariate gaussian likelihood functions required inner products evaluated closed form. future hope apply meta-embeddings speaker recognition similar i-vectors x-vectors sense extracted acoustic feature vectors regard work paper warmexercise proof concept i-vectors rather mfccs input. i-vector inputs proﬁt simple generative models provide elegant closed-form formulas extracting meta-embeddings. speaker recognition already generative discriminative embeddings represent state beyond. additional advantages could expect meta-embeddings? main motivation re-design ground vehicle propagation uncertainty. generative i-vector extractor model provides natural measure uncertainty form i-vector posterior. standard i-vector scoring recipes cosine scoring expected value posterior retained precision discarded. work propagate source uncertainty plda limited success according patrick embeddings machine learning low-dimensional representations complex input patterns property simple geometric operations like euclidean distances products used classiﬁcation comparison tasks. introduce meta-embeddings live general inner product spaces designed better propagate uncertainty embedding bottleneck. traditional embeddings trained maximize between-class minimize within-class distances. meta-embeddings trained maximize relevant information throughput. proof concept speaker recognition derive extractor familiar generative gaussian plda model show gplda likelihood ratio scores given hilbert space inner products gaussian likelihood functions term gaussian meta-embeddings meta-embedding extractors generatively discriminatively trained. gmes extracted gplda ﬁxed precisions propagate uncertainty. show generalization heavy-tailed plda gives gmes variable precisions propagate uncertainty. experiments nist show proposed method applied i-vectors without length normalization accurate gplda applied length-normalized i-vectors. embeddings familiar modern machine learning. neural nets extract word embeddings already proposed bengio embeddings used generally example state-of-the-art face recognition e.g. facenet embeddings becoming popular also speech speaker recognition. interspeech eighteen papers word ‘embedding’ title. speaker recognition spoken language recognition using ivectors—embeddings extracted generative model—for aldecade general embeddings extracted discriminatively trained dnns appearing speaker recognition example google system voxceleb paper jhu’s x-vectors similar embeddings also used spoken language recognition kenny i-vector uncertainty propagation supposed instead gives beneﬁt channel compensator. speculate simplifying modelling assumptions mean-ﬁeld variational bayes approximation factors contributing problem. show below meta-embeddings whether extracted discriminatively generatively designed propagate uncertainty. motivate general pattern recognition context. quantifying uncertainty important pattern recognizer applicable variable sometimes challenging conditions. speaker recognition short noisy narrow-band recording leave much uncertainty speaker long clean wideband recording. face recognition compare well-lit high resolution full-frontal face image grainy resolution partly occluded face. ﬁngerprint recognition compare clean highresolution ten-print single distorted smudged ﬁngermark retrieved crime scene. general terms describe speaker recognition problem partitioning sets recordings according speaker sizes vary entire databases binary trials contain pair recordings. simplicity assume recording single speaker; recordings different speakers independent; recordings given speaker exchangeable. finetti’s theorem exchangeability equivalent concept hidden speaker identity variable familiar speaker recognition thanks work patrick kenny plda meta-embeddings likelihood functions speaker identity variable form traditional embeddings intuitive idea retain output representation much possible relevant information present input metaembeddings idea formalized deﬁnition relevant information speaker must present likelihood function recording. poor likelihood model asserting relevant information won’t good practice probabilistic machine learning task need choose likelihood models wisely ways training parameters models.) denote recordings. paper denote d-dimensional hidden speaker although discriminative x-vector extractor make standard deviations temporal pooling stage uncertainty thus captured propagated subsequent plda scoring backend. arbitrary constant general depend take careful note meta-embedding whole function rather point estimate lives represent different hypotheses might partitioned w.r.t. speaker. partition hypothesized speakers indexed subsets {si}m likewise speakers indexed using within-speaker exchangeability betweenspeaker independence likelihood ratio comparing expressed terms meta-embeddings triangle brackets denote expectation w.r.t. arbitrary scaling constants {kj}n numerator denominator cancel. equation general speaker recognition problem formulated terms partitions expressed purely terms meta-embeddings. illustrates principle likelihoods represent relevant information inputs. look ﬁrst line might conclude recognize speakers principled probabilistic model would always need generative model requires possibly complex probability distributions observed data form keeping mind essentially un-normalized posterior although extract meta-embeddings generative models generative models means required. shows score—and therefore also train—meta-embedding systems purely discriminative ways without requiring complex generative models input data. general case cholesky factorization would standard tool computation gmes extract paper diagonalizable precisions allow much faster computations. follows recording representations ddimensional i-vectors derive metaembedding extractor heavy-tailed plda model. details chapter generative meta-embeddings hidden variable require standard normal prior forms part plda model. plda model says every speaker identity variable sampled independently every i-vector speaker identity variable generated d-by-d factor loading matrix ‘channel’ noise drawn multivariate t-distribution zero mean precision degrees freedom plda model parameters metaembedding model extracts simply shown likelihood ratios form expressed terms primitive operations pooling inner products. since inner products expectations products alternatively primitive operations pooling expectation. given regularity conditions likelihood functions meta-embeddings live hilbert space vector space equipped inner product. although hilbert space typically inﬁnite-dimensional geometry like euclidean space. meta-embedding space norms distances angles well-deﬁned meaningful interpretations practical applications scoring training speaker recognizers. lack space here details practice need primitive operations computationally tractable. paper restrict attention multivariate gaussian likelihood functions. gaussian meta-embedding extracted recording deﬁned represented natural parameters d-by-d positive semi-deﬁnite precision matrix future work envisage discriminatively trained metaembedding extractor would process output sensible representation paper warm-up exercise i-vectors plda model derive relatively simple functions extract course i-vectors x-vectors scored parameterless backend cosine scoring usually less accurate. machine learning e.g. facenet parameterless scoring strategy preferred. philosophy summarized train embedding extractor embeddings face close embeddings different faces apart. brute-force geometric strategy successful appreciated such. however hope additional beneﬁts eventually reaped result probabilistic strategy meta-embedding design. finally worth repeating comparisons meta-embeddings interpreted terms slightly complex geometry terms angles norms. interested reader referred chapter entitled structure meta-embedding space follows denote recordings training database; ‘labels’ true partition database w.r.t. speaker; hidden speaker identity variables generative model parameters variational parameters would needed variational bayes training strategies. training strategies complex probabilistic models perhaps best understood comparison celebrated variational autoencoder described next. training effects approximate maximization marginal likelihood—here classical maximizes lower bound marginal likelihood applicable conjugateexponential models intractable posteriors marginal likelihoods generalizes wider class models using stochastic approximation lower bound parts decoder encoder. decoder generative model likelihood encoder tractable variational posterior approximates true intractable posterior training accomplished maximization stochastic lower bound w.r.t. decoder encoder parameters paper vae. instead used crude shortcut. trained gaussian plda model usual em-algorithm simply plugged hand-selected value make model heavy-tailed. future work however investigate powerful generative training strategy. heavy-tailed plda model section intractable posteriors formed product gaussian prior t-distribution likelihoods. pointed however t-distribution likelihood almost gaussian gaussian variational posteriors expected work well. unfortunately lower bound closed form rather classical would necessary. model variational parameters could tied generative parameters using however allows unconstrained optimization variational parameters could give advantages accuracy computational complexity. since precisions differ scalar simplify pooling expectation. pooling addition precisions simpliﬁes scalar addition. expectations keep cholesky factorizing every time. precomputing eigenanalysis diagonalize i+bj allows fast scoring training ancillary statistic independent speaker variable nevertheless important complete inferences speaker. heavy-tailed noise essentially gaussian noise random variance. ‘bad’ i-vector large noise vector jgrj also large precision scaling factor small. ‘good’ i-vector small noise opposite happens. heavy-tailed plda shown better model i-vectors gaussian plda computational cost considerable. subsequently showed i-vectors could instead gaussianized simple length normalization procedure. matched accuracy heavy-tailed plda negligible extra computational cost. heavy-tailed nature i-vectors also addressed iterative scaling procedure proposed. paper approximation advantage fast closed-form estimate scale factor jgrj. course length normalization would interfere estimate. therefore apply proposed gaussian meta-embeddings i-vectors without length normalization. experiments compare gaussian plda applied i-vectors without length normalization. summary plda model provides functional form extracting gaussian meta-embeddings i-vectors. shall explore generative discriminative methods training parameters extractor. meta-embeddings extracted need additional backend scoring. meta-embeddings contain within everything needed produce scores done general speciﬁcally gmes applied binary trials board switchboard cellphone phases. also nist added training. total contains approximately utterances coming speakers. used randomly selected ﬁles training full train i-vector extractor plda. training extractor training list. however discriminative training stochastic gradient descent split training crossvalidation subsets. cross validation done randomly selected subset speakers leaving training. gave utterances cross validation training. evaluate performance female part nist condition consists english telephone data additionally report results nist evaluation report results whole sre’ also separately language subsets cantonese tagalog. evaluation metrics equal error rate well average minimum detection cost function operating points operating points ones interest nist namely probability target trials mentioned section extractor deﬁned equivalent gaussian plda. case parameters extractor log-likelihood scores computed meta-embeddings equal scores provided gaussian plda. this recall gaussian plda log-likelihood-ratio score i-vectors expressed training might good idea simpler models heavy-tailed plda applied i-vectors. however complex models applied acoustic features discriminative training starts look attractive. training complexity doubling effect—we build complex encoder complex decoder also manage non-trivial interface them—see example training complete decoder longer needed runtime scoring. future work complex models extract metaembeddings acoustic features envisage purely discriminative training methods could provide easier route success. binary cross-entropy applied pairs recordings popular discriminative training criterion speaker recognition indeed also used train meta-embedding extractors paper. scoring formula needed training extractor future work note means option—see variety proposed discriminative training criteria. particular would like highlight computationally attractive pseudolikelihood criterion rely quadratic expansion binary trials. addition pseudolikelihood proper scoring rule training problem stricter sense give advantages calibration-sensitive training evaluation criterion. used -dimensional spectral features mfccs including augmented features short-term mean variance normalized second sliding window. features train diagonal components gender independent fashion. then collect sufﬁcient statistics train i-vector extractor -dimensional i-vectors. i-vectors serve input either plda baseline metaembedding extractor. cases i-vectors transformed global mean normalization. then baseline plda systems also apply length normalization. second plda system applied i-vectors without length normalization. mentioned section used binary cross-entropy scored pairs i-vectors discriminatively train parameters extractor. used initialization gaussian plda described above plugged followed minibatch stochastic gradient descent objective. minibatches formed follows. randomly select sets i-vectors training data serve enrollment test sets. then i-vectors ﬁrst scored second. ﬁlter trials except cases i-vectors scored themselves. expected amount non-target trials batch much higher target trials ratio vary batches. compensate this separately compute target non-target examples current batch normalize individual term correct number targets non-targets respectively. extractor parameters updated backpropagating gradients objective scoring formula extractor formula value remains ﬁxed plugged value throughtraining. training continues objective stops improving held cross-validation set. sre’ evaluation includes trials multiple enrollment recordings. results reported here took shortcut simply averaging enrollment i-vectors. course plda meta-embeddings provide principled enrollment pooling explored future. ﬁrst part table shows results baselines. baseline gaussian plda applied length-normalized i-vectors. baseline same without length normalization. usual length normalization helps lot—it makes data gaussian better gaussian plda model. second part table shows results three conﬁgurations applied i-vectors without length normalization. ﬁrst result sanity check shows equivalence baseline extractor initialized second result previous notice already better cases gaussian plda without length normalization. finally third result shows discriminative training without length normalization better gaussian plda length normalization. effectively relaxed gaussian modelling assumptions. experiments tried several different values found parameter vary wide range values provide similar performance. here picked results indicate training mitigates degradation brought lack length normalization even brings improvements compared baselines cases. experiments starting i-vectors gaussian plda scoring whole sre’ sre’ evaluation sets required respectively wall clock time. solution required roughly double time tasks. implementation kenny’s heavy-tailed plda hand direct comparison know computational complexity thus prevented being widely adopted. paper introduces meta-embeddings intended future alternative i-vectors x-vectors speaker recognition indeed areas machine learning alternative traditional embeddings. chief motivation meta-embeddings build discriminatively trainable recognizers allow principled propagation uncertainty input ﬁnal output. expect advantages noticeable applications varying sometimes challenging quality inputs. full meta-embedding replacement i-vectors x-vectors able demonstrate utility design principles creating i-vector scoring backend accurate long-standing state represented length normalization gaussian plda. sre’ showed relative improvement sre’ absolute improvement. improvement achieved purely replacing backend—without resorting data augmentation fusion domain adaptation score normalization. although results show heavy-tailed plda performs better gaussian plda i-vectors without length normalization remains problem i-vectors extracted recordings short durations. usual i-vector extractor effect standard normal prior shrink short-duration i-vectors towards origin. heavy-tailed plda model breaks cases jgrj decreases extracts meta-embeddings higher precision. opposite want—shorter durations give uncertainty less. inconsistency explained follows. ideal world whole plda model form prior i-vector extractor. however practical forced compromise using simpler standard normal prior instead. conjecture π-vectors might help mitigate problem. π-vectors extracted similarly ivectors without regularizing prior shrink towards zero short durations. another π-vectors point-estimates extracted i-vector likehowever completely satisfactory solution still going point-estimate properly convey uncertainty inherent short-duration recordings. mentioned next goal construct metaembedding extractors work directly acoustic features rather i-vector-like point-estimates. existing extracts x-vectors mfccs followed gaussian plda backend. generalize meta-embedding extractor? solution replace plda backend. replace x-vectors. initialize backend plda did. probably worthwhile learning discriminative training backend already improve accuracy next step jointly optimize extractor backend. encourage uncertainty propagated extractor backend -dimensional complement speaker subspace. variants recipe interface original extractor back-end simpliﬁed ultimately give better solutions difﬁcult initialize. example could replace follows. split d-dimensional x-vector parts rd−d. ν+d−d ν+yy bjxj trainable diagonal matrix positive entries. discriminative training criterion use? experience currently used train kaldi x-vector extractors work training backend implication also joint training stage. existing method pre-train extractor training change criterion. ﬁrst option used paper. successful using train random initialization work plda initialization. look pseudolikelihood criteria proposed worthwhile. coerce suitable regularization penalty) diagonal. would happy assist details. generative heavy-tailed plda model used generate synthetic data known properties. found invaluable experiments explore various discriminative training criteria. work started johns hopkins university hltcoe scale workshop. authors thank workshop organizers inviting attend generous travel funding. themos stafylakis funded european commission program horizon grant agreement work also supported czech ministry interior project drapak google faculty research award program technology agency czech republic project nosici czech ministry education youth sports national programme sustainability project itinnovations excellence science dehak dehak kenny br¨ummer ouellet dumouchel support vector machines versus fast scoring low-dimensional total variability space speaker veriﬁcation interspeech brighton september snyder ghahremani povey garcia-romero carmiel khudanpur deep neural networkbased speaker embeddings end-to-end speaker veriﬁcation ieee workshop spoken language technology mclaren castan nandwana ferrer yilmaz train speaker embeddings extractor odyssey speaker language recognition workshop sables d’olonne submitted. villalba br¨ummer dehak end-to-end versus embedding neural networks language recognition mismatched conditions odyssey speaker language recognition workshop sables d’olonne submitted. snyder garcia-romero mccree sell povey khudanpur spoken language recognition using x-vectors odyssey speaker language recognition workshop sables d’olonne submitted. br¨ummer burget garcia plchot rohdin romero snyder stafylakis swart villalba meta-embeddings probabilistic generalization embeddings machine learning progress. draft available github.com/bsxfan/meta-embeddings kenny bayesian speaker veriﬁcation heavy-tailed priors odyssey speaker language recognition workshop brno czech republic june keynote presentation. kenny stafylakis alam gupta kockmann uncertainty modeling without subspace methods text-dependent speaker recognition speaker odyssey speaker language recognition workshop bilbao independence interchangeability martingales ser. springer texts statistics. york springer kenny joint factor analysis speaker session variability theory algorithms crim montreal tech. rep. crim-/- cernocky glembek gr´ezl karaf´ıat leeuwen matˇejka schwarz strasheim fusion heterogeneous speaker recognition systems stbu submission nist speaker recognition evaluation ieee taslp vol. september burget plchot cumani glembek matˇejka br¨ummer discriminatively trained probabilistic linear discriminant analysis speaker veriﬁcation icassp prague br¨ummer swart leeuwen comparison linear non-linear calibrations speaker recognition odyssey speaker language recognition workshop joensuu available http//arxiv.org/abs/. cumani br¨ummer burget laface plchot vasilakakis pairwise discriminative speaker veriﬁcation i-vector space ieee transactions audio speech language processing vol. ferrer bratt burget cernocky glembek graciarena lawson matejka plchot promoting robustness speaker modeling community prism evaluation https//code.google.com/p/prism-set/", "year": 2018}