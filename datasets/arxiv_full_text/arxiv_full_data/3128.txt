{"title": "Multi-stage Multi-task feature learning via adaptive threshold", "tag": ["cs.LG", "cs.CV", "stat.ML", "68T10", "F.2.2"], "abstract": "Multi-task feature learning aims to identity the shared features among tasks to improve generalization. It has been shown that by minimizing non-convex learning models, a better solution than the convex alternatives can be obtained. Therefore, a non-convex model based on the capped-$\\ell_{1},\\ell_{1}$ regularization was proposed in \\cite{Gong2013}, and a corresponding efficient multi-stage multi-task feature learning algorithm (MSMTFL) was presented. However, this algorithm harnesses a prescribed fixed threshold in the definition of the capped-$\\ell_{1},\\ell_{1}$ regularization and the lack of adaptivity might result in suboptimal performance. In this paper we propose to employ an adaptive threshold in the capped-$\\ell_{1},\\ell_{1}$ regularized formulation, where the corresponding variant of MSMTFL will incorporate an additional component to adaptively determine the threshold value. This variant is expected to achieve a better feature selection performance over the original MSMTFL algorithm. In particular, the embedded adaptive threshold component comes from our previously proposed iterative support detection (ISD) method \\cite{Wang2010}. Empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of this new variant over the original MSMTFL.", "text": "multi-task feature learning aims identity shared features among tasks improve generalization. shown minimizing non-convex learning models better solution convex alternatives obtained. therefore non-convex model based capped-ℓ regularization proposed corresponding eﬃcient multi-stage multi-task feature learning algorithm presented. however algorithm harnesses prescribed ﬁxed threshold deﬁnition capped-ℓ regularization lack adaptivity might result suboptimal performance. paper propose employ adaptive threshold capped-ℓ regularized formulation corresponding variant msmtfl incorporate additional component adaptively determine threshold value. variant expected achieve better feature selection performance original msmtfl algorithm. particular embedded adaptive threshold component comes previously proposed iterative support detection method empirical studies synthetic real-world data sets demonstrate eﬀectiveness variant original msmtfl. *correspondence yilun.wangrice.edu school mathematical sciences university electronic science technology china xiyuan chengdu china full list author information available article restrictive suboptimal. order remedy shortcomings speciﬁc non-convex formulation based capped-ℓ regularized formulation multi-task sparse feature learning proposed corresponding notation scalars vectors denoted lower case letters. matrices sets denoted capital letters. euclidean norm norm norm norm norm frobenius norm denoted respectively. denote i-th column j-th matrix respectively. denotes absolute value scalar number elements set. deﬁne gaussian distribution mean standard deviation tasks share features i.e. columns diﬀerent represent features. purpose feature selection learn weight matrix rd×m consisting weight vectors linear predictors xiwi quality prediction measured loss function assume convex throughout paper. however restrictive real-world applications require relevant features shared tasks. order certain feature shared tasks many eﬀorts made. proposed regularized formulation leverage common features shared among tasks. however regularizer convex relaxation ℓ-type example convex regularizer known loose approximate ℓ-type often achieves suboptimal performance. regularization ||pq non-convex expects achieve better solution theoretically. however diﬃcult solve practice. moreover solution non-convex formulation heavily depends speciﬁc optimization algorithms employed result diﬀerent solutions. non-convex formulation based capped-ℓ regularized model multi-task feature learning proposed corresponding multi-stage multi-task feature learning algorithm also given. capped-ℓ regularizaj= min. proposed model aims simultaneously learn features speciﬁc task common features shared among tasks. advantage msmtfl convergence reproducibility analysis theoretical analysis better performance convex models given. addition capped-ℓ regularization good approximation thresholding parameter distinguishes nonzeros zero components; j-th matrix obviously capped-ℓ penalty optimal solution problem denoted many zero rows. penalty entries nonzero zero. therefore certain feature shared necessarily tasks formulation multi-stage multi-task feature learning algorithm based work proposed solve problem note ﬁrst step msmtfl msmtfl algorithm equivalent solving regularized multi-task feature learning model therefore ﬁnal solution msmtfl algorithm considered reﬁnement lasso’s mtl. although msmtfl globally optimal solution theoretically shown solution obtained algorithm improved performance parameter estimation error bound multi-stage iteration proceeds certain circumstances. details intuitive interpretations parameter settings convergence analysis msmtfl algorithm provided addition need point thresholding parameter proposed formulation algorithm paper would like present study capped-ℓ regularization based multi-task feature learning model corresponding msmtfl still quadratic loss function deﬁned above; parameter balancing quadratic loss regularization; kwjk. diﬀerence models threshold parameter. model data dependent assumed unknown rather prescribed known value. seems natural idea determination trivial present eﬃcient adaptive method based ﬁrst signiﬁcant jump rule estimate details ﬁrst signiﬁcant borrowed idea analysis iterative reweighted algorithm based non-convex compressive sensing extended common sparsity regularization joint sparsity regularized considered paper. note non-convex formulations based capped-ℓ regularization good approximation regularization small θ)/θ close ||t|| indeed practice algorithm usually decreases gradually iteration proceeds though necessarily always monotonically inherent estimation achieve better performance msmtfl adaptive relatively large beginning msmtfl-at results undesirable local minima being ﬁlled correct basin decreasing allows basin deepen expected approach true closely. expect turn notions rigorous proof future work. addition suppose limℓ→∞ exists denote intuitively expect msmtfl-at converges critical point problem rigorous proof also constitute important future research topic. original msmtfl. theorem proves optimization problem msmtfl unique solution rni×d entries drawn continuous probability distribution rnid. considering problem msmtfl-at formula msmtfl solution msmtfl-at also unique main diﬀerence msmtfl-at msmtfl step unique adaptive threshold msmtfl-at could many diﬀerent rules choice based locating ﬁrst signiﬁcant jump increasingly sorted sequence simplicity sorting still notation denotes j-th smallest among rule looks smallest amounts sweeping increasing sequence jump larger rule proved capable detecting many true nonzeros false alarms sequence fast-decaying property addition several simple heuristic methods adopted deﬁne diﬀerent kinds data matrix suggested paper adopt following method following synthetic real number samples. excessively large results penalizing many true nonzeros excessively small results ignoring many false nonzeros leads quality solution. msmtflquite eﬀective appropriate though proper range might case-dependent free lunch theorem nonconvex optimization learning however numerical experiments shown practical performance msmtfl-at less sensitive choice addition need point tuning parameter parameter typically decreases large value small value detect intuitively ﬁrst signiﬁcant jump rule works well partly true nonzeros large magnitude small number false ones large number small magnitude. therefore magnitudes true nonzeros spread false ones clustered. phenomenon heuristic rule still practical importance theoretically rigorous threshold value keeping control false detections still challenging task statistics matrix randomly rows zero vectors elements remaining nonzero entries zeros. generate data matrix gaussian distribution noise sampled i.i.d gaussian distribution responses computed parameter estimation error deﬁned figure corresponds true weight matrix corresponds learning weight matrix ℓ-th iteration. subgraphs plot respectively comparison subgraph clear represents solution lasso-like model contains large number false nonzeros large recovery error expected. iteration proceeds intermediate learning result becomes accurate. example smaller error true nonzeros large magnitude correctly identiﬁed. then well matches true learning matrix even better tiny number false nonzero components. finally exactly nonzero components true error quite small. short figure shows proposed algorithm insensitive small false number attractive self-correction capacity. terms smaller recovery errors. compare msmtfl-at competing multi-task feature learning algorithms msmtfl ℓ-norm multitask feature learning algorithm ℓ-norm multi-task feature learning algorithm eﬃcient ℓ-norm multi-task feature learning algorithm robust multi-task feature learning algorithm above denote number tasks task samples. number features denoted element data matrix rn×d i-th task sampled i.i.d. gaussian distribution entry true weight rd×m sampled i.i.d. uniform distribution deﬁned interval randomly rows zero vectors elements remaining nonzero entries zeros. entry noise sampled i.i.d. gaussian distribution responses computed quality recovered weight matrix measured averaged parameter estimation error theoretical bound referring present averaged parameter estimation error stage comparison msmtfl msmtfl-at figure clear tled value parameter certain degree depict averaged parameter estimation error corresponding diﬀerent values. default value based heuristic formula also tried values figure present averaged parameter estimation error lambda comparison msmtfl diﬀerent msmtfl-at ℓ-norm multi-task feature learning algorithm ℓ-norm multi-task feature learning algorithm settings parameters involved alternative alorder demonstrate advantage msmtfl-at compared eﬃcient ℓ-norm multi-task feature learning algorithm proposed robust multi-task feature learning algorithm proposed eﬃcient-l solves ℓ-norm regularized regression model nesterov’s method. rmtfl employs accelerated gradient descent shows robustness noise. exceeds certain degree sparsity regularization weighs much solutions obtained involved algorithms sparse. cases errors tested algorithms experiments letter labels treated regression values isolet sets. randomly extract training samples task diﬀerent training ratios rest samples form test set. evaluate four multi-task feature learning algorithms according normalized mean squared error averaged means squared error whose deﬁnitions follows experimental results shown figure proposed msmtfl-at superior eﬃcient-l rmtfl msmtfl algorithms terms achieving smallest nmse amse. msmtfl-at performs especially well even case small training ratio. experimental results suggest proposed msmtfl-at algorithm promising approach. conclusions paper proposes non-convex multi-task feature learning formulation adaptive threshold parameter introduces corresponding msmtfl-at algorithm. msmtfl-at combination adaptive threshold learning ﬁrst signiﬁcant jump rule proposed msmtfl algorithm proposed intuition reﬁne estimated threshold using intermediate solutions obtained recent stage. alternative procedure threshold value estimation feature learning leads gradually appropriate threshold value gradually improved solution. experimental results synthetic data real-world data demonstrate eﬀectiveness proposed msmtfl-at comparison several state-of-the-art multi-task feature learning algorithms. future give theoretical analysis convergence msmtfl-at. addition expect perform rigorous analysis msmtfl-at could achieve better practical performance original msmtfl. acknowledgements work supported national basic research program china natural science foundation china fundamental research funds central universities. would also like thank anonymous reviewers many constructive suggestions greatly improved paper. author details school mathematical sciences university electronic science technology china xiyuan chengdu china. center information biomedicine university electronic science technology china xiyuan chengdu china. center applied mathematics cornell university xiyuan chengdu china.", "year": 2014}