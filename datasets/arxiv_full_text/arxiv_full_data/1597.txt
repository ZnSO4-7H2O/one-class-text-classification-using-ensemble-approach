{"title": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with  Weak Supervision (Short Version)", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Extending the success of deep neural networks to natural language understanding and symbolic reasoning requires complex operations and external memory. Recent neural program induction approaches have attempted to address this problem, but are typically limited to differentiable memory, and consequently cannot scale beyond small synthetic tasks. In this work, we propose the Manager-Programmer-Computer framework, which integrates neural networks with non-differentiable memory to support abstract, scalable and precise operations through a friendly neural computer interface. Specifically, we introduce a Neural Symbolic Machine, which contains a sequence-to-sequence neural \"programmer\", and a non-differentiable \"computer\" that is a Lisp interpreter with code assist. To successfully apply REINFORCE for training, we augment it with approximate gold programs found by an iterative maximum likelihood training process. NSM is able to learn a semantic parser from weak supervision over a large knowledge base. It achieves new state-of-the-art performance on WebQuestionsSP, a challenging semantic parsing dataset, with weak supervision. Compared to previous approaches, NSM is end-to-end, therefore does not rely on feature engineering or domain specific knowledge.", "text": "extending success deep neural networks natural language understanding symbolic reasoning requires complex operations external memory. recent neural program induction approaches attempted address problem typically limited differentiable memory consequently cannot scale beyond small synthetic tasks. work propose manager-programmercomputer framework integrates neural networks non-differentiable memory support abstract scalable precise operations friendly neural computer interface. speciﬁcally introduce neural symbolic machine contains sequence-to-sequence neural \"programmer\" nondifferentiable \"computer\" lisp interpreter code assist. successfully apply reinforce training augment approximate gold programs found iterative maximum likelihood training process. able learn semantic parser weak supervision large knowledge base. achieves state-of-the-art performance webquestionssp challenging semantic parsing dataset. compared previous approaches end-to-end therefore rely feature engineering domain speciﬁc knowledge. deep neural networks achieved impressive performance classiﬁcation structured prediction tasks full supervision speech recognition machine translation extending success natural language understanding symbolic reasoning requires ability perform complex operations make external memory. several recent attempts address problem neural program induction learn programs using neural sequence model control computation component. however memories models either low-level differentiable trained backpropagation. makes difﬁcult utilize efﬁcient discrete memory traditional computer limits application small synthetic tasks. \"manager\" provides weak supervision input reward signal indicating well task performed. unlike full supervision weak supervision much easier obtain large scale \"programmer\" takes natural language input generates program sequence tokens. programmer learns reward signal must overcome hard search problem ﬁnding good programs. \"computer\" executes program. operations implemented function high level programming language like lisp. non-differentiable memory enables abstract scalable precise operations requires reinforcement learning. also provides friendly neural computer interface help \"programmer\" reduce search space detecting eliminating invalid choices within framework introduce neural symbolic machine apply semantic parsing. contains sequence-to-sequence neural network model augmented key-variable memory save reuse intermediate results compositionality non-differentiable lisp interpreter executes programs large knowledge base. code assist \"computer\" also helps reduce search space checking syntax semantic errors. compared existing neural program induction approaches efﬁcient memory friendly interface \"computer\" greatly reduce burden \"programmer\" enable model perform competitively real applications. challenging semantic parsing dataset webquestionssp achieves state-of-the-art results weak supervision. compared previous work end-to-end therefore require feature engineering domain-speciﬁc knowledge. describe details neural symbolic machine falls framework applied learn semantic parsing weak supervision. semantic parsing deﬁned follows given knowledge base question produce program logical form executed generates right answer denote entities denote properties knowledge base assertions operations learned current neural network models differentiable memory addition sorting generalize perfectly inputs larger previously observed ones contrast operations implemented ordinary programming language abstract scalable precise matter large input whether seen processed precisely. based observation implement operations necessary semantic parsing ordinary non-differentiable memory allow \"programmer\" high level general purpose programming language. adopt lisp interpreter predeﬁned functions listed \"computer\". programs executed equivalent limited subset λ-calculus easier sequence-to-sequence model generate given lisp’s simple syntax. lisp generalpurpose high level language easy extend model operations implemented functions complex constructs like control ﬂows loops. program list expressions expression either special token \"return\" indicating program list tokens enclosed parentheses functions table take input list arguments speciﬁc types executed returns denotation expression typically list entities saves variable. argument either relation variable variables hold results previous computations either list entities executing expression entity resolved natural language input. create better neural computer interface interpreter provides code assist producing list valid tokens \"programmer\" pick step. first valid token cause syntax error usually checked modern compilers. example previous token stores values variables simpliﬁes task \"programmer\". \"programmer\" needs natural language program sequence tokens references operations values \"computer\". standard sequence-to-sequence model attention augment key-variable memory reference values. typical sequence-to-sequence model consists rnns encoder decoder. used -layer simpliﬁed variant lstm encoder decoder. given sequence words w...wm word mapped multi-dimensional embedding then encoder reads embeddings updates hidden state step step using θencoder parameters. decoder updates hidden states embedding last step’s output token θdecoder parameters. last hidden state encoder used decoder’s initial state. adopt dot-product attention similar tokens program a...an generated using softmax vocabulary valid tokens step figure semantic parsing nsm. embeddings key-variable memory output sequence model certain encoding decoding steps. illustration purposes also show values variables parentheses sequence model never sees values references name variables special token indicates start decoding return indicates decoding. corresponding variable references certain result \"computer\". encoding token last token resolved entity resolved entity saved variable \"computer\" embedding variable average output tokens spanned entity. decoding expression completely ﬁnished gets executed result stored value variable \"computer\". variable keyed output step. every time variable pushed memory variable token added vocabulary decoder. efﬁciently train weak supervision apply reinforce algorithm however reinforce objective known hard optimize starting scratch. therefore augment approximate gold programs found iterative maximum likelihood training process. training model always puts reasonable amount probability best programs found anchoring model high-reward programs greatly speeds training helps avoid local optimum. details training procedure found long version. modern semantic parsers natural language utterances executable logical forms successfully trained large knowledge bases weak supervision require substantial feature engineering. recent attempts train end-to-end neural network semantic parsing either used strong supervision employed synthetic datasets. apply learn semantic parser weak supervision manual engineering. used challenging semantic parsing dataset webquestionssp consists question-answer pairs training testing. questions collected using google suggest answers originally obtained using amazon mechanical turk updated annotators familiar design freebase separate questions training validation set. query pre-prosessing used in-house named entity linking system entities question. quality entity resolution similar gold root entities included resolution results. similar also replaced named entity tokens special token \"ent\". example question \"who plays family guy\" changed \"who plays ent\". following last public available snapshot freebase since training requires random access freebase decoding preprocess freebase removing predicates related world knowledge removing text valued predicates rarely answer. results graph relations nodes edges. evaluate performance using ofﬁcal measures webquestionssp. answer question contain multiple entities values precision recall computed based output individual question. average score reported main evaluation metric. accuracy measures percentage questions answered exactly. comparison previous state-of-the-art shown table besides better performance model rely domain-speciﬁc rules feature engineering. ablation studies analysis included long version. table comparison previous state-of-the-art average main evaluation metric. model achieves better results without hand-crafted rules feature engineering.", "year": 2016}