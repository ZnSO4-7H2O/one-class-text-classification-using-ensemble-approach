{"title": "Customer Lifetime Value Prediction Using Embeddings", "tag": ["cs.LG", "cs.CY", "cs.IR", "cs.NE", "stat.ML"], "abstract": "We describe the Customer LifeTime Value (CLTV) prediction system deployed at ASOS.com, a global online fashion retailer. CLTV prediction is an important problem in e-commerce where an accurate estimate of future value allows retailers to effectively allocate marketing spend, identify and nurture high value customers and mitigate exposure to losses. The system at ASOS provides daily estimates of the future value of every customer and is one of the cornerstones of the personalised shopping experience. The state of the art in this domain uses large numbers of handcrafted features and ensemble regressors to forecast value, predict churn and evaluate customer loyalty. Recently, domains including language, vision and speech have shown dramatic advances by replacing handcrafted features with features that are learned automatically from data. We detail the system deployed at ASOS and show that learning feature representations is a promising extension to the state of the art in CLTV modelling. We propose a novel way to generate embeddings of customers, which addresses the issue of the ever changing product catalogue and obtain a significant improvement over an exhaustive set of handcrafted features.", "text": "introduction asos global e-commerce company based specialising fashion beauty. business entirely online products sold eight country-specic websites mobile apps. time writing million active customers product catalogue contained items. products shipped countries territories annual revenue making asos europe’s largest pure play online retailers. integral element business model asos free delivery returns. free shipping vital online clothing retail customers need items without charged. since asos recoup delivery costs returned items customers easily negative lifetime value. reason customer lifetime value problem particularly important online clothing retail. ctlv system addresses tightly coupled problems cltv churn prediction. dene customer churned placed order past year. dene cltv sales returns customer year period. objective improve three business metrics average customer shopping frequency average order size customer churn rate. model supports objectives allowing asos rapidly identify nurture high-value customers high frequency high-order size both. third objective achieved identifying customers high risk churn controlling amount spent retention activities. state-of-the-art cltv systems large numbers handcraed features ensemble classiers shown perform well highly stochastic problems kind however handcraed features introduce human boleneck dicult maintain fail utilise full richness data. show automatically learned features combined handcraed features produce model aware domain knowledge learn rich paerns customer behaviour data. deployed asos cltv system uses state-of-the-art architecture random forest regression model handcraed features. train forest labels features taken disjoint periods time shown figure labels spend customer past year. training labels features abstract describe customer lifetime value prediction system deployed asos.com global online fashion retailer. cltv prediction important problem e-commerce accurate estimate future value allows retailers eectively allocate marketing spend identify nurture high value customers mitigate exposure losses. system asos provides daily estimates future value every customer cornerstones personalised shopping experience. state domain uses large numbers handcraed features ensemble regressors forecast value predict churn evaluate customer loyalty. recently domains including language vision speech shown dramatic advances replacing handcraed features features learned automatically data. detail system deployed asos show learning feature representations promising extension state cltv modelling. propose novel generate embeddings customers addresses issue ever changing product catalogue obtain signicant improvement exhaustive handcraed features. concepts computing methodologies →supervised learning regression; dimensionality reduction manifold learning; classication regression trees; neural networks; supervised learning classication; applied computing →forecasting; marketing; e-commerce infrastructure; online shopping; consumer products; permission make digital hard copies part work personal classroom granted without provided copies made distributed prot commercial advantage copies bear notice full citation page. copyrights components work owned others author must honored. abstracting credit permied. copy otherwise republish post servers redistribute lists requires prior specic permission and/or fee. request permissions permissionsacm.org. kdd’ august halifax canada. copyright held owner/author. publication rights licensed acm. ----//.... hp//dx.doi.org/./. figure training prediction time-scales cltv. model retrained every using customer data past years. labels customer spend previous year. model parameters learned training period used predict cltv features live system. used learn parameters second figure shows live system parameters training period applied features generated last year’s data produce prediction customer spend next year. provide detailed explanation challenges lessons learned deploying asos cltv system describe latest eorts improve system augmenting learned features. approach inspired recent successes representation learning domains vision speech language. experimented learning representations directly data using dierent approaches training feedforward neural network handcraed features supervised seing augmenting feature unsupervised customer embeddings learnt browsing data novel customer embeddings shown improve cltv prediction performance signicantly compared benchmark. incorporating embeddings long-term prediction models challenging because unlike handcraed features features easily identiable. figure illustrates point using four dimensional embedding. column gure corresponds dierent dimension embeddings space also feature random forest model. training period learn parameters feature. however features labelled order randomly permutes training test time possible training parameters test features. describe problem solved within neural embedding framework using form warm start test period embeddings. figure illustration challenges using components embedded customer vectors features forecasting. column represents component vector representation customers. labelled columns parameters learnt training time. vector components randomly permute train testing time hence require dierent learned parameters. applying learned parameters training time directly embeddings test time work longer attached correct component embedded vectors. related work statistical models customer purchasing behaviour studied decades. early models hampered lack data restricted simple parametric statistical models negative binomial distribution model turn century advent large-scale e-commerce platforms problem began aract aention machine learning researchers. distribution fitting approaches statistical models cltv known ’til models. btyd models place separate parametric distributions customer lifetime purchase frequency require customer recency frequency inputs. models pareto/nbd model assumes exponentially distributed active duration poisson distributed purchase frequency customer. yields pareto-ii customer lifetime negative-binomial distributed purchase frequency. fader replaced pareto-ii betageometric distribution easier implementation assuming customer certain probability become inactive every purchase. bemmaor proposed gamma/gompertz distribution model customer active duration exible pareto-ii non-zero mode skewed directions. recency-frequency-monetary value models expand btyd including additional feature. fader linked model captures time last purchase number purchases purchase values customer cltv estimation. model assumes pareto/nbd model recency frequency purchase values following independent gamma/gamma distribution. successful problems applied dicult incorporate vast majority customer data available modern e-commerce companies rfm/bytd framework. particularly dicult incorporate automatically learned highly sparse features. motivates need machine learning approaches problem. machine learning methods related work vanderveld work explicitly include customer engagement features cltv prediction model. also address challenges learning complex non-linear cltv distribution solving several simpler sub-problems. firstly binary classier identify customers predicted cltv greater zero. secondly customers predicted shop split groups independent regressors trained group. best knowledge deep neural networks successfully applied cltv problem. however work related churn problem wangperawong telecommunications. authors create two-dimensional array customer columns days year rows describe dierent kinds communication used array train deep convolutional neural networks. model also used auto-encoders learn low-dimensional representations customer arrays. neural embeddings neural embeddings technique pioneered natural language processing learning distributed representations words provide alternative one-of-k representation. unlike one-of-k representation uses large sparse orthogonal vectors embeddings compact dense representations encapsulate similarity. embedded representations shown give superior performance sparse representations several downstream tasks language graph analysis embedding methods dene context groups objects. typically data sequence context xed-length window passed popular embedding models skipgram negative sampling developed mikolov found broad range applications describe relevant work. barkan koenigstein used sgns item-level embeddings item-based collaborative ltering called itemvec. context authors basket items purchased together sgns used generate product embeddings authors called prodvec mining receipts email data. customer sequence products built context window goal predict products co-purchased single customer within given number purchases. authors also proposed hierarchical extension called bagged-prodvec groups products appearing together single email. used variant sgns predict next mobile phone user would open. idea consider sequences usage within mobile sessions. data associated time stamps authors used modify original model. instead including every pairwise apps within context selection probability controlled gaussian sampling scheme based inter-open duration. customer lifetime value model asos cltv model uses rich features predict spend customers next months training random forest regressor historic data. major challenges predicting cltv unusual distribution target variable. large percentage customers cltv zero. customers greater zero cltv values dier several orders magnitude. manage problem explicitly model cltv percentiles using random forest regressor. predicted percentiles outputs mapped back real value ranges downstream tasks. features model incorporates features full spectrum customer information available asos. four broad classes data customer demographics purchase history returns history session logs. largest richest classes session logs. apply random forest feature importance rank handcraed features. table shows feature importance breakbroad classes data table shows features. expected number orders number sessions last quarter nationality customer important features cltv prediction. however surprised importance standard deviation order session feature name number orders standard deviation order dates number session last quarter country number items collection number items kept sales days last session number sessions customer tenure total number items ordered days since last order days since last session standard deviation session dates orders last quarter average date order total ordered value number products viewed days since order last year average session date number sessions previous quarter dates particularly maximum spans variables also features. also expect number items purchased collection relevant features. newness major consideration high value fashion customers. architecture high-level system architecture shown figure customer data pre-processed data warehouse stored microso azure blob storage. blob storage data work streams generate customer features handcraed feature generator apache spark experimental customer embedding generator machines running tensorow uses web/app sessions input. model trained stages using apache spark pipeline. stage pre-processes features trains random forests churn classication cltv regression percentiles. second stage performs calibration maps percentiles real values. finally predictions presented figure high-level overview cltv system. solid arrows represent data dashed arrows represents interaction stakeholders systems/data. customer data collected pre-processed data warehouse stored microso azure blob storage. processed data used generate handcraed features spark clusters web/app sessions additionally used produce experimental customer embeddings tensorow. handcraed features customer embeddings machine learning pipeline spark trains calibrated random forests churn classication cltv regression. resulting prediction piped operational systems. training evaluation process live system uses calibrated random forest model features past twelve months re-trained every day. twelve months strong seasonality eects retail failing exact number years would cause uctuations train model historic sales last year proxy cltv labels learn random forest parameters using features generated disjoint period prior label period. illustrated figure every generate feature label periods disjoint prevent information leakage. predictive accuracy live system could evaluated year’s time establish expectation performance model forecasting points past already know actual values illustrated figure area receiver operating characteristic curve performance measure. calibration context calibration refers eorts ensure statistics model predictions consistent statistics data. model predictions derived leaf distributions perform calibration churn cltv prediction. customer churn prediction choosing classier parameters maximise guarantee predictive probabilities consistent realised churn rate generate consistent probabilities calibrate learning mapping estimates realised probabilities. done training one-dimensional logistic regression classier predict churn based probabilities returned random forest. logistic regression output interpreted calibrated probability. similarly estimate cltv guarantees regression estimates achieved minimizing root mean squared error loss function match realised cltv distribution. address problem analogously churn probability calibration forecast cltv percentile predicted percentiles monetary values. case mapping learnt using decision tree. observe advantages performing calibration model becomes robust existence outliers obtain predictions aggregated customers match true values accurately. results optimal meta-parameters -fold cross validation sample data. cltv predictions obtain spearman rank-order correlation coecients figure range density predicted cltv matches actual cltv churn predictions obtain calibrated probabilities. figure predicted cltv actual cltv units average cltv value gbp. distribution prediction actual cltv similar scale central plot shows predictions actual values spearman rank-order correlation coecient figure churn prediction density match predicted probabilities actual probabilities versus optimal calibration predicted probabilities match closely actual probabilities remainder paper describe ongoing eorts supplement handcraed features deployed system automatic feature learning. feature learning process learning features directly data maximise objective function classication regression task. technique frequently used within realms deep learning dimensionality reduction overcome limitations engineered features. despite dicult interpret learnt features avoid resource intensive task constructing features manually data shown outperform best handcraed features domains speech vision natural language. experiment distinct approaches. firstly learn unsupervised neural embeddings using customer product views. learnt embeddings added feature model. secondly train hybrid model combines logistic regression deep neural network uses handcraed features learn higher order feature representations. learn embeddings asos customers using neural embedding models borrowed nlp. re-purpose replace sequences words sequences customers viewing product previous work looked embedding products based sequences customer interactions possible aggregate product embeddings produce customer embedding. however approach fails task producing long-term forecasts products relatively short live reason learn embeddings customers directly. intuitively high-value customers tend browse products higher value less popular products products lowest price market. contrast lowervalue customers tend appear together product sequences sales periods products priced market. information dicult incorporate model using hand-craed features number sequences product views grows combinatorially. figure shows neural architecture customer embedding model. model large weight matrices wout learn distributed representations customers. output model training vector representation customer embedding space. inputs model pairs customers loss function probability observing output customer cout given figure neural network architecture matrix representation network’s input/output weights skipgram model customer embeddings. skipgram model uses neural network hidden layer. customers represented one-hot vectors input output layers weights input hidden layers represented randomly initialised customer input embedding matrix matrix represents customer embedding lighter cells representing negative value darker cells representing positive value. output somax layer figure unit every customer must evaluated training pair. would prohibitively expensive approximately million customers. however approximated using skipgram negative sampling evaluating small random selection total customers training step applying sgns requires three design decisions context usually sliding window word sequences within document. word centre window input word pairs formed every word context. negative samples drawn figure customer pair generation skip-gram model based product views. represents product sold asos sequence customer views product. example context window length considers adjacent view events product. hence exact time product viewed ignored. customers appear context window close embedding representation. figure shows dene context generate customer pairs. product catalogue associated sequence customer views. sliding context window passed sequence customers. every position context window central customer used customers window used form pairs. window length three containing would generate customer pairs empirically found window length worked well. embedding algorithm begins randomly initialising wout. customer pair embedded representations out) negative customer samples cneg drawn. forward pass rows wout updated gradient descent using backpropagation figure upli area receiver operating characteristics curve achieved random test sets customers product view-based embeddings number neurons hidden layer skipgram model. error bars represent condence interval sample mean. figure shows obtained signicant upli using embeddings customers. experimented range embedding dimensions found best performance range dimensions. result shows approach highly relevant working incorporate technique live system point writing paper. embeddings live system. embeddings deployed system necessary make correspondence embedding dimensions training period live system’s feature generation period. figure shows features training cltv model features used live system come disjoint time periods. embedding dimensions unlabelled randomly initialised exchangeable sgns loss function parameters learned training period assumed match embeddings used live system. solve problem instead randomly initialising wout perform following initialisation customers present training period customers initialise uniform random values absolute values small compared training embeddings. live system four types customer pairs and. equation shows update linear combination vout negative vectors. erefore single update pair guaranteed linear combination embedding vectors training period. generate embeddings consistent across time periods order data training epoch update representation customers initialise customers linear combinations embeddings customers scheme work must large proportion customers present training test periods. true customer embeddings true product embeddings. requirement explains chose learn customer embeddings directly instead using aggregations product embeddings. embeddings handcraed features also investigate extent deployed system could improved replacing deep neural network motivated recent successes dnns vision speech recognition recommendation systems results indicate incorporating dnns system improve performance monetary cost training model outweighs performance benets. limit experiments dnns churn problem reduces cltv regression problem binary classication problem interpretable predictions metrics performance. experiment deep feed-forward neural networks hybrid models combining logistic regression deep feedforward neural network similar used deep feedforward neural networks accept continuous-valued features dense embeddings categorical features described section inputs. rectied linear units activations hidden units sigmoid activation output unit. hybrid models logistic regression models incorporating deep feed-forward neural network. output neural network’s hidden layer used alongside continuous-valued features sparse categorical features input. akin skip connections neural networks described inputs connected directly output instead next hidden layer. training neural network part models done mini-batch stochastic gradient descent adagrad optimiser change weights backpropagated layers network. regularisation applied logistic regression part hybrid models ftrl-proximal algorithm described mcmahan evaluate performance scalability models dierent architectures compare machine learning techniques. experimented neural networks three four hidden layers dierent combinations number neurons. neuron architecture train models using subset customer data record maximum achieved evaluating separate subset customer data time taken complete pre-specied number training steps. repeat training/testing multiple times architecture obtain estimate maximum achieved training time. training/testing implemented using tensorflow library tesla machine. introducing bypass connections hybrid models improves predictive performance compared deep feed-forward neural network architecture. figure shows comparison maximum achieved dnns hybrid logistic models test customers. experiments show statistically signicant upli least every figure maximum area receiver operating characteristics curve achieved test customers deep feed-forward neural networks hybrid models dierent numbers hidden layer neurons. error bars represent condence interval sample mean. number hidden-layer neurons recorded following format denotes neural network neurons second hidden layer respectively denotes neural network neurons second third hidden layers. conguration neurons experimented with. believe upli hybrid models’ ability memorize relationship customer aributes churn status logistic regression part. complementary deep neural network’s ability generalise customers based customers similar aggregation features described cheng tried estimate size hybrid model required outperform model. figure shows linear relationship maximum achieved test customers number neurons hidden layer logarithmic scale. notice hybrid model small number neurons hidden layer already gives statistically signicant improvement maximum achieved compared vanilla logistic regression within range experiments could exceed performance model. shaded regions dashed lines figure provide three estimates number neurons would required outperform model. experiments suggest possible hybrid model incorporates deep neural network outperform calibrated model churn classication believe monetary cost required perform training outweighs benet gain performance. figure shows relationship monetary cost train hybrid models number neurons figure maximum area receiver operating characterics curve achieved test customers hybrid models number neurons hidden layers consider hybrid models hidden layers number neurons. error bars represent condence interval sample mean. bottom horizontal line represent maximum achieved vanilla logistic regression model random forest model customers. dashed lines shaded region represent dierent forecast scenarios larger architectures. hidden layer. cost training vanilla model calibrated model indicated horizontal lines. cost rises exponentially increasing number neurons indicating hybrid model outperformed calibrated model practical cost grounds. believe case similar cltv prediction hybrid model handcraed features achieve beer performance current deployed random forest model though much higher cost commercially viable. supported principle preliminary experiments measures root mean squared error hybrid models’ predicted percentile actual percentile customer’s spend. observe increasing number neurons hybrid models decreases rmse unable train hybrid models tens thousands neurons prohibitive runtimes. discussion conclusions described cltv system deployed asos main issues faced building half paper describe baseline architecture achieves state performance problem discuss important issue overlooked literature model calibration. given recent success representation learning across wide range domains second half paper focus ongoing eorts improve model learning additional types representations training feedforward neural network figure mean monetary cost train hybrid models training customers number neurons hidden layers training cost shown relative cost training random forest model. consider hybrid models hidden layers number neurons. bottom horizontal line represents mean cost train vanilla logistic regression model model customers. cost shown based time required train models cost using computational resources spark clusters train models train hybrid models microso azure. vertical dashdotted line represents estimated number neurons layer required two-hidden layer hybrid model out-perform random forest model. handcraed features supervised seing learning embedding customers using session data unsupervised seing augment features showed learning embedding rich source data unsupervised seing improve performance using handcraed features plan incorporate embedding live system well apply approach types events main alternative approach described ways learning representations would deep network learn end-to-end data sources opposed using handcraed features inputs. starting explore approach extremely challenging believe might also provide large improvement versus state art. acknowledgements work partly funded industrial fellowship royal commission exhibition authors thank jedidiah francis useful discussions anonymous reviewers providing many improvements original manuscript. references martin abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jerey dean mahieu devin sanjay ghemawat goodfellow andrew harp georey irving michael isard yangqing lukasz kaiser manjunath kudlur josh levenberg rajat monga sherry moore derek murray shlens benoit steiner ilya sutskever paul tucker vincent vanhoucke vijay vasudevan oriol vinyals pete warden martin wicke yuan xiaoqiang zheng. tensorflow large-scale machine learning heterogeneous distributed systems. hp//download.tensorow.org/paper/whitepaper.pdf ricardo baeza-yates jiang beverly harrison. predicting next going use. proceedings international conference search data mining doihp//dx.doi. org/./. oren barkan noam koenigstein. itemvec neural item embedding collaborative filtering. arxiv doihp//dx.doi.org/. albert bemmaor nicolas glady. modeling purchasing behavior sudden death flexible customer lifetime model. management science doihp//dx.doi.org/./mnsc.. heng-tze cheng levent jeremiah harmsen shaked tushar chandra hrishi aradhye glen anderson greg corrado chai mustafa ispir rohan anil zakaria haque lichan hong vihan jain xiaobing hemal shah. wide deep learning recommender systems. arxiv preprint doihp//dx.doi.org/./. paul covington adams emre sargin. deep neural networks youtube recommendations. proceedings conference recommender systems york hp//dx.doi.org/./. peter fader bruce hardie lee. using iso-value curves customer base analysis. journal marketing research xlii november doihp//dx.doi.org/./jmkr.... jerome friedman trevor hastie robert tibshirani. elements statistical learning. vol. springer series statistics springer berlin. mihajlo grbovic vladan radosavljevic nemanja djuric narayan bhamidipati jaikit savla varun bhagwan doug sharp. e-commerce inbox product recommendations scale categories subject descriptors. proceedings sigkdd international conference knowledge discovery data mining doihp//dx.doi.org/./. hardie kumar gupta ravishanker sriram hanssens kahn. modeling customer lifetime value. journal service research doihp//dx.doi.org/./ yangqing evan shelhamer donahue sergey karayev jonathan long ross girshick sergio guadarrama trevor darrell. convolutional architecture fast feature embedding. proceedings international conference multimedia. brendan mcmahan gary holt sculley michael young dietmar ebner julian grady todd phillips eugene davydov daniel golovin sharat chikkerur martin waenberg arnar hrafnkelsson boulos jeremy kubica. click prediction view trenches. proceedings sigkdd international conference knowledge discovery data mining doihp//dx.doi.org/./ tomas mikolov chen greg corrado jerey dean. distributed representations words phrases compositionality. advances neural information processing systems doihp//dx.doi.org/ ./jmlr...-. donald morrison david schmilein source journal royal statistical society series. generalizing model customer purchases implications worth eort? journal business economic statistics doihp//dx.doi.org/./. bryan perozzi steven skiena. deepwalk online learning social representations. proceedings sigkdd international conference knowledge discovery data mining doihp//dx.doi.org/ tapani raiko harri valpola yann lecun. deep learning made easier linear transformations perceptrons. proceedings international conference articial intelligence statistics. jmlr david schmilein donald morrison richard colombo. counting customers next? management science doihp//dx.doi.org/./mnsc... vanderveld addhyan pandey angela rajesh parekh. engagement-based customer lifetime value system e-commerce. proceedings sigkdd international conference knowledge discovery data mining flavian vasile elena smirnova alexis conneau. meta-prodvec product embeddings using side-information recommendation. proceedings conference recommender systems recsys hp//dx.doi.org/./. pascal vincent hugo larochelle yoshua bengio pierre-antoine manzagol. extracting composing robust features denoising autoencoders. proceedings international conference machine learning. artit wangperawong cyrille brun rujikorn pavasuthipaisit. churn analysis using deep convolutional neural networks autoencoders. arxiv preprint arxiv bianca zadrozny charles elkan. obtaining calibrated probability estimates decision trees naive bayesian classiers. proceedings international conference machine learning", "year": 2017}