{"title": "A Discriminative Framework for Anomaly Detection in Large Videos", "tag": ["cs.CV", "stat.ML"], "abstract": "We address an anomaly detection setting in which training sequences are unavailable and anomalies are scored independently of temporal ordering. Current algorithms in anomaly detection are based on the classical density estimation approach of learning high-dimensional models and finding low-probability events. These algorithms are sensitive to the order in which anomalies appear and require either training data or early context assumptions that do not hold for longer, more complex videos. By defining anomalies as examples that can be distinguished from other examples in the same video, our definition inspires a shift in approaches from classical density estimation to simple discriminative learning. Our contributions include a novel framework for anomaly detection that is (1) independent of temporal ordering of anomalies, and (2) unsupervised, requiring no separate training sequences. We show that our algorithm can achieve state-of-the-art results even when we adjust the setting by removing training sequences from standard datasets.", "text": "abstract. address anomaly detection setting training sequences unavailable anomalies scored independently temporal ordering. current algorithms anomaly detection based classical density estimation approach learning high-dimensional models ﬁnding low-probability events. algorithms sensitive order anomalies appear require either training data early context assumptions hold longer complex videos. deﬁning anomalies examples distinguished examples video deﬁnition inspires shift approaches classical density estimation simple discriminative learning. contributions include novel framework anomaly detection independent temporal ordering anomalies unsupervised requiring separate training sequences. show algorithm achieve state-of-the-art results even adjust setting removing training sequences standard datasets. anomaly detection especially challenging problem because applications prevalent remains ill-deﬁned. attempts deﬁnitions often informal vary across communities applications. paper deﬁne propose solution largely neglected subproblem within anomaly detection constraints exist additional training sequences available; order anomalies occur aﬀect algorithm’s performance instance especially challenging setting cannot build model advance deviations much like clustering outlier detection context deﬁned video itself. setting prominent application ﬁelds robotics medicine entertainment data mining. instance first-time data. robotics team wants create robust algorithms. teleoperate robot performing task operating environment. team would like special cases robot handle perception side list fig. characteristics anomaly detection setting. left training sequences. setting occurs want context drawn solely test video unavailable right temporal independence. often want anomalous frames regardless order appear personalized results context semantically deﬁned coming test set. father wants interesting parts -hour home video family’s christmas. healthcare professional wants review anomalous footage elderly patient living at-home nursing care past week. illustrate practical cases important identify instances anomalies regardless order appear within context testing video. videos available providing context beyond test video acceptable ignore later anomalies long ﬁrst instance recognized many mature methods apply also natural extensions method could incorporate additional context available. consider challenging setting context must derived test video order anomalies occur aﬀect score. general anomaly detection settings cannot traditional supervised approaches impossible suﬃciently representative anomalies. setting given context ahead time; unlike algorithms cannot even build distribution representative familiar events. require approaches operate solely test sequence adapt video’s context. leads denote frames anomalous easily distinguished frames video familiar otherwise. anomaly detection presents unique challenges beyond seen supervised learning paradigm. inability training data classes data leads possible approaches estimate model distribution familiar classify suﬃciently large deviations anomalous; seek points identiﬁable distribution frames label anomalous. approaches seem similar surface lead distinct methodologies diﬀer assumptions data required well type anomalies identify. show latter satisfy setting former comes advantages. traditional approach anomaly detection involves learning model familiarity given video. subsequent time points identiﬁed anomalous deviate model distance metric inverse likelihood. call approaches scanning techniques. examples area include sparse reconstruction dynamic textures human behavior models methods take training data either separate videos hand-chosen frames beginning video build model. many methods update models online need update model temporal order still need large amount training data initialization. method particular achieves reasonable performance small number starting frames still requires manual identiﬁcation frames algorithm. generative models work well domains assumed model normalcy data well. applies model complex enough handle variety events known context allow learner anticipate model data. methods generally assume features come predetermined type distribution therefore likely fail feature distribution changes. complex models computational complexity amount ‘normal’ training data needed initialize model becomes signiﬁcant bottleneck. parameter choices also larger eﬀect ability algorithm data. scanning approaches satisfy anomaly detection setting violate conditions speciﬁed require training instances depend temporal ordering. building model temporal order video makes strong assumptions anomalies detected. setting must ‘most anomalous’ events video regardless order. building models updates temporal order events occur earlier video likely anomalous. instance event type occurs twice large video ﬁrst instance detected second instance ignored. choosing discriminative algorithm acts independently ordering video avoid assumptions pitfalls scanning techniques. method shares discriminative spirit previous works using saliency anomaly detection however saliency methods used require training data local context. objective obtain fully unsupervised method uses context entire video independent ordering anomalies occur. builds graph ﬁnds anomalies independent ordering. however model-based designed work trajectories; goal discriminative able operate features. primary challenge setting inability assume form underlying distribution. non-parametric method preferable generalize many domains assumptions. permutation tests nonparametric methods designed handle cases. general idea test ﬁdelity given statistic possible statistics diﬀerently-labeled dataset. similar approach test distinctiveness frame. method analogous statistic ease given data point distinguished points sampled video. testing frame’s distinguishability diﬀerent groups frames form accurate picture global anomaly score. approach directly estimate discriminability frames reference context video. need model every normal event generate scores anomalous frames; simply attempt discriminate anomalous frames familiar frames diﬀerence distributions. present framework tests discriminability frames. framework perform change detection sequence data video frames distinguishable previous frames. want comparisons independent time create shuﬄes data permuting frames running instance change detection. simple classiﬁers used avoid overﬁtting frame compared many permutations frames. discriminative framework allows perform unique setting. motivate method demonstrate advantages scanning techniques let’s walk example. suppose draw four images mnist dataset diﬀerent label create ‘video’ using noisy copies images. order images shown figure ﬁrst portion video contains instances fig. detections one-class algorithm example. ground truth represents digit classes mnist used generate frame. dashes indicate locations anomalies. shaded region represents detections made algorithm. algorithm without shuﬄing temporal disadvantages online one-class svm. including shuﬄing trigger false positives prevalent examples seen ﬁrst time. also detect full extent anomaly avoid assuming beginning familiar. prevalent. case would hope algorithm classiﬁes instances anomalous considers familiar. one-class kernel instance scanning techniques. figure shows scores static one-class trained ﬁrst portion video algorithm online update algorithm without shuﬄing. algorithm’s performance withshuﬄing similar online one-class svm. model remains static ﬁrst third video classiﬁed anomalous. even online model update ﬁrst classiﬁed anomalous. addition given equal anomaly weights within respective classes. algorithm avoids pitfalls shuﬄing introduced classifying anomalous. using permutation-based framework able evade assumptions familiarity remove eﬀects temporal ordering anomaly scores. issues discussed extend beyond example. scanning methods anomalies appear missed. addition common failures assumption beginning video represents familiarity anomalies appearing beginning familiar events appearing later video. videos context changes frequently create dangerously high number false positives. also note example uses one-class example scanning techniques inherent problems. method developed part circumvent previously unavoidable failure cases. addition hope demonstrate simple discriminative techniques match performance complex generative methods operating setting identiﬁed. taking direct approach. inspired density ratio estimation change point detection take direct approach anomaly detection popular generative approach. main objective density ratio estimation avoid unnecessary work deciding distributions data point generated rather model distributions independently directly compute ratio probabilities data point drawn other. shortcut especially helpful anomaly detection. interested relative probability given frame anomalous rather familiar less interested distribution familiar events. machine learning community covered several ways estimate ratio directly enumerated several cost functions paradigms ratio appears note estimate ratios directly simple logistic regression therefore standard classiﬁer measure deviation groups points system overview. full framework depicted figure recall deﬁnition anomalous frames easily distinguished others video. deﬁnition avoids domain-speciﬁc notions anomalies relies robust features used distinguish anomalies variety domains. assume appropriate features computed forms descriptor frame. discriminative method choice features smaller impact choice algorithm parameters would generative method. overall proposed framework agnostic feature choice; user plug relevant state-of-the-art features based domain knowledge novel feature methods. addition features aggregated within across frames obtain diﬀerent levels spatial temporal resolution. framework make explicit assumptions distribution features simply design choices based cost feature computation desired resolution detections. shuﬄes change detection. remove permutation algorithm would perform simple change detection testing distinguishability sliding window frames frames assumed familiar. conceptual example shown figure ﬁrst iteration classiﬁer learned points ﬁrst points given label second points given label call labels ‘split’ data. point labeled given score probability belongs class instead class according classiﬁer implementation +exp minimizes l-regularized logistic loss. simply second frames within ‘sliding time window’ ‘sliding window’ next iteration frames reassigned label fig. proposed anomaly detection framework. given input video descriptor frame passed anomaly detection algorithm descriptors shuﬄed times. shuﬄe algorithm evaluates anomaly scores sliding window frames. score based density ratio compared frames came sliding window. finally scores combined averaging produce ﬁnal output signal. image depicting dense trajectory features wang next points labeled process repeats sliding window reaches video. sliding window reaches events window compared events past. higher given point larger classiﬁer’s conﬁdence distinguished previous points. sliding window chosen rather moving point-by-point several reasons. first provides inherent regularization classiﬁer since distinguishing point rest misleadingly easy even point familiar. addition number splits algorithm must compute inversely proportional window size seem ‘polluting’ sliding window familiars would ruin anomaly’s chance accurately scored. however case anomalies easily distinguished rest video therefore chance fall near resulting classiﬁer boundary probability familiar event high adding shuﬄes full anomaly detection. pointed earlier disadvantage approach without shuﬄes scanning techniques temporal dependencies cause algorithm miss events occur raise false alarms events prevalent later video beginning. therefore shuﬄe order data repeat change detection process described reduce eﬀect order. producing series distinguishability scores classiﬁers learned diﬀerent permutations frames thought testing hypotheses. aggregating scores. scores computed shuﬄe average results. average outperforms methods aggregation like median maximum. aggregating shuﬄes log-odds computed ﬁnal anomaly score. full overview explained algorithm method based distinguishing labeled subsets data. consider think tradeoﬀs increasing decreasing window size number shuﬄes given choice classiﬁer. attempt provide intuition considering simple analyses suggest might understand tradeoﬀs parameters classiﬁer choices. overall objective. order algorithm work classiﬁer needs enough capacity able tell anomalies familiars apart simple enough unable tell familiars apart familiars. capacity function classiﬁer complexity number points compared subset points compared classiﬁer assumptions. must make basic assumptions classiﬁer. assume classiﬁer able correctly distinguish anomaly familiars. case logistic regression instance assume label anomaly relatively high conﬁdence learned familiars labeled anomaly within familiars fig. visualizing consecutive splits. points blue labeled respectively. window size values probability values familiars close boundary therefore yield odds score anomalies identical. formally anomalies identical feature vectors xna. represents worst case anomaly labeled another anomaly labeled score small. words anomaly negative window negate score another positive window. therefore best split data given anomaly instance occurs ﬁrst video. goal shuﬄing reorder anomalies enough times every anomaly gets opportunity ﬁrst instance reordered frames. formally deﬁne random variable event anomaly appears ﬁrst given shuﬄe. binary variable occurs probability expectation fraction shuﬄes appears ﬁrst also however would like shuﬄes possible ensure event gets close mean means need bound required desired fraction gets smaller eﬀect deviations score anomaly grows proportionally larger. instance requires every anomaly ﬁrst least many larger window sizes decrease eﬀect overﬁtting classiﬁers. given choice parameter framework requires care window size increasing window stride decreases computational load system terms performance also seem best make small possible anomalies easier distinguish smaller window size. addition large window sizes anomalies diﬀerent types fall within window ‘interfere’ others’ scores. however decreasing window size beyond certain point also reduces performance classiﬁer overﬁts familiars become distinguishable familiars. words must choose able trade ability distinguish anomalies without able distinguish familiars. consider theoretical sketch explicating relation complexity classiﬁer choice window size size subset points classiﬁed higher rademacher complexity indicates classiﬁer able easily distinguish randomly labeled data. instance highly regularized linear classiﬁer much lower rademacher complexity rbf-kernel complex neural network. metric especially convenient measured relative data distribution empirically estimated simply computing statistic randomly labeled subsampled data. size decrease probability familiars distinguished other. generalization bounds provide relate error overﬁtting noisy labels classiﬁer complexity dataset size. case true error error classifying anomalous familiar points incorrectly according true labels. training error error trained classiﬁer synthetic labels. careful analysis requires understanding errors ﬁxed design setting traditional i.i.d. random design provides crude guidance algorithm behavior trade-oﬀs. setting rademacher complexity provides generalization bound i.i.d. samples diﬀerence between estimated true error classifying datapoints intuition choose simple classiﬁer l-regularized logistic regression. window size easily chosen empirical testing; variance anomaly signal large familiars easy tell apart window size decreased. anomalies visible increased. avenue dataset contains training videos testing videos locations anomalies marked ground truth pixel-level masks frame testing videos. videos include total frames testing. insights follow generalize dimension complexity measures. useful introduction http//www.cse.cuhk.edu.hk/leojia/projects/detectabnormal/dataset.html algorithm permitted testing videos performed anomaly detection assumptions normality section video. stark contrast methods must train model frames training videos and/or pre-marked sections video. several datasets available anomaly detection algorithm demonstrated reasonable success ones tested. focus avenue dataset speciﬁcally challenging staged datasets recent speciﬁc labeling others personal vacation dataset. dataset also valuable method publicly available code results able compare implementation features recent standard anomaly detection. ucsd pedestrian anomaly detection dataset another well-labeled recent dataset nearly half frames test video contain anomalies provided anomaly labels applicable unsupervised setting. precisely setting frames would deﬁned anomalous since activities labeled dataset often compose half video. implementation. avenue dataset follow feature generation procedure courtesy code provided upon request. features computed video match method exactly resulting gradientbased features spatiotemporal subunits video. normalization subunit represented dimensional vector. using code provided authors able algorithm alongside features. following evaluation treat subunit ‘frame’ framework classifying subunit independently. results smoothed ﬁlter used liblinear’s l-logistic regression classiﬁer framework. experimented several values across videos found long features whitened within order magnitude gives reasonable results misleading anomaly detection setting. therefore using curves corresponding area curve evaluation metric computed reference human-labeled frame pixel ground truth. results. figure shows example detections resulting curves values algorithm avenue dataset. note algorithm operates separate setting sequence individual test video obtain guiding form models familiarity assume partition video familiar. even additional challenges able obtain near-state-of-the-art performance. curve sheds light possible performance bottleneck algorithms last half curves algorithms match closely. highlights diﬃcult anomaly instances similar number false positives seem commonly detected algorithms. believe hitting limitation encoding events feature space either feature space descriptive enough several instances appear anomalous true anomalies feature space. example detections per-video analysis also shed light method’s behavior. since algorithm operating test sequence exhibits false positives time someone enters foreground right. penalized provided ground truth marked relative training data. addition using features operate -frame chunks time often detect events early frames soon. table show algorithm’s robust performance across range parameters avenue dataset. results show sub-optimal parameter choices shuﬄes improve performance. imagine under-regularized classiﬁer classiﬁer easily distinguish normal points subsets data eﬀect reduced number shuﬄes increases. similar argument follows sub-optimal parameters. major beneﬁts shuﬄing cannot seen commonly used datasets anomaly detection test sequences short show context changes multiple instances anomaly commonly found real-world scenarios. consider case video anomalous algorithm markes frames normal would outperforming modern algorithms. extreme class imbalance less prevalent current standard datasets become apparent problem realistic datasets become prevalent. fig. performance avenue dataset. curves show performance nearly matches algorithm require training data. detection examples shown show correctly classiﬁed familiar frame detected anomalous frames false alarm plementary material. method outperforms scene. values good average method sparse method still lower sparse model-based approaches reported indicates change features could make diﬀerence performance gap. developed method identifying anomalies videos setting independent order anomalies appear requires separate training sequences. permutation-testing methodology requires assumptions content descriptors frame user able plug latest optimal features video long anomalous frames distinguishable space. training data needs collected labeled within test video. show anomaly detection algorithm able perform well state standard datasets even adjust setting removing training data. lack assumptions content location familiars valuable ﬁnding true anomalies previously remained unseen.", "year": 2016}