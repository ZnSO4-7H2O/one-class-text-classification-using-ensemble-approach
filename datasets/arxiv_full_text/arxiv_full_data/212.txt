{"title": "Virtual Worlds as Proxy for Multi-Object Tracking Analysis", "tag": ["cs.CV", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Modern computer vision algorithms typically require expensive data acquisition and accurate manual labeling. In this work, we instead leverage the recent progress in computer graphics to generate fully labeled, dynamic, and photo-realistic proxy virtual worlds. We propose an efficient real-to-virtual world cloning method, and validate our approach by building and publicly releasing a new video dataset, called Virtual KITTI (see http://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds), automatically labeled with accurate ground truth for object detection, tracking, scene and instance segmentation, depth, and optical flow. We provide quantitative experimental evidence suggesting that (i) modern deep learning algorithms pre-trained on real data behave similarly in real and virtual worlds, and (ii) pre-training on virtual data improves performance. As the gap between real and virtual worlds is small, virtual worlds enable measuring the impact of various weather and imaging conditions on recognition performance, all other things being equal. We show these factors may affect drastically otherwise high-performing deep models for tracking.", "text": "modern computer vision algorithms typically require expensive data acquisition accurate manual labeling. work instead leverage recent progress computer graphics generate fully labeled dynamic photo-realistic proxy virtual worlds. propose efﬁcient real-to-virtual world cloning method validate approach building publicly releasing video dataset called virtual kitti automatically labeled accurate ground truth object detection tracking scene instance segmentation depth optical ﬂow. provide quantitative experimental evidence suggesting modern deep learning algorithms pre-trained real data behave similarly real virtual worlds pre-training virtual data improves performance. real virtual worlds small virtual worlds enable measuring impact various weather imaging conditions recognition performance things equal. show factors affect drastically otherwise high-performing deep models tracking. although cheap even annotations might used training time weakly-supervised learning experimentally evaluating generalization performance robustness visual recognition model requires accurate full labeling large representative datasets. however challenging practice video understanding tasks like multi-object tracking high data acquisition labeling costs limit quantity variety existing video benchmarks. instance kitti multi-object tracking benchmark contains test sequences captured similar good configure frame video kitti multi-object tracking benchmark middle corresponding rendered frame synthetic clone virtual kitti dataset automatic tracking ground truth bounding boxes. bottom automatically generated ground truth optical sceneinstance-level segmentation depth ditions single source. best knowledge none existing benchmarks computer vision contain minimum variety required properly assess performance video analysis algorithms varying conditions multiple detailed object class annotations different camera settings among many others factors. using synthetic data theory enable full control data generation pipeline hence ensuring lower costs greater ﬂexibility limitless variety quantity. work leverage recent progress computer graphics commodity hardware generate photorealistic virtual worlds used proxies assess performance video analysis algorithms. ﬁrst contribution method generate large photo-realistic varied datasets synthetic videos automatically densely labeled various video understanding tasks. main novel idea consists creating virtual worlds scratch cloning seed realworld video sequences. using method second main contribution creation virtual kitti dataset time publication contains photo-realistic synthetic videos coupled variations each) total approximately high resolution frames automatic accurate ground truth object detection tracking depth optical well scene instance segmentation pixel level. third contribution consists quantitatively measuring usefulness virtual worlds proxies multi-object tracking. ﬁrst propose practical deﬁnition transferability experimental observations across real virtual worlds. protocol rests comparison real-world seed sequences corresponding synthetic clones using real-world pre-trained deep models hyper-parameter calibration bayesian optimization analysis taskspeciﬁc performance metrics second validate usefulness virtual worlds learning deep models showing virtual pre-training followed real-world ﬁne-tuning outperforms training real world data. experiments therefore suggest recent progress computer graphics technology allows easily build virtual worlds indeed effective proxies real world computer vision perspective. fourth contribution builds upon small virtualto-real measure potential impact recognition performance varied weather conditions lighting conditions camera angles things equal something impractical even impossible realworld conditions. experiments show variations signiﬁcantly deteriorate performance normally high-performing models trained large real-world datasets. lack generalization highlights importance open research problems like unsupervised domain adaptation building varied training sets move towards applying computer vision wild. paper organized follows. section reviews related works using synthetic data computer vision. section describes approach build virtual worlds general virtual kitti particular. section reports multi-object tracking experiments using strong deep learning baselines assess transferability observations across real-to-virtual beneﬁts virtual pre-training impact various weather imaging conditions recognition performance conclude section several works investigate synthetic data tackle standard computer vision problems object detection face recognition scene understanding optical estimation early computer vision researchers leveraged computer simulations model articulated objects including human shape face hand appearance even scene interpretation vision inverse graphics however methods typically require controlled virtual environments tuned constrained settings require development taskspeciﬁc graphics tools. addition lack photorealism creates signiﬁcant domain synthetic real world images turn might render synthetic data simplistic tune analyze vision algorithms degree photorealism allowed recent progress computer graphics modern high-level generic graphics platforms enables widespread synthetic data generated less constrained settings. first attempts synthetic data training mainly limited using rough synthetic models synthesized real examples contrast mar´ın went positively answer intriguing question whether learn appearance models pedestrians virtual world learned models detection real world. related approach described scenescene-location speciﬁc detectors ﬁxed calibrated surveillance cameras priori known scene geometry. context video surveillance proposes virtual simulation test system design evaluation. several works models general object pose estimation detection works photo-realistic imagery evaluation purposes cases works focus lowlevel image video processing tasks. kaneva evaluate low-level image features butler propose synthetic benchmark optical estimation popular sintel flow dataset. recent work chen another example basic building blocks autonomous driving. approaches view photo-realistic imagery obtaining ground truth cannot easily obtained otherwise ground-truth collected instance crowdsourcing real-world imagery often preferred synthetic data artifacts latter might introduce. paper show issues partially circumvented using approach particular highlevel video understanding tasks ground-truth data tedious collect. believe current approaches face major limitations prevent broadening scope virtual data. first data generation costly timeconsuming often requires creating animation movies scratch. also limits quantity data generated. alternative consists recording scenes humans playing video games faces similar time costs restricts variety generated scenes. second limitation lies usefulness synthetic data proxy assess real-world performance high-level computer vision tasks including object detection tracking. indeed difﬁcult evaluate conclusions obtained virtual data could applied real world general. limitations previous works exploited full potential virtual worlds possibility generate endless quantities varied video sequences on-the-ﬂy. would especially useful order assess model performance crucial real-world deployment computer vision applications. paper propose steps towards achieving goal addressing main challenges automatic generation arbitrary photo-realistic video sequences ground-truth scripting modern game engines assessing degree transferability experimental conclusions synthetic data real world. approach consists steps detailed following sections acquisition small amount realworld data starting point calibration cloning real-world data virtual world automatic generation modiﬁed synthetic sequences different weather imaging conditions automatic generation detailed ground truth annotations quantitative evaluation usefulness synthetic data describe method particular choices made generate virtual kitti dataset. acquiring real-world data ﬁrst step approach consists acquisition limited amount seed data real world purpose calibration. types data need collected videos real-world scenes physical measurements important objects scene including camera itself. quantity data required approach much smaller typically needed training validating current computer vision models require reasonable coverage possible scenarios interest. instead small ﬁxed core real-world video sequences initialize virtual worlds turn allows generate many varied synthetic videos. furthermore initial seed real-world data results higher quality virtual worlds quantify usefulness derive conclusions likely transfer real-world settings. experiments kitti dataset initialize virtual worlds. standard public benchmark captured driving german city karlsruhe mostly sunny conditions. sensors used capture data include gray-scale color cameras laser scanner inertial navigation system. point clouds captured laser scanner human annotators labeled bounding boxes several types objects including cars pedestrians. experiments consider cars objects interest simplicity main category kitti. annotation data include positions sizes cars rotation angles vertical axis movement camera recorded orientation gps/imu sensor ﬁxed spatial relationship cameras. next step approach consists semiautomatically creating photo-realistic dynamic virtual worlds virtual camera paths follow real world seed sequences generate outputs call synthetic video clones closely resemble real-world data. build virtual kitti select training videos original kitti benchmark real-world seeds create virtual worlds graphic assets scripted reconstruct scene. commercial computer graphics engine unity create virtual worlds closely resemble original ones kitti. engine strong community developed many assets publicly available unity’s asset store. assets include realistic models materials objects. allows efﬁcient crowd-sourcing manual labor initial setup virtual worlds making creation virtual world efﬁcient positions orientations objects interest virtual world calculated based positions orientations relative camera position orientation camera itself available acquired real-world data case kitti. main roads also placed according camera position minor manual adjustment special cases build virtual kitti dataset manually place secondary roads background objects trees buildings virtual world simplicity lack position data them. note could automated using visual slam semantic segmentation. directional light source together ambient light simulates sun. direction intensity manually comparing brightness shadows virtual real-world scenes simple process takes minutes world experiments. changing conditions synthetic videos virtual world created automatically generate clone synthetic video also videos changed components. allows quantitative study impact single factors including rare events difﬁcult observe conditions might occur practice conditions changed generate synthetic videos include number trajectories speeds cars sizes colors models camera position orientation path lighting weather conditions. components randomized modiﬁed demand changing parameters scripts manually adding modifying removing elements scene. illustrate vast possibilities virtual kitti includes simple changes virtual world translate complex visual changes would otherwise require costly process re-acquiring re-annotating data real-world. first turned camera right left lead considerable change appearances cars. second changed lighting conditions simulate different time early morning sunset. third used special effects particle system together changed lighting conditions simulate different weather overcast heavy rain. figure illustration. stated above ground-truth annotations essential computer vision algorithms. kitti dataset bounding boxes used evaluation obtained human annotators drawing rectangular boxes video frames manually labeling truncation occlusion states objects. common practice however costly scale large volumes videos pixellevel ground-truth incorporates varying degrees subjectiveness inconsistency. example bounding boxes usually slightly larger cars margins differ another annotator another. occlusion state also subjective underlying criterion differ case case yielding many important edge cases inconsistent labels. contrast approach automatically generate accurate consistent ground-truth annotations accompanying synthetic video outputs algorithm-based approach allows richer consistent results human annotators. render moment scene four times. first photo-realistic rendering clone scene leveraging modern rendering engine unity. second depth rendered using information stored depth buffer. third per-pixel categoryinstance-level ground-truth efﬁciently directly generated using unlit shaders materials objects. modiﬁed shaders output color affected lighting figure rendered frame automatically generated scene instance-level segmentation ground-truth modiﬁed conditions camera horizontally rotated left rain shading conditions. unique color assigned object interest fourth compute dense optical previous current frames sending model view projection matrices object vertex shader interpolate pixel using fragment shader. note multiple renderings efﬁcient strategy generate pixel-level ground truth effectively leverages shaders ofﬂoading parallel computations gpus virtual kitti resolution around full rendering ground truth generation pipeline segmentation depth optical runs single desktop commodity hardware. generate multi-object tracking ground truth perspective projection object bounding boxes world coordinates camera plane associating bounding boxes corresponding object differentiate object instances adding truncation occlusion meta-data described below. truncation rate approximated dividing volume object’s bounding volume bounding visible part also estimate occupancy rate object dividing number ground-truth pixels segmentation mask area projected bounding includes occluder results perspective projection full bounding object. special case additionally compute visibility object formula used generate effect. comparable experimental protocols reproducible ground truth criteria across real virtual kitti remove manually annotated dontcare areas original kitti training ground truth ignore cars smaller pixels heavily truncated occluded evaluation sequence global thresholds occupancy truncation rates virtual objects close possible original kitti annotations. addition data generation annotation methods novel aspect approach consists assessment usefulness generated virtual worlds computer vision tasks. priori unclear whether using photo-realistic synthetic videos indeed valid alternative real-world data computer vision algorithms. transferability conclusions obtained synthetic data likely depend many factors including tools used quality implementation target video analysis tasks. although using synthetic training data common practice computer vision aware related works systematically study reverse i.e. using real-world training data noisy weakly labeled synthetic test data must accurately labeled where therefore synthetic data obvious beneﬁts. assess robustly whether behavior recognition algorithm similar real virtual worlds propose compare performance initial seed real-world videos corresponding virtual world clones. compare multiple metrics interest obtained ﬁxed hyper-parameters maximize recognition performance real virtual videos simultaneously minimizing performance gap. case bayesian hyperparameter optimization ﬁxed tracker hyperparameters pair real clone videos. objective function multi-object tracking accuracies original real-world videos corresponding virtual clones minus absolute differences normalized mean absolute deviations normalized clear metrics allows quantitatively objectively measure impact virtual world design degree photorealism quality rendering parameters algorithm performance metrics interest. note simple technique direct beneﬁt virtual world generation scheme based synthetically cloning small real-world sensor data. although comparisons depend tasks interest also possible complement task-speciﬁc metrics general measures discrepancy domain mismatch measures finally note protocol complementary standard approach consisting using synthetic training data real-world test data. therefore experiments virtual kitti investigate methods assess usefulness virtual data learning virtual models applied real world evaluating realworld pre-trained models virtual real worlds. section ﬁrst describe models used experiments. report results regarding differences original real-world kitti videos virtual kitti clones. report experiments learning virtual worlds models applied real-world data. finally conclude experiments measure impact camera lighting weather recognition performance real-world pre-trained algorithms. strong deep learning baselines thanks recent progress object detection association-based tracking-by-detection monocular video streams particularly successful widely used recent review). methods consist building tracks linking object detections time. experiments detector recent fastr-cnn object detector girshick combined efﬁcient edge boxes proposals experiments follow experimental protocol learn powerful vgg-based fast-rcnn detector ﬁne-tuning successively imagenet pascal cars kitti object detection benchmark training images. require video training data. maximum posteriori data association problem indeed elegantly formalized special integer linear program whose global optimum found efﬁciently using max-ﬂow min-cost network algorithms particular dynamic programming min-cost algorithm pirsiavash well-founded particularly efﬁcient. although obtains poor results kitti benchmark vastly improved using better detector replacing binary pairwise costs network using intersection-overunion allowing multiple time-skip connections network better handle missed detections. rcnn tracker reaches mota kitti evaluation server improving w.r.t. original note baseline tracker could improved shown recently wang fowlkes method indeed obtains mota related algorithm thanks better appearance motion modeling coupled structured svms learn hyper-parameters training videos. second tracker consider recent state-ofthe-art markov decision process method xiang relies reinforcement learning learn policy data association ground truth training tracks. method reaches mota kitti test using convnet-based detections. experiments requiring pre-trained tracker learned parameters following real-world kitti training videos table contains multi-object tracking performance dp-mcf trackers virtual kitti clone videos original kitti counterparts following protocol described section figure tracking visualizations. according mota metric summarizes aspects real-to-virtual performance minimal real sequences respective virtual clones trackers average trackers. metrics show also limited gap. consequently visual similarity sequences comparable performance behavior tracker across realworld videos virtual worlds counterpart suggest similar causes real virtual worlds likely cause similar effects terms recognition performance. amount expected transferability conclusions real virtual back quantiﬁed difference metrics reported table figure predicted tracks matching frames original videos synthetic clones dp-mcf note visual similarity scenes tracks. differences occluded small truncated objects. table dp-mcf results original real-world kitti train videos virtual world video clones average real sequences. report clear metrics including accuracy precision switches fragmentation complemented mostly tracked mostly lost ratios well detector’s precision recall different metrics motp fraction mostly tracked objects generally lower virtual world. main factor explaining lies inaccurate inconsistent manual annotations frequent corner cases real world contrast virtual kitti ground truth subjective automatically determined thresholding aforementioned computed occupancy truncation rates. discrepancy illustrated figure explains small drop recall sequences note however fastrcnn detector achieves similar performance real virtual worlds drop recall generally compensated increase precision. mentioned previously method quantify real virtual worlds perspective computer vision algorithms complementary widely-used approach leveraging synthetic data train models applied real-world settings. therefore additionally conduct experiments measure usefulness virtual kitti train algorithms. evaluated three different scenarios training real kitti seed sequences training corresponding virtual kitti clones training ﬁrst virtual kitti clones ﬁne-tuning real kitti sequences special form virtual data augmentation call virtual pre-training split real kitti sequences used training test long diverse videos evaluate performance validation short videos used hyper-parameter tuning. fasttable dp-mcf results seven held-out original real-world kitti train videos learning models real seed kitti videos corresponding virtual kitti clones successively training virtual clones real sequences table details metrics. table reports average metrics aforementioned real test sequences trackers trained conﬁgurations. although training virtual data enough best results obtained conﬁguration v→r. therefore virtual pre-training improves performance conﬁrms usefulness virtual worlds high-level computer vision tasks. improvement particularly signiﬁcant dp-mcf tracker less tracker. indeed better handle missed detections works high-precision regime detector strongly improved virtual pre-training. hand dp-mcf robust false positives requires recall signiﬁcantly improved virtual pre-training. cases found validating early stopping criterion second ﬁne-tuning stage conﬁguration critical avoid overﬁtting small real training pretraining virtual one. table contains performance real-world pretrained trackers altered conditions generated either modifying camera position using special effects simulate different lighting weather conditions. trackers trained consistent ideal sunny conditions modiﬁcations negatively affect metrics trackers. particular weather causes strongest degradation performance. expected difﬁcult quantify practice without re-acquiring data different conditions. also suggests empirical generalization performance estimated limited kitti test videos optimistic upper bound best. note tracker suffering stronger overﬁtting dp-mcf suggested bigger performance degradation conditions. table impact variations performance virtual kitti dp-mcf trackers. report average performance virtual clones difference caused modiﬁed conditions order measure impact several phenomena things equal. +deg corresponds camera rotation degrees right morning corresponds typical lighting conditions dawn sunny day. sunset corresponds slightly night time. overcast corresponds lighting conditions overcast weather causes diffuse shadows strong ambient lighting. implemented volumetric formula rain simple particle effect ignoring refraction water drops camera. work introduce fully annotated photorealistic synthetic video dataset called virtual kitti built using modern computer graphics technology novel real-to-virtual cloning method. provide quantitative experimental evidence suggesting real virtual worlds small perspective highlevel computer vision algorithms particular deep learning models multi-object tracking. also show state-of-the-art models suffer over-ﬁtting causes performance degradation simulated modiﬁed conditions approach best knowledge enables scientiﬁcally measure potential impact important phenomena recognition performance statistical computer vision model. future works plan expand virtual kitti adding worlds also including pedestrians harder animate. also plan explore evaluate domain adaptation methods larger scale virtual pre-training data augmentation build robust models variety video understanding tasks including multi-object tracking scene understanding.", "year": 2016}