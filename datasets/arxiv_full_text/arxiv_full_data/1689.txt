{"title": "Latent Topic Models for Hypertext", "tag": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "abstract": "Latent topic models have been successfully applied as an unsupervised topic discovery technique in large document collections. With the proliferation of hypertext document collection such as the Internet, there has also been great interest in extending these approaches to hypertext [6, 9]. These approaches typically model links in an analogous fashion to how they model words - the document-link co-occurrence matrix is modeled in the same way that the document-word co-occurrence matrix is modeled in standard topic models. In this paper we present a probabilistic generative model for hypertext document collections that explicitly models the generation of links. Specifically, links from a word w to a document d depend directly on how frequent the topic of w is in d, in addition to the in-degree of d. We show how to perform EM learning on this model efficiently. By not modeling links as analogous to words, we end up using far fewer free parameters and obtain better link prediction results.", "text": "latent topic models successfully applied unsupervised topic discovery technique large document collections. proliferation hypertext document collection internet also great interest extending approaches hypertext approaches typically model links analogous fashion model words document-link co-occurrence matrix modeled document-word co-occurrence matrix modeled standard topic models. paper present probabilistic generative model hypertext document collections explicitly models generation links. speciﬁcally links word document depend directly frequent topic addition in-degree show perform learning model eﬃciently. modeling links analogous words using fewer free parameters obtain better link prediction results. need automatically infer diﬀerent topics discussed corpus arises many applications ranging search engines summarization software. prominent approach modeling corpus latent topic model document viewed mixture latent topics factors factors shared whole corpus related terms words appearing documents. histogram terms ignoring order terms internal structure documents. entire corpus represented document-term co-occurrence matrix. semantic analysis done projecting document-term co-occurrence matrix onto lower dimensional factor space. algebraic methods latent semantic analysis projected onto linear factor space using svd. statistical methods probabilistic latent dirichlet allocation somewhat general formalism discrete document-term co-occurrence matrix projected onto simplex maximizing observations likelihood. recent years latent topic models extended various ways. particular correlation topics dynamics time directly modeled. additional information provided corpus authorship information studied addition novel models depart words assumption consider internal ordering words sentences within document developed. models combine local dependencies various ways; example combining n-grams hierarchical topic model modeling syntax modeling continuous drift topic another within document paper address question enrich model considering links documents hyperlinks hypertext citations scientiﬁc papers. emergence rapid growth world wide hypertext documents containing links documents become ubiquitous. connectivity documents proven play important role determining importance relevance document information retrieval interest certain user particular dietz recently proposed generative topic model prediction citation inﬂuences called citation inﬂuence model. models particular structure paper citations citations graph described directed acyclic graph previous works extend topic models include link information. cohn hofmann introduce joint probabilistic model content connectivity. model based assumption similar decomposition document term cooccurrence matrix applied cite-document co-occurrence matrix entry count appearances linked-document source document. approach links viewed additional observations analogous additional words vocabulary diﬀerent weight estimating topic mixture document. erosheva also makes decomposition term-document citation-document cooccurrence matrices extending model include generative step citations. note models learn co-occurrence matrix citations without exploiting information conveyed cited documents text. thus citationdocument co-occurrences matrix sparse generalization power models limited. paper suggest novel generative model hypertext document collection name latent topic hypertext model approach includes direct modeling real-world complex hypertext collections links every document every document exist including self-reference model link entity originating speciﬁc word pointing certain document. probability generate link source document target document depends topic word link originating importance target document topic mixture target document observed link directly aﬀects topic mixture estimation target document well source document. moreover non-existence link documents observation serves evidence diﬀerence topic mixtures documents. introduce lthm related models section describe approximate inference algorithm section experimental results obtained learning datasets provided section finally discuss results section bitrary accordingly. means assume forms dag. therefore would like allow document link document allowing loops i.e. directed cycles links originating certain document ending document. particular would like allow self loops links document links itself. solution generative model consists stages. ﬁrst stage document content created. text documents created second stage creating links takes place. contribution paper modeling link generation suggesting approximate inference algorithm studying text documents generated using several various models mentioned section simplicity describe text generation using following section ﬁrst brieﬂy review model second describe second stage link generation finally discuss related models according model collection documents generated latent factors topics. main assumptions model topic single multinomial random variable deﬁnes probability word given topic documents collection. document characterized particular mixture topic distribution deﬁned random variable generation words document corpus contains stages ﬁrst hidden topic selected multinomial distribution deﬁned second given topic word drawn multinomial distribution parameters figure illustrates generative model. figure model. link-lda model. lthm model scenario generating links document document lthm model scenario generating links document document collection documents. assume links originate word word link associated simplicity restrict discussion case link anchored single word. generalization case link anchored sequence words carried forcing topics words identical proposed generation links carried iterating words document word determining whether create link target document. limit discussion ﬁrst case corpus contains documents links generated words document document iterating words word need decide whether create link not. decision contains steps ﬁrst step drawing random variable multinomial general take values degenerated example take values indicates link indicates link document consider adding link document proceed next step randomly drawing topic link topic assignment drawn mixture topics document link created figure illustrates full generative model degenerated example. generalization case generating links single document document collection documents illustrated figure case generation links words document starts selecting every word ..nd′ document drawn random multinomial distribution indicating probability considering link documents link all. measure importance documents corpus. drawn hyperparameter dirichlet prior symmetric favors creating links words associated link also note links document allowed model general case every document contain words linked document generated sequentially going words documents drawing random corresponding described above. link-lda model somewhat closer model. according approach types observed variables modeled words documents citation documents. generation variables carried ﬁrst selecting mixture topics documents words citations document generating hidden topic observation selected random case words case citations; model illustrated figure note lthm probability create link given topic λdθd. additional parameters denote document importance link creation whereas link-lda model additional parameters also lthm existence non-existence link observation explicitly modeled link-lda. moreover according lthm link shares topic word originates time aﬀects topic mixture cited document. exact inference hierarchical models plsa intractable coupling latent topics mixing vectors hypertext model presented paper shares coupling adds unique coupling topic mixing vectors; hence exact inference intractable well. recent years several alternatives approximate inference models suggested variational expectation propagation monte-carlo sampling unlike hypertext topic models lthm identities ends link observations also link’s existence taking account non-existence links sampling-based inference necessitates approximations. therefore perform inference using deviates fully bayesian methods distinguishing latent variables parameters model. latent variables latent topic word latent topic involved link generation variable parameters model topic mixing vectors word mixing vectors document link importance parameter dirichlet hyperparameters ﬁxed. link generation process unless link created value unknown. might created topic mismatch source document document. reason need consider possible options probability inference source document word outgoing link need consider possible variables. number quadratic number documents. therefore infeasible compute explicitly posterior distribution latent variables. however m-step aggregations posterior distributions needed. required aggregations computed eﬃciently taking advantage symmetries model described section appendix. begin m-step equations detailing required expectations. describe required posteriors aggregations computed e-step. estimator takes account topics words links drawn word topics drawn words document link topics topics d′id drawn considering link document cases τd′i regardless whether link created not. purpose inference count na¨ıve computation would require estimating posterior distributions τd′i d′id triplets mentioned before explicit computation posteriors infeasible large number variables. rather computing posterior distribution explicitly aggregations computed. expected number occurrences links incoming document topic case link exists posterior distributions corresponding equal; hence computed summing posterior probabilities expected number times τd′i corpus d′i. basic idea computation factors topic dependent terms document-topic dependent terms. topic dependent terms computed single linear pass corpus document-topic dependent terms speciﬁc udz. combining topic dependent terms compute done constant number operations computation detailed appendix. topics d′id separately links non-links. denote number occurrences topic associated word document number occurrences topic associated incoming link document number times τd′i topic generated link document d′id match topic word document therefore link created. number extensions model considered here approximate inference algorithm described easily adapted many alternatives mentioned section example suppose wishes model text htmm diﬀerence would computation posterior word topics htmm posterior would computed using forward-backward algorithm considering words links topic emission probabilities. alternatively wishes make authorship information applying author-topic model would require consider additional latent variable authorship word compute posterior probabilities modify definition modeling links stays similar; addition latent topic link latent author link needs selected section explore relations links topics discovered lthm evaluate predictive power respect links. compare lthm’s link prediction previous approaches combining links link-plsa link-lda. also compare link prediction non-topic method based frequency page linked ignoring contents text corpus. comparison datasets pages webkb dataset small dataset wikipedia pages begin example strong relationships topics links wikipedia dataset learned lthm. wikipedia dataset collection pages links pages dataset. downloaded pages wikipedia crawling within wikipedia domain starting nips wikipedia page. made data available online http//www.cs.huji.ac.il/∼amitg/lthm.html. used vocabulary words trained lthm hidden aspects. figure shows four hidden topics found model wikipedia dataset. topic show probable words probable links. topic discusses neural networks related links similarly topic speech pattern recognition. topic cities topic cognitive science neuroscience. topics related links. lack space show four example topics topics clear interpretation relevant suggested links. complete list topics words links found http//www.cs.huji.ac.il/∼amitg/lthm.html. found topics dataset comparable quality assignment topics documents diﬀerent lthm especially short documents. table shows comparison document topic vector short wikipedia documents journal machine learning random forests lthm. topics shown. since lthm uses link information assigns weight relevant topics. quantitative evaluation compare lthm topic models link-plsa link-lda frequency-based method task link prediction. frequency-based method ranks documents corpus according number time linked documents. ranking serves link prediction documents. prediction documents corpus depend topics source document. experiments wikipedia dataset webkb dataset. webkb dataset consists html pages. dataset used dictionary words extracted links ends links belong webkb corpus. split data train consisting documents test remaining training text documents provided algorithms links originating documents train visible training. dataset learned hidden aspects. test test document sort documents corpus according probability link figures show several measures performance diﬀerent algorithms. ﬁrst measure percentage documents test least link prediction among true link. motivation measure following question suppose want suggest author figure four example topics learned lthm links related them. topic probable words shown along links probable generated topic. next word link probability generated given topic. table comparison document topic vector short wikipedia documents journal machine learning random forests lthm. topics shown. since lthm uses link information assigns weight relevant topics. figure shows lthm outperforms three methods respect three performance measures. link-plsa link-lda worse frequency-based method. result seem surprising ﬁrst methods general frequency-based method. particular could relative frequency document probability drawn topic. case would predict frequencybased method. inspect performance methods train linkplsa link-lda better frequencybased method. suggests link-plsa linklda overﬁt large number free parameters models modeling links. lthm hand additional parameters modeling links. moreover link generation probabilities depend topic mixtures documents ends link. unlike link-plsa linklda values link parameters cancel dependency. mentioned section thanks symmetries lthm iteration computed time linear size copus times number topics. training webkb dataset topics took hours iterations. training smaller wikipedia dataset topcis took minutes iterations. work presented lthm novel topic model hypertext documents. lthm generation hyperlinks depends topics source word target document link well relative importance target document. compared previous approaches lthm introduces much smaller number additional link parameters. result lthm achieves good generalization results cases models overﬁt fail generalize.", "year": 2012}