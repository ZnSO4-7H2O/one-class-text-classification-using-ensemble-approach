{"title": "RETAIN: An Interpretable Predictive Model for Healthcare using Reverse  Time Attention Mechanism", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "Accuracy and interpretability are two dominant features of successful predictive models. Typically, a choice must be made in favor of complex black box models such as recurrent neural networks (RNN) for accuracy versus less accurate but more interpretable traditional models such as logistic regression. This tradeoff poses challenges in medicine where both accuracy and interpretability are important. We addressed this challenge by developing the REverse Time AttentIoN model (RETAIN) for application to Electronic Health Records (EHR) data. RETAIN achieves high accuracy while remaining clinically interpretable and is based on a two-level neural attention model that detects influential past visits and significant clinical variables within those visits (e.g. key diagnoses). RETAIN mimics physician practice by attending the EHR data in a reverse time order so that recent clinical visits are likely to receive higher attention. RETAIN was tested on a large health system EHR dataset with 14 million visits completed by 263K patients over an 8 year period and demonstrated predictive accuracy and computational scalability comparable to state-of-the-art methods such as RNN, and ease of interpretability comparable to traditional models.", "text": "accuracy interpretability dominant features successful predictive models. typically choice must made favor complex black models recurrent neural networks accuracy versus less accurate interpretable traditional models logistic regression. tradeoff poses challenges medicine accuracy interpretability important. addressed challenge developing reverse time attention model application electronic health records data. retain achieves high accuracy remaining clinically interpretable based two-level neural attention model detects inﬂuential past visits signiﬁcant clinical variables within visits retain mimics physician practice attending data reverse time order recent clinical visits likely receive higher attention. retain tested large health system dataset million visits completed patients year period demonstrated predictive accuracy computational scalability comparable state-of-the-art methods ease interpretability comparable traditional models. broad adoption electronic health record systems opened possibility applying clinical predictive models improve quality clinical care. several systematic reviews underlined care quality improvement using predictive analysis data represented temporal sequences high-dimensional clinical variables sequence ensemble represents documented content medical visits single patient. traditional machine learning tools summarize ensemble aggregate features ignoring temporal sequence relationships among feature elements. opportunity improve predictive accuracy interpretability likely derive effectively modeling temporality high-dimensionality event sequences. accuracy interpretability dominant features successful predictive models. common belief trade accuracy interpretability using three types traditional models identifying rules case-based reasoning ﬁnding similar patients distance metric learning identifying list risk factors interpretable models rely aggregated features ignoring temporal relation among features inherent data. consequence model accuracy sub-optimal. latent-variable time-series models account temporality often limited interpretation abstract state variables. recently recurrent neural networks successfully applied modeling sequential data predict diagnoses model encounter sequences gain accuracy figure common attention models retain using folded diagrams rnns. standard attention mechanism recurrence hidden state vector hinders interpretation model. attention mechanism retain recurrence attention generation components hidden state generated simpler interpretable output. rnns cost model output notoriously difﬁcult interpret. several attempts directly interpreting rnns methods sufﬁciently developed application clinical care. addressed limitation using modeling strategy known retain two-level neural attention model sequential data provides detailed interpretation prediction results retaining prediction accuracy comparable rnn. retain relies attention mechanism modeled represent behavior physicians encounter. distinguishing feature retain leverage sequence information using attention generation mechanism learning interpretable representation. emulating physician behaviors retain examines patient’s past visits reverse time order facilitating stable attention generation. result retain identiﬁes meaningful visits quantiﬁes visit speciﬁc features contribute prediction. retain tested large health system dataset million visits completed patients year period. compared predictive accuracy retain traditional machine learning methods variants using case-control dataset predict future diagnosis heart failure. comparative analysis demonstrates retain achieves comparable performance accuracy speed signiﬁcantly outperforms traditional models. moreover using concrete case study visualization method demonstrate retain offers intuitive interpretation. ﬁrst describe structure sequential data notation follow general framework predictive analysis healthcare using followed details retain method. structure notation. data patient represented timelabeled sequence multivariate observations. assuming different variables n-th patient total patients represented sequence tuples timestamps denotes time i-th visit n-th patient number visits n-th patient. minimize clutter describe algorithms single patient dropped superscript whenever unambiguous. goal predictive modeling predict label time step sequence number labels one. example encounter sequence modeling visit patient’s visit sequence represented varying number medical codes cn}. j-th code vocabulary therefore number variables input }|c| binary vector value j-th coordinate indicates documented i-th visit. given sequence visits goal time step predict codes occurring next visit number labels |c|. case learning diagnose input vector consists continuous clinical measures. different measurements goal given input sequence predict occurrence speciﬁc disease multiple diseases without loss generality describe algorithm seen special case make single prediction visit sequence. rest section abstract symbol denote recurrent neural network variants cope vanishing gradient problem lstm irnn depth attention based neural network models successfully applied image processing natural language processing speech recognition utility attention mechanism seen language translation task inefﬁcient represent entire sentence ﬁxed-size vector neural translation machines ﬁnds difﬁcult translate given sentence represented single vector. intuitively attention mechanism language translation works follows given sentence length original language generate represent words sentence. j-th word target language generate attentions word predict j-th word target language. general attention mechanism allows model focus speciﬁc word given sentence generating word target language. rely conceptually similar temporal attention mechanism generate interpretable prediction models using data. model framework motivated mimics doctors attend patient’s needs explore patient record focus speciﬁc clinical information working present past. figure shows high-level overview model central feature delegate considerable portion prediction responsibility process generating attention weights. intended address part difﬁculty interpreting rnns recurrent weights feed past information hidden layer. therefore consider visit-level variablelevel inﬂuence linear embedding input vector deﬁne denotes embedding input vector size embedding dimension wemb rm×r embedding matrix learn. alternatively sophisticated interpretable representations derived multilayer perceptron used representation learning data sets weights visit-level attention variable-level attention respectively. scalars visit-level attention weights govern inﬂuence visit embedding vectors variable-level attention weights focus coordinate visit embedding vim. rnns rnnα rnnβ separately generate follows figure unfolded view retain’s architecture given input sequence predict label step embedding step generating values using rnnα step generating values using rnnβ step generating context vector using attention representation vectors step making prediction. note steps reversed time. hidden layer rnnα time step hidden layer rnnβ time step rm×q parameters learn. hyperparameters determine hidden layer size rnnα rnnβ respectively. note prediction timestamp generate attention vectors simplicity notation include index predicting different time steps. step sparsemax instead softmax sparser attention weights. noted retain generates attention vectors running rnns backward time; i.e. rnnα rnnβ take visit embeddings reverse order running reversed time order also offers computational advantages since reverse time order allows generate dynamically change values making predictions different time steps ensures attention vectors modiﬁed time step increasing computational stability attention generation process. using generated attentions obtain context vector patient i-th visit follows denotes element-wise multiplication. context vector predict true label follows rs×m parameters learn. cross-entropy calculate classiﬁcation loss follows change cross-entropy example mean squared error. overall attention mechanism viewed inverted architecture standard attention mechanism words encoded attention weights generated mlp. contrast method uses embed visit information preserve interpretability uses generate sets attention weights recovering sequential information well mimicking behavior physicians. note timestamp visit formulation. using timestamps however provides small improvement prediction performance. propose method timestamps appendix example feeding visit embeddings original order rnnα rnnβ generate every time step moreover many cases patient’s recent visit records deserve attention records. need makes process computationally unstable long sequences. finding visits contribute prediction derived using largest straightforward. however ﬁnding inﬂuential variables slightly involved visit represented ensemble medical variables vary predictive contribution. contribution variable determined interpretation alone informs visit inﬂuential prediction why. propose method interpret end-to-end behavior retain. keeping values ﬁxed attention doctors analyze changes probability label relation changes original input xir. yields largest change input variable highest contribution. formally given sequence trying predict probability output vector expressed follows k-th element input vector completely deconstructed variables input allows calculating contribution k-th variable input time step predicting follows index omitted described section generating time step visit sequence therefore index always assumed β’s. additionally shows using binary input value coefﬁcient contribution. however using non-binary input value need multiply coefﬁcient input value correctly calculate contribution. compared performance retain rnns traditional machine learning methods. given space constraints report results learning diagnose task summarize encounter sequence modeling appendix retain source code publicly available https//github.com/mp/retain. experimental setting source data dataset consists electronic health records sutter health. patients years adults chosen heart failure prediction model study. encounter records medication orders procedure orders problem lists extracted visit records consisting diagnosis medication procedure codes. reduce dimensionality preserving clinical information used existing medical groupers aggregate codes input variables. details medical groupers given appendix proﬁle dataset summarized table implementation details implemented retain theano training model used adadelta mini-batch patients. training done machine equipped intel xeon nvidia tesla cuda baselines comparison completed following models. logistic regression compute counts medical codes patient based visits input variables normalize vector zero mean unit variance. resulting vector train logistic regression. feature construction hidden layer size hidden layers size implemented gru. input sequences used. logistic regression applied hidden layer. layers match model complexity retain. rnn+αm layer single directional along time generate input embeddings single hidden layer size generate visit-level attentions input embeddings input mlp. baseline corresponds figure rnn+αr similar rnn+αm uses reverse-order generate visit-level attentions baseline conﬁrm effectiveness generating attentions using reverse time order. comparative visualization baselines provided appendix implementation training method baselines described above. details hyperparameters regularization drop-out strategies baselines described appendix evaluation measures model accuracy measured negative log-likelihood measures model loss test set. loss calculated objective given visit sequence predicted primary care patient diagnosed heart failure special case single disease outcome sequence. since binary prediction task logistic sigmoid function instead softmax step cohort construction source dataset cases selected approximately controls selected case case/control selection criteria fully described supplementary section. cases index dates denote date diagnosed controls index dates corresponding cases. extract diagnosis codes medication codes procedure codes -months window index date. training details patient cohort divided training validation test sets ratio. validation used determine values hyper-parameters. appendix details hyper-parameter tuning. results logistic regression underperformed compared four temporal learning algorithms retain comparable variants terms prediction performance offering interpretation beneﬁt. note rnn+αr model degenerated version retain scalar attention still competitive model shown table conﬁrms efﬁciency generating attention weights using rnn. however rnn+αr model provides scalar visit-level attention sufﬁcient healthcare applications. patients often receives several medical codes single visit important distinguish relative importance target. show case study section table also shows scalability retain training time comparable rnn. test time number seconds generate prediction output entire test set. mini-batch patients assessing training test times. takes longer rnn+αm two-layer structure whereas rnn+αm uses single layer rnn. models rnns take similar time train epoch. however model required different number epochs converge. typically takes approximately epochs rnn+αm rnn+αr epochs retain epochs. lastly training attention models would take considerably longer modeling generates context vectors time step. hand require additional computation embedding visit hidden layer predict target labels time step. therefore training time attention models increase linearly relation length input sequence. model interpretation heart failure prediction evaluated interpretability retain prediction task choosing patient test calculating contribution variables diagnostic prediction. patient suffered skin problems skin disorder benign neoplasm excision skin lesion time showing symptoms cardiac dysrhythmia heart valve disease coronary atherosclerosis diagnosis skin-related codes earlier visits made little contribution prediction expected. retain properly puts much attention hf-related codes occurred recent visits. conﬁrm retain’s ability exploit sequence information data reverse visit sequence figure feed retain. figure shows contribution medical codes reversed visit record. hf-related codes past still making positive contributions much figure figure also emphasizes retain’s superiority interpretable stationary models logistic regression. stationary models often aggregate past information remove temporality input data mistakenly lead risk prediction figure retain however correctly digest sequence information calculates risk score signiﬁcantly lower figure figure shows contributions codes change selected medication data used model. added medications antiarrhythmics anticoagulants used treat cardiac dysrhythmia medications make negative contributions especially towards record. medications decreased positive contributions heart valve disease cardiac dysrhythmia last visit. indeed risk figure temporal visualization patient’s visit records contribution variables diagnosis heart failure summarized along x-axis y-axis indicating magnitude visit code speciﬁc contributions diagnosis. reverse order visit sequence retain properly take account modiﬁed sequence information. medication codes added visit record changes behavior retain. prediction figure lower figure suggests taking proper medications help patient reducing risk. conclusion approach modeling event sequences predictors diagnosis suggest complex models offer superior predictive accuracy precise interpretability. given power rnns analyzing sequential data proposed retain preserves rnn’s predictive power allowing higher degree interpretation. idea retain improve prediction accuracy sophisticated attention generation process keeping representation learning part simple interpretation making entire algorithm accurate interpretable. retain trains reverse time order efﬁciently generate appropriate attention variables. future work plan develop interactive visualization system retain evaluating retain healthcare applications. references mnih kavukcuoglu. multiple object recognition visual attention. iclr bahdanau bengio. neural machine translation jointly learning align translate. chaudhry wang maglione mojica roth morton shekelle. systematic review impact health information technology quality efﬁciency costs medical care. annals internal medicine fleisher sowell taylor gamst petersen thal study. clinical predictors progression alzheimer disease amnestic mild cognitive impairment. neurology gallego walter dunn sivaraman shah longhurst coiera. bringing cohort studies bedside framework ’green button’ support clinical decision-making. journal comparative effectiveness research pages building high-level features using large scale unsupervised learning. icassp jaitly hinton. simple initialize recurrent networks rectiﬁed linear units. before represent timestamp i-th visit n-th patient. following suppress superscript avoid cluttered notation. note timestamp anything provides temporal information i-th visit number days ﬁrst visit number days consecutive visits number days index date event heart failure diagnosis. order timestamps modify step step section follows concatenation visit embedding timestamp generate attentions however obtaining context vector step match dimensionality. entire process could understood temporal information embed visit calculate attentions entire visit sequence. consistent modeling approach lose sequential information embedding visit recover sequential information generating attentions using rnn. using temporal information speciﬁcally number days ﬁrst visit able improve heart failure prediction without hyper-parameter tuning. used validation tune hyper-parameters visit embedding size rnnα’s hidden layer size rnnβ’s hidden layer size regularization coefﬁcient drop-out rates. regularization applied weights except ones rnnα rnnβ. separate drop-outs used visit embedding context vector performed random search predeﬁned ranges dropoutvi dropoutci also performed random search ﬁxed ﬁnal value used train retain heart failure prediction dropoutvi dropoutci regularization coefﬁcient. diagnosis codes medication codes procedure codes dataset respectively using international classiﬁcation diseases generic product identiﬁer current procedural terminology diagnosis codes grouped clinical classiﬁcations software icd- reduces number diagnosis code approximately medication codes grouped generic product identiﬁer drug group reduces dimension approximately procedure codes grouped clinical classiﬁcations software reduces number codes approximately https//www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp http//www.wolterskluwercdi.com/drug-data/medi-span-electronic-drug-ﬁle/ https//www.hcup-us.ahrq.gov/toolssoftware/ccs_svcsproc/ccssvcproc.jsp rnn+αm drop-out rate output hidden layer output αivi. regularization coefﬁcient hidden layer weight generates logistic regression weight. dimension size hidden layers rnn+αr drop-out rate output hidden layer output αivi. regularization coefﬁcient hidden layer weight generates logistic regression weight. dimension size hidden layers rnns case patients years time diagnosis. diagnosis deﬁned qualifying icd- codes appeared encounter records medication orders. qualifying icd- codes displayed table minimum three clinical encounters qualifying icd- codes occur within months other date diagnosis assigned earliest three dates. time span ﬁrst second appearances diagnostic code greater months date second encounter used ﬁrst qualifying encounter. date diagnosis given case denoted hfdx. eligible controls selected case yielding overall ratio controls case. control also assigned index date hfdx matched case. controls selected meet operational criteria diagnosis prior hfdx plus days corresponding case. control subjects required ﬁrst ofﬁce encounter within year matching case patient’s ﬁrst ofﬁce visit least ofﬁce encounter days time case’s diagnosis date ensure similar duration observations among cases controls. objective given sequence visits goal encounter sequence modeling time step predict codes occurring next visit experiment focus predicting diagnosis codes encounter sequence create separate labels contain non-diagnosis codes medication codes procedure codes. therefore contain diagnosis codes next visit xi+. dataset divide entire dataset described table ratio respectively training validation test set. baseline baseline models used prediction. however since predicting binary labels replace logistic regression function softmax function. drop-out regularization policies remain same. step aggregate maximum past input vectors create evaluation metric negative likelihood test evaluate model performance. also recallk additional metric measure prediction accuracy. recallk given sequence visits evaluate model performance based accurately predict diagnosis codes average recallk expressed below argsort returns list indices decrementally sort given vector nonzero returns list indices coordinates non-zero values. recallk similar nature human physician performs differential diagnostic procedure generate list likely diseases undiagnosed patient perform medical practice true disease diseases determined. prediction accuracy table displays prediction performance retain baselines. recallk allow reasonable number prediction trials well cover complex patients often receive multiple diagnosis codes single visit. shows best prediction accuracy encounter diagnosis prediction. however considering purpose encounter diagnosis prediction assist doctors provide quality care patient black-box behavior makes unattractive clinical tool. hand retain performs well attention models slightly inferior provides full interpretation prediction behavior making feasible solution clinical applications. interesting ﬁnding table able perform accurately rnn+αm terms recall. considering fact uses aggregated information past visits assume encounter diagnosis prediction depends frequency disease occurrences rather order occurred. quite different prediction task stationary models performed signiﬁcantly worse sequential models. figure graphical illustration baselines logistic regression multilayer perceptron recurrent neural network attention vectors generated attention vectors generated retain given figure", "year": 2016}