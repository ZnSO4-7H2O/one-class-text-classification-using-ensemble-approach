{"title": "Modeling Label Ambiguity for Neural List-Wise Learning to Rank", "tag": ["cs.IR", "cs.NE", "stat.ML"], "abstract": "List-wise learning to rank methods are considered to be the state-of-the-art. One of the major problems with these methods is that the ambiguous nature of relevance labels in learning to rank data is ignored. Ambiguity of relevance labels refers to the phenomenon that multiple documents may be assigned the same relevance label for a given query, so that no preference order should be learned for those documents. In this paper we propose a novel sampling technique for computing a list-wise loss that can take into account this ambiguity. We show the effectiveness of the proposed method by training a 3-layer deep neural network. We compare our new loss function to two strong baselines: ListNet and ListMLE. We show that our method generalizes better and significantly outperforms other methods on the validation and test sets.", "text": "abstract list-wise learning rank methods considered stateof-the-art. major problems methods ambiguous nature relevance labels learning rank data ignored. ambiguity relevance labels refers phenomenon multiple documents assigned relevance label given query preference order learned documents. paper propose novel sampling technique computing list-wise loss take account ambiguity. show eectiveness proposed method training -layer deep neural network. compare loss function strong baselines listnet listmle. show method generalizes better signicantly outperforms methods validation test sets. introduction important components search engine learning rank model. considers dozens even hundreds relevance signals determines order show documents user based signals. following three main directions emerged permission make digital hard copies part work personal classroom granted without provided copies made distributed prot commercial advantage copies bear notice full citation page. copyrights third-party components work must honored. uses contact owner/author. sigir workshop neural information retrieval august shinjuku tokyo japan copyright held owner/author. -x-xxxx-xxxx-x/yy/mm. ./nnnnnnn.nnnnnnn neural networks. particular focus listnet listmle major diculties list-wise methods consideration ambiguity exists data uses relevance scores. figure relevance labels admit many dierent correct rankings query colors indicate different relevance grades green highly relevant blue relevant relevant. typically documents relevance labels necessarily introduces ambiguity rankings. documents share relevance label freely interchanged thus permutation documents relevance label technically correct. consequence many possible rankings documents would considered correct query brings problem label ambiguity phenomenon multiple documents assigned relevance label given query preference order learned documents. learning preference none exists lead overtting limitations learnerâ€™s ability generalize. listnet computationally expensive model label ambiguity every possible permutation considered. address problem introducing top-k approximation. however argue largely mitigates major attractiveness listnet namely capability learn full ranked list. listmle make simplifying function maps real-valued scores positive numbers. practice usually chosen exponential function. sake simplifying derivations section also choose exponential function. hence assume exp. consider methods build distribution listnet listmle shown losses associated methods continuous convex dierentiable makes easy optimize stochastic gradient descent attractive neural setting. distribution mapping relevance labels mapping generates score vector ground truth retains order relevance labels permutation places documents high relevance gets assigned high probability. possible permutations. compute cross entropy loss consider every possible permutation size resort top-k approximation usually make computation feasible. stochastic top-k listnet variant proposed paper authors sample within top-k subgroups speed training. dierent work completely eliminate need top-k approximation. listmle listmle replaces optimization objective listnet simpler form. simplifying assumption single permutation chosen considered ground truth. directly optimizes probability introduce novel optimize list-wise neural model sampling learning instances directly plackett-luce distribution take account ambiguity relevance labels setting. publish source code method uses chainer gpu-accelerated deep learning framework promoting future research neural list-wise methods. remainder paper organized follows. section introduces notation earlier work list-wise approaches. section discusses details phenomena label ambiguity. present solution section present experimental results section finally conclude section related work section discuss several important works method builds first introduce notation used throughout paper next briey discuss plackett-luce distribution followed listnet listmle preliminaries consider scenario learning rank given collection queries query associated documents corresponding relevance labels document d-dimensional feature vector representing query-document pair. sake brevity drop superscript notation remainder paper. scoring function score every document sort documents scores. objective function resulting ranking optimal regards relevance labels. words wish function assigns high scores documents high relevance scores documents little relevance. plackett-luce distribution plackett-luce distribution used extensively probabilistic list-wise methods. based idea ranking drawn sequentially list item-specic scores item time. distribution probability distribution possible permutations item scores. intuitively assigns high probability permutations place high-scoring items assigning probability permutations place high-scoring items bottom. formally probability permutation given scoring dierent look listpl like data generation method. essentially applying listmle data contains permutations possible rankings weighing looking method perspective reasonable assume sampling-based usual sample ranking uniformly weigh gradient however runs problem sample space enormous suciently many documents query uniformly sampled rankings probability close resulting slow convergence rate. section described method listpl able deal label ambiguity problem described section next present experimental evaluation. experiments results validate eectiveness method listpl compare listnet listmle mslr-webk data data contains queries. represents query-document pairs -dimensional feature vectors grades scale queries training validation testing. architecture network -layer fully connected neural network relu activation units hidden layer. experimented layers found negligible improvements. keep network architecture vary loss function figure shows results mslr-webk data set. evaluate performance methods using ndcg natural evaluation metric web-search setting. listpl performs similar listnet training main drawbacks listmle assumes single perfect ranking known. assumption however hold data sets ambiguous relevance labels. summarize listnet listmle build distribution provide elegant probabilistic ways learn list-wise model. distinction work compared previous eorts introduce method properly deals label ambiguity problem describe next. ambiguity ranking existing work list-wise approaches typically ignores ambiguity labels. instance listmle samples single permutation ground truth whose ordering assumed ground truth ranked list. example take following documents corresponding relevance scores instead optimization objective listmle learns following thus learning algorithm attempt learn problematic according ground truth relative ordering meaningful. attempting learn overly specic relations harmful generalization power learning algorithm. next present method overcomes described issue. instead naively choosing single permutation documents considering permutation ground truth propose sophisticated sampling method. main idea directly sample ranking distribution relevance labels every stochastic update. loss interpreted stochastic variant listmle loss. sample possible permutation corresponding probability sample compute stochastic loss. call method listpl figure ndcg performance mslr-webk. shaded areas indicate standard deviation dierent folds. test performance improvements listnet listmle statistically signicant respectively outperforms listnet listmle validation testing. performance degradations validation test listnet listmle occur epochs indicate methods overtting eectively learning noise coming label ambiguity. results line expectations listpl properly deals ambiguity relevance scores thus generalizes better. ndcg performance test evaluated using fold cross validation. performance improvement listpl listnet statistically signicant whereas performance improvement listmle statistically signicant summarize based extensive experimentation mslrwebk data conclude listpl signicantly outperforms strong baselines fact handles label ambiguity problem well hence generalizes better. overall conclusion introducing sampling method based directly sampling plackett-luce distribution relevance labels able increase ability generalize neural list-wise methods maintaining eciency. specically used modied loss function eciently mitigate problem label ambiguity thereby improve existing list-wise neural methods. extensive experimentation mslr-webk data showed method listpl signicantly outperforms strong baselines listnet listmle method baselines implemented using chainer gpu-accelerated deep learning framework. sharing source code listpl online hope useful future research towards list-wise neural methods. https//github.com/rjagerman/shoelace acknowledgments. research supported ahold delhaize amsterdam data science bloomberg research grant program criteo faculty research award program dutch national program commit elsevier european communityâ€™s seventh framework programme grant agreement microsoft research ph.d. program netherlands institute sound vision netherlands organisation scientic research project hor-- ci-- yandex. content represents opinion authors necessarily shared endorsed respective employers and/or sponsors. seiya tokui kenta oono shohei hido justin clayton. chainer next-generation open source framework deep learning. proceedings workshop machine learning systems twenty-ninth annual conference neural information processing systems", "year": 2017}