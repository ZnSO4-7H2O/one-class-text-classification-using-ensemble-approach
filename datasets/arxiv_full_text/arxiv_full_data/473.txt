{"title": "Dynamic Integration of Background Knowledge in Neural NLU Systems", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "Common-sense or background knowledge is required to understand natural language, but in most neural natural language understanding (NLU) systems, the requisite background knowledge is indirectly acquired from static corpora. We develop a new reading architecture for the dynamic integration of explicit background knowledge in NLU models. A new task-agnostic reading module provides refined word representations to a task-specific NLU architecture by processing background knowledge in the form of free-text statements, together with the task-specific inputs. Strong performance on the tasks of document question answering (DQA) and recognizing textual entailment (RTE) demonstrate the effectiveness and flexibility of our approach. Analysis shows that our models learn to exploit knowledge selectively and in a semantically appropriate way.", "text": "common-sense background knowledge required understand natural language neural natural language understanding systems requisite background knowledge indirectly acquired static corpora. develop reading architecture dynamic integration explicit background knowledge models. task-agnostic reading module provides reﬁned word representations task-speciﬁc architecture processing background knowledge form free-text statements together taskspeciﬁc inputs. strong performance tasks document question answering recognizing textual entailment demonstrate effectiveness ﬂexibility approach. analysis shows models learn exploit knowledge selectively semantically appropriate way. understanding natural language depends crucially common-sense background knowledge example knowledge concepts expressed words read relations hold concepts simple illustration agent needs understand statement king farouk signed abdication entailed king farouk exiled france signing resignation must know abdication means resignation king. neural natural language understanding systems requisite background knowledge implicitly encoded models’ parameters. background knowledge present learned task supervision also pre-training word embeddings however acquisition background knowledge static training corpora limiting reasons. first cannot expect background knowledge could important solving task extracted limited amount training data. second world changes facts inﬂuence text understood likewise change. short building suitably large corpora capture relevant information keeping corpus derived models date changes world would impractical. paper develop architecture dynamically incorporating external background knowledge models. rather relying static knowledge implicitly present training data supplementary knowledge retrieved knowledge base assist understanding text inputs. since systems must necessarily read understand text inputs approach incorporates background knowledge repurposing reading machinery—that read text understood together supplementary natural language statements assert facts relevant understanding content knowledge-augmented systems operate series phases. first given text input system must understand call context relevant supporting assertions retrieved. learning retrieve relevant information solving tasks important question work focus learning incorporate retrieved information simple heuristic retrieval methods identify plausibly relevant background external knowledge base. supplementary texts retrieved word embedding reﬁnement strategy incrementally reads context retrieved assertions starting context-independent word embeddings building successively reﬁned embeddings words ultimately reﬂect relevant supporting assertions input context contextually reﬁned word embeddings serve dynamic memory store newly incorporated knowledge used task-speciﬁc reading architecture. overall architecture illustrated figure although incorporating kind information pipeline strength approach architecture reading module independent ﬁnal task—the requirement ﬁnal architecture word embeddings. carry experiments several different datasets tasks document question answering recognizing textual entailment evaluating impact proposed solution basic task architectures sophisticated task architecture embedding reﬁnement strategy quite effective four standard benchmarks show reﬁnement helps—even reﬁning embeddings using context improve performance signiﬁcantly adding background knowledge helps further. results competitive setting state-of-the-art recent triviaqa benchmarks remarkable considering simplicity chosen task-speciﬁc architecture. finally provide detailed analysis knowledge used system including experiments showing system capable making appropriate counterfactual inferences provided false knowledge. knowledge resources make information could potentially useful improving available variety different formats -triples relational databases structured formats. rather tailoring solution particular structured representation assume supplementary information either already exists natural language statements easily recoded natural language. contrast mapping unstructured structured representations inverse problem terribly difﬁcult. example given triple construct free-text assertion monkey animal using simple rules. finally free-text format means knowledge exists unstructured text form usable system. major question remains answered given text understood supplementary knowledge incorporated? retrieval contextually relevant information knowledge sources complex research topic itself likewise crucially dependent format underlying knowledge base. several statistical recently neural approaches approaches based reinforcement learning work make simple heuristic almost exhaustively retrieve potentially relevant assertions rely reading architecture learn extract relevant information. order incorporate information retrieved input texts propose compute contextually reﬁned word representations prior processing task hand pass task form word embeddings. word embeddings thus serve form memory contains general-purpose knowledge also contextual information incremental reﬁnement process encodes input texts followed updates word embedding matrix using encoded input multiple reading steps. words ﬁrst represented non-contextually conceived columns embedding matrix progressive reading step embedding matrix constructed reﬁning embeddings previous step using contextual information reading step natural language sequences illustration incremental reﬁnement strategy found figure figure illustration context-dependent reﬁnement strategy word representations example snli dataset comprising premise hypothesis additional external information form free-text assertions. reading architecture constructs reﬁnements word representations incrementally incrementally reﬁned reading input text textual renderings relevant background knowledge computing representations used task model ﬁrst representation level consists non-contextual word representations word representations depend input; conceived embedding matrix whose columns indexed words non-contextual word representation single word computed using gated combination ﬁxed pre-trained word vectors formal deﬁnition combination given learned character-based embeddings echar compute echar using single-layer convolutional neural network using convolutional ﬁlters width followed max-pooling operation time. combining pre-trained character based word embeddings common practice. approach follows contextually refined word representations order compute contextually reﬁned word embeddings given prior representations assume given texts read reﬁnement iteration text using embedding matrix sequence word tokens. embed tokens every previous layer word concatenate one-hot vector length position indicating layer currently processed. stacking vectors matrix matrix processed bidirectional recurrent neural network obtain bilstm work. resulting output projected ﬁnally update previous embedding word initially maxpool representations occurrences matching lemma every resulting finally combine context-independent representation form context-sensitive representation gated addition lets model determine much revise embedding newly read information note soften matching condition using lemmatization lemma pooling operation contextual information certain words usually independent current word form appear consequence minor linguistic pre-processing step allows additional interaction tokens lemma. important difference contextual reﬁnement step conventional multi-layer architectures pooling operation performed occurrences tokens share lemma. effectively connects different positions within different texts thereby mitigating problems arising long-distance dependencies. importantly however allows models make additional input relevant background knowledge. experiments four benchmarks popular tasks namely recognizing textual entailment document question answering following describe different aspects experimental setup detail. task-speciﬁc models primary interest explore value reﬁnement strategy relatively generic task architectures. therefore chose basic single-layer bidirectional lstms encoders task-speciﬁc feed-forward neural network ﬁnal prediction. models common baselines tasks considered general reading architectures opposed highly tuned task-speciﬁc systems necessary achieve state-of-the results. however since models frequently underperform customized architectures also reﬁnement module reimplementation state-of-the-art architecture called esim models trained end-to-end jointly reﬁnement module. baseline system simple lemma-in-question feature suggested weissenborn encoding context compare competitive baseline results. provide exact model implementations bilstm baselines general training details appendix question answering apply models recent benchmark datasets squad triviaqa task predict answer span within provided document given question datasets large-scale containing order examples. triviaqa collected distant supervision test divided large noisy distant supervision part much smaller human veriﬁed part. report results both. appendix implementation details. recognizing textual entailment test frequently used snli dataset collection sentence pairs recent multinli dataset given sentences premise hypothesis task determine whether either entails contradicts neutral appendix implementation details. knowledge source make conceptnet freely-available multi-lingual semantic network originated open mind common sense project incorporates selected knowledge various knowledge sources wiktionary open multilingual wordnet opencyc dbpedia. presents information form relational triples. assertion retrieval would like obtain information relations words phrases conceptnet order strengthen connection sequences. assertions conceptnet come form -triples retrieve assertions appears appears vice versa. still many assertions might retrieved instance rank retrievals based respective subject object. compute ranking score inverse product appearances subject object score denotes indicator function. related popular score information retrieval ranks terms higher appear less frequently across different documents. training evaluation retain top-k assertions specify individual experiments separately. note might happen assertions retrieved all. reﬁnement order employing embedding-reﬁnement strategy ﬁrst read document followed question case premise followed hypothesis {q}. additional knowledge form assertions integrated reading task-speciﬁc input preliminary experiments found ﬁnal performance signiﬁcantly sensitive order presentation decided order deﬁned above. table results squad development well triviaqa-wikipedia -web test sets −dimensional models. triviaqa results divided distant supervision results human veriﬁed results model using external knowledge trained top-/ retrieved conceptnet assertions. liq-feature used baseline. wang table presents results question answering benchmarks. report results squad development challenging triviaqa test sets demonstrate introduction reading architecture helps consistently additional gains using background knowledge. systems even outperform current state-of-the-art models triviaqa surprising given simplicity task-speciﬁc architecture complexity others. instance system uses complex multi-hop attention mechanism achieve results. even baseline bilstm system reaches competitive results triviaqa line ﬁndings weissenborn verify additional computation gives performance boosts using reading architecture also experiments -layer bilstms baselines exhibit similar computational complexity bilstm reading. found second layer even hurts performance. demonstrates pooling word/lemma occurrences given context between layers constitutes main difference conventional stacked rnns powerful simple technique. case important ﬁnding experiments knowledge actually helps considerably ./.% improvements f/exact measures. table shows results experiments. general introduction reﬁnement strategy almost always helps without external knowledge. providing additional background knowledge conceptnet bilstm based models improve substantially esim-based models improve difﬁcult multinli dataset. compared previously published state-of-the-art systems models acquit well multinli benchmark competitively snli benchmark. parallel work gong developed novel task-speciﬁc architecture achieves slightly better performance multinli esim+reading+knowledge based models. worth observing knowledge-enhanced embedding architecture generic bilstm-based task model outperforms esim multinli architecturally much complex designed speciﬁcally task. finally remark despite careful tuning re-implementation esim fails http//conceptnet.io/ http//wiktionary.org/ http//compling.hss.ntu.edu.sg/omw/ http//dbpedia.org/ exclude conceptnet assertions created contributor verbosity reduce noise. restrictions code sharing able public evaluation server obtain test scores squad. however remaining tasks report development accuracy held-out test performance. table results snli well multinli-matched -mismatched −dimensional models. model using external knowledge trained top- retrieved conceptnet assertions. chen gong chen table development results reducing training data embedding dimsensionality pca. parenthesis report relative differences respective result directly above. match reported chen however multinli implementation esim performs considerably better instability results suggests well failure custom rte-architecture consistently perform well suggests current sota models overﬁt snli dataset. little impact using external knowledge task using sophisticated task model esim. hypothesize attention mechanisms within esim jointly powerful pre-trained word representations allow recovery important lexical relations trained large dataset. follows reducing number training data impoverishing pre-trained word representations impact using external knowledge become larger. test hypothesis gradually impoverish pre-trained word embeddings reducing dimensionality reducing number training instances time. joint data dimensionality reduction results presented table show indeed slightly larger beneﬁt employing background knowledge impoverished settings largest improvements using novel reading architecture using around examples reduced dimensionality however observe biggest overall impact baseline esim model stems contextual reﬁnement strategy especially pronounced experiments. highlights usefulness reﬁnement strategy even without additional knowledge. contradiction entailment table three examples antonym synonym swapping experiment multinli. p-premise h-hypothesis a-assertion ¯a-swapped assertion. figure performance differences ignoring certain types knowledge i.e. relation predicates evaluation. normalized performance differences measured subset examples assertion respective relation predicate occurs. additional knowledge used? verify whether models make additional knowledge conducted several experiments. first evaluated models trained knowledge tasks providing knowledge test time. ablation drops performance .–.% accuracy multinli squad. indicates model reﬁning representations using provided assertions useful way. models sensitive semantics provided knowledge? previous result show models utilize provided assertions consistent therefore test models sensitivity towards semantics assertions experiment swap synonym antonym predicate provided assertions test time. heuristic retrieval mechanism counterfactuals affect truth inference still expect signiﬁcant impact. performance drop multinli examples either synonym antonym-assertion retrieved bilstm esim model. large drop clearly shows models sensitive semantics provided knowledge. examples prediction changes presented table demonstrate system learned trust presented assertions point make appropriate counterfactual inferences—that change knowledge caused change prediction. knowledge used? establishing models somehow sensitive semantics wanted type knowledge important task. analysis exclude assertions including prominent predicates knowledge base individually evaluating models. results presented figure demonstrate biggest performance drop total stems related assertions. prominent predicate appears much frequently assertions helps connecting related parts input sequences other. believe related assertions offer beneﬁts mainly modeling perspective strongly connecting input sequences thus bridging long-range dependencies similar attention. looking relative drops obtained normalizing performance differences actually affected examples models depend highly presence antonym synonym assertions tasks well partially derived assertions. interesting ﬁnding shows sensitivity models selective wrt. type knowledge task. fact largest relative impact stems antonyms interesting known information hard capture distributional semantics contained pre-trained word embeddings. role background knowledge natural language understanding long remarked especially context classical models however recently begun play role neural network models however previous efforts focused speciﬁc tasks certain kinds knowledge whereas take step towards general-purpose solution integration heterogeneous knowledge systems providing simple general-purpose reading architecture read background knowledge encoded simple natural language statements e.g. abdication type resignation. bahdanau textual word deﬁnitions source information embeddings words. area visual question answering utilize external knowledge form dbpedia comments improve answering ability model. marino explicitly incorporate knowledge graphs image classiﬁcation model. created recall mechanism standard lstm cell retrieves pieces external knowledge encoded single representation conversation model. concurrently dhingra exploit linguistic knowledge using mage-grus adapation grus handle graphs however external knowledge present form triples. main difference approach incorporate external knowledge free text form word level prior processing task hand constitutes ﬂexible setup. exploit knowledge base facts mentioned entities neural language models. bahdanau long create word embeddings on-the-ﬂy reading word deﬁnitions prior processing task hand. pilehvar seamlessly incorporate information word senses representations solving downstream task similar. step seamlessly integrating kinds ﬁne-grained assertions concepts might relevant task hand. another important aspect approach notion dynamically updating wordrepresentations. tracking updating concepts entities sentences dynamic memories active research direction however works typically focus particular tasks whereas approach taskagnostic importantly allows integration external background knowledge. related work includes storing temporary information weight matrices instead explicit neural activations biologically plausible alternative. presented novel task-agnostic reading architecture allows dynamic integration background knowledge neural models. solution based incremental reﬁnement word representations reading supplementary inputs ﬂexible used virtually existing architecture rely word embeddings input. results show embedding reﬁnement using system’s text inputs well supplementary texts encoding background knowledge yield large improvements. particular shown relatively simple task architectures become competitive state-of-the-art task-speciﬁc architectures augmented reading architecture. research conducted internship ﬁrst author deepmind. partially supported german federal ministry education research projects sides bbdc software campus samuel bowman gabor angeli potts christopher christopher manning. large annotated corpus learning natural language inference. emnlp. association computational linguistics qian chen xiaodan zhen-hua ling jiang diana inkpen. recurrent neural network-based sentence encoder gated attention natural language inference. arxiv sosuke kobayashi naoaki okazaki kentaro inui. neural language model dynamically representing meanings unknown words entities discourse. arxiv preprint arxiv. ankit kumar ozan irsoy peter ondruska mohit iyyer james bradbury ishaan gulrajani victor zhong romain paulus richard socher. anything dynamic memory networks natural language processing. icml teng long emmanuel bengio ryan lowe jackie cheung doina precup. world knowledge reading comprehension rare entity prediction hierarchical lstms using external descriptions. emnlp following explain detailed implementation task-speciﬁc baseline models. assume computed contextually reﬁned word representations depending setup embedded input sequences rn×lq rn×lp respectively. word representation update gate initialized bias reﬁne representations slightly beginning training. following before denote hidden dimensionality model fully-connected layer rn×m model trained minimize cross-entropy loss predicted start positions respectively. evaluation extract span best span-score maximum token length encoding analogous encode input sequences bilstms however conditional encoding instead. therefore initially process embedded hypothesis bilstm respective states forward backward lstm initial states forward backward lstm processes embedded premise prediction concatenate outputs forward backward lstms processing premise i.e. connected layer relu activation followed max-pooling operation time resulting hidden state finally used predict label follows probability choosing category {entailment contradiction neutral} deﬁned finally model trained minimize cross-entropy loss predicted category probability distribution pre-processing steps lowercase inputs tokenize additionally make lemmatization described necessary matching. pre-trained word representations -dimensional word-embeddings glove employed adam optimization initial learning-rate halved whenever measure accuracy dropped development minibatches respectively. used mini-batches size rte. additionally regularization make dropout rate computed non-contextual word representations deﬁned dropout mask words batch. models trained different random seeds performance reported.", "year": 2017}