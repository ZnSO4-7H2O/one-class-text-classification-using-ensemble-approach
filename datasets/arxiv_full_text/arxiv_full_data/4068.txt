{"title": "Active Sensing as Bayes-Optimal Sequential Decision Making", "tag": ["cs.AI", "cs.CV"], "abstract": "Sensory inference under conditions of uncertainty is a major problem in both machine learning and computational neuroscience. An important but poorly understood aspect of sensory processing is the role of active sensing. Here, we present a Bayes-optimal inference and control framework for active sensing, C-DAC (Context-Dependent Active Controller). Unlike previously proposed algorithms that optimize abstract statistical objectives such as information maximization (Infomax) [Butko & Movellan, 2010] or one-step look-ahead accuracy [Najemnik & Geisler, 2005], our active sensing model directly minimizes a combination of behavioral costs, such as temporal delay, response error, and effort. We simulate these algorithms on a simple visual search task to illustrate scenarios in which context-sensitivity is particularly beneficial and optimization with respect to generic statistical objectives particularly inadequate. Motivated by the geometric properties of the C-DAC policy, we present both parametric and non-parametric approximations, which retain context-sensitivity while significantly reducing computational complexity. These approximations enable us to investigate the more complex problem involving peripheral vision, and we notice that the difference between C-DAC and statistical policies becomes even more evident in this scenario.", "text": "sensory inference conditions uncertainty major problem machine learning computational neuroscience. important poorly understood aspect sensory processing role active sensing. here present bayes-optimal inference control framework active sensing c-dac unlike previously proposed algorithms optimize abstract statistical objectives information maximization one-step look-ahead accuracy active sensing model directly minimizes combination behavioral costs temporal delay response error sensor repositioning cost. simulate algorithms simple visual search task illustrate scenarios contextsensitivity particularly beneﬁcial optimization respect generic statistical objectives particularly inadequate. motivated geometric properties cdac policy present parametric non-parametric approximations retain context-sensitivity signiﬁcantly reducing computational complexity. approximations enable investigate complex search problem involving peripheral vision notice performance advantage c-dac generic statistical policies even evident scenario. processing especially conditions noise uncertainty non-stationarity human performance often still gold standard important tool brain disposal active sensing goal-directed contextsensitive control strategy prioritizes sensing resources toward rewarding informative aspects environment theoretical models sensory processing presume passiveness considering represent compute given inputs actively intervene input collection process itself especially respect behavioral goals environmental constraints. formal understanding active sensing important advancing neuroscientiﬁc progress also engineering applications developing context-sensitive interactive artiﬁcial agents. well-studied aspect human active sensing saccadic movements early work suggests saccades attracted salient targets differ surround feature dimensions orientation motion luminance color contrast passive explanation take account fact observations made attending task aﬀect ﬁxations decisions follow. recently shift relax constraint passiveness notion saliency reframed probabilistically terms maximizing informational gain given spatial temporal context separately another active formulation proposed saccades chosen maximize greedy one-step look-ahead probability ﬁnding target conditioned self knowledge visual acuity consider scenario observer must produce response based sequentially observed noisy sensory inputs ability choose long collect sensory inputs. bayesian generative model capture observer’s knowledge statistical relationship among hidden causes variables give rise noisy sensory inputs well prior beliefs hidden variables. assume exact bayesian inference recognition model maintain statistically optimal representation hidden state world based noisy data stream. action selection component active vision stochastic control problem agent chooses sensing location number data points collected assume agent optimize process dynamically based ongoing data collection size sensory data exact consequence action perfectly known ahead time. quantifying knowledge gain diﬀerent saccade choices incorporating knowledge sensory noise still limited several respects optimize abstract computational quantities directly relate behavioral goals task constraints relatedly unclear adapt algorithms varying task goals explicit representation time algorithms thus means trading ﬁxation duration number ﬁxations search accuracy. rest paper refer infomax greedy statistical policies sense optimize generic statistical objectives insensitive behavioral objectives contextual constraints. contrast statistical policies propose bayes-optimal framework active sensing call c-dac speciﬁcally assume observer aims optimize context-sensitive objective function takes account behavioral costs temporal delay response error cost switching sensing location another. c-dac uses objective choose collect sensory data based continually updated statistically optimal representation sequentially collected sensory data. framework allows derive behaviorally optimal procedures making decisions acquire sensory inputs move observation location another negotiate exploration-exploitation tradeoﬀ collecting additional data terminating observation process. also compare performance c-dac statistical policies diﬀerent task parameters illustrate scenarios latter perform particularly poorly. finally present approximate value iteration algorithms based lowdimensional parametric non-parametric approximation value function retain contextsensitivity signiﬁcantly reducing computational complexity. sec. describe detail c-dac model. sec. apply model visual search task simulating scenarios c-dac achieves ﬂexible trade-oﬀ speed accuracy eﬀort depending task demands whereas statistical policies fall short forms experimentally testable predictions future investigations. also present approximate value-iteration algorithms extension search problem incorporates peripheral vision. conclude discussion implicaposterior previous time-steps ﬁrst observation location suppose deﬁne value function expected cost associated optimal policy given prior belief initial observation location belief state space stopping region continuation region divided subregions corresponding alternative continuation stopping actions. note optimal decision policy stationary policy value function depends belief state observation location time decision taken time bellman’s dynamic programming principle implies numerical algorithm computing optimal policy guess initial setting value function iterate convergence yields value function section apply active sensing model simple three location visual search task compute exact optimal policy compare performance statistical policies target distractors diﬀer terms likelihood observations received looking them. actions stopping choosing response continuation actions obtaining data point certain observation location. policy produces observation sequence stopping time sequence ﬁxation choices eventual target choice bayes risk minimization framework optimization problem formulated terms minimizing expected cost function e]xs averaged stochasticity true target location data samples assume cost incurred trial takes account temporal delay switch cost response error respectively. accordance typical bayes risk formulation sequential decision problem assume cost function linear combination relevant factors bellman’s dynamic programming equation tells problem optimized time point agent chooses action associated lowest expected cost given current knowledge belief state q-factors stopping actions straight forward csnt rereasonable assumption stating distractor target stimuli diﬀer following ﬁrst present brief description greedy infomax algorithms moving model comparisons. infomax algorithm tries maximize information gained ﬁxation minimizing expected cumulative future entropy. similar deﬁne q-factors cost policy shannon’s entropy. note neither original greedy infomax algorithm provide principled answer stop searching respond. need augmented stop maximum probability location containing target exceeds ﬁxed threshold. come back problem threshold present comparison results. discuss performance diﬀerent models terms behavioral output ﬁrst visually illustrate decision policies belief state represented discretizing two-dimensional belief state space bins dimension although c-dac policy also depends current ﬁxation location show ﬁxating ﬁrst location; representations rotationally symmetric. fig. parameters used c-dac policy statistical policies note simple scenario switch cost infomax policy looks almost like c-dac policy ﬁxate likely location unless strong evidence ﬁxated location contains target case observer stop. greedy policy hand looks completely diﬀerent fact ambiguous sense large belief states policy give unique next ﬁxation location. show instance seemingly random policy note regions policy suggests look either location similarly regions policy suggests look fact performance greedy poor exclude model comparisons below. figure decision policies infomax resembles cdac. blue stop. green ﬁxate location orange ﬁxate location brown ﬁxate location environment threshold infomax greedy fig. shows eﬀects c-dac policy changes diﬀerent parameters task changed. seen ﬁgure stopping region expands cost time increases intuitively makes sense time step costlier observer stop lower level conﬁdence expense higher error rate. similarly case smaller stopping lower level conﬁdence makes sense value additional observation depends noisy data noisier less worthwhile continue observing thus leading lower stopping criterion. lastly arguably interesting case additional switch cost deters algorithm switching even belief given location reduced fact scenario optimizing behavioral objectives turns truly beneﬁcial although infomax approximate c-dac policy switch cost cannot switch cost comes play. next look intuitions policy plots translate output measures terms accuracy response delay number ﬁxations. order stopping threshold infomax policy generous/optimistic setting ﬁrst c-dac policy threshold infomax matches accuracy c-dac compare output measures. choose scenarios switch cost switch cost. simulations algorithm starts uniform prior initial ﬁxation location true target location uniformly distributed. fig. shows accuracy number time steps number switches scenarios. conﬁrming intuition policy plots performance infomax c-dac comparable however switch cost added although accuracy comparable design small improvement search time c-dac notable advantage number switches. behavior infomax policy adapt change behavioral cost function thus incurring overall higher cost. algorithms like infomax maximize abstract statistical objectives lack inherent ﬂexibility adapt changing behavioral goals environmental constraints. even simple visual search example infomax principled setting stopping threshold gave best-scenario outcome adopting stopping policy generated c-dac diﬀerent contexts. since binary search required matching threshold accuracy sensitive w.r.t. threshold settle approximate accuracy match infomax comparable lower c-dac. model formally variant pomdp specifically mixed observability markov decision process diﬀers ordinary pomdp part state space partly hidden partly observable general pomdps hard solve since decision made time step depends past actions observations thus imposing enormous memory requirements. known curse history ﬁrst major hurdle towards practical solution. elegant alleviate belief states serve suﬃcient statistic process history thus requiring maintain single distribution instead entire history. converting pomdp belief-state fact prevalent technique employ. however leads another computational hurdle known curse dimensionality since continuous state-space making tabular representation value function infeasible. work around problem discretize belief state space grid instead ﬁnding value function points belief state simplex ﬁnite number grid points. grid approximation also appealing performance guarantees improve density grid increased evaluate value function points sort interpolation technique however although grid approximation work small state spaces scale well larger practical problems. example used active sensing problem sensing locations uniform grid size complexity. although rich body literature approximate solutions pomdp tackling general well application-speciﬁc approximations inappropriate dealing momdp problem encountered here. furthermore pomdp approximation algorithms focus discounted rewards and/or ﬁnitehorizon problems. formulation fall categories thus require novel approximation schemes. note q-factors resulting value function smooth concave making amenable dimensional approximations. step dimensional representation value function update step value iteration algorithm. speciﬁcally instead recomputing value function grid point generate large number samples uniformly belief state space compute estimate value function locations extrapolate value function everywhere improving parametric stopping costs. find minimum-norm from generate random belief state points evaluate required values using current update using value iteration. find φ)w. repeat steps converges. approximation requires setting several parameters impractical large problems little information available properties true value function. thus also implement nonparametric variation algorithm whereby gaussian process regression estimate value function addition also implement approximations lead considerable computational savings. complexity approximation sensing locations random points chosen step bases. approximation complexity number points used regression. practice approximation algorithms consider converge rapidly though proof holds general case. simulations approximate policy uses random point iteration bases uniformly placed belief simplex unit variance. approximate policy uses unit length scale unit signal strength noisestrength random points used regression. fig. shows exact policy learned approximate policies diﬀerent approximations switch cost notice handcrafted bases good approximation exact policy whereas relaxing parametric form subsequently learning hyperparameters leads slightly poorer robust non-parametric approximation. similar observations made fig. environment added switch cost results shown grid. faster robust approximations motivated apply model complex problems. investigate problem visual search peripheral vision next show model fundamentally diﬀerent existing formulations infomax even cost eﬀort considered. simple three-location visual search problem considered above incorporate possibility peripheral vision general possibility sensor positioned particular location distance-dependent degraded information nearby locations well. therefore consider simple example peripheral vision whereby observer saccade intermediate locations give reduced information either three stimuli. motivated experimental observations humans ﬁxate probable target locations sometimes also center-of-gravity locations intermediate among target locations figure schematics visual search task. general task target amongst distractors drawn scale. task agent ﬁxates target patches given time. task agent ﬁxates blue circle regions given time formally need acuity notion possible gain information stimuli peripheral ﬁxation center quality information decays greater spatial distance away fovea. example task fig. would require continuation action space elethree actions correspond ﬁxating three target locations next three ﬁxating midway between target locations last ﬁxating center three. parameterize quality peripheral vision augmenting observations three-dimensional corresponding three simultaneously viewed locations. assume generated bernoulli distribution favoring target magnitude greatest observer directly ﬁxates stimulus smallest observer directly ﬁxates stimuli. parameters characterize observations agent ﬁxating potential target locations gets observation depending whether target distractor). furthermore since agent look locations cannot target relax assumption agent must look particular location choosing allowing agent stop location declare target. ﬁrst present policies similar discussion simple visual search task show c-dac policy looking ﬁrst location evident fig. c-dac policy diﬀers infomax policy even switch cost considered thus pointing fundamental diﬀerence two. note parameters used here c-dac never chooses look center parameter settings infomax however never even looks actual potential locations favoring midway locations declaring target location. performance comparison terms behavioral output investigate scenarios switch cost switch cost. threshold infomax accuracies matched facilitate fair comparison. simulations alalgorithms diﬀer c-dac state representation inference control and/or approximation scheme. brieﬂy summarize here. problem active gesture recognition studied using historic state representation nearest neighbor q-function approximation. sensing strategies robots robocup competition studied uses states augmented associated uncertainty model-free least square policy iteration approximation context dependent goals considered former concentrates multi-sensor multi-aspect sensing using point based value iteration approximation latter aims provide conditions reduction active sequential hypothesis testing problem passive hypothesis testing. reinforcement learning paradigm reward dependent information gain close saccade brings target optical axis also proposed control strategies like random search sequential sweeping search drosophila-inspired search hierarchical pomdps visual action planning also proposed. choose infomax compare c-dac policy because human-vision inspired model explains human ﬁxation behavior variety tasks also cutting edge computer vision applications related problem domain typically studied pomdp multi-armed bandits classical example problem concerns pulling levers slot machines. person gambling unaware states reward distribution levers ﬁgure lever pull next order maximize cumulative reward. noting correspondence ideas pulling arms ﬁxating location rewards observations framework seems describe active sensing problem. concretely given locations ﬁxated observations received choose location ﬁxate next. however certain characteristics active sensing problem make diﬃcult study framework yet. firstly problem instance restless bandits state change even played. active sensing belief location target change even ﬁxated. whittles index simple rule assigns value restless setting figure decision policies. azure stop choose location blue stop choose indigo stop choose green ﬁxate sea-green ﬁxate olive ﬁxate ﬁxate brown ﬁxate yellow ﬁxate environment threshold infomax gorithm starts uniform prior initial ﬁxation center true target location uniformly distributed. fig. shows accuracy number time steps number switches scenarios. notice c-dac outperforms infomax even switch cost considered contrast simple task without peripheral vision note however c-dac makes switches makes sense since switches cost search time potentially reduced allowing switches. however switch cost c-dac significantly reduces number switches whereas infomax lacks adaptability changed environment. paper proposed pomdp plus bayes riskminimization framework active sensing optimizes behaviorally relevant objectives expectation speed accuracy switching eﬃciency. compared c-dac policy previously proposed infomax greedy policies. found greedy performs poorly although infomax approximate optimal policy simple environments lacks intrinsic context sensitivity ﬂexibility. speciﬁcally diﬀerent environments principled decision threshold either greedy infomax leading higher costs longer ﬁxation durations larger number switches problem settings costs signiﬁcant. performance diﬀerence advantage added ﬂexibility provided cdac becomes even profound consider general visual search problem peripheral vision. family approximations present opens avenue application model complex real world problems. figure comparison c-dac infomax task environments c-dac adjusts time steps number switches depending environment taking little longer reducing number switches eﬀort cost. highest value played. rule asymptotically optimal sub-class problems optimal general. secondly states arms active sensing task correlated work correlated arms speciﬁc structure correlation like clustered arms gaussian process bandits general strategy handling scenario. active learning another related approach hypothesis testing sub-problem related problem active sensing. setting involves unknown true hypothesis agent perform queries providing information underlying hypothesis. task determine query perform next optimally reduce number plausible hypothesis active sensing however although belief hypothesis become arbitrarily number plausible hypothesis reduce. problem investigated near-optimal greedy solution proposed along performance guarantees. besides sub-optimality approach test cannot performed lack provision stems fact noisy observations considered actually deterministic respect hidden noise parameter. thus hard cast active sensing problem framework. thus conclude although rich body literature related problems seen examples presented formulation novel goals principled approach problem active sensing. general framework proposed potential applications visual search host problems ranging active scene categorization active foraging. decision policies generates adaptive environment sensitive contextual factors. ﬂexibility robustness diﬀerent environments makes framework appealing choice variety active sensing applications.", "year": 2013}