{"title": "MuFuRU: The Multi-Function Recurrent Unit", "tag": ["cs.NE", "cs.AI", "cs.CL"], "abstract": "Recurrent neural networks such as the GRU and LSTM found wide adoption in natural language processing and achieve state-of-the-art results for many tasks. These models are characterized by a memory state that can be written to and read from by applying gated composition operations to the current input and the previous state. However, they only cover a small subset of potentially useful compositions. We propose Multi-Function Recurrent Units (MuFuRUs) that allow for arbitrary differentiable functions as composition operations. Furthermore, MuFuRUs allow for an input- and state-dependent choice of these composition operations that is learned. Our experiments demonstrate that the additional functionality helps in different sequence modeling tasks, including the evaluation of propositional logic formulae, language modeling and sentiment analysis.", "text": "recurrent neural networks lstm found wide adoption natural language processing achieve state-ofthe-art results many tasks. models characterized memory state written read applying gated composition operations current input previous state. however cover small subset potentially useful compositions. propose multi-function recurrent units allow arbitrary diﬀerentiable functions composition operations. furthermore mufurus allow inputstate-dependent choice composition operations learned. experiments demonstrate additional functionality helps diﬀerent sequence modeling tasks including evaluation propositional logic formulae language modeling sentiment analysis. recurrent neural networks applied successfully great variety sequence modeling tasks. impressive results achieved tasks involve text language modeling machine translation sentiment analysis document-level question answering recognizing textual entailment name few. modern architectures extend rnns additional functionality like attention external memory previous state combined form state. diﬀerent cell-functions proposed past including traditional tanh-cell long-short-termmemory recently gated recurrent unit allow either replacing keeping additively aggregating features every hidden dimension. decision realized soft gating mechanisms. though diﬀerent extensions variations grus lstms investigated recently none outperform standard grus lstms signiﬁcantly range different tasks. believe promising direction towards task-adaptive architecture allow diﬀerentiable composition operations learn input-dependent selection operations end-to-end task data. therefore propose novel cell -function called multi-function recurrent unit generalization frequently-used recurrent architectures allows introduction arbitrary diﬀerentiable composition operations previous state newly constructed feature vector. crucially selection composition operation learned dependent previous state current input. thus function applied obtain next state change processing inputs. multi-function recurrent unit applies predeﬁned composition operations feature vector previous state decides composition used every feature dimension individually. unit demonstrate mufurus learn evaluate simple logical expressions comparably small memory size without over-ﬁtting memory size increased show mufuru outperforms standard baseline language modeling sentiment analysis approaches state-of-the-art results without hyper-parameter tuning. parameterized size state vector size output vector size input vector. given input sequence start state state output time step computed finally deﬁned recurrent application cell-function inputs previous states time step vanilla simplest cell-function tanh-cell state time step computed non-linear projection current input previous state. note output tanh-cell state. gated recurrent unit updates state element-wise weighted newly constructed feature vector previous state update gate reset gate selects features previous hidden state used create features. useful situations previous state forgotten favor creation features. shows computed time step. table list composition functions used work. note list easily extended diﬀerentiable functions suitable given task. input operation previous state feature vector mufuru generalization existing architectures. therefore slight adaptations correct choice composition functions mufuru becomes similar existing architectures. evaluate mufuru composition operations shown table diﬀerent tasks involve modeling sequences. following experiments always compare performance baseline. every model trained task-speciﬁc hyper-parameters ensure comparability. although mufuru operation controller contains larger parameter believe aﬀect comparability since additional parameters concerned selecting single operation time step goal investigate whether introduction operations beneﬁcial not. besides models easily overﬁt experiments giving advantage models better generalization ability. unit test evaluate mufuru’s ability learn evaluate simple propositional logic formulae. sample sequences boolean binary gates input truth values. instance figure shows test accuracy mufuru diﬀerent hidden dimensions trained epochs. struggles generalize longer sequences mufuru learns evaluate boolean formulae memory size emulate operations needed evaluate boolean gates provided much larger hidden dimension quickly starts overﬁt larger hidden dimensions. contrast mufuru overﬁt even large hidden dimensions indicates learns apply correct arithmetic counterpart every boolean operation. plot average weight operations used mufuru input truth values boolean gates figure weights seem counter-intuitive however note choice operation dependent current input also previous state mufuru learns speciﬁc behavior tailored diﬀerent inputs makes sparse provided operations. language modeling important task applications involving language generation. requires system predict next word conditioned previous text every time step. experiment dataset limited vocabulary words. trained single-layer models hidden units. testset perplexity outperformed mufuru perplexity result substantiates claim mufuru least good modeling sequences gru. sentiment classiﬁcation requires system recognize polarity text. experiment train models sentiment treebank contains annotated phrases collected sentences. trained models hidden units using mini-batches phrases feed ﬁnal output vector phrase input logistic regression classiﬁer. word embeddings tuned initialized glove sampled uniformly unknown words. chose model best accuracy development evaluated every mini-batches. results experiments along current state-of-the-art results presented figure both mufuru achieve high accuracy. mufuru performs better indicates introduction operations helps task. mufuru even approaches state-of-the-art results without need complex structural biases. however using complex structures rnns orthogonal work differentiable operations mufuru integrated architectures. presented multi-function recurrent unit recurrent neural network architecture learns select composition functions combination computed features existing state every time step. thereby generalizes beyond existing models limited small compositions. demonstrate theoretical advantages task evaluates simple propositional formulae provide empirical evidence language modeling task additional compositional functionality useful. since mufurus principle able learn similar behaviour grus lstms used place cell -functions rnns. furthermore task-speciﬁc diﬀerentiable composition functions easily integrated. research partially supported microsoft research scholarship programme german federal ministry education research projects sides bbdc software campus", "year": 2016}