{"title": "Tractable Bayesian Learning of Tree Belief Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In this paper we present decomposable priors, a family of priors over structure and parameters of tree belief nets for which Bayesian learning with complete observations is tractable, in the sense that the posterior is also decomposable and can be completely determined analytically in polynomial time. This follows from two main results: First, we show that factored distributions over spanning trees in a graph can be integrated in closed form. Second, we examine priors over tree parameters and show that a set of assumptions similar to (Heckerman and al. 1995) constrain the tree parameter priors to be a compactly parameterized product of Dirichlet distributions. Beside allowing for exact Bayesian learning, these results permit us to formulate a new class of tractable latent variable models in which the likelihood of a data point is computed through an ensemble average over tree structures.", "text": "paper present priors family priors parameters bayesian observations posterior completely lytically lows main results show factored spanning grated closed amine priors show assumptions framework. algorithm ized maximum a-posteriori operations. evaluating rior given tree takes time. first standard parameter prior tree parameters product parameters last result posterior turn problem learning frame­ bayesian prior work assumes tree distributions main learning dataset independently means finding tribution solution bayes' formula makes defining space non-trivial factor formula prior distribution possible requirement tion prior. tation usually computing therefore maximum a-posteriori approximations peaks. exception so-called model family posterior priors conjugate ponential family distributions paper find conjugate prior family according needs define prior tree structures prior tree parameters hard fixed struc­ ture tree distribution exponential priors less obvious contribution establish shows summing distribu­ tion spanning distribu­ tion factors done closed form computing order determinant proof the­ involves orem well proofs appear {oulv vroot assume convention parent takes value only. fixed. first keep distribution distribution rep­ shown section latter resented sentation choice ever assign exactly observation tinguish data. thus shall require sponding point view prior. assumption words parameter prior parameters edges; prior contain moved dependence prior. instead shall call prior satisfying decomposable resulting decomposable. sumptions euler function xp-le-xdx. line reasoning hgc. assumptions ization hgc. unlike prior general ponential tree graphical \"pairwise counts\" possible likelihood represented num­ variable tion tree structures available directly prior understood sufficient type prior information mains show compute constant structure values parameters' eters directed section model called ensembles extends scribe tion defined steps eters second structure known time choose need specify parameter large actual done easily define decomposable paper presented class priors ters makes exact bayesian decomposable factors edge contributes tree structure allows representing rameters integrate ensemble ture model whose components parametrized corresponds variable taking many values remarkable structures therefore dard assumptions ensure tractability. meiu-predoviciu algorithm worth highlighting sumptions restrictive cally limiting used efficiently trees. e.g. knowledge appear simultaneously structure parameters. trees worthwhile tractable knowledge case general", "year": 2013}