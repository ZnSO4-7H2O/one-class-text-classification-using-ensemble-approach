{"title": "Active Long Term Memory Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Continual Learning in artificial neural networks suffers from interference and forgetting when different tasks are learned sequentially. This paper introduces the Active Long Term Memory Networks (A-LTM), a model of sequential multi-task deep learning that is able to maintain previously learned association between sensory input and behavioral output while acquiring knew knowledge. A-LTM exploits the non-convex nature of deep neural networks and actively maintains knowledge of previously learned, inactive tasks using a distillation loss. Distortions of the learned input-output map are penalized but hidden layers are free to transverse towards new local optima that are more favorable for the multi-task objective. We re-frame the McClelland's seminal Hippocampal theory with respect to Catastrophic Inference (CI) behavior exhibited by modern deep architectures trained with back-propagation and inhomogeneous sampling of latent factors across epochs. We present empirical results of non-trivial CI during continual learning in Deep Linear Networks trained on the same task, in Convolutional Neural Networks when the task shifts from predicting semantic to graphical factors and during domain adaptation from simple to complex environments. We present results of the A-LTM model's ability to maintain viewpoint recognition learned in the highly controlled iLab-20M dataset with 10 object categories and 88 camera viewpoints, while adapting to the unstructured domain of Imagenet with 1,000 object categories.", "text": "continual learning artiﬁcial neural networks suffers interference forgetting different tasks learned sequentially. paper introduces active long term memory networks model sequential multitask deep learning able maintain previously learned association sensory input behavioral output acquiring knew knowledge. a-ltm exploits non-convex nature deep neural networks actively maintains knowledge previously learned inactive tasks using distillation loss distortions learned input-output penalized hidden layers free transverse towards local optima favorable multi-task objective. re-frame mcclelland’s seminal hippocampal theory respect catastrophic inference behavior exhibited modern deep architectures trained back-propagation inhomogeneous sampling latent factors across epochs. present empirical results non-trivial continual learning deep linear networks trained task convolutional neural networks task shifts predicting semantic graphical factors domain adaptation simple complex environments. present results a-ltm model’s ability maintain viewpoint recognition learned highly controlled ilab-m dataset object categories camera viewpoints adapting unstructured domain imagenet object categories. recent interest bridging human machine computations obliges consider learning framework continual sequential nature potentially lifelong .therefore learning framework prone interferences forgetting. positive side intrinsic correlations multiple tasks datasets allow train deep learning architectures make multiple data supervision sources achieve better generalization favorable effect multi-task learning depends shared parametrization individual functions simultaneous estimation averaging multiple losses. trained simultaneously shared layers obliged learn common representation effectively cross-regularizing task. generally sequential estimation case recent task beneﬁts inductive bias older tasks become distorted unconstrained back-propagation errors shared parameters problem identiﬁed catastrophic interferences between tasks humans dyadic interaction hippocampus neocortex thought mitigate problem carefully balancing sensitivity-stability trade-off experiences integrated without exposing system risk abrupt phase transitions. think classical example child exploring objects structured play samples multiple points view lighting directions generates movement object space. exploration creates inputs perceptual systems span homogeneously underlying variation viewing parameters construct general purpose graphical categorical semantic level representations perception. representations used future actively regularize experience environments exploration constrained costly. conjecture arises learning lifespan system distribution locally observed states environment non-stationary potentially chaotic neural representation environment respect mode variation latent graphical semantic factors must stable slowly evolving store non-transient knowledge. early experiments neural networks’ ability learn meso-scale structure training environment required interleaved exposure different semantic variations. heuristic respected modern architectures object recognition trained stochastic mini batches uniform possibly alternated rich sampling categories analogously data augmentation regularizes distribution graphical factors. hypothesize inability cnns learn object categorization strongly correlated batches caused interferences across vast number categories latent graphical factors must memorized. similarly outstanding success deep-q-networks partially found intrinsic replay system augments learning batches state-action transitions observed long time. procedure allows creation representations dqns transverse quasi-hierarchically play. difﬁcult imagine representations could remembered past states visited replay system. simpliﬁed case deep linear networks possible obtain exact analytical treatment network’s learning time function input-output statistics network depth suggesting phase transitions typical catastrophic interference might appear mixing time data generating process longer learning time system obliging neural network track local dependencies factors variation instead learning represent data generating process completeness ergodic state. paper develop active long term memory networks inspired hippocampusneocortex duality based knowledge distillation framework. model composed stable component containing long term task memory ﬂexible module initialized stable component faces environment. capitalize human infant metaphor show possible maintain ability predict viewpoint object adapting domain images object categories viewing conditions original training domain. moreover discuss necessity store replay input domain fully maintain original task. non-convex system store knowledge changing environment important understand knowledge synthesized neural network. general intuition hierarchical models thousands intermediate representations millions parameters difﬁcult identify contained knowledge respect parameters value across layers. without guarantee unique global optimum multiple conﬁgurations weight parameters could sustain input-output making association parameters knowledge vague. knowledge distillation framework introduces concept model compression transfer knowledge computationally expensive ensemble single easy deploy model using prediction complex model supervision compressed one. bornagain-tree paradigm breiman proposed tree model predict outputs random forest ensembles better interpretability. framework knowledge neural networks therefore identiﬁed input-output without regard parametrization. transferring knowledge neural architecture therefore corresponds supervised training student network using logits original teacher network matching soft-probabilities induce. framework received recent interest extended called generalized distillation incorporate theoretical results vapnik’s privileged information algorithm recently large scale experiments carried ability compress fully connected shallow models convolutional models long short term memory networks vice-versa. introduction double streams architecture inspired classical siamese network metric learning consequent generalization multi-task framework domain adaptation offers natural extension distillation happens streams made handle different data supervision sources shared parametrization. case strong effect sequential learning deep neural networks shown respectively semantic graphical factors authors able estimate encoder-decoder model correlated mini-batches using interleaved learning carefully selected factors ratio ad-hoc clamping neurons learning rate. multiple atari games learned interleaved distillation across games correct ratio batch size interleaving carefully curated crucial algorithm success. authors also present novel self-distillation framework remembering minskian sequence teaching selves approach problem learning sequence input-outputs exhibits transitions latent factors using dual system. model inspired seminal theory mcclelland dyadic role hippocampus neocortex preserving long-term memory avoiding catastrophic interferences mammals ﬁrst a-ltm component mature stable network neocortex trained development phase homogeneous environment rich supervision sources. prevent interference experience previously stored functions networks’s learning rate drastically reduced post-developmental phases imitation visuo-cortical critical period plasticity second component a-ltm ﬂexible network hippocampus subject general unstructured environment. weights initialized learning dynamics actively regularized output activity. dual mechanism allows maintain stability without ignoring inputs. quickly adapt information non-stationary environment without generating risk integrity knowledge already stored furthermore long term information actively distilled effect constraining gradient descent dynamics ongoing towards better local minimum able sustain knowledge. operationally development trained controlled environment multiple examples object potential graphical transformation present. train multi-task objective predict semantic graphical labels object. convergence learning rate maturity initialized faces novel environment objects typically available single perspective number categories increased orders magnitude trained multi-task objective predict higher dimensional semantic task output respect a-ltm networks relies core idea tasks memorize need simultaneous experience multi-task optimum critical point objectives. therefore environment unstable input-output missing labels rely predictions complementary case instabilities generated changes distribution input auxiliary replay mechanism also necessary. sequential environment study situation agent interacts sequential environment deﬁned joint distribution visual stimuli binary latent factors agent receives information perceptual mechanism makes decision based hierarchical representation percept agent’s goal name underlying latent factors stimulus. actions chosen agent n-way softmax transforms last layer hierarchical representation sensory inputs probability distribution actions. environment responds actions supervised signal informing agent correct action given particular stimulus. hierarchical representation updated order minimize cross-entropy loss task computationally tedious vast range possible transformations sensory inputs meaningful consequence semantic nature stimulus. call transformations latent graphical factors. perceptual inputs system therefore modes variation catastrophic interference happens distribution supervised signals homogeneous. modeling environment’s non-stationarity language stochastic process could lead interesting insights limit simple regime single discrete transition bridging sequential multi-task learning knowledge distillation multitask function representing input output maps network shared parameters task-speciﬁc parameters deﬁning common representation individual tasks sequential learning corresponds solving sequence following optimization problems. first minimization cross entropy loss environment data generating process softmax probability distribution induced task predictions gaussian initializations solutions multitask learning mitigates problem averaging weight updates across different objectives corresponds solving simultaneously omitted scaling factors drawback approach must available network whole training phase. absence labels task knowledge distillation used surrogate. a-ltm stable module trained problem used hallucinate missing labels. learning phase recast multi-task framework even absence external supervision solving following respect availability inputs belong modality share distribution graphical factors variation active stream perception used train active inactive tasks ﬂexible module. otherwise either inputs stored networks must rely generative mechanism generate imaginary samples nonongoing task. assume presence replay mechanism allowing a-ltm networks access training. relax assumption experiments present results also problem objective function beyond active maintenance memory consolidation article focus early phase memory maintenance successive phase called memory consolidation knowledge distilled necessary non-active long term memory. interpret phase learning model step right consolidation. irreparable damage would create memory loss equivalent temporally graded retrograde amnesia. case bilateral lesions hippocampus previously known information stored would safe recent adaptations newly known environments stored would completely lost. ﬁrst illustration active long term memory framework conduct sequence experiments sequential multitask a-ltm learning. employ three datasets experiments increasing complexity. ﬁrst show interferences emerge trivial case sequential learning function deep linear networks. analyze situation complete forgetting happens sequential learning semantic graphical factors variation. finally demonstrate ability a-ltm preserve memory domain adaptation without access replays previous environment. datasets used synthetic hierarchical features employ procedure based independent sampling branching diffusion process previously used study deep linear networks learning dynamics generate synthetic dataset hierarchical structure similar taxonomy categories typical domain living things. ilabm highly controlled dataset images toy-vehicles collected turntable multiple cameras different lighting directions. ilab totals objects categories rotating angles cameras lighting conditions multiple focus background complete dataset images. object images background. highly controlled nature dataset reach high accuracy random subset images categories camera positions thus exploit variations factors matter simplicity. ilabm freely available. imagenet image database categories images training set. imagenet used dataset large scale object recognition since alexnet submission deep learning model leaderboard annual ilsvrc competition. except dlns experiments train using stochastic gradient descent alexnet’s like architecture drop-out without form data augmentation central -by- crop images. train convergence epochs ilabm epochs imagenet. during multi-domain a-ltm loss function associated ilab scaled factor larger factors tend stuck imagenet training local minima. knowledge distillation implemented using loss decision layers network ilab categories viewpoints; adopted instead cross-entropy avoid tuning additional parameter teacher soft-max temperature. experiments implemented matconvnet three nvidia gpus titan tesla ﬁrst study aforementioned discrete transition deep linear networks trivial case learning sequentially function. dlns exhibits interesting non-linear dynamics typical non-linear counterpart amenable interpretation. optimization problem non-convex since deep structure transformed shallow linear simple factorization layers deep local minimum compared shallow global optima. experiments network architecture input layer single hidden layer output layers task. examine trivial situation task equivalent task situation obviously multi-task solution corresponds network re-learning hidden output layer without modiﬁcation input hidden layer. alternatively multiple solutions maintain input-output relationship constant modify weights input-hidden layer therefore requiring adjustment hidden-output weights possible. ﬁrst train network respect task till convergence begin training respect identical task back-propagation systematically effect input-hidden layer generating interference original network. multi-task learning problem. instead alternate task every epochs generating reciprocal interference decreasing intensity delays convergence epochs respect multi-task case. parametrized nature ilabm good environment early training phase models taught incorporate ability predict multiple semantic graphical factors train single task architectures classify either categories viewpoints weights convergence warm network complementary task. compare inductive bias catastrophic interference effect single multi-task solutions. results table conﬁrm inductive bias viewpoints category dramatic interference sequential training tasks. finally expected multi-task learning easily learn tasks incorporating inductive bias viewpoints categories. figure example turntable setup multiple-camera viewpoint lighting directions objects ilabm. results sequential multi-task learning categories viewpoitns ilabm table test accuracy domain adaptation imagenet memory viewpoint task ilab single task multi-task/domain active long term memory networks without replay. multiple architectures single task network gaussian initialization sequential transfer ilabm multi task network multi-domain networks trained simultaneously ilab imagenet tasks either initialized randomly ilabm multi task network seen table conﬁrm general intuition developed previous experiment. initializing ilab positive effect transferred performance; moreover unconstrained adaptation completely wipes ability network perform viewpoint identiﬁcation. multitask/domain learning able reach performances imagenet maintaining almost completely viewpoint detection task. a-ltm domain adaptation ilabm imagenet without extrinsic supervision multi-task learning seems good strategy ﬁnding local minimum able mantain multiple tasks requires expensive sources supervision images labels original domain. a-ltm architecture able exploit absence supervision using stable component hallucinate missing labels convert otherwise sequential learning multi-task scenario. distribution input homogeneous across datasets input-output stable network expressed therefore distilled using images domain input avoiding completely limitations multi-task learning contrary case replay system either based re-generation storage past input necessary results imagenet table show a-ltm architectures able maintain long term memory viewpoint task cost slower adaptation. reduced memory performance table a-ltm-naive especially strong initial drop viewpoints categories illustrated ﬁgure indicative strong shift underlying factors datasets importance generative mechanisms re-balance differences replay. work introduced model long term memory inspired neuroscience based knowledge distillation framework bridges sequential learning multi-task learning. show empirically ability recognize different viewpoints object maintained also exposure millions examples without extrinsic supervision using knowledge distillation replay mechanism. furthermore report encouraging results ability a-ltm maintain knowledge relying current perceptual stream. theoretical analysis dlns linking convergence time stochastic gradient descent input-output statistics network depth plethora tricks developed successfully train deep networks suggest potential relationship vanishing gradient problem order learn complex environments whose data generating process take long time necessary deeper figure test accuracy ilabm a-ltm categories blue viewpoints red. a-ltm replay indicated circles a-ltm without replay plus signs dashed horizontal lines represents accuracies initialization. models suffers initial drop accuracy ﬁrst epochs a-ltm network access samples original dataset able fully recover. seems easy confuse memory problems supervised classiﬁcation long short term memory networks careful balance positive negative examples minibatches crucial model performance similarly interleaved scheme sampling categories cnns.", "year": 2016}