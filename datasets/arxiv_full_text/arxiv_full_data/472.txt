{"title": "ASR error management for improving spoken language understanding", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "This paper addresses the problem of automatic speech recognition (ASR) error detection and their use for improving spoken language understanding (SLU) systems. In this study, the SLU task consists in automatically extracting, from ASR transcriptions , semantic concepts and concept/values pairs in a e.g touristic information system. An approach is proposed for enriching the set of semantic labels with error specific labels and by using a recently proposed neural approach based on word embeddings to compute well calibrated ASR confidence measures. Experimental results are reported showing that it is possible to decrease significantly the Concept/Value Error Rate with a state of the art system, outperforming previously published results performance on the same experimental data. It also shown that combining an SLU approach based on conditional random fields with a neural encoder/decoder attention based architecture , it is possible to effectively identifying confidence islands and uncertain semantic output segments useful for deciding appropriate error handling actions by the dialogue manager strategy .", "text": "features introduced. architectures based conditional random ﬁelds encoder-decoder neural network structure mechanism attention experimental results showing signiﬁcant reduction french media corpus concepts concept value pairs conﬁrm expected beneﬁt introducing semantic speciﬁc features. optimal combinations architectures provide additional improvements concept error rate relative reduction concept-value error rate relative reduction respect baseline described using features based crfs. systems error prone. part caused certain types errors. general errors reduced estimating model parameters minimizing expected word error rate effect word errors controlled associating single sentence hypothesis word conﬁdence measures. methods proposed constructing conﬁdence features improving quality semantic conﬁdence measure. methods proposed conﬁdence calibration based maximum entropy model distribution constraints conventional artiﬁcial neural network deep belief network latter methods show slightly superior performance higher computational complexity compared ﬁrst one. recently features bidirectional recurrent neural networks proposed error detection. systems reviewed generate hypotheses semantic frame slot tags expressed spoken sentence analyzed system. deep neural networks appeared recent systems described bidirectional rnns long-short term memory used semantic frame slot tagging lstms proposed mechanism attention parsing text sentences logical forms. following convolutional neural network proposed encoding representation knowledge expressed spoken sentence. encoding used attention mechanism constraining hypothesization slot tags expressed sentence. recent papers using sophisticated architectures based rnns best sequence word hypotheses input passed system. paper architectures considered. ﬁrst based encoder bidirectional gated recurrent units used machine translation integrates context information attention based decoder second integrates context information architecture used based conditional random ﬁelds systems receive word hypotheses generated sub-system scored conﬁdence measures computed neural archipaper addresses problem automatic speech recognition error detection improving spoken language understanding systems. study task consists automatically extracting transcriptions semantic concepts concept/values pairs touristic information system. approach proposed enriching semantic labels error speciﬁc labels using recently proposed neural approach based word embeddings compute well calibrated conﬁdence measures. experimental results reported showing possible decrease signiﬁcantly concept/value error rate state system outperforming previously published results performance experimental data. also shown combining approach based conditional random ﬁelds neural encoder/decoder attention based architecture possible effectively identifying conﬁdence islands uncertain semantic output segments useful deciding appropriate error handling actions dialogue manager strategy. index terms spoken language understanding speech recognition robustness errors spite impressive research efforts recent results systems semantic interpretation text speech still make errors. problems common text speech difﬁculty concept mention localization ambiguities intrinsic localized mentions deﬁciency identify sufﬁcient contextual constraints solving interpretation ambiguities. additional problems introduced interaction spoken language understanding system error prone automatic speech recognition system. errors affect mention concept value concept instance. furthermore hypothesization concepts values depends among things context mention localized. thus context errors also introduce errors concept mention location hypothesization. focus paper introduction suitable conﬁdence measures localizing word errors affect performance. used additional features combined lexical syntactic features useful characterizing concept mentions. purpose error detection sub-system endowed conﬁdence features based syntactic dependencies semantically relevant word features. architectures equipped sets conﬁdence speciﬁc word different conﬁdence measures used error detection. ﬁrst word posterior probability computed confusion networks described variant approach introduced latter measure computed multi-stream multi-layer perceptron architecture different heterogeneous conﬁdence features. among them relevant word embeddings targeted word neighbors length current word language model backoff behavior part speech tags syntactic dependency labels word governors. features prosodic features acoustic word embeddings described could also used considered experiments described paper. particular attention carried word embeddings computation result combination different well known word embeddings made neural auto-encoder order improve performances error detection system ms-mlp proposed error detection output units. compute scores correct error labels associated generated hypothesis. hypothesis evaluated softmax value correct label scored ms-mlp. experiments shown calibrated conﬁdence measure effective word posterior probability comparison based normalized cross entropy measures information contribution provided conﬁdence knowledge. table comparison error prediction capabilities probability posteriori conﬁdence measure derived ms-mlp error detection system terms normalized cross entropy media test set. figure shows predictive capability conﬁdence measure based mlp-ms compared word posterior probability media test data. curve shows predicted percentage correct words function conﬁdence intervals. best measure percentages closest diagonal line. figure comparison error prediction capabilities probability posteriori conﬁdence measure derived ms-mlp error detection system media test set. basis architectures considered carry experiments media corpus ﬁrst encoder/decoder recurrent neural architecture mechanism attention similar used machine translation proposed second based conditional random ﬁelds architectures build training model features encoded continuous values ﬁrst discrete values second architectures take features except conﬁdence measures taken partially anaccording powerful conﬁguration. architectures also need calibrated respective hyper-parameters order give best results. best conﬁguration chosen described proposed encoder-decoder architecture attention-based mechanism inspired machine translation architecture depicted ﬁgure concept tagging process considered translation problem words semantic concept tags bidirectional encoder based gated recurrent units computes annotation word input sequence annotation concatenation matching forward hidden layer state backward hidden layer state obtained respectively forward backward comprising bidirectional rnn. annotation contains summaries dialogue turn contexts respectively preceding following considered word. account weighted annotations computed encoder. weighting depends current output target core attention mechanism good estimation weights allows decoder choose parts input sequence attention context vector used decoder conjunction previous emitted label output current state hidden layer make decision current label output detailled description recurent neural networks attention based ones found past experiments described shown best semantic annotation performance manual automatic transcriptions media corpus obtained systems. recently architecture compared popular bi-directionnal results systems outperform bi-rnn architecture media corpus better results observed bi-rnn atis corpus. probably explained fact media contains semantic contents whose mentions difﬁcult disambiguate crfs make possible exploit complex contexts effectively. sake comparison best system proposed wapiti toolkit used nevertheless input features used system proposed paper different used among novelties used system consider syntactic conﬁdence features conﬁguration template different. many experiments performed ﬁnal feature template includes previous following instances words unigram bigram associate semantic label current word. also associated current word semantic categories previous following instances. features considered current position. furthermore tool discretizecrf used apply discretization function conﬁdence measures order obtain several discrete values accepted input features crf. system reported paper baseline. however used paper lower used baseline rigorous conclusions drawn comparisons different components introduced paper. media corpus collected french media/evalda project deals negotiation tourist services. contains three sets telephone human/computer dialogues namely training approximately sentences development sentences evaluation containing sentences. corpus manually annotated semantic concepts characterized label value. types semantic annotations considered paper consistent experimental results provided annotations also associate word sequence concepts. sequences considered estimations concept localized mentions. evaluations performed test sets report concept error rates concept labels concept-value error rates conceptvalue pairs. worth mentioning number concepts annotated turn large variability include annotated concepts. among concepts types some three different types reference connector application domain entities. mentions concepts often short confusable sequences words. experiments variant system developed lium last evaluation campaign french language used system based kaldi speech recognition toolkit training used estimate acoustic models parameters consists speech segments several sources radiophonic broadcast ester ester corpora accounts hours speech each; broadcast etape corpus accounting hours speech; broadcast repere train corpus accounting hours speech lium radio broadcast data hours speech. total hours speech composes training corpus. recordings converted training acoustic models order appropriate media telephone data. inputs mfccs concatenated i-vectors order adapt acoustic models speakers. vocabulary system contains words present media training development corpora words. ﬁrst bigram language model applied decoding process generate word-lattices. lattices rescored applying -gram language model. order training corpus close test corpus models trained transcriptions. avoid deal errors made over-trained media training corpus leave-one-out approach followed dialogue ﬁles training development corpora randomly split subsets. subset transcribed using trained manual transcriptions present blocks linearly interpolated ’generic’ language model trained large french newspaper crawled containing millions words. test data transcribed trained media training corpus generic language model. shown table word error rates training development test corpora around tests performed architectures media set. best conﬁguration chosen respect best results observed applied obtaining test results. results terms error rate precision recall concepts concept value reported best conﬁguration architecture table order evaluate impact conﬁdence measures among input features made experiments summarized table conﬁdence measure provided ms-mlp architecture brings relevant information reduce cver. versions systems considered adding usual media concept labels output tags. training tags replacing usual hypothesized word erroneous. erroneous hypothesized word supporting concept associated error-c error-n otherwise. evaluation error-c error-n hypothesized tags replaced null order perform usual media evaluation protocol. results test obtained best conﬁguration observed reported table results table similar table notice small differences. instance precision better even reduced nn-eda. using four systems executed parallel worth trying improvements obtained combination weight estimated optimal performance set. results reported table compared rover combination applied systems described respect baseline. considering best results manual transcriptions test conclude that solutions presented paper contribution errors overall errors inferior errors observed manual transcriptions. detailed analysis errors observed automatic manual transcriptions show common large error contributions concepts three different types reference connectors domain relevant entities proper names values different attributes. concepts expressed confusable words whose disambiguation requires complex context relations cannot automatically characterized crfs type attention mechanisms used eda. considering case four systems provided output word precision recall observed test set. lack consensus test sets appears correspond cases mentions types concepts. interesting result since suggests investigation particular cases important challenge future work. variations architectures respectively based crfs nn-eda considered. using media corpus compared considered baseline provided best results among seven different approaches reported main novelties proposed architectures among others semantically relevant conﬁdence input features. architectures outperformed nn-eda architectures signiﬁcant improvement baseline. nevertheless nn-eda architectures appeared useful combined ones. results show interaction components beneﬁcial. furthermore architectures show errors concepts whose mentions made short confusable sequences words remain ambiguous even localized. concept types difﬁcult detect even manual transcription indicating interpretation media corpus particularly difﬁcult. thus suggested directions future work consider structured mechanisms attention capable selecting features distant contexts conversation history. objective identify sufﬁcient context features disambiguating local concept mentions. lafferty mccallum pereira conditional random ﬁelds probabilistic models segmenting labeling sequence data proceedings eighteenth international conference machine learning icml vol. nasr bchet j.-f. macaon chane linguistique pour traitement graphes mots traitement automatique langues naturelles session dmonstrations montral lavergne capp´e yvon practical large scale crfs proceedings annual meeting association computational linguistics association computational linguistics july available http//www.aclweb.org/anthology/p- bonneau-maynard rosset ayache kuhn mostefa semantic annotation french media dialog corpus ninth european conference speech communication technology rousseau boulianne del´eglise est`eve gupta meignier lium crim system combination repere evaluation campaign international conference text speech dialogue. springer povey ghoshal boulianne burget glembek goel hannemann motlicek qian schwarz kaldi speech recognition toolkit ieee workshop automatic speech recognition understanding epflconf-. galliano geoffrois gravier bonastre mostefa choukri corpus description ester evaluation campaign rich transcription french broadcast news international conference language resources evaluation gravier adda paulsson carr´e giraudel galibert etape corpus evaluation speechbased content processing french language eighth international conference language resources evaluation istanbul turkey fiscus post-processing system yield reduced word error rates recognizer output voting error reduction automatic speech recognition understanding proceedings. ieee workshop hahn dinarelli raymond lefevre lehnen mori moschitti riccardi comparing stochastic approaches spoken language understanding multiple languages ieee transactions audio speech language processing vol. mangu brill stolcke finding consensus speech recognition word error minimization applications confusion networks computer speech language vol. ogawa hori error detection recognition rate estimation using deep bidirectional recurrent neural networks acoustics speech signal processing ieee international conference mesnil dauphin bengio deng hakkanitur heck using recurrent neural networks slot ﬁlling spoken language understanding ieee/acm transactions audio speech language processing vol. hakkani-t¨ur celikyilmaz y.-n. chen deng y.-y. wang multi-domain joint semantic frame parsing using bi-directional rnn-lstm proceedings annual meeting international speech communication association reddy t¨ackstr¨om collins kwiatkowski steedman lapata transforming dependency structures logical forms semantic parsing transactions association computational linguistics vol. huang xiang zhou dependency-based convolutional neural networks sentence embedding annual meeting association computational linguistics international joint conference natural language processing. association computational linguistics y.-n. chen hakanni-t¨ur celikyilmaz deng syntax semantics? knowledge-guided joint semantic frame parsing ieee workshop spoken language technology diego ghannay esteve camelin word embeddings combination neural networks robustness error detection signal processing conference european. nice france ieee ghannay est`eve camelin dutrey santiago adda-decker combining continuous word representation prosodic features error prediction international conference statistical language speech processing. springer", "year": 2017}