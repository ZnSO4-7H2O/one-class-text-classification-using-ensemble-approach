{"title": "On denoising autoencoders trained to minimise binary cross-entropy", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Denoising autoencoders (DAEs) are powerful deep learning models used for feature extraction, data generation and network pre-training. DAEs consist of an encoder and decoder which may be trained simultaneously to minimise a loss (function) between an input and the reconstruction of a corrupted version of the input. There are two common loss functions used for training autoencoders, these include the mean-squared error (MSE) and the binary cross-entropy (BCE). When training autoencoders on image data a natural choice of loss function is BCE, since pixel values may be normalised to take values in [0,1] and the decoder model may be designed to generate samples that take values in (0,1). We show theoretically that DAEs trained to minimise BCE may be used to take gradient steps in the data space towards regions of high probability under the data-generating distribution. Previously this had only been shown for DAEs trained using MSE. As a consequence of the theory, iterative application of a trained DAE moves a data sample from regions of low probability to regions of higher probability under the data-generating distribution. Firstly, we validate the theory by showing that novel data samples, consistent with the training data, may be synthesised when the initial data samples are random noise. Secondly, we motivate the theory by showing that initial data samples synthesised via other methods may be improved via iterative application of a trained DAE to those initial samples.", "text": "however mean-squared error common loss functions used training autoencoders. especially true data distribution consists images take values pixel intensity thought probability pixel being situation possible binary cross-entropy loss naturally applies random variables off. paper extend theory presented alain bengio present empirical results validate theory practical setting furthermore present application applying theory improve samples synthesised variational autoencoders adversarial autoencoders trained denoising criterion. autoencoders class neural network models conceptually simple provide powerful means learning useful representations observed data unsupervised learning trained autoencoders used downstream tasks either extracting learned representations input algorithms ﬁne-tuning existing model tasks additionally recent work development generative autoencoders i.e. autoencoders learn useful representations allow synthesis novel data samples consistent training data autoencoders consist models encoder decoder arranged series. models trained simultaneously minimise reconstruction loss input encoder output decoder. common variant autoencoder denoising autoencoder trained recover clean versions corrupted input samples formally encoder decoder corruption process trained minimise data samples whose underlying distribution parameters learned. additional regularisation terms loss function. shown optimal reconstruction function learned trained minimise mean-squared reconstruction error approximates gradient log-likelihood data-generating distribution respect data. theoretical result purpose experiments twofold. validate theory presented section show possible synthesise novel data samples consistent training data starting random noise. provide practical motivation showing samples synthesised generative autoencoders using existing sampling methods improved iterative application trained denoising generative autoencoder initial samples. experiments train denoising variants state-of-the-art generative autoencoder models—the variational autoencoder adversarial autoencoder —for reasons speciﬁed above. firstly models order validate equation secondly demonstrate sampling method detailed equation used improve perceptual quality data samples synthesised methods. generative autoencoders autoencoders trained reconstruction loss well extra regularisation loss encourages distribution encoded samples conform speciﬁed prior distribution; simplicity often chosen multivariate normal distribution standard choice experiments. details derivation implementation regularisation losses refer readers original literature turn models denoising variants— denoising variational autoencoders denoising adversarial autoencoders —simply replacing reconstruction loss denoising loss. following alain bengio additive gaussian noise corruption process; noise sampled denoising generative autoencoder models trained till convergence close optilearned reconstruction function line recommendations radford strided convolutional layers encoders followed fractionally-strided convolutions decoders. layer followed batch normalisation relu nonlinearities except ﬁnal layer decoders values restricted sigmoid function. architecture encoders decoders match discriminator generator networks radford adversarial model daae fully-connected network dropout leaky relu nonlinearities. perform sampling using trained generative autoencoders utilise minibatch statistics batch normalisation layers help compensate inputs training distribution. training celeba dataset consists hundred thousand aligned cropped images celebrity faces widely used qualitative evaluation generative models contains large amount high quality image data human faces makes easier unrealistic qualities synthesised samples spotted. preprocess images cropping resizing train models using adam optimiser training split dataset epochs. experiments carried using torch library section showed utilising optimally trained reconstruction model equivalent gradient step direction likely data samples. validate empirically iteratively applying trained reconstruction model initial image pixel sampled independently random distribution. trained reconstruction model applied that according equation iteration produce image samples likely under data-generating distribution. qualitatively means several iterations applying reconstruction model random noise results progressively realistic faces. results sampling procedure given equation shown figure initial samples noise drawn uniform distribution proceeding samples appear like faces. reconstruction iterations sufﬁcient produce face-like images figure applying sampling procedure equation initial data samples drawn uniform distribution shown produce samples shows clear emergence samples consistent training data starting noise. reconstruction function trained dvae trained daae. several iterations result signiﬁcant improvement perceptual quality. results show iterative application equation moves samples regions probability data-generating distribution regions higher probability. consider support images valid faces exist manifolds lower dimensions data gradient samples manifold weak. potential solution problem small amount noise iteration effect smoothing probability distribution hence making easier take gradient steps. demonstrate result applying technique figure show results applying equation many iterations. figure starting random data sample drawn uniform distribution iteratively encoding decoding using trained dvae trained daae gaussian noise added encoding step. applying sampling method equation observe samples moving along manifold towards likely regions. adding noise ﬁxed step size also transitions samples move region near lower local maximum region closer higher local maximum normally samples drawn generative autoencoder drawing latent encoding chosen prior distribution passing trained decoder. refer data sample drawn order improve data samples propose iteratively applying trained reconstruction function order move sample towards regions higher probability data-generating distribution. process detailed equation initialise process conventional prior distribution contrast work bengio work focuses moving towards single data point distribution rather movement modes data-generating distribution. speciﬁcally work addition noise iterations used encourage smoother data space take steps whereas bengio corruption process required allow successful transition modes. note applying iterative process equation starting different initial random noise inputs able generate samples different modes data-generating distribution. found sampling process often lead series nonsensical samples transitioning modes; introduced walkback algorithm involves updating reconstruction model sampling time. need process indicates drawbacks attempting transition modes data distribution especially data distribution consists images. considering speciﬁc application daes images make sense consider gradient method searches single highly likely point distribution rather aiming capture whole distribution starting single sample. because image samples exist manifolds often lower dimensions data space means manifolds unlikely intercept making difﬁcult move mode manifold angenerating sampling consistent training data. figure samples synthesised sampling process detailed equation initial sample drawn trained dvae daae iteratively encoded decoded using models—the dvae daae gaussian noise added encoding step. consider work relation previous work denoising autoencoders used learn homogeneous transition operators markov chains non-homogeneous transition operators construct transition operator markov chain whose stationary distribution data-generating distribution. transition operator implemented corrupting data sample reconstructing reconstruction function trained dae. bengio initialise sampling process sample consistent training data apply iterative sampling process mnist handwritten numbers dataset produce series samples different modes data-generating distribution. example starting process generate well samples correspond numbers. experiments demonstrate iterative sampling leads convergence distribution nguyen train models using because properties shown alain bengio speciﬁc cost function. given results models could constructed using loss appropriate image data. several studies markov chain sampling applied training sample synthesis however emphasise work focused applying markov chains sample synthesis training. application training another avenue future work. rather trying synthesise samples modes data-generating distribution move towards likely data samples. involve moving along manifolds towards likely regions —leading small changes identity pose even gender. further since taking gradient steps ﬁxed size possible sufﬁciently small overshoot local maximum start gradient ascent different higher local maximum explains changes identity pose gender samples. produce variety samples—visiting different modes data-generating distribution—we initialise sampling process samples different location data generating distribution. building previous work show denoising autoencoders trained minimise binary crossentropy loss used approximate gradient density distribution data-generating distribution respect data samples. result sampling process detailed equation applied kind autoencoder trained binary crossentropy denoising loss. empirically validate ﬁndings showing possible synthesise novel samples consistent data-generating distribution starting random noise. addition provide practical application demonstrating possible", "year": 2017}