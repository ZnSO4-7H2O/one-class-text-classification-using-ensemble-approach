{"title": "Uncertain Nearest Neighbor Classification", "tag": ["cs.LG", "cs.AI", "H.2.8"], "abstract": "This work deals with the problem of classifying uncertain data. With this aim the Uncertain Nearest Neighbor (UNN) rule is here introduced, which represents the generalization of the deterministic nearest neighbor rule to the case in which uncertain objects are available. The UNN rule relies on the concept of nearest neighbor class, rather than on that of nearest neighbor object. The nearest neighbor class of a test object is the class that maximizes the probability of providing its nearest neighbor. It is provided evidence that the former concept is much more powerful than the latter one in the presence of uncertainty, in that it correctly models the right semantics of the nearest neighbor decision rule when applied to the uncertain scenario. An effective and efficient algorithm to perform uncertain nearest neighbor classification of a generic (un)certain test object is designed, based on properties that greatly reduce the temporal cost associated with nearest neighbor class probability computation. Experimental results are presented, showing that the UNN rule is effective and efficient in classifying uncertain data.", "text": "work deals problem classifying uncertain data. uncertain nearest neighbor rule introduced represents generalization deterministic nearest neighbor rule case uncertain objects available. rule relies concept nearest neighbor class rather nearest neighbor object. nearest neighbor class test object class maximizes probability providing nearest neighbor. provided evidence former concept much powerful latter presence uncertainty correctly models right semantics nearest neighbor decision rule applied uncertain scenario. eﬀective eﬃcient algorithm perform uncertain nearest neighbor classiﬁcation generic certain test object designed based properties greatly reduce temporal cost associated nearest neighbor class probability computation. experimental results presented showing rule eﬀective eﬃcient classifying uncertain data. general terms algorithms additional words phrases classiﬁcation uncertain data nearest neighbor rule probability density functions nearest neighbor classiﬁcation basic tasks data mining machine learning given examples training objects associated class labels goal classiﬁcation exploit training order build classiﬁer prediction purposes function mapping unseen objects predeﬁned class labels. traditional classiﬁcation techniques deal feature vectors deterministic values. thus data uncertainty usually ignored learning problem formulation. however must noted uncertainty arises real data many ways since data contain errors partially complete uncertainty result limitations equipment indeed physical devices often imprecise measurement errors. another source uncertainty repeated measurements e.g. surface temperature could recorded multiple times day. also applications data values continuously changing positions mobile devices observations associated natural phenomena quantities approximated using uncertain model. simply disregarding uncertainty less accurate conclusions even inexact ones. created need uncertain data management techniques techniques managing data records typically represented probability distributions classiﬁcation methods often rely distances similarity metrics order implement decision rule. must noted diﬀerent concepts similarity uncertain objects proposed literature among distance means expected distance probabilistic threshold distance thus seemingly suitable strategy classify uncertain data make ad-hoc similarity metrics order apply kind data classiﬁcation techniques already designed deterministic setting. call strategy naive approach. however work provide evidence depicted approach weak since guarantee quality class returned naive approach. matter fact naive approach return wrong class even probability object belong class approaches zero. hence major contribution provide novel classiﬁcation rule directly builds certain similarity metrics rather directly exploiting ad-hoc uncertain metrics anyway implements decision rule suitable classifying uncertain data. speciﬁcally conduct investigation context nearest neighbor rule since allows directly exploit similarity metrics classiﬁcation task. nearest neighbor rule assigns unclassiﬁed object label nearest previously classiﬁed objects generalized case nearest neighbors taken account despite seemingly simplicity eﬀective classifying data already pointed main contribution work novel classiﬁcation rule uncertain setting introduced called uncertain nearest neighbor uncertain nearest neighbor rule relies concept nearest neighbor class rather nearest neighbor object latter concept naive approach implemented nearest neighbor rule relies consider binary classiﬁcation problem class labels nearest neighbor class test object probability nearest neighbor comes class greater probability comes class. probability takes simultaneously account distribution functions distances separating training objects. represents generalization certain nearest neighbor rule case uncertain objects represented means arbitrary probability density functions taken account. show rule represents viable compute probable class test object since properties eﬃciently compute nearest neighbor class probability presented; —the experimental campaign conﬁrms superiority rule respect classical classiﬁcation techniques presence uncertainty respect density based classiﬁcation methods speciﬁcally designed uncertain data. moreover meaningfulness classiﬁcation illustrated reallife prediction scenario involving wireless mobile devices. rest paper organized follows. section introduces uncertain nearest neighbor classiﬁcation rule. section properties uncertain nearest neighbor rule stated eﬃcient algorithm solving task hand described. section discusses relationship related works. section reports experimental results. finally section draws conclusions. section uncertain nearest neighbor rule introduced. section organized follows. first uncertain objects formalized behavior nearest neighbor rule presence uncertain objects analyzed ﬁnally uncertain nearest neighbor rule introduced certain object element uncertain object random variable domain associated probability density function denotes probability assume value certain object regarded uncertain whose associated otherwise denoting dirac delta function. section classic nearest neighbor rule recalled furthermore shown direct application classiﬁcation uncertain data misleading. hence concept nearest neighbor class introduced captures right semantics nearest neighbor rule applied objects modeled means arbitrary probability density functions. nearest neighbor class forms basis upon novel uncertain nearest neighbor classiﬁcation rule built following denotes generic certain test object. given labelled certain objects nearest neighbor rule assigns certain test object label nearest neighbor nearest neighbor rule generalized take account nearest neighbors test object nearest neighbor rule whenever value clear context) assigns object class members present among nearest neighbors training applying nearest neighbor rule uncertain data. order applied nearest neighbor rule merely requires availability distance function. context uncertain data diﬀerent similarity measures deﬁned among distance means representing distance expected values uncertain objects expected distance representing mean distances outcomes uncertain objects. thus seemingly faithful strategy correctly classify uncertain data directly exploit nearest neighbor rule order determine training object similar test object return class label also referred naive approach following. however pointed guarantee quality class returned naive approach. speciﬁcally approach defective since return wrong class even probability approaches zero. next illustrative example discussed. example consider figure reporting four -dimensional uncertain training objects whose support delimited circles/ellipsis. certain test object located blue class consists normally distributed uncertain object class consists three uncertain objects bimodal distribution. ease computations probability values noticed object closest according naive approach belonging blue class. however appears probability object closer blue thus outcomes training nearest neighbor comes class naive approach outputs opposite note probability blue class made arbitrarily small adding objects similar already present. objects probability rapidly approaches poor performance nearest neighbor rule explained noticing takes account occurrence probabilities training objects time meaningless strategy presence many objects whose outcome uncertain. following concept probable class introduced takes simultaneously account distribution functions distances separating test object training objects. function outputs argument equals otherwise. informally speaking probability nearest neighbor class summation occurrence probabilities outcomes training nearest neighbor object class clear equations order determine probable class needed compute multi-dimensional integral involving simultaneously possible outcomes test object training objects. following section uncertain nearest neighbor rule introduced provides eﬀective method computing probable class test object according nearest neighbor decision rule. section uncertain nearest neighbor classiﬁcation rule introduced. first concept distance object class deﬁned conducive deﬁnition nearest neighbor class forming basis uncertain nearest neighbor rule. deﬁnitions ﬁrstly introduced binary classiﬁcation task certain test object readily generalized case multiclass setting possibly uncertain test object respectively. complete contribution formally shown rule outputs probable class test object. nearest neighbor class rule. class label certain object. distance denoted random variable whose outcome distance k-th training nearest neighbor class label example figure shows one-dimensional example training composed four uncertain objects. abscissa reports domain values ordinata reports values associated uncertain objects. left right means standard deviations objects left belong class objects belong class consider certain test object dashed blue curve represents probability resp.) denotes abscissa value. example figure reports probabilities associated objects figure together value integral equation computed interval d)). case probability equal moreover also important point enabling nearest neighbor rule handle uncertain data makes robust noise respect case uncertainty associated data ignored. example example consider uncertain objects figure assume disregard uncertainty replacing means. case certain nearest neighbor rule erroneously output blue label since noisy blue object closer ones. contrarily uncertain nearest neighbor correctly classiﬁes since simultaneously considers whole class distribution. generalizing rule. uncertain nearest neighbor rule readily generalized order take account arbitrary values consider possibly uncertain test objects deal multiclass problem accounted following. till binary classiﬁcation problem taken account. order deal multiclass classiﬁcation problem one-against-all -like approach adopted. assume classes equivalence probable class. main properties rule stated. indeed following theorem formally proves uncertain nearest neighbor rule captures right semantics nearest neighbor rule uncertain data taken account. outputs class then among nearest neighbors least objects coming class since majority nearest neighbors class label less objects coming class thus holds outputs class then distance separating k′-th nearest neighbor class stricly smaller distance separating k′-th nearest neighbor class thus case among nearest neighbors least objects coming class objects coming class hence proof theorem proof follows proposition equations noticing random variable representing distance certain test object k-th nearest neighbor class uncertain dataset. section presents classiﬁcation algorithm. first preliminary deﬁnitions provided properties nearest neighbor class probability stated shown probability computed ﬁnally classiﬁcation algorithm described last section discusses steps algorithm accelerated practice. without loss generality assumed uncertain object associated ﬁnite region containing support namely region holds. support inﬁnite ﬁxed small value probability exist outside considered negligible. must noticed assumptions restrictive since error involved calculation probability uncertain objects made arbitrarily small properly selecting regions hence value section presents important properties nearest neighbor class probability. particular shown order compute probability reported equation speciﬁc ﬁnite domain considered instead also speciﬁc subset training objects taken account instead whole training properties direct practical implications since allow computation nearest neighbor class probability means less demanding integration formula section). exist objects class maxdist holds since support within distance thus equation summation subsets size strictly less evaluates zero indeed subset exists least object hence least term productory. term consequence integral equation null computation integral restricted interval value denotes radius greatest hyperball centered contain support training object hence equal mindist. generic object mindist {xj} number objects shown value probability computed sets identical. consider summation equation size |s′| less value subsets summation subsets size less obtained considering following number terms term summation concerning subset less elements terms associated summation particular concerns subset concerns subset ∪{xj}. terms since holds since max). terms since holds concluded summations coincide hence objects safely ignored. also property important practical implication. indeed states that test object given order determine probability computation restricted composed training objects mindist turn depend probabilities moreover functions depend objects given value involve computation multi-dimensional integral domain integration hyper-ball center radius computation probabilities next considered general case arbitrarily shaped multi-dimensional pdfs domain d-dimensional euclidean space known given function points randomly selected according given following approximation holds thus order compute value function otherwise integrated evaluating formula equation points randomly selected according procedure reduces compute relative number sample points lying distance greater precisely exploiting kind strategy suitable approximation whole cumulative distribution function computed single integration operation shown following. indeed probability corresponds probability lies within distance exactly objects among ﬁrst objects within distance plus probability within distance exactly objects among ﬁrst objects within distance technically probability computed means dynamic programming procedure similarly shown procedure makes stores probability upper triangular matrix namely elements main diagonal equal ﬁrst computed applying properties above. then procedure ﬁlls matrix applying equation value ﬁnally computed exploiting elements last computation class probability. order compute integral reported equation histogram composed slots associated class particular slot stores value computed exploiting procedure described section test object uncertain nearest neighbor probability class expressed integral reported equation using formula equation generating points according value integral equation obtained proposition step determines step classes less objects object safely assigned class. otherwise nearest neighbor class probability must computed accounted subsequent steps exploiting technique described section temporal cost. temporal cost algorithm concerned steps cost number training objects cost evaluating distance certain objects. cardinality step costs step costs step involves computation histograms costs noticed term nqkh negligible respect term since small integer number experimentally veriﬁed provides good quality results. summarizing temporal cost algorithm expected much smaller uncertain test objects order classify summation equation computed. accomplished executing times algorithm figure total temporal cost additional spatial cost. spatial cost. spatial cost algorithm concerned method needs store training identiﬁers objects histograms histograms consisting ﬂoating point numbers. summarizing spatial cost noted step algorithm corresponds nearest neighbor query search respect value maxdist step corresponds range query search radius respect value mindist. certain object certain objects. denote positive real value then following relationships satisﬁed thus introduced inequalities used pruning rules embedded exiting certain similarity search methods metric spaces pivot-based indexes vp-trees others order fasten execution steps however practice execution time algorithm take advantage strategy cost computing probability comparable cost computing distance center test object center training object moreover number training objects large besides literature concerning classic nearest neighbor rule works related present concern similarity search methods uncertain data classiﬁcation presence uncertainty. several similarity search methods designed eﬃciently retrieve similar objects query object designed methods partitioned suitable vector spaces allow geometric coordinate information applicable general metric spaces information unavailable. certain nearest neighbor rule beneﬁt methods since fasten search nearest neighbor test object. moreover discussed section methods employed within technique described order accelerate basic steps computation nearest neighbor class. objects proposed literature among distance means expected distance probabilistic threshold distance based notions similarity search methods designed eﬃciently retrieve similar objects query object also designed. problem searching uncertain data ﬁrst introduced authors considered problem querying one-dimensional real-valued uniform pdfs. various pruning methods avoid expensive expected distance calculation introduced. since expected distance metric triangle inequality involving pre-computed expected distances anchor objects uncertain data objects straightforwardly employed order prune unfruitful distance computations. considered problem indexing categorical uncertain data. answer uncertain queries introduced concept probabilistic constrained rectangles object. introduced technique eﬃciently answer range queries uncertain objects general metric spaces. certain neighbor classiﬁcation almost directly built eﬃcient indexing techniques nearest neighbor search already showed straight uncertain nearest neighbor search methods classiﬁcation purposes leads poor decision rule uncertain scenario. thus must pointed method loosely related uncertain nearest neighbor indexing techniques. moreover eﬃciency concerned none indexing methods straightforwardly employed improve execution time since tailored speciﬁc notion similarity among uncertain objects relies concept nearest neighbor class directly built certain similarity metrics. recently several mining tasks investigated context uncertain data including clustering frequent pattern mining outlier detection particularly classiﬁcation methods dealing uncertain data proposed literature among considered problem classifying uncertain data represented means distributions sequences weighted automata extended support vector machines deal distributions using general kernels weighted automata. kind technique particularly suited natural language processing applications. investigates learning model input data corrupted noise. assumed input objects subject additive noise certain object noise follows speciﬁc distribution. speciﬁcally bounded uncertainty model considered ||∆xi|| uniform priors novel formulation support vector classiﬁcation called total support vector classiﬁcation algorithm proposed manage kind uncertainty. method handling error-prone missing data density based approaches presented. estimated error associated dimension d-dimensional data point denoted error value example standard deviation observations large number measurements. basic idea framework construct error-adjusted density data exploiting kernel density estimation then density intermediate representation order perform mining tasks. algorithm classiﬁcation problem presented consisting density based adaptation rule-based classiﬁers. intuitively methods seeks subspaces instance-speciﬁc local density data particular class signiﬁcantly higher density overall data. must noticed none methods investigates extension nearest neighbor decision rule handling uncertain data. moreover experimental section comparison density based methods classiﬁcation investigated. experiments organized follows. section studies eﬀect disregarding data uncertainty classiﬁcation accuracy. section investigates behavior test objects whose label independent theoretical prediction sensitivity noise. section reports execution time using certain uncertain test objects. section compares approach proposed density based classiﬁcation methods uncertain data. section describes real-life scenario data naturally modelled multi-dimensional pdfs. table reports datasets employed experiments characteristics. datasets repository ionosphere dataset projected principal components. dataset listed family uncertain training sets obtained. training family characterized parameter used determine degree uncertainty associated dataset objects. particular certain object original dataset uncertain object randomly normal uniform distribution mean support depending parameter particular randomly generated number interval denotes standard deviation dataset along coordinate algorithms implemented compared namely random eknn algorithms. random algorithm approximates expression reported equation randomly generating outcomes uncertain training hence determines probable class test object eknn algorithm randomly generates outcomes uncertain training classiﬁes test objects applying nearest neighbor rule training ﬁnally reports average label reported random algorithm employed true label. hence accuracy classiﬁcation algorithm compared accuracy eknn algorithm test set. since experiment computes accuracy eknn algorithm respect theoretical prediction determines certain nearest neighbor rule expected perform generic outcome uncertain dataset. words experiment measures accuracy classiﬁcation strategy based disregarding data uncertainty approach encoding object means single measurement employing certain nearest neighbor rule perform classiﬁcation. accuracy moreover compared uncertain nearest neighbor rule which conversely takes account underlying uncertain data distribution. figure shows accuracy eknn methods various values spread clear accuracy high spreads almost always close discrepancies theoretical prediction whose number slightly increases uncertainty data fact approximate computations employed random algorithm. eknn algorithm concerned clear results prediction inaccurate. particular greater level uncertainty data smaller accuracy. recall certain datasets classiﬁcation rules coincide. experiments diﬀerence accuracy eknn respect reach correspondence largest value spread considered. eﬀect parameter appears accuracy eknn gets better larger values though remains unsatisfactory cases. behavior justiﬁed considering rule used generate test objects. objects represent mean randomly selected points hence large fraction outside decision boundary. test objects sorrounded objects class majority vote tends approximate probable class particularly true small spreads since region order study behavior critical test objects objects located along decision boundary experiment repeated thousand test objects called border test objects determined explained next. generic border test object obtained randomly selected certain dataset objects satisﬁes condition mean distances diﬀerence within percent) objects behavior similar exhibited random test objects. table reports accuracy eknn border test objects various values spread nearest neighbors clear accuracy eknn deteriorates accuracy decrease additional percent respect previous experiment. moreover advantage figure shows accuracy eknn random uncertain test objects. uncertain test objects obtained centering multi-dimensional pdfs generated according policy used training objects certain test objects employed experiment figure trend curves similar associated curves obtained certain test objects. particularly many cases accuracy eknn worsens percentage points respect certain test objects. explained since experiment data uncertainty increased. range values spread number nearest neighbors considered respectively employed experiment described previous section. eknn executed uncertain version dataset executed certain dataset. accuracy measured means fold cross validation. note that certain dataset assimilated generic outcome hypothetical true uncertain dataset unknown uncertain dataset employed syntetically generated using arbitrary distributions centered certain dataset objects intended represent true uncertain dataset. thus important point purpose experiment neither demonstrate peforms better show better classiﬁcation results achieved injecting uncertainty data. rather goal experiment study behavior test objects whose label independent theoretical prediction particularly appreciate sensitivity noise. accuracy employed baseline assess accuracy since output represents classiﬁcation achieved considered datasets nearest neighbor classiﬁcation rule uncertainty disappears. figure shows result experiment. curves report accuracy eknn ionosphere haberman transfusion datasets accuracy knn. moreover latter datasets accuracy slightly increasing data uncertainty diﬀerence accuracy justiﬁed noticing mitigates eﬀect noisy points since takes simultaneously account whole class probability according theoretical analysis depicted comparison eknn concerned former method performs always better latter thus conﬁrming result analysis conducted previous section. eﬀect parameter accuracy already discused accuracy eknn improves larger values however well-known diﬃcult select nearly optimum value approach lowest possible probability error. particular increases beyond certain value depends nature dataset probability error begin increase. plots show achieves good results using smallest possible value diﬀerent cases maximum accuracy achieved values smaller greatest value considered figure show execution time haberman iris transfusion datasets certain test objects employed. clearly execution time increases data uncertainty larger spread greater execution time; moreover classifying uncertain test objects requires time classifying certain ones. indeed larger uncertainty larger radius consequently number integrals computed. following table reports relative execution time ratio execution time algorithm time needed compute integral equation training objects taken account. thus table shows time savings obtained exploiting techniques reported section relative execution times reported table show properties exploited accelerate computation guarantee time savings cases. certain test objects cases relative execution time approximatively cases even close e.g. haberman iris datasets. also uncertain test objects many cases approximatively though diﬀerent cases much smaller. spread considerable fraction dataset objects within distance test object hence relative execution time increases. eﬀect evident uncertain test objects taken account. however noted spread large fact case supports training objects rather wide tend partially overlap. experimental setting described considered. following methodology therein proposed uncertain dataset generated starting certain described next. first numerical attributes taken account number. then object original certain dataset uncertain object normal distribution mean standard deviation equal standard deviation dataset objects along j-th attribute. thus value spread determines uncertainty level dataset varied range numerical attributes. latter dataset contains data forest cover type four areas located northern colorado. consists objects numerical attributes. figure reports experimental results. cases exhibited better classiﬁcation accuracy density based algorithm. accuracy methods degrades spread noticed diﬀerence case case substantial methods justiﬁed noticing latter case level uncertainty high dataset objects overlapping domain. however shows sensibly accurate levels uncertainty thus conﬁrming eﬀectiveness concept nearest neighbor class. section describes real-life prediction scenario data naturally modelled means multi-dimensional continuous pdfs general form uncertain objects managed technique introduced illustrates meaningfulness uncertain nearest neighbor classiﬁcation within described task. scenario concerns mobile networks manet collection wireless mobile nodes forming self-conﬁguring network without using existing infrastructure. potential applications manets mobile classrooms battleﬁeld communication disaster relief others. mobility model manet designed describe movement pattern mobile users location velocity acceleration change time. frequently used mobility model manet simulations random waypoint model nodes move independently randomly chosen destination randomly selected velocity within certain simulation area. model spatial node distribution node density maximum center simulation area whereas density almost zero around boundary area hence distribution non-uniform. moreover matter fast nodes move spatial node distribution certain position determined location squared area size centered random waypoint model provided following analytical expression networks nodes dynamically enter network well leave nodes manet typically distinguished limited power processing memory resources well high degree mobility. since nodes able re-charged expected time period energy conservation crucial maintaining life-time nodes. goals protocols minimize energy consumption techinques routing data dissemination varying transmission power multiple hops usually needed node exchange information node matter fact needed transmission power inversely proportional squared distance separating transmitter receiver isotropic antenna radiation distance transmitted signal strength path loss factor depends given propagation environment whose value typically since node correctly receive packets signal strength packet node certain threshold since mobile devices exploit variable-range transmission powersave strategy minimum power supplied node connected network thus pdfs naturally model uncertain objects representing mobile devices also called nodes following. experiment described section uncertain training nodes partitioned classes representing diﬀerent manet networks considered. simulation area unit square centered origin. network nodes randomly positioned whole simulation area allowed move squares size means plus-marks) blue network nodes randomly positioned lower-right corner simulation area allowed move squares size means x-marks). certain objects points plane. randomly generated points within simulation square employed applied described dataset returns nearest neighbor class test object network minimizes expected distance position node determined join neighborhood manet uncertain position nodes figure reports classiﬁcation points plane figure points colored according probability belong classes. figure solid black curve represents decision boundary points nearest neighbor class probability equals dashed curves correspond points class probability form decision boundary informative since diﬀers common facets adjacent voronoi cells associated objects belonging opposite classes decision boundary certain nearest neighbor rule. particular observed centers nodes within support blue class justiﬁed noticing centers close centers blue nodes mobility blue nodes smaller ones. accuracy test measured compared eknn. accuracy eknn good performance nearest neighbor based classiﬁcation methods fact power consumption related euclidean distance devices. note that uncertain nearest neighbor rule reports class probably provides nearest neighbor power depends also distribution distance separating transmitter nearest neighbor explains misclassiﬁcations. also experiment performs better eknn explained since former rule bases decision concept nearest neighbor class thus conﬁrming superiority uncertain nearest neighbor rule even respect classical classiﬁcation techniques presence uncertainty. work uncertain nearest neighbor rule representing generalization certain nearest neighbor rule uncertain scenario introduced. provided evidence uncertain nearest neighbor rule correctly models right semantics nearest neighbor decision rule applied uncertain scenario. moreover algorithm perform uncertain nearest neighbor classiﬁcation generic certain test object presented together properties precisely designed signiﬁcantly reduce temporal cost associated nearest neighbor class probability computation. theoretical analysis experimental campaign presented shown proposed algorithm eﬃcient eﬀective classifying uncertain data. broch maltz johnson y.-c. jetcheva performance comparison multi-hop wireless network routing protocols. mobicom proceedings annual acm/ieee international conference mobile computing networking. mic´o oncina vidal version nearest-neighbour approximating eliminating search algorithm linear preprocessing time memory requirements. pattern recognition letters mitchell machine learning. graw hill. mohri learning uncertain data. colt. ngai chui cheng chau eﬃcient clustering wesolowski mobile communication systems. john wiley sons. kumar quinlan ghosh yang motoda mclachlan zhou z.-h. steinbach hand steinberg algorithms data mining. knowl. inf. syst.", "year": 2011}