{"title": "A Mathematical Theory of Deep Convolutional Neural Networks for Feature  Extraction", "tag": ["cs.IT", "cs.AI", "cs.LG", "math.FA", "math.IT", "stat.ML"], "abstract": "Deep convolutional neural networks have led to breakthrough results in numerous practical machine learning tasks such as classification of images in the ImageNet data set, control-policy-learning to play Atari games or the board game Go, and image captioning. Many of these applications first perform feature extraction and then feed the results thereof into a trainable classifier. The mathematical analysis of deep convolutional neural networks for feature extraction was initiated by Mallat, 2012. Specifically, Mallat considered so-called scattering networks based on a wavelet transform followed by the modulus non-linearity in each network layer, and proved translation invariance (asymptotically in the wavelet scale parameter) and deformation stability of the corresponding feature extractor. This paper complements Mallat's results by developing a theory that encompasses general convolutional transforms, or in more technical parlance, general semi-discrete frames (including Weyl-Heisenberg filters, curvelets, shearlets, ridgelets, wavelets, and learned filters), general Lipschitz-continuous non-linearities (e.g., rectified linear units, shifted logistic sigmoids, hyperbolic tangents, and modulus functions), and general Lipschitz-continuous pooling operators emulating, e.g., sub-sampling and averaging. In addition, all of these elements can be different in different network layers. For the resulting feature extractor we prove a translation invariance result of vertical nature in the sense of the features becoming progressively more translation-invariant with increasing network depth, and we establish deformation sensitivity bounds that apply to signal classes such as, e.g., band-limited functions, cartoon functions, and Lipschitz functions.", "text": "digits’ spatial location within image leads requirement translation invariance. addition desirable feature extractor robust respect handwriting styles. accomplished demanding limited sensitivity features certain nonlinear deformations signals classiﬁed. spectacular success practical machine learning tasks reported feature extractors generated so-called deep convolutional neural networks networks composed multiple layers computes convolutional transforms followed non-linearities pooling operators. dcnns used perform classiﬁcation directly typically based output last network layer also stand-alone feature extractors resulting features classiﬁer svm. present paper pertains latter philosophy. mathematical analysis feature extractors generated dcnns pioneered mallat mallat’s theory applies so-called scattering networks signals propagated layers compute semi-discrete wavelet transform followed modulus non-linearity without subsequent pooling. resulting feature extractor shown translation-invariant stable w.r.t. certain non-linear deformations. moreover mallat’s scattering networks lead state-of-the-art results various classiﬁcation tasks contributions. dcnn-based feature extractors found work well practice employ wide range ﬁlters namely pre-speciﬁed structured ﬁlters wavelets pre-speciﬁed unstructured ﬁlters random ﬁlters ﬁlters learned supervised unsupervised fashion non-linearities beyond modulus function namely hyperbolic tangents rectiﬁed linear units logistic sigmoids iii) pooling operators namely sub-sampling average pooling max-pooling addition ﬁlters non-linearities pooling operators different different network layers goal paper develop mathematical theory encompasses elements full generality. abstract—deep convolutional neural networks breakthrough results numerous practical machine learning tasks classiﬁcation images imagenet data control-policy-learning play atari games board game image captioning. many applications ﬁrst perform feature extraction feed results thereof trainable classiﬁer. mathematical analysis deep convolutional neural networks feature extraction initiated mallat speciﬁcally mallat considered so-called scattering networks based wavelet transform followed modulus non-linearity network layer proved translation invariance deformation stability corresponding feature extractor. paper complements mallat’s results developing theory encompasses general convolutional transforms technical parlance general semi-discrete frames general lipschitz-continuous non-linearities general lipschitz-continuous pooling operators emulating e.g. sub-sampling averaging. addition elements different different network layers. resulting feature extractor prove translation invariance result vertical nature sense features becoming progressively translation-invariant increasing network depth establish deformation sensitivity bounds apply signal classes e.g. bandlimited functions cartoon functions lipschitz functions. context handwritten digit classiﬁcation features extracted case correspond example edges digits. idea behind feature extraction feeding characteristic features signals—rather signals themselves—to trainable classiﬁer improves classiﬁcation performance. speciﬁcally non-linear feature extractors input signal space dichotomies linearly separable linearly separable feature space dichotomies sticking example handwritten digit classiﬁcation would moreover want feature extractor invariant ieee international symposium information theory hong kong china. permitted. however permission material purposes must obtained ieee sending request pubs-permissionsieee.org. transforms employed dcnns interpreted semi-discrete signal transforms corresponding prominent representatives curvelet shearlet transforms known highly effective extracting features characterized curved edges images. transforms theory allows general semi-discrete signal general lipschitz-continuous non-linearities incorporates continuous-time lipschitz pooling operators emulate discrete-time subsampling averaging. finally different network layers equipped different convolutional transforms different non-linearities different pooling operators. practice invariance features crucially governed network depth presence pooling operators averagepooling max-pooling show general feature extractor considered paper indeed exhibits vertical translation invariance pooling plays crucial role achieving speciﬁcally prove depth network determines extent extracted features translation-invariant. also show pooling necessary obtain vertical translation invariance otherwise features remain fully translationcovariant irrespective network depth. furthermore establish deformation sensitivity bound valid signal classes e.g. band-limited functions cartoon functions lipschitz functions bound shows small nonlinear deformations input signal lead small changes corresponding feature vector. techniques draw heavily continuous frame theory develop proof machinery completely detached structures semi-discrete transforms speciﬁc form lipschitz non-linearities lipschitz pooling operators. proof deformation sensitivity bound based elements namely lipschitz continuity feature extractor deformation sensitivity bound signal class consideration namely band-limited functions cartoon functions lipschitz functions shown decoupling approach important practical ramiﬁcations shows whenever deformation sensitivity bounds signal class automatically deformation sensitivity bounds dcnn feature extractor operating signal class. results hence establish vertical translation invariance limited sensitivity deformations—for signal classes inherent deformation insensitivity—are guaranteed network structure rather speciﬁc convolution kernels non-linearities pooling operators. notation. complex conjugate denoted write real imaginary part euclidean inner product denote identity matrix rd×d. matrix rd×d designates entry i-th column tensor rd×d×d tijk refers component. supremum norm matrix rd×d deﬁned |m|∞ supij |mij| supremum norm tensor rd×d×d |t|∞ supijk |tijk|. write open ball radius centered stands orthogonal group dimension special orthogonal group. lebesgue-measurable function integral w.r.t. lebesgue measure stands space lebesgue-measurable functions satisfying |pdx)/p denotes space lebesgue-measurable functions inf{α a.e. gdx. space r-band-limited functions denoted countable stands space sets {sq}q∈q satisfying denotes identity operator tensor product functions rd×rd. operator norm bounded linear operator deﬁned supfp= afq. denote fourier transrd e−πixωdx extend usual convolution gdx. write translation operator eπixωf modulation operator. involution deﬁned ordered d-tuple non-negative integers multiindex denotes differential operator order space functions whose derivatives order continuous designated space inﬁnitely differentiable functions stands schwartz space i.e. space functions whose derivatives along function rapidly decaying sense sup|α|≤n supx∈rdn|| denote gradient function space continuous mappings space k-times continuously differentiable mappings written mapping jacobian matrix jacobian tensor associated norms architecture corresponding feature extractor illustrated fig. known scattering network employs frame modulus non-linearity every network layer include pooling. given corresponds features function generated n-th network layer fig. remark function λw\\{} thought indicating locations singularities speciﬁcally relation canny edge detector described dimension think λw\\{} image scale specifying locations edges image oriented direction furthermore argued generated ﬁrst layer feature vector scattering network similar dimension frequency cepstral coefﬁcients dimension sift-descriptors invariance result asymptotic scale parameter depend network depth i.e. guarantees full translation invariance every network layer. furthermore establishes stable w.r.t. deformations form formally function space deﬁned shown exists constant with deformation error satisﬁes following deformation stability bound stage reviewing scattering networks introduced basis multi-layer architecture involves wavelet transform followed modulus non-linearity without subsequent pooling. speciﬁcally deﬁnes feature vector signal λw\\{} directional wavelets mother wavelet elements ﬁnite rotation group subgroup index associated low-pass ﬁlter corresponds coarsest scale resolved directional wavelets family functions {ψλ}λ∈λw taken form semif tbiψλ underlying frame coefﬁcients. note given actually continuum frame coefﬁcients translation parameter left unsampled. refer figure frequency-domain illustration semi-discrete directional wavelet frame. appendix give brief review general theory semi-discrete frames appendices collect structured example frames respectively. invariance induced. practice signal classiﬁcation based scattering networks performed follows. first function wavelet frame atoms {ψλ}λ∈λw discretized ﬁnite-dimensional vectors. resulting scattering network computes ﬁnitedimensional feature vector whose dimension typically reduced orthogonal least squares step feeds result trainable classiﬁer e.g. svm. state-of-the-art results scattering networks reported various classiﬁcation tasks handwritten digit recognition texture discrimination musical genre classiﬁcation already mentioned scattering networks follow architecture dcnns sense cascading convolutions non-linearities namely modulus function without pooling. general dcnns studied literature exhibit number additional features already mentioned purpose paper develop mathematical theory dcnns feature extraction encompasses aspects proviso pooling operators analyze continuous-time emulations discrete-time pooling operators. formally compared scattering networks n-th network layer replace wavelet-modulus operation convolution atoms general semi-discrete frame {tbigλn}b∈rdλn∈λn countable index followed non-linearity satisﬁes lipschitz property output non-linearity pooled according pooling factor satisﬁes lipschitz property next comment individual elements network architecture detail. frame atoms arbitrary therefore also taken structured e.g. weyl-heisenberg functions curvelets shearlets ridgelets wavelets considered corresponding semi-discrete signal transforms brieﬂy reviewed appendices {gλ}λ∈λ functions indexed countable then mapping gλ}b∈rdλ∈λ tbigλ}λ∈λ called semi-discrete signal transform depends discrete indices continuous variables think mapping analysis operator frame theory proviso given actually continuum frame coefﬁcients translation parameter left unsampled. next state deﬁnitions collect preliminary results needed analysis general dcnn feature extractor considered. basic building blocks network triplets associated individual network layers referred modules. deﬁnition {tbigλn}b∈rdλn∈λn semi-discrete frame lipschitz-continuous operators respectively. then sequence triplets employed successfully literature various feature extraction tasks use—apart wavelets—in dcnns appears new. refer reader appendix detailed discussion several relevant example non-linearities framework. next explain continuous-time pooling operator emulates discrete-time pooling sub-sampling averaging consider one-dimensional discrete-time signal |fd| subsampling factor discrete time deﬁned unitarity continuous-time sub-sampling operation. overall operation general deﬁnition pooling recovered simply taking equal identity mapping next consider average pooling. discrete time average pooling deﬁned averaging kernel averaging factor taking function length amounts computing local averages consecutive samples. weighted averages obtained identifying desired weights averaging kernel operation emulated continuous time according averaging window note recovered taking noting convolution lipschitzcontinuous lipschitz constant trivially satisﬁes remainder paper refer operation lipschitz pooling dilation indicate essentially amounts application lipschitz-continuous mapping followed continuous-time wavelet frame namely low-pass ﬁlter singled generate extracted features according also fig. follow construction designate atoms frame module-sequence output-generating atom layer. atoms {gλn}λn∈λn\\{λ∗ {χn−} thus used across generating consecutive layers sense output layer {gλn}λn∈λn\\{λ∗ propagating signals layer n-th layer according fig. note however theory require output-generating atoms low-pass ﬁlters. slight abuse notation well. finally note shall write λn\\{λ∗ extracting features every network layer outputgenerating atom regarded employing skip-layer connections skip network layers feed propagated signals feature vector. corresponds features function generated n-th network layer fig. corresponds root network. feature extractor well-deﬁned i.e. technical condition modulesequence formalized follows. fig. handwritten digits mnist data practical machine learning tasks often want feature vector invariant digits’ spatial location within image theorem establishes features become translation-invariant increasing layer index satisﬁed simply normalizing frame elements accordingly. refer proposition appendix corresponding normalization techniques which explained section affect neither translation invariance result deformation sensitivity bounds. generating atoms feature extractor exhibits vertical translation invariance sense features becoming translation-invariant increasing network depth. result line observations made deep learning literature e.g. informally argued network outputs generated deeper layers tend translation-invariant. start noting pointwise non-linearities satisfy commutation relation large class non-linearities widely used deep learning literature rectiﬁed linear units hyperbolic tangents shifted logistic sigmoids modulus function employed indeed pointwise hence covered theorem moreover pooling sub-sampling trivially satisﬁes pooling averaging satisﬁes consequence convolution operator commuting translation operator generating atoms {χn}n∈n either satisfy independent bound shows explicitly control amount translation invariance pooling factors result line observations made deep learning literature e.g. informally argued pooling crucial translation invariance extracted features. guarantees thanks asymptotically full translation invariance according fig. handwritten digits mnist data denotes image handwritten digit then—for appropriately chosen τ—the function models images based different handwriting styles corresponding shifted versions handwritten digit figs. increasing network depth increasingly look like features corresponding unshifted handwritten digit fig. casually speaking shift operator increasingly upper bound absorbed quantifying absorption. contrast translation invariance result asymptotic wavelet scale parameter depend network depth i.e. guarantees full translation invariance every network layer. honor difference referring horizontal translation invariance vertical translation invariance. corollary shows absence pooling i.e. taking leads full translation covariance every network layer. proves pooling necessary vertical translation invariance otherwise features remain fully translation-covariant irrespective network depth. finally note scattering networks rendered horizontally translation-invariant letting wavelet scale parameter employed change variables regarding average pooling already mentioned operators general unitary still translation invariance consequence structural properties namely translation covariance convolution operator combined unitary dilation according finally note practice certain applications actually translation covariance sense desirable ttφn example facial landmark detection goal estimate absolute position facial landmarks images. applications features layers closer root network relevant less translation-invariant translation-covariant. reader referred corresponding numerical evidence provided. proceed formal statement translation covariance result. class deformations encompasses non-linear distortions illustrated fig. modulation-like deformations eπiωf occur e.g. signal subject undesired modulation therefore access bandpass version only. deformation sensitivity bound derive signal-class speciﬁc sense applying input signals belonging particular class band-limited functions. proof technique develop applies however signal classes exhibit inherent deformation insensitivity following sense. deﬁnition signal class called deformation-insensitive exist fig. impact deformation functions signal class consists smooth slowly varying functions consists compactly supported functions exhibit discontinuities observe unlike affected mildly fτω. amount deformation induced therefore depends drastically speciﬁc constant exponents depend particular signal class examples deformation-insensitive signal classes class rband-limited functions class cartoon functions class lipschitz functions deformation sensitivity bound applies would desirable example fig. illustrates difﬁculty underlying desideratum. speciﬁcally fig. given impact deformation induced eπiωf depend drastically function itself. deformation stability bound scattering networks reported applies signal class well characterized albeit implicitly depending mother wavelet non-linearity. signal-class speciﬁc deformation sensitivity bound based following ingredients. first establish— proposition appendix i—that feature extractor lipschitz-continuous lipschitz constant i.e. where thanks admissibility condition lipschitz constant completely independent frame upper bounds lipschitz-constants respectively. second derive— proposition appendix j—an upper bound deformation error fτωf r-band-limited functions i.e. deformation sensitivity bound feature extractor obtained setting fτωf using decoupling lipschitz continuity deformation sensitivity bound signal class consideration important practical ramiﬁcations shows whenever deformation sensitivity bound signal class automatically deformation sensitivity bound feature extractor thanks lipschitz continuity. approach used derive deformation sensitivity bounds cartoon functions lipschitz functions. lipschitz continuity according also guarantees pairwise distances input signal space increase feature extraction. immediate consequence robustness feature extractor w.r.t. additive noise sense first note bound holds sufﬁciently small jacobian matrix i.e. long think condition jacobian matrix follows image handwritten digit then {fτωf images handwritten digit fτωf models image generated e.g. based different handwriting style condition imposes quantitative limit amount deformation tolerated. deformation sensitivity bound provides limit much features corresponding images {fτωf strength theorem derives fact condition underlying module-sequence needed admissibility according outlined section easily obtained normalizing frame elements appropriately. normalization impact constant speciﬁcally shown completely independent thanks decoupling technique used prove theorem completely independent structures frames speciﬁc forms lipschitz-continuous operators deformation sensitivity bound general sense applying lipschitz-continuous mappings generated dcnns. bound scattering networks reported depends upon ﬁrst-order secondorder derivatives contrast bound depends implicitly need impose condition bound hold. honor translation invariance result theorem deformation sensitivity bound theorem fact entire theory paper carries long collections {tbigλn}b∈rdλn∈λn satisfy bessel pre-speciﬁed unstructured ﬁlters learned ﬁlters therefore covered theory long satisﬁed. classical frame theory guarantees completeness {tbigλn}b∈rdλn∈λn signal space consideration absence frame lower bound therefore translates lack completeness result frame coefﬁcients tbigλn λn×rd containing essential features signal will general impact practical feature extraction performance ensuring entire frame property prudent. interestingly satisfying frame property does however guarantee feature extractor trivial null-space i.e. refer reader example feature extractor non-trivial null-space. appendix gives brief review theory semidiscrete frames. list structured example frames interest context paper provided appendix case appendix case. semidiscrete frames instances continuous frames appear literature e.g. context translationcovariant signal decompositions intermediate step construction various fully-discrete frames ﬁrst collect basic results semi-discrete frames. deﬁnition {gλ}λ∈λ functions indexed countable collection dependence upper bound bandwidth reﬂects intuition deformation sensitivity bound depend input signal class description complexity. many signals practical signiﬁcance however either band-limited presence sharp edges exhibit large bandwidths. latter case bound effectively rendered void owing linear dependence refer reader deformation sensitivity bounds non-smooth signals established. speciﬁcally main contributions deformation sensitivity bounds—again obtained decoupling—for non-linear deformations according signal classes cartoon functions lipschitz-continuous functions. constant exponent depend particular signal class speciﬁed vertical translation invariance result theorem applies results established present paper taken together show vertical translation invariance limited sensitivity deformations—for signal classes inherent deformation insensitivity—are guaranteed feature extraction network structure rather speciﬁc convolution kernels non-linearities pooling operators. contrast deformation sensitivity bound applies provably space r-band-limited functions finally space depends wavelet frame atoms {ψλ}λ∈λw non-linearity thereby trivunderlying signal transform whereas ially independent module-sequence structural example frames consider weylheisenberg frames obtained modulation prototype function wavelet frames obtained scaling mother wavelet. semi-discrete weyl-heisenberg frames weylheisenberg frames well-suited extraction sinusoidal features applied successfully various practical feature extraction tasks semidiscrete weyl-heisenberg frame collection functions according eπikxg prototype function atoms {gk}k∈z satisfy littlewood-paley condition according popular function satisfying gaussian function semi-discrete wavelet frames wavelets well-suited extraction signal features characterized singularities applied successfully various practical reader might want think semi-discrete frames shift-invariant frames continuous translation parameter countable index labeling collection scales directions frequency-shifts hence terminology semi-discrete. instance scattering networks based semi-discrete wavelet frame following proposition states normalization results semi-discrete frames come handy satisfying admissibility condition discussed section iii. proposition {tbigλ}∈λ×rd semidiscrete frame frame bounds ﬁxed rotation angles. functions referred literature coarse-scale wavelet ﬁne-scale wavelet respectively. integer corresponds coarsest scale resolved atoms {g}∈λdw satisfy littlewood-paley condition according semi-discrete curvelet frame collection functions according {tbig}∈λcb∈r rotation matrix deﬁned πl−j/− scale-dependent rotation angles. functions satisfy littlewood-paley condition according feature extraction tasks semi-discrete wavelet frame collection functions according mother wavelet atoms {gk}k∈z satisfy littlewood-paley condition according semi-discrete curvelet frames curvelets introduced well-suited extraction signal features characterized curve-like singularities applied successfully various practical feature extraction tasks frames two-dimensional wavelets well-suited extraction signal features characterized point singularities applied successfully various practical feature extraction tasks e.g. prominent families two-dimensional wavelet frames tensor wavelet frames directional wavelet frames reverse triangle inequality. furthermore obviously ﬁnally pointwise satisﬁed |x|. rectiﬁed linear unit rectiﬁed linear unit non-linearity deﬁned max{ re)} max{ im)}. semi-discrete ridgelet frames ridgelets introduced well-suited extraction signal features characterized straight-line singularities applied successfully various practical feature extraction tasks a.e. λr\\{} designed constant direction speciﬁed parameter fourier transforms supported pair opposite wedges size dyadic corona fig. refer reader constructions functions satisfying remark examples interesting structured semi-discrete frames refer discusses semidiscrete shearlet frames deals semidiscrete α-curvelet frames. appendix gives brief overview non-linearities widely used deep learning literature theory. example establish satisﬁes conditions theorems corollary speciﬁcally need verify following lipschitz continuity exists constant where again employed triangle inequality. before upper-bound show lipschitzcontinuous. speciﬁcally apply lemma hence supx∈r |sig| conclude lipschitz-continuous used yields establishes lipschitz continuity lipschitz constant furthermore obviously ﬁnally satisﬁed max{ max{ im}. hyperbolic tangent hyperbolic tangent non-linearity deﬁned where again used triangle inequality. order upper-bound show tanh lipschitzcontinuous. make following result. lemma continuously differentiable function satisfying supx∈r then lipschitzcontinuous lipschitz constant noting established above gλn+ assumption follows thanks young’s inequality lipschitz property i.e. mn+− mn+h ln+fq gλn+ pn+h rn+fq gλn+ together mn+h pn+h upper-bound term inside according substituting second term inside upper bound resulting insertion idea proof is—similarly proof theorem upper-bound deviation perfect covariance frequency domain. ease notation again thanks admissibility condition thus ﬁrst write already mentioned beginning section iv-b proof deformation sensitivity bound based ingredients. ﬁrst stated proposition appendix establishes feature extractor lipschitz-continuous lipschitz constant i.e. remark proposition generalizes shows wavelet-modulus feature extractor generated scattering networks lipschitz-continuous lipschitz constant speciﬁcally generalization allows general semi-discrete frames general lipschitz-continuous non-linearities general lipschitz-continuous operators different different layers. moreover thanks admissibility condition lipschitz constant completely independent frame upper bounds lipschitz-constants respectively. proof. idea proof again—similarly proof proposition appendix e—to judiciously employ telescoping series argument. ease notation thanks admissibility condition thus start writing remark similar bound derived scattering networks namely c−j+dτ∞f low-pass ﬁlter semi-discrete directional wavelet frame techniques proving related sense employing schur’s lemma taylor series expansion argument signal-class speciﬁcity bound comes technical elements detailed beginning proof. compact set. next choose compact sets thanks assumption function continuous composition continuous functions therefore also lebesgue-measurable. ﬁrst step follows second step thanks fubini-tonelli theorem noting lebesguemeasurable non-negative last step next need verify conditions determine corresponding fact seek speciﬁc constant form here again follows application fubinitonelli theorem noting functions non-negative continuous compositions continuous functions. inequality follows change variables argument similar combining ﬁnally follows application fubini-tonelli theorem noting functions |∇γ−u)| non-negative continuous compositions continuous functions. finally using thus lecun boser denker henderson howard hubbard jackel handwritten digit recognition back-propagation network proc. international conference neural information processing systems rumelhart hinton williams learning internal representations error propagation parallel distributed processing explorations microstructure cognition press cisse bojanowski grave dauphin usunier parseval networks improving robustness adversarial examples proc. international conference machine learning huang lecun large-scale learning convolutional nets generic object categorization proc. ieee international conference computer vision pattern recognition ranzato huang boureau lecun unsupervised learning invariant feature hierarchies applications object recognition proc. ieee international conference computer vision pattern recognition ranzato poultney chopra lecun efﬁcient learning sparse representations energy-based model proc. international conference neural information processing systems serre wolf poggio object recognition features inspired visual cortex proc. ieee international conference computer vision pattern recognition mutch lowe multiclass object recognition sparse localized features proc. ieee international conference computer vision pattern recognition bruna mallat invariant scattering convolution networks ieee trans. pattern anal. mach. intell. vol. sifre rigid-motion scattering texture classiﬁcation. thesis centre math´ematiques appliqu´ees ´ecole polytechnique paris-saclay glorot bengio understanding difﬁculty training deep feedforward neural networks proc. international conference artiﬁcial intelligence statistics kaiser friendly guide wavelets. birkh¨auser rudin functional analysis. mcgraw-hill antoine murrenzi vandergheynst twodimensional wavelets relatives. cambridge university press davis mermelstein comparison parametric representations monosyllabic word recognition continuously spoken sentences ieee trans. acoust. speech signal process. vol. thomas wiatowski born strzelce opolskie poland december received degrees mathematics technical university munich germany respectively. researcher institute computational biology helmholtz zentrum munich germany. joined zurich graduated degree research interests deep machine learning mathematical signal processing applied harmonic analysis. helmut b¨olcskei born m¨odling austria received dipl.-ing. techn. degrees electrical engineering vienna university technology vienna austria respectively. vienna university technology. postdoctoral researcher information systems laboratory department electrical engineering department statistics stanford university stanford founding team iospan wireless inc. silicon valley-based startup company specialized multiple-input multiple-output wireless systems high-speed internet access cofounder celestrius zurich switzerland. assistant professor electrical engineering university illinois urbana-champaign. zurich since professor electrical engineering. visiting researcher philips research laboratories eindhoven netherlands enst paris france heinrich hertz institute berlin germany. research interests information theory mathematical signal processing machine learning statistics. received ieee signal processing society young author best paper award ieee communications society leonard abraham best paper award vodafone innovations award golden teaching award fellow ieee eurasip fellow distinguished lecturer ieee information theory society erwin schr¨odinger fellow austrian national science foundation included thomson reuters list highly cited researchers computer science padovani lecturer ieee information theory society. served associate editor ieee transactions information theory ieee transactions signal processing ieee transactions wireless communications eurasip journal applied signal processing. editor-in-chief ieee transactions information theory period served editorial board ieee signal processing magazine currently editorial boards foundations trends networking foundations trends communications information theory. co-chair ieee international symposium information theory ieee information theory workshop serves board governors ieee information theory society. delegate president zurich faculty appointments since dettori semler comparison wavelet ridgelet curvelet-based texture classiﬁcation algorithms computed tomography computers biology medicine vol. vaidyanathan multirate systems ﬁlter banks. prentice hall grafakos classical fourier analysis. springer wiatowski tschannen stani´c grohs b¨olcskei discrete deep feature extraction theory architectures proc. international conference machine learning searc´oid metric spaces. springer brent osborn smith note best possible bounds determinants matrices close identity matrix linear algebra applications vol.", "year": 2015}