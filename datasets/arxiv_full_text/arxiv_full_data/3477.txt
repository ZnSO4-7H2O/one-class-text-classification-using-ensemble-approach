{"title": "Jointly Attentive Spatial-Temporal Pooling Networks for Video-based  Person Re-Identification", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Person Re-Identification (person re-id) is a crucial task as its applications in visual surveillance and human-computer interaction. In this work, we present a novel joint Spatial and Temporal Attention Pooling Network (ASTPN) for video-based person re-identification, which enables the feature extractor to be aware of the current input video sequences, in a way that interdependency from the matching items can directly influence the computation of each other's representation. Specifically, the spatial pooling layer is able to select regions from each frame, while the attention temporal pooling performed can select informative frames over the sequence, both pooling guided by the information from distance matching. Experiments are conduced on the iLIDS-VID, PRID-2011 and MARS datasets and the results demonstrate that this approach outperforms existing state-of-art methods. We also analyze how the joint pooling in both dimensions can boost the person re-id performance more effectively than using either of them separately.", "text": "person re-identiﬁcation crucial task applications visual surveillance humancomputer interaction. work present novel joint spatial temporal attention pooling network video-based person re-identiﬁcation enables feature extractor aware current input video sequences interdependency matching items directly inﬂuence computation other’s representation. speciﬁcally spatial pooling layer able select regions frame attention temporal pooling performed select informative frames sequence pooling guided information distance matching. experiments conduced ilids-vid prid- mars datasets results demonstrate approach outperforms existing stateof-art methods. also analyze joint pooling dimensions boost person re-id performance effectively using either separately person re-identiﬁcation viewed subproblems generic object recognition task. also important applications surveillance human-computer interaction communities. given query image task identify matching person images pool usually captured same/different cameras different viewpoints same/different time points. challenging task large variations lighting conditions viewing angles body poses occlusions. figure sample video frames person captured three cameras simulating human compare different video pairs. regions cycles parts visual attentions drawn tation learning distance metric learning cnn-based schemes recently researchers began explore solving problem video-based setting natural perform re-identiﬁcation. intuition kind methods temporal information related person motion captured video. moreover sequences images provide rich samples persons’ appearances helping boosting re-identiﬁcation performance discriminative features. temporal deep neural network architecture combines optical recurrent layers mean-pooling achieves reasonable success. work exploited novel recurrent feature aggregation framework capable learning discriminative sequence level representation simple frame-wise features. main idea video-based methods ﬁrst extract useful representations video images models. exploit distance function judge extent matching. however approaches derive sequence’s representation separately rarely considering impact others neglect mutual inﬂuence video sequences context matching task. let’s think human visual processing works comparing video sequences. example pair-wise case described figure comparing video frames separately different natural brain draw different focuses different frames hand interaction compared sequences also effect spatial dimension guides human attentions different regions input extremely important scenario large viewpoint changes fast moving object. example demonstrates draw different attention comparing different pairs video frames. motivated recent success attention models proposed jointly attentive spatial-temporal pooling networks powerful mechanism learning representation video sequences taking account interdependence among them. speciﬁcally astpn ﬁrst learns similarity measure features extracted recurrent-convolutional networks input items uses similarity scores features compute attention vectors spatial temporal dimensions. next attention vectors used perform pooling. finally siamese network architecture deployed attention vectors. proposed architecture trained efﬁciently end-to-end training schema. perform extensive experiments three datasets ilids-vid prid- mars. results clearly demonstrate proposed method person reidentiﬁcation outperforms well established baselines significantly offers state-of-the-art performance. cross dataset test also derives conclusion. astpn also general component handle wide variety person re-identiﬁcation tasks. person re-id challenging task explored several years still remains focused overcome problems viewpoint difference illumination change occlusions even similar appearance different people. majority recent works mainly develop solutions aspects extracting reliable feature representations learning robust distance metric speciﬁc features including color histograms texture histograms local binary patterns color names widely utilized person re-id address identity information existence challenges like lighting change. meantime metric learning methods figure video-based person re-identiﬁcation system. adopt siamese network architecture spatial-temporal feature extraction jointly attentive spatial-temporal pooling interdependence information learning. large margin nearest neighbor mahalanobis distance metric locally adaptive decision function ranksvm also applied person re-id task. despite prominent progress recent years works still based image-to-image level. video setting intuitively close practical scenario video ﬁrst-hand material captured surveillance camera besides temporal information relevant person’s motion gait instance help discriminate similar pedestrians. moreover video provides abundant samples target cost increasing computation. gradually works began explore videoto-video matching problem person re-id. discriminative video ranking model used discriminative video fragments selection capture accurate space-time information simultaneously learning video ranking function person re-id. bag-of-words method aimed encode frame-wise features global vector. however neither models could considered effective ignoring rich temporal information contained videos. however video-based person re-id raises challenges inter-class difference video-based representation much ambiguous compared using image-based representation since it’s likely different people could similar appearance also similar motions making alignment tough achieve. therefore space-time information must fully utilized solve extra problems. besides top-push distance learning model proposed effectively make space-time information top-push constraint quantify ambiguous video representation deep learning offers approach solve feature representation metric learning problem time. typical architecture composed parts feature extracting network usually multiple metric learning layers make ﬁnal prediction. ﬁrst siamese-cnn structure proposed person re-id leveraged scnns three overlapped parts image. exploited novel recurrent feature aggregation framework capable learning discriminative sequence level representation frame-wise features. recent work used obtain feature representation multiple frames video applied learn interaction them. temporal pooling layer followed recurrent layer aiming capture sequential interdependence layers jointly trained function feature extractor. however maxpooling mean-pooling adapted robust enough compress produce person’s appearance period time since max-pooling employed active feature temporal step whole sequence mean-pooling produced representation averaged time steps thus couldn’t preclude impact ineffective features well. importantly re-id frameworks usually take form similarity measure inputs. prior works ignored mutual inﬂuence items performing representation learning. thus would like introducing attention mechanism already achieved great success image caption generation machine translation question-answering well action recognition presented comparative attention architecture addressed problem spatial dimension. proposed two-way attention mechanism matching text sequence exploited framework temporal pooling component. work builds recurrent-convolutional network jointly attentive spatial-temporal pooling video-based person re-identiﬁcation. astpn architecture works passing pair video sequences siamese networks obtain representations producing euclidean distance them. shown figure input passed network extract feature maps last convolutional layer. feature maps spatial pooling layer obtain image-level representation time step. that take temporal information consideration utilizing recurrent network generate feature video sequence. finally time steps resulting recurrent network combined attentive temporal pooling form figure jointly attentive spatial pooling architecture. conv last convolutional layer. spatial pooling layer spatial pyramid pooling structure multi-level spatial bins image-level representation generated joining pooling outputs spatial bin. crucial part astpn relies jointly attentive spatial-temporal pooling layers. instead using general pooling over-time temporal pooling layers pooling mechanism could take information form distance step allowing model attentive region interests image level effective time step sequence level. moreover attentive spatial-temporal pooling also makes model adaptive image sequence arbitrary resolution/length. detailed techniques attentive spatial temporal pooling presented following subsections. spatial pooling layer person re-identiﬁcation overlooking angle surveillance equipment pedestrians take part whole spatial images. therefore local spatial attention necessary deep networks. design layer generate multi-scales region patches image feed rnn/attention pooling layer; make model robust image sequence arbitrary resolution/length. work spatial pyramid pooling layer component attentive spatial pooling concentrate model important region spatial dimension. shown figure layer multi-level spatial bins generate multi-level spatial representations representations combined ﬁxed-length image-level representation. since image-level representations involve pedestrian position multi-scale spatial information joint attentive spatial pooling mechanism able select regions frame. represents pooling function window size stride str. denote ceiling ﬂoor operations. means reshape operation reshapes matrix vector. besides denotes vector connec hidden state containing information previous time step output time fully-connected weight rl×n projects recurrent layer input rn×n projects hidden state notice recurrent layer embeds feature vector lower-dimensional feature matrix hidden state initialized zero ﬁrst time step time steps hidden state passed tanh activation function. attentive temporal pooling layer although recurrent layer able capture temporal information hidden states temporarily contains much redundant information. instance minor changes series continuous frames shown figure thus features learned sequence input involve redundant information ambiguous background clothing. order avoid money drives good issue propose attentive temporal pooling architecture enable model concentrate effective information. attentive temporal pooling reinforces pooling layer perceive input probe gallery data pair allows probe input sequence directly inﬂuence computation gallery sequence representation attentive temporal pooling layer recurrent layer distance computation layer. training phase attentive temporal pooling jointly learning recurrent-convolutional network spatial pooling layer guiding model effective information extraction temporal dimension. matrices rt×n rt×n whose i-th represents output recurrent layer i-th time step probe data gallery data respectively compute attention matrix rt×t follows rn×n intent information sharing matrix learned networks. convolution recurrent layer employed obtain matrix attention matrix able sight probe gallery sequence features computes weight scores temporal dimension. gradient descent phase updated back propagation inﬂuences parameters convolution hidden state guide model focus effective information. next apply column-wise row-wise pooling respectively obtain temporal weight vector i-th element represents importance score i-th frame probe sequence participation computation vector capture attentive scores gallery features related probe data. that apply softmax function temporal weight vectors generate attention vectors softmax function transforms i-th weight attention ratio instance i-th element computed follows main thought work construct feature extracting network able sequence data feature vector dimensional space feature vectors sequences person close feature vectors sequences different persons separated margin. details components proposed network explained follows. input input network consists three color channels optical ﬂow. color channels provide spatial information clothing background optical channels provide temporal motion information. compared color channels input figure attentive temporal pooling architecture. rnns output matrices compute attention matrix introduce parameter matrix capture attentive score temporal dimension. column/row-wise pooling operation softmax function attention vector obtained contains attentive weight time step. sequence-level representation computed product feature matrices attention vectors siamese network siamese network architecture shown figure mentioned above network architecture grouped four functional parts convolutional layers attentive spatial pooling layer recurrent layer attentive temporal pooling layer. convolutional layers convolutional architecture parameters shown table pooling layer ﬁnal layer replaced attentive spatial pooling layer. notice convolutional layers unrolled along recurrent layer layers share parameters time steps means frames passed spatial feature extractor. similarly recurrent layers also share parameters process pair sequences input. training objective given pair sequences persons sequence-level representations obtained siamese network. that euclidean distance hinge loss train model follows denotes margin separate features different persons hinge loss. training phase network shown positive negative input pairs alternately. testing phase sequence input copy sequence form pair pass pair siamese network obtain identity feature. computing distance identity feature previously saved features identities similar identity indicated lowest distance. addition also take identity classiﬁcation loss consideration following work apply softmax regression ﬁnal features predict identity persons. using cross-entropy loss obtain identity loss since joint learning siamese loss identity loss brings great promotion ﬁnal training objective combination siamese loss identity loss evaluate model video-based person re-id three different datasets ilids-vid prid- mars also investigate joint pooling strategy bring beneﬁt proposed network difference adapting attentive temporal pooling common temporal pooling strategies attentive spatial pooling. frames forming image sequence ranges average length challenging dataset created airport arrival hall multi-camera cctv network whose image sequences accompanied clothing similarities among people lighting viewpoint variations cluttered background occlusions. prid- re-id dataset consists image sequences people captured cameras adjacent other. image sequence composed frames length average number it’s captured relatively simple environments rare occlusions compared ilids-vid dataset. following split whole human sequence pairs ilids-vid prid- randomly subsets equal size. used training used testing. report performance average cumulative matching characteristics curves trials different train/test splits. data augmentation done several forms. firstly since probe gallery sequences variable-length sub-sequences consecutive frames chosen randomly epoch training process. considered ﬁrst camera probe second camera gallery during testing. secondly positive pair composed subsequence camera sub-sequence camera containing person negative pair composed sub-sequence camera person sub-sequence camera person selected arbitrarily rest people training set. positive negative sequence pairs sent system successively model capable distinguishing correct match wrong match. lastly image level augmentation performed cropping mirroring. sub-image width length pixels less progenitor produced cropping ﬁxed cropping area within sequence. mirroring operation randomly applied whole sequence together probability test data also underwent augmentation eliminate bias. preprocessing steps included following actions images converted color space ﬁrstly color channel normalized zero mean unit variance; optical vertical horizontal extracted pair adjoining images using lucas-kanade method optical channels normalized range learning rate network trained stochastic gradient descent beginning batch size one. challenging viper person re-identiﬁcation dataset besides margin siamese cost function dimension feature space alternately showed siamese network positive negative sequence pairs full epoch consisted equal number both. training contains people maximum sequence length takes approximately hours train epochs using nvidia gtx- gpu. recurrent feature aggregation network based lstm aggregates frame-wise human region representation time stamp produces sequence-level representation. spatio-temporal body-action model takes video walking person input builds spatiotemporal appearance representation pedestrian reidentiﬁcation. comparing results proposed architecture rnn-cnn method systems ilidsvid conclude attentive mechanism enables network outperform mentioned networks large margin. note even rank- matching rate method also achieves exceeding rnn-cnn method notice even withattentive spatial pooling layer utilization attentive temporal pooling still lead fairly good performance. thus proposed network capable capturing frame-level human features fusing discriminative representation. another point performances dnns seem apparently surpass existing state-of-the-art algorithms proving power dnns sufﬁcient training data available. less challenging ilids-vid prid- observe overall increments matching rate. model still outperforms methods prominently terms table rank- accuracy achieving %—transcending rnn-cnn method besides system efﬁcient robust since rank rate reaches level rank goes summit quickly level rank tendency accuracy demonstrates system effective space-time feature extractor able obtain discriminative sequence-level representation learning process. dnns still exhibit distinctive capability capturing human features whole accuracy converging earlier point. dataset introduced also claimed largest video re-id dataset date. mars consists different pedestrians captured least cameras. compared ilids-vid prid- mars times larger number identities times larger total tracklets. tracklets mars generated automatically detector gmmcp tracker whose error makes mars realistic course challenging previous dataset. identity tracklets average. instance identities captured cameras identities tracklets contain frames. perform experiments mars simpliﬁcation done steps. firstly pedestrians recorded least cameras randomly chose camera viewpoints person ensemble. probe gallery set. case reduced previous experiences ilids-vid prid-. performances models displayed table compared baseline rnn-cnn. astpn still achieves best accuracy general results dropping obviously contrast table compared ilidsvid prid- improvement larger reasons attributed considerable part image sequences mars accompanied figure variants model tested three datasets respectively. atpn refers attentive temporal pooling network refers attentive spatial pooling network. finally astpn stands combination atpn aspn. atpn overall performance atpn curve obviously better rnn-cnn method figure example atpn curve exceeds rnn-cnn method almost rank accuracy prid. meanwhile ilids-vid atpn curve also outperforms rnn-cnn method rank accuracy. safely conclude atpn efﬁciently utilize temporal human appearance form powerful sequencelevel representation subtle discriminative output simple pooling strategies aspn exhibits equally prominent capability matching compared atpn ilids-vid. although less robust atpn prid- distinct margin still exists aspn curve rnn-cnn method. reason aspn attentive spatial pooling network mainly leverages relevant contextual information enhance discriminative power ﬁnal representation. however mentioned datasets ilids-vid created rather complicated environment means contextual information could valuable clue ambiguity human appearance. contrary aspn thus doesn’t perform competitively atpn figure astpn combining atpn aspn together astpn capitalize frame-wise interactions effectively well selectively propagate additional contextual information network. based figure apparent distinction atpn curve astpn curve observed overall accuracy decreasing caused mars astpn exceeds atpn rank point. it’s proven astpn robust joint method especially dataset challenging mars. data bias inevitable since particular dataset represents small fragment data whole real world. machine-learning model trained dataset would perform much worse tested dataset. regarded over-ﬁtting particular scenario thus reducing generality model. cross-data testing designed evaluate model’s potentials practical application. rnn-cnn trained diverse ilids-vid dataset tested prid- dataset. apart distinction brought cross-dataset training contrast single-shot method multi-shot method also shown table although results much worse table astpn still achieves rank accuracy close srid trained prid- rank accuracy moreover using video-based re-id seems improve scores models using single-shot re-id terms rank score. concluded valuable temporal information provided video-based re-id really enhance generalization performance re-id networks greatly. proposed astpn novel deep architecture jointly attentive spatial-temporal pooling video-based person re-identiﬁcation enabling joint learning representations inputs well similarity measurement. astpn extends standard rnn-cnns decomposing pooling steps spatial-pooling feature attentive temporal-pooling output rnn. effect explicit implicitly attention performed pooling stage select regions frames sequences feature representation learning. extensive experiments ilids-vid prid- mars demonstrated astpn signiﬁcantly outperforms standard temporal pooling approaches. particular executing control experiments show joint pooling power either spatial/temporl pooling separately. additionally astpn simple implement introduces little computational overhead compared general pooling makes desirable design choice deep rnn-cnns used person re-identiﬁcation future. would also consider apply current method target tracking/detection systems", "year": 2017}