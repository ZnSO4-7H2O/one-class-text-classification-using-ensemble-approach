{"title": "Pushing Stochastic Gradient towards Second-Order Methods --  Backpropagation Learning with Transformations in Nonlinearities", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Recently, we proposed to transform the outputs of each hidden neuron in a multi-layer perceptron network to have zero output and zero slope on average, and use separate shortcut connections to model the linear dependencies instead. We continue the work by firstly introducing a third transformation to normalize the scale of the outputs of each hidden neuron, and secondly by analyzing the connections to second order optimization methods. We show that the transformations make a simple stochastic gradient behave closer to second-order optimization methods and thus speed up learning. This is shown both in theory and with experiments. The experiments on the third transformation show that while it further increases the speed of learning, it can also hurt performance by converging to a worse local optimum, where both the inputs and outputs of many hidden neurons are close to zero.", "text": "recently proposed transform outputs hidden neuron multilayer perceptron network zero output zero slope average separate shortcut connections model linear dependencies instead. continue work ﬁrstly introducing third transformation normalize scale outputs hidden neuron secondly analyzing connections second order optimization methods. show transformations make simple stochastic gradient behave closer second-order optimization methods thus speed learning. shown theory experiments. experiments third transformation show increases speed learning also hurt performance converging worse local optimum inputs outputs many hidden neurons close zero. learning deep neural networks become popular topic since invention unsupervised pretraining later works returned traditional back-propagation learning deep models noticed also provide impressive results given either sophisticated learning algorithm simply enough computational power work study back-propagation learning deep networks hidden layers continuing earlier results learning multi-layer perceptron networks back-propagation known transformations speed learning instance inputs recommended centered zero mean nonlinear functions proposed range rather schraudolph proposed centering factors gradient zero mean adding linear shortcut connections bypass nonlinear layer. gradient factor centering changes gradient nonlinear activation functions zero mean zero slope average. such change model itself. assumed discrepancy model gradient issue since errors easily compensated linear shortcut connections proceeding updates. gradient factor centering leads signiﬁcant speed-up learning. paper transform nonlinear activation functions hidden neurons average zero mean zero slope unit variance. earlier results included ﬁrst transformations introduce third one. cdsaasd explain usefulness transformations studying fisher information matrix hessian e.g. measuring angle traditional gradient second order update direction without transformations. well known second-order optimization methods natural gradient newton’s method decrease number required iterations compared basic gradient descent cannot easily used high-dimensional models heavy computations large matrices. practice possible diagonal block-diagonal approximation fisher information matrix hessian. gradient descent seen approximation second-order methods matrix approximated scalar constant times unit matrix. transformations making fisher information matrix close matrix possible thus diminishing difference ﬁrst second order methods. matlab code replicating experiments paper available formulas input vectors assumed supplemented additional component always one. supplement tanh nonlinearity auxiliary scalar variables nonlinearity updated gradient evaluation order help learning parameters deﬁne note making nonlinear activations zero mean disallow nonlinear mapping affect expected output compete bias term. similarly making nonlinear activations zero slope disallow nonlinear mapping affect expected dependency input compete linear mapping traditional neural networks linear dependencies modeled many competing paths input output whereas architecture gathers linear dependencies modeled argue less competition parts model speed learning. another explanation choosing transformations make nondiagonal parts fisher information matrix closer zero goal equation normalize output signals slopes output signals hidden unit time. motivated observing diagonal fisher information matrix contains elements signals slopes. normalizations pushing diagonal elements similar other. cannot normalize signals slopes unity time normalize geometric mean unity. diagonal matrix diagonal elements. schraudolph proposed centering factors gradient zero mean. argued deviations gradient fall linear subspace shortcut connections operate harm overall performance. transforming nonlinearities proposed paper similar effect gradient. equation corresponds schraudolph’s activity centering equation corresponds slope centering. second-order optimization methods natural gradient newton’s method decrease number required iterations compared basic gradient descent cannot easily used large models heavy computations large matrices. natural gradient basic gradient multiplied left inverse fisher information matrix. using basic gradient descent thus seen using natural gradient approximating fisher information unit matrix multiplied inverse learning rate. show ﬁrst proposed transformations move non-diagonal elements fisher information matrix closer zero third transformation makes diagonal elements similar scale thus making basic gradient behave closer natural gradient. expectation gaussian distribution noise equation vector contains elements matrices note random variable thus fisher information depend output data. hessian matrix closely related fisher information depend output data contains terms therefore show analysis simpler fisher information matrix. xit. arnotice equations contain factors making factors close zero possible help making nondiagonal elements fisher information closer zero. instance efj] e]e] assuming hidden units representing different things uncorrelated nondiagonal element fisher information equation becomes exactly zero using transformations. units completely uncorrelated element question approximately zero. argument applies elements equations also highlighting beneﬁt making input data zero-mean. naturally unrealistic assume inputs nonlinear activations slopes uncorrelated goodness approximation empirically evaluated next section. diagonal elements fisher found equations keep similar scale using third transformation equation investigate linear transformations affect gradient comparing second-order method namely newton’s algorithm simple regularization make hessian invertible. figure comparison distributions eigenvalues hessians angles compared second-order update directions using ltmlp regular mlp. eigenvalues distributed evenly using ltmlp. shows gradients transformed networks point directions closer second-order update. even negative eigenvalues cause inversion blow therefore hessian directly include regularization term similarly levenberg-marquardt algorithm resulting second-order update direction denotes unit matrix. basically equation combines steepest descent second-order update rule gets small update direction approaches newton’s method vice versa. computing hessian computationally demanding therefore limit size network used experiment. study mnist handwritten digit classiﬁcation problem dimensionality input data reduced using random rotation network hidden layers architecture –––. network trained using standard gradient descent weight decay regularization. details training given appendix. follows networks three transformations transformations network transformations compared. hessian matrix approximated according equation times regular intervals training networks. ﬁgures shown using approximation epochs training roughly corresponds midpoint learning. however results parallel reported ones along training. studied eigenvalues hessian matrix angles methods compared second-order update direction. distribution eigenvalues figure networks transformations even compared regular mlp. furthermore fewer negative eigenvalues shown plot transformed networks. figure angles gradient second-order update direction compared function equation plots ceases positive deﬁnite decreases. curiously update directions closer second-order method left suggesting necessarily useful respect. figure shows histograms diagonal elements hessian epochs training. distributions bimodal distributions closer unimodal transformations used furthermore variance diagonal elements log-scale smaller using ltmlp suggests transformations used second-order update rule equation corrects different elements gradient vector evenly compared regular backpropagation learning implying gradient vector closer second-order update direction using transformations. figure comparison distributions diagonal elements hessians. coloring according legend shows layers corresponding weights connect diagonal elements concentrated ltmlp spread regular network. notice logarithmic x-axis. conclude section clear evidence another whether addition beneﬁts back-propagation learning however differences approaches. case seems clear transforming nonlinearities beneﬁts learning compared standard back-propagation learning. proposed transformations training networks mnist classiﬁcation task. experiments conducted without pretraining weight-sharing enhancements training known tricks boost performance. weight decay used regularization gaussian noise training data. networks three hidden layers architechtures –––– used. details given appendix. figure shows results number errors classifying test samples. results regular back-propagation without transformations shown blue well line previously published result task. networks architecture trained using proposed transformations results improved signiﬁcantly. however adding addition previously proposed seem affect results data set. best results errors obtained smaller architecture without three-layer architecture result errors. learning seems converge faster especially three-layer case results line obtained networks regularized thoroughly. results show possible obtain results comparable dropout networks using minimal regularization. previously studied auto-encoder network using transformations auto-encoder architecture ––––––. adding third transformation training auto-encoder poses problems. many hidden neurons decoding layers tend relatively inactive beginning training induces corresponding obtain large values. experiments auto-encoder eventually diverge despite simple constraint experimented with behavior illustrated figure subﬁgure shows distribution variances outputs hidden neurons mnist classiﬁcation network used section given mnist training data. corresponding distribution hidden neurons decoder part auto-encoder shown subﬁgure dead neurons seen peak origin. corresponding constrained seen subﬁgure hypothesize behavior fact beginning learning much information reaching bottleneck layer encoder part thus nothing learn decoding neurons. according tentative experiments problem described overcome disabling figure error rate mnist test ltmlp training ltmlp without regular back-propagation. solid lines show results networks hidden layers neurons dashed lines networks three hidden layers neurons. figure histograms variation output hidden neurons given mnist training data decoder part mnist auto-encoder. shows healthy distributions variances whereas includes variances decoder part many dead neurons. neurons induce corresponding histogram shown blow eventually lead divergence. decoder network however seem speed learning compared earlier results transformations also possible experiment weight-sharing constraints overcome difﬁculties shown introducing linear transformation nonlinearities signiﬁcantly improves back-propagation learning networks. addition transformation proposed earlier propose adding third transformation order push fisher information matrix closer unit matrix hypothesis proposed transformations actually mimic second-order update rule conﬁrmed experiments comparing networks transformations regular network second-order update method. however order whether third transformation proposed paper really useful experiments ought conducted. might useful design experiments convergence usually slow thus revealing possible differences methods. hyperparameter selection regularization usually nuisance practical neural networks would interesting whether combining dropouts transformations provide robust framework enabling training robust neural networks reasonable time. effect ﬁrst transformations similar gradient factor centering transforming model instead gradient makes easier generalize contexts learning mcmc variational bayes genetic algorithms would compute basic gradient all. instance consider using metropolis algorithm weight matrices expecially matrices without transformations proposed jumps would affect expected output expected linear dependency ∂yt/∂xt eqs. thus often leading acceptance probability poor mixing. proposed transformations included longer proposed jumps could accepted thus mixing nonlinear part mapping faster. discussion section implications proposed transformations contexts left future work.", "year": 2013}