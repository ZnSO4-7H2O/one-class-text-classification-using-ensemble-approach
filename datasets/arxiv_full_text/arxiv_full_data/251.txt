{"title": "A Classification-Based Perspective on GAN Distributions", "tag": ["cs.LG", "cs.CV", "cs.NE", "stat.ML"], "abstract": "A fundamental, and still largely unanswered, question in the context of Generative Adversarial Networks (GANs) is whether GANs are actually able to capture the key characteristics of the datasets they are trained on. The current approaches to examining this issue require significant human supervision, such as visual inspection of sampled images, and often offer only fairly limited scalability. In this paper, we propose new techniques that employ a classification-based perspective to evaluate synthetic GAN distributions and their capability to accurately reflect the essential properties of the training data. These techniques require only minimal human supervision and can easily be scaled and adapted to evaluate a variety of state-of-the-art GANs on large, popular datasets. Our analysis indicates that GANs have significant problems in reproducing the more distributional properties of the training dataset. In particular, when seen through the lens of classification, the diversity of GAN data is orders of magnitude less than that of the original data.", "text": "fundamental still largely unanswered question context generative adversarial networks whether gans actually able capture characteristics datasets trained current approaches examining issue require signiﬁcant human supervision visual inspection sampled images often offer fairly limited scalability. paper propose techniques employ classiﬁcation–based perspective evaluate synthetic distributions capability accurately reﬂect essential properties training data. techniques require minimal human supervision easily scaled adapted evaluate variety state-of-the-art gans large popular datasets. analysis indicates gans signiﬁcant problems reproducing distributional properties training dataset. particular seen lens classiﬁcation diversity data orders magnitude less original data. generative adversarial networks garnered signiﬁcant amount attention ability learn generative models multiple natural image datasets since conception fundamental question regarding gans extent truly learn underlying data distribution. issue multiple reasons. scientiﬁc perspective understanding capabilities common gans shed light precisely adversarial training setup allows learn. engineering standpoint important grasp power limitations framework applying concrete applications. broad potential applicability gans researchers investigated question variety ways. evaluate quality obvious ﬁrst check establish generated samples support true distribution. case images corresponds checking generated samples look realistic. indeed visual inspection generated images currently common assessing quality given gan. individual humans performs task quickly reliably various gans achieved impressive results generating realistic-looking images faces indoor scenes established gans produce realistic-looking images next concern might simply memorizing training dataset. hypothesis cannot ruled entirely evidence gans perform least non-trivial modeling unknown distribution. previous studies show interpolations latent space generator produce novel meaningful image variations clear disparity generated samples nearest neighbors true dataset taken together results provide evidence gans could constitute successful distribution learning algorithms motivates studying distributions detail. direct approach compare probability density assigned generator estimates true distribution however context gans high-dimensional image distributions complicated factors. first gans naturally provide probability estimates samples. second estimating probability density true distribution challenging problem since reliably computing probability densities high dimensions challenging instead study behavior gans low-dimensional problems two-dimensional gaussian mixtures. here common failure gans mode collapse wherein generator assigns disproportionately large mass subset modes true distribution raises concerns lack diversity synthetic distributions recent work shows learned distributions common gans indeed support size celeba dataset however approach arora zhang heavily relies human annotator order identify duplicates. hence easily scale comparing many variants gans asking ﬁne-grained questions collision statistics. overall understanding synthetic distributions remains blurry largely lack versatile tools quantitative evaluation gans realistic settings. focus work precisly address question propose evaluation techniques distributions. methods inspired idea comparing moments distributions heart many methods classical statistics. although simple moments high-dimensional distributions often semantically meaningful extend idea distributions realistic images leveraging image statistics identiﬁed using convolutional neural networks. particular train image classiﬁers order construct test functions corresponding semantically meaningful properties distributions. important feature approach requires light human supervision easily scaled evaluating many gans large synthetic datasets. using evaluation techniques study state-of-the-art gans celeba lsun datasets arguably common testbeds advanced gans. gans signiﬁcantly distort relative frequency even basic image attributes hair style person type room indoor scene. clearly indicates mismatch true synthetic distributions. moreover conduct experiments explore diversity distributions. synthetic data train image classiﬁers signiﬁcantly lower accuracy classiﬁers trained true data set. points towards lack diversity data towards discrepancy true synthetic distributions. fact additional examinations show diversity gans comparable subset true data smaller. comparing distributions common ﬁrst test compute low-order moments mean variance. distributions simple enough quantities provide good understanding similar are. moreover low-order moments precise deﬁnition usually quick compute. hand low-order moments also misleading complicated high-dimensional distributions. concrete example consider generative model digits generator produces digits shifted signiﬁcant amount otherwise perfect probably still consider good approximation true distribution. however expectation generator distribution different expectation true data distribution. raises question properties high-dimensional image distributions easy test semantically meaningful. next subsections describe concrete approaches evaluate synthetic data easy compute capture relevant information distribution. common theme employ convolutional neural networks order capture properties distributions hard describe mathematically precise usually well-deﬁned human automating process annotating images high-level information allow study various aspects synthetic data. gans elegant visualizations phenomena somewhat restricted problems low-dimensional distributions image datasets common rely human annotators therein derived heuristics methods merits restrictive scale granularity testing. propose classiﬁcation-based tool assess good gans assigning right mass across broad concepts/modes. trained classiﬁer expert annotator labels important features synthetic data analyze resulting distribution. speciﬁcally goal investigate trained well-balanced dataset learn reproduce balanced structure. represent dataset size classes denote image-label pair drawn true data. dataset balanced contains images class. procedure computing class distribution synthetic data train annotator using dataset train unconditional images dataset without using class labels. create synthetic dataset sampling images labeling using annotated data generated procedure provide insight gan’s class distribution scale entire dataset. moreover vary granularity mode analysis choosing richer classiﬁcation tasks i.e. challenging classes larger number them. section technique visualize mode collapse several state-of-the-art gans celeba lsun datasets. studied gans show signiﬁcant mode collapse effect becomes pronounced granularity annotator increased also investigate temporal aspect setup dominant mode varies widely course training. approach also enables benchmark compare gans different datasets based extent mode collapse learned distributions. method inspecting distribution modes synthetic data provides coarse look statistics underlying distribution. resulting quantities semantically meaningful capture simple notions diversity. holistic view sample diversity synthetic distribution describe second classiﬁcation-based approach evaluating distributions. main question motivates gans recover aspects real data enable training good classiﬁer? believe interesting measure sample diversity reasons. first classiﬁcation high-dimensional image data challenging problem good training dataset require sufﬁciently diverse sample distribution. second augmenting data classiﬁcation problems proposed cases gans gans truly able capture quality diversity underlying data distribution expect almost classiﬁers trained true data synthetic data gan. generic method produce data gans classiﬁcation train separate gans class dataset samples class-wise gans pooled together labeled synthetic dataset. note labels trivially determined based class modeled particular sample drawn. perform following steps assess classiﬁcation performance synthetic data true data train classiﬁer true data benchmark comparison. train separate unconditional gans class dataset generate balanced synthetic labeled dataset size consolidating equal number samples drawn gans. labels obtained aggregating samples per-class gans designated default\" labels synthetic dataset. note design true synthetic datasets samples examples class. tried alternate approach using class-conditional gans labeled datasets. method yield good samples since common gans designed conditional structure place. using conditional gan. found samples dcgan performed comparably true data nearest neighbor classiﬁcation. obtained similar good results mnist could efﬁcacy gans learning mnist distribution ease getting good accuracy mnist even small training clarify question restrict analysis complex datasets speciﬁcally celeba lsun. observe studied gans diversity metric. particular accuracy achieved classiﬁer trained data comparable accuracy classiﬁer trained subsampled version true dataset. even draw samples gans produce training several times larger true dataset improvement performance. looking classiﬁcation accuracy gives compare different models potential downstream application gans. interestingly visual quality samples necessarily correlate good classiﬁcation performance. literature common investigate performance using metrics involve human supervision. arora zhang proposed measure based manually counting duplicates samples heuristic support diversity learned distribution. manual classiﬁcation small sample generated mnist images used test missing certain modes. annotator-based metrics clear advantages identifying relevant failure-modes synthetic samples explains visual inspection still popular approach assess samples. also various attempts build good metrics gans based manual heuristics. parzen window estimation used approximate log-likelihood distribution though known work poorly high-dimensional data develop method better estimate log-likelihood using annealed importance sampling. salimans propose metric known inception score entropy labels predicted pre-trained inception network used assess diversity samples. gans shown promise generating realistic samples resulting efforts apply broad spectrum datasets. however large-scale celebfaces attributes large-scale scene understanding datasets remain popular canonical ones developing evaluating variants. conveniently datasets also rich annotations making particularly suited classiﬁcation–based evaluations. details setup classiﬁcation tasks datasets given appendix. figure illustration dataset used proposed classiﬁcation benchmarks. shown alongside images sampled various unconditional gans trained dataset. labels samples obtained using pre-trained classiﬁer annotator. aforementioned gans unconditional however imgan access class labels part semi-supervised training process. standard implementations models details provided appendix also used prescribed hyper-parameter settings including number iterations train for. analysis based samples size generated samples tend high quality. also visual inspection ascertain perceptual quality samples experiments comparable reported previous studies. demonstrate sample images figures began converge experiments lsun dataset hence excluded corresponding analysis. linear model network one-fully connected layer input output softmax non-linearity. dimensions input output respectively linear models implement function matrix vector softmax function. it’s simplicity model serve useful baseline experiments. experimental results quantifying mode collapse classiﬁcation tasks described section presented below. table gives details datasets used analysis size number classes accuracy annotator i.e. classiﬁer pre-trained true data used label synthetic gan-generated data. figure presents class distribution synthetic data determined using annotators. left panel compares relative distribution modes true data various gan-generated datasets. datasets created drawing samples trained corresponding true dataset. right panel illustrates evolution class distributions various gans course training. results visualization lead following ﬁndings dataset celeba makeup smiling celeba male mouth open celeba bangs smiling lsun bedroom kitchen classroom lsun bedroom conference room dining room kitchen living room table details celeba lsun subsets used studies mode collapse section here classiﬁer trained true data annotator let’s infer label distribution synthetic gan-generated data. training size number classes true synthetic datasets. annotator’s accuracy refers accuracy classiﬁer test true data. celeba combination attribute-wise binary classiﬁers annotators higher accuracy compared single classiﬁer trained jointly four classes. figure visualizations mode collapse synthetic gan-generated data produced training chosen subsets celeba lsun datasets. left panel shows relative distribution classes samples drawn synthetic datasets extracted training process compares true data distribution right shown evolution analogous class distribution different gans course training. began converge lsun tasks hence excluded corresponding analysis. gans seem suffer signiﬁcant mode-collapse. becomes apparent annotator granularity increased considering larger classes. instance compare relatively balanced class distributions -class lsun task near-absence modes -class task. mode collapse prevalent gans throughout training process seem recede time. instead dominant mode often ﬂuctuate wildly course training. task often common modes onto distinct gans exhibit collapse. addition viewing method approach analyze mode collapse also benchmark comparison. perspective observe consistently shows lowest mode collapse amongst studied gans. behavior gans appears vary based dataset celeba dcgan learns somewhat balanced distributions wgan began imgan show prominent mode collapse. contrast results obtained lsun where example wgan exhibit relatively small mode collapse dcgan imgan show signiﬁcant mode collapse. points general challenge real world applications gans often perform well datasets designed extension datasets straightforward. temporal analysis mode-collapse shows wide variation dominant mode wgan improved whereas began mode often dominates entire training process. using procedure outlined section perform quantitative assessment sample diversity gans celeba lsun datasets. restrict experiments binary classiﬁcation sufﬁcient complexity highlight disparity true synthetic data. results classiﬁcation-based evaluation gans presented table figure preliminary check inspect quality labeled datasets. this high-accuracy annotators section predict labels generated data measure consistency predicted default labels also inspect conﬁdence scores deﬁned softmax probabilities predicted class annotator. motivation behind metrics classiﬁer correctly high-conﬁdence predict labels labeled samples likely convincing examples class hence good quality\". empirical results label agreement annotator conﬁdence generated datasets shown table figure appendix. table also report equivalent inception score similar described section using inception network label distribution meaningful face scene images. instead compute inception score using label distribution predicted annotator networks. score computed exp)||p]) refers label predictions annotators next train classiﬁers using true labeled gan-generated datasets study performance terms accuracy hold-out true data. resnets yield good classiﬁcation performance true data suffer severe overﬁtting synthetic data leading poor test accuracy already indicates possible problem gans diversity data generate. highlight problem better avoid issues stem overﬁtting also look classiﬁer always overﬁt synthetic data. however observed even training simple networks fully connected layer hidden units overﬁtting synthetic data. hence resorted basic linear model described section table shows results binary classiﬁcation experiments using linear models training test accuracies classiﬁer various datasets. finally better understanding underlying \"diversity\" synthetic datasets train linear models using down-sampled versions true data compare performance synthetic data shown table down-sampling data factor denoted implies selecting random subset data visualizations classiﬁcation performance compares true data figure here bold curve shows test accuracy classiﬁer trained true data function true dataset table results comparative study classiﬁcation performance true data generated synthetic data celeba lsun datasets. label correctness measures agreement default labels synthetic datasets predicted annotator classiﬁer trained true data. shown alongside equivalent inception scores computed using labels predicted annotator training test accuracies linear model classiﬁer various true synthetic datasets reported. also presented corresponding accuracies linear model trained down-sampled true data oversampled synthetic data test accuracy resnets trained datasets also shown though noticeable deep networks suffer issues trained synthetic datasets. figure illustration classiﬁcation performance true data gan-generated synthetic data. classiﬁcation performed using basic linear model performance reported terms accuracy test true data. bold curve shows variation classiﬁcation performance models trained true data size training here training sets various sizes obtained down-sampling true dataset random. dashed lines show performance classiﬁers trained various gan-generated datasets size natural argument defense gans oversample them i.e. generate datasets much larger size training data. results linear models trained using -fold oversampling gans denoted show table results major ﬁndings experiments based table figure strong agreement annotator labels true labels synthetic data scores test true data. thus apparent images high-quality expected based visual inspection. scores lower lsun celeba potentially lower quality generated lsun images. results broad understanding good gans producing convincing/representative samples different classes across datasets. also shows simple classiﬁcation-based benchmarks highlight relevant properties synthetic datasets. equivalent inception score informative similar true synthetic datasets. surprising given simple nature binary classiﬁcation task fact true synthetic datasets almost uniform distribution labels. evident table large performance true synthetic data classiﬁcation tasks. inspection training accuracies shows linear models able nearly synthetic datasets grossly underﬁtting true data. given high scores synthetic data previous experiments assess dataset ‘quality’ likely poor classiﬁcation performance indicative lack ‘diversity’. comparing performance down-sampled true data reveals learned distribution trained datasets around hundred thousand data points exhibits diversity mere couple hundreds true data samples constitute shows that least point view classiﬁcation diversity generated data severely lacking. oversampling gans -fold produce larger datasets improve classiﬁcation performance. disparity true synthetic data remains nearly unchanged even signiﬁcant oversampling highlighting lack diversity gans. terms conclusions relative performance various gans observe wgan perform consistently better gans celeba lsun datasets. began samples good perceptual quality performs badly classiﬁcation tasks. hand wgan samples relatively poor visual quality seem outperform gans classiﬁcation tasks. strong indicator need consider metrics ones proposed paper addition visual inspection study gans. lsun true synthetic data much larger classiﬁers getting near random performance synthetic datasets. note classiﬁers poor test accuracy lsun overﬁtting training data. case speculate lower performance could lower quality diversity lsun samples. summary experimental ﬁnding even simple classiﬁcation–based tests hold tremendous potential shed insight learned distribution gans. helps deeper understanding many underlying issues also provides quantitative rigorous platform compare different gans. techniques could principle also applied assess generative models variational auto-encoders kingma welling however vaes signiﬁcant problems generating realistic samples datasets used analysis ﬁrst place arora zhang paper forth techniques examining ability gans capture characteristics training data lens classiﬁcation. tools scalable quantitative automatic thus capable studying state-ofthe-art gans realistic large-scale image datasets. further serve mean perform nuanced comparison gans identify relative merits including properties cannot discerned mere visual inspection. used developed techniques perform empirical studies popular gans celeba lsun datasets. examination showed mode collapse indeed prevalent issue gans. also observed synthetic gan-generated datasets signiﬁcantly reduced diversity least examined classiﬁcation perspective. fact diversity synthetic data often orders magnitude smaller true data. furthermore diversity seem bridged simply producing much larger datasets oversampling gans. also methods viewed benchmarks gans. perspective observed consistently outperforms studied gans terms extent modecollapse well diversity learned distribution assessed classiﬁcation point-of-view. finally noticed good perceptual quality samples necessarily correlate might sometimes even anti-correlate distribution diversity. ﬁndings suggest need beyond visual inspection–based evaluations look quantitative tools assessing quality gans ones presented paper. emily denton soumith chintala fergus deep generative image models using laplacian pyramid adversarial networks. advances neural information processing systems vincent dumoulin ishmael belghazi poole alex lamb martin arjovsky olivier mastropietro aaron courville. adversarially learned inference. international conference learning representations goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems salimans goodfellow wojciech zaremba vicki cheung alec radford chen. improved techniques training gans. advances neural information processing systems fisher yinda zhang shuran song seff jianxiong xiao. lsun construction large-scale image dataset using deep learning humans loop. arxiv preprint arxiv. zhang hongsheng shaoting zhang xiaolei huang xiaogang wang dimitris metaxas. stackgan text photo-realistic image synthesis stacked generative adversarial networks. arxiv preprint arxiv. assess performance perspective classiﬁcation construct classiﬁcation tasks celeba lsun datasets. case lsun dataset images annotated scene category labels makes straightforward data binary multiclass classiﬁcation. hand image celeba dataset labeled binary attributes. result single image multiple associated attribute labels. here construct classiﬁcation tasks considering binary combinations attribute attributes used experiments chosen resulting dataset large classiﬁers trained true data high-accuracy good annotators synthetic data. figure illustration datasets celeba used proposed classiﬁcation-based benchmarks evaluate gans. shown alongside images sampled various unconditional gans trained dataset. labels samples obtained using pre-trained classiﬁer annotator. dcgan https//github.com/carpedm/dcgan-tensorflow wgan https//github.com/martinarjovsky/wassersteingan https//github.com/ishmaelbelghazi/ali began https//github.com/carpedm/began-tensorflow improved https//github.com/openai/improved-gan figure illustration datasets lsun used proposed classiﬁcation-based benchmarks evaluate gans. shown alongside images sampled various unconditional gans trained dataset. labels samples obtained using pre-trained classiﬁer annotator. benchmark experiments ascertain visual quality samples produced gans comparable reported prior work. examples random samples drawn multi-class datasets true synthetic data shown figures celeba dataset figure lsun dataset. sections describe techniques study classiﬁcation performance labeled gangenerated datasets true data. order assess quality gan-generated synthetic datasets evaluate label agreement default labels synthetic datasets obtained using annotators described section shown table figure inspect conﬁdence scores deﬁned softmax probabilities predicted class annotator making predictions. seen results annotator conﬁdence synthetic data comparable test true data. thus seems likely generated samples good quality truly representative examples respective classes expected based visual inspection.", "year": 2017}