{"title": "Batch Reinforcement Learning on the Industrial Benchmark: First  Experiences", "tag": ["cs.LG", "cs.AI", "cs.NE", "cs.SY"], "abstract": "The Particle Swarm Optimization Policy (PSO-P) has been recently introduced and proven to produce remarkable results on interacting with academic reinforcement learning benchmarks in an off-policy, batch-based setting. To further investigate the properties and feasibility on real-world applications, this paper investigates PSO-P on the so-called Industrial Benchmark (IB), a novel reinforcement learning (RL) benchmark that aims at being realistic by including a variety of aspects found in industrial applications, like continuous state and action spaces, a high dimensional, partially observable state space, delayed effects, and complex stochasticity. The experimental results of PSO-P on IB are compared to results of closed-form control policies derived from the model-based Recurrent Control Neural Network (RCNN) and the model-free Neural Fitted Q-Iteration (NFQ). Experiments show that PSO-P is not only of interest for academic benchmarks, but also for real-world industrial applications, since it also yielded the best performing policy in our IB setting. Compared to other well established RL techniques, PSO-P produced outstanding results in performance and robustness, requiring only a relatively low amount of effort in finding adequate parameters or making complex design decisions.", "text": "ieee. personal material permitted. permission ieee must obtained uses current future media including reprinting/republishing material advertising promotional purposes creating collective works resale redistribution servers lists reuse copyrighted component work works. abstract—the particle swarm optimization policy recently introduced proven produce remarkable results interacting academic reinforcement learning benchmarks off-policy batch-based setting. investigate properties feasibility real-world applications paper investigates pso-p so-called industrial benchmark novel reinforcement learning benchmark aims realistic including variety aspects found industrial applications like continuous state action spaces high dimensional partially observable state space delayed effects complex stochasticity. experimental results pso-p compared results closed-form control policies derived modelbased recurrent control neural network model-free neural fitted q-iteration experiments show pso-p interest academic benchmarks also real-world industrial applications since also yielded best performing policy setting. compared well established techniques psop produced outstanding results performance robustness requiring relatively amount effort ﬁnding adequate parameters making complex design decisions. process controlling industrial plants parts such involves variety challenging aspects reinforcement learning algorithms need tackle. coping adequately complexity real-world systems important challenges need considered continuous state action spaces high-dimensional partially observable state spaces stochasticity induced heteroscedastic sensor noise latent variables delayed effects multi-criterial reward components non-stationarity optimal steerings i.e. optimal policy approach ﬁxed operation point. plant’s dynamics. contrast batch algorithms generate policy based existing data deployed plant training. setting either value function system dynamics trained historic operational plant data four-tuples called batch following. research past decades suggests family batch algorithms meet requirements real-world systems especially involving neural networks modeling either state/action value function system dynamics moreover batch algorithms data efﬁcient batch data utilized repeatedly training phase. following investigate different approaches industrial benchmark comes challenges vital industrial settings described above. report results applying particle swarm optimization policy powerful algorithm continuous actions achieves remarkable results box. actions perform derived rollouts system model simulates ib’s transition dynamics. model recurrent neural network trained batch transitions sampled applying random actions real-world applications however transitions usually available form historic operational data might produced constant default controller. compare performance pso-p approaches utilize batch different ways. first recurrent control neural network model-based algorithm continuous actions uses system model training controller. second neural fitted q-iteration model-free algorithm discrete actions controller learned iteratively applying watkins’ q-learning algorithm batch data. here system model used select best policy training ﬁnished. policy selection necessary performance policies decrease signiﬁcantly iterations. well-known phenomenon context neuro-dynamic programming assume reward function also known consumption fatigue observable. however except values steerings remaining part markov state remains unobservable. yields observation vector consisting ib’s features possibility freeze stochasticity. hand data generation online experiments policy evaluation stochasticity makes benchmark realistic challenging. hand settings freezing randomness stochastic effects might useful. realized remembering applied seed ib-internal pseudo-random number generator instance experiments presented section searched true maximum reward given certain markov state current seed upper bound technique performance evaluation. done applying pso-p directly system dynamics provided full knowledge future encoded seed. particle swarm optimization policy framework solving problem reformulated optimization problem. area machine learning markov decision problem solved learning observed state transitions representing markovian states applied action real-valued reward discrete time steps goal policy maximizing expected cumulative reward γkrk called return so-called discount factor since true underlying markovian state observable approximated sufﬁcient amount historic observations i.e. information contained approximated horizon given system model trained supervised learning methods previous observations ﬁnding best action given observation horizon respect return described industrial benchmark aims realistic sense includes variety aspects found vital industrial applications. designed approximation speciﬁc real-world system pose comparable hardness complexity found many industrial applications. state action space continuous state space high-dimensional partially observable. actions consist three continuous components affect three steerings. moreover includes stochastic delayed effects. optimization task multi-criterial sense reward components show opposite dependencies actions. dynamical behavior heteroscedastic state-dependent observation noise statedependent probability distributions based latent variables. furthermore depends external driver cannot inﬂuenced actions. designed optimal policy approach ﬁxed operation point three steerings. speciﬁc choice driven experience industrial challenges. time step agent inﬂuence environment actions three dimensional vectors action interpreted three proposed changes three observable state variables called current steerings. current steerings velocity gain shift limited yielding applying action environment transitions next time step yielding internal state st+. state successor state markovian states environment. many industrial applications operatordeﬁned load applied system. depending load control values system shows fatigue consumes resources power fuel etc. represented consumption both external drivers response generates values part internal state st+. reward solely determined fig. error comparison system model trained different types neural approximation models reward relevant variables consumption fatigue ﬁrst type called self input’ setting models predict respective variable without getting variable input networks i.e. ot\\{ct full observation. experiences real industrial applications shown circumstances negative feedback effects occur corrupt long term predictions rollout settings. contrast learning tasks prediction quality could dramatically increased model received history variable forecast called ’self input’ setting i.e. ot\\{ft} ot\\{ct}. experiment compared types rnns rolling randomly drawn trajectories included calculating average absolute errors step respect true variables’ future values given observations note that network’s prediction accuracy consumption dramatically collapses time steps self input applied prediction error remains almost level self input given contrast that prediction fatigue beneﬁts seeing historic fatigue values least evaluated period result decided forecast consumption without self input fatigue self input. recurrent control neural network consists parts. system model trained predict return rollout length second part policy network computes step rollout action system model. policy network takes input internal state system model markov property approximately policy network trained data system model uses neurons internal states consumption fatigue networks input followed hidden layers neurons respectively three output neurons encode changes three steering variables velocity gain shift. hidden layers hyperbolic tangent activation function output layer uses sine function. analogues receding horizon control ﬁrst element returned. yields policy conducts optimization every observation. might computationally expensive rely predeﬁned closed-form policy structure often hard asses priori assumption common algorithms novel problems. rcnn rcnn trained calculate respective gradients policy’s weight update step. despite considered model-free still useful evaluate policy’s performance q-iteration step since performance drops training likely occur applying off-policy batch data. therefore experiments policy best performance according model saved returned ﬁnal training result. system model chosen consist recurrent neural networks predict consumption fatigue respectively. models unfolded sufﬁcient number steps past time steps future. time step take observable variables past present input. whereas future branch rnns steerings used input. topology rnns described markov decision process extraction network. models trained rprop learning algorithm data training validation data early stopping. training process repeated times networks lowest validation error chosen. experiments could validate training process rnns stable results depend little chosen learning algorithm. weights networks trained using non-batch stochastic gradient descent manually tuned constant learning rate setting produced better results using rprop suggested riedmiller weights trained rprop tended unstable learning dataset. starting training training samples permuted. furthermore divided training data validation data early-stopping. input data presented neural network using z-score transformation. qvalues output layer scaled interval activation function proposed gabel overall training evaluation procedure follows. creating random initial weights interval performed iterations. training data presented neural network maximum training epochs case error validation start raise within epochs. experiments observed performance policies successfully learned degrade time. consistent ﬁndings therefore utilized policy selection process experiment evaluated q-function iteration system model saved policy best performance according subsequently policy retained performance evaluated initial states pso-p setup applied search particles searching iterations best trajectory found returned. planning time horizon yielded discount factor. particles arranged called star topology i.e. particle connected every single particle swarm calculation particles’ ﬁtnesses could computed parallel cpus resulting overall computation time less seconds compute today computation time might still long several real-world industrial applications future increase speed and/or parallelization well computation clusters might make pso-p computational tractable applications. fig. condenses results policies highlights superior performance pso-p. technique produces signiﬁcantly robust results rcnn. note techniques trained/evaluated system model fig. rcnn self assessment. average reward estimated simulation model next time steps used select good policies. plot shows relation average reward estimated simulation model true average reward measured several policies trained rcnn. policies generated independent training runs include fully trained policies also intermediate ones trained epochs respectively. conﬁguration parameters chosen almost tuning. tuning necessary conﬁgure training process policy network though. neither rprop vario-eta momentum-backprop stable training behaviors observed. best results observed online-backprop small mini-batches small learning rates used random learning rates uniformly chosen logarithm batch size one. note concerning possibility assess quality trained policy without executing real system validation error system model lower average rollout sufﬁcient length better system model selected system model estimates higher return rollout sufﬁcient length given policy policy likely perform better executed policies neural fitted q-iteration trained using -layered feed-forward neurons input layer observation action neurons hidden layer. output layer comprises neuron associated q-value neurons neural network utilize logistic activation function. since algorithm discrete actions discretized three delta steerings towards setting either steering total yields different actions. fig. comparison trajectories different iterations. experiments noticed technique’s training process highly unstable. ﬁrst iterations resulting policies constantly improve performance process starts decreasing iterations. later iterations performance completely drops percentage completely incapable policies increases. given example ﬁgure depicts policies different iterations experiment. ﬁrst column steerings resulting rewards plotted policy selected performance predicted system model second column steerings rewards given iteration experiment. obvious second policy completely incapable steering implies high robustness different initial conditions like particle positions velocities. fig. gives detailed explanation properties best performing pso-p result. paper compared approach psop standard techniques rcnn recently introduced industrial benchmark. benchmark designed imitate realistic behavior aspects found real-world industrial applications. experiments show important steps off-policy batch based method stack necessary applying industrial facilities. starting limited exploration data system model trained tested. model crucial applying random policies real system usually prohibited real-world applications. despite classiﬁed model-free technique experiments show still requires precise system model policy selection. model used either training closed-form neural network policy policy selection exploiting model ﬁnding optimal actions inherent limitation discrete actions tendency instability training process produced worst performing policies. although higher performance could achieved example increasing discrete fig. comparison rcnn pso-p experiment results. boxes cover data around median whiskers show maximum/minimum data outliers depicted ’*’. average points pso-p yielded best performing policies also produced stable results compared rcnn. results lowest performance experiments. partially explained fact applied discrete actions limitation given designed work continuous action spaces. nevertheless second nfq-inherent problem revealed highly unstable training behavior off-policy batch-based trainings settings. training process gives answer question stop training. might think might good plan perform training long computationally feasible result last iteration. experiments procedure would created policies signiﬁcantly better results achieved evaluating resulting policy iteration system model policy produced highest approximated average reward declared best policy experiment. fig. gives detailed explanation properties best performing policy. rcnn experiments produced better results compared nfq. rcnn policies operate continuous action space yield compact closed-form policies. even though trained policies yielded similar training errors real performance evaluated differs quite lot. property implies rcnn rather sensitive towards prediction errors system model fig. gives detailed explanation properties best performing rcnn policy. experiments pso-p demonstrated high reward performance outstanding robustness observed point values pso-p yielded best policy average. moreover performances experiments close other fig. best performing trajectory. comparison techniques operates region gain steering similarly pso-p throttles velocity shift steering could follow optimal strategy periodic trajectory amplitude cycle duration maintains shift around indeed local optimum requires cyclic behavior. rcnn ability apply continuous actions inherent policy performance measure computed closed-form policies good performance. possible improvements example different network topologies bigger neural networks advanced neural learning algorithms. fig. best performing rcnn trajectory. closed-form policy rcnn tends generate smoother regular trajectories. despite implying strict periodic change gain depicted rcnn policy performs regular gain trajectory. rcnn reduces velocity signiﬁcantly pso-p did. again trajectory looks regular. since velocity change exact cyclic duration shift steering might cross talk like effect caused shared weights policy outputs. despite rcnn discovered beneﬁcial apply cyclic trajectory shift capable reveal true underlying cyclic duration would produced highest reward. every point. biggest disadvantage technique lies computational effort required determination next action. experiments next action computed less seconds still long many industrial applications. improvements increasing computational power speed might lower value project report based supported funds german federal ministry education research project number sole responsibility report’s contents lies authors. tables contain average step rewards experiments. maximum achievable average reward given brackets ﬁrst column. values computed applying pso-p real system dynamics preservation initial seed pseudo-random number generator i.e. optimizer searched best actions given ﬁxed inﬁnitely replicable future. result accurate estimate maximum achievable average reward initial markov state evaluated. riedmiller neural ﬁtted iteration ﬁrst experiences data efﬁcient neural reinforcement learning method proceedings european conference machine learning vol. springer schneegass udluft martinetz neural rewards regression near-optimal policy identiﬁcation markovian partial observable environments proceedings european symposium artiﬁcial neural networks improving optimality neural rewards regression dataefﬁcient batch near-optimal policy identiﬁcation proceedings international conference artiﬁcial neural networks sch¨afer udluft h.-g. zimmermann recurrent control neural network data efﬁcient reinforcement learning proceedings ieee international symposium approximate dynamic programming reinforcement learning. ieee hein hentschel sterzing tokic udluft. introduction industrial benchmark. available https//arxiv.org/abs/. fig. best performing pso-p trajectory. since pso-p bound speciﬁc policy representation capability freely change steerings wherever model predicts best reward. leads potentially less smooth less regular trajectories. similarly rcnn pso-p maintains gain around velocity levels around optimal point pso-p technique capable following optimal shift strategy given cyclic duration amplitude initially randomly distributed particles trajectories prone appear slightly irregular. effect tackled applying particles well iterations computationally feasible. summary ﬁrst experiences made indeed contains many realistic objectives issues features. experiments shown benchmark could solved three completely different techniques realistic off-policy batch-based setting. hein hentschel runkler udluft reinforcement learning particle swarm optimization policy continuous state action spaces. international journal swarm intelligence research vol. duell udluft sterzing solving partially observable reinforcement learning problems recurrent neural networks neural networks tricks trade ser. lecture notes computer science montavon k.-r. m¨uller eds. springer berlin heidelberg vol. neuneier h.-g. zimmermann train neural networks neural networks tricks trade ser. lecture notes computer science montavon k.-r. m¨uller eds. springer berlin heidelberg vol. hans duell udluft agent self-assessment determining policy quality without execution proceedings ieee symposium adaptive dynamic programming reinforcement learning gabel lutz riedmiller improved neural ﬁtted iteration applied novel computer gaming learning benchmark proceedings ieee symposium approximate dynamic programming reinforcement learning. paris france ieee press april", "year": 2017}