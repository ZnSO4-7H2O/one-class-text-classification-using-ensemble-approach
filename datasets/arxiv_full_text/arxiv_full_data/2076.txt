{"title": "Feature and Variable Selection in Classification", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "The amount of information in the form of features and variables avail- able to machine learning algorithms is ever increasing. This can lead to classifiers that are prone to overfitting in high dimensions, high di- mensional models do not lend themselves to interpretable results, and the CPU and memory resources necessary to run on high-dimensional datasets severly limit the applications of the approaches. Variable and feature selection aim to remedy this by finding a subset of features that in some way captures the information provided best. In this paper we present the general methodology and highlight some specific approaches.", "text": "amount information form features variables available machine learning algorithms ever increasing. lead classiﬁers prone overﬁtting high dimensions high dimensional models lend interpretable results memory resources necessary high-dimensional datasets severly limit applications approaches. machine learning ﬁeld develops becomes clear issue ﬁnding good features often diﬃcult task using features create classiﬁcation model. often features available reasonably expected used using many features lead overﬁtting hinders interpretability computationally expensive. example. study genetic cause cancer might participants cancer without. participant gene expressions. assume number genes combination cause cancer even underestimate number possible genomes assuming expressions binary possible models. huge number possible models bound arbitrarily complex observation perfectly little predictive power would limit complexity model example discarding nearly possible variables would attain better generalisation. take classiﬁcation task want gain information trained model model complexity hinder insights. take gene example small model might actually show proteins cause cancer might lead treatment. often solution problem needs fulﬁl certain time constraints. robot takes second classify ball ﬂying able catch problem lower dimensionality computational complexity goes dows well. paper ﬁrst discuss conclusions guyon elisseeﬀ general approaches taken feature selection section discuss creation features section ways validate model section continue showing recent developments ﬁeld section figure ranking procedure would features equally useless separate data would discard them. taken together however feature would separate data well. task predict accurately possible algorithm safeguard overﬁtting might better ranking. pipeline scenario considered something treats following phases blackbox would useful. even time reduce dimensionality valuable ranking would help. example. authors discriminative subset genes whether tumor malignant benign. order prune feature base rank variables according correlation classes make preliminary selection discards genes order speed sophisticated procedures select features. implicitly assumes features uncorrelated gives poor results not. ﬁgure variables ground truth roughly variable taken separately gives absolutely information variables selected however would perfectly discriminant feature. since useless would rank high would probably discarded ranking procedure seen ﬁgure ranking approaches ignore value variable connection another ﬁlters select subset features according determined criterion. criterion independent classiﬁer used ﬁltering step. hand allows train following classiﬁer once might cost-eﬀective. hand also means heuristics available well classiﬁer afterwards. filtering methods typically reduce in-class variance boost interclass distance. example approach ﬁlter would maximize correlation variable classiﬁcation minimize correlation variables themselves. heuristic variables correlate don’t provide much additional information compared taking them necessarily case seen ﬁgure variable noisy second correlated variable used better signal seen ﬁgure problem ﬁltering approach performance classiﬁer might depend much would hope proxy measure used subset. scenario might better assess accuracy classiﬁer itself. wrappers allow look classiﬁer blackbox therefore break pipeline metaphor. optimize performance measure classiﬁer objective function. gives superior results heuristics ﬁlters also costs computation time since classiﬁer needs trained time though shortcuts might available depending classiﬁer trained. wrappers large search procedures feature subset space atomic movements remove certain feature. means many combinatorical optimization procedures applied simulated annealing branch-and-bound etc. since subset space number features feasible perform exhaustive search therefore greedy methods applied start either full feature reduce number features optimal start features smart prior model makes less complex models probable. example found section somewhat similar weigth constraint exchange means non-discriminative variables weight. also possible take step optimizing number variables directly since limp→ exactly number non-zero variables vector. previous chapter distinction variables features necessary since could used input classiﬁer feature selection. section features vector oﬀered classiﬁer variables vector handed feature creation step i.e. inputs collected. much reasons motivated feature selection feature creation smaller number features compared number variables provided. essentially information needs compressed stored fewer variables. formally expressed mapping highdimensional space bottleneck hope results recovering dimensional concepts created high-dimensional representation ﬁrst place. case means typical features created similar intuition eﬃcient codes compression simple feature occurs often giving representation reduce loss representing less common feature. fact compression algorithms seen kind feature creation. also related idea manifold learning variable space actual space variables vary much smaller manifold hidden variables embedded variable space. example. human body modelled dimensional model probabilistic principal component analyis assumed hidden variables distributed gaussians dimensional space linearly mapped high dimensional space positions pixels image. allows learn typical positions human body track body shapes even neither camera human ﬁxed. goal point simple model performs well training hope model perform well data never seen before minimizing generalization error. section concerned estimating error. typical approach cross-validation independent identically distributed datapoint split data train model part measure performance rest. even assume data identically distributed requires careful curation data achieve independence example. assume take corpus historical books segment them. could cross-validate pixels would anything independent. able train model half pixels page check half would naturally perform quite well since actually able learn style page. split page-wise learn speciﬁc characteristics author. split author-wise might hope resemblence independence. another approach probing instead modifying data comparing data modify feature space. random variables predictive power feature set. measure well models fare pure chance. performance measure signal-to-noise ratio model. nested subset methods feature subset space greedily examined estimating expected gain adding feature forward selection expected loss removing feature backward selection. estimation called objective function. possible examine objective function classiﬁer directly better performance gained embedding search procedure possible training evaluating classiﬁer necessary step. seen generalisation case output variable taken account develop idea). standard supervised assumes output distributed gaussian distribution dangerous simpliﬁcation classiﬁcation setting. procedure iterates eigenvectors natural parameters joint distribution input output adds show improvement current model order capture inﬂuence input output optimally. variables least favorable score dropped. algorithms iterates ﬁxed number times features hopefully globally optimal feature subset found. paper gene selection using logistic regressions based criteria zhou wang dougherty authors describe problem classifying gene expressions determine whether tumor part certain class since feature intuitive interpretation assume variable either ignored case speciﬁc value matter taken account case value inﬂuences model. assume uniform distribution models ones ignore become probable accumulate probability weight possible values. minimum descriptor length related algorithmic probability states space necessary store descriptor gives best heuristic complex model implicitly causes variable selection. approximation value seen paper itself. since ﬁtting computationally expensive authors start simple ranking variables discard best repeatedly respective models collect number appearances variables rank best genes. step seen additional ranking step seems unnecessary since ﬁtted model construction would already selected best model. even still manage avoid overﬁtting ﬁnding viable subset discriminative variables. deep networks diﬃcult train since show many local minima many show poor performance around this hinton salakhutdinov propose pretraining model stacked restricted bolzmann machines devising global optimisation like stochastical gradient descent. restricted bolzmann machines easy train understood learning probability distribution layer below. stacking means extracting probable distributions features somewhat similar distribution histograms example sift representative visual form object. long speculated low-level features could captured setup show that given enough resources autoencoder learn high level concepts like recognizing face without supervision billion image training set. impressive result beats state supervised learning adding simple logistic regression bottleneck layer. implies features learned network capture concepts present image better sift visual words human created features learn variety concepts parallel. since result single best neuron already discriminative gives evidence possibility grandmother neuron human brain neuron recognizes exactly object case grandmother. using single feature would also take feature selection extreme without beneﬁt computationally advantageous. segmentation task document analysis pixels need classiﬁed regions like border image text taken account pixel values order incorporate spartial information edges etc. heterogeneous features consider pixel evaluation would take long useful. ﬁrst thing consider whether strong prior many features useful. example cancer detection known small number mutation caused tumor model hundred genes could easily discarded. unfortunately case segmentation features don’t causal connection true segmentation. finding good features segmentation requires ﬁnding good proxy feature true segmentation. next might consider loss missclassiﬁcation computer vision task pixel missclassiﬁcations expected smoothed over. computational complexity however severely limit possible applications algorithm. note using bigger dataset advantageous using best algorithm would favour eﬃcient procedure accurate would allow train bigger training set. since variables likely correlated ranking give results. fast classiﬁcation training linear time training methods compared). taking linear regression could additionally advanageous since soft classiﬁcation would allow better joining continuous areas document. many concepts presented still apply however examples fall short statistical justiﬁcation. since applications variable feature selection feature creation developed driven advances computing power high-level feature extraction autoencoders others motivated integrating prior assumptions sparcity model usage probabilistic principal component analysis shape reconstruction. interpretability computational eﬃciency opinion problems best tackled integrating models learned classiﬁer expect embedded approach best ensure optimal treatment them. since many popular eﬃcient classiﬁers support vector machines linear regression neural networks extended incorporate constraints relative ease expect usage ranking ﬁltering wrapping pragmatic ﬁrst step sophisticated learners sparse models employed. advances embedded approaches make performance accuracy advantages stand even more. feature creation seen advances especially eﬃcient generalisations principal component analysis algorithm kernel supervised extensions. predominantly rely bayesian formulation problem expect drive innovation ﬁeld seen spin-oﬀ reconstructing shape images using bayesian network discussed", "year": 2014}