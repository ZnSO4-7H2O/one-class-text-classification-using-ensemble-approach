{"title": "Order-Planning Neural Text Generation From Structured Data", "tag": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "abstract": "Generating texts from structured data (e.g., a table) is important for various natural language processing tasks such as question answering and dialog systems. In recent studies, researchers use neural language models and encoder-decoder frameworks for table-to-text generation. However, these neural network-based approaches do not model the order of contents during text generation. When a human writes a summary based on a given table, he or she would probably consider the content order before wording. In a biography, for example, the nationality of a person is typically mentioned before occupation in a biography. In this paper, we propose an order-planning text generation model to capture the relationship between different fields and use such relationship to make the generated text more fluent and smooth. We conducted experiments on the WikiBio dataset and achieve significantly higher performance than previous methods in terms of BLEU, ROUGE, and NIST scores.", "text": "content arthur ignatius conan doyle edinburgh scotland july crowborough england author writer physician british university edinburgh medical school detective ﬁction fantasy also observe real summaries table ﬁelds provide illuminating clues constraints text generation. biography domain example nationality person typically mentioned occupation. could beneﬁt explicit planning content order neural text generation. paper propose order-planning method table-to-text generation. model built upon encoder-decoder framework text synthesis attention table entries. different exiting neural models design table ﬁeld linking mechanism inspired temporal memory linkage differentiable neural computer ﬁeld linking mechanism explicitly models relationship different ﬁelds enabling neural network better plan ﬁrst next. further incorporate copy mechanism model cope rare words. generating texts structured data important various natural language processing tasks question answering dialog systems. recent studies researchers neural language models encoder-decoder frameworks table-to-text generation. however neural network-based approaches model order contents text generation. human writes summary based given table would probably consider content order wording. biography example nationality person typically mentioned occupation biography. paper propose order-planning text generation model capture relationship different ﬁelds relationship make generated text ﬂuent smooth. conducted experiments wikibio dataset achieve signiﬁcantly higher performance previous methods terms bleu rouge nist scores. generating texts structured data important various natural language processing tasks question answering dialog systems. table shows example wikipedia infobox text summary. early years text generation usually accomplished human-designed rules templates hence generated texts ﬂexible. recently researchers apply neural networks generate texts structured data neural encoder captures table information recurrent neural network decodes information natural language sentence. although neural network-based approach capable capturing complicated language trained end-to-end fashion lacks explicit modeling content order text generation. generates word time step conditioned previous generated words well table information less shortsighted differs human writer does. suggested book elements style previous state-of-the-art results terms bleu rouge nist metrics. extensive ablation tests verify effectiveness component model; also perform visualization analysis better understand proposed order-planning mechanism. model takes input table generates natural language summary describing information based rnn. neural network contains three main components encoder captures table information; dispatcher—a hybrid contentlinkage-based attention mechanism table contents—plans generate next; encoder table representation design neural encoder represent table information. shown figure content ﬁeld split separate words entire table transformed large sequence. recurrent neural network long short term memory units read contents well corresponding ﬁeld names. concretely number content words table; embeddings content corresponding ﬁeld respectively input lstm-rnn concatenation denoted output denoted encoded information corresponding content word i.e. denotes element-wise product denotes sigmoid function. weights bias terms omitted equations clarity. gforget gout known input forget output gates. notice that separate embedding matrices ﬁelds content words. observe ﬁeld names different data samples mostly come ﬁxed candidates reasonable particular domain. therefore assign embedding ﬁeld regardless number words ﬁeld name. example ﬁeld notable work table represented single ﬁeld embedding instead embeddings notable work. content words represent conventional word embeddings lstm-rnn integrate information. table ﬁelds contain sequence words used weighting content representations model dispatcher hybrid contentlink-based attention discussed detail follows. content-based attention. traditionally computation attention based content representation well state decoding call content-based attention also component dispatcher. since ﬁeld name content contain important clues text generation compute attention weights based encoded vector table content also ﬁeld embedding thus obtaining ﬁnal attention αcontent re-weighting other. content words. therefore require link matrix probabilistic distribution normalize probability afterwards equation turns work well empirically. besides would like point link-based attention inspired differentiable neural computer contains linkagebased addressing mechanism track consecutively used memory slots thus integrate order information memory addressing. likewise design link-based attention capture temporal order different ﬁelds. different linking strength heuristically deﬁned link matrix model directly parameterized trained end-to-end manner. hybrid attention. combine attention mechanisms self-adaptive gate sigmoid unit parameter vector. last step’s hidden state decoder rnn. embedding word generated last step; ﬁeld embeddings weighted current step’s ﬁeld attention αlink emphasize content link aspects respectively self-adaptive gate aware both. practice tends address link-based attention decoder sentence generation decoder lstm-rnn predicts target words sequence. also attention mechanism summarizes source information i.e. table scenario weighted yielding attention vector hidden representation obtained table encoder. αhybrid probabilistic distribution— determined content link information—over content words enables decoder focus relevant information time serving order-planning mechanism table-to-text generation. concatenate attention vector embedding last step’s generated word single-layer neural network information feeding decoder rnn. words decoder rnn’s input learnable parameters; vector representations ﬁeld name encoded content respectively row. αcontent content-based attention weights. ideally larger contentbased attention indicates relevant content last generated word. link-based attention. propose link-based attention mechanism directly models relationship different ﬁelds. intuition stems observation that wellorganized text typically reasonable order contents. illustrated previously nationality person often mentioned occupation therefore propose link-based attention explicitly model order information. construct link matrix rnf×nf number possible ﬁeld names dataset. element real-valued score indicating likely ﬁeld mentioned ﬁeld indexes matrix.) link matrix part model parameters learned backpropagation. although link matrix appears large size large number elements used ﬁelds co-occur least data sample; total effective parameters here. scenarios low-rank approximation used reduce number parameters. formally αt−i attention probability table contents last time step generation. particular data sample whose content words ﬁelds f··· ﬁrst weight linking scores previous attention probability normalize weighted score obtain link-based attention probability given intuitively link matrix analogous transition matrix markov chain whereas term αt−j similar step transition markov chain. however scenario table particular data sample contains ﬁelds ﬁeld occur several times contains exp{st)} refers vocabulary list refers content words particular data sample. copy mechanism either generate word vocabulary directly copy word source side. hepful scenario ﬁelds table contain rare unseen words copy mechanism cope naturally. simplicity greedy search inference i.e. time step word largest probability chosen given argmaxw decoding process terminates special symbol <eos> generated indicating sequence. data sample comprises infobox table ﬁeldcontent pairs input system. generation target ﬁrst sentence biography follows setting previous work although ﬁrst sentence considered experiment sentence typically serves summary article. fact target sentence tokens average actually long. also sentence contains information spanning multiple ﬁelds hence order-planning mechanism useful scenario. decoder rnn’s state. score function thought input softmax layer classiﬁcation normalized probabilistic distribution. incorporate copy mechanism approach normalization accomplished considering copying score introduced follows. copy mechanism. copy mechanism scores content word hidden representation encoder side indicating likely content word directly copied target generation. settings decapitalized words kept vocabulary size content words generation candidates also followed previous work even reasonably large vocabulary size out-of-vocabulary words. rationalizes copy mechanism. names table ﬁelds treated special token. removing nonsensical ﬁelds whose content none grouping ﬁelds occurring less times unknown ﬁeld different ﬁeld names total. experiments words’ table ﬁelds’ embeddings -dimensional lstm layers dimensional. notice that ﬁeld content/generation word even string considered different tokens; hence different embeddings. randomly initialized embeddings tuned training. baselines compared model previous results using either traditional language models neural networks. template lebret grangier auli train interpolated kneserney language model comparison kenlm toolkit. also train language model templates. report model performance terms several metrics namely bleu- rouge- nist- computed standard software nist mteval-va.pl rouge-.. include perplexity measure lebret grangier results overall performance. table compares overall performance previous work. that modern neural networks considerably better traditional models without templates. moreover base model outperforms lebret grangier auli showing better engineering efforts. adding proposed components obtain bleu rouge improvement nist improvement achieving state-of-the-art results. ablation test. table provides extensive ablation test verify effectiveness component model. half table shows results without copy mechanism bottom half incorporates copying score described previously. observe copy mechasnim consistently effective different types attention. compare content-based attention link-based attention well hybrid results show that link-based attention alone effective content-based attention. however achieve better performance combining together adaptive gate i.e. proposed hybrid attention. results consistent halves table terms metrics implies content-based attention link-based attention capture different aspects information hybrid suited task table-to-text generation. effect gate. interested effect gate balances content-based attention αcontent table comparing different possible ways using ﬁeld information. none ﬁeld information back network i.e. content-based attention computed equation word american appropriate sentence corrupts phrase former governor federal reserve system appears reference. however link-based attention added network aware order between ﬁelds nationality occupation generates nationality american occupation economist. process could also visualized figure here plot model’s content-based attention link-based attention hybrid. generating emmett john rice content-based attention skips nationality focuses occupation. link-based attention hand provides strong clue suggesting generate nationality ﬁrst occupation. obtained sentence compliant conventions. text generation long aroused interest community wide applications including automated navigation weather forecasting traditionally text generation divided several steps content planning deﬁnes information conveyed generated sentence; sentence planning determines generate sentence; surface realization actually generates sentences words. early years surface realization often accomplished templates statistically learned models e.g. probabilistic context-free grammar language models hand-crafted features rules. therefore methods weak terms quality generated sentences. planning researchers also apply machine learning approaches. barzilay lapata example model collective classiﬁcation problem whereas liang jordan klein generative semi-markov model align text segment assigned meanings. generally planning realization work separate difﬁculty capturing complexity language nature shallow models. link-based attention αlink. deﬁned equation computation depends decoding state well table information; hence self-adaptive. would like verify adaptiveness useful. verify this designed controlled experiment gate manually assigned advance ﬁxed training. words setting essentially interpolation αcontent αlink. speciﬁcally tuned granularity plot bleu scores comparison metric figure seen interpolation contentlink-based attention generally better either single mechanism shows effectiveness hybrid attention. however peak performance simple interpolation worse self-adaptive gate implying gating mechanism automatically adjust importance αcontent αlink particular time based current state input. different ways using field information. curious whether proposed order-planning mechanism better possible ways using ﬁeld information. conducted controlled experiments follows. similar proposed approach multiplied attention probability ﬁeld matrix thus obtained weighted ﬁeld embedding. either computation content-based attention i.e. equations decoder’s input i.e. equation cases last step’s weighted ﬁeld embedding concatenated embedding generated word yt−. table feeding ﬁeld information computation αcontent interferes content attention leads performance degradation feeding decoder slightly improves model performance. however controlled experiments worse proposed method. results conﬁrm order-planning mechanism indeed useful modeling order ﬁelds outperforming several approaches ﬁeld information na¨ıve fashion. emmett john rice former governor federal reserve system cornell university economics professor expert monetary systems developing countries father current national security advisor president barack obama susan rice emmett john rice economist author public ofﬁcial former american governor federal reserve system ﬁrst african american emmett john rice american economist author public ofﬁcial former governor federal reserve system expert monetary systems developing countries paper proposes order-planning approach designing hybrid contentlink-based attention. model inspired hybrid contentlocation-based addressing differentiable neural computer location-based addressing deﬁned heuristically. instead propose transition-like link matrix models likely ﬁeld mentioned another suited scenario. paper propose order-planning neural network generates texts table text generation process built upon attention table contents. different traditional content-based attention explicitly model order contents link matrix based compute link-based attention. self-adaptive gate balances contentlink-based attention mechanisms. incorporate copy mechanism model cope rare unseen words. evaluated approach newly proposed large scale dataset wikibio. experimental results show outperform previous results large margin terms bleu rouge nist scores. also extensive ablation test showing effectiveness copy mechanism well hybrid attention content linking information. compared order-planning mechanism possible ways modeling ﬁeld; results conﬁrm proposed method better feeding ﬁeld embedding network na¨ıve fashion. finally provide case study visualize attention scores better understand model. future work would like deal text generation multiple tables. particular would design hierarchical attention mechanisms ﬁrst select table containing information select ﬁeld generation would improve attention efﬁciency. would also like apply proposed method text generation structured data e.g. knowledge graph. figure visualization attention probabilities model. x-axis generated words american economist y-axis ﬁeld content word pairs table. content-based attention. link-based attention. hybrid attention. subplot exhibits strips because deﬁnition link-based attention yield score content words ﬁeld. please also note columns ﬁgure plot part attention probabilities. tomatically capture highly complicated patterns endto-end training successful applications including machine translation dialog systems text summarization researchers beginning text generation structured data. bansal walter propose coarse-to-ﬁne grained attention mechanism selects records precomputed ﬁxed probability dynamically attends relevant contents decoding. lebret grangier auli incorporate copy mechanism generation process. however approaches explicitly model order contents. also nontrivial combine traditional planning techniques end-to-end learned rnn.", "year": 2017}