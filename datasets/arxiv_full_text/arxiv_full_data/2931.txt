{"title": "Self-Supervised Visual Planning with Temporal Skip Connections", "tag": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "abstract": "In order to autonomously learn wide repertoires of complex skills, robots must be able to learn from their own autonomously collected data, without human supervision. One learning signal that is always available for autonomously collected data is prediction: if a robot can learn to predict the future, it can use this predictive model to take actions to produce desired outcomes, such as moving an object to a particular location. However, in complex open-world scenarios, designing a representation for prediction is difficult. In this work, we instead aim to enable self-supervised robotic learning through direct video prediction: instead of attempting to design a good representation, we directly predict what the robot will see next, and then use this model to achieve desired goals. A key challenge in video prediction for robotic manipulation is handling complex spatial arrangements such as occlusions. To that end, we introduce a video prediction model that can keep track of objects through occlusion by incorporating temporal skip-connections. Together with a novel planning criterion and action space formulation, we demonstrate that this model substantially outperforms prior work on video prediction-based control. Our results show manipulation of objects not seen during training, handling multiple objects, and pushing objects around obstructions. These results represent a significant advance in the range and complexity of skills that can be performed entirely with self-supervised robotic learning.", "text": "abstract order autonomously learn wide repertoires complex skills robots must able learn autonomously collected data without human supervision. learning signal always available autonomously collected data prediction. robot learn predict future predictive model take actions produce desired outcomes moving object particular location. however complex open-world scenarios designing representation prediction difﬁcult. work instead enable self-supervised robot learning direct video prediction instead attempting design good representation directly predict robot next model achieve desired goals. challenge video prediction robotic manipulation handling complex spatial arrangements occlusions. introduce video prediction model keep track objects occlusion incorporating temporal skipconnections. together novel planning criterion action space formulation demonstrate model substantially outperforms prior work video prediction-based control. results show manipulation objects seen training handling multiple objects pushing objects around obstructions. results represent signiﬁcant advance range complexity skills performed entirely self-supervised robot learning. bottleneck enabling robots autonomously learn wide range skills need human involvement example hand-specifying reward functions providing demonstrations resetting arranging environment episodic learning. learning action-conditioned predictive models environment promising approach self-supervised robot learning allows robot learn entirely autonomously collected data. order robot able predict happen response actions needs representation environment suitable prediction. complex open-world environments difﬁcult construct concise sufﬁcient representation prediction. high level number types objects scene might change substantially trial trial making impossible provide single ﬁxed-size representation. level type information important object might change object object motion rigid might depend shape friction coefﬁcient motion deformable stuffed animal might depend variety factors hard determine hand. instead engineering representations different object types object scenes instead directly predict robot’s sensory observations. rationale behind that robot able predict future observations acquired sufﬁcient understanding environment leverage planning actions. major challenge directly predicting visual observations robotic control lies deducing spatial arrangement objects ambiguous observations. example object passes front another robot must remember occluded object still persists. human visual perception referred object permanence known take several months emerge infants robot commanded manipulate object become occluded manipulation must able accurately predict object respond occlusion. work propose visual predictive model robotic control reason spatial arrangements objects using monocular images without providing form camera calibration. previous learning methods proposed explicitly model object motion image-space explicitly model motion using point cloud measurements depth cameras lacked capability maintain information objects occluded predicted motion. propose simple model require explicitly deducing full structure scene provide effective handling occlusions storing appearance occluded objects memory. furthermore unlike prior prediction methods model require additional external systems providing point-to-point correspondences frames different time-steps. technical contribution work three-fold. first present video prediction method accurately maintain object permanence occlusions incorporating temporal skip connections. second propose planning objective control video prediction leads signiﬁcantly improved long-term planning performance including planning occlusions compared prior work finally propose mechanism planning discrete continuous actions video prediction models. evaluation demonstrations components combined enable learned video prediction model perform range realworld pushing tasks. experiments include manipulation previously unseen objects handling multiple objects pushing objects around obstructions moving around obstacle-objects representing signiﬁcant advance range complexity skills acquired entirely self-supervised learning. large-scale self-supervised robotic learning. large-scale robotic data collection explored number recent works. oberlin tellex proposed collect object scans using multiple robots. several prior works focused autonomous data collection individual skills grasping obstacle avoidance contrast methods approach learns predictive models used perform variety manipulations require success measure reward function data collection. several prior approaches also sought learn inverse forward models sensory data without supervision methods demonstrated effective generalization objects limited complexity tasks time-scale tasks could performed. method proposed agrawal able plan single pokes greedily execute multiple pokes sequence. method finn levine performed long-horizon planning effective short motions. comparisons method demonstrate substantial improvement length complexity manipulations performed models. sensory prediction models. action-conditioned video prediction explored context synthetic video game images robotic manipulation video prediction without actions studied unstructured videos driving several works sought complex distributions future images example using autoregressive models often produces sharp predictions resulting models extremely demanding computationally applied real-world robotic control. work extend video prediction methods based predicting transformation previous image prior work also sought predict motion directly using point clouds obtained depth camera requiring point-to-point correspondences time makes hard apply previously unseen objects. predictive model effective wide range real-world object manipulations require depth sensing point-to-point correspondences frames. prior work also proposed plan learned models differentiation though visual inputs instead stochastic sampling-based planning method extend handle mixture continuous discrete actions. section deﬁne image-based robotic control problem present formulation visual model predictive control pixel motion summarize prior video prediction models based image transformation. visual problem formulation follows problem statement outlined prior work assume user deﬁnes goal robot terms pixel motion given image robot’s camera user choose pixels image choose destination pixel moved. example user might select pixel object robot move left. formally user speciﬁes source pixel locations initial image goal locations source goal pixel locations denoted coordinates given goal robot plans sequence actions time steps planning horizon. problem formulated minimization cost function depends predicted pixel positions planner makes learned model predicts pixel motion. given distribution time model predicts distributions positions time achieve best results imperfect models actions iteratively replanned real-world time step τmax} following framework model-predictive control real-world step model used plan steps future ﬁrst action plan executed. ﬁrst real-world time step distribution pt=d initialized location designated pixel zero elsewhere. subsequent steps -step ahead prediction previous step used initialize pt=d. cost function predicted time steps ...t prior work proposed negative log-probability placing pixel goal location cost discuss section performance improved substantially instead using expected distances. planning performed using cross-entropy method gradientfree optimization procedure consists iteratively resampling action sequences reﬁtting gaussian distributions actions best predicted cost. details found prior work visual requires model effectively predict motion selected pixels steps future. work extend model proposed prediction capability emerges implicitly therefore external pixel motion supervision required. future images generated transforming past observations. model uses stacked convolutional lstms predict collection pixel transformations time step corresponding composition masks. model previous image transformed separate transformations transformed images composited together according weights obtained predicted masks. intuitively transformation corresponds motion different object scene mask corresponds spatial extents object. denote transformed images predicted masks ...mn -channel mask size image. next image prediction predict multiple time steps future model applied recursively. transformations represented convolutions pixel transformed image formed applying convolution kernel previous one. method represent wide range local transformations. convolution kernels normalized interpreted transition probabilities allowing make probabilistic predictions future locations individual pixels. predict future positions designated pixels transformations used images applied pt+d tdmi normalizing constant ensure pt+d adds spatial dimension image. since prior predictive model outputs single image time step unable track pixel positions occlusions. therefore model suitable planning motions user-selected pixels occluded manipulation restricts cluttered environments multiple selected pixels. next section discuss proposed occlusion-aware model lifts limitation employing temporal skip connections. enable effective tracking objects occlusions propose extension dynamic neural advection model incorporates temporal skip connections. model uses similar multilayer convolutional lstm structure; however transformations conditioned context previous images. generic version involves conditioning transformation time previous images ...it though practice found greatly simpliﬁed version model performed well practice. therefore ﬁrst present generic model describe practical simpliﬁcations. refer model skip connection neural advection model since handles occlusions copying pixels prior images history pixel occluded still reappear later sequence. predicting next image ˆit+ generic model transforms image history according different transformation different masks produce ˆit+ case negative values simply reuse ﬁrst image sequence. generic formulation computationally expensive since number masks transformations scales tractable model found works comparably well practice robotic manipulation setting assumes occluded objects typically static throughout prediction horizon. assumption allows dispense intermediate transformations provide skip connection ﬁrst image sequence also real image since subsequent images predicted model model needs output masks. observed similar prediction performance using transformed initial image place therefore used simpliﬁed model equation experiments. provide example model recovering occlusion figure ﬁgure moves front designated pixel marked blue figure graphs figure show predicted probability designated pixel stationary entire motion original position step. precisely occludes designated pixel pixel’s probability point decreases. indicates model ‘unsure’ pixel unoccludes designated pixel become visible again probability designated pixel original position case model variants probability mass increase object reappears. model cannot recover information object ‘overwritten’ predictions. consequently models probability stays moves causes model believe occluded pixel also moves arm. identiﬁed major causes planning failure using prior models. contrast case model probability correct pixel position increases rapidly right unoccludes object. furthermore probability unoccluded object’s position becomes increasingly sharp original position moves away. figure simpliﬁed model based equation arrow indicates image ﬁrst time step concatenated transformed images multiplying channel separate mask produce predicted frame step choice objective function visual large impact performance method. intuitively model’s predictions uncertain future planner relies cost function provide reasonable estimate overall distance goal. prior work used probability chosen pixel reaching goal position time steps discussed section trajectories needed reach goal long objective provides relatively little information progress towards goal since predicted probabilities value close zero goal-pixel. work propose cost function still makes uncertainty estimates pixel position provides substantially smoother planning objective resulting improved performance complex longer-horizon tasks. straightforward choice smooth cost function deterministic settings euclidean distance current desired location desired pixel given since video prediction model produces distribution pixel location time step expected value distance cost function summed entire figure rows predicted images moving front green object designated pixel bottom rows predicted probability distributions designated pixel obtained repeatedly applying transformations. figure integrating actions vertical direction enables moving obstacles. results natural shorter paths. executed trajectory left diamond indicates designated pixel green indicates goal predictions probability designated pixel time step expectation computed summing positions predicted image cost corresponds hadamard product pixel location probability distances pixel position goal. cost function encourages movement designated objects right direction step execution regardless whether position reached within time steps. multi-objective tasks multiple designated pixels costs summed together weighting equally. although well-shaped cost functions explored extensively prior work combination cost shaping visual studied extensively. choice action representation visual signiﬁcant effect performance resulting controller. action representation must allow planner sufﬁcient freedom maneuver perform wide variety tasks constraining sufﬁciently create tractable search space. found particularly well-suited action representation tabletop manipulation constructed combining continuous discrete actions contrast prior work used continuous end-effector motion vectors actions actions consist horizontal motion end-effector well discrete lift action allows robot command end-effector lift table vertically. discrete lifting action take values specify many time steps robot lift end-effector table. unlike continuous vertical motion commands discrete commands result end-effector staying table multiple time steps even random data collection. order incorporate hybrid action space stochastic cembased optimization visual sample real-valued parameters discrete action round nearest valid integer obtain discrete actions. usual iteratively reﬁt multivariate gaussian distribution best performing samples treating entire action vector continuous. figure shows example situation discrete vertical motion component action used model lift gripper object order push opposite side. experimental evaluation compares proposed occlusion-aware video prediction model well improved cost function planning previously proposed model based dynamic neural advection sawyer robot shown figure push variety objects tabletop setting. appendix include details hyperparameters analysis sample complexity discussion robustness limitations. evaluate long pushes multi-objective tasks object must pushed without disturbing another. supplementary video links code data available https //sites.google.com/view/sna-visual-mpc figure shows example task pushing benchmark. collected trajectories novel objects training object. table shows results pushing benchmark. column distance refers mean distance between goal pixel designated pixel ﬁnal time-step. column improvement indicates much designated pixel objects could moved closer goal compared starting location. true locations designated pixels pushing annotated human. results table show proposed planning cost equation substantially outperforms planning cost used prior work performance model experiments comparable prior model planning cost since task involve occlusions. although model slightly better mean distance compared well within standard deviation suggesting difference signiﬁcant. examine well approach handle occlusions devised second task requires robot push object keeping another object stationary. stationary object robot must move goal object around shown figure left. this gripper occlude stationary object task performed successfully model make accurate predictions occlusion. tasks speciﬁed picking pixel target object obstacle. obstacle commanded figure left task setup green marking obstacle. right ﬁrst predicted frames generated sna. second probability distribution designated pixel moving object note part distribution shifts left indicated goal. third probability distribution designated pixel obstacleobject although distribution increases entropy occlusion recovers remains original position. table results multi-objective pushing object/goal conﬁgurations seen novel objects. values indicate improvement distance starting position higher better. units pixels images. remain stationary target object destination location chosen side obstacle. used four different object arrangements training objects objects unseen training. found that cases model able valid trajectory prior model mostly unable solution. figure shows example model successfully predicting position obstacle occlusion ﬁnding trajectory avoids obstacle. ﬁndings reﬂected quantitative results shown table indicating importance temporal skip connections. showed visual predictive models trained entirely videos random pushing motions leveraged build model-predictive control scheme able solve wide range multiobjective pushing tasks spite occlusions. also demonstrated combine discrete continuous actions action-conditioned video prediction framework perform complex behaviors lifting gripper move objects. although method achieves signiﬁcant improvement prior work number limitations. behaviors experiments relatively short. principle visual approach allow robot repeatedly retry task succeeds ability retry limited model’s ability track target pixel tracking deteriorates time although model achieves substantially better tracking occlusions prior work repeated occlusions still cause lose track. improving quality visual tracking designated pixels allow system retry task succeeds. complex behaviors picking placing also difﬁcult learn randomly collected data. expect goal-directed data collection would substantially improve model’s ability perform complex tasks. furthermore better predictive models incorporate hierarchical structure reason variable time scales would improve capabilities visual carry temporally extended tasks. fortunately video prediction methods continue improve expect methods improve capability. would like thank prof. patrick smagt technical university munich insightful technical discussions helping organize frederik ebert’s visit berkeley. would also like thank roberto calandra assistance robotic experiments printing. furthermore would like thank ashvin nair pulkit agrawal insightful discussions. work supported fellowship within fitweltweit program german academic exchange service stipend german industrial foundation support siemens national science foundation iis- iis- equipment donation nvidia. chelsea finn alex also partially supported national science foundation graduate research fellowships. references agrawal nair abbeel malik levine. learning poke poking experiential learning intuitive physics. advances neural information processing systems pages boots byravan fox. learning predictive models depth camera manipulator execution traces. international conference robotics automation pages ieee levine pastor krizhevsky ibarz quillen. learning hand-eye coordination robotic grasping deep learning large-scale data collection. international journal robotics research lotter kreiman cox. deep predictive coding networks video prediction unsupervised learning. international conference learning representations rubinstein kroese. cross-entropy method uniﬁed approach combinatorial optimization monte-carlo simulation machine learning. springer science business media xingjian chen wang d.-y. yeung w.-k. wong w.-c. woo. convolutional lstm network machine learning approach precipitation nowcasting. advances neural information processing systems pages video prediction network trained sequences steps taken trajectories length randomly shifting -step long window thus providing form data augmentation. model trained iterations minibatch-size using learing rate adam optimizer visual niter cem-iterations. every iteration action sequences sampled best samples selected ﬁtting gaussian examples. actions sampled according ﬁtted distribution procedure repeated niter iterations. visual algorithm prototyped tested simple block pushing task simulated mujoco physics engine. code made available repository. using simulator training data could collected orders magnitude faster. furthermore benchmark require manual reset manual labeling objects’ positions. main downside simulation however complexity real-world dynamics real-world visual scenes cannot matched. tested effect using different amounts training data compare results visually figure complete size data trajectories. evaluation models trained iterations. quality predictions probability distribution designated pixel vary widely different amounts training data quality predictions crucial performance visual mpc. fact performance still improves large amounts training data suggests model sufﬁcient expressive power leverage amount experience. future investigate effect using even data evaluate effect larger number different training objects. visual fails objects vastly different appearance used training. example objects much bigger training objects methods tend perform poorly. also observed failure object bright color occur training set. however model usually generalizes well novel objects similar size appearance training objects. apart observed main failure modes ﬁrst occurs probability mass designated pixel drifts away object interest since feedback correcting position designated pixel. alleviate problem tracker integrated system enabling visual keep retrying object behave differently expected. second failure mode occurs visual action sequence moves designated pixel closer goal. happen goalpoint pushing object away unlikely sample trajectory moving closer goal. cases problem avoided simply increasing number samples used applying cem. however slows planning process. order enable temporally extended action sequences potential solution could increase efﬁciency sampling process e.g. introducing macro-actions consist sequence actions negative inﬂuence clutter workspace long object moved free path goal. obstacle path marked second designated pixel avoid collision using additional cost function. case planner usually figure rows predicted images. bottom rows predicted probability distributions designated pixel green object marked figure using data model able predict object movement distribution remains initial position object. percent object distribution appears much smeared data. manages around obstacle. however visual current type model capable reasoning push object around obstacles marked explicitly. reason model large uncertainty multiple collisions occur model robust small changes viewpoint likely reason data collection camera slightly displaced position. robustness could improved training model several viewpoints time. prediction problem also becomes harder data might required.", "year": 2017}