{"title": "Convolutional Monte Carlo Rollouts in Go", "tag": ["cs.LG", "cs.AI"], "abstract": "In this work, we present a MCTS-based Go-playing program which uses convolutional networks in all parts. Our method performs MCTS in batches, explores the Monte Carlo search tree using Thompson sampling and a convolutional network, and evaluates convnet-based rollouts on the GPU. We achieve strong win rates against open source Go programs and attain competitive results against state of the art convolutional net-based Go-playing programs.", "text": "work present mcts-based go-playing program uses convolutional networks parts. method performs mcts batches explores monte carlo search tree using thompson sampling convolutional network evaluates convnet-based rollouts gpu. achieve strong rates open source programs attain competitive results state convolutional net-based go-playing programs. game remains unsolved computer algorithms despite advances past decade monte carlo tree search recent work convolutional networks playing produced neural move predictors accuracy datasets historical game records. however modern competitive computer program move predictor component monte carlo tree search loop. previous work used accurate move predictor built deep convolutional network guide search tree exploration mcts exploration half mcts; half consists simulations rollouts original form execute uniformly random moves starting leaf node search tree reaching terminal states produce fast unbiased estimates optimal value function states search tree. nonuniformly random rollout policies improve winning rate mcts compared uniformly random policy. early non-uniform rollout policy pioneered go-playing program mogo matched board patterns ﬁrst local vicinity previous move rest board. coulom extended pattern features weighting relative strength using minorization-maximization algorithm choosing patterns probability determined bradley-terry model. reasonable extension consider incorporating convolutional network place traditional pattern-based rollouts. practice limit thinking time available monte carlo rollouts given ﬁxed budget rollouts ﬁxed policy executing rollouts important execute many rollouts time allotted play possible. simply combining convolutional sequential mcts algorithm impractical inference convolutional great computation latency even executed high throughput perform monte carlo backups rapidly enough mcts. additionally deterministic meaning traversal tree identical monte carlo backups. kind deterministic behavior generally undesirable batched execution monte carlo rollouts. contribution threefold implement mcts-based go-playing program uses convolutional networks executed parts; perform mcts batches maximize throughput convolutions rollouts; demonstrate thompson sampling exploration search tree mcts viable alternative combining three techniques address earlier concerns program consistently wins open source program also competitive deep convolutional net-based programs. convolutional network-based move predictors used greedy move selection applied exploration part mcts. sutskever nair trained -layer convolutional networks move prediction clark storkey maddison later extended results deep convolutional networks. recently tian showed training multiple labels long term prediction improved accuracy playing strength deep convolutional move predictors. works rollout part mcts implemented consisted traditional pattern-based rollouts. several non-convolutional net-based methods predicting ranking moves introduced past. pioneering go-playing program mogo featured pattern-based rollouts stern learned patterns local features using bayesian ranking. coulom computed likelihood patterns local features bradley-terry model. wistuba schmidt-thieme used latent factor ranking achieve move prediction accuracy thompson sampling applied monte carlo tree search non-computer domains perick imagawa kaneko previous works thompson sampling compared bandit algorithms based performance speciﬁc tasks rather focus using thompson sampling guide batch parallelism. cazenave jouandeau chaslot introduced approaches parallelizing mcts including root parallelism tree parallelism. like root parallelism batching approach traverses tree multiple times consecutive backups; however root parallelism share backups parallel trees whereas batched backups naturally unlike tree parallelism asynchronous batching bulk-synchronous. monte carlo tree search literature terms rollout playout simulation often used synonymously deﬁne randomly simulated plays. avoid confusion exclusively term rollout. mcts neural networks applied diﬀerent parts exploration tree rollouts. parts similar that given state classiﬁcation neural network used produce probability selecting action range available actions neural network eﬀectively computing policy probability call network policy network. speciﬁcally call neural used compute prior knowledge probabilities moves ﬁxed game position exploration tree term prior policy network similarly neural used compute probabilities next move take single position rollout called rollout policy network using terminology brieﬂy compare method state convolutional net-based programs. table method utilizes prior policy network rollout policy network. hand previous strong methods prior policy networks supplementing traditional pattern-based monte carlo rollouts. strongest modern playing programs versions mcts. mcts builds search tree game position nodes node keeps track monte carlo values total number trials pass node number successful trials search tree updated three repeating phases exploration rollout backup. canonical version mcts selection criterion exploration phase determined multi-armed bandit algorithm given node search tree denote total number trials node’s j-th child number original rollout policy mcts consisted choosing action according uniform distribution. however quickly found using nonuniformly random rollouts resulted stronger play mcts-based go-playing programs addition basics mcts number widely used heuristic methods biasing pruning search including rave progressive bias progressive widening heuristic associated hyperparameters achieving strong play generally requires hyperparameter tuning. recent successful move predictors trained using supervised learning historical game records. typically board position preprocessed dense mask relatively easy compute binary real features provided input convolutional network. common input features include board conﬁguration point exists number chain liberties stone history distance since last move. using earlier terminology neural network-based move predictor kind policy network. accurate convolutional network architectures predicting moves tend deep least dozen layers typical architecture larger sized ﬁrst convolutional layer followed many convolutional layers. layers also hundreds convolution ﬁlters like modern deep convolutional nets rectiﬁed linear units nonlinearity mcts incorporate policy network provide prior knowledge equivalent experience search tree exploration using earlier terminology policy network providing prior knowledge probabilities prior policy network. ideally prior knowledge derived move evaluator computes approximation optimal value next position optimal action-value move directly using probabilities prior policy network also passable heuristic intuitively directly using probabilities bias monte carlo values makes sense probablities moves closely positively correlated corresponding optimal action-values. additionally helps state values action-values unit interval probabilities values comparable range. theory also possible policy network select nonuniformly random moves rollouts. network called rollout policy network would take place traditional pattern-based rollout policies. however related obstacles preventing convolutional rollout policy networks working eﬀectively convolutions computationally expensive deterministic algorithm. performing inference convolutional network prefers evaluate input batches maximize throughput hardware platforms modern gpus. batching convolutions well known technique forms basis modern minibatch stochastic gradient methods batching eﬀective input states need suﬃciently unique. naively explore monte carlo search tree batches using many states within batch would duplicate states. asynchronous parallel versions also encountered problem getting around using heuristic virtual losses introduce variance tree exploration instead substitute probabilistic bandit algorithm thompson sampling search policy mcts choice justiﬁed recent empirical evidence well proofs comparable regret bounds speciﬁcally thompson sampling bernoulli rewards described optimal action time step selected choosing randomly sampled values gpu. although incurs communication overhead main memory memory believe splitting work optimal especially combined multicore multi-gpu systems. used gogod winter dataset professional game records train prior rollout policies dataset consists historical modern games. limited experiments subset games satisﬁed following criteria board modern standard komi handicap stones. distinguish rulesets somewhat strict criteria produced training games. used relatively concise input feature representation form stack planes features each. time step player tracked binary features marking placement stones points one-hot features denoting whether point belongs chain liberties. tracked last time steps total feature planes. used total diﬀerent network architectures experiments. architectures described table consist ﬁrst layer followed repeated inner layers completed last layer softmax probability output layer; architectural pattern used maddison tian three deep nets prior policy networks. consist shallow minimally deep used rollout policy networks. convolution layers zero-padded unit stride rectifying nonlinearity σrelu used convolutional layers except ﬁnal feeds softmax output instead. table architectures used go-playing program. architectures state deep convolutional nets predicting moves architectures prior policy networks architectures rollout policy networks. layers convolutional dimensions described format optional fourth ﬁeld denoting number layers. architectures validation accuracy measured subset dataset. train convolutional networks predict next move given current board position using stochastic gradient descent. note tian trained deep convolutional move predictors predict next moves found yielded stronger networks compared using single-step labels architectures initialized learning rate annealed learning rate factor every epochs. architecture learning rate tune used zero momentum zero weight decay. architectures followed similar training protocol initialized training learning rate annealed learning rate iterations running least epochs. used momentum weight decay. compared winning rates program open source programs version traditional program level pachi version retsugen mcts program ﬁxed playouts move pondering opponent’s turn disabled. main reason disable pondering pachi batched convolutional rollouts implemented quite expensive execute typical throughput rollout/s rollout/s executing single gpu. parallelizing rollouts multiple gpus signiﬁcantly improve throughput example system consisting high-end nvidia maxwell gpus attain peak throughput approximately rollout/s. show winning rate diﬀerent variants program variants diﬀered combinations prior policy network rollout policy network. also tested variants mcts instead greedily played prior policy network’s best softmax activation; rows marked none/greedy rollouts column. results listed table least interesting observations here. first -layer prior policy network -layer -layer rollout policy network able obtain comparable results -layer networks without mcts promising result shows sophistication rollout policy make weaker prior policies. second winning rate variants -layer rollout policy network seems less programs -layer rollout policy network despite -layer higher accuracy -layer rpn. counterintuitive known paradox previous authors mcts programs encountered possible solution monte carlo simulation balancing de-bias rollout policy explore idea paper instead show remaining results using -layer rollout policy network. table eﬀect network depth rate. prior policy networks width ﬁlters. rows mcts rollouts enabled rollout policy networks width ﬁlters number rollouts move batch size rate measured versus includes standard error bars. number trials row. table compare winning rates method playing mcts-based program pachi. version mcts-enabled method -layer prior policy network -layer rollout policy network perform comparably -layer prior policy network without mcts well -layer network maddison without mcts. mcts program -layer prior policy -layer rollout policy comparable larger -step darkforest -layer network without mcts tian network trained using extra features including point stone history opponent rank also convolution ﬁlters compared ﬁlters network. -step darkfores -layer network traditional pattern-based mcts rollouts tian performs best pachi although also employ enhanced training method using -step lookahead long-term prediction tuned learning rate whereas train networks using single-step lookahead. explore eﬀect diﬀerent mcts batch sizes overall winning rate program pachi shown table interestingly varying batch size little eﬀect winning rate. promising result suggests given ﬁxed number rollouts increasing batch size equivalently decreasing number batch backups limit penalize playing strength. work demonstrated combining convolutional networks exploration phase mcts convolutional nets rollout phase practical eﬀective thompson sampling-based batched mcts. evaluated program open source programs pachi found achieved rates competitive state among convolutional net-based implementations. also found winning rate batched mcts convolutional networks fairly insensitive reasonable values batch size suggesting scaling done. compared winning rate program open source programs reported results convolutional net-based programs commercial otherwise closed source programs much stronger include programs crazy stone baram handicap games highly ranked professional players. future work includes incorporating batched mcts approaches training stronger convolutional nets using long-term prediction training applying deep reinforcement learning methods approximate optimal value action-value function.", "year": 2015}