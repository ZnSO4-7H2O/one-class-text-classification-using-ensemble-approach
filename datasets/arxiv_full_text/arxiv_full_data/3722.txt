{"title": "Measurement-based adaptation protocol with quantum reinforcement  learning", "tag": ["quant-ph", "cond-mat.mes-hall", "cs.AI", "cs.LG", "stat.ML"], "abstract": "Machine learning employs dynamical algorithms that mimic the human capacity to learn, where the reinforcement learning ones are among the most similar to humans in this respect. On the other hand, adaptability is an essential aspect to perform any task efficiently in a changing environment, and it is fundamental for many purposes, such as natural selection. Here, we propose an algorithm based on successive measurements to adapt one quantum state to a reference unknown state, in the sense of achieving maximum overlap. The protocol naturally provides many identical copies of the reference state, such that in each measurement iteration more information about it is obtained. In our protocol, we consider a system composed of three parts, the \"environment\" system, which provides the reference state copies; the register, which is an auxiliary subsystem that interacts with the environment to acquire information from it; and the agent, which corresponds to the quantum state that is adapted by digital feedback with input corresponding to the outcome of the measurements on the register. With this proposal we can achieve an average fidelity between the environment and the agent of more than $90\\% $ with less than $30$ iterations of the protocol. In addition, we extend the formalism to $ d $-dimensional states, reaching an average fidelity of around $80\\% $ in less than $400$ iterations for $d=$ 11, for a variety of genuinely quantum as well as semiclassical states. This work paves the way for the development of quantum reinforcement learning protocols using quantum data, and the future deployment of semi-autonomous quantum systems.", "text": "machine learning employs dynamical algorithms mimic human capacity learn reinforcement learning ones among similar humans respect. hand adaptability essential aspect perform task eﬃciently changing environment fundamental many purposes natural selection. here propose algorithm based successive measurements adapt quantum state reference unknown state sense achieving maximum overlap. protocol naturally provides many identical copies reference state measurement iteration information obtained. protocol consider system composed three parts environment system provides reference state copies; register auxiliary subsystem interacts environment acquire information agent corresponds quantum state adapted digital feedback input corresponding outcome measurements register. proposal achieve average ﬁdelity environment agent less iterations protocol. addition extend formalism d-dimensional states reaching average ﬁdelity around less iterations variety genuinely quantum well semiclassical states. work paves development quantum reinforcement learning protocols using quantum data future deployment semi-autonomous quantum systems. intelligence focuses implementation learning algorithms undergone great development recent years classiﬁed broad groups namely learning means data interactions. ﬁrst group classes supervised learning uses previously classiﬁed data train learning program inferring function relationship classify data. case e.g. pattern recognition problems class unsupervised learning require training data paradigm uses data distribution obtain optimal classify using speciﬁc characteristics. example clustering problem second group learning interactions case reinforcement learning similar paradigm human learning process. general framework follows deﬁne basic systems agent environment often useful deﬁne register auxiliary system. concept consists inferring information direct interaction indirectly using mediator system obtained information makes decision perform certain task. result task good agent receives reward otherwise punishment. addition algorithms divided three basic parts policy reward function value function policy subdivided three stages ﬁrst interaction environment. stage interacts speciﬁed. second information extraction indicates obtains information finally action takes decision information previous step. refers criterion award reward punishment iteration. evaluates utility referred given task. example consists artiﬁcial players chess protocols exploitation-exploration relation. exploitation refers ability make good decisions exploration possibility making diﬀerent decisions. example want select sports exploitation given quality tested exploration size search area choose test. paradigm good exploitation-exploration relation guarantee convergence learning process optimization depends algorithm. fig. bloch representation environment state initial time state agent. bloch representation environment iteration state agent rotated previous iterations. hand quantum mechanics known improve computational tasks natural question learning algorithms modiﬁed quantum domain? answer question quantum machine learning ﬁeld emerged. recent years fruitful area quantum algorithms developed show possible speedup certain situations relation classical counterparts however novel works focus mainly learning classical data encoded quantum systems processed quantum algorithm decoded read classical machine. context speedup quantum machine learning often balanced necessary resources encode decode information leads unclear quantum supremacy. hand recent works analyze paradigm purely quantum quantum systems learn quantum data. article present quantum machine learning algorithm based reinforcement learning approach convert quantum state system unknown state encoded multiple identical copies another system assisted measurements third system propose coherent feedback loops conditioned measurements order perform adaptation process without human intervention. numerical calculations obtain average ﬁdelities qubit states less measurements qudits protocol achieves average ﬁdelities using iterations dimensions either genuinely quantum semiclassical states. proposal useful implement semi-autonomous quantum devices. framework follows. assume known quantum system called agent many copies unknown quantum state provided system called environment also consider auxiliary system called register interacts then obtain information measuring employ result input reward function. finally perform partially-random unitary transformation depends output idea improve ﬁdelity without projecting state measurements. protocol diﬀers quantum state estimation fact propose semi-autonomous quantum agent future quantum agent learn state environment without human intervention. authors considered inverse problem unknown state evolved known state assisted measurements deviate machine learning paradigm. therefore optimal measurement performed step certain number autonomous iterations agent converges large ﬁdelity unknown state. scripts indicate iteration. example refers operator acts subsystem iteration. moreover lack indices indicates referring general object iterations and/or subsystems. start case subsystem described qubit state. assume described arbitrary state expressed bloch sphere cos/)|e e−iφ sin/)|e initial state reads first introduce general elements reinforcement learning protocol policy policy perform controlled-not gate control target order copy information obtaining measure register qubit basis {||} probability obtain state respectively result means collapse nothing result means measure orthogonal component thus accordingly modify agent. additional information environment perform partially-random unitary operator given random angles form random number range random angles spin component. initialize register qubit state employ copy obtaining next initial state second iteration measure probabilities cos/) sin/) outcomes respectively. finally apply given point that probabilistically terms exploitation-exploration relation means exploitation decreases often) increase exploration increase probability making beneﬁcial change exploitation improves many times) reduce exploration allow small changes following iterations. diagram protocol shown fig. fig. shows numerical calculation mean ﬁdelity single-qubit case. computation random initial states protocol reach ﬁdelities less iterations. fig. depicts evolution exploration parameter iteration values constant fig. parameter small ﬁdelity increases quickly requiring less iterations reach high ﬁdelities however maximum value average ﬁdelity smaller increases. means small changes scan parameter result higher slower learning. fig. quantum circuit diagram measurement-based adaptation protocol. labelled indicates projective measurement process lines denote feedback loops. choose simplicity that every time state measured value reduced increased case. also fact means punishment reward strength words protocol yields number outcomes exploration range change. finally deﬁned value iterations. therefore protocol improves ﬁdelity section extend previous protocol case described d-dimensional qudit state. ingredients qubit case cnot gate. here extension cnot gate multilevel states also known gate action gate given index refers control state respectively denotes diﬀerence modulo dimension subsystem. cnot gate important properties namely hermitian ja|kb ja|b propu erties maintained gate deﬁned policy essentially previous case consider multiple outcomes d−}) result measuring first introduce deﬁnition previous case assume initial state initialized moreover state fig. performance measurement-based adaptation protocol total random state given shows mean ﬁdelity initial random states value iteration. ﬁgures black dashed line indicates average ﬁdelity random numbers normalization factor. figure shows numerical calculations case gives average ﬁdelity initial states given evolution iteration. also shows exploration parameter reduced ﬁdelity grows fig. protocol reach mean ﬁdelities iterations equivalently protocol increases mean ﬁdelity using iterations. fig. performance measurement-based adaptation protocol qubit case. shows mean ﬁdelity initial random states value iteration. ﬁgures black dashed line indicates average ﬁdelity case measure multiple outcomes. therefore separated groups. first outcome probability cos. second outcomes probability obtain sin/). previous case means either measure state orthogonal subspace. information perform partially-random unitary operation agent using deﬁnition outcome measurement. random angles deﬁned qubit case. changes slightly given delta function. equation means measure value decreases next iteration measure increases. remember qubit case binary since results equally nonbeneﬁcial give punishment agent. reason policy qubit protocol case multiple levels. case single qubit state parameter plays fundamental role learning process handling speed learning maximum learning understand follows. fig. performance measurement-based adaptation protocol coherent state form shows mean ﬁdelity random pairs value iteration. ﬁgures black dashed line indicates average ﬁdelity protocol implemented platform enables logical operator qudits digital feedback loops case circuit quantum electrodynamics platform takes particular relevance fast development quantum computation current technology cqeds allows digital quantum feedback loops elapsed times ﬁdelities around well-controlled twoqubits gates ﬁdelities less qubits coherence times allows iterations protocol suﬃcient number feasible implementation. aditionally last decade multilevel gates theoretically proposed well eﬃcient multiqubit gates recently proposed using approach providing necessary elements experimental implementation general framework learning protocol. propose analyse quantum reinforcement learning protocol adapt quantum state another unknown quantum state context several identical copies unknown state available. main goal proposal agent acquire information environment semi-autonomous namely reinforcement learning spirit. show ﬁdelity increases rapidly number iterations reaching qubit states average ﬁdelities less measurements. also states dimension obtain average ﬁdelity measurements. performance improved special cases coherent states states states form |n)/ performance protocol handled value parameter number states involved superposition environment state measurement basis. small high learning speed reduced maximum learning. moreover number states superposition related overall performance protocol superposition fewer terms provides better performance increases learning speed well maximum learning requiring less iterations obtain high ﬁdelity. facts imply possible improvement protocol achieved using dynamic parameter measurement device change measurement basis throughout protocol reduce number states involved overlap state besides since protocol increases ﬁdelity small number iterations useful even number copies limited. finally protocol opens door implementation semi-autonomous quantum reinforcement learning fig. performance measurement-based adaptation protocol genuinely quantum states. shows mean ﬁdelity states form mean ﬁdelity repetitions protocol using superposition |e)/ environment. ﬁgures black dashed line indicates average ﬁdelity case positive real random numbers smaller truncate since probabilities tain bounded |e−|α|/α/ figure shows ﬁdelity between iteration reaching values less iterations. figure depicts value process. also observe exploration reduced approaches previous case. figure shows calculation states case reach ﬁdelities measurements. moreover fig. shows results similar qubit case given fig. surpassing ﬁdelities less iterations. last ﬁgure reﬂects fact state protocol reduced qubit case given states involved superposition. thus states form performance qubit case. figs. learning speed inversely proportional parameter means small value implies rapid increase ﬁdelity increases speed learning. hand maximum learning also directly proportional words small value means lower maximum ﬁdelities pertinent emphasize protocol qubit multilevel cases employs two-level operators iteration needs calculate biswas jiang kechezhi knysh mandr`a o’gorman perdomo-ortiz petukhov realpeg´omez rieﬀel venturelli vasko wang parallel computing high-end computing nextgeneration scientiﬁc discovery. perdomo-ortiz feldman ozaeta isakov o’gorman katzgraber diedrich neven kleer arxiv preprint arxiv. sasaki carlini phys. rev. lloyd mohseni arxiv. barends shabani lamata kelly mezzacapo heras babbush fowler campbell chen chen chiaro dunsworth jeﬀrey lucero megrant mutus neeley neil o’malley quintana roushan sank vainsencher wenner white solano neven martinis nature", "year": 2018}