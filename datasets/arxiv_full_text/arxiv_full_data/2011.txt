{"title": "Dense Associative Memory for Pattern Recognition", "tag": ["cs.NE", "cond-mat.dis-nn", "cs.LG", "q-bio.NC", "stat.ML"], "abstract": "A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in deep learning. On the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime. On the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer. This family of activation functions includes logistics, rectified linear units, and rectified polynomials of higher degrees. The proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions - the higher rectified polynomials which until now have not been used in deep learning. The utility of the dense memories is illustrated for two test cases: the logical gate XOR and the recognition of handwritten digits from the MNIST data set.", "text": "model associative memory studied stores reliably retrieves many patterns number neurons network. propose simple duality dense associative memory neural networks commonly used deep learning. associative memory side duality family models smoothly interpolates limiting cases constructed. limit referred feature-matching mode pattern recognition prototype regime. deep learning side duality family corresponds feedforward neural networks hidden layer various activation functions transmit activities visible neurons hidden layer. family activation functions includes logistics rectiﬁed linear units rectiﬁed polynomials higher degrees. proposed duality makes possible apply energy-based intuition associative memory analyze computational properties neural networks unusual activation functions higher rectiﬁed polynomials used deep learning. utility dense memories illustrated test cases logical gate recognition handwritten digits mnist data set. pattern recognition models associative memory closely related. consider image classiﬁcation example pattern recognition. problem network presented image task label image. case associative memory network stores memory vectors. typical query network presented incomplete pattern resembling identical stored memories task recover full memory. pixel intensities image combined together label image vector serve memory associative memory. image thought partial memory cue. task identifying appropriate label subpart associative memory reconstruction. limitation using idea pattern recognition. standard model associative memory works well limit number stored patterns much smaller number neurons equivalently number pixels image. order pattern recognition small error rate would need store many memories typical number pixels presented images. serious problem. solved modifying standard energy function associative memory quadratic interactions neurons including higher order interactions. properly designing energy function models higher order interactions store reliably retrieve many memories number neurons network. deep neural networks proven useful broad range problems machine learning including image classiﬁcation speech recognition object detection etc. models composed several layers neurons output layer serves input next layer. neuron calculates weighted inputs passes result non-linear activation function. traditionally deep neural networks used activation functions hyperbolic tangents logistics. learning weights networks using backpropagation algorithm faced serious problems issues largely resolved introducing unsupervised pre-training made possible initialize weights subsequent backpropagation could gently move boundaries classes without destroying feature detectors recently realized rectiﬁed linear units instead logistic functions speeds learning improves generalization rectiﬁed linear functions usually interpreted ﬁring rates biological neurons. rates equal zero input certain threshold linearly grow input threshold. mimic biology output small zero input threshold much less clear behavior activation function inputs exceeding threshold. grow linearly sub-linearly faster linearly? choice affect computational properties neural network? functions would work even better rectiﬁed linear units? questions best knowledge remain open. paper examines questions lens associative memory. start discussing family models associative memory large capacity. models higher order interactions neurons energy function. associative memory description mapped onto neural network hidden layer unusual activation function related hamiltonian. show varying power interaction vertex energy function force model learn representations data either terms features terms prototypes. standard model associative memory uses system binary neurons values conﬁguration neurons denoted vector model stores memories denoted moment also assumed binary. model deﬁned energy function given dynamical update rule decreases energy every update. basic problem following presented pattern network respond stored memory closely resembles input. large amount work community statistical physicists investigating capacity model maximal number memories network store reliably retrieve. demonstrated case random memories maximal value order tries store patterns several neighboring memories conﬁguration space merge together producing ground state hamiltonian nothing stored memories. modifying hamiltonian removes second order correlations stored memories possible improve capacity mathematical reason model gets confused many memories stored several memories produce contributions energy order. words energy decreases slowly pattern approaches memory conﬁguration space. order take care problem consider modiﬁcation standard energy case polynomial function network reduces standard model associative memory term becomes sharper compared case thus memories packed conﬁguration space cross-talk intervenes. deﬁned energy function derive iterative update rule leads decrease energy. asynchronous updates ﬂipping unit time. update rule argument sign function difference energies. conﬁguration i-th units clumped current states i-th unit state. similar conﬁguration i-th unit state. rule means system updates unit given states rest network energy entire conﬁguration decreases. case polynomial energy function similar family models considered update rule models based induced magnetic ﬁelds however difference energies. slightly different presence self-coupling terms. throughout paper energy-based update rules. many memories model store reliably retrieve? consider case random patterns element memories equal equal probability. imagine system initialized state equal memories derive stability criterion i.e. upper bound number memories network stays initial state. deﬁne energy difference initial state state spin ﬂipped i-th becomes unstable magnitude ﬂuctuation exceeds energy sign ﬂuctuation opposite sign energy gap. thus probability state single neuron unstable equal numerical constant depends threshold case corresponds standard model associative memory gives well known result perfect recovery memory obtains higher powers capacity rapidly grows non-linear allowing network store reliably retrieve many patterns number neurons accord non-linear scaling relationship capacity size network phenomenon exploit. n-dependent coefﬁcient depends exact form hamiltonian update rule. references allow repeated indices products neurons energy function therefore obtain different coefﬁcient. hamiltonian coincides ours update rule different which however results exactly coefﬁcient study family models kind function small many terms contribute approximately equally. limit dominant contribution comes single memory largest overlap input. turns optimal computation occurs intermediate range. case elementary instructive. presented three reasons. first illustrates construction simplest case. second shows increases computational capabilities network also increase. third provides simplest example situation number memories larger number neurons network works reliably. problem following given inputs produce output truth table satisﬁed. treat task associative memory problem simply embed four examples input-output triplets memory. therefore network identical units used inputs output memories four lines truth table. thus energy equal coefﬁcients denote numerical constants. order solve problem present network incomplete pattern inputs output adjust minimize energy three-spin conﬁguration holding inputs ﬁxed. network clearly cannot solve problem since energy depend spin conﬁguration. case standard model associative memory. also thought linear perceptron inability solve problem represents well known statement linear perceptrons cannot compute without hidden neurons. case provides interesting solution. given inputs choose output minimizes energy. leads update rule thus simple case network capable solving problem higher values cannot case rectiﬁed polynomials similar construction solves problem network works well spite fact example pattern recognition problem case mnist mnist data collection handwritten digits training examples test images. goal classify digits classes. visible neurons pixel combined together classiﬁcation neurons vector deﬁnes state network. visible part vector treated incomplete pattern associative memory allowed calculate completion pattern label image. dense associative memory recurrent network every neuron updated multiple times. purposes digit classiﬁcation however model used limited capacity allowing perform update classiﬁcation neurons. network initialized state visible units clamped intensities given image classiﬁcation neurons state network allowed make update classiﬁcation neurons keeping visible units clamped produce output update rule similar except sign replaced continuous function tanh parameter regulates slope proposed digit class given number classiﬁcation neuron producing maximal output. throughout section rectiﬁed polynomials used functions learn effective memories pattern classiﬁcation objective function deﬁned penalizes discrepancy figure network visible neurons classiﬁcation neurons. visible units clamped intensities pixels classiﬁcation neurons initialized state updated state behavior error test training progresses. curve corresponds different combination hyperparameters optimal window determined validation set. arrows show ﬁrst time error falls threshold. models memories output target output. objective function minimized using backpropagation algorithm. learning starts random memories drawn gaussian distribution. backpropagation algorithm ﬁnds collection memories minimize classiﬁcation error training set. memories normalized stay within performance proposed classiﬁcation framework studied function power next section shows rectiﬁed polynomial power energy function equivalent rectiﬁed polynomial power used activation function feedforward neural network hidden layer neurons. currently common choice activation functions training deep neural networks relu language corresponds energy function. although currently used train deep networks case would correspond rectiﬁed parabola activation function. start comparing performances dense memories cases. performance network depends remaining hyperparameters thus hyperparameters optimized value order test variability performances various choices hyperparameters given window hyperparameters network works well validation determined. many networks trained various choices hyperparameters window evaluate performance test set. test errors training progresses shown fig.b. substantial variability among samples average cluster trajectories achieves better results test error rates compared error rates backpropagation alone without generative pretraining various kinds regularizations adversarial training could added construction necessary. class models best published results range also controls agrees results case slightly better clear fig.b samples performing better higher rectiﬁed polynomials also faster training compared relu. case error crosses threshold ﬁrst time training range epochs. case happens earlier average epochs. higher powers speed-up larger. huge effect small dataset mnist. however speed-up might helpful training large networks large datasets imagenet. similar effect reported earlier transition saturating units logistics hyperbolic tangents relu family models result corresponds moving computation performed neural network change varies? extreme classes theories pattern recognition feature-matching formation prototype. according former input decomposed features compared stored memory. subset stored features activated presented input interpreted object. object many features; features also appear object. prototype theory provides alternative approach objects recognized whole. prototypes necessarily match object exactly rather blurred abstract figure show randomly selected memories four networks rectiﬁed polynomials degrees energy function. magnitude memory element corresponding pixel plotted location pixel color explains color code. histograms bottom explained text. error rates refer particular four samples used ﬁgure. stands recognition unit. representations include features object has. argue computational models proposed describe feature-matching mode pattern recognition small prototype regime large anticipated sharpness contributions memory makes total energy large function peaks much sharply around memory compared case small thus large information digit must written memory small information distributed among several memories. case intermediate learned memories behave like features others behave like prototypes. classes memories work together model data efﬁcient way. feature prototype transition clearly seen memories shown fig.. memory look like digit resembles pattern activity might useful recognizing several different digits. many memories recognized digits surrounded white margins representing elements memories approximately zero values. margins describe variability thicknesses lines different training examples mathematically mean energy depend whether corresponding pixel off. memories represent prototypes whole digits large portions digits small admixture feature memories resemble digit. feature prototype transition visualized showing feature detectors situations natural ordering pixels. ordering exists images example. general situations however preferred permutation visible neurons would reveal structure therefore useful develop measure permits distinction made features prototypes absence visual space. towards approximately equal training recognition connections choose arbitrary cutoff count number recognition connections state memory. distribution function number shown left histogram fig.. intuitively quantity corresponds number different digit classes particular memory votes for. small memories vote three different digit classes behavior characteristic features. increases memory specializes votes single class. case example memories vote class behavior characteristic prototypes. second feature prototype transition look number memories make large contributions classiﬁcation decision test image memory makes largest contribution energy count number memories contribute largest contribution. small many memories satisfy criterion distribution function long tail. regime several memories cooperating make classiﬁcation decision. however test images single memory would make contribution comparable largest one. result sensitive arbitrary choice cutoff. interestingly performance remains competitive even large spite fact networks different kind computation compared small section derive simple duality dense associative memory feedforward neural network layer hidden neurons. words show computational model different descriptions terms associative memory terms figure left feedforward neural network layer hidden neurons. states visible units transformed hidden neurons using non-linear function states hidden units transformed output layer using non-linear function right model dense associative memory step update models equivalent. network layer hidden units. using correspondence transform family dense memories constructed different values power language models used deep learning. resulting neural networks guaranteed inherit computational properties dense memories feature prototype transition. construction similar except classiﬁcation neurons initialized state equal fig.. limit expand function dominant contribution comes term linear parameter thus model associative memory step update equivalent conventional feedforward neural network hidden layer provided activation function visible layer hidden layer equal derivative energy function simply statement labels contain less information data itself. point view associative memory dominant contribution shaping basins attraction comes energy states. therefore mathematically determined asymptotics activation function energy function thus different activation functions similar asymptotics fall universality class similar computational properties. table list common activation functions used models deep learning associative memory counterparts power determines asymptotic behavior energy function ∞.the results section suggest large speed learning improve increases. consistent previous observation relu faster training hyperbolic tangents logistics last table corresponds rectiﬁed polynomials higher degrees. best knowledge activation functions used neural networks. results suggest problems higher power activation functions even better computational properties rectiﬁed liner units. relationship capacity dense associative memory calculated section neural network step update used digit classiﬁcation? consider limit large hyperbolic tangent approximately equal sign function limit sufﬁciently large network operating prototype regime. presented image places initial state network close local minimum energy corresponds prototypes. cases step update classiﬁcation neurons sufﬁcient bring initial state nearest local minimum thus completing memory recovery. true however stored patterns stable basins attraction around least size neuron exactly condition given correlated patterns maximal number stored memories might different however still rapidly increases increase associative memory step update exactly equivalent full associative memory multiple updates limit. calculation random patterns thus theoretically justiﬁes expectation good performance prototype regime. summarize paper contains three main results. first shown general framework associative memory pattern recognition. second family models constructed learn representations data terms features terms prototypes smoothly interpolates extreme regimes varying power interaction vertex. third exists simple duality step update version associative memory model feedforward neural network layer hidden units unusual activation function. duality makes possible propose class activation functions encourages network learn representations data various proportions features prototypes. activation functions used models deep learning effective standard choices. allow networks train faster. also observed improvement generalization ability networks trained rectiﬁed parabola activation function compared relu case mnist. ideas illustrated using simplest architecture neural network layer hidden units proposed activation functions also used multilayer architectures. study various regularizations added construction. performance model supplemented regularizations well performance common benchmarks reported elsewhere. networks trained using stochastic gradient descent minibatches relatively large size digits class digits total. training done epochs. initial weights generated gaussian distribution momentum used smooth oscillations gradients coming individual minibatches. learning rate decreasing time according number update index unites visible classiﬁcation units. proposed update normalized largest update weights hidden unit equal normalization equivalent using different learning rates individual memory. prevents network getting stuck plateau. weights constrained stay within range. therefore update weights exceeded truncated make equal slope function controlled effective temperature measured neurons pixels. large temperature kept constant throughout entire training small found useful start high temperature linearly decrease ﬁnal value ﬁrst epochs temperature stays constant that. models memories mnist dataset contains training examples randomly split training cases validation cases. hyperparameter window values selected error validation epochs less certain threshold. entire examples used train network various values hyperparameters optimal window evaluate performance test set. validation used early stopping. objective function given target output case corresponds standard quadratic error. large powers function small rapidly grows therefore higher values emphasize training examples produce largest discrepancy target output strongly compared examples already sufﬁciently close target output. emphasis encourages network concentrate correcting mistakes moving decision boundary farther away barely correct examples rather ﬁtting better better training examples already easily correctly classiﬁed. although much discuss valid arbitrary value including found higher values reduce overﬁtting improve generalization least limit large small used larger values worked better. also tried cross-entropy objective function together softmax output units. results worse presented here. training done associative memory description neural network description. related duality section give explicit expressions update rule methods. consider minibatch size associative memory framework deﬁne matrices indices united tensor product index sums efﬁciently calculated using matrix-matrix multiplication. training network closely related theoretical calculations presented main text computationally inefﬁcient. second dimension matrices times larger size minibatch. become problematic classiﬁcation problem involves many classes. reason computationally easier train dense memory dual description closely related conventional methods used deep learning. framework minibatch matrix summation visible index assumed. expressions similar conventional derivatives used networks rectiﬁed linear activation functions power activation functions instead. minibatch training efﬁciently implemented gpu. section main text theoretical calculation capacity model presented case power energy functions. section intuitive argument given arguing capacities models power energy functions rectiﬁed polynomial energy functions similar. appendix compare theoretical results section numerical simulations numerically validate intuitive argument energy states. random binary memory vectors generated model neurons. collection random initial conﬁgurations binary spins evolved according convergence. quality memory recovery measured overlap remember energy function power dual activation function power recovery perfect quantity equal spins failed match memory vector quantity smaller fig. histograms overlaps shown case power rectiﬁed polynomial energy functions. number memories errors places model capacity errors thus model unable reconstract memories. errors thus distribution sharply peaks number memories capacity convergence. number memories half samples perfectly converge memories. fig. dependence shown power rectiﬁed models solid curve given eq.. results numerical simulations case power activation functions consistent theoretical calculation results rectiﬁed polynomials little theoretical curve show similar non-linear behavior. thank chazelle huse levine mitchell monasson peliti raskovalov members simons center systems biology useful discussions. especially thank roudi pointing reference work supported charles brown membership ias.", "year": 2016}