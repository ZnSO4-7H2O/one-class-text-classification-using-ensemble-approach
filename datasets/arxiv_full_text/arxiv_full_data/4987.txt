{"title": "Efficient Architecture Search by Network Transformation", "tag": ["cs.LG", "cs.AI"], "abstract": "Techniques for automatically designing deep neural network architectures such as reinforcement learning based approaches have recently shown promising results. However, their success is based on vast computational resources (e.g. hundreds of GPUs), making them difficult to be widely used. A noticeable limitation is that they still design and train each network from scratch during the exploration of the architecture space, which is highly inefficient. In this paper, we propose a new framework toward efficient architecture search by exploring the architecture space based on the current network and reusing its weights. We employ a reinforcement learning agent as the meta-controller, whose action is to grow the network depth or layer width with function-preserving transformations. As such, the previously validated networks can be reused for further exploration, thus saves a large amount of computational cost. We apply our method to explore the architecture space of the plain convolutional neural networks (no skip-connections, branching etc.) on image benchmark datasets (CIFAR-10, SVHN) with restricted computational resources (5 GPUs). Our method can design highly competitive networks that outperform existing networks using the same design scheme. On CIFAR-10, our model without skip-connections achieves 4.23\\% test error rate, exceeding a vast majority of modern architectures and approaching DenseNet. Furthermore, by applying our method to explore the DenseNet architecture space, we are able to achieve more accurate networks with fewer parameters.", "text": "techniques automatically designing deep neural network architectures reinforcement learning based approaches recently shown promising results. however success based vast computational resources making difﬁcult widely used. noticeable limitation still design train network scratch exploration architecture space highly inefﬁcient. paper propose framework toward efﬁcient architecture search exploring architecture space based current network reusing weights. employ reinforcement learning agent meta-controller whose action grow network depth layer width function-preserving transformations. such previously validated networks reused exploration thus saves large amount computational cost. apply method explore architecture space plain convolutional neural networks image benchmark datasets restricted computational resources method design highly competitive networks outperform existing networks using design scheme. cifar- model withskip-connections achieves test error rate exceeding vast majority modern architectures approaching densenet. furthermore applying method explore densenet architecture space able achieve accurate networks fewer parameters. great success deep neural networks various challenging applications paradigm shift feature designing architecture designing still remains laborious task requires human expertise. recent years many techniques automating architecture design process proposed promising results designing competitive models humandesigned models reported benchmark datasets despite promising results reported success based vast computational resources making difﬁcult used practice individual researchers small sized companies university research teams. another drawback still design train network scratch exploring architecture space without leverage previously explored networks results high computational resources waste. fact architecture design process many slightly different networks trained task. apart ﬁnal validation performances used guide exploration also access architectures weights training curves etc. contain abundant knowledge leveraged accelerate architecture design process like human experts furthermore typically many well-designed architectures human automatic architecture designing methods achieved good performances target task. restricted computational resources limits instead totally neglecting existing networks exploring architecture space scratch economical efﬁcient alternative could exploring architecture space based successful networks reusing weights. paper propose framework called efﬁcient architecture search meta-controller explores architecture space network transformation operations widening certain layer inserting layer adding skip-connections etc. given existing network trained task. reuse weights consider class function-preserving transformations allow initialize network represent function given network different parameterization trained improve performance signiﬁcantly accelerate training network especially large networks. furthermore combine framework recent advances reinforcement learning based automatic architecture designing methods employ based agent meta-controller. experiments exploring architecture space plain convolutional neural networks purely consists convolutional fully-connected pooling layers without skip-connections branching etc. image benchmark datasets show limited computational resources design competitive architectures. best plain model designed cifar- standard data augmentation achieves test error rate even better many modern architectures skip-connections. apply method explore densenet architecture space achieve test error rate cifar without data augmentation cifar- standard data augmentation surpassing best results given original densenet still maintaining fewer parameters. automatic architecture designing long standing study automatic architecture designing. neuroevolution algorithms mimic evolution processes nature earliest automatic architecture designing methods authors used neuro-evolution algorithms explore large architecture space achieved networks match performances human-designed models. parallel automatic architecture designing also studied context bayesian optimization recently reinforcement learning introduced automatic architecture designing shown strong empirical results. authors presented q-learning agent sequentially pick layers; authors used auto-regressive recurrent network generate variable-length string speciﬁes architecture neural network trained recurrent network policy gradient. solutions rely designing training networks scratch signiﬁcant computational resources wasted construction. paper address efﬁciency problem. technically allow reuse existing networks trained task take network transformation actions. functionpreserving transformations alternative based meta-controller used explore architecture space. moreover notice complementary techniques learning curve prediction improving efﬁciency combined method. network transformation knowledge transfer generally modiﬁcation given network viewed network transformation operation. paper since utilize knowledge stored previously trained networks focus identifying kind network transformation operations would able reuse pre-existing models. idea reusing pre-existing models knowledge transfer neural networks studied before. netnet technique introduced describes speciﬁc function-preserving transformations namely netwidernet netdeepernet respectively initialize wider deeper student network represent functionality given teacher network proved signiﬁcantly accelerate training student network especially large networks. similar function-preserving schemes also proposed resnet particularly training deep architectures additionally network compression technique presented prunes less important connections order shrink size neural networks without reducing accuracy. paper instead focus utilizing network transformations reuse pre-existing models efﬁciently economically explore architecture space automatic architecture designing. reinforcement learning background metacontroller work based techniques training agent maximize cumulative reward interacting environment reinforce algorithm similar updating meta-controller advanced policy gradient methods applied analogously. action space however different based approach actions network transformation operations like adding deleting widening etc. others speciﬁc conﬁgurations newly created network layer preceding layers. speciﬁcally model automatic architecture design procedure sequential decision making process state current network architecture action corresponding network transformation operation. steps network transformations ﬁnal network architecture along weights transferred initial input network trained real data validation performance calculate reward signal used update meta-controller policy gradient algorithms maximize expected validation performances designed networks meta-controller. section ﬁrst introduce overall framework meta-controller show speciﬁc network transformation decision made later extend function-preserving transformations densenet architecture space directly applying original netnet operations problematic since output layer subsequent layers. consider learning meta-controller generate network transformation actions given current network architecture speciﬁed variable-length string able generate various types figure overview based meta-controller consists encoder network encoding architecture multiple separate actor networks taking network transformation actions. network transformation actions keeping metacontroller simple encoder network learn lowdimensional representation given architecture separate actor network generate certain type network transformation actions. furthermore handle variable-length network architectures input take whole input architecture consideration making decisions encoder network implemented bidirectional recurrent network input embedding layer. overall framework illustrated figure analogue end-to-end sequence sequence learning actor networks given dimensional representation input architecture actor network makes necessary decisions taking certain type network transformation actions. work introduce speciﬁc actor networks namely netwider actor netdeeper actor correspond netwidernet netdeepernet respectively. netwider actor netwidernet operation allows replace layer wider layer meaning units fully-connected layers ﬁlters convolutional layers preserving functionality. example consider convolutional layer kernel whose shape denote ﬁlter width output channels ﬁrst introduce random remapping function deﬁned work ﬂexible efﬁcient netwider actor simultaneously determines whether layer extended. speciﬁcally layer decision carried shared sigmoid classiﬁer given hidden state layer learned bidirectional encoder network. moreover follow previous work search number ﬁlters convolutional layers units fully-connected layers discrete space. therefore netwider actor decides widen layer number ﬁlters units layer increases next discrete level e.g. structure netwider actor shown figure netdeeper actor netdeepernet operation allows insert layer initialized adding identity mapping layers preserve functionality. convolutional layer kernel identity ﬁlters fully-connected layer weight matrix identity matrix. thus layer number ﬁlters units layer ﬁrst could wider netwidernet operation performed fully preserve functionality netdeepernet operation constraint activation function i.e. must satisfy vectors property holds rectiﬁed linear activation fails sigmoid tanh activation. however still reuse weights existing networks sigmoid directly applying original netnet operations problematic. section introduce several extensions original netnet operations enable function-preserving transformations densenet. different plain densenet layer would receive outputs preceding layers input concatenated channel dimension denoted output would subsequent layers. denote kernel layer shape replace layer wider layer such output wider layer random remapping function deﬁned since output layer subsequent layers densenet replication result replication inputs layers layer. such instead modifying kernel next layer done original netwidernet operation need modify kernels subsequent layers densenet. layer input becomes widening layer thus perspective layer equivalent random remapping function written insert layer densenet suppose layer inserted layer. denote output layer onew input therefore layer input insertion preserve functionality similar netwidernet case onew replication entries possible since input layer ﬁlter layer represented tensor denoted shape knew denote width height ﬁlter number input channels. make output replication figure netdeeper actor uses recurrent network sequentially determine insert layer corresponding parameters layer based ﬁnal hidden state encoder network given input architecture. tanh activation could useful compared random initialization. additionally using batch normalization need output scale output bias batch normalization layer undo normalization rather initialize ones zeros. details netdeepernet operation provided original paper structure netdeeper actor shown figure recurrent network whose hidden state initialized ﬁnal hidden state encoder network. similar previous work allow netdeeper actor insert layer step. speciﬁcally divide architecture several blocks according pooling layers netdeeper actor sequentially determines block insert layer speciﬁc index within block parameters layer. convolutional layer agent needs determine ﬁlter size stride fullyconnected layer parameter prediction needed. architectures fully-connected layer convolutional pooling layers. avoid resulting unreasonable architectures netdeeper actor decides insert layer fully-connected layer ﬁnal global average pooling layer layer restricted fully-connected layer otherwise must convolutional layer. function-preserving transformation densenet original netnet operations proposed discussed scenarios network arranged layer-by-layer i.e. output layer next layer. such modern architectures output layer would multiple subsequent layers densenet modify kernels accordingly. line previous work apply proposed image benchmark datasets explore high performance architectures image classiﬁcation task. notice performances ﬁnal designed models largely depend architecture space computational resources. experiments evaluate different settings. cases restricted computational resources compared previous work used gpus. ﬁrst setting apply explore plain architecture space purely consists convolutional pooling fully-connected layers. second setting apply explore densenet architecture space. image datasets cifar- cifar- dataset consists training images test images. standard data augmentation scheme widely used cifar- denote augmented dataset original dataset denoted preprocessing normalized images using channel means standard deviations. following previous work randomly sample images training form validation using remaining images training exploring architecture space. svhn street view house numbers dataset contains images original training images test additional images extra training set. preprocessing divide pixel values perform data augmentation done follow original training architecture search phase randomly sampled images validation training ﬁnal discovered architectures using training data including original training extra training set. step meta-controller samples networks taking network transformation actions. since sampled networks trained scratch reuse weights given network scenario trained epochs relative small number compared epochs besides smaller initial learning rate reason. settings training networks cifar- svhn similar speciﬁcally nesterov momentum weight decay batch size initial learning rate annealed cosine learning rate decay accuracy held-out validation used compute reward signal sampled network. since gain improving accuracy much larger instead directly using validation accuracy accv reward done perform nonlinear transformation accv i.e. transformed value reward. additionally exponential moving average previous rewards decay baseline function reduce variance. explore plain architecture space start applying explore plain architecture space. following previous automatic architecture designing methods searches layer parameters discrete limited space. every convolutional layer ﬁlter size chosen number ﬁlters chosen stride ﬁxed every fully-connected layer number units chosen additionally table simple start point network. denotes convolutional layer ﬁlters ﬁlter size stride denote average pooling layer ﬁlter size stride respectively; denotes fullyconnected layer units; denotes softmax layer output units. space second experiment architectures discovered ﬁrst experiment start points explore larger architecture space svhn. experiment dataset takes around days gpus. summarized results comparing humandesigned automatically designed architectures similar design scheme reported table model designed plain architecture space outperforms similar models large margin. speciﬁcally comparing humandesigned models test error rate drops svhn. comparing metaqnn q-learning based automatic architecture designing method achieves relative test error rate reduction svhn. also notice best model designed metaqnn depth though maximum original paper suppose maybe trained designed network scratch used aggressive training strategy accelerate training resulted many networks performed especially deep networks. since reuse weights pre-existing networks deep networks validated accurately thus design deeper accurate networks metaqnn. also report comparison state-of-the-art architectures advanced techniques skipconnections branching etc. table though fair comparison since incorporate advanced techniques search space experiment still model designed highly competitive even comparing state-of-the-art modern architectures. speciﬁcally -layers plain parameters outperforms resnet stochastic depth variant pre-activation variant. also approaches best result given densenet. comparing automatic architecture designing methods instart small network begin exploration using small network achieves accuracy held-out validation start point. different restricted start empty ﬂexibly discovered architecture start point. such take advantage ﬂexibility also reduce search space saving computational resources time divide whole architecture search process stages allow meta-controller take steps netdeeper action steps netwider action ﬁrst stage. networks sampled take network performs best currently train longer period time used start point second stage. similarly second stage also allow meta-controller take steps netdeeper action steps netwider action stop exploration networks sampled. progress stages architecture search shown figure gradually learns pick high performance architectures stage. takes function-preserving transformations explore architecture space also sampled architectures consistently perform better start point network stage. thus usually safe explore architecture space eas. take networks discovered second stage further train networks epochs using full training set. finally best model achieves test accuracy furthermore justify transferability discovered networks train architecture svhn random initialization epochs using full training achieves test accuracy better human-designed automatically designed architectures plain architecture space would like emphasize required computational resources achieve result much smaller required speciﬁcally takes less days geforce gpus totally networks trained achieve test error rate starting small network. action. result reported figure shows based meta-controller effectively focus right search direction random search cannot thus high performance architectures efﬁciently random search. explore densenet architecture space also apply explore densenet architecture space. densenet-bc start point. growth rate i.e. width nonbottleneck layer chosen result reported table applying explore densenet architecture space achieve test error rate better best result i.e. given original densenet less parameters. achieve test error rate also outperforming best result i.e. given original densenet less parameters. paper presented framework toward economical efﬁcient architecture search meta-controller implemented agent. learns take actions network transformation explore architecture space. starting existing network reusing weights class function-preserving transformation operations able utilize knowledge stored previously trained networks take advantage existing successful architectures target task explore architecture space efﬁciently. experiments demonstrated eas’s outstanding performance efﬁciency compared several strong baselines. future work would like explore network transformation operations apply different purposes searching networks high accuracy also keep balance size performance. corporate skip-connections search space layers plain model beats except post-processing much deeper parameters model. moreover gpus train hundreds networks gpus train tens thousands networks. comparison random search framework restricted based metacontroller. beside also take network transformation actions explore architecture space random search effective cases experiment compare performances based meta-controller random search meta-controller architecture space used experiments. speciﬁcally network table start point meta-controller take steps netdeeper action steps netwider", "year": 2017}