{"title": "How (not) to Train your Generative Model: Scheduled Sampling,  Likelihood, Adversary?", "tag": ["stat.ML", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "abstract": "Modern applications and progress in deep learning research have created renewed interest for generative models of text and of images. However, even today it is unclear what objective functions one should use to train and evaluate these models. In this paper we present two contributions.  Firstly, we present a critique of scheduled sampling, a state-of-the-art training method that contributed to the winning entry to the MSCOCO image captioning benchmark in 2015. Here we show that despite this impressive empirical performance, the objective function underlying scheduled sampling is improper and leads to an inconsistent learning algorithm.  Secondly, we revisit the problems that scheduled sampling was meant to address, and present an alternative interpretation. We argue that maximum likelihood is an inappropriate training objective when the end-goal is to generate natural-looking samples. We go on to derive an ideal objective function to use in this situation instead. We introduce a generalisation of adversarial training, and show how such method can interpolate between maximum likelihood training and our ideal training objective. To our knowledge this is the first theoretical analysis that explains why adversarial training tends to produce samples with higher perceived quality.", "text": "modern applications progress deep learning research created renewed interest generative models text images. however even today unclear objective functions train evaluate models. paper present contributions. firstly present critique scheduled sampling state-of-the-art training method contributed winning entry mscoco image captioning benchmark show despite impressive empirical performance objective function underlying scheduled sampling improper leads inconsistent learning algorithm. secondly revisit problems scheduled sampling meant address present alternative interpretation. argue maximum likelihood inappropriate training objective end-goal generate natural-looking samples. derive ideal objective function situation instead. introduce generalisation adversarial training show method interpolate maximum likelihood training ideal training objective. knowledge ﬁrst theoretical analysis explains adversarial training tends produce samples higher perceived quality. building sophisticated generative models produce realistic-looking images text important current frontier unsupervised learning. renewed interest generative models attributed factors. firstly thanks active investment machine learning internet companies several products practical use-cases generative models texture generation speech synthesis image caption generation machine translation conversation dialogue generation secondly recent success generative models particularly based deep representation learning raised hopes systems reach sophistication required practical cases. noticable progress made generative modelling many applications still generating fully realistic samples. open questions objective functions train evaluate generative models model likelihood often considered principled training objective research past decades focussed maximum lieklihood approximations thereof hinton hyv¨arinen kingma welling recently seen promising training strategies based adversarial networks kernel moment matching least surface related maximum likelihood. departure motivated fact exact likelihood intractable models. however authors recently observed even models whose likelihood tractable training leads undesired behaviour introduced training procedures deliberately differ maximum likelihood. focus scheduled sampling example this. believe objective function used training reﬂect task want ultimately model for. context paper focus generative models created sole purpose generating realistic-looking samples from. narrower deﬁnition extends use-cases image captioning texture generation machine translation dialogue systems excludes tasks unsupervised pre-training supervised learning semisupervised learning data compression denoising many others. scheduled sampling improper ﬁrst half paper focus autoregressive models sequence generation. models interesting mainly exact maximum likelihood training tractable even relatively complex models stacked lstms however observed autoregressive generative models trained undesired behaviour used generate samples. revisit recent attempt remedy problems scheduled sampling. reexpress scheduled sampling training objective terms kullback-leibler divergences show fact improper training objective. therefore recommend scheduled sampling care. kl-divergence model perceptual loss latter part paper seek alternative solution problem scheduled sampling meant address. uncover fundamental problem applies generative models likelihood right training objective goal generate realistic samples. maximum likelihood thought minimising kullback-leibler divergence klq] real data distribution probabilistic model present model suggests generative models instead trained minimise kullback-leibler divergence opposite direction. differences minimising klq] well understood explain observed undesirable behaviour autoregressive sequence models. generalised adversarial training unfortunately even harder optimise likelihood unlikely yield viable training procedure. instead suggest minimise information quantity call generalised jensen-shannon divergence. show divergence effectively interpolate behaviour klq] klq] thereby containing maximum likelihood ideal perceptual objective function special case. also show generalisations adversarial training procedure proposed employed approximately minimise divergence function. analysis also provides theoretical explanation success adversarial training producing qualitatively superior samples. section focus particularly useful class probabilistic models call autoregressive generative models autoregressive probabilistic model explicitly deﬁnes joint distribution sequence symbols recursively follows note technically equation holds joint distributions further assume component distributions tractable easy compute. autoregressive models considered relatively easy train model likelihood typically tractable. allows train even complicated deep models stacked lstms coherent well understood framework maximum likelihood estimation despite elegance closed-form maximum likelihood training bengio observed maximum likelihood training leads undesirable behaviour models used generate samples from. section review symptoms throughout paper explore different strategies aimed explaining typically training model minimises predictive likelihood symbol training sentence conditioned previous symbols sequence collectively call preﬁx. thought special case maximum likelihood learning joint likelihood symbols sequence factorises conditionals chain rule probabilities. using trained model generate sample sequences generate sequence symbol-by-symbol recursive fashion assuming already generated preﬁx sybols feed preﬁx conditional model output predictive distribution character. character sampled distribution added preﬁx. crucially training time sees preﬁxes real training sequences. however generation time generate preﬁx never seen training data. unlikely preﬁx generated model typically hard time recovering mistake start outputting seemingly random string symbols ending sample poor perceptual quality unlikely true sequence distribution authors stipulate cause observed poor behaviour disconnect model trained it’s used address this authors propose alternative training strategy called scheduled sampling scheduled sampling network sometimes given synthetic data preﬁx instead real preﬁx training time. this authors argue simulates environment model used generating samples method called scheduled sampling describe hyperparameter annealed training initial value here would like understand limiting behaviour training procedure whether appropriate address shortcomings maximum likelihood training. keep notation simple consider case learning sequences length pairs random symbols formulate closed form training objective corresponds scheduled sampling. kept original rather replaced sample scheduled sampling objective fact remains maximum likelihood. understand maximum likelihood minimising following divergence true data distribution approximation here denote marginal distributions ﬁrst symbol respectively qx|x=z px|x=z denote conditional distributions second symbol conditioned value ﬁrst symbol case need consider replaced sample model case training objective expressed following divergence notice second term divergence measured rather conditional real value ﬁrst symbol never shown model asked predict second symbol scheduled sampling choose randomly cases full objective described convex combination dalternative above worth noting point divergence idealised form scheduled sampling. actual algorithm expectations bernoulli would implemented sampling. divergence describes method’s limiting behaviour limit inﬁnite training data. natural requirement divergence function used assess goodness probabilistic models minimised statistics property referred strictly proper scoring rule estimation working strictly proper divergences guarantees consistency training procedure ultimately recover true assuming model class ﬂexible enough enough training data provided. analysis shows scheduled sampling consistent estimation strategy. divergence globally minimised factorised distribution rather correct joint distribution model still inconsistent intermediate values used case divergence global optimum somewhere true joint factorised distribution pxpx. based analysis suggest scheduled sampling works pushling models towards trivial solution memorising distribution symbols conditioned position sequence more precisely maximum likelihood minimises cross-entropy klq] rather preﬁx preceding symbols. recurrent neural network terminology would means optimal architecture uses hidden states merely implement simple counter learns attention whatsoever content sequence preﬁx. indeed lead models likely recover mistakes believe fails address limitations maximum likelihood authors initially solve. could inconsistent training procedure still achieve state-of-the-art performance image captioning challenge? multiple possible explanations this. speculate optimisation full convergence perhaps improvement maximum likelihood solution found coincidence interplay early stopping random restarts speciﬁc structure model class annealing schedule discussing scheduled sampling method proposed remedy symptoms explained section seek better explanation symptoms exist ﬁrst place. leave autoregressive model class consider probabilistic generative models full generality. symptoms outlined section attributed mismatch loss function used training loss used evaluating model problem need training objective closely matches perceptual metric used evaluation ideally allows consistent statistical estimation framework. researchers evaluate generative models perceptual quality draw samples lack better word eyeball samples. visual information processing often referred no-reference perceptual quality assessment using model application like caption generation typically draw sample conditional model qy|x mathbf represents context query present human observer. would like sample pass turing test. want human observer feel like plausible naturally occurring response within context query section propose divergence used idealised objective function describe no-reference perceptual quality assessment scenario. first make assumption perceived quality sample related surprisal qhuman human observers’ subjective prior stimuli qhuman cite. assume human observer learnt accurate model natural distribution stimuli thus qhuman assumptions suggest order optimise chances turing test scenario need minimise following cross-entropy perplexity term however objective eqn. would maximised model deterministically picks likely stimulus. enforce diversity simultaneously maximise entropy leaves following divergence optimise known minimised therefore minimising would correspond consistent estimation strategy. however well-deﬁned positive bounded full support case empirical distribution samples smooth probabilistic model. reason viable practical training objective explain symptoms? differences behaviour klq] well understood exploited example context approximate bayesian inference differences visible model underspeciﬁcation present imagine trying model multimodal simpler unimodal model minimising klq] corresponds moment matching tendency models cover modes cost placing probability mass none. minimising case leads mode-seeking behaviour optimal typically concentrate around largest mode cost completely ignoring smaller modes. differences illustrated visually figure panels context generative models means minimising klq] often leads models overgeneralise sometimes produce samples unlikely would explain recurrent neural networks trained maximum likelihood also tendency produce completely unseen sequences. minimising klq] create model generate behaviour observed real data cost introducing behaviours never seen. contrast train generative model minimising model conservatively avoid behaviour unlikely comes cost ignoring modes completely unless additional modes modelled without introducing probability mass regions none. again klq] deﬁne consistent estimation strategies. differ kind errors make severe model misspeciﬁcation particularly high dimensions. generalised adversarial training theorised meaningful training objective improve perceptual quality generative models impractical objective function. show generalised version adversarial training used approximate training based adversarial training described minimising approximation jensen-shannon divergence divergence deﬁned following formula unlike divergence divergence symmetric arguments understood somewhere klq] terms behaviour. therefore hope would behave like therefore ultimately tend produce realistic samples. indeed behaviour minimisation model misspeciﬁcation similar klq] illustrated figure empirically methods built adversarial training tend produce appealing samples however even formally show divergence indeed interpolation divergences following sense. consider general deﬁnition jensen-shannon divergence parametrised non-trivial probability figure illustrating behaviour generalised divergence model underspeciﬁcation range values data drawn multivariate gaussian distribution approximate single isotropic gaussian contours show level sets approximating distribution overlaid histogram observed data. divergence minimisation behaves like maximum likelihood resulting characteristic moment matching behaviour. behaviour becomes akin mode-seeking behaviour minimising intermediate value recover standard divergence approximated adversarial training. produce illustraiton used software made available theis easy show divergence converges limit crucially shown gradients respect extremes recover klq] respectively. proof property obtained considering taylorexpansion positive deﬁnite hessian substituting follows limiting behaviour also implies small values optima klq]. values close optima thus minimising divergence range allows interpolate behaviour klq] note also approximated adversarial training described practical meaning parameter ratio labelled samples adversarial implies disctiminator network receives training procedure. adversarial network faced balanced classiﬁcation problem case original algorithm samples real distribution overrepresented. similarly classiﬁcation problem biased towards thus generality generative adversarial paper goal understand objective functions work ones don’t context generative models. interested models created purpose drawing samples from excluded use-cases semi-supervised feature learning. maximum likelihood used training objective goal draw realistic samples model. models trained maximum likelhiood tendency overgeneralise generate unplausible samples. scheduled sampling designed overcome shortcomings maximum likelihood fails address fundamental problems showed inconsistent training strategy. theorise could used idealised objective function describe no-reference perceptual quality assessment scenario impractical practice. propose generalised jensen-shannon divergence promising tractable objective function effectively interpolate maximum likelihood ]-minimisation. analysis highlighted merits adversarial training noted method still serious practical limitations. firstly generative adversarial network algorithm based sampling approximate model highly inefﬁcient high dimensional spaces. limits applicability methods low-dimensional problems increases sensitivity hyperparameters. secondly unclear employ adversarial traning discrete probabilistic models sampling process cannot described differentiable operation. make adversarial training practically viable limitations need addressed future work. would like thank lucas theis hugo larochelle useful discussions. would like thank authors theis oord bethge kindly providing source code creating illustrations figure bengio samy vinyals oriol jaitly navdeep shazeer noam scheduled sampling sequence prediction recurrent neural networks. advances neural information processing systems nips http//arxiv.org/abs/.. dziugaite gintare karolina daniel ghahramani zoubin. training generative neural networks maximum mean discrepancy optimization. arxiv preprint arxiv. goodfellow pouget-abadie jean mirza mehdi bing warde-farley david ozair sherjil courville aaron bengio yoshua. generative adversarial nets. advances neural information processing systems lacoste-julien simon husz´ar ferenc ghahramani zoubin. approximate inference loss-calibrated bayesian. international conference artiﬁcial intelligence statistics tsung-yi maire michael belongie serge hays james perona pietro ramanan deva doll´ar piotr zitnick lawrence. microsoft coco common objects context. computer vision–eccv springer zhijian zhang yang. probabilistic acoustic tube probabilistic generative model speech speech analysis/synthesis. international conference artiﬁcial intelligence statistics sordoni alessandro galley michel auli michael brockett chris yangfeng mitchell margaret jian-yun jianfeng dolan bill. neural network approach contextsensitive generation conversational responses. arxiv preprint arxiv. theis lucas hosseini reshad bethge matthias hsiao chuhsing kate. mixtures conditional gaussian scale mixtures applied multiscale image representations. plos wang zhou sheikh hamid bovik alan no-reference perceptual quality assessment jpeg compressed images. image processing. proceedings. international conference volume ieee", "year": 2015}