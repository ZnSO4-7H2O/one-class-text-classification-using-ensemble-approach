{"title": "Towards Deep Learning Models Resistant to Adversarial Attacks", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.", "text": "recent work demonstrated neural networks vulnerable adversarial examples i.e. inputs almost indistinguishable natural data classiﬁed incorrectly network. fact latest ﬁndings suggest existence adversarial attacks inherent weakness deep learning models. address problem study adversarial robustness neural networks lens robust optimization. approach provides broad unifying view much prior work topic. principled nature also enables identify methods training attacking neural networks reliable certain sense universal. particular specify concrete security guarantee would protect adversary. methods train networks signiﬁcantly improved resistance wide range adversarial attacks. also suggest notion security ﬁrst-order adversary natural broad security guarantee. believe robustness well-deﬁned classes adversaries important stepping stone towards fully resistant deep learning models. recent breakthroughs computer vision speech recognition bringing trained classiﬁers center security-critical systems. important examples include vision autonomous cars face recognition malware detection. developments make security aspects machine learning increasingly important. particular resistance adversarially chosen inputs becoming crucial design goal. trained models tend eﬀective classifying benign inputs recent work shows adversary often able manipulate input model produces incorrect output. phenomenon received particular attention context deep neural networks quickly growing body work topic computer vision presents particularly striking challenge small changes input image fool state-of-the-art neural networks high probability holds even benign example classiﬁed correctly change imperceptible human. apart security implications phenomenon also demonstrates current models learning underlying concepts robust manner. ﬁndings raise fundamental question sizable body work proposing various attack defense mechanisms adversarial setting. examples include defensive distillation feature squeezing several adversarial example detection approaches works constitute important ﬁrst steps exploring realm possibilities here. they however oﬀer good understanding guarantees provide. never certain given attack ﬁnds most adversarial example context particular defense mechanism prevents existence welldeﬁned class adversarial attacks. makes diﬃcult navigate landscape adversarial robustness fully evaluate possible security implications. paper study adversarial robustness neural networks lens robust optimization. natural saddle point formulation capture notion security adversarial attacks principled manner. formulation allows precise type security guarantee would like achieve i.e. broad class attacks want resistant formulation also enables cast attacks defenses common theoretical framework. prior work adversarial examples naturally framework. particular adversarial training directly corresponds optimizing saddle point problem. similarly prior methods attacking neural networks correspond speciﬁc algorithms solving underlying constrained optimization problem. conduct careful experimental study optimization landscape corresponding saddle point formulation. despite non-convexity non-concavity constituent parts underlying optimization problem tractable all. particular provide strong evidence ﬁrst-order methods reliably solve problem. supplement insights ideas real analysis motivate projected gradient descent universal ﬁrst-order adversary i.e. strongest attack utilizing local ﬁrst order information network. explore impact network architecture adversarial robustness model capacity plays important role here. reliably withstand strong adversarial attacks networks require signiﬁcantly larger capacity correctly classifying benign examples only. shows robust decision boundary saddle point problem signiﬁcantly complicated decision boundary simply separates benign data points. building insights train networks mnist cifar robust wide range adversarial attacks. approach based optimizing aforementioned saddle point formulation uses optimal ﬁrst-order adversary. best mnist model achieves accuracy strongest adversaries test suite. particular mnist network even robust white attacks iterative adversary. cifar model achieves accuracy adversary. furthermore case weaker black box/transfer attacks mnist cifar networks achieve accuracy respectively. best knowledge ﬁrst achieve levels robustness mnist cifar broad attacks. overall ﬁndings suggest secure neural networks within reach. order support claim invite community attempt attacks mnist cifar networks form challenge. evaluate robustness accurately potentially lead novel attack methods process. complete code along description challenge available https//github.com/madrylab/mnist_challenge https//github.com/madrylab/cifar_challenge. much discussion revolve around optimization view adversarial robustness. perspective captures phenomena want study precise manner also inform investigations. consider standard classiﬁcation task underlying data distribution pairs examples corresponding labels also assume given suitable loss function instance cross-entropy loss neural network. usual model parameters. goal model parameters minimize risk e∼d]. empirical risk minimization tremendously successful recipe ﬁnding classiﬁers small population risk. unfortunately often yield models robust adversarially crafted examples formally eﬃcient algorithms take example belonging class input examples xadv xadv close model incorrectly classiﬁes xadv belonging class order reliably train models robust adversarial attacks necessary augment paradigm appropriately. instead resorting methods directly focus improving robustness speciﬁc attacks approach ﬁrst propose concrete guarantee adversarially robust model satisfy. adapt training methods towards achieving guarantee. ﬁrst step towards guarantee specify attack model i.e. precise deﬁnition attacks models resistant data point introduce allowed perturbations formalizes manipulative power adversary. image classiﬁcation choose captures perceptual similarity images. instance ∞-ball around recently studied natural notion adversarial perturbations focus robustness ∞-bounded attacks paper remark comprehensive notions perceptual similarity important direction future research. next modify deﬁnition population risk incorporating adversary. instead feeding samples distribution directly loss allow adversary perturb input ﬁrst. gives rise following saddle point problem central object study first formulation gives unifying perspective encompasses much prior work adversarial robustness. perspective stems viewing saddle point problem composition inner maximization problem outer minimization problem. problems natural interpretation context. inner maximization problem aims adversarial version given data point achieves high loss. precisely problem attacking given neural network. hand goal outer minimization problem model parameters adversarial loss given inner attack problem minimized. precisely problem training robust classiﬁer using adversarial training techniques. second saddle point problem speciﬁes clear goal ideal robust classiﬁer achieve well quantitative measure robustness. particular parameters yield vanishing risk corresponding model perfectly robust attacks speciﬁed attack model. paper investigates structure saddle point problem context deep neural networks. investigations lead training techniques produce models high resistance wide range adversarial attacks. turning contributions brieﬂy review prior work adversarial examples describe detail formulation. perspective saddle point problem gives answers questions. attack side prior work proposed methods fast gradient sign method multiple variations fgsm attack ∞-bounded adversary computes adversarial example interpret attack simple one-step scheme maximizing inner part saddle point formulation. powerful adversary multi-step variant fgsmk essentially projected gradient descent negative loss function defense side training dataset often augmented adversarial examples produced fgsm. approach also directly follows linearizing inner maximization problem. solve simpliﬁed robust optimization problem replace every training example fgsm-perturbed counterpart. sophisticated defense mechanisms training multiple adversaries seen better exhaustive approximations inner maximization problem. current work adversarial examples usually focuses speciﬁc defensive mechanisms attacks defenses. important feature formulation attaining small adversarial loss gives guarantee allowed attack fool network. deﬁnition adversarial perturbations possible loss small perturbations allowed attack model. hence focus attention obtaining good solution unfortunately overall guarantee provided saddle point problem evidently useful clear whether actually good solution reasonable time. solving saddle point problem involves tackling non-convex outer minimization problem non-concave inner maximization problem. contributions demonstrating that practice solve saddle point problem all. particular discuss experimental exploration structure given non-concave inner problem. argue loss landscape corresponding problem surprisingly tractable structure local maxima. structure also points towards projected gradient descent ultimate ﬁrst-order adversary. sections show resulting trained networks indeed robust wide range attacks provided networks suﬃciently large. recall inner problem corresponds ﬁnding adversarial example given network data point problem requires maximize highly non-concave function would expect intractable. indeed conclusion reached prior work resorted linearizing inner maximization problem pointed above linearization approach yields well-known methods fgsm. training fgsm adversaries shown successes recent work also highlights important shortcomings one-step approach understand inner problem detail investigate landscape local maxima multiple models mnist cifar. main tool experiments projected gradient descent since standard method large-scale constrained optimization. order explore large part loss landscape re-start many points balls around data points respective evaluation sets. surprisingly experiments show inner problem tractable least perspective ﬁrst-order methods. many local maxima spread widely apart within tend well-concentrated loss values. echoes folklore belief training neural networks possible loss typically many local minima similar values. observe loss achieved adversary increases fairly consistent plateaus rapidly performing projected gradient descent randomly chosen starting points inside investigating concentration maxima further observe large number random restarts loss ﬁnal iterate follows well-concentrated distribution without extreme outliers figure cross-entropy loss values creating adversarial example mnist cifar evaluation datasets. plots show loss evolves runs projected gradient descent starts uniformly random point ∞-ball around natural example adversarial loss plateaus small number iterations. optimization trajectories ﬁnal loss values also fairly clustered especially cifar. moreover ﬁnal loss values adversarially trained networks signiﬁcantly smaller naturally trained counterparts. demonstrate maxima noticeably distinct also measured distance angles pairs observed distances distributed close expected distance random points ball angles close along line segment local maxima loss convex attaining maximum endpoints reduced constant factor middle. nevertheless entire segment loss considerably higher random point. finally observe distribution maxima suggests recently developed subspace view adversarial examples fully capturing richness attacks particular observe adversarial perturbations negative inner product gradient example deteriorating overall correlation gradient direction scale perturbation increases. experiments show local maxima found similar loss values normally trained networks adversarially trained networks. concentration phenomenon suggests intriguing view problem robustness adversary yields robustness ﬁrst-order adversaries i.e. attacks rely ﬁrst-order information. long adversary uses gradients loss function respect input conjecture signiﬁcantly better local maxima pgd. give experimental evidence hypothesis section train network robust adversaries becomes robust wide range attacks well. figure values local maxima given cross-entropy loss examples mnist cifar evaluation datasets. example start projected gradient descent uniformly random points ∞-ball around example iterate loss plateaus. blue histogram corresponds loss naturally trained network histogram corresponds adversarially trained counterpart. loss signiﬁcantly smaller adversarially trained networks ﬁnal loss values concentrated without outliers. hard ﬁrst order methods even large number random restarts function values signiﬁcantly diﬀerent loss values. incorporating computational power adversary attack model reminiscent notion polynomially bounded adversary cornerstone modern cryptography. there classic attack model allows adversary solve problems require polynomial computation time. here employ optimization-based view power adversary suitable context machine learning. developed thorough understanding computational complexity many recent machine learning problems. however vast majority optimization problems solved ﬁrst-order methods variants eﬀective training deep learning models particular. hence believe class attacks relying ﬁrst-order information sense universal current practice deep learning. together ideas chart towards machine learning models guaranteed robustness. train network robust adversaries robust wide range attacks encompasses current approaches. fact robustness guarantee would become even stronger context transfer attacks i.e. attacks adversary direct access target network. instead adversary less speciﬁc information model architecture training data set. view attack model example zero order attacks i.e. attacks adversary direct access classiﬁer able evaluate chosen examples without gradient feedback. discuss transferability section appendix. observe increasing network capacity strengthening adversary train improves resistance transfer attacks. also expected resistance best models attacks tends signiﬁcantly larger ﬁrst order attacks. preceding discussion suggests inner optimization problem successfully solved applying pgd. order train adversarially robust networks also need solve outer optimization problem saddle point formulation model parameters minimize adversarial loss value inner maximization problem. context training neural networks main method minimizing loss function stochastic gradient descent natural computing gradient outer problem computing gradient loss function maximizer inner problem. corresponds replacing input points corresponding adversarial perturbations normally training network perturbed input. priori clear valid descent direction saddle point problem. however case continuously diﬀerentiable functions danskin’s theorem classic theorem optimization– states indeed true gradients inner maximizers corresponds descent directions saddle point problem. despite fact exact assumptions danskin’s theorem hold problem experiments suggest still gradients optimize problem. applying using gradient loss adversarial examples consistently reduce loss saddle point problem training seen figure observations suggest reliably optimize saddle point formulation thus train robust classiﬁers. formally state danskin’s theorem describe applies problem section appendix. solving problem equation successfully suﬃcient guarantee robust accurate classiﬁcation. need also argue value problem small thus providing guarantees performance classiﬁer. particular achieving small value corresponds perfect classiﬁer robust adversarial inputs. ﬁxed possible perturbations value problem entirely dependent architecture classiﬁer learning. consequently architectural capacity model becomes major factor aﬀecting overall performance. high level classifying examples robust requires stronger classiﬁer since presence adversarial examples changes decision boundary problem complicated experiments verify capacity crucial robustness well ability successfully train strong adversaries. mnist dataset consider simple convolutional network study behavior changes diﬀerent adversaries keep doubling size network figure conceptual illustration natural adversarial decision boundaries. left points easily separated simple decision boundary. middle simple decision boundary separate ∞-balls around data points. hence adversarial examples misclassiﬁed. right separating ∞-balls requires signiﬁcantly complicated decision boundary. resulting classiﬁer robust adversarial examples bounded ∞-norm perturbations. initial network convolutional layer ﬁlters followed another convolutional layer ﬁlters fully connected hidden layer units. convolutional layers followed max-pooling layers adversarial examples constructed results figure cifar dataset used resnet model performed data augmentation using random crops ﬂips well image standarization. increase capacity modiﬁed network incorporating wider layers factor results network residual units ﬁlters each. network achieve accuracy trained natural examples. adversarial examples constructed results capacity experiments appear figure observe following phenomena capacity alone helps. observe increasing capacity network training using natural examples increases robustness one-step perturbations. eﬀect greater considering adversarial examples smaller fgsm adversaries don’t increase robustness training network using adversarial examples generated fgsm observe network overﬁts adversarial examples. behavior known label leaking stems fact adversary produces restricted adversarial examples network overﬁt networks poor performance natural examples don’t exhibit kind robustness adversaries. case smaller loss ofter linear enough ball around natural examples fgsm ﬁnds adversarial examples close found thus reasonable adversary train against. case small capacity networks weak models fail learn non-trivial classiﬁers. attempting train strong adversary prevents network learning anything meaningful. network converges always predicting ﬁxed class even though could converge accurate classiﬁer natural training. small capacity network forces training procedure sacriﬁce performance natural examples order provide kind robustness adversarial inputs. value saddle point problem decreases increase capacity. fixing adversary model training value drops capacity increases indicating model adversarial examples increasingly well. capacity stronger adversaries decrease transferability. either increasing capacity network using stronger method inner optimization problem reduces eﬀectiveness transferred adversarial inputs. validate experimentally observing correlation gradients source transfer network becomes less signiﬁcant capacity increases. describe experiments section appendix. figure eﬀect network capacity performance network. trained mnist cifar networks varying capacity natural examples fgsm-made adversarial examples pgd-made adversarial examples. ﬁrst three plots/tables dataset show natural adversarial accuracy changes respect capacity training regime. ﬁnal plot/table show value cross-entropy loss adversarial examples networks trained corresponds value saddle point formulation diﬀerent sets allowed perturbations. mnist cifar adversary choice projected gradient descent starting random perturbation around natural example. corresponds notion \"complete\" ﬁrst-order adversary algorithm eﬃciently maximize loss example using ﬁrst order information. since training model multiple epochs beneﬁt restarting multiple times batch start chosen next time example encountered. training adversary observe steady decrease training loss adversarial examples illustrated figure behavior indicates indeed successfully solving original optimization problem training. figure cross-entropy loss adversarial examples training. plots show adversarial loss training examples evolves training mnist cifar networks adversary. sharp drops cifar plot correspond decreases training step size. plots illustrate consistently reduce value inner problem saddle point formulation thus producing increasingly robust classiﬁer. mnist. iterations projected gradient descent adversary step size norm i.e. adding sign gradient since makes choice step size simpler). train evaluate perturbations size network consisting convolutional layers ﬁlters respectively followed max-pooling fully connected layer size trained natural examples network reaches accuracy evaluation set. however evaluating examples perturbed fgsm accuracy drops given resulting mnist model robust investigated learned parameters order understand aﬀect adversarial robustness. results investigation presented appendix table mnist performance adversarially trained network diﬀerent adversaries model attack show successful attack bold. source networks used attack network indepentenly initialized trained copy network architecture cifar. cifar dataset architectures described trained network adversary projected gradient descent again time using steps size total hardest adversary chose steps settings since hyperparameter choices didn’t oﬀer signiﬁcant decrease accuracy. results experiments appear table adversarial robustness network signiﬁcant given power iterative adversaries still satisfactory. believe results improved pushing along directions training networks larger capacity. table cifar performance adversarially trained network diﬀerent adversaries model attack show eﬀective attack bold. source networks considered attack network independtly initialized trained copy network copy network trained natural examples order perform broader resistance diﬀerent values -bounded attacks. evaluation adversarial robustness models kinds additional experiments. hand investigate resistance ∞-bounded attacks diﬀerent values hand examine resistance model attacks bounded opposed norm. results appear figure emphasize models examining correspond training ∞-bounded attacks original value mnist cifar. particular mnist model retains signiﬁcant resistance -norm-bounded perturbations quite good accuracy regime even value perspective provide sample corresponding adversarial examples figure appendix observe underlying perturbations large enough even human could confused. figure performance adversarially trained networks adversaries diﬀerent strength. mnist cifar networks trained adversaries respectively plots). notice less equal value used training performance equal better. mnist sharp drop shortly after. growing body work adversarial examples focus related papers here. compare contributions remark robust optimization studied outside deep learning multiple decades. refer reader overview ﬁeld. recent work adversarial training imagenet also observed model capacity important adversarial training contrast paper training multi-step methods lead resistance adversaries. moreover study loss landscape saddle point problem detail. version min-max optimization problem also considered adversarial training. however three important diﬀerences formerly mentioned result present paper. firstly authors claim inner maximization problem diﬃcult solve whereas explore loss surface detail randomly re-started projected gradient descent often converges solutions comparable quality. shows possible obtain suﬃciently good solutions inner maximization problem oﬀers good evidence deep neural network immunized adversarial examples. secondly consider one-step adversaries work multi-step methods. additionally experiments produce promising results evaluated fgsm. however fgsm-only evaluations fully reliable. evidence reports accuracy adversary allowed perturb pixel construct uniformly gray image thus fooling classiﬁer. recent paper also explores transferability phenomenon. exploration focuses mostly region around natural examples loss linear. large perturbations allowed region give complete picture adversarial landscape. conﬁrmed experiments well pointed another recent paper considers adversarial training using black-box attacks similar networks order increase robustness network adversaries. however eﬀective defense white-box setting consider since adversary reliably produce adversarial examples networks. ﬁndings provide evidence deep neural networks made resistant adversarial attacks. theory experiments indicate design reliable adversarial training methods. insights behind unexpectedly regular structure underlying optimization task even though relevant problem corresponds maximization highly non-concave function many distinct local maxima values highly concentrated. overall ﬁndings give hope adversarially robust deep learning models within current reach. mnist dataset networks robust achieving high accuracy wide range powerful adversaries large perturbations. experiments cifar reached level performance yet. however results already show techniques lead signiﬁcant increase robustness network. believe exploring direction lead adversarially robust networks dataset. aleksander mądry aleksandar makelov dimitris tsipras supported grant google research fellowship sloan research fellowship. ludwig schmidt supported google fellowship. adrian vladu supported grants", "year": 2017}