{"title": "Low-rank and Sparse Soft Targets to Learn Better DNN Acoustic Models", "tag": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "abstract": "Conventional deep neural networks (DNN) for speech acoustic modeling rely on Gaussian mixture models (GMM) and hidden Markov model (HMM) to obtain binary class labels as the targets for DNN training. Subword classes in speech recognition systems correspond to context-dependent tied states or senones. The present work addresses some limitations of GMM-HMM senone alignments for DNN training. We hypothesize that the senone probabilities obtained from a DNN trained with binary labels can provide more accurate targets to learn better acoustic models. However, DNN outputs bear inaccuracies which are exhibited as high dimensional unstructured noise, whereas the informative components are structured and low-dimensional. We exploit principle component analysis (PCA) and sparse coding to characterize the senone subspaces. Enhanced probabilities obtained from low-rank and sparse reconstructions are used as soft-targets for DNN acoustic modeling, that also enables training with untranscribed data. Experiments conducted on AMI corpus shows 4.6% relative reduction in word error rate.", "text": "conventional deep neural networks speech acoustic modeling rely gaussian mixture models hidden markov model obtain binary class labels targets training. subword classes speech recognition systems correspond context-dependent tied states senones. present work addresses limitations gmm-hmm senone alignments training. hypothesize senone probabilities obtained trained binary labels provide accurate targets learn better acoustic models. however outputs bear inaccuracies exhibited high dimensional unstructured noise whereas informative components structured lowdimensional. exploit principle component analysis sparse coding characterize senone subspaces. enhanced probabilities obtained low-rank sparse reconstructions used soft-targets acoustic modeling also enables training untranscribed data. experiments conducted corpus shows relative reduction word error rate. based acoustic models state-of-the-art automatic speech recognition past years input consists multiple frames acoustic features target output obtained frame level gmm-hmm forced alignment corresponding context dependent tied triphone states senones procedure results inefﬁciency acoustic modeling unlike conventional practice present work argues optimal targets probability distributions rather kronecker deltas earlier studies optimal training neural network decoding provides rigorous theoretical analysis supports idea here propose based data driven framework obtain accurate probability distributions improved acoustic modeling. proposed approach relies modeling low-dimensional senone subspaces posterior probabilities. speech production known result activations highly constrained articulatory mechanisms leading generation linguistic units low-dimensional non-linear manifolds context acoustic modeling low-dimensional structures exhibited space senone posteriors low-rank sparse representations found promising characterize senone-speciﬁc subspaces senone-speciﬁc structures superimposed high-dimensional unstructured noise. hence projection posteriors underlying low-dimensional subspaces enhances posterior accuracies. work propose application enhanced posteriors generate accurate soft targets earlier works exploiting low-dimensionality acoustic modeling focus exploiting low-rank sparse representations modify architectures small footprint implementation. low-rank decomposition neural network’s weight matrices enables reduction complexity memory footprint. similar goals achieved exploiting sparse connections sparse activations hidden layers dnn. another line research soft targets based training found effective enabling model compression knowledge transfer accurate complex model smaller network approach relies soft targets providing information training binary hard alignments. propose bring together advantage higher information content soft targets accurate model senone space provided low-rank sparse representations train superior acoustic models. soft targets enable characterization senone-speciﬁc subspaces quantifying correlations between senone classes well sequential dependencies information manifested form structures visible among large population training data posterior probabilities. potential posteriors used soft targets training reduced presence unstructured noise. therefore obtain reliable soft targets perform lowrank sparse reconstruction training data posteriors preserve global low-dimensional structures discarding random high-dimensional noise. dnns trained low-rank sparse soft targets capable estimating test posteriors low-dimensional space results better performance. consider dictionary based sparse coding generating low-rank sparse representations respectively. strength lies capturing linear regularities data whereas over-complete dictionary used sparse coding learns model non-linear space union lowdimensional subspaces. dictionary based sparse reconstruction also reduces rank senone posterior space experimental evaluations conducted corpus collection recordings multi-party meetings large vocabulary speech recognition. show section low-rank sparse soft targets lead training better acoustic models. reductions word error rate observed baseline hybrid dnn-hmm system without need explicit sparse coding low-rank reconstruction test data posteriors. moreover enable effective out-of-domain untranscribed data augmenting training data knowledge transfer fashion. dnns trained low-rank sparse soft targets yield upto relative improvement whereas trained non-enhanced soft targets fails exploit knowledge provided untranscribed data. best knowledge signiﬁcant beneﬁt generated soft targets training accurate fig. correlation among senones long input context acoustically similar root decision trees. show examples posterior probabilities particular senone class highlight low-dimensional patterns super-imposed unstructured noise. sparse coding enable recovery underlying patterns discarding unstructured noise provide reliable soft targets training. denotes size outputs equal total number senones. rest paper proposed approach described section experimental analysis carried section section presents concluding remarks directions future work. section describes novel approach towards reliable soft target estimation. study reasons regularities among senone posteriors investigate systematic approaches obtain accurate probabilities soft targets acoustic modeling. towards better targets training earlier works distillation knowledge show potential soft targets model compression sub-optimal nature hard alignments although hard targets assign particular senone label relatively long sequence acoustic frames senone durations usually shorter. long context input frames lead presence acoustic features corresponding multiple senones input assumption binary outputs renders inaccurate. contrast soft outputs quantify sequential information using non-zero probabilities multiple senone classes. contextual senone dependencies arising soft targets attributed ambiguities phonetic transitions furthermore procedure senone extraction leads acoustic correlations among multiple classes corresponding phone-hmm states share root decision tree dependencies characterized analyzing large number senone probabilities training data. frequent dependencies exhibited regularities among correlated dimensions senone posteriors. result matrix formed concatenation class-speciﬁc senone posteriors low-rank structure. words class-speciﬁc senones low-dimensional subspaces dimension higher unity violates principal assumption binary hard targets. practice inaccuracies training lead presence unstructured high-dimensional errors therefore initial senone posterior probabilities obtained forward pass trained hard alignments accurate quantifying senone dependency structures. previous work demonstrates erroneous estimations separated using low-rank sparse representations present study consider application sparse coding obtain reliable soft targets acoustic model training. low-rank reconstruction using eigenposteriors denote forward pass estimate posterior probabilities senone classes {sk}k given acoustic feature time trained using initial labels obtained gmm-hmm forced alignment. collect senone posteriors labeled class gmm-hmm forced alignment mean-center logarithmic domain follows mean collected posteriors log-domain. skewed distribution posterior vectors logarithm posteriors better gaussian assumption pca. concatenate senone posterior vectors operation shown form matrix rk×n sake brevity subscript dropped subsequent expressions. however calculations performed senone classes individually. principal components senone space obtained eigenvector decomposition covariance matrix factorize covariance matrix obtained covariance matrix rk×k identiﬁes eigenvectors diagonal matrix containing sorted eigenvalues. eigenvectors correspond large eigenvalues constitute frequent regularities subspace whereas others carry high-dimensional unstructured noise. hence low-rank projection matrix deﬁned truncation keeps ﬁrst eigenvectors discards erroneous variability captured components. select relatively variability preserved low-rank reconstruction original senone matrix take exponent obtain low-rank senone posterior acoustic frame low-rank posteriors obtained training data used soft targets learning better dnns assume variability quantiﬁes low-rank regularities senone spaces parameter independent senone class. fig. low-dimensional reconstruction senone posterior probabilities achieve accurate soft targets acoustic model training used extract principle components linear subspaces individual senone classes. sparse reconstruction dictionary senone space representatives used non-linear recovery low-dimensional structures. sparse reconstruction using dictionary learning unlike over-complete dictionary learning sparse coding enables modeling non-linear low-dimensional manifolds. sparse modelling assumes senone posteriors generated sparse linear combination senone space representatives collected dictionary dsp. online dictionary learning algorithm learn over-complete dictionary senone using collection training data posteriors senone kaldi toolkit used training dnn-hmm systems. dnns frames temporal context acoustic input hidden layers neurons each. input features dimensional mfcc+∆+∆∆ output dimensional senone probability vector. pronunciation dictionary words bigram model decoding. dictionary learning sprase coding spams toolbox used. regularization factor. dropped subscript calculations still senonespeciﬁc. sparse reconstruction senone posteriors thus obtained ﬁrst estimating sparse representation sparse reconstructed senone posteriors previously found accurate acoustic models dnn-hmm speech recognition particular shown rank senonespeciﬁc matrices much lower sparse reconstruction. present work investigate could also provide accurate soft targets training regularization parameter controls level sparsity level noise removed sparse reconstruction. fig. summarises low-rank sparse reconstruction senone posteriors. section evaluate effectiveness low-rank sparse soft targets improve performance dnn-hmm speech recognition. also investigate importance better acoustic models exploit information untranscribed data. database speech features experiments conducted corpus contains recordings spontaneous conversations meeting scenarios. recordings individual head microphones comprising around hours train hours development hours test set. training data used cross-validation training whereas used tuning regularization parameters experiments using untranscribed additional training data icsi meeting corpus librispeech corpus data icsi corpus consists meeting recordings librispeech data read speech audio-books hour subset baseline dnn-hmm using hard soft targets baseline hybrid dnn-hmm system trained using forced aligned targets using baseline test set. another baseline trained using non-enhanced soft targets baseline. system gives soft-target based dnns randomly initialized trained using cross-entropy loss backpropagation. generation low-rank sparse soft targets group forward pass senone probabilities training data class-speciﬁc senone matrices. this senone labels ground truth based gmm-hmm hard alignments used. matrix restricted vectors senone probabilities facilitate computation principal components sparse dictionary learning. found average rank senone matrices deﬁned number singular values required preserve variability dictionaries size columns learned senone making nearly times overcomplete. procedure depicted fig. implemented generate low-rank sparse soft-targets. also encountered memory issues storing large matrices senone probabilities training cross-validation data. requires enormous amounts storage space hence preserve precision upto ﬁrst decimal places soft targets followed normalizing vector storing disk. assume essential information might dimensions small probabilities. although thresholding compromise approach experiments higher precision signiﬁcant improvement asr. low-rank sparse reconstruction still computed full soft-targets without rounding; perform thresholding storing targets disk. first tune variability preserving low-rank reconstruction parameter sparsity regularizer better performance set. variability preserved principal components space accurate soft targets achieved acoustic modeling resulting smallest wer. likewise found optimal value sparse reconstruction. noted low-rank sparse reconstruction optimal amount enhancement needed improving asr. table performance various systems additional untranscribed training data used. system hard-targets based baseline dnn. paranthesis denotes supervised enhancement outputs system fp-n shows forward pass using system less enhancement leads continued presence noise soft targets much results loss essential information. dnn-hmm speech recognition speech recognition using dnns trained soft targets obtained low-rank sparse reconstruction compared table system- baseline hard target based dnn. system built supervised enhancement soft outputs obtained system- training data shown fig. expected training soft targets yields lower baseline hard targets. sparse reconstruction result accurate acoustic modeling sparse reconstruction achieves absolute reduction wer. sparse reconstruction found work better low-rank reconstruction asr. higher accuracy sparse model characterizing non-linear senone subspaces unlike previous works required stages forward pass explicit low-dimensional projection single learned estimates probabilities directly lowdimensional space. training untranscribed data given accurate acoustic model untranscribed input speech data obtain soft targets data forward pass. assuming initial model generalize well unseen data additional soft targets thus generated used augment original training data. propose learn better acoustic models using augmented training set. method reminiscent knowledge transfer approach typically used model compression. work network architecture experiments. dnns trained low-rank sparse soft targets used generate soft targets icsi corpus librispeech sources untranscribed data. table shows interesting observations various experiments using data augmentation. first system- built augmenting enhanced training data icsi soft targets generated system-. consider icsi corpus consisting spontaneous speech meeting recordings in-domain corpus. based successfully exploits information additional icsi data showing significant improvement system- system- observed using sparsity based dnn. next system- built augmenting enhanced data librispeech soft targets obtained system read audio book speech data librispeech out-of-domain compared spontaneous speech ami. still system- achieves similar reductions observed system- built using in-domain icsi data. system built explore could extract even information out-of-domain librispeech data using soft targets system- instead system-. note system- trained using soft targets icsi spontaneous speech data accurate model system indeed system perform better previous systems using surprisingly soft targets obtained sparse reconstruction able exploit unseen data systems. speculate dictionary learning sparse coding captures nonlinearities speciﬁc database. nonlinear characteristics correspond channel recording conditions vary different databases transcended. hand local linearity assumption leads extraction highly restricted basis captures important dynamics senone probability space. regularities mainly address acoustic dependencies among senones generalizable acoustic conditions. hence eigenposteriors invariant exceptional effects channel recording conditions. sparse reconstruction able mitigate undesired effects long seen training data. given superior performance sparse reconstruction posteriors believe sparse modeling might powerful labeled data unseen acoustic conditions made available dictionary learning. noted training additional untranscribed data effective non-enhanced soft targets used. fact systems without low-rank sparse reconstruction perform worse system- although seen training data. presented novel approach improve acoustic model training using low-rank sparse soft targets. sparse coding employed identify senone subspaces enhance senone probabilities low-dimensional reconstruction. lowrank reconstruction using relies existance eigenposteriors capturing local dynamics senone subspaces. although sparse reconstruction proves effective achieve reliable soft targets transcribed data provided low-rank reconstruction found generalizable out-of-domain untranscribed data. trained low-rank reconstruction acheives relative reduction whereas trained using non-enhanced soft targets fails exploit additional information additional data. eigenposteriors better estimated using robust sparse better modeling senone subspaces. furthermore probabilistic maximum likelihood eigen decomposition reduce computational cost large scale applications. study supports probabilistic outputs acoustic modeling. speciﬁcally enhanced soft targets effective training small footprint dnns based model compression. future plan investigate usage cross-lingual knowledge transfer also study domain adaptation based notion eigenposteriors. geoffrey hinton deng dong george dahl abdelrahman mohamed navdeep jaitly andrew senior vincent vanhoucke patrick nguyen tara sainath deep neural networks acoustic modeling speech recognition shared views four research groups ieee signal processing magazine vol. steve young julian odell philip woodland treebased state tying high accuracy acoustic modelling proceedings workshop human language technology. association computational linguistics herve bourlard yochai konig nelson morgan remap recursive estimation maximization posteriori probabilities; application transition-based connectionist speech recognition icsi technical report tr-- simon king frankel karen livescu erik mcdermott korin richmond mirjam wester speech production knowledge automatic speech recognition journal acoustical society america pranay dighe afsaneh asaei herv´e bourlard sparse modeling neural network posterior probabilities exemplar-based speech recognition speech communication pranay dighe luyet afsaneh asaei herve bourlard exploiting low-dimensional structures enhance based acoustic modeling speech recognition ieee icassp luyet pranay dighe afsaneh asaei herv´e bourlard low-rank representation nearest neighbor phone posterior interprobabilities enhance acoustic modeling speech tara sainath brian kingsbury vikas sindhwani ebru arisoy bhuvana ramabhadran low-rank matrix factorization deep neural network training high-dimensional output targets ieee icassp jian kang cheng meng wei-qiang zhang neuron sparseness versus connection sparseness deep neural network large vocabulary speech recognition icassp april brian hutchinson mari ostendorf maryam fazel sparse plus low-rank exponential language model limited resource scenarios ieee transactions audio speech language processing vol. iain mccowan jean carletta kraaij ashby bourban flynn guillemot hain kadlec karaiskos meeting corpus proceedings international conference methods techniques behavioral research vol. gillick larry gillick steven wegmann don’t multiply lightly quantifying problems acoustic model assumptions speech recognition ieee workshop automatic speech recognition understanding jonathon shlens tutorial principal component analysis julien mairal francis bach jean ponce guillermo sapiro online learning matrix factorization sparse coding journal machine learning research vol. adam janin baron jane edwards ellis david gelbart nelson morgan barbara peskin thilo pfau elizabeth shriberg andreas stolcke icsi meeting corpus ieee icassp daniel povey arnab ghoshal gilles boulianne luk´aˇs burget ondˇrej glembek nagendra goel mirko hannemann petr motl´ıˇcek yanmin qian petr schwarz kaldi speech recognition toolkit himawan motlicek imseng potard learning feature mapping using deep neural network bottleneck features distant large vocabulary speech recognition ieee icassp guangcan zhouchen shuicheng yong robust recovery subspace structures low-rank representation ieee transactions pattern analysis machine intelligence pawel swietojanski arnab ghoshal steve renals unsupervised cross-lingual knowledge transfer dnn-based lvcsr ieee spoken language technology workshop", "year": 2016}