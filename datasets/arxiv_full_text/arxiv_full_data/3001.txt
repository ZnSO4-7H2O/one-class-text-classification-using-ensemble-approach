{"title": "Learning to Act by Predicting the Future", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The presented formulation enables learning without a fixed goal at training time, and pursuing dynamically changing goals at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments.", "text": "present approach sensorimotor control immersive environments. approach utilizes high-dimensional sensory stream lower-dimensional measurement stream. cotemporal structure streams provides rich supervisory signal enables training sensorimotor control model interacting environment. model trained using supervised learning techniques without extraneous supervision. learns based sensory input complex three-dimensional environment. presented formulation enables learning without ﬁxed goal training time pursuing dynamically changing goals test time. conduct extensive experiments threedimensional simulations based classical ﬁrst-person game doom. results demonstrate presented approach outperforms sophisticated prior formulations particularly challenging tasks. results also show trained models successfully generalize across environments goals. model trained using presented approach full deathmatch track visual doom competition held previously unseen environments. machine learning problems commonly divided three classes supervised unsupervised reinforcement learning. view supervised learning concerned learning input-output mappings unsupervised learning aims hidden structure data reinforcement learning deals goal-directed behavior reinforcement learning compelling considers natural setting organism acting environment. generally taken comprise class problems mathematical formalization problems family algorithmic approaches reinforcement learning achieved signiﬁcant progress challenges remain. sensorimotor control sensory input complex dynamic threedimensional environments learned directly experience. another acquisition general skills ﬂexibly deployed accomplish multitude dynamically speciﬁed goals work propose approach sensorimotor control aims assist progress towards overcoming challenges. approach departs reward-based formalization commonly used instead monolithic state scalar reward consider stream sensory input {st} stream measurements {mt}. sensory stream typically high-dimensional include visual auditory tactile input. measurement stream lower dimensionality constitutes data pertain agent’s current state. physical system measurements include attitude supply levels structural integrity. three-dimensional computer game include health ammunition levels number adversaries overcome. guiding observation interlocked temporal structure sensory measurement streams provides rich supervisory signal. given present sensory input measurements goal agent trained predict effect different actions future measurements. assuming goal expressed terms future measurements predicting provides information necessary support action. reduces sensorimotor control supervised learning supporting learning experience without extraneous data. supervision proapproach signiﬁcant beneﬁts. first contrast occasional scalar reward assumed traditional measurement stream provides rich temporally dense supervision stabilize accelerate training. sparse scalar reward feedback available board game multidimensional stream sensations appropriate model organism learning function immersive environment second advantage presented formulation supports training without ﬁxed goal pursuing dynamically speciﬁed goals test time. assuming goal expressed terms future measurements model trained take goal account prediction future. test time agent predict future measurements given current sensory input measurements goal simply select action best suits present goal. evaluate presented approach immersive three-dimensional simulations require visually navigating complex three-dimensional environment recognizing objects interacting dynamic adversaries. classical ﬁrst-person game doom introduced immersive three-dimensional games popular culture presented approach given visual input statistics shown player game health ammunition levels. human gameplay used model trains experience. experimental results demonstrate presented approach outperforms state-of-the-art deep models particularly complex tasks. experiments demonstrate models learned presented approach generalize across environments goals vectorial measurements instead scalar reward beneﬁcial. model trained presented approach full deathmatch track visual doom competition took place previously unseen environments. presented approach outperformed second best submission employed substantially complex model additional supervision training supervised learning perspective learning interacting environment dates back decades. jordan rumelhart analyze approach review early work argue choice versus guided characteristics environment. analysis suggests efﬁcient environment provides sparse scalar reward signal whereas advantageous temporally dense multidimensional feedback available. sutton analyzed temporal-difference learning argued preferable prediction problems correctness prediction revealed many steps prediction made. sutton’s inﬂuential analysis assumes sparse scalar reward. policy gradient methods since come dominate study sensorimotor learning natural imitation learning conjunction model-based formulation sensorimotor learning experience supervised learning rare work suggests learner exposed dense multidimensional sensory feedback direct future prediction support effective sensorimotor coordination complex dynamic environments. approach similarities monte carlo methods. convergence methods analyzed early seen theoretically advantageous particularly function approximators used choice learning monte carlo methods argued practical grounds based empirical performance canonical examples understanding convergence types methods since improved argument versus monte carlo empirical sharp negative examples exist work deals general setting vectorial feedback parameterized goals shows simple monte-carlo-type method performs extremely well compelling instantiation setting. vector-valued feedback considered context multi-objective decision-making transfer across related tasks analyzed konidaris parameterized goals studied context continuous motor skills throwing darts target general framework sharing value function approximators across states goals described schaul work closely related framework schaul presents speciﬁc formulation goals deﬁned terms intrinsic measurements control based direct future prediction. provide architecture handles realistic sensory measurement streams achieves state-of-the-art performance complex dynamic three-dimensional environments. learning simulated environments focus signiﬁcant attention following successful application deep atari games mnih number recent efforts applied related ideas three-dimensional environments. lillicrap considered continuous high-dimensional action spaces learned control policies torcs simulator. mnih described asynchronous variants deep methods demonstrated navigation three-dimensional labyrinth. augmented deep q-networks external memory evaluated performance tasks minecraft. recent technical report kulkarni proposed end-to-end training successor representations demonstrated navigation doom-based environment. another recent report blundell considered nonparametric approach control conducted experiments three-dimensional labyrinth. experiments reported section demonstrate approach signiﬁcantly outperforms state-ofthe-art deep methods. prediction future states dynamical systems considered littman singh predictive representations form generalized value functions advocated sutton recently learned predict future frames atari games. prediction full sensory input realistic three-dimensional environments remains open challenge although signiﬁcant progress made work considers prediction future values meaningful measurements rich sensory input shows prediction supports effective sensorimotor control. consider agent interacts environment discrete time steps. time step agent receives observation executes action based observation. assume observations following structure sensory input measurements. experiments image agent’s view threedimensional environment. generally include input multiple sensory modalities. measurements indicate attitude supply levels structural integrity physical system health ammunition score computer game. distinction sensory input measurements somewhat artiﬁcial constitute sensory input different forms. model measurement vector distinguished sensations ways. first measurement vector part observation agent predict. present predicting full sensory streams beyond capabilities oord impressive recent progress). therefore designate subset sensations measurements predicted. second assume agent’s goals deﬁned terms future measurements. speciﬁcally temporal offsets mt+τ mt+τn corresponding differences future present measurements. assume goal agent pursue deﬁned maximization function parametric function used. experiments goals expressed linear combinations future measurements vector parameterizes goal dimensionality model generalizes standard reinforcement learning formulation scalar reward signal viewed measurement exponential decay possible conﬁguration goal vector. action learned parameters resulting prediction. matches dimensionality note prediction function dimensionality current observation considered action goal. test time given learned parameters agent choose action yields best predicted outcome predictor trained experiences collected agent. starting random policy agent begins interact environment. interaction takes place episodes last ﬁxed number time steps terminal event occurs. consider experiences collected agent yielding training examples input output example pre= fi}n dictor trained using regression loss classiﬁcation loss used predicting categorical measurements necessary experiments. agent collects experiences training predictor used agent change. maintain experience memory recent experiences mini-batch examples randomly sampled every iteration solver. parameters predictor used agent updated every experiences. setup departs pure onpolicy training observed adverse effect using small experience memory. additional details provided appendix regimes agent follows ε-greedy policy acts greedily according current goal probability selects random action probability value initially decreased training according ﬁxed schedule. predictor deep network parameterized network architecture shown figure network three input modules perception module measurement module goal module experiments image perception module implemented convolutional network. measurement goal modules fully-connected networks. outputs three input modules concatenated forming joint input representation used subsequent processing future measurements predicted based input representation. network emits predictions future measurements actions once. could done fully-connected module absorbs input representation outputs predictions. however found introducing additional structure prediction module enhances ability learn differences between outcomes different actions. build ideas wang figure network structure. image measurements goal ﬁrst processed separately three input modules. outputs modules concatenated joint representation joint representation processed parallel streams predict expected measurements normalized action-conditional differences {ai} combined produce ﬁnal prediction action. split prediction module streams expectation stream action stream expectation stream predicts average future measurements potential actions. number actions. normalization layer action stream ensures average predictions action stream zero future measurement normalization layer subtracts average actions prediction forcing expectation stream compensate predicting average values. output expectation stream dimensionality vector future measurements. output action stream dimensionality dim. output network prediction future measurements action composed summing output expectation stream normalized action-conditional output action stream evaluate presented approach immersive three-dimensional simulations based classical game doom. simulations agent ﬁrst-person view environment must based visual information shown human players game. interface game engine vizdoom platform developed kempka advantages platform allows running simulation thousands frames second single core enables training models tens millions simulation steps single day. compare presented approach state-of-the-art deep methods four scenarios increasing difﬁculty study generalization across environments goals evaluate importance different aspects model. gathering health kits square room. gathering health kits avoiding poison vials maze. defending adversaries gathering health ammunition maze. defending adversaries gathering health ammunition compliﬁrst scenarios provided vizdoom platform. agent square room health declining constant rate. survive must move around collect health kits distributed abundantly room. task easy long agent learns avoid walls keep traversing room performance good. agent maze health declining constant rate. must collect health kits increase health must also avoid blue poison vials decrease health. task harder agent must learn traverse irregularly shaped passageways distinguish health kits poison vials. tasks agent access three binary sub-actions move forward turn left turn right. combination three used given time resulting possible actions. measurement provided agent scenarios health. last scenarios challenging designed using elements vizdoom platform. agent armed attack alien monsters. monsters spawn abundantly move around environment shoot ﬁreballs agent. health kits ammunition sporadically distributed throughout environment collected agent. environment simple maze complex scenarios agent access eight sub-actions move forward move backward turn left turn right strafe left strafe right shoot. combination sub-actions used resulting model. future predictor network used experiments conﬁgured close possible model mnih ensure fair comparison. additional details architecture provided appendix training testing. agent trained tested episodes. episode terminates steps agent’s health drops zero. statistics reported ﬁgures tables summarize ﬁnal values respective measurements episodes. temporal offsets predicted future measurements steps experiments. latest three time steps contribute objective function coefﬁcients details provided appendix comparison prior work. compared presented approach three deep methods standard baseline visuomotor control impressive performance atari games. recent commonly regarded state area. described recent technical report included authors also used vizdoom platform experiments albeit simple task. details setup prior approaches provided appendix performance different approaches training shown figure reporting results experiments refer approach ﬁrst scenarios approaches trained maximize health. scenarios figure reports average health episode course training. last scenarios approaches trained maximize linear combination three normalized measurements coefﬁcients scenarios figure reports average frags episode. presented curve averages information three independent training runs data point computed steps testing. trained million steps. training procedure much slower process roughly million simulation steps day. reason able evaluate basic scenario able perform extensive hyperparameter tuning. report results technique days training. sufﬁcient approach number steps afforded approaches.) table reports performance models training. fully trained model tested million simulation steps. table reports average health episode scenarios average frags episode also report average training speed approach millions simulation steps training. performance different models additionally illustrated supplementary video figure performance different approaches training. achieve similar performance basic scenario. outperforms prior approaches three scenarios multiplicative performance complex ones basic scenario perform well. reported table performance virtually identical reaches complex navigation scenario signiﬁcant opens consistent experiments mnih achieves best performance scenario percentage point advantage testing. note ﬁrst scenarios given single measurement time step complex battle battle scenarios dominates approaches. outperforms test time factor factor note advantage particularly signiﬁcant scenarios provide richer measurements three measurements time step effect multiple measurements evaluated controlled experiments reported below. generalization across environments. evaluate behaviors learned presented approach generalize across different environments. created randomly textured versions mazes scenarios used training testing disjoint sets textures training testing environments. call scenarios d-tx d-tx. table shows performance approach different combinations training testing regimes. example entry d-tx column shows performance model trained tested d-tx. surprisingly model trained simple environment learn sufﬁcient invariance surface appearance generalize well environments. training complex multitexture environment yields better generalization trained model performs well exhibits non-trivial performance d-tx d-tx. finally exposing model signiﬁcant variation surface appearance d-tx d-tx training yields good generalization. last column table additionally reports performance higher-capacity model trained d-tx. combination referred d-tx-l. shown table model performs even better. architecture detailed appendix visual doom competition. evaluate presented approach participated visual doom competition held september competition evaluated sensorimotor control models based visual input. competition form tournament submitted agents play multiple games other performance measured aggregate frags. competition included tracks. limited deathmatch track held known environment given participants advance training time. full deathmatch track evaluated generalization previously unseen environments took place multiple environments available participating teams training time. enrolled full deathmatch track. model trained using variant d-tx-l regime. model outperforming second best submission submission described lample chaplot constitutes strong baseline. deep recurrent q-network incorporates lstm trained using reward shaping extra supervision game engine. speciﬁcally authors took advantage ability provided vizdoom platform internal conﬁguration game including ground-truth knowledge presence enemies ﬁeld view training. authors’ report shows additional supervision improved performance signiﬁcantly. model simpler achieved even higher performance without additional supervision. goal-agnostic training. evaluate ability presented approach learn without ﬁxed goal training time adapt varying goals test time. experiments performed battle scenario. three training regimes ﬁxed goal vector training random goal vector value sampled uniformly every episode random goal vector value sampled uniformly every episode. details provided appendix intuitively second regime agent instructed maximize different measurements knowledge relative importance. third regime makes assumptions whether measured quantities desirable not. results shown table group columns corresponds training regime different test-time goal. goals given weights three measurements objective function. ﬁrst test-time goal table goal vector used battle scenarios prior experiments second seeks maximize frag count third paciﬁst fourth seeks aimlessly drain ammunition ﬁfth aims maximize health. group columns reports average value three measurements episode. note health level episode negative agent suffered major damage pre-terminal step. draw main conclusions. first main task models trained without knowing goal advance perform nearly well dedicated model trained speciﬁcally eventual goal without knowing eventual goal training agent performs task almost well speciﬁcally trained second models generalize goals equally well. models trained variety goals generalize much better model trained ﬁxed goal. table generalization across goals. group three columns corresponds training regime corresponds test-time goal. results ﬁrst indicate approach performs well main task even without knowing goal training time. results rows indicate goal-agnostic training supports generalization across goals test time. ablation study. perform ablation study using d-tx scenario. speciﬁcally evaluate importance vectorial feedback versus scalar reward effect predicting measurements multiple temporal offsets. results summarized table table reports performance full model ablated variants predict frags and/or predict farthest temporal offset. results demonstrate predicting multiple measurements signiﬁcantly improves performance learned model even evaluated measurements. predicting measurements multiple future times also beneﬁcial. supports intuition dense multivariate measurements better training signal scalar reward. presented approach sensorimotor control immersive environments. approach simple demonstrates supervised learning techniques adapted learning complex dynamic three-dimensional environments given sensory input intrinsic measurements. model trains experience interacting environment without extraneous supervision. natural supervision provided cotemporal structure sensory measurement streams. experiments demonstrated simple approach outperforms sophisticated deep reinforcement learning formulations challenging tasks immersive environments. experiments demonstrated multivariate measurements provides signiﬁcant advantage conventional scalar rewards trained model effectively pursue goals speciﬁed training. presented work extended multiple ways important broadening range behaviors learned. first presented model purely reactive acts based current frame only explicit facilities memory test-time retention internal representations. recent work explored memory-based models integrating ideas presented approach yield substantial advances. second signiﬁcant progress behavioral sophistication likely require temporal abstraction hierarchical organization learned skills third presented model developed discrete action spaces; applying presented ideas continuous actions would interesting finally predicting features learned directly rich sensory input blur distinction sensory measurement streams michał kempka marek wydmuch grzegorz runc jakub toczek wojciech ja´skowski. vizdoom doom-based research platform visual reinforcement learning. ieee conference computational intelligence games volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski stig petersen charles beattie amir sadik human-level control deep reinforcement learning. nature volodymyr mnih adri`a puigdom`enech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. icml st´ephane ross narek melik-barkhudarov kumar shaurya shankar andreas wendel debadeepta andrew bagnell martial hebert. learning monocular reactive control cluttered natural environments. icra david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot sander dieleman dominik grewe mastering game deep neural networks tree search. nature richard sutton joseph modayil michael delp thomas degris patrick pilarski adam white doina precup. horde scalable real-time architecture learning knowledge unsupervised sensorimotor interaction. aamas a¨aron oord sander dieleman heiga karen simonyan oriol vinyals alex graves kalchbrenner andrew senior koray kavukcuoglu. wavenet generative model audio. arxiv. detailed architectures network variants basic large shown tables basic network follows architecture mnih closely possible. large network similar layers starting third wider factor two. networks leaky relu nonlinearity lrelu nonterminal layer. initialize weights proposed results reported table modiﬁcations basic architecture hurt performance showing two-stream formulation beneﬁcial providing current measurements network increases performance crucial. sensory input agent observed image grayscale without additional preprocessing. resolution pixels basic model pixels large one. normalized measurements standard deviations random exploration. precisely divided ammo count health level frag count respectively. performed frame skipping training testing. agent observes environment selects action every frame. selected action repeated skipped frames. accelerates training without sacriﬁcing accuracy. paper step always refers steps frame skipping played human doom runs frames second step agent equivalent milliseconds real time. therefore frame skipping added beneﬁt bringing reaction time agent closer human. temporal offsets predicted future measurements steps experiments. longest temporal offset corresponds seconds real time. experiments latest three predictions contributed objective function ﬁxed coefﬁcients therefore scenarios multiple measurements available agent goal vector speciﬁed three numbers relative weights three measurements objective function. goal-directed training ﬁxed goal-agnostic training sampled uniformly random used experience memory steps sampled mini-batch samples every experiences added. added experiences memory using copies agent running parallel. networks experiments trained using adam algorithm initial learning rate gradually decreased training. basic networks trained mini-batch iterations large iterations. compared approach three prior methods used authors’ implementations independent implementation scenarios used change health reward. used linear combination changes three normalized measurements coefﬁcients presented approach tested three learning rates default alternatives hyperparameters left default values. trains faster performed search learning rates ﬁrst tasks; last tasks trained models random learning rates sampled log-uniformly random sampled log-uniformly baselines report best results able obtain.", "year": 2016}