{"title": "Rethinking Skip-thought: A Neighborhood based Approach", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "We study the skip-thought model with neighborhood information as weak supervision. More specifically, we propose a skip-thought neighbor model to consider the adjacent sentences as a neighborhood. We train our skip-thought neighbor model on a large corpus with continuous sentences, and then evaluate the trained model on 7 tasks, which include semantic relatedness, paraphrase detection, and classification benchmarks. Both quantitative comparison and qualitative investigation are conducted. We empirically show that, our skip-thought neighbor model performs as well as the skip-thought model on evaluation tasks. In addition, we found that, incorporating an autoencoder path in our model didn't aid our model to perform better, while it hurts the performance of the skip-thought model.", "text": "study skip-thought model proposed kiros neighborhood information weak supervision. speciﬁcally propose skip-thought neighbor model consider adjacent sentences neighborhood. train skip-thought neighbor model large corpus continuous sentences evaluate trained model tasks include semantic relatedness paraphrase detection classiﬁcation benchmarks. quantitative comparison qualitative investigation conducted. empirically show that skip-thought neighbor model performs well skip-thought model evaluation tasks. addition found that incorporating autoencoder path model didn’t model perform better hurts performance skip-thought model. interested learning distributed sentence representation unsupervised fashion. previously skip-thought model introduced kiros learns explore semantic continuity within adjacent sentences supervision learning generic sentence encoder. skip-thought model encodes current sentence decodes previous sentence next instead itself. independent decoders applied since intuitively previous sentence next sentence drawn different conditional distributions respectively. tion learning sentence representation ﬁrst drop decoders decoder reconstruct surrounding sentences time. empirical results show skip-thought neighbor model performs well skip-thought model evaluation tasks. then inspired hill tested effect incorporating autoencoder branch proposed fastsent model also conduct experiments explore reconstructing input sentence well skip-thought neighbor model skip-thought model. results tell model didn’t beneﬁt autoencoder path reconstructing input sentence hurts performance skip-thought model. furthermore conduct interesting experiment decoding next sentence without previous sentence gave best results among models. model details discussed section distributional sentence representation learning involves learning word representations compositionality words within given sentence. previously mikolov proposed method distributed representation learning words predicting surrounding words empirically showed additive composition learned word representations successfully captures contextual information phrases sentences. similarly mikolov proposed method learns ﬁxed-dimension vector sentence predicting words within given sentence. however training representation sentence hard derive since requires optimizing sentence representation towards objective. representation learning proposed model combines lstm encoder lstm decoder learn language representation unsupervised fashion supervised evaluation datasets ﬁnetunes lstm encoder supervised tasks datasets. successfully show learning word representation compositionality words could done time end-to-end machine learning system. since rnn-based encoder processes input sentence word order obvious dependency representation starting words decrease encoder processes words. modiﬁed plain lstm network tree-structured lstm network helps model address long-term dependency problem. modifying network structure additional supervision could also help. bowman proposed model learns parse sentence time processing input sentence. proposed model supervision comes objective function supervised tasks parsed sentences means training sentences need parsed prior training. methods require additional preprocessing training data could slow need deal large corpus. instead learning compose sentence representation word representations skip-thought model kiros utilizes structure relationship adjacent sentences large unlabelled corpus. inspired skip-gram model sentence-level distributional hypothesis skip-thought model encodes current sentence ﬁxed-dimension vector instead predicting input sentence itself decoders predict previous sentence next sentence independently. skip-thought model provides alternative unsupervised sentence representation learning shown great success. learned sentence representation encoder outperforms previous unsupervised pretrained models evaluation tasks ﬁnetuning results comparable supervised trained models. triantaﬁllou ﬁnetuned skip-thought models stanford natural language inference proposed fastsent model takes summation word representations compose sentence representation predicts words previous sentence next sentence. results semantic related task comparable rnn-based skip-thought model skip-thought model still outperforms fastsent model classiﬁcation tasks. later siamese cbow aimed learn word representations make cosine similarity adjacent sentences representation space larger sentences adjacent. section present skip-thought neighbor model. ﬁrst brieﬂy introduce skipthought model discuss explicitly modify decoders skip-thought model skip-thought neighbor model. skip-thought model given sentence tuple encoder computes ﬁxed-dimension vector representation sentence learns distribution stands parameters encoder. then conditioned representation separate decoders applied reconstruct previous sentence next sentence respectively. call previous decoder next decoder denotes parameters decoder. since conditional distributions learned decoders parameterized independently implicitly utilize sentence order information within sentence tuple. intuitively given current sentence inferring previous sentence considered different inferring next sentence si+. figure skip-thought neighbor model. shared parameters indicated colors. blue arrow represents dependency representation produced encoder. given sentence model tries reconstruct neighbors si+. skip-thought neighbor model hypothesis that even without order information within given sentence tuple skipthought model behave similarly terms reconstruction error perform similarly evaluation tasks. modify skipthought model given assume inferring inferring si+. deﬁne {si− si+} neighbors inferring process denoted neighborhood conditional distribution learned decoder parameterized experiments directly drop decoders decoder reconstruct previous sentence next sentence time given representation skip-thought neighbor model considered sharing parameters previous decoder next decoder original skip-thought model. illustration shown figure skip-thought neighbor autoencoder previously deﬁned {si− si+} neighbors addition assume could also neighbor itself. therefore neighborhood becomes {si− si+}. inorder make comparison fair choose recurrent neural network gated recurrent unit recurrent unit used kiros since comparison among different recurrent units main focus decided fast stable recurrent unit. addition chung shows that language modeling tasks performs well long short-term memory suppose sentence contains words arbitrary time step reencoder produces hidden state gard representation previous subsequence time time hidden state represents given sentence computation experiments shown make comparison fair reimplemented skip-thought model settings according kiros publicly available theano code. adopted multi-gpu trainferring neighborhood involves adding autoencoder path skip-thought neighbor model. experiments decoder model required reconstruct three sentences {si− si+} neighborhood time. objective function becomes previously hill tested adding autoencoder path fastsent model. results show that additional autoencoder path performance classiﬁcation tasks slightly improved signiﬁcant performance gain loss semantic relatedness task. skip-thought neighbor target skip-thought neighbor model given sentence decoder needs reconstruct sentences neighborhood {si− si+} targets. denote inference process {si− si+}. next sentence adam algorithm optimization. instead applying gradient clipping according norm gradient used kiros directly gradient make within stable training. compared proposed skip-thought neighbor model skip-thought model evaluation tasks include semantic relatedness paraphrase detection question-type classiﬁcation benchmark sentiment subjective datasets. unsupervised training bookcorpus dataset parameters encoder apply sentence representation extractor tasks. semantic relatedness sick dataset adopt feature engineering idea proposed given sentence pair encoder computes pair representations denoted concatenation component-wise product absolute difference regarded feature given sentence pair. train logistic regression feature predict semantic relatedness score. evaluation metrics pearsons spearmans mean squared error dataset paraphrase detection microsoft paraphrase detection corpus follow feature engineering idea compute single feature sentence pair. train logistic regression -fold cross validation applied optimal hyperparameter settings. results shown table model name given encoder type model type model size. tried three different types encoder denoted uni- combinetable ﬁrst uni-directional computes -dimension vector sentence representation. second bi-directional computes dimension vector direction vectors concatenated serve sentence representation. third training unidirectional model bi-directional model representation models concatenated together represent sentence denoted combine-. table -nrefers skip-thought neighbor model -n-nextrefers skip-thought neighbor predicting next sentence -skiprefers original skip-thought model. skip-thought neighbor skip-thought results show table tell skip-thought neighbor models perform well skip-thought models fewer parameters means neighborhood information effective terms helping model capture sentential contextual information. skip-thought neighbor model incorporating antoencoder means that besides reconstructing neighbors decoder also needs reconstruct skipthought model since implicit hypothesis model different decoders learn different conditional distributions another decoder skip-thought model reconstruct sentence results also shown table skip-thought neighbor+ae models outperform skip-thought+ae models signiﬁcantly. speciﬁcally skip-thought model adding autoencoder branch hurts performance sick subj mpqa dataset. reconstruction error autoencoder branch decreases drastically during training reconstruction errors previous decoder next decoder ﬂuctuates widely larger model without autoencoder branch. seems that autoencoder branch hurt skip-thought model capture sentential contextual information surrounding sentences. could vary weights three independent branches better results main focus paper. skip-thought neighbor model inclusion autoencoder constraint problem. autoencoder branch model gets lower errors reconstructing three sentences. however doesn’t help model perform better evaluation tasks. also explored adding neighbors skip-thought neighbor model. besides using decoder predict previous sentence next sentence expand neighborhood contain sentences previous sentences next sentences. case decoder required reconstruct sentences time. experiments model evaluated trained encoder tasks. signiﬁcant performance gain loss model trained neighbors; seems that increasing number neighbors doesn’t improve performance also doesn’t hurt performance. hypothesis that reconstructing four different sentences neighborhood parameters hard task might distract model capturing sentential contextual information. compared skip-thought model skipthought neighbor model target contains fewer parameters runs faster training since given sentence model needs reconstruct next sentence skipthought model needs reconstruct surrounding sentences. third section table presents results model target. surprisingly overall performs well skip-thought models previous models. interesting observation found investigating publicly available code kiros training representation produced encoder directly sent decoders however training output encoder normalized keep l-norm sentence representation normalized vector. conducted experiments effect normalization evaluation evaluated skip-thought neighbor model implemented skip-thought model. generally normalization step slightly hurts performance semantic relatedness sick task improves performance across classiﬁcation tasks. table presents results normalization step table presents results without normalization sick dataset. wish better answer question wish knew answer wish answer kept eyes shadowed road watching every step kept eyes feet ahead trail kept walking examining could corner world prepared hours unreal silence made seem magical really world changed single moment time ﬁlled hour thought everything magical describing world words could ignore phone buzzed awoke trance ﬂipped phone shut drifted sleep grabbed phone groggy eyes shut alarm threw took shoes kicked shoes fell without bothering remove shoes table section ﬁrst sentence query second nearest neighbor retrieved database third nearest neighbor. similarity every sentence pair measure cosine similarity representation space. mentioned normalization improves performance model distance measure applied sentence retrieval experiment cosine distance. retrieved sentences look semantically related viewed sentential contextual extension query sentences. several samples found table since models trained minimizing reconstruction error across whole training corpus reasonable analyze behavior decoder conditional sentence generation. ﬁrst randomly pick sentences training corpus compute representation them. then greedily decode representations sentences. table presents generated sentences. several interesting observations worth mentioning here. erated sentence targets lead doubt decoder able generate least grammatically-correct english sentences. results shows generated sentences grammatically-correct generally meaningful. also observe that generated sentences tend similar starting words usually negative expression etc. investigating training corpus noticed observation caused dataset bias. majority training sentences start high chance negation comes addition generated sentences rarely sentential contextual extension associated input sentences skipthought models. investigations needed conditional sentence generation. proposed hypothesis neighborhood information effective learning sentence representation empirically tested hypothesis. skip-thought neighbor models trained unsupervised fashion evaluated tasks. results showed models perform well skip-thought models. furthermore model target performs better skip-thought model. future work could explore skip-thought neighbor model target progratefully thank jeffrey elman benjamin bergen seana coulson marta kutas insightful discussion thank thomas andy keller thomas donoghue larry muhlstein reina mizrahi suggestive chatting. also thank adobe research gpus support thank nvidia dgx- trial well support references samuel bowman gabor angeli christopher potts christopher manning. large annotated corpus learning natural language inference. emnlp. samuel bowman gauthier abhinav rastogi raghav gupta christopher manning christopher potts. fast uniﬁed model parsing sentence understanding. acl. kyunghyun bart merrienboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. emnlp. junyoung chung caglar gulcehre kyunghyun yoshua bengio. empirical evaluation gated recurrent neural networks sequence modeling. arxiv preprint arxiv. william dolan chris quirk chris brockett. unsupervised construction large paraphrase corpora exploiting massively parallel news sources. coling. marco marelli stefano menini marco baroni luisa bentivogli raffaella bernardi roberto zamparelli. sick cure evaluation compositional distributional semantic models. lrec. yukun ryan kiros richard zemel ruslan salakhutdinov raquel urtasun antonio torralba sanja fidler. aligning books movies towards story-like visual explanations watching movies reading books. iccv pages", "year": 2017}