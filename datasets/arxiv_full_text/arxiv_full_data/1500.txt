{"title": "Co-attending Free-form Regions and Detections with Multi-modal  Multiplicative Feature Embedding for Visual Question Answering", "tag": ["cs.CV", "cs.AI", "cs.CL"], "abstract": "Recently, the Visual Question Answering (VQA) task has gained increasing attention in artificial intelligence. Existing VQA methods mainly adopt the visual attention mechanism to associate the input question with corresponding image regions for effective question answering. The free-form region based and the detection-based visual attention mechanisms are mostly investigated, with the former ones attending free-form image regions and the latter ones attending pre-specified detection-box regions. We argue that the two attention mechanisms are able to provide complementary information and should be effectively integrated to better solve the VQA problem. In this paper, we propose a novel deep neural network for VQA that integrates both attention mechanisms. Our proposed framework effectively fuses features from free-form image regions, detection boxes, and question representations via a multi-modal multiplicative feature embedding scheme to jointly attend question-related free-form image regions and detection boxes for more accurate question answering. The proposed method is extensively evaluated on two publicly available datasets, COCO-QA and VQA, and outperforms state-of-the-art approaches. Source code is available at https://github.com/lupantech/dual-mfa-vqa.", "text": "figure co-attending free-form regions detection boxes based question whole image detection boxes better utilizing complementary information solve task. state-of-the-art approaches utilize visual attention mechanism relate question meaningful image regions accurate question answering. visual attention mechanisms categorized free-form region based methods detection-based methods free-form region based methods question features learned long short-term memory network image features learned convolutional neural network fused either addictive multiplicative concatenation operations every image spatial location. free-form attention obtained applying softmax non-linearity operation across fused feature map. since restriction obtained attention free-form attention region able attend global visual context speciﬁc foreground objects inferring answers. however since restriction free-form attentive regions might focus partial objects irrelevant context sometimes. instance question what animal see? free-form region attention might mistakenly focus part foreground generate answer dog. hand detection-based attention methods attention mechanism utilized relating question pre-speciﬁed detection boxes task gained increasing attention artiﬁcial intelligence. existing methods mainly adopt visual attention mechanism associate input question corresponding image regions effective question answering. freeform region based detection-based visual attention mechanisms mostly investigated former ones attending free-form image regions latter ones attending pre-speciﬁed detection-box regions. argue attention mechanisms able provide complementary information effectively integrated better solve problem. paper propose novel deep neural network integrates attention mechanisms. proposed framework effectively fuses features free-form image regions detection boxes question representations multi-modal multiplicative feature embedding scheme jointly attend question-related free-form image regions detection boxes accurate question answering. proposed method extensively evaluated publicly available datasets coco-qa outperforms state-of-the-art approaches. source code available https//github. com/lupantech/dual-mfa-vqa. recent years multi-modal learning language vision gained much attention artiﬁcial intelligence. great progress achieved different tasks including image captioning visual question generation video question answering text-to-image retrieval visual question answering task recently emerged challenging task. algorithms required answer natural language questions given image’s contents. compared conventional visual-language tasks image captioning text-to-image retrieval task requires algorithms better understanding input image question order infer answer. doll´ar instead applying softmax operation image spatial locations operation calculated detection boxes. therefore attended regions restricted pre-speciﬁed detection-box regions question-related regions could effective answering questions foreground objects. however restrictions also cause challenges types questions. instance question weather today? might exist detection resulting failure answering question. better understanding question image contents relations good algorithm needs identify global scene attributes locate objects identify object attributes quantity categories make accurate inference. argue mentioned types attended mechanisms provide complementary information effectively integrated uniﬁed framework take advantages attended free-form regions attended detection regions. take mentioned questions examples question animal could effectively answered detection-based attention maps question weather better answered free-form region based attention maps. paper propose novel dual-branch deep neural network solving problem combines free-form region based detection-based attention mechanisms overall framework consists attention branches associates question relevant free-form regions relevant detection regions input image. obtaining question-related attention weights types regions propose learn joint feature representation input question whole image detection boxes multiplicative feature embedding scheme. multiplicative scheme share parameters branches shown result robust answering performance existing methods. twofold. propose novel dual-branch deep neural network effectively integrates free-form region based detection-based attention mechanisms uniﬁed framework; order better fuse features different modalities novel multiplicative feature embedding scheme proposed learn joint feature representations question whole image detection boxes. state-of-the-art algorithms mainly based deep neural networks learning visual-question features predicting ﬁnal answers. establishment dataset online evaluation platform increasing number algorithms proposed every year. networks solve task multi-class classiﬁcation problem. proposed simple baseline learns image features question features lstm concatenated features predict answer. instead using lstm learning question representations used trained question embedding. different mentioned methods addressing task classiﬁcation problem work image features question representations lstm generate answer sequence-tosequence learning. also high-order approaches multi-modal feature embedding. designed bilinear pooling approaches learn multi-modal feature embedding vqa. generalized multimodal pooling framework proposed shows special cases. inspired resnet proposed element-wise multiplication joint residual mappings. attention mechanism vqa. large quantity recent works focused incorporating attention mechanisms solving task. attention weights different image regions calculated based semantic similarity question image regions updated question features obtained weighted different regions. proposed multi-stage attention framework stacks attention modules search question-related image regions iterative feature fusion. attention mechanism limited image modality proposed co-attention mechanism simultaneously attends question image joint visual-question feature representations. since questions related objects images object detection results explored replace visual features obtained whole-image region. utilized attention mechanism generate visual features detection boxes task. similarly proposed framework updates joint visual-question feature embedding iteratively attending detection boxes. however attention-based methods focus type image regions question-image association limitations solving certain types questions. contrast order better utilize complementary information types image regions proposed approach effectively integrates attention mechanisms uniﬁed framework. overall structure proposed deep neural network illustrated figure takes question whole image detection boxes inputs learns associate questions free-form image regions detection boxes simultaneously infer answer. proposed model consists co-attention branches figure illustration overall network structure solving task. network attention branches proposed multiplicative featucre embedding scheme branch attends free-form image regions another branch attends detection boxes encoding question-related visual features. free-form image regions detection boxes respectively. calculating attention weights type regions joint feature representations question whole-image visual features detection visual features obtained multiplicative approach. attention weights type regions learned three inputs. importantly joint multimodal feature embedding different parameters branch leads better answering accuracy. resulting visual features branches fused question representation added obtain ﬁnal question-image embedding. multi-class linear classiﬁer adopted obtaining ﬁnal predicted answer. input feature encoding introduced section section introduces branch visual attention mechanism associating free-form image regions input questions multiplicative feature embedding scheme section describes another branch detection based attention mechanism. ﬁnal answer prediction introduced section encoding whole-image features. utilize imagenet pre-trained resnet- learning image visual features. input image resnet resized output feature last convolution layer used encode whole-image visual features spatial size number visual feature channels. encoding detection-box features. adopt fasterrcnn framework obtain object detection boxes input image. object proposals associated detection scores generated fasterrcnn non-maximum suppression intersection union applied top-ranked detection boxes chosen detection-box features overall framework. -dimensional visual features faster-rcnn’s layer concatenated boxes’ detection scores utilized encode visual feature encoding question features. gated recurrent unit adopted encode question features show effectiveness recent methods cell consists update gate reset gate given question one-hot vector position length question. convert word feature vector linear transformation weqt. time step word feature vector sequentially encode input question. step updates update gate reset gate outputs hidden state update process operates represents sigmoid activation function. weight matrices bias vector learnable parameters gru. take ﬁnal hidden state question embedding i.e. denotes embedding length. following utilize pre-trained skip-thought model initialize embedding matrix question language model. since skip-thought model previously trained large text corpus able transfer external languagebased knowledge task ﬁne-tuning initial point. proposed framework branches attending question-related free-form image regions learn wholeimage features attending detection-box regions learn question-related detection features. attention branch takes question feature whole-image feature detection-box feature inputs outputs question-attended visual features question answering. free-form region attention branch tries associate input question relevant regions input image. restriction attended regions free-form able capture global visual context attributes image. unlike existing methods fuse question image features simple concatenation addition guide calculation attention weights attention branch fuses three types input features multiplicative feature embedding scheme utilizing full information inputs. network structure attending visual features multiplicative feature embedding scheme show figure given question embedding whole-image representation detection representation ﬁrst embed dimensional common space following equations learnable weight parameters bias parameters represents vector tanh hyperbolic tangent function. learning attention weights whole-image feature transformed detection features averaged across detection boxes following feature fusion. mapping input features -dimensional common space detection feature question feature spatially replicated grid form match spatial size whole-image feature r××. indicates element-wise multiplication. freeform attention obtained convolving joint feature representation convolution followed softmax operation grid softmax second branch focuses learning question-related detection features attending detection-box features joint input feature representation. similar ﬁrst branch fuse question representation whole-image features detection-box features learning attention weights detection boxes. unlike previous detection based attention methods proposed attention mechanism integrates whole-image features better understanding overall image contents. structure learning joint feature representation shown figure similar joint representation free-form image region attention given question embedding image representation detection representation transform representation common semantic space obtain dimensional joint feature representation learnable parameters linear transformation. transformed question feature whole-image feature replicated across number dimension match dimension transformed detection features calculating joint representation following learning answer prediction similar existing approaches model multi-class classiﬁcation problem. given attended whole-image feature attended detection feature input question feature question-image joint encodings obtained element-wise multiplication transformed question features attended features branch learnable parameters transforming input question feature reason choose different question transformation parameters branches attended visual features different branches captures different information input image. attended free-form region features able capture global context attributes scene. another attended detection features able extract information foreground objects. merging question-image encodings branches addition linear classiﬁer trained ﬁnal answer prediction dataset based microsoft coco image data dataset consists training questions validation questions testing questions generated total images. three types questions including yes/no number other. question freeresponse answers provided. take frequent answers candidate outputs learning classiﬁcation model similar covers answers training validation sets. types questions object number color location. answers single-words considered valid answers namely answers used possible answer classiﬁcation. since formulate classiﬁcation task accuracy metric measure performances different models datasets. particular dataset predicted answer regarded correct matches three ground truth answers. wups calculates similarity words based common subsequence taxonomy tree. addition wupalmer similarity also reported coco-qa dataset. previous work report wups scores thresholds encoding questions length questions ﬁxed word embedding vector size hidden state given question image detection representations joint feature embedding inputs following take glimpses attention branch another attention weights trained whole-image features ﬁrst branch another detection features second branch. sets attended features concatenated branch ﬁnal feature. implement model torch library. rmsprop method used training network initial learning rate momentum weightdecay batch size trained iterations. validation process performed every iterations early stopping validation accuracy stops improving validations. dropout applied every linear transformation gradient clipping techniques used regularization training. multi-gpu parallel technology adopted accelerate training process. comparison state-of-the-arts table shows results test openended multiple-choice tasks proposed approach compared methods. approaches shown table trained train+val split dataset evaluated test split test-dev normally used validation test-std standard testing. models ﬁrst part table based simple joint question-image feature embedding methods. models second part table employ detection-based attention mechanism models third part freeform region-based attention mechanism. compare results single models since approaches adopt model ensemble strategy. ﬁnal model improves state-of-theart approach open-ended task multiple-choice task test-std set. speciﬁcally question types number other proposed approach brings improvements test-std set. state-of-the-art detection-based attention method model signiﬁcantly outperforms test-std set. also compare approach baseline model dual-mlb consists attention branches based tensor fusion module. baseline model fuses question free-form image region embedding branch fuses question image detection embedding another branch. baseline dual-mlb also outperforms test-dev shows improvements integration attention mechanisms also caused effective joint feature embedding question image detection features. table compares approach state-of-the-art approaches coco-qa test set. ﬁnal model dualmfa improves state-of-the-art hiecoatt particular model achieves improvement question type color. similar results dataset model significantly outperforms state-of-the-art detection-based attention method section conduct ablation experiments study effectiveness individual component designs model. table shows results baseline models replacing different components model trained training tested validation set. following compared approaches test used study online submission restrictions. speciﬁcally compare different multi-modal feature embedding designs investigate roles attention mechanisms. method mfa-mul* mfa-add mfa-norm* norm mfa-power dual-mfa-add* dual-mfa-mul dual-mfa-cat mfa-d* qru-d mlb-d mfa-r* mlb-r mutan dual-mlb dual-mfa* ﬁrst part table shows different element-wise operations used joint embedding three input features. element-wise multiplication performs better element-wise addition second part table shows normalization joint feature embedding works better model unsigned power operation model without normalization fusing outputs attention branches element-wise addition achieves better perforthird fourth parts table compare multimodal multiplicative feature embedding single attention mechanism detection-based attention baseline models regionbased attention baseline models replace multiplicative feature embedding respectively. mutan approach also used comparison. results show models single attention branch outperform compared detection-based region-based baselines. finally compare ﬁnal model dual-mfa baseline model dual-mlb. dual-mfa model beneﬁts effective multi-modal multiplicative feature embedding scheme achieves improvement qualitative evaluation visualize co-attention maps generated model figure present examples test set. figures show model attends corresponding image regions attention branches leads correct answers higher conﬁdence. also cases attention branch able attend correct image region obtain correct answer. figure free-form region based attention able attend blue ground detection-based attention fails pre-given detection boxes covering image regions. figure detection-based attention higher weights giraffes generate correct answer free-form attention attends incorrect regions. failure case also presented figure model fails generate correct answer classiﬁcation label turning right exit training despite attending correct image region. paper propose novel deep neural network co-attention mechanism visual question answering. deep model contains branches visual attention select free-form image regions detection boxes related input question. generate visual attention weights novel multiplicative embedding scheme fuses question whole-image detection-box features effectively. ablation study demonstrates effectiveness individual components proposed model. experimental results large datasets show proposed model outperforms stateof-the-art approaches. work supported part national natural science foundation china grant national basic research program china grant shmec part sensetime group limited part general research fund sponsored research grants council hong kong part hong kong innovation technology support programme grant its//fx. also thank tong xiao wang kang helpful discussions.", "year": 2017}