{"title": "Shared Learning : Enhancing Reinforcement in $Q$-Ensembles", "tag": ["cs.LG", "cs.AI"], "abstract": "Deep Reinforcement Learning has been able to achieve amazing successes in a variety of domains from video games to continuous control by trying to maximize the cumulative reward. However, most of these successes rely on algorithms that require a large amount of data to train in order to obtain results on par with human-level performance. This is not feasible if we are to deploy these systems on real world tasks and hence there has been an increased thrust in exploring data efficient algorithms. To this end, we propose the Shared Learning framework aimed at making $Q$-ensemble algorithms data-efficient. For achieving this, we look into some principles of transfer learning which aim to study the benefits of information exchange across tasks in reinforcement learning and adapt transfer to learning our value function estimates in a novel manner. In this paper, we consider the special case of transfer between the value function estimates in the $Q$-ensemble architecture of BootstrappedDQN. We further empirically demonstrate how our proposed framework can help in speeding up the learning process in $Q$-ensembles with minimum computational overhead on a suite of Atari 2600 Games.", "text": "effectively sample trajectory. idea ﬁrst introduced deep q-learning later able idea experience replay create sample efﬁcient actor-critic along modiﬁcations. hindsight experience replay tries samples replay memory help agent learn desired undesired states environment efﬁciently. another line work involves auxiliary tasks learn control environment high dimensional visual observations recently combining model-based methods model-free methods shown make learning data-efﬁcient develop agent learn efﬁciently sample experience replay. framework propose builds upon ideas transfer reinforcement learning literature particular focused transfer action advice perform online transfer. online transfer done framework differs algorithm sense action advice happens learning samples collected experience replay. q-ensemble suitable architecture perform online transfer since take advantage independent value function estimates within architecture. bootstrappeddqn architecture example q-ensemble algorithm learns explore complex environments efﬁciently planning timesteps. experimentally bootstrappeddqn shown perform better double atari games. proposed algorithms ensemble voting exploration ucb+infogain exploration inspired concepts bayesian reinforcement learning bandit algorithms. algorithms shown perform better bootstrappeddqn double many atari games. paper present framework shared learning learns share knowledge value function estimates bootstrappeddqn ensemble voting allows data-efﬁciency show framework extended deep reinforcement learning able achieve amazing successes variety domains video games continuous control trying maximize cumulative reward. however successes rely algorithms require large amount data train order obtain results human-level performance. feasible deploy systems real world tasks hence increased thrust exploring data efﬁcient algorithms. propose shared learning framework aimed making q-ensemble algorithms data-efﬁcient. achieving this look principles transfer learning study beneﬁts information exchange across tasks reinforcement learning adapt transfer learning value function estimates novel manner. paper consider special case transfer value function estimates q-ensemble architecture bootstrappeddqn. further empirically demonstrate proposed framework help speeding learning process q-ensembles minimum computational overhead suite atari games. reinforcement learning deals learning environment trying maximize cumulative payoff however early approaches scale well environments large state spaces. recently deep reinforcement learning gained great deal interest ability high-dimensional observations actions using neural network function approximator development many algorithms researchers able show effectiveness deep reinforcement learning trying solve problems complex domains. however architectures require large amount training data order near human-level performance. problem data inefﬁciency well established previously improving data efﬁciency reinprior attempts involved experience forcement replay mechanism allowed agent replay trajectories memory buffer learn rest paper structured follows look related works data-efﬁcient reinforcement learning q-ensembles transfer learning. following touch upon necessary background moving main section shared learning. ﬁrst motivate analyze framework helps markov decision process chains. further show efﬁcacy proposed framework arcade learning environment atari games. data efﬁciency evolution deep learning many algorithms developed could solve many complex problems domains like robotics games. however algorithms suffer fundamental issues highlighted issues include data inefﬁciency brittle nature learned policies inability adapt tasks. here focus works tackle problem data efﬁciency. ﬁrst work deep q-learning introduced idea experience replay improve data-efﬁciency replaying trajectories visited agent agent able decorrelate samples training also learn robustly sample. able apply idea experience replay along modiﬁcations on-policy actor-critic make whole architecture sample efﬁcient. recently proposed hindsight experience replay method allowing agent learn much information undesired outcomes would desired outcomes. take input observation goal state achieved train neural network order goal state. samples experience replay reward agent upon reaching different states visited agents trajectories. allows information learned environment. introduced method learning multiple unsupervised auxiliary tasks visual data stream allow agent learn control predict different aspects environment. proposed agent unreal signiﬁcantly outperformed previous baselines based on-line architecture. algorithm also learned robust policies able learn much less data compared tries predict actions required taken agent along predictions game features lead increase performance vizdoom also tries reproduce current state environment auxiliary task along successor model-based methods known best methods data-efﬁcient learning algorithms perform planning. however algorithms hinged fact access model environment. easy task case deep reinforcement learning. able achieve success combining model-based model-free methods data-efﬁcient learning algorithm. algorithm makes imperfect model environment create imagination-based trajectories ﬁnal policy value function provided combination model-based model-free algorithm. exploration strategies exploration strategies learning able achieve near-optimal guarantees many small domains like markov decision process chains puddle world past however near-optimal algorithms able scale well large domains deep reinforcement learning aims solve. recently proposed exploration strategies made pseudo counts state visitations hashing exploration bonuses intrinsic motivation however need strategies perform deep exploration multiple time-steps. q-ensemble architecture ﬁrst introduced bootstrappeddqn architecture perform deep exploration along level planning. architecture derives inspiration posterior sampling reinforcement learning bootstrappeddqn maintains multiple q-value estimates able perform efﬁcient exploration double majority atari games. recently modiﬁed bootstrappeddqn improve exploration adapting algorithm q-ensemble architecture using disagreement among value function estimates order provide reward bonus signal. additionally paper also proposed exploitation q-ensemble algorithm called ensemble voting agent follows policy majority vote heads. transfer transfer reinforcement learning studied great extent past several methods performing transfer proposed past study action advice reinforcement learning. action advice teacher advice student kind action needs taken step. tries provide action advice here represents weights online network refers target network weights. target network ensures learning happens stable manner replay memory ensures network learn independent identically distributed samples hence ensures stable neural network training. operator q-learning shown produce overestimation error value function overcome overestimation error dqns introduced double deep q-networks computational overhead algorithm greedy policy evaluated using online network value given target network learning update. thus loss signal ddqn becomes esars argmaxaq θi); tiple trained experts agent capture different skills teacher positive policy transfer. methods like involved knowledge transfer source tasks target task order speed learning process learning transitions expert agents. methods also shown able learn general representations environment atari games. however algorithms depend agent trained completely task perform knowledge transfer fresh agent. ﬁrst papers present online transfer agents still learning environment. paper goes show classical transfer speciﬁc case online transfer prove convergence q-learning sarsa using online transfer. notations common approach solving problem value functions indicate expected reward obtained state. paper concern ourselves state-action value functions return agent expected receive particular state upon taking action common approach learning value functions off-policy td-algorithm called q-learning optimal policy achieved behaving greedily respect learned state-action value function state. update rule q-learning follows deep q-networks environments large state spaces possible learn values every possible state-action pair. need generalizing experience small subset state space give useful approximations value function becomes issue neural networks attractive potential value function approximators known unstable even diverge reinforcement learning problems. seminal work representation learning using deep neural networks able create methods learning algorithms high dimensional visual observations. successfully used deep neural networks order carry q-learning using high dimensional atari screen observation input. additionally also overcomes problem stability learning using crucial ideas replay memory target networks. parametrized value function trained using expected bootstrappeddqn introduces novel exploration strategy performs deep exploration environments large state spaces. idea mainly draws inspiration prior works psrl rlsvi q-ensemble architecture maintains multiple parametrized value function estimates heads. bootstrappeddqn depends independent initializations heads fact head trained different samples. however shown training head different samples much important criteria independent initializations. figure comparison number steps taken shared learning-bootstrap bootstrap q-learning double q-learning algorithm algorithm converged zero variance line corresponding fastest path goal state takes steps n-state chain. bootstrappeddqn implemented adding multiple heads branch output convolutional layers shown figure suppose heads network. outputs head represent different independent estimates action-value function. value estimate target value estimate head. loss signal head given follows esars argmaxaqk θi); motivating example consider task solving chain environment figure state space action space {jump right left no-op}. task agent begins state reach state reward goes state gets reward episode terminates either state reached. good exploration algoenvironment rithms solve fast. algorithms like q-learning double q-learning hard solve environment. shown graph figure tabular adaptation bootstrappeddqn call bootstrap algorithm here able solve larger chains compared q-learning double q-learning. however algorithm takes long time converge goal state value function estimates learn independent experiences. need method transfer knowledge among value function estimates. shared learning steps framework q-ensemble algorithms able allow ensemble estimates learn action advice learning update. results shared learning figure indicate framework converges goal state solution faster bootstrap. knowledge sharing learning update motivation shared learning allow different value function estimates learn complete agent solve task much faster efﬁciently. knowledge must transferred apparent expert estimates. transfer learning literature suggests order make sure estimates able replicate rewarding trajectories expert perform action advice. instead introduce novel method trying inﬂuence estimate attempt along direction chosen estimate minor modiﬁcation learning update bootstrappeddqn given equation esars argmaxaqm θi); since requirement follow directions along maximum reward choose estimate able give highest expected reward perform knowledge sharing. modify equation robust target estimates proposing double q-learning dqns author proposed computational overhead modiﬁcation shown reduce overestimation errors dqns using online network provide greedy policy using target network value estimate. target network previous iteration online network coupled online network. reduces capability double q-learning update reported however shared learning ensure time estimate providing greedy action estimate provides target value. hence effect coupling reduces update learning becomes robust. second advantage framework believe allows better learning sample replay memory leading better understanding world. additionally would like note framework expected better reducing overestimations increasing number heads since chances given head providing greedy action target estimate reduces coupling effect also reduces. combined ability transferring knowledge rewarding states robust learning updates shared learning able replay rewarding sequences often also better understanding rewarding states. empirically observed results chain environment table figure shared learning algorithm able make goal-state visitations bootstrap algorithm. additionally also effect choice best head shared learning able goal state often random head algorithm random head chosen give target estimate. sense shared learning assumed developed biased implicit curriculum towards rewarding states simpler goals learnt much faster framework allowing agent focus complex goals environment. algorithm shared learning input value function networks outputs {qk}k replay buffer storing experience training select best interval best head selected. pick action according q-ensemble algorithm take action receive state reward store transition sample minibatch train network loss function given equation numsteps numsteps numsteps select best experiments important parameter dealt frequency choose best head among value function estimates. here chosen frequency steps i.e. head required perform knowledge sharing chosen every steps. code taken openai baselines algorithms evaluated atari games openai simulated arcade learning environment trained million frames game. graphs plotted learning curves algorithms number episodes played million frames. believe proposed framework plays lesser number episodes within number timesteps still gets higher score essentially shows framework makes data efﬁciency. shared learning-bootstrappeddqn training curves shared learning-bootstrappeddqn bootstrappeddqn shown figure also summarised highest scores obtained training different games table refer algorithm bootstrappeddqn bdqn application shared learning bdqn slbdqn. graph table shared learning able equal better bootstrappeddqn games terms score also able achieve scores lesser data. claim ability framework replay rewarding trajectories much often knowledge sharing happens between value function estimates making framework data-efﬁcient exploration algorithm. further graphs increasing slope seaquest kangaroo enduro indicates better scores training. table experimental results shared learning-bootstrap bootstrappeddqn experiments. meanmax represents scores mean curves figure epmax represents maximum score achieved episode algorithm. shared learning-ensemble voting training curves shared learning-ensemble voting ensemble voting shown figure also summarised highest scores obtained training different games table refer algorithm ensemble voting application shared learning slev. graph table shared learning able equal better ensemble voting games terms score also able achieve scores lesser data reasons mentioned previous subsection. however believe data efﬁciency improved scores profound bootstrappeddqn applying biased curriculum exploitation algorithm. shared learningtable experimental ensemble voting ensemble voting experiments. meanmax represents scores mean curves figure epmax represents maximum score achieved episode algorithm. proposed computational overhead framework shared learning make q-ensemble algorithms dataefﬁcient. also shown framework applicable exploration exploitation algorithms bootstrappeddqn ensemble voting respectively. action advice learning update framework shows signiﬁcant improvements game score atari games indicating faster learning robust estimates. paper ﬁxed interval choosing make dynamic. example figure upon changing parameter steps steps freeway make framework learn faster initially. could also tune number heads share knowledge improve robustness learning rule. since shown framework able perform better learning updates compared doubledqn could test effect learning update masking bootstrappeddqn. leave modiﬁcations shared learning future work.", "year": 2017}