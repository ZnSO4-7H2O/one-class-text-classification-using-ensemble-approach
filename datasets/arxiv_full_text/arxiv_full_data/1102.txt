{"title": "Shifting Mean Activation Towards Zero with Bipolar Activation Functions", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "We propose a simple extension to the ReLU-family of activation functions that allows them to shift the mean activation across a layer towards zero. Combined with proper weight initialization, this alleviates the need for normalization layers. We explore the training of deep vanilla recurrent neural networks (RNNs) with up to 144 layers, and show that bipolar activation functions help learning in this setting. On the Penn Treebank and Text8 language modeling tasks we obtain competitive results, improving on the best reported results for non-gated networks. In experiments with convolutional neural networks without batch normalization, we find that bipolar activations produce a faster drop in training error, and results in a lower test error on the CIFAR-10 classification task.", "text": "propose simple extension relu-family activation functions allows shift mean activation across layer towards zero. combined proper weight initialization alleviates need normalization layers. explore training deep vanilla recurrent neural networks layers show bipolar activation functions help learning setting. penn treebank text language modeling tasks obtain competitive results improving best reported results non-gated networks. experiments convolutional neural networks without batch normalization bipolar activations produce faster drop training error results lower test error cifar- classiﬁcation task. recurrent neural networks able model complex dynamical systems known hard train reason vanishing exploding gradient problem gated rnns like long short-term memory hochreiter schmidhuber alleviate problem widely used reason. however proper initialization non-gated rnns rectiﬁed linear units also achieve competitive results choice activation function strong implications learning dynamics neural networks. long known zero-centered inputs layer leads faster convergence times training neural networks gradient descent inputs layer mean shifted zero corresponding bias direction weight updates slows learning clevert showed mean shift away zero introduces bias shift units next layer removing shift zero-centering activations brings standard gradient closer natural gradient rectiﬁed linear unit deﬁned seen great success training deep networks. derivative positive values preserve magnitude error signal sigmoidal activation functions would diminish thus extent alleviating vanishing gradient problem. however since non-negative mean activation greater zero. several extensions relu proposed replace zero-valued part negative values thus allowing mean activation closer zero. leaky relu replaces negative inputs values scaled factor interval parametric leaky relu scaling factor learned training. randomized leaky relus randomly sample scale negative inputs. exponential linear units replace negative part smooth curve saturates negative value. concurrent work klambauer proposed scaled activation function also self-normalizing properties although takes orthogonal complementary approach proposed here. concatenated rectiﬁed linear unit concatenates relu function applied positive negated input balduzzi combined crelu mirrored weight initialization initialization. resulting function initially linear thus mean preserving training starts. another approach maintain mean zero across layer explicitly normalize activations. early example suggested zero-centering activations subtracting units mean activation passing next layer. glorot bengio shown problem vanishing gradients deep models mitigated unit variance layer activations. batch normalization normalizes mean variance across mini-batch. success batch normalization deep feed forward networks created research interest similar normalization mean variance extended rnns. despite early negative results keeping separate statistics timestep properly initializing parameters batch normalization applied recurrent setting approaches hidden state normalization include layer normalization weight normalization norm stabilization layer-sequential unit variance algorithm iteratively initializes layer network layer unit variance output. network maintain approximately unit variance throughout training attractive option runtime overhead. paper propose bipolar activation functions keep layer activations approximately zero-centered. explore training deep recurrent feed forward networks lsuv-initialization bipolar activations without using explicit normalization layers like batch norm. neural network relu preserves positive inputs thus shifts mean activation positive direction. however every neuron preserve negative inputs effect convolutional layers activation function half feature maps. theorem layer bipolar relu units trick ensure zero-centered i.i.d. input vector give zero-centered output vector. input vector mean different zero output mean shifted towards zero. proof. input vectors ordinary units ﬂipped units respectively. output vectors populations min. since i.i.d. distribution expectation value expectation value output simpliﬁed min] min] theorem layer bipolar units trick ensure i.i.d. input vector give output mean shifted towards point interval parameter deﬁning negative saturation value elu. proof. input vectors ordinary units ﬂipped units respectively. negative values. output vectors four populations also write .eα. different output mean shifted towards point inside interval theorem says bipolar relu input vector zero-centered mean pushed towards zero. theorem says bipolar input vector pushed towards value interval properties stabilizing effect activations. figure shows evolution dynamical system different activation functions seen bipolar activation functions stable dynamics less prone exhibiting exploding mean variance. figure iterations different activation functions lsuv procedure approximately unit variance. graphs show mean variance averaged separate runs different sampled run. ﬁrst layer encode input ﬁxed embedding vector subsequent layers output layers below hi−. layer neural network scales input factor scale layer leads exponentially exploding vanishing activation magnitudes deep networks. notice phenomenon holds path computational graph unrolled within-timestep activation magnitudes across layers within-layer activation magnitudes across timesteps. order avoid exploding vanishing dynamics want unit variance adapt lsuv weight initialization procedure recurrent setting considering single timestep setting input recurrent connections lsuv procedure simply layer sequentially adjusting magnitude produce output unit variance propagating activations network weight adjustment. since magnitude start initialization procedure scaled synchrony magnitude initialization procedure complete. means input-to-hidden connections hidden-to-hidden connections contribute equal parts unit variance thus gradient ﬂows equal magnitude across horizontal vertical connections. after lsuv initialization possible rescale order explicitly trade extent gradient along direction. choosing trade portion variance matrix contributes maintaining variance output layer lsuv initialization allows training work deeper stacks layers even lsuv trouble stacks deep enough visualizing gradient reveals gradient ﬁrst layer last takes diagonal path backwards time. effect initial learning layer strongly inﬂuenced input timesteps past. skip connections shown previously learning deeply stacked rnns note lsuv init know input skip connection approximately unit variance. this initialization procedure could scale zero still unit variance avoid effect scale skip connection slightly setting train character level language models penn treebank text corpus vocabulary characters. small size dataset proper regularization getting good performance. rnns consider follow architecture described previous section stacks simple layers skip connections groups layers lsuv initialization inputs encoded ﬁxed embedding. seek investigate effect depth effect bipolar activations deeply stacked rnns experiments illuminate this. models trained using adam optimizer learning rate batch size non-overlapping sequences length since training data cleanly divide epoch choose random crop data does done cooijmans calculated validation loss every epoch divided learning rate validation loss improve. gradient clipping used. regularization used combination various forms dropout. used standard dropout layers done pham zaremba recurrent connections followed rnndrop used dropout mask every timestep sequence. adapted stochastic depth recurrent setting stochastically dropping entire blocks layers replacing recurrent non-recurrent connections identity connections timestep unlike huang rescaling droppable blocks test time since would goal unit variance output block. look model depths activation functions relu bipolar versions brelu belu. since good performance dataset highly dependent regularization order fair comparison various depths activation functions need good regularization parameters combination. dropout recurrent connections non-recurrent connections blocks four layers thus separate dropout probabilities consider. order limit large parameter search space ﬁrst exploratory search dropout probabilities depths functions relu belu. recurrent connections dropout block dropout best results dropout probability however ideal between-layer dropout probability decreases model depth. therefore freeze parameters consider between-layer dropout probabilities relu networks depths optimal probabilities depth found activation functions. table relu-rnn performed worse increasing depth. elu-rnn learning converge. bipolar version avoids problem performance degrade increasing depth layers. overall best validation achieved layer belu-rnn. figure shows training error curves -layer activation functions shows bipolar variants faster drop training error. brieﬂy explore substituting selu unit -layer rnn. there training quickly diverges. phenomenon occurs even learning rates. however substitute selu bipolar variant training works again. -layer bselu-rnn converges validation error similar belu. previously reported results test included table listed models approximately number parameters. results layer belurnn better best reported results non-gating architectures -rnn) also competitive best normalized regularized lstm architectures. notably belu-rnn outperforms lstm recurrent batch normalization mean variance normalized. network tanh zoneout relu zoneout mi-rnn dot-rnn lstm lstm stoch. depth lstm recurrent lstm dropout lstm rec. dropout lstm layer norm lstm zoneout delta-rnn dropout hm-lstm layer norm hypernetworks belu text simpliﬁed version enwik corpus vocabulary characters. contains ﬁrst characters wikipedia mar. also trained predict next character sequence. dataset split taking ﬁrst training next validation ﬁnal testing line common practice. test results reported test error epoch lowest validation error. network architecture identical layer network used penn treebank experiments except used larger layer size used initial learning rate halved validation error improve epoch next. match previously reported results constrained number parameters lstm network. chose dropout probability stochastic depth recurrent non-recurrent dropout hyperparameter search dataset. compare result test reported results obtained approximately number parameters. table result layer belu-rnn improves upon best reported result non-gated architectures network mi-tanh lstm skipping-rnn mi-lstm lstm lstm mlstm lstm recurrent lstm stochastic depth lstm zoneout recurrent highway network hm-lstm layer norm belu explore effect different bipolar activation functions deep convolutional networks conducted simple experiments cifar- dataset recent well performing architectures. duplicated network architectures oriented response networks wide residual networks except removed batch normalization used lsuv initialization. compared performance networks without bipolar activation functions. used network variants layers widening factor dropout gave best results cifar- expirements zagoruyko komodakis zhou also duplicated data preprocessing using simple mean/std normalization images horizontal ﬂipping random cropping data augmentation. removing batch normalization required lower learning rate. network looked highest possible learning rate starting retrying half learning rate learning diverged. learning rate eased ﬁrst batches. networks trained epochs learning rate divided epoch table lists test error last epoch run. networks bipolar activations allowed training times higher learning rates. seen figure networks bipolar activations faster drop training error achieved lower test errors. note neither setup beats originally reported results networks batch normalization introduced bipolar activation functions pull mean activation layer towards zero deep neural networks. series experiments show bipolar relu units improve trainability deeply stacked simple recurrent networks convolutional networks. deeply stacked rnns unbounded activation functions provide challenging testbed learning dynamics. present empirical evidence bipolarity helps trainability setting several networks trained using bipolar versions activation functions necessary networks converge. deeply stacked rnns achieve test errors improve upon best previously reported results non-gated networks penn treebank text character level language modeling tasks. ingredients model addition bipolar activation functions residual connections depth model lsuv initialization proper regularization. experiments convolutional networks without batch normalization found bipolar activation functions allow training much higher learning rates resulting training process sees much quicker fall training error ends lower test error non-bipolar variants. david balduzzi marcus frean lennox leary lewis kurt wan-duo brian mcwilliams. shattered gradients problem resnets answer question? corr abs/. http//arxiv.org/abs/.. yoshua bengio patrice simard paolo frasconi. learning long-term dependencies gradient descent difﬁcult. ieee trans. neural networks ./.. http//dx.doi.org/./.. djork-arné clevert thomas unterthiner sepp hochreiter. fast accurate deep network learning exponential linear units corr abs/. http//arxiv.org/abs/. xavier glorot yoshua bengio. understanding difﬁculty training deep feedforward neural networks. proceedings thirteenth international conference artiﬁcial intelligence statistics aistats chia laguna resort sardinia italy http//www. jmlr.org/proceedings/papers/v/glorota.html. xavier glorot antoine bordes yoshua bengio. deep sparse rectiﬁer neural networks. proceedings fourteenth international conference artiﬁcial intelligence statistics aistats fort lauderdale april volume jmlr proceedings jmlr.org http//www.jmlr.org/proceedings/papers/v/glorota/glorota.pdf. huang zhuang daniel sedra kilian weinberger. deep networks stochastic computer vision eccv european conference amsterdam netherlands depth. october proceedings part ./----_. http//dx.doi.org/./----_. alexander ororbia tomas mikolov david reitter. learning simpler language models delta recurrent neural network framework. corr abs/. http//arxiv.org/abs/ sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. proceedings international conference machine learning icml volume jmlr proceedings jmlr.org http//jmlr.org/ proceedings/papers/v/ioffe.html. david krueger tegan maharaj jános kramár mohammad pezeshki nicolas ballas rosemary anirudh goyal yoshua bengio hugo larochelle aaron courville chris pal. zoneout regularizing rnns randomly preserving hidden activations. corr abs/. http//arxiv.org/abs/ césar laurent gabriel pereyra philemon brakel ying zhang yoshua bengio. batch normalized recurrent neural networks. ieee international conference acoustics speech signal processing icassp shanghai china march ./icassp... http//dx.doi.org/./icassp... vinod nair geoffrey hinton. rectiﬁed linear units improve restricted boltzmann machines. proceedings international conference machine learning june haifa israel http//www.icml.org/papers/.pdf. behnam neyshabur yuhuai ruslan salakhutdinov nati srebro. path-normalized optimization recurrent neural networks relu activations. sugiyama luxburg guyon garnett advances neural information processing systems curran associates inc. http//arxiv.org/abs/.. razvan pascanu çaglar gülçehre kyunghyun yoshua bengio. construct deep recurrent neural networks. corr abs/. http//dblp.uni-trier.de/db/journals/ corr/corr.htmlpascanugcb. razvan pascanu tomas mikolov yoshua bengio. difﬁculty training recurrent neural networks. proceedings international conference machine learning icml atlanta june http//jmlr.org/proceedings/papers/v/ pascanu.html. pham christopher kermorvant jérôme louradour. dropout improves recurrent neural networks handwriting recognition. corr abs/. http//arxiv.org/abs/.. salimans diederik kingma. weight normalization simple reparameterization accelerate training deep neural networks. sugiyama luxburg guyon garnett advances neural information processing systems curran associates inc. http//arxiv.org/abs/.. wenling shang kihyuk sohn diogo almeida honglak lee. understanding improving convolutional neural networks concatenated rectiﬁed linear units. corr abs/. http//arxiv. org/abs/.. yuhuai saizheng zhang ying zhang yoshua bengio ruslan salakhutdinov. multiplicative integration recurrent neural networks. corr abs/. http//arxiv.org/ abs/.. figure norm gradient output layer propagates back time. gradient calculated last timestep batch backpropagated time. computed ﬁrst batches learning process averaged those. maximal redness indicates maximum gradient magnitude. without skip connections. bottom skip connections. networks -layer vanilla rnns bipolar units lsuv-initialized trained character-level penn treebank. looking gradient reveals problem horizontal connections vertical connections approximately equal magnitude gradient distributed equal parts vertically horizontally. effect seen figure gradient smeared degree angle away origin. undesirable example layer network error signal reaches ﬁrst layer mostly relates inputs around timesteps past. figure skip connections help learning deeply stacked lsuv-initialized rnns. left without skip connections. right skip connections connecting groups four layers. plots show training loss vanilla rnns bipolar units layer lsuv-initialized trained character-level penn treebank. figure training loss various activation functions -layer rnns penn treebank. brelu-rnn lower training error relu-rnn times curves comparable belu-rnn lowest training error all. elu-rnn training diverged quickly.", "year": 2017}