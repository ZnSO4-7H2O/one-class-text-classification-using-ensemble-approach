{"title": "Large Scale Variational Bayesian Inference for Structured Scale Mixture  Models", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Natural image statistics exhibit hierarchical dependencies across multiple scales. Representing such prior knowledge in non-factorial latent tree models can boost performance of image denoising, inpainting, deconvolution or reconstruction substantially, beyond standard factorial \"sparse\" methodology. We derive a large scale approximate Bayesian inference algorithm for linear models with non-factorial (latent tree-structured) scale mixture priors. Experimental results on a range of denoising and inpainting problems demonstrate substantially improved performance compared to MAP estimation or to inference with factorial priors.", "text": "natural image statistics exhibit hierarchical dependencies across multiple scales. representing prior knowledge non-factorial latent tree models boost performance image denoising inpainting deconvolution reconstruction substantially beyond standard factorial sparse methodology. derive large scale approximate bayesian inference algorithm linear models nonfactorial scale mixture priors. experimental results range denoising inpainting problems demonstrate substantially improved performance compared estimation inference factorial priors. leaps performance realized low-level computer vision problems denoising inpainting deconvolution image coding undersampled reconstruction acquisition optimization adopting super-gaussian image priors. methods employ simple factorial priors single coeﬃcients groups substantial gains obtained modelling higher-order dependencies structured non-factorial prior distributions example representing dependencies among multi-scale wavelet coeﬃcients tree structure boost accuracy image compression reconstruction however previous approaches employing non-factorial priors either much slower standard factorial methodology sacriﬁce performance adopting suboptimal estimation naive mean ﬁeld factorization assumptions ignoring posterior covariance uncertainty altogether problems highly underdetermined. paper derive large scale approximate bayesian inference algorithm generalized linear models non-factorial scale mixture priors. contributions follows inference hybrid models. require factorization assumptions image pixels wavelet coeﬃcients. method based standard scalable technology operate scales estimation. boost performance compared factorial ones bayesian hyperparameter learning improves posterior mean prediction substantially compared default initialization. structure paper follows. describe motivate image model section develop large scale bayesian inference learning algorithm section present experimental results image denoising inpainting section close conclusions. range prior work employed latent treestructured priors represent dependencies wavelet coeﬃcients. crouse gaussian mixture potentials well hidden markov tree discrete mixture indicators estimating latent signal mixture parameters expectation maximization. portilla employ continuous scale mixtures based latent gaussian tree mapped coordinate-wise nonlinearities. estimate parameters nonlinear optimization. papandreou employ hidden markov tree gaussian mixture potentials overcomplete wavelet representation estimating signal parameters viterbi training. none employ bayesian inference image non-gaussian potentials. spike-and-slab prior potentials hidden markov tree indicators. perform standard naive mean ﬁeld variational inference employing posterior distribution represent dependencies coeﬃcients. assumptions lead simple update equations iterate parallel. algorithm seem reduce standard scalable optimization primitives. moreover method extend beyond spikeand-slab sparsity potentials. framework seen extension scalable double loop algorithm variational inference super-gaussian models proposed however consider hybrid models latent tree non-factorial prior distributions bayesian learning hyperparameters here. surements) noise and/or blur additional statistical information form image prior distribution renders image reconstruction well-posed problem. given infer image posterior distribution image statistics tend leptokurexample even simple factorial image priors respecting properties lead dramatic improvements classical methodology beyond marginals image statistics exhibit complex dependencies capturing non-factorial priors lead leaps performance however probabilistic inference becoming much diﬃcult expensive extended models enjoy little popularity compared simpler factorial alternatives. contrast methodology developed paper scales estimation variational inference factorial priors. coeﬃcient belongs scale level analysis coarsest ﬁnest scale even though approximately uncorrelated natural images well known substantial causal dependencies coeﬃcients neighbouring levels typically modbinary variables well mixture potentials tj−δj tjδj enforces strongly high state |sj| penalized accordingly. quad-tree structure encourages inherij non-factorial scale mixture model faithfully represents causal inheritance wavelet coeﬃcient sizes coarse levels. normalization constant since orthonormal transform. figure illustrate eﬀect non-factorial prior results inpainting problem latent tree prior employs student’s potentials high gaussian potentials state hyperparameters learned automatically also show results factorial prior student’s potentials comparison whose hyperparameters learned way. nonfactorial prior leads faithful reconstruction wavelet coeﬃcient coarser scales motivates superior inpainting results section figure shown horizontal ﬁlterbands levels wavelet transform natural image complete image inpainting results pixels first transform sparse large coeﬃcients occur scales localized clustering across levels clearly visible. second factorial prior lead suppression energy coarser scales third latent tree scale mixture prior represents wavelet statistics much better energy levels well reconstructed. fourth corresponding marginals denotes inverse covariance matrix concave representation based corresponding fenchel duality. since dependence neither depends plugging pulling minu∗z outside obtain ﬁnal upper bound π)+hδ)+d ]−g∗ minimized notice linearity. obviously strong dependencies components contrast previous work require factorization assumptions them. high level idea behind approach iterative decoupling. combine standard variational bound decouple double loop framework decouples mean covariance computations latter provides computational reduction convex penalized least squares optimization gaussian sampling crucial scalability. shown below minor simpliﬁcations result inference maximum posteriori estimation algorithms non-factorial factorial priors based underlying code. here used linear standard-form penalized least squares problem solved large number recent algorithms developed estimation. experiments employ nonlinear conjugate gradients substantial number hyperparameters adjusted problem hand. example wavelet coeﬃcients exhibit higher variance coarser ﬁner scales corresponding prior potentials take account gaussians experiments non-factorial priors scale levels giving rise hyperparameters many reasonably non-bayesian methods like cross-validation. section show learn hyperparameters automatic bayesian way. method folded variational inference process lets optimize hyperparameters dataset hand. stress bayesian learning operates data clean underlying images required. bayesian learning works maximizing marginal likelihood w.r.t. hyperparameters. obvious variational approximation maximize lower bound instead. indeed treat hyperparameters another parameters minimize over thereby folding learning inference approximation. importantly update part inner loop optimization ﬁxed withcompromising overall convergence recall comes fenchel duality direct primitives reduces entirely solving moderate number linear systems diﬀerent done eﬃciently stateof-the-art preconditioned conjugate gradients solvers. double loop structure implies expensive updates done least frequently. derived algorithm variational bayesian inference structured non-factorial prior importantly obtain scalable algorithms estimation inference factorial non-factorial priors making minor simplifying modiﬁcation otherwise using exactly underlying code. first using factorial prior form sists single penalized least squares problem second estimation obtained simply setting skipping variances computations running single outer loop iteration. estimation non-factorial prior proceeds expectationmaximization fashion alternating penalized least squares belief propagation smooth convex function easily minimized one-dimensional newton solver. student’s hyperparameters updated reﬁt corresponding continue another inner loop. present experiments range denoising inpainting problems comparing variational inference estimation diﬀerent models. results averaged frequently used images compare methods estimation variational inference factorial prior latent tree scale mixture prior laplacian student’s potentials lap-tree model uses laplace potentials diﬀerent hyperparameters pair level. student’s potentials high state pair hyperparameters level. fact setups. student’s shape parameter ﬁxed initialize hyperparameters maximizing prior probability data optimize minimizing hyperparameters updated outer loop iteration. runs outer loop iterations. perturb&map estimation required inpainting only samples conjugate gradients iterations. belief propagation calls iteration tree setups iterations nonlinear iteration requires matrix-vector multiplications choices optimized maximum eﬃciency. applied models featuring student’s potentials algorithm detailed requires nonconvex problems solved inner loop. commonly used ﬁrst order solvers fail dramatically non-convex problems. since adopt double loop strategy anyway simpler robust additional bounding order obtain convex inner loop problem. idea previously described applied student’s potentials applications here well hyperparameter learning method novel. recall representation student’s potential convex written convex term concave term moreover concave part represented fenchel dual∩. overall parameter vector updated alongside deﬁne therefore perturb&map dominating cost general required. results shown table application diﬀerences reconstruction signiﬁcant. hand non-factorial prior improves psnr somewhat. hyperparameter learning improves performance substantially especially student’s contrast help potentials used. performance. mask images. design matrix noise variance ﬁxed results shown table psnr always correlate well visual quality show range images figure figure figure posterior mean predictions clearly superior reconstruction non-factorial latent tree prior performs best. factorial laplace prior shows similar psnr values vb-lap-tree visual appearance results latter clearly superior additional runtime compared estimation mainly estimation variances pays problems. presented double loop algorithm variational bayesian inference linear models non-factorial scale mixture priors based latent discrete tree distribution. method operate scales estimation posterior mean prediction strongly consistently outperforms posterior mode across range inpainting problems. selective smoothing predictive variances coupling wavelet coeﬃcient across scales latent tree contribute removal artefacts plague results estimation inference factorial priors. free hyperparameters learned automatically marginal likelihood maximization folded variational optimization.", "year": 2012}