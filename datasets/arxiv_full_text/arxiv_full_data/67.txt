{"title": "Reasoning about Entailment with Neural Attention", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "68T50", "I.2.6; I.2.7"], "abstract": "While most approaches to automatically recognizing entailment relations have used classifiers employing hand engineered features derived from complex natural language processing pipelines, in practice their performance has been only slightly better than bag-of-word pair classifiers using only lexical similarity. The only attempt so far to build an end-to-end differentiable neural network for entailment failed to outperform such a simple similarity classifier. In this paper, we propose a neural model that reads two sentences to determine entailment using long short-term memory units. We extend this model with a word-by-word neural attention mechanism that encourages reasoning over entailments of pairs of words and phrases. Furthermore, we present a qualitative analysis of attention weights produced by this model, demonstrating such reasoning capabilities. On a large entailment dataset this model outperforms the previous best neural model and a classifier with engineered features by a substantial margin. It is the first generic end-to-end differentiable system that achieves state-of-the-art accuracy on a textual entailment dataset.", "text": "approaches automatically recognizing entailment relations used classiﬁers employing hand engineered features derived complex natural language processing pipelines practice performance slightly better bag-of-word pair classiﬁers using lexical similarity. attempt build end-to-end differentiable neural network entailment failed outperform simple similarity classiﬁer. paper propose neural model reads sentences determine entailment using long short-term memory units. extend model word-by-word neural attention mechanism encourages reasoning entailments pairs words phrases. furthermore present qualitative analysis attention weights produced model demonstrating reasoning capabilities. large entailment dataset model outperforms previous best neural model classiﬁer engineered features substantial margin. ﬁrst generic end-to-end differentiable system achieves state-of-the-art accuracy textual entailment dataset. ability determine semantic relationship sentences integral part machines understand reason natural language. recognizing textual entailment task determining whether natural language sentences contradicting other related whether ﬁrst sentence entails second sentence task important since many natural language processing problems information extraction relation extraction text summarization machine translation rely explicitly implicitly could beneﬁt accurate systems state-of-the-art systems relied heavily engineered pipelines extensive manual creation features well various external resources specialized subcomponents negation detection despite success neural networks paraphrase detection end-to-end differentiable neural architectures failed close acceptable performance lack large high-quality datasets. end-to-end differentiable solution desirable since avoids speciﬁc assumptions underlying language. particular need language features like part-of-speech tags dependency parses. furthermore generic sequence-to-sequence solution allows extend concept capturing entailment across sequential data natural language. recently bowman published stanford natural language inference corpus accompanied neural network long short-term memory units achieves accuracy dataset. ﬁrst time generic neural model without hand-crafted features close accuracy simple lexicalized classiﬁer engineered features rte. explained high quality size snli compared orders magnitude smaller partly synthetic datasets used evaluate systems. bowman al.’s lstm encodes premise hypothesis dense ﬁxed-length vectors whose concatenation subsequently used multi-layer perceptron classiﬁcation. contrast proposing attentive neural network capable reasoning entailments pairs words phrases processing hypothesis conditioned premise. contributions threefold present neural model based lstms reads sentences determine entailment opposed mapping sentence independently semantic space extend model neural word-by-word attention mechanism encourage reasoning entailments pairs words phrases provide detailed qualitative analysis neural attention benchmark lstm achieves accuracy snli outperforming simple lexicalized classiﬁer tailored percentage points. extension word-by-word neural attention surpasses strong benchmark lstm result percentage points setting state-of-the-art accuracy recognizing entailment snli. methods section discuss lstms describe applied introduce extension lstm neural attention word-by-word attention finally show attentive models easily used attending ways premise conditioned hypothesis hypothesis conditioned premise recurrent neural networks long short-term memory units successfully applied wide range tasks machine translation constituency parsing language modeling recently lstms encompass memory cells store information long period time well three types gates control information cells input gates forget gates output gates given input vector time step previous output cell state lstm hidden size computes next output cell state rk×k trained matrices trained biases parameterize gates transformations input denotes element-wise application sigmoid function element-wise multiplication vectors. lstms readily used independently encoding premise hypothesis dense vectors taking concatenation input classiﬁer demonstrates lstms learn semantically rich sentence representations suitable determining textual entailment. contrast learning sentence representations interested neural models read sentences determine entailment thereby reasoning entailments pairs words phrases. figure shows high-level structure model. premise read lstm. second lstm different parameters reading delimiter hypothesis memory figure recognizing textual entailment using conditional encoding lstms premise hypothesis conditioned representation premise attention based last output vector word-by-word attention based output vectors hypothesis state initialized last cell state previous lstm i.e. conditioned representation ﬁrst lstm built premise wordvec vectors word representations optimize training. out-ofvocabulary words training randomly initialized sampling values uniformly optimized training. out-of-vocabulary words encountered inference time validation test corpus ﬁxed random vectors. tuning representations words wordvec vectors ensure inference time representation stays close unseen similar words wordvec embeddings. linear layer project word vectors dimensionality hidden size lstm yielding input vectors finally classiﬁcation softmax layer output non-linear projection last output vector target space three classes train using cross-entropy loss. attentive neural networks recently demonstrated success wide range tasks ranging handwriting synthesis digit classiﬁcation machine translation image captioning speech recognition sentence summarization geometric reasoning idea allow model attend past output vectors thereby mitigating lstm’s cell state bottleneck. precisely lstm attention need capture whole semantics premise cell state. instead sufﬁcient output vectors reading premise accumulating representation cell state informs second lstm output vectors premise needs attend determine class. rk×l matrix consisting output vectors ﬁrst lstm produced reading words premise hyperparameter denoting size embeddings hidden layers. furthermore vector last output vector premise hypothesis processed lstms respectively. attention mechanism produce vector attention weights weighted representation premise rk×k trained projection matrices trained parameter vector denotes transpose. note outer product whhn repeating linearly transformed many times words premise hence intermediate attention representation word premise obtained non-linear combination premise’s output vector transformed attention weight word premise result weighted combination values ﬁnal sentence-pair representation obtained non-linear combination attentionweighted representation premise last output vector using determining whether sentence entails another good strategy check entailment contradiction individual wordphrase-pairs. encourage behavior employ neural word-by-word attention similar bahdanau hermann rush difference attention generate words obtain sentence-pair encoding ﬁne-grained reasoning soft-alignment words phrases premise hypothesis. case amounts attending ﬁrst lstm’s output vectors premise second lstm processes hypothesis word time thus generating attention weight-vectors output vectors premise every word hypothesis modeled follows previous section ﬁnal sentence-pair representation obtained non-linear combination last attention-weighted representation premise last output vector using tanh inspired bidirectional lstms read sequence reverse improved encoding introduce two-way attention rte. idea model attend premise conditioned hypothesis well attend hypothesis conditioned premise simply swapping sequences. produces sentence-pair representations concatenate classiﬁcation. conduct experiments stanford natural language inference corpus corpus orders magnitude larger existing corpora sentences involving compositional knowledge furthermore large part training examples sick generated heuristically examples. contrast sentence-pairs snli stem human annotators. size quality snli make suitable resource training neural architectures ones proposed paper. adam optimization ﬁrst momentum coefﬁcient second momentum coefﬁcient every model perform small grid search model lexicalized classiﬁer lstm conditional encoding shared conditional encoding shared conditional encoding attention attention two-way word-by-word attention word-by-word attention two-way combinations initial learning rate dropout regularization strength subsequently take best conﬁguration based performance validation evaluate conﬁguration test set. results snli corpus summarized table total number model parameters including tunable word representations denoted |θ|w+m ensure comparable number parameters bowman al.’s model encodes premise hypothesis independently using lstm also experiments conditional encoding parameters lstms shared opposed using independent lstms. addition compare attentive models benchmark lstms whose hidden sizes chosen least many parameters attentive models. since tuning word vectors wordvec embeddings total number parameters |θ|w+m models considerably smaller. also compare models benchmark lexicalized classiﬁer used bowman constructs features bleu score premise hypothesis length difference word overlap unibigrams part-of-speech tags well cross unibigrams. conditional encoding found processing hypothesis conditioned premise instead encoding sentence independently gives improvement percentage points accuracy bowman al.’s lstm. argue information able part model processes premise part processes hypothesis. specifically model waste capacity encoding hypothesis read hypothesis focused checking words phrases contradictions entailments based semantic representation premise. interpretation lstm approximating ﬁnite-state automaton another difference bowman al.’s model using wordvec instead glove word representations importantly ﬁne-tune word embeddings. drop accuracy train test less severe models suggest ﬁne-tuning word embeddings could cause overﬁtting. lstm outperforms simple lexicalized classiﬁer percentage points. best knowledge ﬁrst instance neural end-to-end differentiable model achieve state-ofthe-art performance textual entailment dataset. attention incorporating attention mechanism found percentage point improvement single lstm hidden size percentage point increase benchmark model uses lstms conditional encoding attention model produces output vectors summarizing contextual information premise useful attend later reading hypothesis. therefore reading premise model build semantic representation whole premise instead representation helps attending right output vectors processing hypothesis. contrast output vectors premise used benchmark lstms. thus models build representation whole premise carry cell state part processes hypothesis—a bottleneck overcome degree using attention. word-by-word attention enabling model attend output vectors premise every word hypothesis yields another percentage point improvement compared attending based last output vector premise. argue model able check entailment contradiction individual words phrases hypothesis demonstrate effect qualitative analysis below. two-way attention allowing model also attend hypothesis based premise seem improve performance rte. suspect entailment asymmetric relation. hence using lstm encode hypothesis premise might lead noise training signal. could addressed training different lstms cost doubling number model parameters. instructive analyze output representations model attending deciding class example. note interpretations based attention weights taken care since model forced solely rely representations obtained attention following visualize discuss attention patterns presented attentive models. attentive model hand-picked examples randomly drawn samples validation set. attention figure shows extent attentive model focuses contextual representations premise lstms processed premise hypothesis respectively. note model pays attention output vectors words semantically coherent premise contradiction caused single word multiple words interestingly model shows contextual understanding attending yellow color pink color coat. however involved examples longer premises found attention uniformly distributed suggests conditioning attention last output limitations multiple words need considered deciding class. word-by-word attention visualizations word-by-word attention depicted figure found word-by-word attention easily detect hypothesis simply reordering words premise furthermore able resolve synonyms capable matching multi-word expressions single words also noteworthy irrelevant parts premise words capturing little meaning whole uninformative relative clauses correctly neglected determining entailment word-by-word attention seems also work well words premise hypothesis connected deeper semantics common-sense knowledge furthermore model able resolve one-to-many relationships attention fail example sentences words entirely unrelated cases model seems back attending function words sentence-pair representation likely dominated last output vector instead attention-weighted representation paper show state-of-the-art recognizing textual entailment large humancurated annotated corpus improved general end-to-end differentiable models. results demonstrate lstm recurrent neural networks read pairs sequences produce ﬁnal representation simple classiﬁer predicts entailment outperform neural baseline well classiﬁer hand-engineered features. extending models attention premise provides improvements predictive abilities system resulting state-of-the-art accuracy recognizing entailment stanford natural language inference corpus. models presented general sequence models requiring appeal natural languagespeciﬁc processing beyond tokenization therefore suitable target transfer learning pre-training recurrent systems corpora conversely applying models trained corpus entailment tasks. future work focus transfer learning tasks well scaling methods presented larger units text using hierarchical attention mechanisms. additionally would worthwhile exploring other structured forms attention forms differentiable memory could help improve performance neural models presented paper. furthermore investigate application generic models non-natural language sequential entailment problems. dagan oren glickman bernardo magnini. pascal recognising textual entailment challenge. machine learning challenges. lecture notes computer science volume pages springer alex graves. generating sequences recurrent neural networks. arxiv preprint arxiv. alex graves j¨urgen schmidhuber. framewise phoneme classiﬁcation bidirectional lstm alex graves greg wayne danihelka. neural turing machines. arxiv preprint arxiv. edward grefenstette karl moritz hermann mustafa suleyman phil blunsom. learning transduce sergio jimenez george duenas julia baquero alexander gelbukh juan dios b´atiz mendiz´abal. unal-nlp combining soft cardinality features semantic textual similarity relatedness entailment. semeval page marco marelli luisa bentivogli marco baroni raffaella bernardi stefano menini roberto zamparelli. semeval- task evaluation compositional distributional semantic models full sentences semantic relatedness textual entailment. semeval oriol vinyals meire fortunato navdeep jaitly. pointer networks. nips kelvin jimmy ryan kiros kyunghyun aaron courville ruslan salakhutdinov richard zemel yoshua bengio. show attend tell neural image caption generation visual attention. icml", "year": 2015}