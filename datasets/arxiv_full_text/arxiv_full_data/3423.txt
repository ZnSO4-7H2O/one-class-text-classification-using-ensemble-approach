{"title": "Neural Networks for Beginners. A fast implementation in Matlab, Torch,  TensorFlow", "tag": ["cs.LG", "cs.CV", "cs.MS", "stat.ML"], "abstract": "This report provides an introduction to some Machine Learning tools within the most common development environments. It mainly focuses on practical problems, skipping any theoretical introduction. It is oriented to both students trying to approach Machine Learning and experts looking for new frameworks.", "text": "report provides introduction machine learning tools within common development environments. mainly focuses practical problems skipping theoretical introduction. oriented students trying approach machine learning experts looking frameworks. dissertation artiﬁcial neural networks since currently trend topic achieving state performance many artiﬁcial intelligence tasks. ﬁrst individual introduction framework setting general practical problems carried simultaneously order make comparison easier. since treated argument widely studied continuos fast growing pair document on-line documentation available github repository dynamic hope kept updated possibly enlarged. matlab powerful instrument allowing easy fast handling almost every kind numerical operation algorithm programming testing. intuitive friendly interactive interface makes easy manipulate visualize analyze data. software provides mathematical built-in functions every kind task extensive easily accessible documentation. mainly designed handle matrices hence almost functions operations vectorized i.e. manage scalars well vectors matrices tensors. reasons eﬃcient avoid loops cycles operations exploiting matrices multiplication. document show simple machine learning related instruments order start playing anns. assume basic-level knowledge address oﬃcial documentation informations. instance informations obtain software oﬃcial site. indeed license free even universities provide classroom license students maybe could possible access current packages. particular statistic machine learning toolboxtm neural network toolboxtm provide built-in functions models implement diﬀerent anns architectures suitable face every kind task. access tools fundamental prosecution even refer simple independent examples. easy to-go nnstart function activates simple guiding user trough deﬁnition simple -layer architecture. allows either load available data samples work customize data train network analyze results however functions available speciﬁc tasks. instance function patternnet speciﬁcally designed pattern recognition problems newfit suitable regression whereas feedforwardnet ﬂexible allows build customized complicated networks. versions implemented similar main options methods apply everyone. next section show manage customizable architectures starting face basic problems. detailed informations dedicated section oﬃcial site. computing matlabrequires parallel computing toolboxtm cuda installation machine. detailed informations check gpus devices found computing oﬃcial page issues distributed computing cpus/gpus introduced too. however basic operations graphical cards general quite simple. data moved hardware function gpuarray back function gather. dealing anns dedicated function nndatagpu provided organizing tensors eﬃcient conﬁguration order speed computation. alternative carry training process correspondent option function train https//ch.mathworks.com/products/matlab.html?s_tid=hp_products_matlab http//ch.mathworks.com/help/nnet/getting-started-with-neural-network-toolbox.html https//ch.mathworks.com/help/nnet/ug/neural-networks-with-parallel-and-gpu-computing.html well-known classiﬁcation problem simple eﬀective order understand basic properties many machine learning algorithms. even writing eﬃcient ﬂexible architecture requires language expertise elementary implementation found matlabsection github repository document. suitable face real tasks since customizations allowed useful give general tips design personal module. code present basic easily improved keep simple understand fundamental steps. stressed above avoid loops exploiting matlabeﬃciency matrix operations forward backward steps. point substantially aﬀects running time large data. below deﬁne train eﬃcient architectures exploiting built-in functions neural network toolboxtm since face classiﬁcation problem sort experiments using function patternnet. start declare object kind network selected function contains variables methods carry optimization process. function expects optional arguments representing number hidden units back-propagation algorithm exploited training phase. number hidden units provided single integer number expressing size hidden layer integer vector whose elements indicate size correspondent hidden layers. command creates object named kind network representing -layer units single hidden layer. object several options reached notation object.property explore clicking interactively visualization object matlabcommand window allows available options property too. second optional parameter selects training algorithm string saved trainfcn property default case takes value ’trainscg’ network object still fully deﬁned since variables adapted data dimension calling function train. however function configure taking input object data problem faced allows complete network options optimization starts. matlabexpects targets provided form -class problem targets provided vector length number samples. multi-class problem targets provided one-hot encoding form i.e. matrix many columns number samples composed position indicating class. layer object kind nnetlayer created stored cell array ﬁeld layers network object. number connections units corresponds layer input dimension. options layer reached notation object. layer{numberof layer}.property. ﬁeld initfcn contains weights initialization methods. activation function stored transferfcn property. hidden layers default values ’tansig’ whereas output layers ’logsig’ ’sof tmax’ -dimensional multi-dimensional target respectively. ’crossentropy’ penalty function default ﬁeld performfcn. point global architecture network visualized command function train makes available many options directly accessible interactive help window. however take input network object input target matrices. optimization starts dividing data training validation test sets. splitting ratio changed options divideparam. default setting data randomly divided want example decide data used test change data distributed option dividefcn. case small size dataset drop validation test setting following code training function classic gradient descent method ’traingd’ deactivate training interactive nn.trainparam.showwindow activate printing training state command window nn.trainparam.showcommandline also learning rate part trainparam options ﬁelds indicates training stops number epoch reached column shows state stopping criterions used analyze details next section. output variable stores training options. ﬁelds perf vperf tperf contain performance network evaluated epoch training validation test sets respectively used example plot performances. pass data organized single matrix function exploit full batch learning method accumulating gradients overall training set. mini-batch mode data manually split sub-matrix number column organized cell array. however consider moment general data composed samples features space target dimension rd×n rc×n mini-batches size general convenient choose batch size factor case generate data training function organizing input target correspondent cell-array however order perform pure stochastic gradient descent optimization anns parameters updated sample training function ’trains’ employed skipping split data previously. remark done since particular function support computing. network easily evaluated data passing input data argument function named network object. performance prediction respect targets evaluated function perform according correspondent loss function option object. performfcn early stopping well known procedure machine learning avoid overﬁtting improve generalization. routines neural network toolboxtm diﬀerent kind stopping criterions regulated network object options ﬁeld trainparam. arbitrarily methods based number epochs training time criterion based training check loss parameters gradients reach minimum threshold. general early stopping method implemented checking error validation interrupting training validation error improve number consecutive epochs given max_fail regularization methods conﬁgured property performparam network object. ﬁeld regularization contains weight balancing contribution term trying minimizing norm network weights versus satisfaction penalty function. however network designed mainly rely validation checks indeed regularization applies kind penalties default weight dealing dimensional data useful visualize prediction network directly input space. kind task matlabmakes available built-in functions many options interactive data visualization. section show main functions useful realize customized separation surfaces learned respect speciﬁc experiments. brieﬂy comments instruction referring suite help speciﬁc knowledge single function. network predictions evaluated grid input space generated matlabfunction meshgrid since main functions used plotting require input three matrices dimensions expressing correspondent element coordinates point trained network described boundary -classes separation showed figure generated code listing whereas figure report evaluation training -layers network using units ﬁrst second third hidden layers respectively using relu activation network deﬁned torch easy eﬃcient scientiﬁc computing framework essentially oriented machine learning algorithms. package written guarantees high eﬃciency. however completely interaction possible luajit interface provides fast intuitively scripting language. moreover contains libraries necessary integration cuda environment computing. moment writing used tool prototyping anns kind topology. indeed many packages constantly updated improved large community allowing develop almost kind architectures simple way. informations installation found getting started section oﬃcial site. procedure straightforward unix based operative systems whereas oﬃcially supported windows even alternative provided. cuda already installed also packages cutorch cunn added automatically containing necessary utilities deal nvidia gpus. torch acts interface c/cuda routines. programmer cases worry functions. therefore explain language works necessary deal torch. scripting language syntax similar python semantic close javascript. variable considered global default. local declaring usually recommended require explicit declaration placing keyword local name variable. chosen scripting languages python fastest crucial feature dealing large data complex programs common machine learning. seven native types boolean number string userdata function table even power related last one. table behaves either hash array table considered array contains numerical keys starting value complex structure classes built torch extends capabilities table implementing tensor class. many matlab-like functions provided order initialize manipulate tensors concise fashion. http//torch.ch/docs/getting-started.html https//github.com/torch/torch/wiki/windows reference manual available here https//www.lua.org/manual/./ http//atamahjoubfar.github.io/torch_for_matlab_users.pdf provided packages developed following strong modularization crucial feature keep code stable dynamic. provides several already built-in functionalities easily imported code. main course torch installed beginning. packages included ﬁrst installation easy shell command need create kind anns contained package every element inside package inherits abstract class nn.module. main state variables output gradinput result forward backward steps stored. forward backward methods class notation). invoke updateoutput updategradinput respectively abstract deﬁnition must derived classes. main advantage package gradients computations back-propagation step automatically realized thanks built-in functions. requirement call forward step backward. containers abstract modules allow build multi-layered networks. nn.sequential connect several layers feed-forward manner. nn.parallel nn.concat important build complex structure input ﬂows separated architectures. layers activation functions even criterions added inside containers. detailed documentation package refer oﬃcial webpage. another useful package could interested building complex architectures found nngraph repository. since c++/cuda programming integration trivial develop important interface simple possible linking tools. torch provides clean solution dedicated packages cutorch cunn installed). objects transferred memory gpus method cuda back double. operations executed hardware involved objects possible among variables unit. listing show examples correct wrong statements. order give concrete feeling presented tools show examples classical problem previous section. code showed found torch section document’s github repository useful play parameters become familiar environment. deﬁne standard anns hidden layer composed hidden units hyperbolic tangen transfer function identity output function. structure network stored container necessary modules added. standard feed-forward architecture deﬁned sequential container named mlp. network assembled adding sequentially desired modules function https//github.com/torch/nn detailed documentation https//github.com/torch/nngraph available https//github.com/ailabusiena/neuralnetworksforbeginners/tree/master/torch/xor. training composed tensor samples paired tensor targets. usually true false boolean values respectively associated however propose equivalent diﬀerent approach shift values showed listing input target initialized false values true values placed according truth table. forward returns output multi layer perceptron w.r.t given input; updates input/output states variables modules preparing network backward step; output immediately passed loss function compute error. criterion forwarded returns error input arguments. updates modules state variable gets ready compute gradients tensor loss backward step back-propagated multilayer perceptron. modules possible criterions used functions forward backward others. whole training procedure since training procedure manually deﬁned particular stopping criterion completely user. simplest based reaching ﬁxed number epochs explicitly depends upper bound cycle. since methods related presence validation deﬁne example early stopping criterion listing section simple criterion based vanishing gradients simply exploiting function getparameters deﬁned modules returns weights gradients network -dimensional vector another regularization method accomplished implementing weight decay method shown listing presented code intended introductory example even understand class inheritance mechanisms torch. implement weight decay coherently package need create novel class inheriting nn.sequential overloads method updateparameters nn.module. ﬁrst declare class name torch optionally specify parent class. case class called nn.weightdecaywrapper class inherits container nn.sequential. constructor deﬁned within function weightdecay__init. scope function variable self table used refer attributes methods abstract object. calling function __init parent class automatically original properties. functions weigthdecaygetweigthdecay weigthdecayupdateparameters compute respectively weight decay gradient. methods loop modules container parameters order compute either error gradients coming weight decay contribution. argument alpha represent regularization parameter weight decay provided assumed null. also worth mention fact that weigthdecayupdateparameters overloads method implemented nn.module updating parameters according standard gradient descent rule. point expecting possible weight decay regularization declared replacing nn.sequential container proposed nn.weightdecaywrapper. framework data visualization allowed package gnuplot provides tools plot points lines curves example training procedure presented listing vector storing penalty evaluated epoch produced. idea state network training save image containing trend error code listing output shown figure torch dedicated functions visualize separation surfaces produced data hence generate random grid across input space plotting points predicted close enough half possible target correspondent result showed figure generated code listing exploiting fact support logical indexing matlab. figure trend loss versus number epochs. estimated separation surface obtained -layers composed hidden units hyperbolic tangent activation linear output. tensorflow open source software library numerical computation youngest respect others machine learning frameworks. originally developed researchers engineers google brain team purpose encourage research deep architectures. nevertheless environment provides large tools suitable several domains numerical programming. computation conceived concept data flow graphs. nodes graph represent mathematical operations graph edges represent tensors core package written provides well documented python api. main characteristic symbolic approach allows general deﬁnition forward models leaving computation correspondent derivatives entirely environment itself. tensorflow model easily written using python intuitive object-oriented programming language. python distributed open-source license commercial too. oﬀers nice integration many programming languages provides extended standard library includes numpy python runs windows linux/unix operative systems. data flow graph leverage parallel computational power multi-core even clusters gpus dynamic numerical computations conceived directed graph node represents mathematical operation edges describe input/output relation nodes. variable symbolic objects designed represent parameters. exploited compute derivatives symbolical level general must explicitly initialized session. optimizer component provides methods compute gradients loss function apply back-propagation variables. collection available tensorflow implement classic optimization algorithms. information download installation python tensorflow available oﬃcial webpages. notice dedicated procedure must followed installation. it’s worth quick remark cuda versions. indeed versions oﬃcially supported installation could straightforward versions preceding moreover registration accelerate computing developer program required install package cudnn mandatory enable support. again data deﬁned matrices containing input data correspondent target called respectively. data deﬁned list numpy array. used placeholder actually deﬁne type dimensionality. tensorflow provides placeholders symbolic variables representing data computation. placeholders object initialized given type dimensionality suitable represent desired element. case deﬁne object respectively input data target python webpage https//www.python.org/ tensorflow webpage https//www.tensorflow.org/ https//developer.nvidia.com/accelerated-computing-developer description network depends essentially architecture parameters since parameters estimated deﬁned variabile model whereas architecture determined conﬁguration symbolic operations. -layers deﬁne matmul function performs tensors multiplication. variable constructor class variable. needs initialization value must tensor. function random_uniform returns tensor speciﬁed shape ﬁlled valued picked uniform distribution speciﬁed values. module contains common activation functions taking input tensor evaluating non-linear transferring component-wise tensorflow provides functions perform common operations tensors. function reduce_sum example reduces tensor dimension summing along speciﬁed dimension. train module provide common optimizers employed training process. previous code chose gradient descent algorithm optimize network parameters respect penalty function deﬁned cost using learning rate equal point variables still initialized. whole graph exist symbolic level instantiated creating session. example placeholders assigned elements moment. speciﬁcally initialize_all_variables creates operation running variables initializer. function session creates instance class session correspondent method moves ﬁrst time data flow graph cpu/gpu allocates variables ﬁlls initial values. sess.run calling runs operations previously deﬁned ﬁrst argument case optimizer step second argument dictionary feed_dict pairing placeholder correspondent input. function used also evaluate cost epochs. section show -layer order face mnist classiﬁcation problem well known data handwritten characters recognition. extensively used test compare general machine learning algorithms computer vision methods. data provided pixels images handwritten digits. training test sets contain respectively instances. files .zip available oﬃcial site together list performance achieved common algorithms. show setting standard -layer units hidden layer represented figure since architecture reported oﬃcial website obtained results easily compared. input reshaped feed network -dimensional vector elements. image originally represented matrix containing grayscale value pixels normalized output elements prediction vector since labels element expressed one-hot encoding binary vector null bits position indicating class. activation penalty functions diﬀerent within diﬀerent environments provide overview diﬀerent approaches. data downloaded oﬃcial mnist site matlabfunctions available stanford university site extract data ﬁles organize inputsize –by–numberofsamples matrix form. extraction routines reshape normalizes data unzipped data functions folder straightforward upload images matlabworkspace loadmnistimages function training test grouped matrix evaluate performance provided test training. correspondent labels loaded grouped similar function loadmnistlabels original labels provided -dimensional vector containing number according correspondent digit. one-hot encoding target matrix whole dataset generated exploiting matlabfunction indvec already said command creates -layer hidden layer units hyperbolic tangent activation whereas output function computed softmax. penalty function cross-entropy criterion. case change data splitting data used test comes original test prevent samples among train validation test set. step completely customizable method dividefcn ﬁelds options divideparam. divideind method picks data according provided indexes data matrix figure left performance mnist dataset training -layer hidden units. training stopped epochs validation checking. right report misclassiﬁed samples. network reaches classiﬁcation accuracy test figure show misclassiﬁed digits indicating original label predicted one. visualization obtained matlab function image listing show evaluate classiﬁcation accuracy confusion matrix data give coherent results respect reported oﬃcial site architecture already said torch environment provides tools machine learning included routines download prepare common datasets. wide overview useful tutorials demos introduction common methods found dedicate webpage including loading popular datasets section. here link mnist loader page available data informations correspondent mnist package installation provided. installation data loaded code listing data loaded table named train digits expressed numberofsamples-by-by- tensor type bytetensor stored ﬁeld data expressing value gray levels pixel targets stored vector expressing digits labels ﬁeld label. convert data doubletensor format again normalize input features values reshape original tensor input vector. labels incremented since crossentropycriterion accepts target indicating class avoiding null values last perform random shuﬄing data order prepare train/validation splitting function code build proposed -layer model reported listing network assembled sequential container using time relu activation hidden layer whereas output penalty functions used previous section network training deﬁned similar proposed listing width training data time convenient minibatch training showed listing moreover also deﬁne early stopping criterion stops training penalty validation start increase preventing overﬁtting problems. training function expects inputs network criterion evaluate loss training validation data organized table ﬁelds data label deﬁned listing optional conﬁguration table options provided indicating number training epochs learning rate mini-batch size number consecutive increasings validation loss causes preventive training stop worth remark function split deﬁned tensor class used divide data batches stored indexed table. training vector containing loss evaluated epoch returned. validation loss computed help function evaluate splits computation smaller batches preventing heavy computations number parameters samples large. case training stopped validation criterion epoch producing classiﬁcation accuracy test figure report trend error training. since general useful visualize confusion matrix figure show obtained function imagesc package gnuplot give color matrix passed input. even tensorflow environment makes available many tutorials preloaded dataset including mnist. fast introductive section beginners found oﬃcial page. however show functions download import dataset simple way. indeed data directly loaded help proposed function init_weights deﬁne parameters learned computation. represent respectively weights biases hidden layer. similarly respectively weights biases output layer. need deﬁne cost function optimizer. however time square euclidean norm regularizer global cost function composed cross-entropy plus regularization term scaled coeﬃcient beginning session tensorflow moves data flow graph cpus gpus initializes variables. tensorflow provides function next_batch mnist class randomly extract batches speciﬁed size. data split shuﬄed partitions indicated size means implicit counter function slide along batches calling allowing fast implementation mini-batch gradient descent method. case used loop scan across batches executing training step extracted data iteration. whole training processed loss training validation test sets computed. operations repeated loop whose steps representing epochs training. loop stops maximum number epochs reached network start overﬁt training data. early stopping implemented checking validation error training stopped improvements obtained ﬁxed number consecutive epochs maximum number epochs learning rate must advance. prediction accuracy trained model evaluated test similar presented problem. time need exploit argmax function convert one-hot encoding correspondent labels. section introduce convolutional neural networks important powerful kind learning architecture widely diﬀused especially computer vision applications. currently represent state algorithm image classiﬁcation tasks constitute main architecture used deep learning. show build train structure within proposed frameworks exploring general functions setting experiments mnist pointing important features. main function classes build train cnns matlabare contained neural network toolboxtm statistic machine learning toolboxtm nevertheless parallel computing toolboxtm becomes necessary too. moreover train network cuda -enabled nvidia required. again focus much main theoretical properties general issues cnns main implementation instruments. network stored matlabobject kind layer sequentially composed diﬀerent sub-layers. list available options always explored interactive help options command window. common convolutional objects deﬁned functions imageinputlayer creates layer deals original input image requiring argument vector expressing size input image given height width number channels; convolutiondlayer deﬁnes layer convolutional ﬁlters whose size speciﬁed ﬁrst argument whereas second argument speciﬁes total number ﬁlters; main options ’stride’ indicates sliding step means pixel directions) ’padding’ whose appear namevalue pairs; listing show basic face pixels images mnist showed figure global structure network deﬁned vector composed diﬀerent modules. initialized object layer named cnnm visualized command window giving data mnist loaded stanford routines showed section time input images required tensor size numberofsamples–by–channels–by–height –by–width hence modify provided function loadmnistimages reshape data showed ﬁrst line following code second command used convert targets categorical matlabvariable kind nominal required train network exploiting function trainnetwork. also require input object specifying training options instantiated function trainingoptions. command selects stochastic gradient descent algorithm using momentum. many optional parameters available additional parameter namevalue pairs notation again. common trainednet contain trained network trainop training variables. training starts command line printing useful variables indicating training state similar assuming ytest -dimensional vector class labels classiﬁcation accuracy calculated meaning boolean comparing computed predictions vector predictions. useful built-in function compute confusion matrix provided requiring nominal labels converted categorical predictions. setting ﬁnal classiﬁcation accuracy test close dealing cnns important type object introduced convolutional ﬁlters. want deep theoretical explanations however sometimes could useful visualize composition convolutional ﬁlters order idea kind features ﬁlter detects. filters represented weights convolution stored layers weights tensor size height –by–width–by–numberofchannels–by–numberoffilters. case example ﬁlters ﬁrst convolutional accessed notation cnnm.layer.weights. figure show conﬁguration training suitable visualization). within torch environment straightforward deﬁne package presented section indeed network stored container composed speciﬁc modules. again give short list description common ones integrated standard transfer functions standard linear layer introduced before spatialconvolution deﬁnes convolutional layers required arguments number input channels number output channels height width ﬁlters. step-size zero-padding height width optional parameters network images lower resolution. general architecture diﬀer deﬁned section ﬁrst layers. proposed network generated code listing whereas figure show global architecture. training function deﬁned listing optimization stops validation check epochs producing classiﬁcation accuracy give idea diﬀerence obtained performances reduction information expressed input images. figure show ﬁlters size extracted ﬁrst convolutional layer. weights obtained function parameters package return indexed table storing weights layer. case ﬁrst element table contains tensor dimension representing weights ﬁlters. visualization generated exploiting function imagesc package gnuplot reshaping line format. section show build face mnist data using tensorflow. ﬁrst step import libraries mnist dataset deﬁne main variables. additional package required numpy matrices computations matplotlib.pyplot visualization issues. following function deﬁne convolution pooling. vector strides speciﬁes ﬁlter sliding window move along dimension. vector ksize speciﬁes dimension sliding window. padding option ’same’ automatically adds empty pixels allow convolution centered even boundary pixels. order deﬁne model start reshaping input original size i.e. sample represented matrix pixels. deﬁne ﬁrst convolution layer computes features using ﬁlters. finally perform relu activation ﬁrst pooling step. point network returns feature maps reshaped vectors given input last fully connected linear layer. linear layer equipped hidden units relu activation functions. following piece code report optimization process deﬁned cnn. standard case organized loop. time chose adam gradient-based optimization function adamoptimizer. epoch performs training step mini-batches extracted dedicated function next_batch introduced section time computations within interactivesession. diﬀerence regular session interactivesession sets default session building allowing variables without needing constantly refer session object. instance method eval implicitly session operations. section would like outline overall picture across presented environments. even table provide scoring based features thought mainly relevant machine learning software development work would like bound analysis poor evaluation. instead hope propose useful guideline help people trying approach anns machine learning general order orientate within environments depending personal background requirements. complete statistically relevant comparisons found summarize help speed single global task developing. ﬁrst give general description environment compare pros cons speciﬁc requirements. carry indicative numerical analysis computational performances diﬀerent tasks could also topic comparison discussion. programming language intuitive software provides complete package allowing user deﬁne train almost kind anns architecture without writing single line speciﬁc code. code parallelization automatic integration cuda straightforward too. available builtfunctions customizable optimized providing fast extended setting experiments easy access variable network in-depth analysis. however enlarging integrating matlabtools require advanced knowledge environment. could drive user start rewriting code beginning leading general decay computational performances. features make perfect statistical analysis toolbox maybe slow developmental environment. results sometimes heavy handled calculator hand user-friendly provides best graphical data visualization. documentation complete well organized within oﬃcial site. programming language sometimes results little tricky supposed faster among languages. provides needed cuda integrations parallelization automatic. module-based structure allows ﬂexibility anns architecture relatively easy extend provided packages. also powerful packages general require acquire expertise achieve conscious handling. torch could easily used prototyping environment speciﬁc generic algorithms testing. documentation spread torch github repository sometimes solve speciﬁc issues could immediate. employment programming language dynamic python makes code scripting light user. parallelization automatic exploiting graph-structure computation easy take advantage computing. provides good data visualization possibility beginners access ready packages even treated document. power symbolic computation involves user forward step whereas backward step entirely derived tensorflow environment. ﬂexibility allows fast development users level expertise. look example webpage http//hammerprinciple.com/therighttool skip treatment optim provides various gradient descent back-propagation procedure development flexibility again matlabis penalized forces medium users become specialized language integrate provided tools write proper code general slow software development programming language integration parallelization function customizability symbolic calculus network structure customizability data visualization installation compatibility built-in function availability language performance development flexibility license table compare running times diﬀerent tasks analyzing advantages diﬀerences cpu/gpu computing. results averaged trials carried machine intel xeon .ghz cores geforce memory. debian gnu/linux test standard gradient descent procedure varying network architecture batches size samples batch full batch) hardware architecture proposed figure performances obtained trying optimization procedures similar possible. practice diﬃcult reply speciﬁc optimization techniques exploited matlabbuilt-in toolboxes. skip case second architecture torch huge computational time obtained ﬁrst architecture. miss case anns architecture matlabcase since training function ’trains’ supported computing matter fact could uncommon case study report results best completeness. skip full batch trials large memory requirement table averaged time running epochs training diﬀerent architectures mnist data within presented environments. architectures built using relu activation softmax output function cross-entropy penalty.", "year": 2017}