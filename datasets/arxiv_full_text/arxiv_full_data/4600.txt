{"title": "PEGASUS: A Policy Search Method for Large MDPs and POMDPs", "tag": ["cs.AI", "cs.LG"], "abstract": "We propose a new approach to the problem of searching a space of policies for a Markov decision process (MDP) or a partially observable Markov decision process (POMDP), given a model. Our approach is based on the following observation: Any (PO)MDP can be transformed into an \"equivalent\" POMDP in which all state transitions (given the current state and action) are deterministic. This reduces the general problem of policy search to one in which we need only consider POMDPs with deterministic transitions. We give a natural way of estimating the value of all policies in these transformed POMDPs. Policy search is then simply performed by searching for a policy with high estimated value. We also establish conditions under which our value estimates will be good, recovering theoretical results similar to those of Kearns, Mansour and Ng (1999), but with \"sample complexity\" bounds that have only a polynomial rather than exponential dependence on the horizon time. Our method applies to arbitrary POMDPs, including ones with infinite state and action spaces. We also present empirical results for our approach on a small discrete problem, and on a complex continuous state/continuous action problem involving learning to ride a bicycle.", "text": "approaches pomdp either tories erative arbitrary model these roughly mentation internal provide needs processes partially observable mdps mains value q-functions cated difficult simple well. observation direct markov decision tran­ sition probabilities bution upon taking action state discount factor; reward function rmax· sake concreteness less otherwise stated hypercube. terministic tensions measurable since interested problem sume given model mdp. much pre­ vious work studied generative takes input state-action according paper assume stronger deterministic fixed -pair uniform distributed according dis­ transition words draw sample tribution fixed need draw uni­ formly take sample. call model deterministic mdp. since deterministic simulative model allows simu­ late generative generative models ever computer models. consider also provide generative procedure takes sand makes calls random number generator procedure simulative istic simulative interface erative \"resets\" readily =sup goal find policy close opt. note framework family consists pects state. attention policies restriction free policies. ables\" process limited-memory belief examine simple examples sim­ pair ulative state-action states choose real number normal distribution tive distribution choose fact probability transition ulative e.g. pomdps using essentially els. however representing decide later indeed impact performance \"simpler\" generally executing deter­ mined gif) function varying whole family func­ tions urrlfrr gp)} mapping successor thought able pomdp though since definition model depend particular simulative chosen also i-th coordinate i-th coordinate corresponding coordinate ping thus captures corresponding reward given observes sequence states obtains distribution would generated corresponding lows that corresponding also implies best possible expected returns same vapnik-chervonenkis states well defined. shatters realize action combina­ tions them size largest keams suffices shattered give following that. viewed space original formed pomdp evaluating also akin generating empirical average \"fixed\" ference -rr. evaluating \"reused\" used scenarios search policies policy search method pegasus policy evaluation-of­ goodness deterministic optimize dard optimization action space continuous smoothly parameterized differentiable ties differentiable tives used optimize often discontinuous goal region elsewhere. using transformation finite tially \"complexity bound given theorem dependence size state space \"complexity\" pomdp's transitions vc-dimension uniform pendently theorem therefore results gence occurs long hypothesis dimension underlying ways choose action itively nite action space; also reasonable policy classes true simple policy classes gence certainly enjoy uniform convergence. case show. state main theorem optimizing theorem pomdp state spaces possibly class deterministic policy model dfor tions. lipschitz family bound reward lipschitz tion also lipschitz lipschitz given pegasus using scenarios pass directions. actions random direction class small enough exhaustively mization exhaustive picking horizon ported problem averages deterministic simulative model ...................................................................................................................................................... -previously simulative ministic -pair uniform random variable maps uniform random variable. istic simulative that much \"complex\" reader's intuition formal measures predict shown pomdp transformed \"equivalent\" determin­ istic. state distribution estimate policy search optimizing established formly good experimental working well. also straightforward methods results counted reward £-mixing", "year": 2013}