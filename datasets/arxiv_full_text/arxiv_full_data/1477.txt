{"title": "Ask Your Neurons: A Deep Learning Approach to Visual Question Answering", "tag": ["cs.CV", "cs.AI", "cs.CL"], "abstract": "We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Ask Your Neurons, a scalable, jointly trained, end-to-end formulation to this problem.  In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language inputs (image and question). We provide additional insights into the problem by analyzing how much information is contained only in the language part for which we provide a new human baseline. To study human consensus, which is related to the ambiguities inherent in this challenging task, we propose two novel metrics and collect additional answers which extend the original DAQUAR dataset to DAQUAR-Consensus.  Moreover, we also extend our analysis to VQA, a large-scale question answering about images dataset, where we investigate some particular design choices and show the importance of stronger visual models. At the same time, we achieve strong performance of our model that still uses a global image representation. Finally, based on such analysis, we refine our Ask Your Neurons on DAQUAR, which also leads to a better performance on this challenging task.", "text": "fig. approach neurons question answering recurrent neural network using long short term memory answer question image feed both image question lstm. question encoded generate answers answer generation phase previously predicted answers lstm symbol predicted. section details. advances natural language processing image understanding complex demanding tasks become within reach. take advantage recent developments push state-of-the-art answering natural language questions real-world images. task unites inference question intent visual scene understanding answer generation task. recently architectures based idea layered endto-end trainable artiﬁcial neural networks improved state across wide range diverse tasks. prominently convolutional neural networks raised image classiﬁcation tasks long short term memory networks dominating performance range sequence prediction tasks machine translation abstract propose deep learning approach visual question answering task machines answer questions real-world images. combining latest advances image representation natural language processing propose neurons scalable jointly trained endto-end formulation problem. contrast previous efforts facing multi-modal problem language output conditioned visual natural language inputs evaluate approaches daquar well dataset also report various baselines including analysis much information contained language part only. study human consensus propose novel metrics collect additional answers extend original daquar dataset daquar-consensus. finally evaluate rich design choices encode combine decode information proposed deep learning formulation. recently trends employing neural architectures combined fruitfully methods generate image video descriptions conditioned visual features stem deep learning architectures employ recurrent neural network approaches produce descriptions. push boundaries explore limits deep learning architectures propose architecture answering questions images. contrast prior work task needs conditioning language well visual input. modalities interpreted jointly represented answer depends inferred meaning question image content. rich body work natural language understanding addressed textual question answering tasks based semantic parsing symbolic representation deduction systems also seen applications question answering images evidence deep architectures indeed achieve similar goal motivates work seek end-to-end architectures learn answer questions single holistic model. propose neurons approach question answering recurrent neural network core. overview given figure image information encoded convolutional neural network question together visual representation long short term memory network. system trained produce correct answer question image. lstm trained jointly end-toend starting words pixels. outline. section present novel approach based recurrent neural networks challenging task answering questions images presented originally malinowski approach combines lstm end-to-end architecture predicts answers conditioning question image. section shows proposed approach doubles performance compared prior symbolic approach task. collect additional data study human consensus task propose metrics sensitive effects provide baseline asking humans answer questions without observing image. demonstrate variant system also answers question without accessing visual information beats human baseline. also frame multimodal approach answer questions images combines lstm special instance encoder-decoder framework. modular perspective shown section allows study different design choices large scale visual question answering dataset vqa. section shows analysis leads improved visual question answering architecture. deeper visual encoding together several important design choices lead model achieves stronger performance daquar datasets. since proposed challenge ﬁrst methods answering questions real-world images frequently also referred visual question answering numerous follow works appeared. following ﬁrst discuss related tasks subtasks early approaches tackle broader visual turing test datasets proposed finally discuss relations work. component answer questions images extract information visual content. since proposal alexnet convolutional neural networks become dominant successful approaches extract relevant representation image. cnns directly learn representation image data trained large image corpora typically imagenet interestingly models pre-trained imagenet typically adapted tasks. work evaluate well dominant successful models adapted visual turing test. speciﬁcally evaluate alexnet googlenet resnet models reportedly achieve accurate results imagenet dataset hence arguably serve increasingly stronger models visual perception. important component answer question image understand natural language question means building representation variable length sequence words ﬁrst approach encode words question bag-of-words hence ignoring order sequence words. another option similar image encoding pooling handle variable length input finally recurrent neural networks methods developed directly handle sequences shown recent success natural language tasks machine translation work investigate bag-of-words variants encode question. task describing visual content like still images well videos successfully addressed combination encoding image cnns decoding i.e. predicting sentence description achieved using model ﬁrst gets observe visual content trained afterwards predict sequence words description visual content. work extends idea question answering formulate model trained either generate classify answer based visual well natural language input. dealing natural language input involve association words meaning. often referred grounding problem particular meaning associated sensory input. problems historically addressed symbolic semantic parsing techniques recent trend machine learning-based approaches associations. approaches partially enabled recently proposed larger scale datasets providing phrases referential expressions associated corresponding image location. answering questions images interpreted ﬁrst grounding question image predicting answer. approach thus similar latter approaches enforce evaluate particular representation meaning language image modality. treat latent leave joint training approach establish appropriate hidden representation link visual textual representations. answering purely textual questions studied community state techniques typically employ semantic parsing arrive logical form capturing intended meaning infer relevant answers. recently success previously mentioned neural sequence models namely rnns carried task speciﬁcally iyyer dependency-tree recursive instead lstm reduce question-answering problem classiﬁcation task. weston propose different kind network memory networks used answer questions short stories. work parts story embedded different memory cells next network trained attend relevant cells based question decode answer that. similar idea also applied question answering images instance yang recently large number architectures proposed approach visual turing test mainly tackle particularly important subtask tests machines regarding abilities answer questions real-world images. methods range symbolic neural based. also architectures combine symbolic neural paradigms together. approaches explicit visual representation form bounding boxes surrounding objects interest global full frame image representation soft attention mechanism. others external knowledge base helps answering questions. symbolic based approaches. ﬁrst work towards visual turing test present question answering system based semantic parser varied human question-answer pairs. although ﬁrst attempt handle question answering daquar despite introspective beneﬁts rule-based approach requires careful schema crafting scalable ﬁnally strongly depends output visual analysis methods joint training model possible. limitations community rather shifted towards either neural based combined approaches. deep neural approaches full frame cnn. several contemporary approaches global image representation i.e. encode whole image cnn. questions encoded contrast symbolic based approaches neural based architectures offer scalable joint end-to-end training liberates ontological commitment would otherwise introduced semantic parser. moreover approaches ‘hard’ conditioned visual input therefore naturally take advantage different language biases question answer pairs interpret learning common sense knowledge. attention-based approaches. following proposed spatial attention image description yang saenko chen shih fukui predict latent weighting spatially localized images features based question. weighted image representation rather full frame feature representation used basis answering question. contrast previous models using attention dynamic memory networks ﬁrst pass spatial image features bi-directional captures spatial information neighboring image patches next retrieve answer recurrent attention based neural network allows focus subset visual features extracted ﬁrst pass. aninteresting direction taken ilievski state-of-the-art object detector classes extracted words question. contrast attention mechanisms approach offers focused question dependent hard attention. answering external knowledge base. argue approach ﬁrst represents image intermediate semantic attribute representation next query external knowledge sources based prominent attributes relate question. help external knowledge bases approach captures richer semantic representation world beyond directly contained images. compositional approaches. different direction taken andreas predict important components answer question natural language parser. components mapped neural modules composed deep neural network based parse tree. question induces different network modules trained jointly across questions. work compares malinowski fritz exploiting explicit assumptions compositionality natural language sentences. related visual turing test malinowski fritz also combined neural based representation compositionality language text-to-image retrieval task. dynamic parameters. image recognition network recurrent neural network dynamically change parameters visual representation based question. precisely parameters second last layer dynamically predicted question encoder network changing question. question encoding image encoding pre-trained network learns parameter prediction image-question-answer triples. datasets driving force recent progress visual question answering. large number visual question answering datasets recently proposed. ﬁrst proposed datasets daquar contains thousands manually annotated question-answer pairs indoor scenes dataset originally contained single answer question work extend dataset collecting additional answers questions. captures uncertainties evaluation. evaluate approach dataset discuss several consensus evaluation metrics take extended annotations account. parallel work geman developed another variant visual turing test. work however focuses sequences yes/no type questions provides detailed object-scene annotations. shortly introduction daquar three large-scale datasets proposed. based ms-coco annotated images chinese question answer pairs together corresponding english translations. taken advantage existing annotations purpose image description generation task transform question answer pairs help hand-designed rules syntactic parser procedure approximately generated question answer pairs. finally currently popular large scale dataset question answering images approximately questions visual content real-world images similarly consensus idea provides answers image. purpose challenge test answers publicly available. perform part experimental analysis paper dataset examining different variants proposed approach. datasets simpliﬁed evaluation even introducing visual madlibs multiple choice question answering ﬁlling blanks task. task question answering architecture choose four provided answers given image prompt. formulating question answering task wiped ambiguities answers simple accuracy metric used evaluate different architectures task. task requires holistic reasoning images despite simple evaluation remains challenging machines. visualw extends canonical question answer pairs additional groundings objects appearing questions answers image annotating correspondences. contains natural language answers also answers require locate object similar task explicit grounding discussed above. visualw builds question answer pairs based visual genome dataset contains questions. contrast others daquar collected unconstrained question answer pairs visual genome focuses called what where when answered natural language answer. additional question requires bounding location answer. similarly visual madlibs visualw also contains multiple-choice answers. related visual turing test chowdhury proposed collective memories xplore-m-ego dataset images natural language queries media retrieval system. work focuses user centric dynamic scenario provided answers conditioned questions also geographical position questioner. moving asking questions images questions video enhances typical questions temporal structure. propose task requires blanks captions associated videos. task requires inferring past describing present predicting future diverse video description data ranging cooking videos videos movies tapaswi propose movieqa requires understand long term connections plot movie. given difﬁculty data works provide multiplechoice answers. kind extend introducing general modular encoder-decoder perspective encapsulates different neural approaches. next broaden original analysis done daquar analysis different neural based approaches showing importance getting details right together beneﬁts stronger visual encoder finally transfer lessons learnt daquar showing signiﬁcant improvement challenging task represent vector parameters learn answers. question sequence words i.e. t-th word question encoding question mark question. following describe represent details. scenario multiple word answers consequently decompose problem predicting answer words ﬁnite vocabulary number answer words given question image. approach named neurons propose tackle problem follows. predict multiple words formulate problem predicting sequence words vocabulary extra token indicates answer sequence points question fully answered. thus formulate prediction procedure recursively ˆat− ˆat−} previous words beginning approach given answer word far. approach terminated evaluate method solely based predicted answer words ignoring extra token ensure uniqueness predicted answer words want predict answer words prediction procedure trivially changed maximizing ˆat−. however practice algorithm learns predict previously predicted words. following ﬁrst present neurons models multi-word answers single recurrent network question image encoding answer prediction present general modular framework question image encoders well answer decoder modules shown figure figure feed approach neurons question sequence words. since problem formulated variable-length inputoutput-sequence decide model parametric distribution neurons recurrent neural network softmax prediction layer. precisely neurons deep network built long-short term memory decide lstm recently shown effective learning variable-length sequence-to-sequence mapping question answer words represented one-hot vector encoding embedded lower dimensional space using jointly learnt latent linear embedding. training phase augment question words sequence corresponding ground truth answer words sequence i.e. test time prediction phase time step augment previously predicted answer words ˆa..t i.e. means question previous answer words encoded implicitly hidden states lstm latent hidden representation learnt. encode image using provide every visualized detail figure lstm unit takes input vector time step predicts output word equal latent hidden state discussed above linear embedding corresponding answer word contrast simple unit lstm unit additionally maintains memory cell allows learn long-term dynamics easily signiﬁcantly reduces vanishing exploding gradients problem precisely lstm unit described zaremba denotes element-wise multiplication. weights biases network learnt jointly cross-entropy loss. conceptually shown figure equation corresponds input gate equation input modulation gate equation forget gate determines much keep previous memory state. figures suggest output predictions occur question mark excluded loss computation model penalized solely based predicted answer words. matrix one-hot binary vector word exactly pointing place ’word’ vocabulary encode ordering words question especially questions swapped arguments spatial prepositions become indistinguishable i.e. ψbow ψbow sentence representation. fig. reﬁned neurons architecture answering questions images includes following modules visual question encoders answer decoder. multimodal embedding combines encodings joint space decoder decodes from. section details. previous section described achieve visual question answering single recurrent network question image encoding answering. section abstract away particular design choices taken describe modular framework question encoder combined visual encoder order produce answers answer decoder modular representation allows systematically investigate evaluate range design choices different encoders multimodal embeddings decoders. main goal question encoder capture meaning question write encoder range structured ones like semantic parser used malinowski fritz liang explicitly model compositional nature question orderless bag-of-word approaches merely compute histogram question words work investigate encoders within spectrum compatible proposed deep learning approach recurrent question encoders lstm assume temporal ordering questions well aforementioned bow. convolutional neural network convolutional neural networks proposed encode language since shown fast compute result good accuracy. since consider larger context arguably maintain structure model long term dependencies recurrent neural networks. figure depicts architecture similar yang convolves word embeddings three convolutional kernels length sake clarity show kernels ﬁgure. either learn jointly whole model glove experiments. call architecture kernel lengths views cnn. kernel’s outputs temporarily aggregated ﬁnal question’s representation. either pooling recurrent neural network accomplish step. second important component encoder-decoder architectures visual representation. convolutional neural networks become state-of-the-art framework provide features images. typical protocol using visual models ﬁrst pre-train imagenet dataset large scale recognition dataset next input rest architecture. fine-tuning weights encoder task hand also possible. experiments chronologically oldest architecture fully trained imagenet caffe implementation alexnet well recently introduced deeper networks caffe implementations googlenet recent extremely deep architectures facebook implementation layered residualnet seen experiments section strong visual encoder plays important role overall performance architecture. presented neural question encoders transform linguistic question vector space. similarly visual encoders encode images vectors. multimodal fusion module combines vector spaces another vector based answer decoded. question representation representation image. function embeds vectors. work investigate three multimodal embedding techniques concatenation element-wise multiplication summation. since last techniques require compatibility number feature components concatenation element-wise multiplication summation fusion techniques respectively. equation decompose matrices equation element-wise multiplication. similarity equation equation interesting latter former weight sharing additional decomposition wve. answer words generation. last component architecture answer decoder. inspired work image description task uses lstm decoder shares parameters encoder. section benchmark method task answering questions images. compare different variants proposed model prior work section addition section analyze well questions answered without using image order gain understanding biases form prior knowledge common sense. provide human baseline task. section discuss ambiguities question answering tasks analyze introducing metrics sensitive phenomena. particular wups score extended consensus metric considers multiple human answers. material available project webpage start evaluation neurons full daquar dataset order study different variants training conditions. afterwards evaluate reduced daquar additional points comparison prior work. results full daquar. table shows results neurons method full images question-answer pairs available test time. addition evaluate variant trained predict single word well variant visual features note however single word refers training procedure. methods table evaluated full daquar dataset test time also contains multi-word answers. comparison prior work observe strong improvements points accuracy wups scores note that achieve improvement despite fact published number available comparison full uses ground truth object annotations puts method disadvantage. improvements observed train single word answer doubles accuracy obtained prior work. attribute joint training language visual representations dataset bias answers contain single word. analyze effect figure show performance approach dependence number words answer performance single word variants one-word subset shown horizontal lines. although accuracy drops rapidly longer answers model capable producing signiﬁcant number correct words answers. single word variants edge single answers beneﬁt dataset bias towards type answers. quantitative results single word model one-word answers subset daquar shown table made substantial progress compared prior work still points margin human accuracy wups score experimental protocol. evaluate approach section daquar dataset provides human question answer pairs images indoor scenes follow evaluation protocol providing results accuracy wups score experiments full dataset well proposed reduced restricts output space object categories uses test images. addition also evaluate methods different subsets daquar word answers present. default hyper-parameters lstm models ﬁrst pre-trained imagenet dataset next randomly initialize train last layer together lstm network task. step crucial obtain good results. explored layered lstm model consistently obtained worse performance. pilot study found googlenet architecture consistently outperforms alexnet architecture model task model. wups scores. base experiments consensus metrics wups scores metric generalization accuracy measure accounts word-level ambiguities answer words. instance ‘carton’ ‘box’ associated similar concept hence models strongly penalized type mistakes. formally order analyze ﬁnding collected human baseline human answer image asked participants answer daquar questions without looking images. turns humans guess correct answer cases exploiting prior knowledge common sense. interestingly best question-only model outperforms human baseline percent points. substantial number answers plausible resemble form common sense knowledge employed humans infer answers without seen image. observe many cases inter human agreement answers given image question also reﬂected human baseline performance question answering task study analyze effect extending dataset multiple human reference answers section proposing measure inspired work psychology handles agreement section well conducting additional experiments section fig. question-only neurons multi word models evaluated different subsets daquar. consider word subsets. blue horizontal lines represent single word variants evaluated answers exactly word. results reduced daquar. order provide performance numbers comparable proposed multiworld approach malinowski fritz also method reduced object classes images question-answer pairs test time. table shows neurons also improves reduced daquar achieving accuracy wups substantially outperforming malinowski fritz percent points accuracy wups points. similarly previous experiments achieve best performance using single word variant method. note reported accuracy wups wups respectively. however anvariant reduced daquar dataset multiple word answers removed. roughly accounts original reduced daquar dataset. order study much information already contained questions train version model ignores visual input. results shown table table question-only best question-only models compare well terms accuracy best models include vision. latter order study effects consensus question answering task asked multiple participants answer question daquar dataset given respective image. follow scheme original data collection effort answer words numbers. impose restrictions answers. extends original data average test answers image fig. study inter human agreement. x-axis consensus least half consensus full consensus results left consensus whole data right consensus test data. question collected in-house annotators. annotators ﬁrst tested english proﬁciency would able accomplish task. instructed verbally given image entered answer given question text editor. regular quality checks performed random question-answer-image triplets. refer dataset daquar-consensus. i-th question answer generated architecture k-th possible human answer corresponding k-th interpretation question. answers sets words membership measure instance call metric average consensus metric since limits approaches total number humans truly measure inter human agreement every question. consensus. average consensus metric puts weights mainstream answers summation possible answers given humans. order measure result least human agreement propose consensus metric replacing averaging equation operator. call metric consensus suggest using using multiple reference answers daquar-consensus show detailed analysis inter human agreement. figure shows fraction data answers agree available questions least available questions agree observe majority data partial agreement even full disagreement possible. split dataset three parts according criteria agreement agreement full agreement evaluate models splits subsets stronger agreement achieve substantial gains points accuracy full subset agreement respectively. splits seen curated versions daquar allows studies factored ambiguities. aforementioned average consensus metric generalizes notion agreement encourages predictions agreeable answers. hand consensus metric desired effect providing optimistic evaluation. table shows application measures data models. moreover table shows applied human answers test time captures ambiguities interpreting questions improving score human baseline malinowski fritz cooperates well wups takes word ambiguities account gaining higher score. show predicted answers different architecture variants tables chose examples highlight differences neurons questiononly. multiple words approach table otherwise single word model shown. despite failure cases question-only makes reasonable guesses like predicting largest object could table object could found pillow doll. method answers correctly large part challenge spatial relations account substantial part daquar remain challenging. errors involve questions small objects negations shapes training data points aforementioned cases contribute mistakes. table shows examples failure cases include strong occlusion possible answer captured ground truth answers unusual instances although question-only ignores image still able make reasonable guesses exploiting biases captured dataset. biases interpret type common sense knowledge. instance kettle often sits oven cabinets usually brown chair typically placed front table commonly keep photo cabinet hand biases hardly related common sense knowledge. instance answer question many bottles desk? clock answer question what black white object right brown board?. effect analysed figure data point plot represents correlation question predicted answer words question-only model versus correlation human answers despite reasonable guesses question-only architecture neurons predicts average better answers instance table questiononly model incorrectly answers question section ﬁrst extend experiments baseline methods next guided ﬁndings shown section show results reﬁned model context larger body results daquar. baseline methods. gain better understanding effectiveness neural-based approach relate obtained results baseline techniques. similar baselines also introduced antol although target either different dataset anvariant daquar. constant technique uses frequent answer training answer every question test set. constant question type ﬁrst break questions categories frequent answer category answer every question category test time. table provides details chosen categories. look-up table builds hash textual question frequent answer question training time. test time method looks answer question hashmap. question exists popular answer question provided otherwise ‘empty’ answer given. addition also remove articles ‘the’ questions. however brings miimprovement. finally experiment nearestneighbor methods. methods rely bag-of-words representation questions every question word encoded glove following call representation semantic space. nearest neighbor question-only searches test time similar question semantic space training set. takes answer corresponds question. nearest neighbor inspired similar baseline introduced antol test time ﬁrst search similar questions semantic space available training set. next form candidate images correspond aforementioned questions. last step choose answer associated best match visual space. latter done cosine similarity global representations test image every candidate image. experiment several representations surprise little performance difference them. decide googlenet results slightly better. baseline results. constant shows dataset biased w.r.t. frequent answer. answer turns number also explain good performance neurons many question subset daquar. constant question type shows question types provide quite strong clues answering questions. kaﬂe kanan take advantage clues bayesian hybrid models next baseline looktable seen extreme case constant question type model. also gives surprisingly good results even though cannot answer novel questions. result also conﬁrms intuitions question-only biases important ‘question answering images’ datasets. finally next nearest-neighbor baselines show visual representation still helps. state-of-the-art. based analysis also applied improved model daquar signiﬁcantly outperform malinowski presented section experiments ﬁrst choose last training validation order determine number training epochs next train model epochs. evaluate model variants daquar data points subset containing single word answers consists original dataset. table shows model vision language glove residual sums visual question representations outperforms model malinowski accuracy wups wups respectively. shows important strong visual model well aforementioned details used training. likewise conclusions also observing improvement attention based models section analyses original architecture daquar dataset section analyze different variants design choices neural question answering large-scale visual question answering dataset currently largest popular visual question answering dataset human question answer pairs. following describing experimental setup ﬁrst describe several experiments examine different variants question encoding looking language input predict answer then examine full model table start regular expression table deﬁne question type extract corresponding subset questions. next subset removed questions form remaining set. subsequently replace whole remaining continue procedure. therefore subset questions corresponding count type contain color question. provide splits project web-page under http//mpii.de/visual_turing_test. table comparison state-of-the-art daquar. reﬁned neurons architecture lstm vision glove resnet-. neurons architecture originally presented malinowski results comparison original data subset single word answers covers original data. asterisk method denotes using ﬁlter smooths otherwise noisy validation accuracies. dash denotes lack data. evaluate dataset built ms-coco dataset although offers different challenge tasks focus efforts real open-ended visual question answering challenge. challenge consists answers question training questions validation questions test questions. cation problem frequent answers training set. evaluation different model variants design choices train training test validation set. ﬁnal evaluations evaluated test challenge evaluate parts test-dev test-standard answers publicly available. performance measure consensus variant accuracy introduced antol predicted answer gets score matches least three human answers. adam throughout extable results validation question-only model analysis number frequent answer classes different question encoders. using glove; accuracy section discussion. periments found performs better momentum. keep default hyper-parameters adam. employed recurrent neural networks maps input question dimensional vector representation. cnns text using feature maps experiments output dimensionality also depends number views. preliminary experiments found removing question mark questions slightly improves results report numbers setting. since answers associated question need consider suitable training strategy takes account. examined following strategies picking answer randomly randomly possible annotated conﬁdently answered answers choosing frequent answer. following report results using last strategy found little difference accuracy strategies. allow training evaluating many different models limited time computational power ﬁne-tune visual representations experiments although model would allow models publicly available together tutorial implemented keras theano start analysis question-only models images answer questions. note questiononly baselines play important role question answering images tasks since much performance added vision. hence better overall performance model obscured better language model. understand better different design choices conducted analysis along different ‘design’ dimensions. ﬁrst examine different hyper-parameters cnns encode question. ﬁrst consider ﬁlter’s length convolutional kernel. model different kernel lengths ranging notice increasing kernel lengths improves performance length performance levels thus kernel length following experiments interpreted trigram model. also tried simultaneously kernels different lengths. table view corresponds kernel length views correspond kernels length three views correspond length etc. however best performance still achieve single view kernel length alternatively neural network encoders consider bagof-words approach one-hot representations question words ﬁrst mapped shared embedding space subsequently summed word surprisingly simple approach gives competitive results compared encoding discussed previous section table results validation vision language analysis different language encoders glove word embedding vgg- summation combine vision language. results section discussion. means dealing unknown words questions test time apart using special token indicate class. address shortcoming investigate pre-trained word embedding transformation glove encodes question words choice naturally extends vocabulary question words million words extracted large corpus data common crawl used train glove embedding. since architecture scenario becomes shallow extra hidden layer pooling classiﬁcation table summarizes experiments glove. question encoders word embedding consistently improves performance conﬁrms using word embedding model learnt larger corpus helps. lstm beneﬁts glove embedding archiving overall best performance accuracy. experiments reported table investigate predictions using different number answer classes. experiment truncation frequent classes. question encoders truncation words best apparently good compromise answer frequency missing recall. table results validation vision language summary results results section discussion. columns denote left right word embedding learnt together architecture glove embedding replaces learnt word embedding truncating dataset frequent answer classes ﬁnally added visual representation model although question-only models answer substantial number questions arguably capture common sense knowledge order address full problem also observing image question based table investigates different techniques combine visual language representations. speed training combine last unit question encoder visual encoder explicitly shown figure experiments concatenation summation element-wise multiplication language encoder glove word embedding features extracted vgg- net. addition also investigate using normalization visual features divides every feature vector norm. experiments show normalization crucial obtaining good performance especially concatenation summation. remaining experiments summation. next question encoder lstm vary different visual encoders caffe variant alexnet googlenet vgg- recently introduced layered resnet table conﬁrms hypothesis stronger visual models perform better. show predicted answers using best model test tables show chosen examples ‘yes/no’ ‘counting’ ‘what’ questions model according opinion makes valid predictions. moreover table shows predicted compound answers. table summarises ﬁndings validation set. hand methods contextual language information lstm performing better hand adding strong vision becomes crucial. furthermore best found models experiments test sets test-dev teststandard. prevent overﬁtting latter restricts number submissions submissions total. here also study effect larger datasets ﬁrst train training next train epochs joint training validation set. train join consider question answer pairs answers among frequent answer classes training validation sets. training joint gained implies hand data indeed helps arguably also need better models exploit current training datasets effectively. ﬁndings summarized table section ﬁrst ﬁndings broader context compare reﬁned version neurons other publicly available approaches. table compares reﬁned neurons model approaches. methods likewise approach global image representation attention mechanism dynamically predict question dependent weights external textual sources fuse compositional question’s representation neural networks. table shows trends. first better visual representation signiﬁcantly helps leading approaches also uses variants resnet among strongest approaches image classiﬁcation task. however important normalize visual features additionally best models explicit attention mechanism work however focus extension plain neurons model uses global fullframe image representation. similar representation used reﬁned neurons ibowimg team lstm q+i. best performing approaches also different variants recurrent neural networks question encoding outperforms bag-of-words representations hypothesize multimodal embedding plays important role. shown table also emphasized leading approaches methods novel multimodal embedding techniques build upon element-wise multiplication. finally using external textual resources also seems beneﬁcial presented neural architecture answering natural language questions images contrasts prior efforts based semantic parsing outperforms prior symbolic based approach doubling performance challenging task. variant model image answer question already explains substantial part overall performance helps understand contribution visual features task. comparison human baseline humans table results test comparison state-of-the-art accuracy challenge test server. dash denotes lack data. leaderboard found http//www.visualqa.org/roe.html. baselines antol considered. shown image answer question conclude language-only model learnt biases patterns seen forms common sense prior knowledge also used humans accomplish task. extended existing daquar dataset daquar-consensus provides multiple reference answers allows study inter-human agreement consensus question answering task. propose metrics average consensus takes account human disagreement consensus captures disagreement human question answering. finally extend analysis large-scale dataset showing competitive performance still using global visual model training model solely provided question answer image triples. broader analysis different deep learning components design choices model improved results highlights importance strong visual model. acknowledgements marcus rohrbach supported fellowship within fitweltweit-program german academic exchange service project part supported collaborative research center german research foundation", "year": 2016}