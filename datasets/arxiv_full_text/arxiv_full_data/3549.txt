{"title": "Between-class Learning for Image Classification", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "In this paper, we propose a novel learning method for image classification called Between-Class learning (BC learning). We generate between-class images by mixing two images belonging to different classes with a random ratio. We then input the mixed image to the model and train the model to output the mixing ratio. BC learning has the ability to impose a constraint on the shape of the feature distributions, and thus the generalization ability is improved. BC learning is originally a method developed for sounds, which can be digitally mixed. Mixing two image data does not appear to make sense; however, we argue that because convolutional neural networks have an aspect of treating input data as waveforms, what works on sounds must also work on images. First, we propose a simple mixing method using internal divisions, which surprisingly proves to significantly improve performance. Second, we propose a mixing method that treats the images as waveforms, which leads to a further improvement in performance. As a result, we achieved 19.4% and 2.26% top-1 errors on ImageNet-1K and CIFAR-10, respectively.", "text": "figure argue cnns aspect treating input data waveforms. case mixture images mixture waveforms. make sense machines although visually make sense humans. proposing simple powerful novel learning method named between-class learning deep sound recognition. learning aims learn classiﬁcation problem solving problem predicting mixing ratio different classes. generated betweenclass examples mixing sounds belonging different classes random ratio. input mixed sound model trained model output mixing ratio. advantages learning limited increase variation training data. also argue learning ability impose constraint feature distributions cannot achieved standard learning thus generalization ability improved. carefully designed method mixing sounds considering difference sound energies achieve satisfactory performance. result learning improves performance various sound recognition networks datasets data augmentation schemes achieved performance surpasses human level sound classiﬁcation tasks. paper propose novel learning method image classiﬁcation called between-class learning generate between-class images mixing images belonging different classes random ratio. input mixed image model train model output mixing ratio. learning ability impose constraint shape feature distributions thus generalization ability improved. learning originally method developed sounds digitally mixed. mixing image data appear make sense; however argue convolutional neural networks aspect treating input data waveforms works sounds must also work images. first propose simple mixing method using internal divisions surprisingly proves signiﬁcantly improve performance. second propose mixing method treats images waveforms leads improvement performance. result achieved top- errors imagenet-k cifar- respectively. deep convolutional neural networks achieved high performance various tasks image recognition speech recognition sound recognition biggest themes research image recognition network engineering. many types networks proposed mainly ilsvrc competition furthermore training deep neural networks difﬁcult many techniques proposed achieve high performance data augmentation techniques network layers dropout batch normalization optimizers adam thanks research studies training deep neural networks become relatively easy mixing ratio used irrespective modality input data. learning applicable sounds sound kind wave motion mixture multiple sound data still counts sound. however image kind wave motion mixing multiple image data visually make sense. show example mixed image fig. mixed image loses objectness count image. therefore appears inappropriate apply learning images. however important thing humans perceive mixed data machines perceive them. argue cnns aspect treating input data waveforms considering recent studies speech sound recognition characteristics image data pixel values. assume cnns recognize images treating waveforms quite similar manner recognize sounds. thus mixture images mixture waveforms shown fig. would make sense machines. therefore effective sounds would also effective images. thus propose learning images paper. first propose simplest mixing method using internal divisions. surprisingly mixing method proves perform well. second also propose mixing method treats images waveform data. normalizing mean standard deviation whole training data subtract per-image mean value make image zero-mean. deﬁne image energy standard deviation image normalizing images considering image energy quite similar manner sounds. mixing method also simple easy implement leads improvement performance. experimental results show learning also improves performance various types image recognition networks simple network state-of-the-art networks. top- error resnext- imagenet-k improved using simplest learning. moreover error rate state-of-the-art shake-shake regularization cifar dataset improved using improved learning. finally visualize learned features show learning indeed imposes constraint feature distribution. contributions paper follows remainder paper organized follows. section provide summary learning sound recognition related work proposing propose learning image recognition section explaining relationship learning sounds. section compare performance standard learning demonstrate effectiveness learning. finally conclude paper section standard learning classiﬁcation problems select single training example dataset input model. train model output onehot class label. contrast learning select training examples different classes examples using random ratio. input mixed data model train model output mixing ratio. learning uses mixed data labels thus never uses pure data labels. method mixing sounds greatly affects performance. sounds belonging different classes randomly selected training dataset one-hot labels. generated random ratio uniform distribution mixed sets data labels ratio. mixed labels simply aimed train model output mixing ratio. explained carefully designed achieve satisfactory performance. simplest method here better sound energy proportional square amplitude. however auditory perception sound mixed method would difference sound pressure level large. case training model label inappropriate. considered using coefﬁcient instead mixed sounds sound pressure level respectively. deﬁned auditory perception mixed sound becomes hypothesized ratio auditory perception network ratio amplitude solved argued learning ability provide constraint feature distribution cannot achieved standard learning thus generalization ability improved. hypothesized mixed sound mixr projected onto point near internally dividing point original features. hypothesis came fact linearly-separable features learned hidden layer close output layer deep neural networks humans perceive sounds louder softer digitally mixed sound. expected internally dividing point input space almost corresponds high-level feature space least sounds. visualized feature distributions standard-learned model using showed hypothesis indeed correct. then argued learning following effects hypothesis. enlargement fisher’s criterion. argued learning leads enlargement fisher’s criterion feature space. explain reason fig. fisher’s criterion small feature distribution mixed sounds becomes large would large overlap feature distributions class class case mixed sounds projected onto classes shown ﬁgure model cannot output mixing ratio. learning gives penalty situation learning trains model output mixing ratio. model output mixing ratio make penalty learning small fisher’s criterion large shown fig. case overlap becomes small model becomes able output mixing ratio learning gives small penalty. therefore learning enlarges fisher’s criterion classes feature space. regularization positional relationship among feature distributions. expected learning also effect regularizing positional relationship among class feature distributions. standard learning constraint positional relationship among classes long features classes linearly separable. figure argued learning ability provide constraint feature distribution cannot achieved standard learning. ﬁgure represents class distribution feature space. black dashed line represents trajectory feature input mixture particular sounds model changing mixing ratio found standard-learned model sometimes misclassiﬁes mixed sound class class class undesirable situation little possibility mixed sound classes becomes sound classes. case assume features class distributed depicted fig. decision boundary class appears class class trajectory features mixed sounds crosses decision boundary class learning avoid situation decision boundary another class appears classes because learning trains model output mixing ratio instead misclassifying mixed sound different classes. this assumed features class become distributed shown fig. feature distributions three classes form acuteangled triangle decision boundary class appear class class therefore argued learning ability provide constraint feature distribution thus learning improves generalization ability. training examples different classes examples using random ratio. input mixed data model train model output mixing ratio. learning uses mixed data labels thus never uses pure data labels. examples also important images. first propose simplest mixing method section second discuss learning also applied images section finally section propose better version learning considering discussion simple mixing images belonging different classes randomly selected training dataset one-hot labels. note already preprocessed applied data augmentation size input network. generate random ratio sets data labels ratio. labels simply train model output mixing ratio. explain proposed carefully designed mixing method sounds considering difference sound energies mentioned section sound data absolute center distance represents sound energy. however pixel values image data absolute center appears concept energy. thus ﬁrst propose following mixing method simplest method using internal divisions learning applicable sounds mixed sound still counts sound. sound kind wave motion mixing sounds physically makes sense. humans recognize sounds perceive sounds louder softer digitally mixed sound. however image data pixel values kind wave motion humans. therefore mixing multiple images visually make sense. however important thing whether mixing data physically makes sense whether humans perceive mixed data machine perceives mixed data. argue cnns aspect treating input data waveforms. fact recent studies demonstrated cnns learn speech sounds directly waveforms ﬁlter learns respond particular frequency area also known images pixel values transformed components various frequency areas using fourier transform convolutional ﬁlters frequency ﬁlters therefore expected convolutional ﬁlter learns extract frequency features. assume cnns recognize images treating waveforms quite similar manner recognize sounds. thus mixture images mixture waveforms machines effective sounds would also effective images. visualized feature distribution standardlearned model mixed data using pca. used output layer -layer trained cifar-. mixed images using eqn. results shown fig. magenta dots represent feature distribution mixed images automobile deer bird ratio black dotted line represents trajectory feature input mixture particular images model changing mixing ratio ﬁgure shows mixture images projected onto point near internally dividing point features features mixed images distributed classes tendency observed sounds therefore effect learning sounds i.e. enlargement fisher’s criterion feature space regularization positional relationship among feature distributions classes expected. compare feature distributions learned standard learning demonstrate different shape feature distribution learned learning visualization experiment. images waveform data waveform consisting vectors. recent stateof-the-art methods input data normalized channel using mean standard deviation calculated whole training data case mean image equal image data represented static component wave component respectively. here simplest mixing method eqn. rewritten mixr assume performance improvement eqn. mainly owing wave component cnns treat input data waveforms. moreover static component effect mixing waveforms generally hypothesizes static components waveforms same. table results resnext- imagenet-k dataset. learning improves performance using default learning schedule. furthermore performance improved using longer learning schedule singlecrop top- error improved around compared default performance reported remove static component subtracting per-image mean value normalizing mean standard deviation whole training data. treat image zero-mean waveform apply schemes sounds described section first consider mixing images with second take difference energies consideration order make perception mixed image coefﬁcient instead images image energies. deﬁne image energy square standard deviation image hypothesize ratio image perception network ratio amplitude main component functions cnns conv/fc relu pooling average pooling satisfy homogeneity ignore bias. solve finally obtain proposed mixing method essential difference mixing method sounds deﬁnition energies. mixing method also easy implement experimentally proves lead improvement performance compared simplest mixing method eqn. figure training curves resnext- imagenetk dataset. dashed lines represent training curves using default learning schedule solid lines represent training curves using longer learning schedule. first compare performance standard learning -class imagenet classiﬁcation task experiment used simple learning. selected resnext- model training state-of-the-art level performance moreover ofﬁcial torch training codes available. validate comparison incorporated learning ofﬁcial codes. using learning selected training images applied default data augmentation scheme described image obtained images. mixed images random ratio selected addition default learning schedule also tried longer learning schedule -epochs training started training learning rate divided learning rate epoch table results cifar- cifar- datasets. show average standard error trials. learning improves performance various settings. note trained different learning setting default. epochs training also started training learning rate divided learning rate epoch reported classiﬁcation errors validation using single-crop testing crop testing results shown table also show training curves fig. performance learning default -epochs training signiﬁcantly improved standard learning. moreover performance learning improved -epochs training standard learning improved achieved .%/.% single-crop top-/top- validation errors .%/.% -crop validation top/top errors. single-crop top- error improved around compared default performance reported discussion. learning between-class examples among classes difﬁcult tends require large number training epochs. shown fig. performance ﬁrst epochs -epochs training learning worse performance standard learning. therefore learning schedule carefully designed. furthermore assume usage curriculum learning would helpful speed training; namely early stage generate mixing ratio close input relatively pure examples gradually change distribution ﬂat. trained standard -layer resnet- resnext- densenet shake-shake regularization validate comparison also incorporated learning original torch training codes. -layer also incorporated them. codes standard shifting/mirroring data augmentation scheme widely used datasets. added training epochs smaller learning rate default learning schedule total epochs resnet- resnext- densenet. show difference default settings appendix well conﬁguration -layer cnn. trained model times -layer resnet- times networks. report average standard error ﬁnal top- errors. summarize results table performances networks cifar- improved simple learning. furthermore improved version learning treat image data waveforms performance improved. best result cifar- shake-shake regularization. performance stable error rate trials range .%–.%. know whether result counted state-of-the-art; however learning proves able improve performance various networks simple network state-of-the-art networks. also show training curves fig. note training curves represent average trials. contrary training curves imagenet-k testing error learning decreases almost speed standard learning early stage training. furthermore last training epochs -layer densenet leads lower testing error using learning. performances cifar- also improved learning. although difﬁcult learn between-class examples among classes improvement performance resnext- shakeshake regularization learning shows signiﬁcant improvement -layer resnet- densenet. relationship data augmentation. here show performance using data augmentation table show average trials. shown table degree improvement performance level even smaller using standard data augmentation although variation training data increases approximately assume potential within-class variance small using data augmentation. within-class variance feature space small variance features mixed images also becomes small overlap fig. becomes small. therefore effect learning becomes small result. assume learning compatible with even strengthened strong data augmentation scheme. understand part important learning conducted ablation analysis following trained -layer cifar- cifar- using various settings. implemented codes ablation analysis using chainer results shown table well average trials. zero-mean mixing method. differences improved learning simplest learning follows subtract par-image mean; considering waveform energy proportional square amplitude; take difference image energies consideration. investigated great effect. result considering difference image energies proved little signiﬁcance comparing true eqn. true eqn. would variance image energies smaller sound energies. however per-image mean subtraction dividing square root important result shows treating image data waveforms contributes improvement performance. label. compared different labels applied mixed image. performance worsened used single label softmax cross entropy loss. would inappropriate train model recognize mixed image particular class. using multi label sigmoid cross entropy loss marginally improved performance proposed ratio label loss performed best. model learn between-class examples efﬁciently using ratio label. improved mixed images belonging class additionally selected images completely randomly allowed images sometimes class performance worse proposed images always different classes. learning method providing constraint feature distribution different classes select images belonging different classes. also tried mixtures three different classes probability addition mixtures different classes performance signiﬁcantly improved moreover performance used mixtures three different classes worse despite larger variation training data. assume mixing classes cannot efﬁciently provide constraint feature distribution. mix. investigated occurs examples within network. here used simple mixing method eqn. performance also improved mixed examples layer near input layer. assume activations lower layers treated waveforms spatial information preserved extent. additionally mixing layer close output layer little effect performance. interesting performance worsened mixed examples middle point network expected middle layer network extracts features represent spatial semantic information simultaneously mixing features would make sense machines. visualization finally visualize features learned standard learning fig. applied activations layer -layer trained cifar- training data. shown ﬁgure features obtained learning spherically distributed small within-class variances whereas obtained standard learning widely distributed near decision boundaries. conducted further analysis learned features appendix. learning indeed imposes constraint feature distribution cannot achieved standard learning. conjecture classiﬁcation performance improved learning. proposed novel learning method image classiﬁcation called learning. argued cnns aspect treating input data waveforms attempted apply similar idea sounds. result performance signiﬁcantly improved simply mixing images using internal divisions training model output mixing ratio. moreover performance improved mixing method treats images waveforms. learning simple powerful method impose constraint feature distribution. assume learning applied images sounds also modalities. acknowledgement", "year": 2017}