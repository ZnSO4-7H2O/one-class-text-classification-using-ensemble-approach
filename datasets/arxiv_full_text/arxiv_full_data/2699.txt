{"title": "Hashing Over Predicted Future Frames for Informed Exploration of Deep  Reinforcement Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In reinforcement learning (RL) tasks, an efficient exploration mechanism should be able to encourage an agent to take actions that lead to less frequent states which may yield higher accumulative future return. However, both knowing about the future and evaluating the frequentness of states are non-trivial tasks, especially for deep RL domains, where a state is represented by high-dimensional image frames. In this paper, we propose a novel informed exploration framework for deep RL tasks, where we build the capability for a RL agent to predict over the future transitions and evaluate the frequentness for the predicted future frames in a meaningful manner. To this end, we train a deep prediction model to generate future frames given a state-action pair, and a convolutional autoencoder model to generate deep features for conducting hashing over the seen frames. In addition, to utilize the counts derived from the seen frames to evaluate the frequentness for the predicted frames, we tackle the challenge of making the hash codes for the predicted future frames to match with their corresponding seen frames. In this way, we could derive a reliable metric for evaluating the novelty of the future direction pointed by each action, and hence inform the agent to explore the least frequent one. We use Atari 2600 games as the testing environment and demonstrate that the proposed framework achieves significant performance gain over a state-of-the-art informed exploration approach in most of the domains.", "text": "reinforcement learning tasks efﬁcient exploration mechanism able encourage agent take actions lead less frequent states yield higher accumulative future return. however knowing future evaluating frequentness states non-trivial tasks especially deep domains state represented high-dimensional image frames. paper propose novel informed exploration framework deep tasks build capability agent predict future transitions evaluate frequentness predicted future frames meaningful manner. train deep prediction model generate future frames given state-action pair convolutional autoencoder model generate deep features conducting hashing seen frames. addition utilize counts derived seen frames evaluate frequentness predicted frames tackle challenge making hash codes predicted future frames match corresponding seen frames. could derive reliable metric evaluating novelty future direction pointed action hence inform agent explore least frequent one. atari games testing environment demonstrate proposed framework achieves signiﬁcant performance gain state-of-the-art informed exploration approach domains. reinforcement learning involves agent progressively interacting initially unknown environment order learn optimal policy objective maximizing cumulative rewards collected environment throughout learning process agent alternates primal behaviors exploration novel states could potentially lead high future rewards; exploitation perform greedily according learned knowledge past exploitation learned knowledge well studied efﬁciently explore state space still remained critical challenge especially deep domains. deep domains state represented low-level sensory inputs only image pixels often high-dimensional or/and continuous. thus state space deep huge often intractable searching. performing exploration huge state space existing deep works adopt simple exploration heuristic \u0001-greedy strategy agent takes random action probability e.g. uniform sampling discrete-action domains corrupting action i.i.d. gaussian noise continuous-action domains agent explores state space without conscious i.e. without incorporating meaningful knowledge environment. exploration heuristic turns work well simple problem domains fails handle challenging domains extremely sparse rewards lead exponentially increasing state space large action space. unlike unconscious exploratory behavior agents using \u0001-greedy strategy human beings intending explore unfamiliar task domain often actively applies domain knowledge task accounts state space less frequently visited intentionally tries actions lead novel states. work mimic exploratory behaviors improve upon \u0001-greedy strategy random action selection come efﬁcient informed exploration framework deep agents. hand develop agent’s knowledge environment make able predict future trajectories. hand integrate developed knowledge hashing techniques high-dimensional state space order make agent able realistically evaluate novelty predicted future trajectories. speciﬁcally proposed informed exploration framework ﬁrst train action-conditional prediction model predict future frames given state-action pair. second perform hashing high-dimensional state space seen agent train deep convolutional autoencoder generate high-level features state apply locality-sensitive hashing high-level state features generate binary codes represent state. however learned hashing function counting actually seen states need query counts predicted future frames compute novelty. hence introduce additional training phase autoencoder match hash codes predicted frames corresponding ground-truth frames able utilize environment knowledge hashing techniques high-dimensional states generate reliable novelty evaluation metric future direction pointed action given state. recently works enhancing exploration behavior deep agent demonstrated great potential improving performance various deep task domains. asynchronous training techniques adopted multiple agents created perform gradient-based learning update model parameters separately atari domain. ensemble q-functions trained reduce bias values approximated increases exploration depth atari domain. exploration strategy based maximizing information gain agent’s belief environment dynamics adopted tasks continuous states actions. kl-divergence probability obtained learned dynamics model actual trajectory used measure surprise experience. novelty state measured based count-based mechanisms reward shaping performed adding reward bonus term q-value computed based counts. approaches exploration strategy incorporated either function approximation optimization process agent still needs randomly choose action explore without relying knowledge model. work conduct informed exploration i.e. utilize model-based knowledge derive deterministic choice action agent explore. exploration deep prediction models recent works aiming incentivize exploration deep prediction models shown promising results deep domains. autoencoder model trained jointly policy model reconstruction error autoencoder used determine rareness state. pixelcnn trained jointly policy model density model state. prediction gain state measured difference state density given pixelcnn observing state. approaches novelty state measured loss output another model exact statistics. work counts derived hashing state space reliably infer novelty state. mostly related work ours action-conditional prediction model trained predict future frame given state-action pair. compute gaussian kernal distance predicted future frame history frames inform agent take action leads frame dissimilar compared window recent frames. work action-conditional architecture construct prediction model. however involve hashing mechanism count state space determine novelty state based counting statistics. enable model query exact counting hashing deep domain running algorithms discretized features yields faster learning promising performance. shown latent features learned autoencoder trained unsupervised manner great promise efﬁciently discretize high-dimensional state space state space atari domain ﬁrst discretized using latent features derived autoencoder model. hashing performed encourage exploration computing reward bonus term form like mbie-eb work also introduce hashing state space based latent features trained deep autoencoder model exploration mechanism signiﬁcantly different first work count actually seen image frames query hash predicted frames. second reward bonus added function approximation target approach counts future states inﬂuence previous states backwards bellman equation whereas work count used informed exploration direct inﬂuence approximated q-values. paper consider discounted ﬁnite-horizon markov decision process discrete actions. formally deﬁned tuple states could high-dimentional continuous actions state transition probability distribution specifying probability transiting state issuing action state reward function mapping state-action pair reward discount factor. goal agent learn policy maximizes expected cumulative future γtr]. context deep step agent receives state observation rr×m×n number consequent frames represent state dimension frame. agent selects action among possible choices receives reward propose informed exploration framework mimic exploratory behavior human beings unfamiliar task domain. generally agent longer randomly selects action explore without incorporating domain knowledge. instead agent intentionally select action leads least frequent future states thus explore state space informed deterministic manner. build capability agent performing following tasks predicting future transitions evaluating visiting frequency predicted future frames. figure deep neural network architectures adopted informed exploration. left actionconditional prediction model predicting future transition frames; right autoencoder model conducting hashing state space. transitions given state-action pair. state input recent image frames action input represented one-hot vector number actions task domain. predict state model predicts single frame time denoted rm×n. state formed concatenating predicted frame recent frames. adopt action-conditional transformation proposed form joint feature state input action input. speciﬁcally state input ﬁrst passed three stacked convolutional layers form feature vector one-hot action feature perform linear transformation multiplying corresponding rk×l. linear transformation features transformation matrix shaped dimensionality. features state action linear transformation performs multiplicative interaction form joint feature follows afterwards joint feature passed stacked deconvolutional layers sigmoid layer form ﬁnal prediction output. predict multiple future steps prediction model progressively composes state using prediction result predict next-step transition. evaluate novelty state adopt hashing model count state space. ﬁrst train autoencoder model frames s∈rm×n→ ˆs∈rm×n unsupervised manner reconstruction loss follows ˆstij reconstructed pixel i-th j-th column. architecture autoencoder model shown figure speciﬁc convolutional layer followed rectiﬁer linear unit layer pooling layer kernel size discretize state space hash last frame state. adopt output last relu layer encoder high-level state features denote corresponding feature generates high-level feature vector zt∈rd state i.e. discretize state feature locality-sensitive hashing adopted upon projection matrix rp×d randomly initialized i.i.d. entries drawn standard gaussian projecting feature sign outputs form binary code introduced discretization scheme able count state space problem domain. process hash table created. count state denoted stored queried updated hash table. overall process counting state expressed following formulas. derive novelty predicted frames updating hash table seen frames need match predictions realities i.e. make hash codes predicted frames corresponding ground-truth seen frames training. introduce additional training phase autoencoder model make hash codes same derived feature vectors predicted frames ground-truth seen fames need close other. introduce additional loss function pair ground-truth seen frame predicted frame follows parameter autoencoder. note even though prediction model could generate almost identical frames training autoencoder reconstruction loss lead distinct state codes task domains therefore effort matching codes necessary. however matching state code guaranteeing satisfying reconstruction behavior extremely challenging. fine tuning autoencoder fully trained lrec code matching loss lmat would fast disrupt reconstruction behavior code loss could decrease expected level. training autoencoder scratch lrec lmat also difﬁcult lmat initially lrec high. network hardly direction consistently decrease lrec imbalance. therefore work propose train autoencoder phases ﬁrst phase uses lrec train convergence second phase uses composed loss function proposed address requirement matching prediction reality. computing novelty states prediction model autoencoder model trained agent could perform informed exploration following manner. step agent performs exploration probability less perform greedy action selection otherwise. given state performing exploration agent predicts future trajectories length possible actions formally novelty score action given state denoted computed ψt+i count future state st+i derived predicted frames {¯st+j}i predeﬁned prediction length real-valued discount rate. evaluating novelty possible actions agent selects highest novelty score explore. overall policy agent proposed informed exploration strategy deﬁned empirical evaluation arcade learning environment consists atari video games testing domain. choose representative games require signiﬁcant exploration learn policy breakout freeway frostbite ms-pacman q-bert. among games frostbite large action space consists full actions breakout state distribution changes signiﬁcantly ability policy network changes others sparse rewards. tasks state representation concatenates consequent image frames size architecture prediction model identical shown figure train prediction model create training dataset consists transition records generated fully trained agent performing \u0001-greedy uniform random action selection equal training adopt adam optimization algorithm learning rate mini-batch size moreover discount gradient scale multiplying gradient value state input normalized dividing pixel values show pixel prediction loss mean square error multi-step future prediction table task domains prediction errors within small scale. prediction error increases increase prediction length. demonstrate trained prediction models able generate realistic future frames visualized close ground-truth frames results shown figure architecture autoencoder model identical shown figure autoencoder trained dataset collected identical manner prediction model. trained phases. ﬁrst phase trained reconstruction loss. adam optimization algorithm learning rate mini-batch size discount gradient multiplying second phase train autoencoder based loss second phase adam optimization algorithm learning rate mini-batch size value discount gradient multiplying gradient value figure prediction reconstruction result task domain. task present sets frames consists four frames. sets frames domain side-by-side row. four frames organized follows ground-truth frame seen agent; predicted frame prediction model; reconstruction autoencoder trained reconstruction loss; reconstruction autoencoder trained second phase overall extremely challenging match state codes predicted frames corresponding seen frames maintaining satisfying reconstruction performance. demonstrate figure showing code loss measured terms number mismatch binary codes pair predicted frame corresponding ground-truth frame. presented result derived averaging pairs codes. first result shows without second phase impossible perform hashing autoencoder trained reconstruction loss since average code losses domains distinct hash codes count values returned querying hash table meaningless. second result shows training second phase code loss signiﬁcantly reduced. also show reconstruction errors measured terms training phases domain figure incorporating code matching loss reconstruction behavior autoencoder receives slightly negative effect. comparison frame reconstruction effect training phases shown figure shown training match state codes reconstructed frames slightly blurred still able reﬂect essential features problem domain except q-bert. moreover breakout illustrative example demonstrate presented hashing framework generate meaningful hash codes predicted future frames given ground-truth frame show predicted frames length taking action. found trajectories ball positions predicted regardless action choice different actions lead different board positions. hash codes ﬁrst actions no-op lead little change frames almost future frames hashed code. last actions right left lead signiﬁcant changes board position codes future frames much distinct other. result also shows hash code less sensitive ball position compared board position game domain. figure left predicted future trajectories action breakout. ﬁrst frame ground-truth frame following frames predicted future trajectories length agent takes following actions no-op; ﬁre; right; left. right hash codes frames ordered top-down manner. save space four binary codes grouped code i.e. range color normalized linearly value. evaluate efﬁciency proposed informed exploration framework integrate algorithm compare baselines performs \u0001-greedy uniform random choice action exploration denoted dqn-random; state-of-the-art informed exploration approach proposed denoted dqn-informed. proposed informed exploration approach paper denoted dqn-informed-hash. note dqn-informed proposed approach adopt base algorithm. experiment following standard setting used methods train agent million frames evaluate agent episodes game play. length hashing future frames. report result table performance scores dqn-random dqn-informed obtained except game breakout frostbite included work. among test domains dqn-informed-hash outperforms baseline approaches signiﬁcant performance gains observed domain. note breakout agent fails progress dqn-informed always scores almost kernel-based pixel distance evaluation metric used dqn-informed encourages agent explore states dissimilar recent history insufﬁcient agent explore. note dqn-informed-hash demonstrates superior performance deterministic exploration mechanism. indicates counting predicted future frames could provide meaningful direction exploration. paper propose informed exploration framework deep tasks discrete action space. incorporating deep convolutional prediction model future transitions hashing mechanism based deep autoencoder model enable agent predict future trajectories intuitively evaluate novelty future action direction based hashing result. empirical result atari domain shows proposed informed exploration framework could efﬁciently encourage exploration several challenging deep domains.", "year": 2017}