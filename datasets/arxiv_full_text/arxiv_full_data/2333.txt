{"title": "Modeling Events with Cascades of Poisson Processes", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We present a probabilistic model of events in continuous time in which each event triggers a Poisson process of successor events. The ensemble of observed events is thereby modeled as a superposition of Poisson processes. Efficient inference is feasible under this model with an EM algorithm. Moreover, the EM algorithm can be implemented as a distributed algorithm, permitting the model to be applied to very large datasets. We apply these techniques to the modeling of Twitter messages and the revision history of Wikipedia.", "text": "present probabilistic model events continuous time event triggers poisson process successor events. ensemble observed events thereby modeled superposition poisson processes. eﬃcient inference feasible model algorithm. moreover algorithm implemented distributed algorithm permitting model applied large datasets. apply techniques modeling twitter messages revision history wikipedia. real-life observations often naturally represented events—bundles features occur particular moment time. events generally nonindependent event cause others occur. given observations events wish produce probabilistic model used prediction parameter estimation also identifying structure relationships data generating process. present approach building probabilistic models collections events event induces poisson process triggered events. approach lends eﬃcient inference algorithm distributed across computing clusters thereby applied massive datasets. present case studies ﬁrst involving collection twitter messages ﬁnancial data second focusing revision history wikipedia. latter example particularly large-scale problem; data consist billions potential interactions among events. ism. relatively small number machine learning papers focused continuous-time graphical models; examples include poisson networks rajaram continuoustime bayesian networks described nodelman approaches diﬀer assume small possible event labels directly apply structured label spaces. ﬂexible approach presented wingate deﬁne nonparametric bayesian model latent events causal structure. work diﬀers several ways importantly discrete-time model allows interaction adjacent time steps. finally work extension generalization continuous-time noisy-or presented simma also large literature statistics point process modeling provides context work. speciﬁc connection fundamental stochastic process model known statistics mutually self-exciting point process also connections applications seismology notably epidemic type aftershocksequences framework ogata involves model similar applied earthquake prediction. representation collections events based formalism marked point processes. event represented pair r+×f timestamp associated features taking values feature space. dataset sequence observations denote events occuring times events occur? events occur? features possess? classical approach answering questions proceeds follows number distributed poisson timestamps associated event independent identically distributed ﬁxed distribution features drawn independently ﬁxed distribution average occurrence rate density locations marking density denotes inhomogeneous poisson process. might wish density capture periodic activity time-of-day eﬀects example intensity step function time. however real collections events often exhibit dependencies cannot captured standard poisson process capture dependencies consider processes poisson processes random mean measure. particular consider mean measures take form latent markov processes. queueing theory kind model referred markov-modulated poisson process used model packets networks paper take diﬀerent approach modeling collections dependent events occurrence event triggers poisson process consisting events. speciﬁcally model triggered poisson process intensity model view random forest event either caused background poisson process previous event augment representation include cause event object generated random forest event node tree timestamp features attached. parent event event caused exist must root node. event caused parent exist. usually parenthood information available must estimated corresponds estimating tree structure enumeration nodes topological sort timestamps features. show distribution estimated algorithm. parameters model estimated algorithm cause event known every event would possible estimate parameters using standard results maximum likelihood estimation poisson distribution. since handled introducing latent variables—one element. thus credit-assigning step builds distribution past events potential causes also individual components mixture. design choice choice expected number events. ranges small space possible directly estimate however larger feature space approach infeasible computational statistical reasons functional form fertility function must learned. presenting fertility models assume simplicity binary feature vector. linear fertility consider restriction poisson additivity ixi= part algorithm build distribution allocation features events collecting suﬃcient statistics estimate values. note important restriction since mean constituent poisson random variables must nonnegative. multiplicative fertility linear model fertility places signiﬁcant limits negative inﬂuence features allowed exhibit also implies fertility eﬀect feature always regardless context. alternatively assume dimensions constant leading derivatives form exact solution single readily obtained optimize either coordinate descent gradient steps. alternative approach based poisson thinnings described simma observed iteratively estimate latent variables maximize parameters. uniformity notation assume dummy event fbase treat baseline intensity intensities resulting events. introduce zxtx) expectations latent zxtx) corresponds expectation =x)). neglecting terms don’t depend variables variables soft-assignment proxies allow compute expected suﬃcient statistics estimating parameters fbase speciﬁc details computation depend speciﬁc choices made fbase basically reduces estimation task estimating distribution weighted samples. example fbase labelregardless delay labeling distributions relative intensities diﬀerent events total intensity total mean measure equal number events observed. either treated constraint step possible simple form) results step projected onto solutions scaling fbase increasing likelihood process. possible develop additive components. sophisticated models making consider mixture complex. individual densities. example wikipedia edit modeling domain produce events similar time close whereas correspond thoughtful responses occur later also diﬀer substantially event caused them. since algorithm introduces latent variable every additive component inside logarithm separation components possible concern requires ﬁtting large number parameters. special case particular structure reason believe composed groups variables interact multiplicatively within group linearly among groups case multiplicative models used subset variables. certain selections delay transition distributions possible collapse certain statistics together signiﬁcantly reduce amount bookkeeping required. consider setting small number possible labels small delay distribution exponential distribution memorylessness exponential distribution avoid need explicitly build distribution possible causes event. weighted-average delay counting number type events triggering type done similar techniques letting bijk events caused change event encountered. transition density sparse need incremented rest left unmodiﬁed long missing exponential decay accounted later. computational technique works restricted models computational complexity average number non-zero entries much computationally eﬃcient direct method large number somewhat closely spaced events. large-scale experiments wikipedia hadoop open-source implementation mapreduce object collection page neighbors link graph. operation also accesses hyperparameters shared across pages runs multiple iterations events associated page. learned parameters returned reducer updates hyperparameters another mapreduce models updated hyperparameters. thus reduce step accumulates statistics hyperparameters well collects log-likelihoods. hadoop requires object mapped kept memory requires careful attention representation compression; memory limits challenge scaling. neighborhood memory possible break pieces step phase reduce phase suﬃcient statistics maximize parameters requires many chained mapreduce jobs ineﬃcient. experiments careful engineering compression suﬃcient. twitter popular microblogging website used quickly post short comments world see. collected twitter messages contained references stock tickers message body. messages form conversation; others posted result real-world event inspiring commentary. dataset collected contains messages covers period days. modeling message represented triple user timestamp binary vector features. typical message this generated sequence mapreduce jobs ﬁrst compute diﬀs featurize page gather list neighbors require page’s history ﬁnally page sends copy neighbors. page’s body insuﬃcient determine neighbors since body contains outgoing links incoming links need collected ﬁrst. remaining aspect model transition distribution speciﬁes types events expected result event type let’s consider possible relationships message trigger message probability another message increased original event acts proxy general user activity. kinds messages represent variation baseline event rate captured baseline process unrelated triggering message content take distribution prior. occurs features aapl goog missing features msft has_link. length constraints internet culture messages tend completely grammatical english often message simply shortened link brief commentary. addition stocks involved whether links involved features also denote presence absence keywords option. baseline intensities simplest possible baseline intensity time-homogeneous poisson process empirical intensity periodic. better baseline break intervals hour assume intensity uniform within hour pattern repeats. pt/. log-likelihoods baselines reported table worth noting gain incorporating periodicity baseline much smaller gain parts model. timing model must combined feature distribution. fully independent model feature present independently others. feature. clearly estimates simply empirical fraction data contains feature. events trigger events induces poisson process successor events. factor intensity process αg|x)h constituents described intensity implemented multiplicative model expected number events exp. delay distribution must capture empirical fact responses occur shortly original message exist responses take signiﬁcantly longer meaning needs suﬃciently heavy tail. candidates consider uniform piecewise uniform exponential gamma distributions. log-likelihoods diﬀerent delays reported figure transition function used described later. best performing delay distribution gamma shape parameters less shape parameter also estimated results table note results show choice delay distribution smaller impact overall likelihood transition distribution. part fact individual event features embedded large space type homogeneous baseline periodic baseline transition intensity doesn’t depend features delay transition feature-dependent intensity delay identity transition delay transition shared intensity shared delay mixture transition mixture mixture phenomenon intended capture three eﬀects. densities x)dtdx. reit’s easy compute sults shown figure indicate models signiﬁcantly superior ﬁrst three demonstrating separating multiple phenomena useful. exponential distribution. model transition distributions share fertility delay functionswhereas model distribution fertility delay. shown figure latter performs signiﬁcantly better indicating three diﬀerent categories message relationships diﬀerent associated fertility parametrizations delays. plot shows proportions component mixture deﬁned ratio average fertility component total fertility. bottom plot demonstrates mean delay overall mixture remains almost constant throughout iterations diﬀerent individual components substantially diﬀerent delay means. table reports results cascade models increasing sophistication demonstrating gains result building ﬁnal model. ﬁrst stage improvements homogeneous periodic baseline independent transition model focuses times events occur shows roughly equivalent gains follow modeling periodicity capturing less periodic variability exponential moving average. boost comes better labeling distribution allows features events depend previous events capturing topicwise trends speciﬁc conversations. denote important special cases resultant event drawn independently caused events must identical trigger. exponential delay distribution ﬁxed equivalent setting poisson intensity exponential moving average decay parameter determined algorithm used optimal decay parameter reported results show model inferior utilizes features events. earlier enumerated relationships message trigger. example retweets completely identical original possible exception username reference transition would response would similar features diﬀer features densityproxy message would features independent causing message corresponding models density-proxy phenomenon. consider possible models greek letters represent parameters estimated course shape induced poisson process eﬀect. diﬀerent types transitions distinctly diﬀerent estimated means delay distributions expected since capture different eﬀects. seen figure overall-intensity proxying independent transition highest mean since level activity averaged labels changes slower activity particular stock topic. shape lower higher-variance gamma distributions work best. ﬁnal component fertility model depends features event allows events cause successors others. actually less impact log-likelihood components model. wikipedia public website aims build complete encyclopedia user edits. work build probabilistic model predicting edits page based revisions pages linking causes outside neighborhood considered. reasons restriction primarily computational—considering edits potential causes edits even within short time window impractical large scale. demonstration scale model pages total revisions involving billions considered interactions events. major insert often text migrated different page obtain addition many words removal none few. user’s perspective corresponds typing pasting body text minimal editing context. revert edit reverts content page previous state. often immediately previous state sometimes goes back. revert typically response vandalism though eddone good faith also reverted. since pages many neighbors event large number possible causes mean measure event many possible triggers. means exact shape delay distribution important cases possible triggers considered. model delay mixture three exponentials intending capture short medium longer-term eﬀects. page estimate parameters mixing weights. figure shows histogram estimated means. component fast response average minutes same-page minutes adjacent-page delay. page component captures edits caused other either individual making multiple modiﬁcations saving page along diﬀerent user noticing revisions news feed instantly responding changing undoing them. remaining components capture periodic eﬀects time-varying levels interest topic well reactions speciﬁc edits. model needs capture signiﬁcant attributes revision addition timestamp don’t completely model exact content edit inadequacies aspect model would dominate likelihood. instead identify features edits build distribution events described features edits. page features triggers event features latter vector drawn distribution possible features. number possible feature combinations small transition matrix directly learned multiple features features take many values need structured distribution. partition features parts features appear revision identity page. note take many values appearing relatively infrequently. vast number observations directly learn transition matrix model transition which conjugacy corresponds shrinkage towards transitions observed page’s transition probability becomes driven speciﬁc observed probabilities page. allocation components directly maximized magnitude chosen validation set. handled ﬁxing particular page ﬁtting model revisions refer then process pages page superposition processes possible figure shows log-likelihoods successive iterations model. regularized versions dirichlet prior; others estimate page independently. bars correspond figure log-likelihoods various models. models regularized transition matrices perform significantly better unseen data non-trivially worse training indicating strong regularization. baseline-only shown training test log-likelihoods. agonal predominantly positive indicating event particular type neighbor makes event type likely current page. note signiﬁcantly positive rectangle transitions massive inserts deletions changes. magnitude ratio almost identical rectangle; signiﬁcant modiﬁcations induce large modiﬁcations speciﬁc type modiﬁcation whether made known user irrelevant. large changes indications interest topic signiﬁcant structural changes related pages. remaining block represents edits page causing changes page responsible observations. stronger positive diagonal component above similar events co-occur. large changes especially anonymous users lead over-representation reverts following them. hand reverts result extra large changes large modiﬁcations made reverted come back feeding edit war. reverts actually over-produce reverts. ﬁrst-order eﬀect since reverts rarely undo previous undo rather captures controversial moments. presence revert indication previously unmeritorious edit made suggests future unmeritorious edits need reverted likely. ﬁxed page edit occurs neighbor would expect identity neighbor aﬀect likelihood causing event turns eﬀectively estimating intensities between pair pages impractical unless large number revisions observed. even high-data regimes strong regularization required. tried regularizing fertilities towards zero toward common per-page mean using penalties regularizers empirically poorer likelihoods using single scalar neighbors suggesting enough data accurately estimate individual reason pages large number events also large number neighbors estimation always diﬃcult regime. furthermore hypothetical ‘true’ values parameters change time neighbors appear change. figure learned transition matrix. area circles corresponds logarithm conditional probability observed feature divided marginal. yellow light-colored circles correspond transition likely average; correspond transition less likely. represents intensity baseline labels events whose cause previous event. positive values correspond event types events-triggering-events aspect model less eﬀective capturing thus over-represented otherwise-unexplained column. reverts known anonymous contributors signiﬁcantly underrepresented indicating rest model eﬀective capturing them. revisions made known contributors under-represented rest model captures better edits made anonymous contributors. events generated account total observed events. subset wikipedia graph includes pages revisions improves held-out likelihoods compared single neighbors. improvement small however certainly smaller impact aspects model. example pages intensities estimated neighbors shown table presented framework building models events based cascades poisson processes demonstrated applications demonstrated scalability massive dataset. techniques described paper exploit wide range delay transition fertility distributions allowing applications many diﬀerent domains. direction investigation provide support latent events root causes observed data. another bayesian formulation integrates instead maximizes parameters; work better complex fertility transition distributions lack suﬃcient observations accurately maximum likelihood. extensions complicate inference reduce scalability; indeed wingate propose bayesian model latent events scaling issue. furthermore allowing parameters model depend time would useful though again computational issues concern. nodelman shelton koller. expectation maximization complex duration distributions continuous time bayesian networks. uncertainty artiﬁcial intelligence", "year": 2012}