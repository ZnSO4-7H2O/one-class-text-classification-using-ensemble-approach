{"title": "Stochastic Generative Hashing", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Learning-based binary hashing has become a powerful paradigm for fast search and retrieval in massive databases. However, due to the requirement of discrete outputs for the hash functions, learning such functions is known to be very challenging. In addition, the objective functions adopted by existing hashing techniques are mostly chosen heuristically. In this paper, we propose a novel generative approach to learn hash functions through Minimum Description Length principle such that the learned hash codes maximally compress the dataset and can also be used to regenerate the inputs. We also develop an efficient learning algorithm based on the stochastic distributional gradient, which avoids the notorious difficulty caused by binary output constraints, to jointly optimize the parameters of the hash function and the associated generative model. Extensive experiments on a variety of large-scale datasets show that the proposed method achieves better retrieval results than the existing state-of-the-art methods.", "text": "learning-based binary hashing become powerful paradigm fast search retrieval massive databases. however requirement discrete outputs hash functions learning functions known challenging. addition objective functions adopted existing hashing techniques mostly chosen heuristically. paper propose novel generative approach learn hash functions minimum description length principle learned hash codes maximally compress dataset also used regenerate inputs. also develop eﬃcient learning algorithm based stochastic distributional gradient avoids notorious diﬃculty caused binary output constraints jointly optimize parameters hash function associated generative model. extensive experiments variety large-scale datasets show proposed method achieves better retrieval results existing state-of-the-art methods. image document retrieval. formally given reference dataset {xi}n negative euclidean distance used i.e. corresponds nearest neighbor search problem; inner product used i.e. becomes maximum inner product search problem. work focus lnns simplicity however method handles mips problems well shown supplementary material brute-force linear search expensive large datasets. alleviate time storage bottlenecks research directions studied extensively partition dataset subset data points searched; represent data codes similarity computation carried eﬃciently. former often resorts search-tree bucket-based lookup; latter relies binary hashing quantization. groups techniques orthogonal typically employed together practice. work focus speeding search binary hashing. hashing similarity search popularized inﬂuential works locality sensitive hashing shown give stronger empirical results tend less eﬃcient hamming search binary codes data-dependent hash functions well-known perform better randomized ones learning hash functions binary codes discussed several papers including spectral hashing semi-supervised hashing iterative quantization others main idea behind works optimize objective function captures preferred properties hash function supervised unsupervised fashion. even though methods shown promising performance several applications suﬀer main drawbacks objective functions often heuristically constructed without principled characterization goodness hash codes optimizing binary constraints crudely handled relaxation leading inferior results work introduce stochastic generative hashing address issues. propose generative model captures encoding binary codes input decoding input provides principled hash learning framework hash function learned minimum description length principle. therefore generated codes compress dataset maximally. generative model also enables optimize distributions discrete hash codes without necessity handle discrete variables. furthermore introduce novel distributional stochastic gradient descent method exploits distributional derivatives generates higher quality hash codes. prior work binary autoencoders also takes generative view hashing still uses relaxation binary constraints optimizing parameters leading inferior performance shown experiment section. also show binary autoencoders seen special case formulation. work mainly focus unsupervised setting. hash codes preserve local neighborhood structure original space. works focus modeling reverse process generating input binary codes reconstructed input small reconstruction error. fact generative view provides natural learning objective hashing. following intuition relaxed non-relaxed objectives. another approach enforce model parameterization particular structure applying alternating optimization algorithm alternate updating parameters binarization eﬃciently. example imposed orthogonality constraint projection matrix proposed circulant constraints introduced kronecker product structure. although constraints alleviate diﬃculty optimization substantially reduce model ﬂexibility. contrast avoid constraints propose optimize distributions binary variables avoid directly working binary variables. attained resorting stochastic neuron unlike relies solving expensive integer programs model end-to-end trainable using distributional stochastic gradient descent algorithm requires iterative steps unlike iterative quantization training procedure much eﬃcient guaranteed convergence compared alternating optimization itq. following sections ﬁrst introduce generative hashing model section then describe corresponding process generating hash codes given input section finally describe training procedure based minimum description length principle stochastic neuron reparametrization sections also introduce distributional stochastic gradient descent algorithm section {ui}l modeled multivariate bernoulli distribution hash codes additive model reconstructs summing selected columns given bernoulli prior distribution hash codes. joint distribution written generative model seen restricted form general markov random fields sense parameters modeling correlation latent variables correlation shared. however ﬂexible compared gaussian restricted boltzmann machines extra quadratic term modeling correlation latent variables. ﬁrst show generative model preserves local neighborhood structure frobenius norm bounded. neighborhood preservation used here. fact even sophisticated models multiple layers nonlinear functions. experiments complex generative models tend perform similarly gaussian model datasets sift-m gist-m. therefore gaussian model simplicity. tractable ﬁnding solution posterior involves solving expensive integer programming subproblem. inspired recent work variational auto-encoder propose bypass diﬃculties parameterizing encoding function involves linear projection followed sign operation common hashing literature. computing model thus amount computation except without orthogonality constraints. since goal reconstruct using least information binary codes train variational auto-encoder using minimal description length principle ﬁnds best parameters maximally compress training data. principle seeks minimize expected amount information communicate parameters generative model deﬁned comes encoding function deﬁned objective sometimes called helmholtz free energy true posterior falls family becomes true posterior leads shortest description length emphasize objective longer includes binary variables parameters therefore avoids optimizing discrete variables directly. paves continuous optimization methods stochastic gradient descent applied training. aware ﬁrst time procedure used problem unsupervised learning hash. methodology serves viable alternative relaxation-based approaches commonly used past. however cannot compute stochastic gradients w.r.t. depends stochastic binary variables order back-propagate stochastic nodes possible solutions proposed. first reparametrization trick works introducing auxiliary noise variables model. however diﬃcult apply stochastic variables discrete case model. hand gradient estimators based reinforce trick suﬀer high variance. although variance reduction remedies proposed either biased require complicated extra computation practice. next section ﬁrst provide unbiased estimator gradient w.r.t. derived based distributional derivative then derive simple eﬃcient approximator. derive estimator ﬁrst introduce stochastic neuron reparametrizing bernoulli distribution. stochastic stochastic neuron reparameterize note behaves binary variables replacing deterministically given gives reparameterized version original training objective distributional stochastic gradient descent objective given point randomly sampled {xi}n easily computed standard way. however reparameterization function longer diﬀerentiable respect discontinuity stochastic neuron namely algorithm readily applicable. overcome diﬃculty adopt notion distributional derivative generalized functions distributions distributional derivative stochastic neuron open set. denote space functions inﬁnitely diﬀerentiable compact support space continuous linear functionals considered dual space. elements space often called general distributions. exactly dirac-δ function. based deﬁnition distributional derivatives chain rules able compute distributional derivative function provided following lemma. therefore combine distributional derivative estimators stochastic gradient descent algorithm variants designate distributional sgd. detail presented algorithm denote unbiased stochastic estimator gradient constructed sample compared existing algorithms learning hash require substantial eﬀort optimizing binary variables proposed distributional much simpler also amenable online settings general distributional derivative estimator requires forward passes model dimension. accelerate computation approximate distributional derivative exploiting mean value theorem taylor expansion algorithm interestingly approximate stochastic gradient estimator stochastic neuron established distributional derivative coincides heuristic pseudo-gradient constructed please refer supplementary material details derivation approximate gradient estimator caveat potential discrepancy distributional derivative traditional gradient whether distributional derivative still descent direction whether algorithm integrated distributional derivative converges remains unclear general. however learning hash problem easily show distributional derivative indeed true gradient. proof first deﬁnition easily verify mild condition continuous -norm bounded. hence suﬃces show distribution du∇u deﬁnition u∂φdx. hand always bois-reymond’s lemma proposed stochastic generative hashing general framework. section reveal connection several existing algorithms. iterative quantization formed eigenvectors covariance matrix orthogonal matrix assume joint distribution without stochasticity optimization becomes extremely diﬃcult binary constraints. proposed algorithm exploit stochasticity bypass diﬃculty optimization. stochasticity enables accelerate optimization shown section section evaluate performance proposed distributional commonly used datasets hashing. eﬃciency consideration conduct experiments mainly approximate gradient estimator evaluate model algorithm several aspects demonstrate power proposed reconstruction loss. demonstrate ﬂexibility generative modeling compare reconstruction error showing beneﬁts modeling without orthogonality constraints. convergence distributional sgd. evaluate reconstruction error showing proposed algorithm indeed converges verifying theorems. training time. existing generative works require signiﬁcant amount time training model. contrast algorithm fast train terms number examples needed wall time. nearest neighbor retrieval. show recall plots standard large scale nearest neighbor search benchmark datasets mnist sift-m gist-m sift-b achieve state-of-the-art among binary hashing methods. reconstruction visualization. generative nature model regenerate original input bits. mnist cifar qualitatively illustrate templates correspond resulting reconstruction. used several benchmarks datasets i.e. mnist contains digit images size pixels cifar- contains pixel color images classes sift-m sift-b contain samples dimensional vector gist-m contains samples dimensional vector. reconstruction loss method generative model easily compute regenerated input argmax compute loss regenerated input original i.e. also trains minimizing binary quantization loss described equation essentially reconstruction loss magnitude feature vectors compatible radius binary cube. plotted reconstruction loss method sift-m figure mnist gist-m figure x-axis indicates number examples seen training algorithm y-axis shows average reconstruction loss. training time comparison listed table method arrives better reconstruction loss comparable even less time compared itq. lower reconstruction loss demonstrates claim ﬂexibility proposed model aﬀorded removing orthogonality constraints indeed brings extra modeling ability. note generally regarded technique fast training among existing binary hashing algorithms algorithms take much time train. demonstrate convergence distributional derivative adam numerically sift-m gist-m minst bits bits. convergence curves sift-m shown figure results gist-m mnist similar shown figure supplementary material obviously proposed algorithm even biased gradient estimator converges quickly matter many bits used. reasonable bits model data better reconstruction error reduced further. line expectation distributional trains much faster since bypasses integer programming. benchmark actual time taken train method convergence compare binary autoencoder hashing sift-m gist-m minst. illustrate performance sift-m figure results gist-m mnist datasets follow similar trend shown supplementary material empirically takes signiﬁcantly time train settings expensive cost solving integer programming subproblem. stochastic neuron well whole training procedure done tensorflow. released code github. competing methods directly used code released authors. compared stochastic generative hashing lnns task several state-of-the-art unsupervised algorithms including k-means hashing iterative quantization spectral hashing spherical hashing binary autoencoder scalable graph hashing demonstrate performance binary codes standard benchmark experiments approximate nearest neighbor search comparing retrieval recall. particular compare unsupervised techniques also generate binary codes. query linear search hamming space conducted approximate neighbors. following experimental setting plot recalln curve mnist sift-m gist-m sift-b datasets varying number bits figure sift-b datasets compared since training cost competitors prohibitive. recall deﬁned fraction retrieved true nearest neighbors total number true nearest neighbors. recalln recall ground truth neighbors retrieved samples. note recalln generally challenging criteria recalln better characterizes retrieval results. completeness results various recall curves found supplementary material show similar trend recalln curves. figure shows proposed consistently performs best across settings datasets. searching time number bits algorithms optimized implementation popcnt based hamming distance computation priority queue. point many baselines need signiﬁcant parameter tuning experiment achieve reasonable recall except method hyperparameters experiments used batch size learning rate stepsize decay. method less sensitive hyperparameters. important aspect utilizing generative model hash function generate input hash code. inputs images corresponds image generation allows visually inspect hash bits encode well diﬀerences original generated images. experiments mnist cifar- ﬁrst visualize template corresponds hash i.e. column decoding dictionary gives interesting insight hash represents. unlike components look like averaged images rest high frequency noise image template encodes distinct information looks much like ﬁlter banks convolution neural networks. empirically template also looks quite diﬀerent encodes somewhat meaningful information indicating bits wasted duplicated. note obtain representation by-product without explicitly setting model supervised information similar case convolution neural nets. figure illustration mnist cifar- templates regenerated images diﬀerent methods hidden binary variables. mnist four rows number bits used encode paper proposed novel generative approach learn binary hash functions. justiﬁed theoretical angle proposed algorithm able provide good hash function preserves euclidean neighborhoods achieving fast learning retrieval. extensive experimental results justify ﬂexibility model especially reconstructing input hash codes. comparisons approximate nearest neighbor search several benchmarks demonstrate advantage proposed algorithm empirically. emphasize proposed generative hashing general framework extended semi-supervised settings learning hash scenarios detailed supplementary material. moreover proposed distributional unbiased gradient estimator approximator applied general integer programming problems independent interest.", "year": 2017}