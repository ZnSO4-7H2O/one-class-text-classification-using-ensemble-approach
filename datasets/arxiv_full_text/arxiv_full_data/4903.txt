{"title": "C-RNN-GAN: Continuous recurrent neural networks with adversarial  training", "tag": ["cs.AI", "cs.LG"], "abstract": "Generative adversarial networks have been proposed as a way of efficiently training deep generative neural networks. We propose a generative adversarial model that works on continuous sequential data, and apply it by training it on a collection of classical music. We conclude that it generates music that sounds better and better as the model is trained, report statistics on generated music, and let the reader judge the quality by downloading the generated songs.", "text": "generative adversarial networks proposed efﬁciently training deep generative neural networks. propose generative adversarial model works continuous sequential data apply training collection classical music. conclude generates music sounds better better model trained report statistics generated music reader judge quality downloading generated songs. generative adversarial networks class neural network architectures designed generating realistic data approach involves training neural models conﬂicting objectives generator discriminator forcing improve. generator tries produce samples looks real discriminator tries discriminate generated samples real data. using framework makes possible train deep generative models without expensive normalizing constants technique proven produce highly realistic samples data work investigate feasibility using adversarial training sequential model continuous data evaluate using classical music freely available midi ﬁles. recurrent neural networks often used model sequences data. models usually trained using maximum likelihood criterion. e.g. language modelling trained predict next token point sequence i.e. model conditional probability next token given sequence preceding tokens. sampling conditional distribution generate reasonably realistic sequences sampling non-trivial usually resort beam-search generate good sequences tasks machine translation rnns used model music knowledge always symbolic representation. contrast work demonstrates train highly ﬂexible expressive model fully continuous sequence data tone lengths frequencies intensities timing. propose recurrent neural network architecture c-rnn-gan trained adversarial training model whole joint probability sequence able generate sequences data. system demonstrated training sequences classical music midi-format evaluated using metrics scale consistency tone range. conclude generative adversarial training viable training networks model distribution sequences continuous data potential also modelling many types sequential continuous data. work using rnns music generation includes modelling blues songs discrete tone values combining restricted boltzmann machines representing distinct tones. trained adversarial training applying policy gradient methods cope discrete nature symbolic representation employed. contrast this work represents tones using real valued continuous quadruplets frequency length intensity timing. allows standard backpropagation train whole model end-to-end. presented recurrent model adversarial training generate images. lapgan model another sequential model adversarial training generates images coarse-to-ﬁne fashion. proposed model recurrent neural network adversarial training. adversaries different deep recurrent neural models generator discriminator generator trained generate data indistinguishable real data discriminator trained identify generated data. training becomes zero-sum game nash equilibrium generator produces data discriminator cannot tell real data. deﬁne following loss functions sequence uniform random vectors sequence training data. dimensionality data random sequence.) input cell random vector concatenated output previous cell. feeding output previous cell common practice training rnns language models also used music composition discriminator consists bidirectional recurrent allowing take context directions account decisions. work recurrent network used long short-term memory internal structure gates help vanishing gradient problem learn longer dependencies work evaluate viability using generative adversarial models learn generating distribution behind classical music. inspired venerable midi format communicating signals digital musical instruments model signal four realvalued scalars every data point tone length frequency intensity time spent since previous tone. modelling data allows network represent polyphonous chords evaluate effect polyphony also experimented three tones represented output lstm cell tone represented quadruplet values described above. refer version c-rnn-gan-. similarly midi format absence tone represented zero intensity output. model layout details lstm network depth lstm cell internal units. bidirectional layout unidirectional. output lstm cell fully connected layer weights shared across time steps sigmoid output cell averaged ﬁnal decision sequence. baseline baseline recurrent network similar generator trained entirely predict next tone event point recurrence. dataset training data collected form music ﬁles midi format containing well-known works classical music. midi event type note loaded saved together duration tone intensity time since beginning last tone. tone data internally represented corresponding sound frequency. internally data normalized tick resolution quarter note. data contains midi ﬁles different composers classical music. source code available github including utility download data used study different websites. training backpropagation time mini-batch stochastic gradient descent used. learning rate apply regularization weights model pretrained epochs squared error loss predicting next event training sequence. adversarial setting input lstm cell random vector concatenated output previous time step. uniformly distributed chosen number features tone pretraining used schema sequence length beginning short sequences randomly sampled training data eventually training model increasingly long sequences. consider form curriculum learning begin learning short passages relations points near time. adversarial training noticed become strong resulting gradient cannot used improve effect particularly clear network initialized without pretraining. reason apply freezing means stopping updates whenever training loss less training loss corresponding thing become strong. also employ feature matching approach encourage greater variance avoid overﬁtting current discriminator replacing standard generator loss normally objective generator maximize error discriminator makes feature matching objective instead produce internal representation level discriminator matches real data. choose representations last layer ﬁnal logistic classiﬁcation layer deﬁne objective figure statistics generated music evaluated models. c-rnn-gan generates music increasing complexity training proceeds. number unique tones used vaguely increasing trend scale consistency seems stabilize ﬁfteen epochs. -tone repetition increasing trend ﬁrst epochs stays quite level seemingly correlated number tones used. baseline model reach level variation. number unique tones used consistently much lower scale consistency seems similar c-rnn-gan. tone span follows number unique tones closely c-rnn-gan suggesting baseline less variability tones used. c-rnn-gan- obtains higher polyphony score contrast c-rnn-gan baseline. reaching state many zero-valued outputs around epoch c-rnn-gan- reaches substantially higher values tone span number unique tones intensity span tone repetitions. real music intensity span similar generated music. scale consistency slightly higher also varies more. polyphony score similar c-rnn-gan-. -tone repetitions much higher difﬁcult compare songs different length. count normalized dividing lr/lg length real music length generated music. evaluation c-rnn-gan done using number measurements generated output. polyphony measuring often tones played simultaneously note rather restricted metric give score music simultaneous tones start exactly time. scale consistency computed counting fraction tones part standard scale reporting number best matching scale. repetitions short subsequences counted giving score much recurrence sample. metric takes tones order account timing. tone span number half-tone steps lowest highest tone sample. tool implemented compute estimates available github together source code used work. results experimental study presented figure adversarial training helps model learn patterns variability larger tone span larger intensity span. allowing model output tone lstm cell helps generate music higher polyphony score. performed thorough listening study work impressions author co-workers model trained feature matching gets better trade-off structure surprise variants. versions maximum-likelihood pretraining tends give enough surprise versions mini-batch features tended sample music little structure interesting real listener. surprising something intend look into. music http//mogren.one/publications//c-rnn-gan/. paper proposed recurrent neural model continuous data trained using approach based generative adversarial networks. experimentation needs done believe results promising. noted adversarial training helps recurrent neural network generate music varies number tones used span intensities played tones. generated music compare music training data human judgement. reasons remain explored. however evaluation scores music generated using c-rnn-gan show resemblance scores real music music generated using baseline. generated music polyphonous polyphony score experimental evaluation measuring often tones played exactly time c-rnn-gan scored low. allowing lstm cell output three tones time resulted model scored much better polyphony. hear generated samples timing vary quite sample sample generally rather consistent within track giving feeling tempo generated songs. references soumith chintala alec radford luke metz. unsupervised representation learning deep convolutional generative adversarial networks. international conference learning representations emily denton soumith chintala fergus deep generative image models using laplacian pyramid adversarial networks. advances neural information processing systems pages douglas juergen schmidhuber. finding temporal structure music blues improvisation lstm recurrent networks. neural networks signal processing proceedings ieee workshop pages ieee sepp hochreiter. vanishing gradient problem learning recurrent neural nets problem solutions. international journal uncertainty fuzziness knowledge-based systems pascal vincent nicolas boulanger-lewandowski yoshua bengio. modeling temporal dependencies high-dimensional sequences application polyphonic music generation transcription. proceedings international conference machine learning page salimans goodfellow wojciech zaremba vicki cheung alec radford chen. improved techniques training gans. advances neural information processing systems pages", "year": 2016}