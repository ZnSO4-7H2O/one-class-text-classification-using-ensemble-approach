{"title": "Meta-Learning and Universality: Deep Representations and Gradient  Descent can Approximate any Learning Algorithm", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "Learning to learn is a powerful paradigm for enabling models to learn from data more effectively and efficiently. A popular approach to meta-learning is to train a recurrent model to read in a training dataset as input and output the parameters of a learned model, or output predictions for new test inputs. Alternatively, a more recent approach to meta-learning aims to acquire deep representations that can be effectively fine-tuned, via standard gradient descent, to new tasks. In this paper, we consider the meta-learning problem from the perspective of universality, formalizing the notion of learning algorithm approximation and comparing the expressive power of the aforementioned recurrent models to the more recent approaches that embed gradient descent into the meta-learner. In particular, we seek to answer the following question: does deep representation combined with standard gradient descent have sufficient capacity to approximate any learning algorithm? We find that this is indeed true, and further find, in our experiments, that gradient-based meta-learning consistently leads to learning strategies that generalize more widely compared to those represented by recurrent models.", "text": "learning learn powerful paradigm enabling models learn data effectively efﬁciently. popular approach meta-learning train recurrent model read training dataset input output parameters learned model output predictions test inputs. alternatively recent approach meta-learning aims acquire deep representations effectively ﬁne-tuned standard gradient descent tasks. paper consider meta-learning problem perspective universality formalizing notion learning algorithm approximation comparing expressive power aforementioned recurrent models recent approaches embed gradient descent meta-learner. particular seek answer following question deep representation combined standard gradient descent sufﬁcient capacity approximate learning algorithm? indeed true experiments gradient-based meta-learning consistently leads learning strategies generalize widely compared represented recurrent models. deep neural networks optimize effective representations enjoyed tremendous success human-engineered representations. meta-learning takes step optimizing learning algorithm effectively acquire representations. common approach metalearning train recurrent memory-augmented model recurrent neural network take training dataset input output parameters learner model alternatively approaches pass dataset test input model outputs corresponding prediction test example recurrent models universal learning procedure approximators capacity approximately represent mapping dataset test datapoint label. however depending form model lack statistical efﬁciency. contrast aforementioned approaches recent work proposed methods include structure optimization problems meta-learner particular model-agnostic meta-learning optimizes initial parameters learner model using standard gradient descent learner’s update rule then meta-test time learner trained gradient descent. incorporating prior knowledge gradient-based learning maml improves statistical efﬁciency black-box meta-learners successfully applied range meta-learning problems cost? natural question arises purely gradient-based meta-learners maml whether indeed sufﬁcient learn initialization whether representational power fact lost learning update rule. intuitively might surmise learning update rule expressive simply learning initialization gradient descent. paper seek answer following question simply learning initial parameters deep neural network representational power arbitrarily expressive meta-learners directly ingest training data meta-test time? concisely representation combined standard gradient descent sufﬁcient capacity constitute learning algorithm? analyze question standpoint universal function approximation theorem. compare theoretical representational capacity meta-learning approaches deep network updated gradient step meta-learner directly ingests training test input outputs predictions test input studying universality maml that sufﬁciently deep learner model maml theoretical representational power recurrent meta-learners. therefore conclude that using deep expressive function approximators theoretical disadvantage terms representational power using maml black-box meta-learner represented example recurrent network. since maml representational power universal meta-learner next question might beneﬁt using maml approach? study question analyzing effect continuing optimization maml performance. although maml optimizes network’s parameters maximal performance ﬁxed small number gradient steps analyze effect taking substantially gradient steps meta-test time. initializations learned maml extremely resilient overﬁtting tiny datasets stark contrast conventional network initialization even taking many gradient steps used meta-training. also maml initialization substantially better suited extrapolation beyond distribution tasks seen meta-training time compared meta-learning methods based networks ingest entire training set. analyze setting empirically provide intuition explain effect. preliminaries section review universal function approximation theorem extensions considering universal approximation learning algorithms. also overview model-agnostic meta-learning algorithm architectural extension section universal function approximation universal function approximation theorem states neural network hidden layer ﬁnite width approximate continuous function compact subsets arbitrary precision theorem holds range activation functions including sigmoid relu functions. function approximator satisﬁes deﬁnition often referred universal function approximator similarly deﬁne universal learning procedure approximator input output denotes training dataset test input denotes desired test output. furthermore hornik showed neural network single hidden layer simultaneously approximate function derivatives mild assumptions activation function used target function’s domain. property section part meta-learning universality result. model-agnostic meta-learning bias transformation model-agnostic meta-learning method proposes learn initial parameters gradient steps computed using small amount data task leads effective generalization task tasks typically correspond supervised classiﬁcation regression problems also correspond reinforcement learning problems. maml objective computed many tasks {tj} follows α∇θl) corresponds training task outer loss evaluates generalization test data multiple gradient steps; though paper focus single gradient step setting. meta-training wide range tasks model quickly efﬁciently learn held-out test tasks running gradient descent starting meta-learned representation maml compatible neural network architecture differentiable loss function recent work observed architectural choices improve performance. particularly effective modiﬁcation introduced finn concatenate vector parameters input. model parameters updated inner loop gradient descent initial value meta-learned. modiﬁcation referred bias transformation increases expressive power error gradient without changing expressivity model itself. finn report empirical beneﬁt modiﬁcation architectural design symmetry-breaking mechanism universality proof. meta-learning universality broadly classify rnn-based meta-learning methods categories. ﬁrst approach meta-learner model parameters takes input dataset particular task test input outputs estimated output input meta-learner typically recurrent model iterates dataset input recurrent neural network model satisﬁes theorem approach maximally expressive represent function dataset test input second approach meta-learner takes input dataset particular task current weights learner model outputs parameters learner model. then test input learner model produce predicted output process written follows note that form written above approach expressive previous approach since meta-learner could simply copy dataset predicted weights reducing model takes input dataset test example. several versions approach i.e. ravi larochelle malik recurrent meta-learner operate order-invariant features gradient objective value averaged datapoints dataset rather operating individual datapoints themselves. induces potentially helpful inductive bias disallows coupling datapoints ignoring ordering within dataset. result meta-learning process produce permutation-invariant functions dataset. denotes initial parameters model also corresponds parameters meta-learned corresponds loss function respect label prediction. since approaches approximate update rule clearly least expressive gradient descent. less obvious whether maml update imposes constraints learning procedures acquired. study question deﬁne universal learning procedure approximator learner approximate function training datapoints test point clear fmaml approximate function theorem; however obvious fmaml represent function input output pairs since theorem consider gradient operator. ﬁrst goal paper show fmaml; universal function approximator one-shot setting dataset consists single datapoint then consider case k-shot learning showing fmaml; universal functions invariant permutation datapoints. cases discuss meta supervised learning problems discrete continuous labels loss functions universality hold. possible model must neural network least hidden layers since dataset copied ﬁrst layer weights predicted output must universal function approximator dataset test input. figure deep fully-connected neural network layers relu nonlinearities. generic fully connected network prove that single step gradient descent model approximate function dataset test input. universality one-shot gradient-based learner ﬁrst introduce proof universality gradient-based meta-learning special case training point corresponding one-shot learning. denote training datapoint test input universal learning algorithm approximator corresponds ability meta-learner represent function ftarget) arbitrary precision. proceed construction showing exists neural network function approximates ftarget) arbitrary precision α∇θ) non-zero learning rate. proof holds standard multi-layer relu network provided sufﬁcient depth. discuss section loss function cannot loss function standard cross-entropy mean-squared error objectives suitable. proof start presenting form deriving value gradient step. then show universality construct setting weight matrices enables independent control information coming forward backward start constructing which shown figure generic deep network layers relu nonlinearities. note that particular weight matrix layer single gradient step α∇wi represent rank- update matrix gradient outer product vectors aibt error gradient respect pre-synaptic activations layer forward post-synaptic activations layer expressive power single gradient update single weight matrix corresponding multiple linear layers possible acquire rank-n update linear function represented note deep relu networks like deep linear networks input pre-synaptic activations non-negative. motivated reasoning construct deep relu network number intermediate layers linear layers ensure showing input pre-synaptic activations layers non-negative. allows simplify analysis. simpliﬁed form model follows represents input feature extractor parameters scalar bias transfori= product square linear weight matrices fout function output learned parameters {θft θb{wi} θout}. input feature extractor output function represented fully connected neural networks hidi= corresponds disregard last term assuming comparatively small higher order terms vanish. general terms necessarily need vanish likely would improve expressiveness gradient update disregard sake simplicity derivation. ignoring terms note post-update value provided input given fout; goal show exists setting fout function approximate function show universality indepenˆ dently control information multiplexing forward information backward information achieve decomposing error gradient three parts follows initial value components equal numbers rows middle components. result likewise made three components denote lastly construct component error gradient whereas middle bottom components linear function discuss achieve gradient latter part section deﬁne fout section appendix show choose particular form simplify products matrices equation following form chosen symmetric positive-deﬁnite matrix chosen positive deﬁnite matrix. appendix show deﬁnitions weight matrices satisfy condition activations non-negative meaning model represented generic deep network relu nonlinearities. finally need deﬁne function fout output. training input passed need fout propagate information label deﬁned equation test input passed need different function deﬁned thus deﬁne fout neural network approximates following multiplexer function derivatives summary chosen particular form weight matrices feature extractor output function decouple forward backward information recover post-update function above. goal show function universal learning algorithm approximator function notational clarity denote inner product equation noting viewed type kernel rkhs deﬁned connection kernels fact needed proof provides convenient notation interesting observation. deﬁne following lemma intuitively equation viewed basis vectors weighted passed hpost produce output. likely number ways prove lemma appendix provide simple though inefﬁcient proof brieﬂy summarize here. deﬁne indicator function indicating takes particular value indexed then deﬁne vector containing information then result summation vector containing information label value indexed finally hpost deﬁnes output value bias transformation variable plays vital role construction breaks symmetry within ki). without asymmetry would possible constructed function represent function gradient step. conclusion shown exists neural network structure universal approximator ftarget). chose particular form decouples forward backward information ﬂow. choice possible impose desired post-update function even face adversarial training datasets loss functions e.g. gradient points wrong direction. make assumption inner loss function training dataset chosen adversarially error gradient points direction improvement likely much simpler architecture sufﬁce require multiplexing forward backward information separate channels. informative loss functions training data allowing simpler functions indicative inductive bias built gradient-based meta-learners present recurrent meta-learners. result section implies sufﬁciently deep representation combined single gradient step approximate one-shot learning algorithm. next section show universality maml k-shot learning algorithms. consider general k-shot setting aiming show maml approximate permutation invariant function dataset test datapoint ...k} note need small. reduce redundancy overview differences -shot setting section. include full proof appendix k-shot setting parameters updated according following rule appendix show function approximate function ...k} invariant ordering training datapoints ...k}. showing select setting vector containing discretization frequency counts discretized datapoints. vector completely describes without loss information hpost universal function approximator approximate continuous function compact subsets rdim. it’s also worth noting form equation greatly resembles kernel-based function approximator around training points substantially efﬁcient universality proof likely obtained starting premise. loss functions previous sections showed deep representation combined gradient descent approximate learning algorithm. section discuss requirements loss function must satisfy order results sections hold. might expect main requirement label recoverable gradient loss. seen deﬁnition fout equation pre-update function given gpre gpre used back-propagating information label learner. stated equation require error gradient respect make term gradient equal causes pre-update ∇ˆy. prediction next note thus linear function require loss function linear function invertible. essentially needs recoverable loss function’s gradient. appendix prove following theorems thus showing standard cross-entropy losses allow universality gradient-based meta-learning. theorem gradient standard mean-squared error objective evaluated linear invertible function theorem gradient softmax cross entropy loss respect pre-softmax logits linear invertible function evaluated consider popular loss functions whose gradients satisfy label-linearity property. gradients hinge losses piecewise constant thus allow universality. huber loss also piecewise constant areas domain. error functions effectively lose information simply looking gradient insufﬁcient determine label. recurrent meta-learners take gradient input rather label e.g. andrychowicz also suffer loss information using error functions. experiments shown meta-learners standard gradient descent sufﬁciently deep representation approximate learning procedure equally expressive recurrent learners natural next question empirical beneﬁt using meta-learning approach versus another cases? answer question next empirically study figure effect additional gradient steps test time attempting solve tasks. maml model trained inner gradient steps improve steps. methods provided data examples gradient step computed using datapoints. figure learning performance out-of-distribution tasks function task variability. recurrent meta-learners snail metanet acquire learning strategies less generalizable learned gradient-based meta-learning. inductive bias gradient-based recurrent meta-learners. then section investigate role model depth gradient-based meta-learning theory suggests deeper networks lead increased expressive power representing different learning procedures. first empirically explore differences gradient-based recurrent metalearners. particular answer following questions learner trained maml improve additional gradient steps learning tasks test time start overﬁt? inductive bias gradient descent enable better few-shot learning performance tasks outside training distribution compared learning algorithms represented recurrent networks? study questions consider simple fewshot learning domains. ﬁrst -shot regression family sine curves varying amplitude phase. trained models uniform distribution tasks amplitudes phases second domain -shot character classiﬁcation using omniglot dataset following training protocol introduced santoro comparisons recurrent meta-learners state-of-the-art meta-learning models snail metanetworks experiments also compare task-conditioned model trained input task description label. like maml task-conditioned model ﬁne-tuned data using gradient descent trained few-shot adaptation. include experimental details appendix figure comparison ﬁnetuning maml-initialized network network initialized randomly trained scratch. methods achieve training accuracy. maml also attains good test accuracy network trained scratch overﬁts catastrophically examples. interestingly mamlinitialized model begin overﬁt even though meta-training used steps graph shows answer ﬁrst question ﬁne-tuned model trained using maml many gradient steps used meta-training. results sinusoid domain shown figure show maml-learned initialization trained fast adaption steps improve beyond gradient steps especially out-of-distribution tasks. contrast task-conditioned model trained without maml easily overﬁt out-of-distribution tasks. omniglot dataset seen figure maml model trained inner gradient steps ﬁne-tuned gradient steps without leading drop test accuracy. expected model initialized randomly trained scratch quickly reaches perfect training accuracy overﬁts massively examples. next investigate second question aiming compare maml state-of-the-art recurrent meta-learners tasks related outside distribution training tasks. three methods achieved similar performance within distribution training tasks -way -shot omniglot classiﬁcation -shot sinusoid regression. omniglot setting compare method’s ability distinguish digits sheared scaled varying amounts. sinusoid regression setting compare sinusoids extrapolated amplitudes within phases within results figure appendix show clear trend maml recovers generalizable learning strategies. combined theoretical universality results experiments indicate deep gradient-based meta-learners equivalent representational power recurrent meta-learners also considered strong contender settings contain domain shift meta-training meta-testing tasks strong inductive bias reasonable learning strategies provides substantially improved performance. effect depth proofs sections suggest gradient descent deeper representations results expressive learning procedures. contrast universal function approximation theorem requires single hidden layer approximate function. seek empirically explore theoretical ﬁnding aiming answer question scenario model-agnostic meta-learning requires deeper representation achieve good performance compared depth representation needed solve underlying tasks learned? figure comparison depth keeping number parameters constant. task-conditioned models need hidden layer whereas meta-learning maml clearly beneﬁts additional depth. error bars show standard deviation three training runs. answer question study simple regression problem meta-learning goal infer polynomial function input/output datapoints. polynomials degree coefﬁcients bias sampled uniformly random within input values range within similar conditions proof meta-train meta-test gradient step mean-squared error objective relu nonlinearities bias transformation variable dimension compare relationship depth expressive power compare models ﬁxed number parameters approximately vary network depth hidden layers. point comparison models trained meta-learning using maml trained standard feedforward models regress input -dimensional task description output. task-conditioned models oracle meant empirically determine depth needed represent polynomials independent meta-learning process. theoretically would expect task-conditioned models require hidden layer universal function approximation theorem. contrast would expect maml model require depth. results shown figure demonstrate task-conditioned model indeed beneﬁt hidden layer whereas maml clearly achieves better performance depth even though model capacity terms number parameters ﬁxed. empirical effect supports theoretical ﬁnding depth important effective meta-learning using maml. conclusion paper show exists form deep neural network initial weights combined gradient descent approximate learning algorithm. ﬁndings suggest that standpoint expressivity theoretical disadvantage embedding gradient descent meta-learning process. fact experiments found learning strategies acquired maml successful faced out-of-domain tasks compared recurrent learners. furthermore show representations acquired maml highly resilient overﬁtting. results suggest gradient-based meta-learning number practical beneﬁts theoretical downsides terms expressivity compared alternative meta-learning models. independent type meta-learning algorithm formalize means meta-learner able approximate learning algorithm terms ability represent functions dataset test inputs. formalism provides perspective learning-to-learn problem hope lead discussion research goals methodology surrounding meta-learning. thank sharad vikram detailed feedback proof well justin ashvin nair kelvin feedback early draft paper. also thank erin grant helpful conversations nikhil mishra providing code snail. research supported national science foundation iis- graduate research fellowship well nvidia. references marcin andrychowicz misha denil sergio gomez matthew hoffman david pfau schaul nando freitas. learning learn gradient descent gradient descent. neural information processing systems adam santoro sergey bartunov matthew botvinick daan wierstra timothy lillicrap. meta-learning memory-augmented neural networks. international conference machine learning prove lemma proceed showing choose summation contains complete description values then hpost universal function approximator able approximate function since essentially ignore ﬁrst last elements deﬁning small positive constant ensure positive deﬁniteness. then rewrite summation omitting ﬁrst last terms deﬁned function next deﬁne terms sum. goal summation contain complete information chose linear function outputs stacked copies then deﬁne matrix selects copy position corresponding i.e. position achieved using diagonal matrix diagonal values positions corresponding vector elsewhere used ensure positive deﬁnite. result post-update function follows position within vector satisﬁes discr satisﬁes discr) note vector complete description decoded therefore since hpost universal function approximator input contains information function hpost universal function approximator respect inputs initial value components equal dimensions middle components bottom components scalars. result likewise made three components denote where lastly construct component error gradient whereas middle bottom components linear function using deﬁnitions noting equation middle component following output function section derive post-update version output function fout. recall fout deﬁned neural network approximates following multiplexer function derivatives parameters part θout addition parameters required estimate indicator functions corresponding products. since hpost gradient step taken error gradients respect parameters last term equation approximately zero. furthermore seen deﬁnition gpre section value gpre also zero resulting gradient approximately zero ﬁrst indicator function. post-update value fout therefore appendix provide full proof universality gradient-based meta-learning general case datapoints. proof share content proof -shot setting include completeness. show deep representation combined step gradient descent approximate permutation invariant function dataset test datapoint ...k} note need small. proceed construction exists neural network function approximates ftargetk} arbitrary precision ∇θ)) learning rate. discuss section loss function cannot loss function standard cross-entropy mean-squared error objectives suitable. proof start presenting form deriving value gradient step. then show universality construct setting weight matrices enables independent control information coming forward inputs {xk} backward labels {yk}. start constructing motivation section construct following represents input feature extractor parameters scalar bias transformation product square linear weight matrices fout readout function output learned parameters {θft θb{wi} θout}. input feature extractor readout function represented fully connected neural networks corresponds linear layers. note deep relu networks like deep linear networks input pre-synaptic activations non-negative. later show indeed case within linear layers meaning neural network function fully generic represented deep relu networks visualized figure next derive form post-update prediction denote gradient respect loss gradient respect weight matrices single datapoint given move summation left disregard last term assuming comparatively small higher order terms vanish. general terms necessarily need vanish likely would improve expressiveness gradient update disregard sake simplicity derivation. ignoring terms note post-update value provided input given fout; goal show exists setting fout function approximate function show universality independently control information {xk} {yk} multiplexing forward information {xk} backward information {yk}. achieve decomposing error gradient three parts follows initial value components equal numbers rows middle components. result likewise made three components denote ˇzk. lastly construct component error gradient whereas middle bottom components linear function discuss achieve gradient latter part section deﬁne fout section appendix show choose particular form simplify products matrices equation following form chosen symmetric positive-deﬁnite matrix chosen positive deﬁnite matrix. appendix show deﬁnitions weight matrices satisfy condition activations non-negative meaning model represented generic deep network relu nonlinearities. finally need deﬁne function fout output. training input passed need fout propagate information corresponding label deﬁned equation test input passed need function deﬁned thus deﬁne fout neural network approximates following multiplexer function derivatives summary chosen particular form weight matrices feature extractor output function decouple forward backward information recover post-update function above. goal show function universal learning algorithm approximator function notational clarity denote inner product equation noting viewed type kernel rkhs deﬁned intuitively equation viewed basis vectors weighted passed hpost produce output. appendix show choose equation approximate continuous function compact subsets rdim. one-shot setting bias transformation variable plays vital role construction breaks symmetry within ki). without asymmetry would possible constructed function represent function gradient step. conclusion shown exists neural network structure universal approximator ftargetk} section show form approximate function ...k} invariant ordering training datapoints ...k}. proof similar one-shot setting proof appendix similar appendix ignore ﬁrst last elements deﬁning small positive constant ensure positive deﬁniteness. re-index ﬁrst summation ...n instead indexing variables follows discr denotes function produces one-hot discretization input denotes -indexed standard basis vector. likewise chose linear function outputs stacked copies then deﬁne matrix selects copy position corresponding i.e. position achieved using diagonal matrix diagonal values positions corresponding vector elsewhere used ensure positive deﬁnite. position within vector satisﬁes discr satisﬁes discr discrete one-shot labels summation amounts frequency counts triplets yk). setting continuous labels cannot attain frequency counts access discretized version label. thus must make assumption datapoints share input value assumption summation contain output values index corresponding value discrete continuous labels representation redundant nonetheless contains sufﬁcient information decode test input datapoints since hpost universal function approximator input contains information function hpost universal function approximator respect appendix show network architecture linear layers analyzed sections represented deep network relu nonlinearities. showing input activations within linear layers non-negative. first consider input input deﬁned consist three terms. term deﬁned appendices discretization parameters updated. middle term deﬁned constant bottom term deﬁned gradient update used afterward. next consider weight matrices determine activations non-negative positive semi-deﬁnite. appendix ˜mi+ deﬁned positive deﬁnite; appendix thus conditions products satisﬁed. appendices deﬁned symmetric positive deﬁnite matrices. appendix deﬁne thus also symmetric positive deﬁnite therefore positive deﬁnite. finally purpose weights provide nonzero gradients input thus positive value sufﬁce. figure left another comparison out-of-distribution tasks varying phase sine curve. clear trend gradient descent enables better generalization out-of-distribution tasks compared learning strategies acquired using recurrent meta-learners snail. right another example shows resilience maml-learned initialization overﬁtting. case maml model trained using inner step gradient descent -way -shot omniglot classiﬁcation. maml random initialized network achieve perfect training accuracy. expected model trained scratch catastrophically overﬁts training examples. however maml-initialized model begin overﬁt even gradient steps. standard softmax cross-entropy loss function discrete one-hot labels gradient constant vector value denoting pre-softmax logits. since one-hot representation rewrite gradient constant matrix value since invertible cross entropy loss also satisﬁes requirement. thus shown standard supervised objectives mean-squared error cross-entropy allow universality gradientbased meta-learning. section provide additional comparisons out-of-distribution task using additional gradient steps shown figure also include additional experimental details. omniglot meta-learning methods trained using code provided authors respective papers using default model architectures hyperparameters. model embedding architecture across methods using convolutional layers kernels ﬁlters stride batch normalization relu nonlinearities. convolutional layers followed single linear layer. methods used adam optimizer default hyperparameters. hyperparameter choices speciﬁc algorithm found respective papers. maml sinusoid domain used fully-connected network hidden layers size relu nonlinearities bias transformation variable size concatenated input. model trained meta-iterations inner gradient steps size snail sinusoid domain model consisted blocks following dilated convolutions kernels channels dilation size respectively attention block key/value dimensionality ﬁnal layer convolution output. like maml model trained convergence iterations using adam default hyperparameters. evaluated maml snail models trials reporting mean conﬁdence intervals. computational reasons evaluated metanet model using trials also reporting mean conﬁdence intervals. following prior work downsampled omniglot images scaling shearing digits produce out-of-domain data transformed original omniglot images downsampled depth comparison models trained convergence using iterations. model deﬁned ﬁxed number hidden units based total number parameters number hidden layers. thus models hidden layers units layer respectively. model hidden layer found using hidden units corresponding parameters resulted poor performance. thus results reported paper used model hidden layer units performed much better. trained model three times report mean standard deviation three runs. performance individual computed using average", "year": 2017}