{"title": "Bayesian Inference in Monte-Carlo Tree Search", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Monte-Carlo Tree Search (MCTS) methods are drawing great interest after yielding breakthrough results in computer Go. This paper proposes a Bayesian approach to MCTS that is inspired by distributionfree approaches such as UCT [13], yet significantly differs in important respects. The Bayesian framework allows potentially much more accurate (Bayes-optimal) estimation of node values and node uncertainties from a limited number of simulation trials. We further propose propagating inference in the tree via fast analytic Gaussian approximation methods: this can make the overhead of Bayesian inference manageable in domains such as Go, while preserving high accuracy of expected-value estimates. We find substantial empirical outperformance of UCT in an idealized bandit-tree test environment, where we can obtain valuable insights by comparing with known ground truth. Additionally we rigorously prove on-policy and off-policy convergence of the proposed methods.", "text": "monte-carlo tree search methods drawing great interest yielding breakthrough results computer paper proposes bayesian approach mcts inspired distributionfree approaches signiﬁcantly diﬀers important respects. bayesian framework allows potentially much accurate estimation node values node uncertainties limited number simulation trials. further propose propagating inference tree fast analytic gaussian approximation methods make overhead bayesian inference manageable domains preserving high accuracy expected-value estimates. substantial empirical outperformance idealized bandit-tree test environment obtain valuable insights comparing known ground truth. additionally rigorously prove on-policy oﬀ-policy convergence proposed methods. monte-carlo tree search methods provide means on-the-ﬂy planning complex sequential decision problems. mcts couples traditional tree search techniques node evaluations based stochastic simulations i.e. playouts. mcts spectacular results domain computer within years best programs gone weak intermediate play grandmasgo. mcts also soundly defeated traditional alphabeta search programming recent general game-playing competition held aaai result intense interest developmcts algorithms ﬁrst developed speciﬁcally computer recently number general theoretically principled algorithms developed including bast algorithms build upon seminal work bandit sampling algorithm focus providing strong worst-case distribution-free convergence guarantees bounds cumulative regret. also merit computational complexity. great results obtained using french program mogo several programs probably reponsible current excitement mcts methods. contrast above-mentioned distribution-free algorithms propose paper fundamentally diﬀerent approach based bayesian inference. basic premise work that practical applications mcts mdps games developers usually know detailed characteristics reward distributions domain. fact note current applications mcts envision trials performed exclusively simulation; accurate reward model thus prerequisite performing faithful simulated mcts trials. algorithms making reward models could potentially outperform distribution-free methods assume knowledge reward distributions. proposed bayesian mcts approach stochastic trial results leaf nodes combined prior reward information yield posterior distributions; propagate upward according appropriate inference model determine interior node distributions. mdps parent’s distribution obtained applying distributional operator child node distributions two-player zero-sum games parent nodes operators according odd/even node depths. leaf node priors inference models correct methodology enable bayes-optimal estimation interior node values. show section estimates much accurate limited number trials analogous estimates based simple average-value backup. noted average-value estimates very ineﬃcient often signiﬁcantly underestimate true expected-max value parent nodes. problem becomes especially pronounced trees non-uniform tree width bayesian mcts substantially outperform case. recognize bayesian inference entails greater computational cost existing mcts methods therefore automatically provide terms quality solution given amount simulation time. however discuss section recent advances fast analytic gaussian approximation min/max distributions reduce bayesian mcts compute times within order magnitude uct. suggests bayesian mcts eﬀective applications cost executing simulation steps greatly exceeds time needed compute mcts sampling decisions. many examples compute-intensive simulation domains e.g. routing scheduling large-scale networks drug design computational physics. game also property node evaluated executing hundred steps stochastic playout policy terminal state reached. addition expect signiﬁcant speedups obtained developing incremental versions approximation methods; topic ongoing future work. ﬁnal potential advantage bayesian mcts allow robust convergence wide range sampling policies. convergence relies focusing vast majority trials optimal path. contrast intuitively clear bayesian mcts converge correct minimax values even uniform random sampling. section provide sketches rigorous proofs onpolicy oﬀ-policy convergence bayesian version uct. expect oﬀ-policy convergence enable robust convergence massively parallel implementations mcts would difﬁcult achieve ucb’s focusing eﬀect. probabilistic inference previously studied game-tree contexts ﬁrst study treat uncertainties arising stochastic sampling. moreover despite title believe work ﬁrst truly bayesian approach minimax trees employs context multi-stage decision problems generative model available simulate problem. algorithm intended apply variety problems including games strict tree structures well general mdps allowing trajectories containing loops. consequently give precise speciﬁcation regarding state space representation nodes added tree/state space time. characteristics precisely speciﬁed number simulated trials launched common start state/node average reward statistics maintained states/nodes representation encountered trials; decision every step made maximizing bound formula equation common implementation two-player games makes policy plus additional playout policy. trials begin root node descend tree according node encountered child moves represented tree nodes. point leaf node created play continues using playout policy terminal game state reached. trial result added average reward statistics every tree node path trial next trial starts root node. selective eﬀect trials gradually focus minimax optimal line play result average reward values eventually converge correct game-theoretic values. crude estimate uncertainty node value. much informed uncertainty estimate could obtained based number child nodes associated values uncertainties. present bayesian reformulation mcts comprising mechanisms computing leaf node posterior distributions propagating inference tree compute interior node distributions computing distribution-based upper conﬁdence estimates basis choosing sample next. formal description limited case uncorrelated payoﬀs leaf nodes although gaussian approximation framework section permit usage general correlation matrices pairs sibling node distributions. assume static leaf nodes prior/posterior calculations performed. additionally assume strict tree structure loops duplicate nodes initial formulation include loopy belief propagation. node tree maintains probability distribution true expected reward value. inference interior node probability distributions begins leaves propagates root node. trials performed leaf nodes initialized conjugate prior distributions appropriate leaf node reward distributions. example payoﬀs leaf nodes eﬀectively number prior wins losses. trials leaf nodes performed results combined priors standard compute posterior distributions. case sample wins sample losses obtain posterior adding expression. propagate posteriors tree represent either numerically gaussian matching mean standard deviation detailed section distributions leaf nodes propagate upward parent grandparent etc. nodes. interior node computes extremum distribution child node distributions depending type. extremum distribution independent rancomputed distributions interior nodes consider distributional analogs existing distribution-free sampling formulae. initial studies focused speciﬁcally bayesian analogs simplicity practical success applications propose modiﬁed versions descend tree choose sample next. ﬁrst version simply replaces average reward child node mean strongly believe equation constitutes strict improvement independence assumption leaf node priors correct since posterior mean provides accurate estimator true node value average reward ¯ri. equation motivated central limit theorem result compelling intuitive notion interval estimation sampling according expected value plus expected uncertainty provides eﬀective tradeoﬀ exploration exploitation. uncertainty measures credible interval even eﬀective using provides simplest approach begin investigations uncertainty-based sampling using distributional information. provide sketches rigorous convergence proof equation well general oﬀ-policy convergence below. theorem consider ﬁxed ﬁnite-sized bandit tree binary reward leaf nodes priors lemma assume sibling nodes exactly identical minimax payoﬀ rates. sampling policy samples leaf nodes unbounded number times total number samproof sketch show parent’s child nodes converge delta functions correct minimax values parent’s distribution converges distribution best child according ﬁnite separation child values. induction leaf node convergence lemma interior nodes converge. primary approach representing probability distributions approximate gaussians using recent advances fast analytic computation max/min gaussians propagate inference tree. potential major advantages approach orders magnitude faster numeric methods plus also take correlations between gaussians account. speciﬁcally correlation model available sibling nodes estimate correlation max/min distribution pair siblings third sibling. make property test domain likely play important role development bayesian mcts methods. potential disadvantage gaussian approximation approximation errors could arise sources. first leaf node distributions trials certain priors poorly approximated gaussians. secondly known gaussian family closed operation making approximation introduce error. address issue below. uncorrelated case largest approximation errors occur input gaussians similar means diﬀerent widths. fortunately bandit sampling algorithms prevent happening explain later section closed-form analytic expressions mean variance maximum distribution gaussian random variables ﬁrst calculated clarke denote means denote standard deviations input gaussians correlation coeﬃcient expanding moment generating function yields following expressions standard deviation provides interesting test bayes-uct using uniform prior former case prior correct latter case assumed prior diﬀers substantially correct prior. interior node tree typically common width although also experiments node’s width uniform random choice minimum maximum width. assume tree topology bayes-uct searches static i.e. tree nodes already contained representation added either sampling prior modeling node additions occur searches. figure compares bayes-uct bayes-uct algorithms simple -ply minimax bandit tree topology containing root node children children simple bandit arms. assigned uniform random leaf node payoﬀ rates bayes-uct uniform prior correct. function number simulator trials plot average greedy decision error i.e. true loss top-level move highest estimated mean value. plot averages results bandit trees single performed extending clarke’s work sinha recently studied error approximating max/min input gaussians iterative pairwise combination input gaussians combined approximated gaussian combined third gaussian input gaussians combined. error overall result depend signiﬁcantly order combination. sinha propose stagewise minimum-error combining scheme using error lookup table estimates error combining input pair based three independent quantities σi/σj deﬁned above. stage pairwise errors evaluated pair minimum error combined. implemented min-error combining scheme computing errors combining choose pairs storing heap. combine pair minimum error update combining errors iterate done. algorithm found give favorable approximation errors takes operations space store heap. also implemented much simpler random-order combining scheme appears perform well min-error combining bandit-tree domain running much faster. appears bandit-style sampling generally avoid worst-case gaussian approximation error scenarios discuss evidence section compare gaussian framework alternative numeric representation scheme distributions represented exactly inference propagates numeric integration. speciﬁcally grid points represent pdfs compute cdfs numeric integration using trapezoid rule error testbed initial studies bayes-uct idealized bandit-tree simulator constructs minmax tree depth starting root node leaf nodes ordinary bandit arms bandit trees. methods widened plots conﬁrm that bayes-uct slightly underperforms early simulation; bayes-uct outperforms bayes-uct; gaussian outperforms numeric representation. likewise ﬁgures show widening outperformance tree depth increases still bayes-uct outperforms bayes-uct. however gaussian performance appears weaken increasing depth gaussian approximation underperforms exact numeric representation. present quantitative evidence comparing number simulated trials needed bayes-uctg achieve expected error level also included runs employing gaussian distributed payoﬀ rates uniform prior used bayes-uct implementation incorrect. results shown table correct prior strong evidence scaling bayes-uctg advantage depth width. even incorrect prior less compelling still clear scaling. also inadvertently discovered uniform payoﬀs unable converge feasible simulation time whereas bayes-uct exhibits signs unusual convergence diﬃculty. tree. duplicate random trees algorithm pairwise comparisons algorithms much higher signiﬁcance using independent random trees algorithm. ﬁgure bayes-uctn bayes-uctg algorithms initially perform slightly worse achieve clearly lower errors approximately trials. bayes-uct algorithms outperform bayes-uct counterparts initially asymptotically bayes-uctg achieving clearly lowest error among algorithms. figure conﬁrms intuition bayes-uct outperform virtue accurate node value estimates. also conﬁrms expected improvement bayes-uct using accurate estimate node uncertainty exploration term. however given potential errrors gaussian approximation quite surprised gaussians methods outperform numeric counterparts also expected value estimates would progressively less accurate relative bayesoptimal estimates tree increases either width depth thus bayes-uct obtain greater outperformance either increases. intuition turned correct ﬁgure shows results similar experiments table number trials achieve average error rate various bandit tree topologies. ratio trials bayes-uctg trials increases width depth bandit tree. ﬁnal hypothesis investigated particular trouble trees non-uniform width neglecting width either expected node values exploration term. study this modiﬁed simulator generate random-width trees node assigned uniform random number child nodes number allowable children. experiment using two-ply random-width bandit trees mean width around level. experiments yielded striking results shown ﬁgure bayes-uct massively outperforms particularly correct prior still substantial improvement even using incorrect prior. table gives measures bayes-uct simulation speed representative experiments. since stochastic playouts negligible cost bayesuct obviously runs much slower uct. model expected behavior application like heavyweight playouts computed adjusted ﬁgures assuming playout simulation time msec trial. scenarios bayes-uct speed within factor compensated reduction required number trials. competitive programs playout simulation times msec common would make results look even favorable. chose conservative ﬁgure msec corresponding typical playout simulation time bayes-uct calculates much accurate node value estimates veriﬁed bandit tree experiment ﬁgure bayesuct’s node estimates much accurate simple estimates. seen ﬁgure plot average absolute error top-level node value estimates function many times sampled. every time top-level node sampled recompute probability distribution mean note absolute diﬀerence mean true value data point according total trials performed toplevel node. graph shows improve top-level however estimation accuracy entire story experiment shown ﬁgure experiment devised hybrid algorithm top-level preferences computed based bayesian estimates actual sampling decisions performed using uct. performance substantially worse full bayes-uct showing bayesuct derives signiﬁcant beneﬁt better sampling decisions addition accurate value estimates. remaining major puzzle gaussian approximation could well relative exact numeric distributions. turns estimated ﬁrst moments using gaussians extremely close essentially exact estimates obtained numeric integration. instances observed diﬀerence numeric mean gaussian mean accuracy predicting node’s true value comparing gaussian numeric accuracy ﬁgure takeaways observations gaussian approximation better realized estimating mean extremum distribution nature banditbased sampling tend encounter largeseveral extensions known literature; core search strategy provide alternative upper-bound functions guide node selection. many ways bayes-uct similar flat-ucb flat-ucb uses standard bound function terminal nodes bounds interior nodes computed maximum value bounds node’s children. bayes-uct extends approach using distributions including exploration term interior nodes. bast extensions provide better asymptotic regret smooth trees bast alternative bound function combines ucb-pac smoothing term represents smoothness local tree. uses principles bast bound function adapted solve continuous optimization problems. applicability bast similar problems questionable exhibit local smoothness bast designed exploit. vary depth instead constant value nodes game already decided. sample variance. paper proposes ucbtuned includes separate exploration term variance. authors demonstrate ucb-tuned outperforms simple bandit problems eﬀectiveness ucb-normal evaluated. results showing advantages bayes-uct bayes-uct suggest similar advantages might obtained ucb-normal standard uct. work natural evolution probabilistic minimax search applying mcts search static evaluation function needed. important computer design eﬀective evaluation functions elusive. error cases occur input gaussians nearly equal means diﬀerent sigmas. ever happen ucb-type sampling algorithms would devote many samples wider input shrink match narrower input. similarly scrutinized diﬀerences numeric analytic estimates sigma values. turns analytic estimates close numeric values except particular case mean close distribution highly skewed. case analytic sigmas systematically much higher correct values. argue error actually beneﬁt gaussian method setting sigma high gaussian capturing tail mass distribution higher-order moments. so-called tailed distributions desirable explore dictated true second moment distribution. figure average error estimated mean toplevel nodes. performance bayesian selection top-level moves hybrid algorithm samples guided normal uct. tree nodes oﬀering number theoretical advantages including on-policy well robust oﬀpolicy convergence guarantees. empirically obtained interesting surprising insights testing methods idealized bandit-tree domain. encouraged signiﬁcant scalable improvements even using incorrect prior especially case non-uniform tree widths. also surprised gaussian approximation framework turned accurate worked much better originally anticipated. immediate next step future work compare bast bayesian counterparts much done uct. bast already incorporates simple min/max operations bound computations designed scale better depth interesting whether bayes-bast obtain magnitude improvements uct. also interested explorations issues arising non-uniform tree width. regard interesting compare bayes-uct pac-ucb extension include number bandit arms estimating upper conﬁdence bounds. third important direction practical implementations develop incremental versions gaussian combining schemes studied here; could yield order magnitude speedup eliminate speed advantage bayes-uct. however important direction future practical bayesian mcts develop methods automatically modeling correlation sibling nodes given domain. gaussian min/max approximation framework allow correlated input gaussians unaware methods automatically estimate appropriate correlation given mdp/game state. appears main challenge applying bayes-uct computer crude correlation models shown advantage uct. currently investigating several possible machine learning approaches modeling correlations challenging single-agent planning problems. authors thank mark wegman kephart david silver csaba szepesvari several anonymous reviewers numerous helpful comments suggestions. work supported part darpa gale project contract hr-c-.", "year": 2012}