{"title": "Concept Modeling with Superwords", "tag": ["stat.ML", "cs.CL", "cs.IR", "cs.LG"], "abstract": "In information retrieval, a fundamental goal is to transform a document into concepts that are representative of its content. The term \"representative\" is in itself challenging to define, and various tasks require different granularities of concepts. In this paper, we aim to model concepts that are sparse over the vocabulary, and that flexibly adapt their content based on other relevant semantic information such as textual structure or associated image features. We explore a Bayesian nonparametric model based on nested beta processes that allows for inferring an unknown number of strictly sparse concepts. The resulting model provides an inherently different representation of concepts than a standard LDA (or HDP) based topic model, and allows for direct incorporation of semantic features. We demonstrate the utility of this representation on multilingual blog data and the Congressional Record.", "text": "information retrieval fundamental goal transform document concepts representative content. term representative challenging deﬁne various tasks require diﬀerent granularities concepts. paper model concepts sparse vocabulary ﬂexibly adapt content based relevant semantic information textual structure associated image features. explore bayesian nonparametric model based nested beta processes allows inferring unknown number strictly sparse concepts. resulting model provides inherently diﬀerent representation concepts standard based topic models allows direct incorporation semantic features. demonstrate utility representation multilingual blog data congressional record. information overload ubiquitous problem affects nearly members society researchers sifting millions scientiﬁc articles users trying gauge public opinion reading blogs. even information retrieval methods evolve move beyond traditional search paradigm varied retrieval tasks focused combating overload remain reliant suitable document representations. representations ultimately distill contents document collection fundamental concepts representing atomic units instance common desideratum retrieval tasks diversity result here document representation must expressive enough recognizable documents idea. ﬁne-grained representation lead many concepts meaning. example president obama barack obama president potus distinct named entities refer president united states distinct concepts depending representation. spectrum coarse-grained representations topics topic model conﬂate together many ideas vaguely related. vagueness particularly problem systems attempt personalize results user’s individual tastes need estimate user’s level interest concept paper seek learn concept representations appropriate level granularity representing concept words functionally equivalent particular task hand. take computer vision community refer concepts superwords. desire following characteristics model approach addresses properties relying machinery bayesian nonparametric methods. particular nested beta process prior provide strict sparsity concepts used document words associated concept time allowing uncertainty number concepts prior encourages sharing concepts word choices among documents provides ﬂexibility documents diﬀer. instance democrats healthcare reform republicans obamacare referring concept. previous models assume topics document model phenomenon either create multiple topics refer idea else single conﬂated topic probability mass words informed populations. avoid undesirable options using expressive prior. moreover described above inherent uncertainty granularity concepts. speciﬁcally choose concepts fewer words fewer concepts words? question identiﬁability nested beta process. thus inform sought-after sparsity structure augment prior semantic feature matrix word vocabulary associated observed feature vector. matrix additional beneﬁt allowing fuse multiple sources information. example feature vector capture sentence co-occurrence words vocabulary. information harnesses structure text lost simple bag-of-words formulation. alternatively feature vector include non-textual information features images word features learned user feedback useful personalization. semantic feature matrix modeled weighted combination latent concept semantic features weighting based word assignments concept across corpus thereby encoding semantic similarity concept membership. model propose paper fundamentally diﬀerent topic models generative models text represent concept sparse words rather topics distributions entire vocabulary. such concepts coherent focused traditional topics. example figure compares concept learned model corresponding topic hierarchical dirichlet process learned congressional record corfigure contains word clouds comparing concept learned using model topic involving word obamacare. bottom concept topic displayed users user study described sec. model able focus salient idea word obamacare pejorative term used republicans describe president obama’s health care reform package. topic provides much vaguer diﬀuse representation. additionally following williamson prior allows decouple prevalence concept strength. words model possible rare concept highly important documents contain characteristic diﬃcult obtain traditional topic modeling approaches. novel nested beta process prior deﬁne ability elegantly incorporate semantic side mcmc inference procedure learning empirical results–including user study–on wish model situation unknown collection concepts world document sparse subset them. natural model uncertainty unboundedness probabilistic manner look bayesian nonparametric methods. instance dirichlet processes long used priors mixture models number mixture components unknown however measure obtain predictive distribution simply concept assignments ˆcd. taking concentration parameter yields indian buﬀet process griﬃths ghahramani culinary metaphor describes sparsity structure shared across diﬀerent draws bernoulli process. document represented customer indian buﬀet inﬁnitely many dishes dish represents concept. ﬁrst customer samples oisson number dishes. customer selects previously tasted dish probability mj/d number customers previously sample dish chooses oisson number dishes. metaphor easy sparsity pattern concepts shared across documents document likely pick concept many previous documents selected above described beta process prior wellsuited modeling presence absence concepts document document collection. however still left task modeling presence absence words particular concept. case concept level document likely activate word concept many documents also word active concept leads natural extension culinary metaphor include condiments added alongside dish. speciﬁcally customer selects dishes indian buﬀet selects assortment chutneys accompany dish. analogous happens concept level customer ﬁrst sample dish selects oisson types chutney. customer sample dish selects chutney probability mji/d number previous customers sampled chutney alongside dish selects oission types chutney. case rather assigning observation single cluster done require featural model document made several concepts. such base model nested version beta process described section. consider situation countably inﬁnite number concepts world represented coin particular bias attributes deﬁne concept process assigning concepts particular document could thus coins coin lands heads assign concept document. process ﬂipping coins assign concepts document known formally bernoulli process since cording beta process concentration parameter base measure measurable space construction biases interval thus mass base measure ﬁnite ﬁnite expected measure obtain desired concept sparsity. formally beta process deﬁned realization nonhomogenous poisson process rate measure deﬁned product base measure improper beta distribution. special case base measure contains discrete atoms asbernoulli process realization prior determines subset concepts active document thibaux jordan conjugacy analytically marginalize beta process important note that unlike dirichlet process base measure need equal beta process completely random measure realizations disjoint sets independent random variables. process results concept assignments word assignments concept obtained sampling bernoulli lower level beta prof cess indicates whether word active concept document related idea nesting beta processes discussed important note that analogy hierarchical clustering weakly identiﬁable various combinations dish-chutney probabilities lead likelihood data. thus prior speciﬁcation diﬀerentiates entities hierarchical clustering models often propose identiﬁability constraints fact components correspond class closer components corresponding classes. model sec. avoid explicit constraints instead incorporate global information reduces sensitivity model hyperparameter settings. maintains exchangeability model. given document collection documents vocabulary words wish model concepts therein superwords. particular entails identifying concepts present document along words active concept document binary variables nested beta process features described sec. representing chosen dishes chosen chutneys. particular assume prior come vocabulary. together deﬁne makeup concepts presence document. however superwords actual document text must specify generative process observed words. first model relative importance concept document i.i.d. gamma-distributed random variables sociate word document concept assignment drawn multinomial distribution proportional refers element-wise hadamard product). such take values likewise given assignment document generates word multinomial distribution proportional here parameter model indicating relative importance words. featural based model decouples concept presence document prevalence. additionally concepts select overlapping sets words need marginal probability shared word. visual depiction graphical model well summary full generative process provided figure model presented thus suﬀers weak identiﬁability issues described sec. particular representational ﬂexibility model lead many concepts containing words concepts many words little diﬀerence likelihood data cases. address problem incorporating semantic incorporating semantic knowledge. assume word vocabulary associated observed real-valued feature vector -dimensional semantic space. fundamental assumption words appearing together concept similar semantic representations. beyond addressing identiﬁability concerns explicitly modeling semantic features vocabulary allows elegantly incorporate variety side information help guide makeup concepts. instance simplicity bag-of-words document representation provides many beneﬁts terms computational eﬃciency much structure thrown away could useful particular retrieval tasks. such want concepts consist words related synonymous meanings might consider sentence co-occurrence counts basis semantic features. long history work linguistics studying distributional similarity popularized firth stated shall know word company keeps rather ignore structural information incorporate part semantic feature experiments consider sec. model idea assuming concepts associated latent semantic features -dimensional space. words often co-occur concept expected semantic representations well-explained features concept formally consider observed random matrix dimensions column corresponds semantic feature vector word likewise random matrix rows countably inﬁnite number columns concept. words simultaneously active multiple concepts assume expected value feature representation word equal weighted average concepts weights proportional number documents matrix notation write weight matrix deterministically computed important note order incorporate semantic information generative model rely fact concepts explicitly represented sparse sets words. inclusion exclusion word concept directly informs concepts responassuming independence features gaussian noise specify generative distribution described figure particular place conjugate normal inverse gamma priors latent concept features variance terms respectively allowing analytically marginalize inference. example learning multilingual concepts images. modeling restrictions semantic data expect features real-valued zero mean. hence take advantage ﬂexibility model semantics variety diﬀerent forms limited simply textual features. demonstrate ﬂexibility consider following problem given small collection dessert recipes english german downloaded food blogs learn concepts coherent across languages? based image-based semantic features address problem without relying parallel corpora explicit dictionary multilingual topic modeling speciﬁcally assuming image associated word vocabulary semantic feature model encourages concepts choose include word latent feature vectors similar image-based feature vector associated word hypothesize that despite lack co-occurrence languages within small corpus still obtain reasonable multilingual concepts apple looks like apfel eggs look like eier speciﬁcally english german vocabulary words ﬁrst collect three search results google images. transform images following approach oliva torralba simple -dimensional gist-based features word semantic features. sampling procedure sec. samples infer marginal probabilities words active concept. order quantitatively verify hypothesis take marginal probability matrix word rank words probability cooccurrence create ground truth ﬁnding pairs words vocabulary considered english-german translations according google translate. come sible semantic meaning word. incorporate information traditional topic modeling approaches would topics contributing semantic meaning every word. interleaves metropolis-hastings gibbs sampling updates. given primarily interested concept deﬁnitions existence prevalence document marginalize random variables collapsed sampler. marginalization analytically possible conjugacy exists throughout model. collapsing particularly critical performance sampler given number binary indicator variables model. instance sample without marginalized forced keep long word document another important consideration that model represents inﬁnite number concepts ﬁnite number ever instantiated given time. thus context-speciﬁc independence graphical model variables sampled as-needed basis. figure rank accuracy multilingual concepts inferred english german recipes; model combining image corpus data outperforms predictions based either text images alone. giving pairs sets like {egg eggs} {eier}. pair german translation high ranked wg’s marginal probability ranking compute rank accuracy percentage words ranked higher than. directions set-pairs. comparison also compute rank accuracy when rather using model rank words directly based l-distance -dimensional gist feature space. figure shows model combining corpus-based concept modeling simple imagebased semantic features improves performance using image features alone. course expect model without incorporating images achieve poor performance little crosslanguage information text. order evaluate well sparsity-inducing nested beta process prior gives concept coherency ﬂexibility desire compare concepts learned model topics learned hdp. good model comparison widely used probabilistic model document collections unknown number topics model sparsity either topic word level. empirical evaluations section corpus drawn congressional record health care debate marked ﬁrst years barack obama’s presidency. document represents member congress compilation statements chamber ﬂoor days debate. analysis selected documents corresponding active speakers measured length ﬂoor transcripts. standard stop word removal extracted named entities noun phrases removed tokens appear least documents leaving vocabulary size augmented corpus semantic features computed taking sentencelevel word co-occurrence matrix projecting principal components parallel chains sampler iterations choose sample maximium joint likelihood across chains comparisons. also parallel chains inference procedure sec. samples select probable sample. illustrated figure previously discussed introduction concepts model ﬁnds coherent focused topics learned hdp. word sparsity concepts evident histogram figure showing empirical probability mass concepts/word fewer. figure shows three additional concepts illustrative beneﬁts incorporating sparsity-inducing prior semantic features. example concept found figure concisely represents so-called donut hole histogram illustrating word-level sparsity model provides. word clouds representing concept instantiated democratic republican members congress respectively. figure word clouds representing three concepts generated model applied congressional data using sentence co-occurrance-based semantic features. order measure coherency quantitative manner conducted user study presented participants pairs word clouds corresponding ideas health care debate asked choose cloud pair provides coherent description word. fairness asked public policy expert select words vocabulary representing salient ideas debate used words generate word clouds. given word found topic assigned highest probability well concept model included times across corpus. word clouds generated removing word question displaying remaining words proportional weight. hide identity approach participants truncate topic number words corresponding concept model. gives topics illusion sparsity naturally have hides advantages topic models. example obamacare word clouds shown figure transformed ones bottom purpose study. despite handicapping model manner users found model produces coherent concept representations hdp. speciﬁcally participants prefer concepts topics mean preference moreover note task relied subtle understanding american politics. example overall participants preferred word cloud obamacare ours considering participants claimed followed health care debate closely preference ﬂipped ours. details study found supplemental material. word clouds concept representing subset documents. data natural separation documents partitions representing democrats republicans. figure shows word clouds model representing concept comes democrats republicans. qualitative diﬀerence word weights populations discrimination women active concept democrats employer market associated republicans. several similar anecdotes level individual documents. example look leadership parties concept word obamacare active republican eric cantor whereas health care bill used democrat chris hollen. motivated critical problems information retrieval paper introduce novel modeling technique representing concepts document collections. popular generative models text tended focused representing ideas document collection probability distributions entire vocabulary often leading diﬀuse uninformative topic descriptions documents. contrast discrete sparse representation provides focused concise description example enable systems accurately describe user’s preferences. second contribution work model naturally incorporates semantic information captured alternative approaches; example demonstrated images used create multilingual concepts without translation information. side information often easily obtained help guide topic models towards semantic meaningful predictions. despite promise model suﬀers common limitations faced many bayesian nonparametric methods. incorporating semantic features helpful modeling perspective matrix operations required incorporate data expensive. experiments utilized parallelization approximation techniques reduce running time believe notion superwords characterized sparse concepts incorporation side information semantic features signiﬁcantly improve eﬀectiveness techniques capturing nuances users’ preferences.", "year": 2012}