{"title": "Recurrent Ladder Networks", "tag": ["cs.NE", "cs.AI", "cs.LG", "stat.ML"], "abstract": "We propose a recurrent extension of the Ladder networks whose structure is motivated by the inference required in hierarchical latent variable models. We demonstrate that the recurrent Ladder is able to handle a wide variety of complex learning tasks that benefit from iterative inference and temporal modeling. The architecture shows close-to-optimal results on temporal modeling of video data, competitive results on music modeling, and improved perceptual grouping based on higher order abstractions, such as stochastic textures and motion cues. We present results for fully supervised, semi-supervised, and unsupervised tasks. The results suggest that the proposed architecture and principles are powerful tools for learning a hierarchy of abstractions, learning iterative inference and handling temporal information.", "text": "propose recurrent extension ladder networks whose structure motivated inference required hierarchical latent variable models. demonstrate recurrent ladder able handle wide variety complex learning tasks beneﬁt iterative inference temporal modeling. architecture shows close-to-optimal results temporal modeling video data competitive results music modeling improved perceptual grouping based higher order abstractions stochastic textures motion cues. present results fully supervised semi-supervised unsupervised tasks. results suggest proposed architecture principles powerful tools learning hierarchy abstractions learning iterative inference handling temporal information. many cognitive tasks require learning useful representations multiple abstraction levels. hierarchical latent variable models appealing approach learning hierarchy abstractions. classical learning models postulating explicit parametric model distributions random variables. inference procedure evaluates posterior distribution unknown variables derived model approach adopted probabilistic graphical models success deep learning however explained fact popular deep models focus learning inference procedure directly. example deep classiﬁer like alexnet trained produce posterior probability label given data sample. representations network computes different layers related inference implicit latent variable model designer model need know them. however actually tremendously valuable understand kind inference required different types probabilistic models order design efﬁcient network architecture. ladder networks motivated inference required hierarchical latent variable model. design ladder networks emulate message passing algorithm includes bottom-up pass top-down pass information results bottom-up top-down computations combined carefully selected manner. original ladder network implements iteration inference algorithm complex models likely require iterative inference. paper propose recurrent extension ladder network iterative inference show architecture used temporal modeling. also show proposed architecture inference engine complex models handle multiple independent objects sensory input. thus proposed architecture suitable type inference required rich models learn hierarchy abstractions handle temporal information model multiple objects input. figure structure recurrent ladder networks. encoder shown decoder shown blue decoder-to-encoder connections shown green. dashed line separates iterations type hierarchical latent variable models rladder designed emulate message passing. graph static model. fragment graph temporal model. white circles unobserved latent variables gray circles represent observed variables. arrows represent directions message passing inference. paper present recurrent extension ladder networks conducive iterative inference temporal modeling. recurrent ladder recurrent neural network whose units resemble structure original ladder networks every iteration information ﬁrst ﬂows bottom stack encoder cells. then information ﬂows back bottom stack decoder cells. encoder decoder cells also information propagated horizontally. thus every iteration encoder cell l-th layer receives three inputs output encoder cell level output decoder cell level previous iteration encoder state level previous iteration. updates state value passes output vertically horizontally passed vertically horizontally. exact computations performed cells tuned depending task hand. practice used lstm cells encoder cells inspired original ladder networks decoder similarly ladder networks rladder usually trained multiple tasks different abstraction levels. tasks highest abstraction level typically formulated highest layer. conversely output decoder cell bottom level used formulate low-level task corresponds abstractions close input. low-level task denoising possibilities include object detection segmentation temporal setting prediction. weighted costs different levels optimized training. rladder architecture designed mimic computational structure inference procedure probabilistic hierarchical latent variable models. explicit probabilistic graphical model inference done algorithm propagates information nodes graphical model compute posterior distribution latent variables static graphical models implicitly assumed rladder messages need propagated input level hierarchy highest level bottom shown fig. appendix present derived iterative inference procedure simple static hierarchical model give example message-passing algorithm. also show inference procedure implemented rladder computational graph. case temporal modeling type graphical model assumed rladder shown fig. task next step prediction observations online inference procedure update knowledge latent variables using observed data compute predictive distributions input xt+. assuming distributions latent variables previous time instances kept ﬁxed inference done propagating messages observed variables latent variables bottom-up top-down past future shown fig. architecture rladder designed emulate message-passing procedure information propagate required directions bottom-up top-down past future. appendix present example message-passing algorithm derived temporal hierarchical model show related rladders’s computation graph. even though motivation rladder architecture emulate message-passing procedure nodes rladder directly correspond nodes speciﬁc graphical model. rladder directly learns inference procedure corresponding model never formulated explicitly. note also using stateful encoder cells strictly motivated message-passing argument practice skip connections facilitate training deep network. mentioned previously rladder usually trained multiple tasks formulated different representation levels. purpose tasks encourage rladder learn right inference procedure hence formulating right kind tasks crucial success training. example task denoising encourages network learn important aspects data distribution temporal modeling task next step prediction plays similar role. rladder useful problems require accurate inference multiple abstraction levels supported experiments presented paper. rladder architecture similar recently proposed models temporal modeling recurrent connections placed lateral links encoder decoder. make easier extend existing feed-forward network architecture case temporal data recurrent units participate bottom-up computations. hand recurrent units receive information makes impossible higher layers inﬂuence dynamics lower layers. architectures quite similar could potentially derive beneﬁt decoder-to-encoder connections successive time instances aforementioned connections well justiﬁed message-passing point view updating posterior distribution latent variable combine latest information bottom decoder contains latest information top. show empirical evidence importance connections section section demonstrate rladder learn accurate inference algorithm tasks require temporal modeling. consider datasets passing information time abstraction hierarchy important achieving good performance. consists mnist digits downscaled pixels ﬂying white dataset. background white vertical horizontal occlusion bars which digit ﬂies behind them occludes pixels digit also restrict velocities randomly chosen eight discrete velocities pixels/frame apart bouncing movement deterministic. digits split training validation test sets according original mnist split. primary task classify digit partially observable given moment time steps. order optimal classiﬁcation would need assimilate information digit identity keeping track observed pixels feeding resultant reconstruction classiﬁer. order encourage optimal inference next step prediction task rladder bottom decoder rladder trained predict next occluded frame network never sees un-occluded digit. thus mimics realistic scenario ground truth known. assess importance features rladder also ablation study. addition compare three networks. ﬁrst comparison network optimal reconstruction digit frames static feed-forward network encoder rladder derived. gold standard obtaining similar results implies close optimal temporal inference. second temporal baseline deep feed-forward network recurrent neural network that design network propagate temporal information high level level. third hierarchical stack convolutional lstm units convolutional layers between rladder amputated decoder. fig. appendix schematics details architectures. fully supervised learning results. results presented table ﬁrst thing notice rladder reaches classiﬁcation accuracy obtained network given optimal reconstruction digit. furthermore rladder decoder decoder-to-encoder connections trained without auxiliary prediction task classiﬁcation error rise almost level temporal baseline. means even network rnns lowest levels task encourages develop good world model information cannot travel decoder encoder high level task cannot truly beneﬁt lower level temporal modeling. next notices table top-level classiﬁcation cost helps low-level prediction cost rladder mutually supportive relationship high-level low-level inferences nicely illustrated example fig. time step inclusively network believes digit optimal reconstruction static classiﬁer temporal baseline hierarchical rladder prediction task rladder decoder-to-encoder conn. rladder classiﬁcation task rladder such network predicts right part occluded stick behind occlusions digit moves right next time step using decoder-to-encoder connections decoder relay expectation encoder encoder compare expectation actual input right part absent without decoder-to-encoder connections comparison would impossible. using upward path encoder network relay discrepancy higher classiﬁcation layers. higher layers large receptive ﬁeld conclude since must three thanks decoder higher classiﬁcation layers relay information lower prediction layers change prediction seen appropriately without decoder would bring high level information back level drastic update prediction would impossible. information lower prediction layer predict top-left part three appear next time step behind occlusion indeed happens semi-supervised learning results. following experiment test rladder semisupervised scenario training data contains labeled sequences unlabeled ones. make unlabeled data added extra auxiliary task level consistency cost targets provided mean teacher model thus rladder trained three tasks next step prediction bottom classiﬁcation consistency outputs top. shown table rladder improves dramatically learning better model help unlabeled data independently addition semi-supervised learning methods. temporal baseline model also improves classiﬁcation accuracy using consistency cost clearly outperformed rladder. section evaluate rladder midi dataset converted piano rolls dataset consists piano rolls various piano pieces. train -layer rladder containing convolutional lstms fully-connected lstm. details found appendix table shows figure example prediction rladder occluded moving mnist dataset. first ground truth digit network never sees train second actual frames seen network trains. third predicted next frames trained rladder. fourth stopped-gradient readout bottom layer decoder trained ground truth probe aspects digit represented neurons predict next frame. notice network know direction digit move predicts superposition possible movements. notice network thought digit supposed materialize side occlusion expected network immediately concluded correctly actually three. negative log-likelhoods next-step prediction obtained music dataset results reported mean plus minus standard deviation seeds. rladder competitive best results gives best results amongst models outputting marginal distribution notes time step. fact rladder beat midi datasets shows limitations rladder. models table output joint probability distribution notes unlike rladder outputs marginal probability note. models output probability note take input notes previous time instances also ground truth notes left time instance. rladder that takes input past notes played. even though example digit turning three seeing absent shows internally rladder models joint distribution. section show rladder used inference engine complex model beneﬁts iterative inference temporal modeling. consider task perceptual grouping identifying parts sensory input belong higher-level perceptual models outputting joint distribution notes nade masked nade rnn-rbm rnn-nade lstm-nade tp-lstm-nade balstm models outputting marginal probabilities note lstm rladder components enhance previously developed model perceptual grouping called tagger replacing originally used ladder engine rladder. another perspective problem also extends tagger recurrent neural network expectation maximization point view. tagger model designed perceptual grouping. applied images modeling assumption pixel belongs objects described binary variables pixel belongs object otherwise. reconstruction whole image using object µµµk vector many elements pixels. thus assumed probabilistic model written follows vector elements latent variables deﬁne shape texture objects. fig. graphical representation model fig. possible values model variables textured mnist dataset used experiments section model deﬁned noisy image tagger trained auxiliary low-level task denoising. inference procedure model evaluate posterior distributions latent variables µµµk groups given corrupted data making approximation variables groups independent posteriori inference procedure could implemented iteratively updating approximate distributions model approximation deﬁned explicitly. tagger explicitly deﬁne probabilistic model learns inference procedure directly. iterative inference procedure implemented computational graph copies ladder network inference groups every iteration inference procedure produces posterior probabilities pixel belongs object point estimates reconstructions µµµk outputs used form low-level cost inputs next iteration paper replace original ladder engine tagger rladder. refer model rtagger. figure graphical model perceptual grouping. white circles unobserved latent variables gray circles represent observed variables. examples possible values model variables textured mnist dataset. computational graph implements iterative inference perceptual grouping task graph iterations drawn. plate notation represent copies graph. figure example image brodatz-textured mnist dataset. image reconstruction group learned background. image reeconstruction group learned digit. original image colored using found grouping πππk. textures brodatz dataset example generated image shown fig. create greater diversity textures randomly rotated scaled brodatz textures producing training data. network trained textured mnist dataset architecture presented fig. three iterations. number groups details rladder architecture presented appendix network trained tasks low-level segmentation task formulated around denoising tagger model top-level cost log-likelihood digit class last iteration. table presents obtained performance textured mnist dataset fully supervised semi-supervised settings. experiments seeds. report results mean plus minus standard deviation. runs tagger experiments converge reasonable solution include runs evaluations. following segmentation accuracy computed using adjusted mutual information score mutual information ground truth segmentation estimated segmentation πππk scaled give segmentations identical zero output segmentation random. comparison trained tagger model dataset. comparison method feed-forward convolutional network architecture resembling bottom-up pass rladder trained classiﬁcation task only. thing notice results obtained rtagger clearly improve iterations supports idea iterative inference useful complex cognitive tasks. also observe rtagger outperforms tagger approaches signiﬁcantly outperform convolutional network baseline classiﬁcation task supported input-level task. also observed top-level classiﬁcation tasks makes rtagger faster train terms number updates also supports high-level low-level tasks mutually beneﬁt other detecting object table results brodatz-textured mnist. i-th column corresponds intermediate results rtagger i-th iteration. fully supervised case tagger trained successfully seeds given results seeds. semi-supervised case able train tagger successfully. labeled figure example segmentation generation rtagger trained moving mnist. first frames input sequence frames ground truth future. second next step prediction frames future frame generation rtagger colors represent grouping performed rtagger. boundaries using textures helps classify digit knowing class digit helps detect object boundaries. figs. show reconstructed textures segmentation results image fig. rtagger model perform perceptual grouping video sequences using motion cues. demonstrate this applied rtagger moving mnist sequences length low-level task prediction next frame. applied temporal data rtagger assumes existence objects whose dynamics independent other. using assumption rtagger separate moving digits different groups. assessed segmentation quality score computed similarly ignoring background case uniform zero-valued background overlap regions different objects color. achieved averageami score example segmentation shown fig. tried tagger dataset able train successfully single seed three. possibly speed intermediate level abstraction represented pixel level. reccurent connections rtagger keep representations time step next segment accordingly something difﬁcult tagger might explain training instability. paper presented recurrent ladder networks. proposed architecture motivated computations required hierarchical latent variable model. empirically validated recurrent ladder able learn accurate inference challenging tasks require modeling dependencies multiple abstraction levels iterative inference temporal modeling. proposed model outperformed strong baseline methods challenging classiﬁcation tasks. also produced competitive results temporal music dataset. envision purposed recurrent ladder powerful building block solving difﬁcult cognitive tasks.", "year": 2017}