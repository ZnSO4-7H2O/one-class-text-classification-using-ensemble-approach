{"title": "Glass-Box Program Synthesis: A Machine Learning Approach", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Recently proposed models which learn to write computer programs from data use either input/output examples or rich execution traces. Instead, we argue that a novel alternative is to use a glass-box loss function, given as a program itself that can be directly inspected. Glass-box optimization covers a wide range of problems, from computing the greatest common divisor of two integers, to learning-to-learn problems.  In this paper, we present an intelligent search system which learns, given the partial program and the glass-box problem, the probabilities over the space of programs. We empirically demonstrate that our informed search procedure leads to significant improvements compared to brute-force program search, both in terms of accuracy and time. For our experiments we use rich context free grammars inspired by number theory, text processing, and algebra. Our results show that (i) performing 4 rounds of our framework typically solves about 70% of the target problems, (ii) our framework can improve itself even in domain agnostic scenarios, and (iii) it can solve problems that would be otherwise too slow to solve with brute-force search.", "text": "figure representations problem ﬁnding frequent character string. example inputoutput pairs. bottom glass-box representation summing scores test strings. score number occurrences tests randomly generates strings. glass-box program synthesis. better understand glass-box representation consider program synthesis contest. programming contests among humans problems often described english scored automatically automated scoring programs based output certain inputs certainly difﬁcult computers understand english descriptions instead propose describing problem synthesis system source code scoring program. glass-box model need separate problem description examples scoring program. illustrated figure problem frequent character string score program outputting character string would number occurrences total score would scores randomly generated strings. note specify problem input-output examples needs solve problem several examples ambiguities multiple different functions mapping casting problem programming optimizing glass-box program-scoring program must write program precisely deﬁnes score/utility synthesized program. however arguably necessary step posing recently proposed models learn write computer programs data either input/output examples rich execution traces. instead argue novel alternative glass-box loss function given program directly inspected. glass-box optimization covers wide range problems computing greatest common divisor integers learning-to-learn problems. paper present intelligent search system learns given partial program glass-box problem probabilities space programs. empirically demonstrate informed search procedure leads significant improvements compared brute-force program search terms accuracy time. experiments rich context free grammars inspired number theory text processing algebra. results show performing rounds framework typically solves target problems framework improve even domain agnostic scenarios solve problems would otherwise slow solve brute-force search. computers program computers must ﬁrst address programming problems represented performance evaluated. ﬁeld program synthesis main approaches specifying problems examples number example input-output pairs provided input goal output function satisfying possibly minimizing criteria speciﬁcation formal speciﬁcation particular language given. generally utility synthesized program. assuming utility function written program-scoring program propose approach giving synthesis direct access scoring-program’s source code program synthesis optimizing glass-box program scoring objective used evaluate paper illustrate potential glass-box program synthesis approach designing ∗most work done internship msr. ironically term white commonly used indicate transparency even though white boxes necessarily transparent. hence glass box. problem general programming contests. term glass-box contrasts black-box access. black-box access would mean ability score arbitrary programs without access scoring program. glass-box access course least powerful black-box access scoring program itself. cases glass-box program synthesis applied traditional optimization problems linear programming traveling salesman problem number theory problems greatest common divisor factoring. problems efﬁciently scored easy verify factors primality. programming example case program scored accuracy mapping ﬁxed inputs outputs combined regularization term e.g. prevent over-ﬁtting. optimizing algorithm’s performance simulation. example would designing network protocol evaluated network simulator. case scoring program could measure performance large network. meta-optimization learning learn learn. problem synthesizing program synthesis posed form meta-scorer generates problems various domains runs candidate synthesizer problems averages resulting program scores. learning synthesize solutions synthesizing problems. deﬁning representation evaluation programming problem demonstrate feasibility approach system learns synthesize programs. athletes various exercises improve performance sport program synthesis system improve practicing learning synthesizing solutions various problems. menon introduced machine learning approach synthesis learns synthesize across problems. repository real-world problems relatively small selected small number features suited text-processing pbe. around shortage data generate problems practice synthesizing solutions. particular practice problems iteratively improved solutions problems interleaving search training logistic regression-based model guides search intelligently following menon recall glass-box synthesis problem scoring function immediately know score synthesized program. similar approach creating artiﬁcial problems introduced independently balog differences work perform synthesis perform glass-box synthesis utilize deep learning make multi-class logistic regression. experiments. show practice possible synthesize solutions number problems interest would prohibitively slow naive approach. also show glass-box program synthesis system improve even domain agnostic framework union grammars various domains considered; although better results achieved domain-speciﬁc grammars. contributions. paper formalize learning synthesize successful solutions programming problems machine learning problem using glass-box optimization. main contributions three-fold introduce glass-box program-scoring programs formalize machine learning framework glassbox optimization synthesizing practice problems learning patterns among problems discovered solutions. present experiments demonstrate ability framework learn generate well-formed python programs across domains. rest paper organized follows. discussing related work introduce concepts needed understand approach present details proposed learning write programs framework glassps. then present experimental results measure empirical performance glassps range problem domains. programming example program synthesis system attempts infer program input/output example pairs searching composition base functions. success various domains notable example \"flash fill\" string manipulation microsoft excel end-user give pairs natural language description task. recent advances deep learning augmentation deep networks end-to-end trainable abstractions neural turing machine hierarchical attentive memory neural stack given rise neural programming paradigm works trained using examples except model trained rich supervision execution traces. works combine learning-to-program approaches machine learning program synthesis perform guided search space synthesized programs. successful views probabilistic programming perspective i.e. representing program generative probabilistic model programming speciﬁcation approach i.e. specifying partial program ‘sketch’ capturing high-level structure implementation letting computer synthesize low-level details. relationship works. work using perspective machine learned program synthesis introduces novel glass-box introspection along contextual features inform search. closest works differences work examples condition search propose glass-box problem representation deep networks logistic regression. also work problem-speciﬁc learned weights partial program representation guide search separate model learned task. another factor differentiating various works expressiveness domain speciﬁc language used. many learning-to-program works demonstrate programwriting single domains like string processing approach generate code general-purpose programming language covering various domains number theory strings root ﬁnding; open interesting possibilities general problem solving said this goal paper compete existing systems neural programmer interpreter ones. instead wish provide glass-box representation alternative tool guide program generation. concepts formalize problem learning synthesize programs solutions glass-box optimization problems start discussion high-level concepts program. program computes function inputs outputs. distinguish program function computes different programs compute function. glassps program input python objects program output objects plus special symbol indicates program crashed produce output allotted time. glass-box problems. glass-box synthesis deﬁned problems. problem represented glass-box scoring program problem computes function measures score solution problem i.e. synthesizer. synthesizer generates solution program based program scored. hence goal synthesizer attempt maximize score maxs∈s importantly takes scoring-program source code glass-box input. though used simulate blackbox access generating examples synthesizer potentially achieve higher scores black-box access alone. solutions. denote programs output synthesizer refer solutions. hence note problems solutions programs. grammars. program expressed composition building blocks; blocks constitute rules context-free grammar allows recursive compositional structure. denote grammar rules solution programs problem programs solution glassps uses generates program trees converted strings evaluated python interpreter. includes rules instance generates code numbers concatenate strings combine objects supported python operator. rule creates function variable rule converts string lower-case. grammar supports iteration recursion. avoid halting issues bounding total number routine calls allowed. simplicity types uses non-terminal; leaving learning system learn generate programs raise exceptions. note formulation support directly reaching scoring program e.g. extract constants though functionality could added system. problem glassps uses also generates code evaluated python interpreter. contains multiple non-terminals roughly grouped python type facilitate generating well-formed scoring programs. program tree. glass-box/solution programs derived corresponding represented rooted trees node associated rule cfg. problem/solution trees constructed top-down probabilistically sampling rule probabilities respectively. choice trees representing programs convenient easy extract features trees machine learning purposes. learning natural learn synthesize programs based collection problem-solution pairs. would ideally access large repository samples problem solution programs. menon provide small problem/solution pairs text processing pbe. since relatively small domain knowledge construct small number hand-coded features learning. instead similarly balog synthesize practice problems synthesize solutions problems. glass-box synthesis approach amounts synthesizing scorers i.e. glass-box problems. scorer synthesize number solution programs speciﬁc scorer choose highest scoring program. learn collection problemsolution pairs improve model used synthesis. iteratively improved solutions problems interleaving search training model helps guide search intelligently. overview framework shown figure speciﬁcs described next. node whose rule assigned. simplicity represent context one-hot-encoding parent node’s rule one-hot encoding current node’s child index thus one-hot-encoding vectors glass-box problem parent-node-rule child-index concatenated comprise input learner. learning rule probabilities every node candidate solution program tree want predict using derived features described above rule probable. thus problem program inference i.e. searching space programs expressed reduced multi-label classiﬁcation problem solution program collection rules simultaneously present. reduced multi-class classiﬁcation predicting class-rule separately whether present not. number classes equals |s|. represent target label correct rule successful solution program glass problem one-hot-encoding vector r|s| next entries. node rule present given general formulation multi-class classiﬁer applicable. work purposes proof-ofconcept illustration multi-class logistic regression model parameters particular learn parameter vectors class. illustration mapping features solution rule shown figure important note parameters learned based constantly updated dataset consisting thus found successful program pairs. given featurization procedure described found successful solution program e.g. nodes contributes samples learning classiﬁer; samples input features differ context features change. scoring solution programs. order include successful problem-solution pair constantly updated training dataset notion success needs speciﬁed. recall solution program successful maximizes score corresponding problem. although framework entirely capable handling arbitrary continuous scoring functions nature problems induced grammars used experiments scoring programs return scores except solution throws exception case score makes easier evaluate system average score indicates fraction although bag-of-word representation seem simplistic loses program structure information preliminary experiments using richer representations sequence models seem offer much value trade-off computational expenses. synthesizing practice problems. since glass-box problems programs themselves represented program trees synthesized randomly expanding nodes problems expressed. duplicate problems removed synthesized problems divided training test sets random train/test split. training problems denoted test problems challenge problems. apart practice problems synthesize test problems held pool synthesized problems validation also challenge problems. challenge problems problems created manually written terms intended representative programming challenges naturally expressed glass-box synthesis problems. problems given experiments section. synthesizing solutions. practice/test/challenge problem ﬁxed number candidate solution program trees synthesized top-down probabilistically using best according scoring function problem hand chosen. every rule associated probability. program collection rules probability product probabilities rules comprise learning probabilities rules equal purpose using machine learning learn problem-speciﬁc rule probabilities successfully make successful solution programs probable thus easier ﬁnd. follows specify formalize learning write programs optimize glass-box functions machine learning problem. input features. input machine learning classiﬁer types features glass-box problem features context features. problem features used learning bag-of-rules representation problem. problem rule feature number occurrences problem’s program tree. context refer partial candidate program created far. speciﬁcally since synthesis top-down mean path rules root generated tree current learning write typed programs. would nice glassps generated subset python programs fact initially generates many nonsensical programs uses single terminal thus unsurprisingly ﬁrst grammar mostly generates programs raise exceptions things like trying lower-case number result learning system utilizing found pairs glass problems-solutions progressively learn compose well-formed programs addition learning solve problems. algorithmic procedure summarize framework formal description procedure followed given algorithm system operates iterations round glassps calls solve module order attempt solve test problems synthesized based solve system generates practice problems using uniform probabilities. solve generated practice problems system calls train module. goal train construct training dataset problems solutions learner’s parameters learned. achieve system uses parameters previous round’s logistic regression model guide program trees solution programs built speciﬁcally every node construction solution program tree features extracted module featurize creates bag-of-word features problem context. given features current learner’s model used predict probability every rule solution inferred probabilities used rule weights perform weighted sampling rule next candidate program tree. learning guides search programs. procedure every problem candidate solution programs constructed. candidate program scored corresponding scoring program programs successfully solve respective glassbox problems i.e. maximum score using shortest length break ties used construct solution rule training data calling featurize routine representation constructed training dataset used learn logistic regression model used solutions test challenge problems subsequently used next learning round guide search candidate solution programs. illustrative example guided search works move experimental results better understand learning framework works demonstrate could successfully synthesize solution program computing greatest common divisor positive integers using framework glassps. imum score achieved factor python program written succinctly lambda ntprog ntprog computes score domain pairs. also implements early stopping optimality criteria improves efﬁciency without changing behavior algorithm. successful solution program else i.e. recursive form euclid’s algorithm. illustration purposes consider cfgs shown table actually subsets grammars used experiments. contain rules typically useful number theory problems. calling featurize module algorithm glassbox problem written one-hot-encoding vector prob r|p| non-zero values xgcd values number occurrences respective rule. recall decide rule become next program tree node learned logistic regression model based found pairs problems-solutions table grammars illustration purposes. callrec refers call function recursively outer function called recursively ntprog function early-stopping based evaluations synthesized program various argument values. contains many terminals contains single terminal used inference mode predict given problem context features target class likely. thus construct successful ﬁnding-gcd program tree top-down guided search proceed sampling rules next node using predicted probabilities root node features xgcd prob concatenated all-zero vector root parent. target sampled class i.e. rule maximum sampled predicted probability prob xparent xchild] prob previous node xparent r|s| xgcd one-hot encoding vector xchild one-hot vector ﬁrst entry ﬁrst child. sampled target class predicted probability vector short rules high probability xgcd prob certain parent child index contexts. ﬁnding corresponding glass-box pgcd pair added training dataset used learn next round’s setup. evaluation metric fraction test problems successfully solved. sets practice/train test problems change throughout experiment. programs limited size nodes. generating solutions instead generating programs independently random generate programs likely using search algorithm menon produce duplicates. since problem generation bottleneck problems generated simply sampling uniformly grammar removing duplicates. domains. consider cfgs following domains number theory. example target problems include ﬁnding largest non-trivial factor number using brute force approach. numerous trivial functions also arise practice given numbers output smaller two. finding roots. example target problems ﬁnding root algebra expressions i.e. solve function summation formulas. example target problems ﬁnding closed-form expression computing strings. simplicity considered problems desired output single character input meeting certain objective. example problem ﬁnding frequent character string ﬁnding alphabetically ﬁrst character string. veriﬁcation. problems performance evaluated bounded domain possible inputs range possible outputs makes easy check optimality enables early stopping efﬁciency. grammars. brieﬂy grammars typed program-scoring grammar solution grammar include operators math functions tanh arctanh constants passed arguments operators boolean operators expressions string functions startswith endwith count upper lower ord. additionally grammar contains recursive functions operators lists sets tuples. results practice/ test problems empirically evaluate proposed framework conducted experiments experimental conditions. ﬁrst condition considered context-free grammars relevant problems trying solve. example solving string problems considered string second condition took union grammar rules four different domains considered problems four domains. refer all-domain experiment. ﬁrst condition considered practice train problems test problems. progressively performed four training rounds rounds seem improve learning. round training minutes. show figure fraction practice train/test problems solved glassps rounds learning progress performing rounds framework figure domain-speciﬁc experiment fraction problems solved multiple rounds learning domain all-domain experiment similar results achieved minutes learning round additional rounds beyond seem improve test accuracy. challenge problem number solution time without learning solution time all-domain learning solution time domain-speciﬁc learning challenge problem number solution time without learning solution time all-domain learning solution time domain-speciﬁc learning second setup all-domain experiment show results figure order achieve similar fraction train/test problems solved ﬁrst setup every training round double time i.e. mins instead conclude although better results achieved domain-speciﬁc learning all-domain experiment proof concept experiment even without knowing domain glassps improve time. table report time hours minutes needed solve challenge problems system. compare times needed search guided learning i.e. brute-force all-domain setup domain-speciﬁc learning best performing across problems. notably framework solve problems would otherwise prohibitively slow without learning. notably found hours learning took minutes using domain-speciﬁc learning hours minutes all-domain experiment. paper introduced general framework learning write computer programs maximize score glass-box objective functions. formulated machine learning problem showing proof-of-concept simple classiﬁer logistic regression successfully learn patterns among features scoring programs features generated solution programs scored. shown experimentally framework learns time generate python typed code solves problems across domains even though provide types give access domain type problem comes from. learning could certainly improved approach shown sound extent ﬂexible enough combine multiple domains synthesis single system. opens interesting directions multi-domain learning systems learn solve problems.", "year": 2017}