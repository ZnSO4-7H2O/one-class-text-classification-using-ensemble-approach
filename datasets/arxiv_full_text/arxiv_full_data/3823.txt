{"title": "TheanoLM - An Extensible Toolkit for Neural Network Language Modeling", "tag": ["cs.CL", "cs.NE"], "abstract": "We present a new tool for training neural network language models (NNLMs), scoring sentences, and generating text. The tool has been written using Python library Theano, which allows researcher to easily extend it and tune any aspect of the training process. Regardless of the flexibility, Theano is able to generate extremely fast native code that can utilize a GPU or multiple CPU cores in order to parallelize the heavy numerical computations. The tool has been evaluated in difficult Finnish and English conversational speech recognition tasks, and significant improvement was obtained over our best back-off n-gram models. The results that we obtained in the Finnish task were compared to those from existing RNNLM and RWTHLM toolkits, and found to be as good or better, while training times were an order of magnitude shorter.", "text": "figure recurrent nnlm lstm tanh hidden layers. -of-n encoded words projected lower-dimensional vectors. lstm layer passes hidden state next time step like standard recurrent layer also cell state designed convey information extended time intervals. several toolkits made available language modeling neural networks support research prototyping work well. important limitation difﬁculty extending toolkits recently proposed methods different network architectures. also training large networks slow without support. theanolm nnlm tool developed motivated ongoing research improving conversational finnish automatic speech recognition goal make versatile fast easy write code easily extensible. next section give introduction theanolm works. evaluate conversational tasks large data sets. order verify works correctly results compared toolkits. present tool training neural network language models scoring sentences generating text. tool written using python library theano allows researcher easily extend tune aspect training process. regardless ﬂexibility theano able generate extremely fast native code utilize multiple cores order parallelize heavy numerical computations. tool evaluated difﬁcult finnish english conversational speech recognition tasks signiﬁcant improvement obtained best back-off n-gram models. results obtained finnish task compared existing rnnlm rwthlm toolkits found good better training times order magnitude shorter. index terms automatic speech recognition conversational language neural network language models known outperform traditional n-gram language models speech recognition accuracy modeling word sequences temporal dependencies recurrent neural network attractive model limited ﬁxed window size. perhaps simplest variation used language modeling contains hidden layer ability model temporal dependencies limited vanishing gradient problem. various modiﬁcations proposed standard structure reduce problem popular long short-term memory figure shows typical lstm network. following architecture bengio ﬁrst layer projects words continuous lower-dimensional vector space followed hidden layer. recurrent networks hidden layer state passed next time step. lstm special case recurrent layer also passes cell state sigmoid gates control information added removed cell state making easy maintain important information cell state extended periods time. ﬁnal neural network layer normalizes output probabilities using tmax. another inﬂuential design choice concerns performance vocabulary large. computational cost training evaluating network softmax output layer highly dependent number output neurons i.e. size vocabulary. feedforward networks basically n-gram models straightforward neural network predict words short-list back-off model infrequent words replacing single softmax layer hierarchy softmax layers improves performance although number model parameters reduced. using word classes input type=class name=class_input layer type=projection name=projection_layer input=class_input size= layer type=dropout name=dropout_layer_ input=projection_layer dropout_rate=. layer type=lstm name=hidden_layer_ input=dropout_layer_ size= layer type=dropout name=dropout_layer_ input=hidden_layer_ dropout_rate=. layer type=tanh name=hidden_layer_ input=dropout_layer_ size= layer type=dropout name=dropout_layer_ input=hidden_layer_ dropout_rate=. layer type=softmax name=output_layer input=dropout_layer_ theano python library high-performance mathematical computation provides versatile interface building neural network applications used many neural network tasks. language modeling toolkits rnnlm popular easier approach. developed theanolm complete package training applying recurrent neural network language models speech recognition machine translation well generating text sampling nnlm. freely available github. neural network represented python objects symbolic graph mathematical expressions. theano performs symbolic differentiation making easy implement gradientbased optimization methods. already implemented nesterov’s accelerated gradient adagrad adadelta adam rmsprop optimizers addition plain stochastic gradient descent evaluation expressions performed using native code transparently. compilation introduce short delay program start train model typical application delay negligible compared actual execution. hand execution highly parallelized using speeding training large networks fraction training times. standard training sensitive learning rate hyperparameter. initial value high possible given optimization still converges. gradually decreasing learning rate enables ﬁner adjustments later optimization process. theanolm perform crossvalidation development data regular intervals order decide quickly annealing occur. however adaptive learning rate methods adagrad adadelta require manual tuning learning rate—cross-validation needed determining stop training. especially finnish highly agglutinative languages vocabulary large ﬁnal layer predict words directly. work class-based models word belongs exactly word class classes chosen carefully model necessarily perform worse word-based model finnish data enough examples rarer words give robust estimates word probabilities. advantage arbitrary network architecture provided text list layer descriptions. layers speciﬁed order network constructed meaning network acyclic. network contain word class inputs followed projection layer. projection layer followed number lstm layers well non-recurrent layers. ﬁnal layer softmax hierarchical softmax layer. multiple layers element input layer multiple inputs case inputs concatenated. example figure would create lstm network projection layer neurons hidden layers neurons. layers dropout layer contains neurons sets activations randomly zero train time prevent overﬁtting. conﬁguration used paper train larger theanolm models. order develop models conversational finnish recorded transcribed conversations held students pairs basic course digital signal processing aalto university. collected corpus called aalto university course conversation corpus available research language bank finland. corpus updated annually; currently includes recordings years addition used spontaneous sentences speecon corpus findialogue subcorpus finintas transcribed radio conversations totaling hours acoustic training data. practically conversational finnish data available research. collected language modeling data internet ﬁltered match transcribed conversations similar data available language bank finland. augmented words transcribed spoken conversations dspcon totaling million words. -word development used language modeling. table language model training times word error rates given model alone interpolated best kneser-ney model. finnish results evaluation used optimizing language model weights. sentences unseen speakers totaling minutes representing natural spontaneous speech various topics used evaluation. numerous ways conversational finnish written down output evaluated transcripts contain alternative word forms. another suitable evaluation data used optimizing language model weights. lattices generated using aalto triphone acoustic model. theanolm used adagrad optimizer without annealing. could evaluate different optimizers extensively adagrad seemed among best terms speed convergence performance ﬁnal model. nesterov’s accelerated gradient manual annealing gave slightly better model considerably longer training time. toolkits standard sgd. kneser-ney smoothed -grams used back-off models. data sets collected different sources varied size quality. instead pooling data together baseline back-off models weighted mixtures models estimated subset. weights optimized using development data. back-off model vocabulary limited different word forms occurred training data selected algorithm maximize likelihood development data out-of-vocabulary rate evaluation data task obtained results also freely available nnlm toolkits rwthlm rnnlm comparison. rwthlm theanolm used lstm layer tanh layer projection layer figure rnnlm supports simple recurrent network hidden layer. faster training time theanolm also able larger model times number neurons layer. model includes dropout layer. architecture description given figure trained also models third existing toolkit cslm unable meaningful sentence scores. cslm supports feedforward networks fast support. toolkits also differ handle vocabulary. rwthlm theanolm used word classes. word classes created using exchange algorithm tries optimize probability bigram model training data moving words classes. rnnlm creates classes frequency binning uses words input output neural network. classes used decomposition output layer speeds training evaluation millions words vocabulary number parameters rnnlm model neurons larger rwthlm theanolm models neurons. evaluation neural network probabilities slow usable ﬁrst decoding pass large-vocabulary continuous speech recognizer. another pass performed decoding rescoring word lattices created using traditional n-gram model. rwthlm able rescore word lattices directly toolkits rescore n-best lists created word lattices. word error rates rescoring shown table table also includes word error rates given interpolation nnlm scores word -gram weighted model. interpolated sentence scores vocabulary size affects perplexity computation omitted limited-vocabulary models perplexity results table finnish vocabulary times larger perplexities considerably higher english language experiment expected. values reported tool; might differences e.g. unknown words sentence starts ends handled. perplexities kneser-ney models computed using srilm excludes unknown words computation. theanolm used behaviour also recorded training times table although noted jobs compute cluster assigns different hardware reported durations indicative. rwthlm supports parallelization various math libraries. rnnlm able core means computation inevitably slow. theanolm used nvidia tesla gpu. rescoring output neural network models outperformed back-off models even without interpolation back-off scores. since back-off model scores ﬁrst decoding pass without additional cost reasonable look performance models combined interpolation. improves results. back-off model clearly improved weighting individual corpora because different corpora homogeneous. time writing implemented training weighting schemes theanolm well improvements smaller well-established back-off model weighting. english task used advanced acoustic models trained using kaldi discriminative training using maximum mutual information criterion. lattices generated using maximum likelihood linear regression speaker adaptation. acoustic training data hours icsi meeting corpus. language model training used also part fisher corpus matching data internet. total text data contained million words million words transcribed conversations. development evaluation data nist rich transcription spring meeting recognition evaluation. development contained words evaluation minutes consisted utterances. back-off models vocabulary limited different words occurred training data. evaluation tokens left outside vocabulary. theanolm used train models architecture using parameters finnish task. task roughly twice amount data. training times slower still manageable. results table show beneﬁt larger network pronounced task. smaller network clearly incapable modeling larger data well. also different languages data sets work best different algorithms architectures good solution needs versatile. offer toolkit implemented using python theano provides implementations latest training methods artiﬁcial neural networks easily extensible. another important advantage speed—theano provides highly optimized implementations expensive matrix operations automatically utilize gpu. back-off n-gram models still orders magnitude faster train training model using theanolm order magnitude faster training similar model toolkits. speed advantage theanolm mainly support adagrad optimizer used theanolm also faster converge. epochs required convergence using adagrad toolkcontinued improve perplexity least epochs. faster training time makes practical train larger networks theanolm. data available beneﬁt larger network becomes pronounced training without support becomes impractical. conversational finnish speech recognition task used practically data available research. -gram kneser-ney model trained simply concatenating training data gave wer. baseline took mixture model combined smaller models optimized interpolation weights. baseline model gave wer. reached interpolating theanolm baseline scores relative improvement rwthlm gives similar results similar neural network architecture increases conﬁdence correctness implementation. evaluating progress made conversational finnish task better previous record however collected acoustic data explains baseline paper. acoustic models used experiments still state art. order absolute performance models also need obtain proper development optimizing language model scale interpolation weight. appears types nnlms work better language another. rnnlm perform well expected finnish task probably network architecture optimal. rnnlm offers simple network architecture takes words input contains hidden layer. number input connections huge finnish vocabulary size hidden layer limited want training time reasonable. previously interpolating rnnlm kneser-ney model shown give relative improvement conversational english english meeting recognition task data available smaller neural network better weighted mixture back-off model. still neural network brings information much interpolation smaller nnlm scores back-off scores improves baseline larger network brings improvement wer. data available larger network needed makes training speed even important. research focus conversational finnish highly agglutinative languages. considering much effort gone english task results satisfactory show theanolm works also relatively standard conversational english task. schwenk gauvain connectionist language modeling large vocabulary continuous speech recognition proceedings ieee international conference acoustics speech signal processing vol. i––i–. language models based dissertation brno unineural available versity technology http//www.ﬁt.vutbr.cz/research/view_pub.php?id= cernocký network based international speech communication association sep. available http//www.isca-speech.org/archive/interspeech_/i_.html merrienboer gülçehre bougares schwenk bengio learning phrase representations using encoder-decoder statistical machine translation conference empiricial methods natural language processing available http//arxiv.org/abs/. bengio ducharme vincent janvin neural probabilistic language model journal machine learning research vol. available http//dl.acm.org/citation.cfm?id=. schwenk cslm modular open-source continuous proceedings space language modeling toolkit international annual conference speech communication association aug. available http//www.isca-speech.org/archive/interspeech_/i_.html srivastava hinton krizhevsky sutskever simple salakhutdinov overﬁtting prevent neural networks journal machine learning research vol. jan. available http//dl.acm.org/citation.cfm?id=. hirsimäki pylkkönen kurimo importance high-order n-gram models morph-based speech recognition ieee transactions audio speech language processing vol. morin bengio hierarchical probabilistic neural network language model proceedings international workshop artiﬁcial intelligence statistics society artiﬁcial intelligence statistics available http//www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats.pdf forming word classes language modstatistical clustering statistical elling quantitative linguistics köhler springer netherlands available http//dx.doi.org/./----_ sundermeyer schlüter rwthlm rwth aachen university neural network language modeling toolkit proceedings annual conference international speech communication association sep. mikolov kombrink deoras burget ˇcernocký rnnlm recurrent neural network language modeling toolkit proceedings ieee workshop automatic speech recognition understanding ieee signal processing society dec. ieee catalog cfpsrw-usb. available http//www.ﬁt.vutbr.cz/research/view_pub.php?id= bastien lamblin pascanu bergstra goodfellow bergeron bouchard bengio theano features speed improvements nips workshop deep learning unsupervised feature learning mikolov kombrink burget ˇcernocký khudanpur extensions recurrent neural network language model proceedings ieee international conference acoustics speech signal processing venkataraman wang techniques effective vocabulary selection proceedings european conference speech communication technology sep. available http//www.isca-speech.org/archive/eurospeech_/e_.html povey ghoshal boulianne burget glembek goel hannemann motlicek qian schwarz silovsky stemmer vesely kaldi speech recognition toolkit proceedings ieee workshop automatic speech recognition understanding ieee signal processing society dec. ieee catalog cfpsrw-usb. kombrink mikolov karaﬁát burbased language proceedings modeling meeting recognition international annual conference speech communication association aug. available http//www.isca-speech.org/archive/interspeech_/i_.html", "year": 2016}