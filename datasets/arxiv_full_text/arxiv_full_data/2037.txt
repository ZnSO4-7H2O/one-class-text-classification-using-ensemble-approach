{"title": "Scoring and Searching over Bayesian Networks with Causal and Associative  Priors", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "A significant theoretical advantage of search-and-score methods for learning Bayesian Networks is that they can accept informative prior beliefs for each possible network, thus complementing the data. In this paper, a method is presented for assigning priors based on beliefs on the presence or absence of certain paths in the true network. Such beliefs correspond to knowledge about the possible causal and associative relations between pairs of variables. This type of knowledge naturally arises from prior experimental and observational data, among others. In addition, a novel search-operator is proposed to take advantage of such prior knowledge. Experiments show that, using path beliefs improves the learning of the skeleton, as well as the edge directions in the network.", "text": "signiﬁcant theoretical advantage searchand-score methods learning bayesian networks accept informative prior beliefs possible network thus complementing data. paper method presented assigning priors based beliefs presence absence certain paths true network. beliefs correspond knowledge possible causal associative relations pairs variables. type knowledge naturally arises prior experimental observational data among others. addition novel search-operator proposed take advantage prior knowledge. experiments show that using path beliefs improves learning skeleton well edge directions network. search-andone score approach learning bayesian networks versus constraintbased approach former naturally accepts priors network. since number possible networks exponential practical setting number nodes assign priors implicit way. paper consider prior beliefs possible paths variable pairs. paths directly correspond causal associative relations. joint beliefs paths employed assign prior network. causal knowledge naturally derives prior experimental data associative knowledge stems observational data. example consider dataset measuring average amount exercise week calcium diet occurrence osteoporosis smoking cohort women. bayesian network could induced appropriate learning method. however prior experimental study showed increasing amount exercise reduces occurrence disease knowledge belief probability incorporated learning. similarly prior cohort study shown smoking correlates reduced exercising knowledge probability also included. belief strengths depend several factors statistical power study. notice fact correspond presence edge network edge implies direct causal relation depend context. path beliefs inherently dependent. example believes certainty believe consistent. therefore consider joint distribution input path beliefs instead marginal distributions separately. however unlikely complete joint distribution available. instead could marginal distributions infer joint distribution. however several technical diﬃculties consider. example assume given wish compute hand several choices joint given marginal beliefs. scenario infer hand beliefs maybe incoherent extendable joint distribution satisﬁes probability axioms. priors; incoherent joint chosen coherent induces path probabilities close input beliefs. joint computed employed eﬃciently compute prior network. furthermore take advantage prior knowledge introduce novel search-operator. simulated proof-of-concept experiments show scoring method indeed take advantage prior knowledge. provided causal knowledge able better learn orientations edges causal relations. informative priors also facilitate learning skeleton network. finally show proposed search-operator signiﬁcantly improves quality learned model. several methods make prior knowledge learning network review). example using knowledge regarding parameters network causal total order variables presence absence directed edges network possibly beliefs assigned prior network used assign prior probabilities network based distance network general argued type knowledge existing methods incorporate learning form easily acquired. result uniform thus uninformative priors commonly used learning bayesian networks data. problem incorporating informative priors learning listed list open problems recent causality editorial speciﬁbeliefs. cally considers methods borboudakis tsamardinos assume ﬁrst learns markov-equivalence class bayesian networks maximal ancestral graphs data then path constraints imposed graph. contrast work network learned help prior knowledge. method presented incorporating beliefs paths relies computationally expensive markov chain monte carlo simulations. however neither latter method dealing prior knowledge deals issues dependent possibly incoherent beliefs. random variables {vi}k bayesian network pair directed acyclic graph representing conditional independencies variables joint probability distribution graph distribution must connected equation equation equivalent called markov condition. network ﬁxed context drop indexes equations. skeleton undirected graph constructed ignoring orientations triple vertices called collider collider unshielded adjacent called markov equivalent skeleton unshielded colliders. partially directed acyclic graph graph representing markov equivalent bns. skeleton representatives edge directed invariant representatives. directed path denoted denote case share common ancestor neither ancestor reverse. d-connecting path exists either absence d-connecting path denoted rest paper assume faithfulness condition implies d-connecting path nodes statistically associated complete multinomial dataset variables probability network score network often obtained taking logarithm equals bayesian scoring methods bdeu approximate log-likelihood based diﬀerent assumptions. priors uniform term ignored maximization. setting however term become important. second equation stems fact given graph data independent factor normalizing constant need computed maximize equation diﬀerent graphs. section mention several approximations computing factor focus prior ﬁrst equation holds function factor prior graph given speciﬁc conﬁguration holds. given preference knowledge assign prior graphs conﬁguration. number dags nodes sharing conﬁguration /ncg markov-equivalent graphs satisfy path-beliefs obtain score. last term equation graphs sharing conﬁguration. ﬁrst term markov-equivalent graphs provided employs appropriate scoring function score uninformative prior beliefs graphs equiprobable priori number graphs nodes uninformative beliefs expect encounter given conﬁguration probability equal proportion graphs satisfying conﬁguration i.e. case uniform priors would expect. pair prior belief possible paths connecting nodes network. important devise cases paths mutually exclusive allow representation common types causal associative knowledge. possible follows deﬁne path variables taking values domain semantics respectively. speciﬁc nodes refer important single index variable probability distribution πrij possible value. input method path beliefs path variables probability distributions associated them. example shown table expressing belief likely directed path possible paths nodes dictate possible causal associative relations. interpreted causally equivalent addition discussed previous section equivalent thus distribution πrxy corresponds beliefs causal associative relations. useful allow user specpractice prior beliefs directly events cause associated distribution πrxy derived opposite. diﬃcult example given mass probability distributed reasonable three values. however avoid belief representation simplify presentation method. section derive score given data path beliefs important requirement computation score knowledge joint distribution marginals correspond distributions assume already computed; following sections describe details computation. j.p.d. stemming table shown table denote given joint instantiation values path variables deﬁne important notice graph conﬁguration uniquely determined. example j.p.d. table graph hold table path beliefs three pairs nodes. beliefs incoherent imply induced coherent beliefs stemming solving problem part j.p.d. computed solving input number dags nodes conﬁgurations also shown. notice zero counts zero probability invalid. follows properties point fact factor /ncg seem provide counter-intuitive results ﬁrst glance. reason that everything else equal higher priors tend assigned graphs small conﬁgurations consistent graphs. desirable drop factor. however score used place property satisﬁed more. order good estimate using fulll sample huge number dags. improve upon developed another method approximate method based observation that often certain subsets path variables almost independent. exploit factorize uninformative prior distribution conﬁguration denoted conﬁguration computed number dags nodes solved closed-form however best knowledge closed-form number dags satisfy certain path-constraints. number nodes small enumerate dags compute counting. number possible dags however grows exponentially number nodes complete enumeration option. case estimate counts sampling number dags uniformly random. speciﬁcally implemented recent method that unlike prior work avoids expensive mcmc methods. estimated ·sc/s number sampled dags conform conﬁguration number conﬁgurations large nc/n small never sample graph consistent resulting zero estimates. happen even small sets path variables grows exponentially number path variables avoid zero estimates apply laplace correction s+cl arbitrary parameter. suggest close zero. later refer method fulll. give intuitive understanding main idea consider following scenario given path variables notice nodes common. assume value depending value values become less likely. example holds values become less likely since restricts graph contain fewer edges eﬀectively reducing possibility form paths hand becomes likely. formally ux⇒y ux⇒y ux⇐y ux⇐y ux⇔y ux⇔y ux<y ux<y however claim number nodes suﬃciently large diﬀerence negligible formally urxy |rwz urxy alindependent. illustrate simple example. assume case clear value heavily constrains graph since contains nodes. however keep adding nodes possibilities created satisfy value consider conﬁgurations note prior probability directed path nodes equal pair nodes. assuming factorized ⇒z|x⇒y ⇐z|x⇒y hold. holds ⇒z|x⇒y ⇐z|x⇒y follows. however given becomes less likely since dags case example dags conﬁguration dags conﬁguration thus cannot factorized case. scenarios give rough intuitive understanding basic idea. next subsection provide experimental results support claims. generalize basic ideas path beliefs. since parts independent partition nodes common conﬁguration part directly inﬂuence part; however indirect inﬂuence nodes graph which negligible. hand path variables part directly aﬀect independent partition path variables computed follows construct constraint graph connected components easy connected components independent partition tion prior distribution i-th part respectively. estimated sci/s number sampled dags conform conﬁguration finally again recommend laplace correction. then s·uc s+l·c refer method factl. setup number nodes varied step-size used three sets path variables number independent partitions paritition consists path beliefs respectively. number valid conﬁgurations respectively. number sampled dags suﬃciently large fulll approximate well. laplace correction parameter since correction necessary case. used kl-divergence measure distance probability distributions ufulll representing true distribution. results results shown figure claimed ﬁxed path beliefs ufactl approaches ufulll number nodes increases. similar results expected larger independent partitions. second experiment show relatively small compared factl provides better approximation fulll. important sampling large number dags costs time memory essentially setting upper limit which relatively large result poor approximation fulll. show this know exact distribution however computationally infeasible large numbers nodes experiment small setup number nodes number dags respectively. small used path variables sampled between dags step-size done simulate case access complete dags given. laplace correction constant measured kl-divergence ufulll well ufactl experiment repeated times averages reported. results results shown figures small factl provides better approximation fulll. reason works that partitioned multiple sets containing relatively small number path variables distributions easier approximate. figure kl-divergence full fact diﬀerent sets path variables. distance fact true distribution approximated fact decreases number nodes increases. kl-divergence true distribution approximation methods number samples increases. small fact provides better approximation true distribution full. system inﬁnite solutions. argue choose solution j.p.d. close uninformative possible. distribution introduce bias towards certain conﬁgurations even prior knowledge suggest preference conﬁgurations. words uninformative j.p.d. coherent extension path beliefs reason prefer solution natural information-theoretic approach select j.p.d. minimizes kldivergence problem formulated case incoherent beliefs j.p.d. equal marginal input beliefs. instead requesting coherent beliefs ignoring incoherency seek joints marginals close possible user’s input beliefs. solve problem implemented method proposed called gema. gema extension ipfp converges even incoherent beliefs. order solve problem allows marginals change small amount measured so-called i-aggregate criteria. although gema tends minimize criteria guarantee convergence global local minima provided. conducted anecdotal experiments gema seems produce reasonable results. section show compute joint probability distribution denote probability takes value unknown quantities conﬁguration s.t. conﬁgurations variable obtains value obtain following constraints words marginals j.p.d. equal input path beliefs. recall path beliefs independent general. thus important consider following constraints stemming path semantics variables conﬁguration invalid cannot satisﬁed example contains directed cycles. algorithm detect invalid conﬁgurations discussed section complete problem speciﬁcation impose that constraints eqs. satisﬁed j.p.d. adhering probability axioms found prior marginal beliefs hold. case deﬁnition coherent otherwise incoherent. table contains j.p.d. stemming table computed gema. comparison input beliefs table contains marginal beliefs implied gema. values table table close latter representing coherent beliefs. figure shows dags diﬀerent conﬁgurations obtaining diﬀerent prior scores. cost solving dominated number variables high practice optimization problem solved eﬃciently path beliefs. obvious that even best case would need least time memory output procedure full j.p.d. natural improve upon factorize unfortunately general seems possible without loss information. however stated uninformative joint distribution factorizes respect sets variables result ipfp also factorizes respect sets variables ∃{ri}k thus factl instead fulll compute usually compute signiﬁcantly faster larger sets path beliefs instead total limit path beliefs part independent partition used factl limit path beliefs. practice case priors misleading correct value path variable lower probability value always possible detect cases; however possible path beliefs dependent majority gives preference correct relation. illustrate simple example. assume correct relation variables expert suggests assume path beliefs incoherent probability axioms follows. method implicitly consider increase reducing eﬀect even higher path beliefs suggest high. example pairs path beliefs method assign that although initially given conditions suﬃcient identify invalid conﬁgurations necessary. simplest example dataset variables conﬁguration invalid variable serve common ancestor. cases identify such. however number variables data large relative number path variables conditions also necessary. assume number nodes suﬃciently large. paper greedy search method searching space dags. method starts given initial performs hill-climbing search considering dags resulting edge-insertion edge-removal edge-reversal operation. greedy search trivially extended additionally consider prior score this ﬁrst determine conﬁguration computed time given transitive closure transitive closure computed time node keep track visited nodes. faster complex algorithms trivial method usually faster smaller graphs figure assume path beliefs table corresponding table conﬁguration holds graph. obtain score conﬁguration holds graph. obtain score expected higher prior since given higher probability table signiﬁcant computational overhead. straight-forward optimization dynamically update closure edge insertion removal. various methods exist trading time takes update closure querying reachability. assuming unit query time update time optimal using method time-complexity reduced take advantage extra information provided path beliefs additional search-operators. standard operators make small local improvements without considering global information provided path beliefs. thus operator desirable able simultaneously make multiple adjustments order also change conﬁguration path variables. propose swap-equivalent-operator. idea simple step application standard operator allow algorithm swap markov equivalent highest path belief score increase. data score score-equivalence property resulting data score higher prior score. computed simple modiﬁcation algorithm presented space limitations algorithm described here. employing causal knowledge. consider graph path belief distribute remaining mass probability remaining values proportional values correspond uniform prior. repeat following experiment times randomly select number states variable either sample cpts variable gamma distribution shape parameter scale parameter sample dataset size increase samples dataset provided scoring method step size identify highest scoring network possible dags using informative uninformative priors bdeu score equivalent sample size results figure plots percentage time pdag true network found exactly without informative priors. first notice true pdag found edges also always oriented correctly since true network higher prior markov-equivalent graph. perhaps surprising though notice informative priors also improve learning skeleton. belief tends path associations always higher equal association thus correct path tends induced rather network path employing associative knowledge. similar proof-of-concept experiment true network single collider settings three cases correct associative priors uniform priors incorrect associative priors results results shown figure expected correct prior beliefs clearly improve chances identifying true pdag; eﬀect exactly opposite misleading incorrect beliefs provided algorithm. course asymptotically nonzero priors play role. learning larger networks. generate path beliefs three parameters number independent components number nodes appearing independent component whether want coherent incoherent. path variables generated follows given randomly pick non-overlapping sets containing nodes network consider possible pairs path variables resulting total figure learning orientations skeleton facilitated causal prior knowledge. learning graph facilitated correct associative prior knowledge hindered incorrect priors. learning alarm insurance networks. average structural hamming distance shown increasing sample size component size number components incoherent beliefs. using path beliefs especially combined swap-equivalent operator produces better networks average. nc·cs·/ path variables. done order able consider large sets path variables. then randomly assign probability true value path variable split remaining mass probability uninformative remaining values. process repeated independent component coherent incoherent depending input parameter. estimate sampled dags machine epsilon. used alarm insurance networks evaluate methods. employed greedy search bdeu metric ess=. method starting empty graph uninformative informative priors well without swap-equivalent-operator case informative priors. finally compute structural hamming distance pdag true network. used pdag avoid introducing unfair advantage methods; methods markov equivalent dags ones using path beliefs correctly oriented edges. sample size varied within path belief parameters varied within respectively coherent incoherent cases. experiment repeated times randomly sampled datasets path beliefs combinations input parameters. results space limitations report results incoherent path beliefs results similar both coherent incoherent priors. also smaller diﬀerence uninformative informative methods smaller work notice diﬀerence uninformative method informative method without operator decreases sample size increases. reason that sample size increases data score becomes important prior score tends ignored; usually considered close local maxima small improvements data score made. however swap-equivalent operator used happen tries maintain high prior score whole search. finally notice counter-intuitive behavior increasing increasing sample size figure samples. anecdotal experiments suggest value parameter reason behavior. however swap-equivalent operator used phenomenon almost nonexistent. present method computing informative priors given causal associative beliefs pairs variables well novel search-operator take advantage them. priors employed search-and-score learning algorithm. method ﬁrst time addresses issues incoherent possibly dependent priors. providing correct priors pairwise causal associative relations improves learning terms identifying orientation edges also terms identifying skeleton network. numerous issues still address regarding method general problem. algorithm exponential worst-case time complexity thus eﬃcient algorithms desirable. closedform solutions computing number graphs given path constraints also desirable. finally including types prior knowledge well incorporating strength causal eﬀects associ-", "year": 2012}