{"title": "Consensus Message Passing for Layered Graphical Models", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "Generative models provide a powerful framework for probabilistic reasoning. However, in many domains their use has been hampered by the practical difficulties of inference. This is particularly the case in computer vision, where models of the imaging process tend to be large, loopy and layered. For this reason bottom-up conditional models have traditionally dominated in such domains. We find that widely-used, general-purpose message passing inference algorithms such as Expectation Propagation (EP) and Variational Message Passing (VMP) fail on the simplest of vision models. With these models in mind, we introduce a modification to message passing that learns to exploit their layered structure by passing 'consensus' messages that guide inference towards good solutions. Experiments on a variety of problems show that the proposed technique leads to significantly more accurate inference results, not only when compared to standard EP and VMP, but also when compared to competitive bottom-up conditional models.", "text": "perhaps signiﬁcant challenge generative modelling framework inference hard. sampling-based methods risk slow mixing message passing-based methods converge slowly converge solutions fail converge all. whilst signiﬁcant efforts made improve accuracy message passing algorithms many challenges remain including difﬁculty implementation problem computational cost question structured approximation chosen. present work aims alleviate problems general-purpose message-passing algorithms. starting observation general purpose message passing inference algorithms fail even simplest computer vision models. claim models failure attributed algorithms’ inability determine values relatively small number inﬂuential variables call ‘global’ variables. without accurate estimation global variables difﬁcult message passing make meaningful progress variables model. latent variables vision models often organised layered structure observed image pixels bottom high-level scene parameters top. additionally knowledge values variables level sufﬁcient reason global variable layer properties mind develop method called consensus message passing learns exploit layered structures estimate global variables early stages inference. experimental results variety problems show leads signiﬁcantly accurate inference results whilst preserving computational efﬁciency standard message passing. implication work twofold. first adds useful tool toolbox techniques improving general-purpose inference second overcomes bottleneck restricted model-based machine learning computer vision. generative models provide powerful framework probabilistic reasoning. however many domains hampered practical difﬁculties inference. particularly case computer vision models imaging process tend large loopy layered. reason bottom-up conditional models traditionally dominated domains. widely-used general-purpose message passing inference algorithms expectation propagation variational message passing fail simplest vision models. models mind introduce modiﬁcation message passing learns exploit layered structure passing consensus messages guide inference towards good solutions. experiments variety problems show proposed technique leads signiﬁcantly accurate inference results compared standard also compared competitive bottom-up conditional models. generative models provide powerful framework probabilistic reasoning applicable across wide variety domains including computational biology natural language processing computer vision. example computer vision graphical models express process face rendered image incorporating knowledge surface normals lighting even approximate symmetry human faces. models make effective information generalize well require less labelled training data consensus message passing exploits layered characteristic vision models order overcome aforementioned inference challenges. illustration layers latent variables model shown fig. using factor graph notation latent variables function latent variables global variables experiments follow recurring pattern appears many models interest vision. example case face modeling variables correspond normals global variable light vector shading intensities reasoning follows recursive structure. assume moment fig. messages layer inter-layer factors informative accurate refer messages collectively contextual messages. would desirable purposes speed accuracy could ensure messages sent layer also possess properties. access oracle could give correct belief global variables image could send accurate initial messages step compute informative accurate messages inter-layer factors layer above. practice however access oracle. work train regressors predict values global variables given messages layer below. prediction good enough messages layer informative accurate inductive argument hold. describe regressors trained sec. summarize approach consists following components models useful employ second type displayed graphically fig. global layer variables absent loops graphical model global variables layers. here consensus message sent variable latent layer above given contextual messages. figure consensus message passing. vision models tend large layered loopy. adjacent layers latent variables model kind consensus messages computed contextual messages sent global variables guiding inference layer. consensus message passing different kind situations loops graphical model global variables layers. priority within layer sent bottom naturally consensus message sent contextual messages computed. desirable able ensure ﬁxed point reached scheme also ﬁxed point standard message passing model. approach reduce certainty consensus messages course inference pass ﬁrst iterations. experiments found even passing consensus messages ﬁrst iteration accurate inference therefore follow strategy remainder paper. worth emphasizing message-passing equations remain unchanged used scheduling scheme experiments important highlight crucial difference consensus message passing heuristic initialization. latter predictions made observations matter high hierarchy target variable whereas predictions made using messages sent variables immediately target variables interest. prediction task much simpler since relationship target variables variables layer immediately much less complex relationship target variables observations. furthermore know layered structure model relevant information observations contained variables layer below. target variables layer conditionally independent layers below given values layer ﬁnal note capacity regressors. course true inﬁnite capacity regressor make perfect predictions given enough data however interested practical ways obtaining accurate results models increasing complexity lack capable regressors unlimited data inevitable. important feature makes predictors scalable since regressions made adjacent latent layers. recap wish perform inference layered model observed variables latent variables predictor function collection contextual messages {ck} produces consensus message i.e. adopt approach learn function task parameterized i.e. seen instance canonical regression task. given family regressors goal training parameters capture relationship context consensus message pairs {}d=...d training examples. first discuss training data obtained. least three different sources beliefs convergence. technique useful standard message passing works slow. standard message passing inference model large number iterations collection different observations {xd}. message passing scheduled precisely would present however consensus messages sent. observation collection marginals latent variables e.g. fig. layer predictor }d=...d. challenging since inputs outputs regression problem messages special care needs taken account fact. follow closely methodology eslami random forests used predict outgoing messages factor. detailed description random forest implementation provided supplementary material. review forests criminisi shotton ﬁrst illustrate application diagnostic models circles second squares. approach improve inference challenging vision model intrinsic images faces. ﬁrst experiment predictors trained beliefs convergence second samples model third annotated labels showcasing various use-cases cmp. show cases proposed technique leads signiﬁcantly accurate inference results whilst preserving computational efﬁciency message passing. experiments performed infer.net using default settings unless stated otherwise. number trees forest fact although inference require many iterations message passing message initialization signiﬁcant effect speed convergence demonstrate done automatically using cmp. given noisy sample points {xi}i=...n circle plane noise axis) wish infer coordinates circle’s center radius express data generation process using graphical model cartesian point rotated radians generate translated generate latent ﬁnally produces noisy observation model expressed lines code infer.net. circle model interesting purposes since layered loopy vanilla message passing inference model take surprisingly large number iterations converge. draw points {xi} circles random centers radii record accuracy marginals latent variables iteration. repeat experiment times plot results fig. seen ﬁgure marginals contain signiﬁcant errors even iterations message passing. experiment consensus message passing. predictor trained send consensus message initial stages inference given messages coming predictor trained ﬁnal beliefs iterations standard message passing sample problems. figure accelerated inference using cmp. distance mean marginal posterior true value function number inference iterations consensus message passing signiﬁcantly accelerates convergence. similar plot radius seen fig. single consensus message effect signiﬁcantly increasing rate convergence also inference robustness comparison also plot well regressor capacity used directly estimate latent variables without using graphical model fig. consensus message passing gives best worlds example speed comparable one-shot bottom-up prediction accuracy message passing inference good model problem. next turn attention challenging problem even best message passing scheme could devise frequently ﬁnds completely inaccurate solutions. task infer center side length square image unlike previous problem knew points belonged circle must ﬁrst determine pixels belong square not. might also wish reason colour foreground background making task inference signiﬁcantly harder. graphical model problem shown fig. experiment test images perform inference using sequential schedule recording accuracy marginals latent variables iteration. additionally place damping step size messages square factor center found choices best performing standard message passing algorithm. despite this observed inference accuracy disappointingly poor fig. that many images message passing converges highly inaccurate marginals center. quality inference also seen quantitative results figs. figure square problem. wish infer square’s center side length. graphical model problem. boolean variable indicating square’s presence position depending value gate copies appropriate colour implement predictors different layers model ﬁrst layer send consensus messages respectively given messages coming take form independent gaussians centered appearances observed pixels therefore effectively make initial guesses values foreground background colours image given observed image. split features internal nodes regression forest designed test equality randomly chosen pixel positions sparse regressors used leaves prevent overﬁtting. second layer sends consensus message given messages coming messages take form independent bernoullis indicating algorithm’s current beliefs presence square pixel. therefore predictor’s predict square’s side length probabilistic segmentation map. note much easier implement regressor perform task using original observed image pixels predictors sufﬁcient stable inference implement fourth predictor experiment single stage lower predictors active figure robustiﬁed inference using cmp. position inferred centers relative groundtruth. image boundaries shown blue scale. distance mean posterior true values. consistently increases inference accuracy. results averaged different problems. stage makes lower predictors ∆bg. results experiments shown fig. observe signiﬁcantly improves accuracy inference center also latent variables note fact single stage appears insufﬁcient guiding message passing good solutions. whereas circle example accelerated convergence example demonstrates make inference possible models outside capabilities standard message passing. also investigate realistic application face modelling. estimation reﬂectance shape single image human face well-studied problem computer vision primary motivation task reﬂectance shape invariant confounding light effects therefore useful downstream tasks recognition. problem ill-posed however modern approaches make heavy prior knowledge order obtain good solutions e.g. form average reﬂectance normal statistics morphable models figure face problem. observe image wish infer corresponding reﬂectance normal graphical model problem. symmetry priors shown. model. given observation pixels {xi} wish infer reﬂectance value normal vector pixel fig. model shown variables represents following image formation process thereby assuming lambertian reﬂection inﬁnitely distant directional light source variable intensity. place gaussian priors reﬂectances {ri} normals {ni} light parameters priors using training data. additionally place soft symmetry prior {ri} {ni} reﬂecting prior knowledge faces. symmetry priors added model lines code illustrating model-based methods lend themselves rapid prototyping experimentation. although model crude approximation true image formation process similar approximations found useful prior work additionally successfully develop algorithms perform accurate reliable inference class models would able increase usefulness simply updating reﬂect true image formation process accurately. note even relatively small image size model contains latent variables factors show below standard message passing model routinely fails converge accurate solutions. consensus message passing. predictors levels model tackle problem. ﬁrst sends consensus messages reﬂectance pixel making instance type described fig. here consensus message predicted using information contextual messages denote predictors second predictor sends consensus message using information messages denoted ﬁrst level predictors effectively make guess reﬂectance image denoised observation second layer predictor produces estimate light shading image reﬂectance predictors powered single random forest however pixel position used feature exploit create location speciﬁc behaviour. tree parameterization contextual messages reﬂectance predictor also includes features mean median gradients patch around pixel. tree parameterization contextual messages lighting predictor consists means mean shading messages blocks. deliberately simple features maintain generality could imagine specialized regressors maximal performance. datasets. experiment ‘yale ‘extended yale datasets together contain images subjects illumination directions. remove images taken extreme light angles almost entirely shadow leaving around images images downsampled subject. groundtruth normals reﬂectances dataset however common practice create proxy groundtruths using photometric stereo using code qu´eau images subjects training test remaining subjects. results. begin qualitatively assessing different inference schemes. fig. show inference results reﬂectance maps normal maps lights obtained following iterations message passing reﬂectance would like inference produce estimates match closely groundtruth produced photometric stereo also display reﬂectance estimates produced strong baseline biswas reference. note baseline achieves excellent accuracy regions strong lighting however produces blurry estimates regions shadow. figure visual comparison inference results. randomly chosen test images show inference results obtained competing methods. observed images. inferred reﬂectance maps. stereo estimate proxy groundtruth bottom-up reﬂectance estimate biswas forest consensus prediction. variance inferred reﬂectance estimate produced high variance regions correlate strongly cast shadows. visualization inferred light. inferred normal maps. seen fig. standard variational message passing ﬁnds solutions highly inaccurate continued presence illumination artefacts areas cast show. contrast inference using produces artefact-free results much closely resemble stereo groundtruths. arguably also improves baseline since estimates blurry regions cast shadows. made possible presence symmetry priors model. additionally note variance inference reﬂectance correlates strongly cast shadows observed images suggesting future work would fruitful notion cast shadows explicitly built model. figs. show analogous results lighting normal maps fig. demonstrates cmp’s ability robustly infer reﬂectance maps images single subject taken varying lighting conditions. task subject recognition quantitative measure inference accuracy difﬁcult measure direct ways reﬂectance estimate produced algorithm compared training subjects’ groundtruth reﬂectances assigned label closest match. found evaluation reﬂect quality inference choose simplicity. fig. shows result experiment real images also synthetic images produced taking stereo groundtruths adding artiﬁcial lighting show analogous results light fig. error deﬁned cosine angle distance estimated light photometric stereo reference. first note standard variational message passing performs poorly producing reﬂectance estimates much less useful recognition biswas second note model produces inferences signiﬁcantly useful downstream. horizontal line labelled ‘forest’ represents accuracy consensus messages without message passing showing model-based ﬁne-tuning provides signiﬁcant beneﬁt. finally highlight fact initializing light directly image running message passing leads worse estimates demonstrating layered predictions opposed direct predictions observations. results demonstrate helps message passing better ﬁxed points even presence model mis-match make full potential generative model. inspiration stems kinds distinctions made decades so-called ‘intuitive’ bottom-up fast inference techniques iterative ‘rational’ inference techniques seen implementation ideas context message passing consensus messages form ‘intuitive’ part inference following standard message passing forms ‘rational’ part. analogues intuitive rational inference also exist sampling bottom-up techniques used compute proposals mcmc leading signiﬁcant speedup inference rezende kingma welling proposed techniques learning parameters figure robustness varying illumination. left right observed image photometric stereo estimate biswas estimate result consensus forest estimate mean variance. idea ‘learning infer’ also long history. early examples include hinton dedicated ‘recognition’ parameters learned drive inference. modern instances ideas message passing performed sequence predictions deﬁned graphical model predictors jointly trained ensure system produces correct labellings. however techniques resulting inference procedure longer corresponds original graphical model. important distinction predictors completely within framework message passing ﬁnal inference results correspond valid ﬁxed points original model interest. finally note recent works heess eslami make regressors learn pass messages. works concerned reducing computational cost computing individual messages make attempt change accuracy rate convergence message passing inference whole. contrast learns pass messages speciﬁcally reducing total number iterations required accurate inference given generative model. presented consensus message passing shown computationally efﬁcient technique used improve accuracy message passing inference variety vision models. crux approach recognize importance global variables take advantage layered model structures commonly seen vision make rough estimates values. figure reﬂectance inference accuracy demonstrated recognition accuracy. allows make full potential generative model thereby outperforming competitive bottom-up method biswas completely automated took care work generic features applied broad class problems. forests implemented extensible manner envisage building library choose from simply inspecting data types contextual target variables. future work would like exploit beneﬁts framework applying challenging problems computer vision. examples sec. extended various ways e.g. making considerations multiple objects incorporating occlusion squares example cast shadows faces example developing realistic priors. also seeking understand domains application ideas fruitful. broadly major challenge machine learning enriching models scalable way. continually seek models provide interpretations increasingly complicated heterogeneous data sources. graphical models provide appealing framework manage complexity difﬁculty inference long barrier achieving goals. framework takes step direction overcoming barrier. acknowledgements. thank anonymous reviewers minka christopher williams peter gehler sebastian nowozin andrew fitzgibbon feedback suggestions.", "year": 2014}