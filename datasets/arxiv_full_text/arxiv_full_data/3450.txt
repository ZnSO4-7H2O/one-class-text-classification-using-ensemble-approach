{"title": "Hashing as Tie-Aware Learning to Rank", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Hashing, or learning binary embeddings of data, is frequently used in nearest neighbor retrieval. In this paper, we develop learning to rank formulations for hashing, aimed at directly optimizing ranking-based evaluation metrics such as Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We first observe that the integer-valued Hamming distance often leads to tied rankings, and propose to use tie-aware versions of AP and NDCG to evaluate hashing for retrieval. Then, to optimize tie-aware ranking metrics, we derive their continuous relaxations, and perform gradient-based optimization with deep neural networks. Our results establish the new state-of-the-art for image retrieval by Hamming ranking in common benchmarks.", "text": "figure applying hashing nearest neighbor retrieval integer-valued hamming distance produces ties left uncontrolled different tie-breaking strategies could give drastically different values evaluation metric e.g. address issue using tie-aware ranking metrics implicitly average permutations closed form. tie-aware ranking metrics optimization objectives deep hashing networks leading state-of-the-art results. ture unfortunately learning hash literature still largely lacks tie-awareness current evaluation protocols rarely take tie-breaking account. thus ﬁrst advocate using tie-aware ranking metrics evaluation hashing implicitly average permutations tied items permit efﬁcient closed-form evaluation. natural next step learn hash functions optimizing tie-aware ranking metrics. seen instance learning rank listwise loss functions advantageous compared many ranking-inspired hashing formulations. solve associated discrete np-hard optimization problems relax problems continuous counterparts closed-form gradients available perform gradient-based optimization deep neural networks. speciﬁcally study optimization ndcg ranking metrics hashing learning binary embeddings data frequently used nearest neighbor retrieval. paper develop learning rank formulations hashing aimed directly optimizing ranking-based evaluation metrics average precision normalized discounted cumulative gain ﬁrst observe integer-valued hamming distance often leads tied rankings propose tie-aware versions ndcg evaluate hashing retrieval. then optimize tie-aware ranking metrics derive continuous relaxations perform gradient-based optimization deep neural networks. results establish state-of-the-art image retrieval hamming ranking common benchmarks. paper consider problem hashing concerned learning binary embeddings data order enable fast approximate nearest neighbor retrieval. take task-driven approach seek optimize learning objectives closely match test-time performance measures. nearest neighbor retrieval performance frequently measured using ranking-based evaluation metrics average precision normalized discounted cumulative gain optimization metrics deemed difﬁcult hashing literature propose novel learning rank formulation tackle difﬁcult optimization problems main contribution gradient-based method directly optimizes ranking metrics hashing. coupled deep neural networks method achieves state-of-the-art results. formulation inspired simple observation. performing retrieval binary vector encodings integer-valued hamming distance resulting ranking usually contains ties different tie-breaking strategies lead different results fact ties common problem ranking much attention paid including kendall’s classical work rank correlation modern information retrieval literaconsider supervised learning setting supervised hashing supervision speciﬁed using pairwise afﬁnities. formally assume access afﬁnity oracle whose value indicates notion similarity examples called similar dissimilar paper restrict take values ﬁnite covers important special cases. first binary afﬁnities extensively studied current literature. binary afﬁnities derived agreement class labels thresholding original euclidean distance second case multi-level afﬁnities consists non-negative integers. ﬁne-grained model similarity frequently considered information retrieval tasks including search engines. throughout paper assume setup query retrieved database retrieval performed ranking instances increasing distance using distance metric. termed retrieval hamming ranking hashing literature. ranking represented index vector whose elements form permutation |s|}. below i-th element unless otherwise noted implicitly assume dependency notation. ranking-based evaluation. ranking-based metrics usually measure form agreement ranking ground truth afﬁnities capturing intuition retrievals high afﬁnity query ranked high. first case binary afﬁnity deﬁne |{xi s|aq average precision averages precision cutoff cutoffs hashing widely used approach practical nearest neighbor retrieval thanks efﬁciency evaluating hamming distances using bitwise operations well memory storage footprint. theoretically demonstrated data-dependent hashing methods outperform data-independent ones locality sensitive hashing tackle supervised hashing problem also known afﬁnity-based hashing supervision given form pairwise afﬁnities. regarding optimization discrete nature hashing usually results np-hard problems. solution uses continuous relaxations line relaxation-based methods e.g. differs alternating methods preserve discrete constraints two-step methods supervised hashing cast special case distance metric learning formulated learning rank problem optimizing ranking metrics ndcg received much attention learning rank literature. instance surrogates ndcg optimized structural framework bound optimization algorithms exist ndcg alternatively gradient-based methods based smoothing approximating metrics methods consider applications hashing. learning hash literature different strategies proposed handle difﬁculties optimizing listwise ranking metrics. example decomposes listwise supervision local triplets structural svms optimize surrogate losses maximizes precision optimize ndcg surrogates. recent methods using deep neural networks learning objectives designed match ranking evaluation metrics e.g. contrast directly optimize listwise ranking metrics using deep neural networks. formulation observation integervalued hamming distance results rankings ties. however fact widely taken consideration previous work. ties sidestepped using weighted hamming distance cost reduced efﬁciency. fortunately tie-aware versions common ranking metrics found information retrieval literature inspired results propose optimize tie-aware ranking metrics hamming distances. gradient-based optimization uses recent differentiable histogram binning technique evaluating information retrieval systems special attention required exist ties distances case ranking unique tied items ordered arbitrarily tie-breaking strategy sizable impact result. given example fig. surprisingly found current ranking-based hashing evaluation protocols usually take tie-breaking account could result ambiguous comparisons even unfair exploitation. perhaps importantly ties render formulation direct optimization unclear tie-breaking strategy assume using ndcg optimization objectives? thus believe important seek tie-aware evaluation metrics hashing. rather picking ﬁxed tie-breaking strategy relying randomization tie-aware solution propose average value ranking metric possible permutations tied items. solution appealing several ways deterministic unambiguous cannot exploited reduces ordinary version ties. however caveat generating permutations tied items requires time super-exponential prohibitive. fortunately observes average computed implicitly commonly used ranking metrics gives tie-aware versions closed form. based result describe efﬁciently compute tie-aware ranking metrics exploiting structure hamming distance. focus ndcg denote tie-aware versions dcgt respectively. first deﬁne notation. integer-valued hamming distances redeﬁne ranking collection ties i.e. {i|dφ retrievals hamming distance query. deﬁne discrete histograms conditioned afﬁnity values {i|aq v}|∀v cuj≤d njv. time complexity analysis. given hamming distances {dφ|x ﬁrst step generate ranking populate ties {r}. step essentially counting sort integers time complexity. computing either dcgt time makes total time complexity formulation number bits constant therefore complexity linear contrast real-valued distances sorting generally takes time dominating factor. normalized ndcgt normalizing factor unaffected ties computing still requires sorting gain values descending order. assumption afﬁnity values consists non-negative integers number unique gain values counting sort applied time. total time complexity thus also linear provided since focus optimizing ranking metrics work connections learning rank many supervised hashing formulations loss functions deﬁned pairs triplets training examples correspond pointwise pairwise approaches learning rank terminology. collectively refer local ranking losses. since optimize evaluation metrics deﬁned ranked list approach falls listwise category well-known listwise ranking approaches generally superior pointwise pairwise approaches. note exists mismatch optimizing local ranking losses optimizing evaluation performance. listwise evaluation metrics position-sensitive errors made individual pairs/triplets impact results differently depending position list near top. address mismatch local ranking methods often need nontrivial weighting sampling heuristics focus errors made near top. fact sampling especially crucial triplet-based methods e.g. since possible triplets size training examples prohibitive enumerate. triplet-based methods also popular metric learning literature similarly observed careful sampling weighting stable learning. contrast directly optimize listwise ranking metrics without requiring sampling weighting heuristics minibatches sampled random weighting training instances used. section describe approach optimizing tie-aware ranking metrics. discrete hashing optimization np-hard since involves combinatorial search conﬁgurations binary bits. instead interested relaxation approach using gradient-based deep neural networks. therefore apply continuous relaxation discrete optimization problems. continuous relaxation needs address types discrete variables. first universal hashing formulations bits hash code binary. second tie-aware metrics involve integer-valued histogram counts {ndv}. result relaxation hash mapping distance function real-valued denoted respectively. remaining discreteness histogram counts {ndv}. also relax real-valued soft histograms {cdv} whose cumulative sums denoted {cdv}. however face another difﬁculty deﬁnitions dcgt involve ﬁnite lower upper limits variables relaxed. approximate ﬁnite sums continuous integrals removing second source discreteness. outline results proposition leave proof error analysis appendix. figure computation model. input images mapped b-bit binary codes deep neural network training minibatch example used query rank rest batch producing histogram hamming distances bins. tie-aware ranking metrics computed histograms averaged batch. maintain end-to-end differentiability derive continuous relaxations ndcgt employ differentiable approximations non-differentiable operations relax employ technique binary indicator replaced differentiable function easy-to-compute gradients. speciﬁcally linearly interpolates d-th slope shown fig. train models using minibatchbased stochastic gradient ascent. within minibatch example retrieved rest minibatch. example minibatch size used query once participates database example times. then objective averaged queries. conduct experiments image retrieval datasets commonly used hashing literature cifar- nus-wide labelme imagenet dataset split test database examples database used training. test time queries test used perform hamming ranking database performance metric averaged test set. cifar- canonical benchmark image classiﬁcation retrieval single-labeled images classes. following consider experimental settings. ﬁrst setting test constructed random images class rest used database images class used training second setting uses standard split entire database used training. nus-wide multi-label dataset flickr images. database subset images associated frequent labels images label sampled construct test size training contains images label imagenet subset imagenet containing images classes totaling images. setup images class sampled training images selected classes ilsvrc validation used queries. retrieval-based evaluation supervised hashing recently question points multi-class datasets binary encoding classiﬁer outputs already competitive solution. important point deriving pairwise afﬁnities multi-class label agreement special case formulation. mentioned sec. formulation uses general pairwise afﬁnity oracle derived labels either binary multi-level. fact datasets consider range multi-class/single-label multi-label unlabeled ﬁrst case addressed multi-class classiﬁcation. multi-level afﬁnities also propose evaluation protocol using ndcg. term method talr compare range classical stateof-the-art hashing methods. vast hashing literature exhaustive comparison unfortunately feasible. focusing learning rank aspect select representative methods three categories pointwise methods deﬁne loss functions instance pairs binary reconstructive embeddings fast supervised hashing hashing using auxiliary coordinates deep pair-supervised hashing hashing continuation listwise compare listwise ranking methods structured hashing optimizes ndcg surrogate hashing mutual information optimizes mutual information ranking surrogate binary afﬁnities. since tie-aware evaluation hamming ranking performance reported hashing literature re-train evaluate methods using publicly available implementations. evaluate optimization three labeled datasets cifar- nus-wide imagenet. mentioned earlier labeled data afﬁnities inferred label agreements. speciﬁcally cifar- imagenet examples neighbors share class label. multiﬁrst carry optimization experiments well-studied datasets cifar- nus-wide. experiments perform ﬁnetuning using imagenetpretrained vgg-f network used dpsh dtsh recent top-performing methods. methods amenable end-to-end training train fc-layer features vgg-f. cifar- compare methods ﬁrst setting second setting compare end-to-end methods dpsh dtsh mihash ours. include hashnet uses different network architecture compare later imagenet. present optimization results table optimizing relaxation end-to-end fashion method achieves state-of-the-art datasets outperforming pair-based tripletbased methods signiﬁcant margins. compared listwise ranking solutions talr-ap outperforms structhash signiﬁcantly taking advantage deep learning outperforms mihash matching training objective evaluation metric. side note nus-wide customary previous work report evaluated maximum cutoff since ranking full database inefﬁcient using general-purpose sorting algorithms. however focusing ranking overestimates true seen table using counting sort able evaluate full database efﬁciently talr-ap also outperforms methods terms apk. imagenet experiments closely follow setup hashnet ﬁne-tune alexnet architecture pretrained imagenet. space limitations report comparisons recent state-of-the-art methods imagenet. ﬁrst competitor hashnet empirically superior wide range classical recent methods previously state-of-the-art method imagenet. also compare mihash second-best method cifar- nus-wide previous experiment. minibatch size methods learning rate pretrained convolution fully connected layers scaled down since model ﬁne-tuned dataset originally trained cutoff used evaluation metric. method machash fasthash structhash dpsh dtsh mihash talr-ap method dpsh dtsh mihash talr-ap trained using parameters recommended authors dtsh. table comparison cifar- nus-wide vgg-f architecture. cifar- compare methods ﬁrst setting deep learning methods second report tie-aware additionally nus-wide. talr-ap optimizes tie-aware using stochastic gradient ascent achieves state-of-the-art performance. bits). indicates direct optimization approach produces better compact binary representations preserve desired rankings. state-of-the-art performance compact codes important implications cases memory storage resources restricted indexing large-scale databases. evaluate ndcg optimization multi-level afﬁnity setup i.e. afﬁnity values ﬁnite non-negative integers. multi-level afﬁnities common information retrieval tasks offer ﬁne-grained speciﬁcation desired structure learned hamming space. knowledge setup considered hashing literature. multi-label nus-wide dataset deﬁne afﬁnity value examples number labels share keep settings experiment. unlabeled labelme dataset derive afﬁnities thresholding euclidean distances between examples. inspired existing binary afﬁnity setup deﬁnes neighbors euclidean distance within training four thresholds assign afﬁnity values emphasizes assigning high ranks closest neighbors original feature space. learn shallow models precomputed gist features labelme. gradient-based methods means using linear hash functions i.e. methods designed multi-level afﬁnities convert afﬁnities binary values; reduces standard binary afﬁnity setup datasets. give ndcg results table again method tie-aware ndcg objective outperforms competing methods datasets. interestingly labelme methods restricted learn shallow models gist features observe slightly different trends compared datasets. example without learning deep representations dpsh dtsh appear perform less competitively indicating mismatch objectives evaluation metric. closest competitors talr-ndcg labelme indeed listwise ranking methods structhash optimizes ndcg surrogate using boosted decision trees mihash designed binary afﬁnities. talr-ndcg outperforms methods notably linear hash functions lower learning capacity compared structhash’s boosted decision trees. highlights beneﬁt direct optimization formulation. lastly discuss effect tie-breaking evaluating hashing algorithms. mentioned sec. tie-breaking uncontrolled parameter current evaluation protocols table ndcg comparison nus-wide labelme talrndcg optimizes tie-aware ndcg using stochastic gradient ascent consistently outperforms competing methods. figure effects tie-breaking plot ranges test-time values spanned possible tie-breaking strategies methods considered cifar- experiment horizontal axis map. black dots values tie-aware apt. without controlling tie-breaking relative performance comparison different methods ambiguous. ambiguity eliminated tie-awareness. affect results even exploited. demonstrate this consider example experiment cifar-’s ﬁrst setting presented sec. method included experiment plot range test spanned possible tie-breaking strategies. seen fig. ranges corresponding different methods generally overlap; therefore without controlling tie-breaking relative performance comparison different methods essentially ambiguous. ranges shrink code length increases since number ties generally decreases bins histogram. current hashing methods usually compute test-time ndcg using random tie-breaking general-purpose sorting algorithms. interestingly experiments observe produces values close tieaware ndcgt. reason randomly ordered database averaging tie-unaware metric sufﬁciently large test behaves similarly tieaware solution averaging permutations. therefore results reported current literature indeed quite fair found evidence exploitation tie-breaking strategies. still recommend using tie-aware ranking metrics evaluation completely eliminate ambiguity counting sort hamming distances much efﬁcient general-purpose sorting. last least would like emphasize although random tie-breaking close approximation tieawareness test time answer question optimize ranking metrics training. original motivation optimize ranking metrics hashing existence closed-form tie-aware ranking metrics makes direct optimization feasible. proposed approach hashing nearest neighbor retrieval emphasis directly optimizing evaluation metrics used test-time. study commonly used retrieval hamming ranking setup consider issue ties advocate using tie-aware versions ranking metrics. make novel contribution optimizing tie-aware ranking metrics hashing focusing important special cases ndcg. tackle resulting discrete np-hard optimization problems derive continuous relaxations closed-form gradients. then optimization performed end-to-end stochastic gradient ascent deep neural networks. results state-of-the-art common image retrieval benchmarks.", "year": 2017}