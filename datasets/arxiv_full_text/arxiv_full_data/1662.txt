{"title": "Adversarial Advantage Actor-Critic Model for Task-Completion Dialogue  Policy Learning", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "This paper presents a new method --- adversarial advantage actor-critic (Adversarial A2C), which significantly improves the efficiency of dialogue policy learning in task-completion dialogue systems. Inspired by generative adversarial networks (GAN), we train a discriminator to differentiate responses/actions generated by dialogue agents from responses/actions by experts. Then, we incorporate the discriminator as another critic into the advantage actor-critic (A2C) framework, to encourage the dialogue agent to explore state-action within the regions where the agent takes actions similar to those of the experts. Experimental results in a movie-ticket booking domain show that the proposed Adversarial A2C can accelerate policy exploration efficiently.", "text": "paper presents method adversarial advantage actor-critic signiﬁcantly improves efﬁciency dialogue policy learning taskcompletion dialogue systems. inspired generative adversarial networks train discriminator differentiate responses/actions generated dialogue agents responses/actions experts. then incorporate discriminator another critic advantage actor-critic framework encourage dialogue agent explore state-action within regions agent takes actions similar experts. experimental results movie-ticket booking domain show proposed adversarial accelerate policy exploration efﬁciently. growing interest exploiting reinforcement learning policy learning task-oriented dialogue systems biggest challenges approaches reward sparsity issue. dialogue policy learning complex tasks movieticket booking travel planning requires exploration large state-action space often takes many conversation turns user agent fulﬁll task leading long trajectory. thus reward signals often delayed sparse. deal reward sparsity different approaches proposed recently promising empirical results. approach leverage prior knowledge learned expert-generated dialogue. example instead learning dialogue policy scratch construct initial policy learned human-human dialogues imitation learning hand-crafted rules. prior work showed pre-trained supervised policy weak rule-based policy signiﬁcantly improve efﬁciency exploration another approach introduce heuristics often form intrinsic reward guide exploration extrinsic reward could sparse possible intrinsic reward action order guide agent explore region effectively. example vime maximizes information gain agents belief environment dynamics adds intrinsic reward bonus reward function quantiﬁes agents surprise encourage agent explore regions relatively unexplored. bbqn encourages agent explore state-action regions agent relatively uncertain action selection unreal converts training signals three auxiliary tasks intrinsic rewards signiﬁcantly improved learning speed robustness agent paper present method combines strength approaches mentioned above. similar ﬁrst approach also leverage expert-generated dialogues prior knowledge. however instead constructing initial dialogue policy using prior knowledge inspired generative adversarial networks train discriminator differentiate responses generated dialogue agents human experts. then output discriminator intrinsic reward encourage dialogue agent explore state-action regions agent takes actions similar human experts speciﬁcally incorporate discriminator another critic advantage actor-critic framework resulting model called adversarial advantage actor-critic modeling assumption behind method expert policies reasonably good thus agent-selected actions similar expert-selected ones lead often successful dialogues positive rewards. word remedy reward sparse problem fronts leveraging humanhuman dialogues prior knowledge introducing intrinsic rewards. experiments movie-ticket booking domain show proposed adversarial model heuristic intrinsic reward function guide actor towards expert-like regions. another related topic inverse reinforcement learning recover reward function expert demonstrations samples trajectories executed experts ermon also drew connection inverse reinforcement learning generative adversarial networks learn reward function framework compared work focused learning extrinsic reward paper intrinsic reward speech training. training objective policy-based approaches policy maximizes expected reward possible dialogue trajectories. ext= γtrt dialogue length reward time stamp discount factor. policy parametrized probabilistic mapping function state space action space long-term reward value. however gradients usually high variance makes learning task challenging. baseline function usually employed reduce variance keeping estimated gradient unchanged simply choose state value function baseline strategy rewrite using advantage function however setting functions parameters need learned. order reduce number required parameters improve stability temporal difference error employed unbiased estimate advantage function policy network termed actor yield dialogue system action advantage function critic indicating good executing action given state. classic architecture shown bottom part figure without discriminator. figure illustrates typical task-completion dialogue system contains three main components language understanding converts natural language system-readable semantic frames natural language generation converts system actions natural language dialogue manager dialogue manager controls state tracking policy learning dialogue policy learning regarded sequential decision process. system learn select best response action step maximizing long-term objective associated reward function. paper focuses dialogue policy learning input policy learner dialogue state representation consists latest user action last agent action history dialogue turns available database results. learned dialogue policy helps agent decide action take turn conversation order maximize future cumulative reward. aforementioned dialogue policy optimization formulated sequential decision problem maximize long term objective associated reward function. advantage actor-critic method achieved superior performance solving sequential decision problems applied actor-critic model dialogue policy optimization proved superiority convergence methods deep q-networks similarly employ actor-critic approach learn dialogue policy model. addition inspired form minimax game generator discriminator judge whether action performed expert actor. discriminator regarded another critic servers perform action according actor receive reward switch state store transition tuple buffer train actor gradients train value function minimizing error sample state action pairs expert demonstration demo train actor gradients update reward log) train value function update discriminator parameters furthermore training dialogue policy stand-alone adversarial model impractical high dimensionality state-and-action space. address issue propose adversarial advantage actor-critic method depicted figure combines reward function learned adversarial model serves another additional critic actor several ways combine critics linear combination reward functions alternately optimizing reward function. experiments alternating optimization. algorithm outlines full procedure training adversarial model. goal encourage actor select better actions guided discriminator order improve efﬁciency effectiveness exploration. verify performance proposed model evaluated task-completion dialogue system movie-ticket booking. system agent gather information users conversations eventually book movie tickets them. environment judges binary outcome conversation based whether movie ticket booked whether booked ticket satisﬁes constraints requested user. minimax competing game generator discriminator. scenario actor viewed generator aims generate actions purposefully confuse discriminator discriminator expected identify state-action pair either expert demonstration simulation experience. cannot distinguish actions generated actor experts believe improved previous state. moreover viewed reward function extracted experts’ trajectories. figure shows discriminator training procedure using adversarial learning. training objective saddle point simu demo represent simulation experience expert demonstration respectively. thus actor improved using actor-critic log) reward function. updated gradients reformed experts single-domain movie-ticket booking dataset contains dialogue acts slots including informable slots requestable slots total labeled dialogues average length turns. order perform end-to-end training dialogue system user simulator required interact system natural way. adopted publicly available useragenda-based simulator experiments taskcompletion dialogue setting user simulator ﬁrst generates user goal dialogue agent tries help user accomplish goal course conversation without explicitly knowing user goal. user goal normally consists parts inform slots representing slot-value pairs serve constraints user request slots representing slots whose value user information about wants information agent conversation. experiment user goals generated labeled conversational data. figure expert demonstrations collected either human pre-trained agent. experiment collected successful dialogues pre-trained agent. discriminator binary classiﬁer single-layer neural network hidden units. actor single-layer neural network hidden size pre-trained rulebased examples order give acceptable initialization. during adversarial model training critics applied alternatively value functions single-layer neural networks hidden units. parameters optimized rmsprop. training model updated dialogue episode. movie-ticket booking task benchmark proposed adversarial model three baseline models three metrics success rate average rewards average number turns dialogue session. rule agent handcrafted rule-based policy informs requests hand-picked subset necessary slots. agent trained pre-deﬁned reward function bbqn-map agent best agent among bbqn variants variants demonstrated great efﬁciency policy exploration task-completion dialogue systems figure shows learning curves dialogue agents mentioned above table shows evaluation performance agent averaged runs. learning curves figure shows adversarial agent learn much faster better exploration capability. learning curve also stable compared others. table suggests adversarial agent yield better dialogue policies approaches terms success rate average reward average number turns dialogue. paper presents adversarial advantage actor-critic model explore policy learning task-completion dialogue systems great efﬁciency. proposed model learns discriminator expert demonstrations online experience learned discriminator serves additional critic guide policy learning. experiments movie-ticket booking domain demonstrate superiority efﬁciency proposed model policy learning compared state-of-the-art approaches. promising results suggest several interesting future directions employing variance-reducing methods stabilize gradient calculation order address high variance issue policy gradient estimation applying model complicated dialogue tasks composite task-completion dialogues extending work deep reinforcement learning benchmark tasks domains. tiancheng zhao maxine eskenazi towards end-to-end learning dialog state tracking management using deep reinforcement learning proceedings sigdial pei-hao milica gasic nikola mrksic lina rojasbarahona stefan ultes david vandyke tsung-hsien steve young continuously learning neural dialogue management arxiv preprint arxiv. jason williams kavosh asadi geoffrey zweig hybrid code networks practical efﬁcient end-to-end dialog control supervised reinforcement learning proceedings bhuwan dhingra lihong xiujun jianfeng yunnung chen faisal ahmed deng towards end-toend reinforcement learning dialogue agents information access proceedings baolin peng xiujun lihong jianfeng asli celikyilmaz sungjin kam-fai wong composite taskcompletion dialogue policy learning hierarchical deep reinforcement learning emnlp goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio generative adversarial nets nips volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning nature vol. david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search nature vol. david silver julian schrittwieser karen simonyan ioannis antonoglou huang arthur guez thomas hubert lucas baker matthew adrian bolton mastering game without human knowledge nature vol. pei-hao david vandyke milica gasic nikola mrksic tsung-hsien steve young reward shaping recurrent neural networks speeding on-line policy learning spoken dialogue systems proceedings sigdial volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu asynchronous methods deep reinforcement learning icml zachary lipton jianfeng lihong xiujun faisal ahmed deng efﬁcient exploration dialogue policy learning networks replay buffer spiking arxiv preprint arxiv. jaderberg volodymyr mnih wojciech marian czarnecki schaul joel leibo david silver koray kavukcuoglu reinforcement learning unsupervised auxiliary tasks arxiv preprint arxiv.", "year": 2017}