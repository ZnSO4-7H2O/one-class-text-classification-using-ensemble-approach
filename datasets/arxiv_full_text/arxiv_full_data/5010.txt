{"title": "Disentangled Variational Auto-Encoder for Semi-supervised Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "In this paper, we develop a novel approach for semi-supervised VAE without classifier. Specifically, we propose a new model called SDVAE, which encodes the input data into disentangled representation and non-interpretable representation, then the category information is directly utilized to regularize the disentangled representation via equation constraint. To further enhance the feature learning ability of the proposed VAE, we incorporate reinforcement learning to relieve the lack of data. The dynamic framework is capable of dealing with both image and text data with its corresponding encoder and decoder networks. Extensive experiments on image and text datasets demonstrate the effectiveness of the proposed framework.", "text": "paper develop novel approach semi-supervised without classiﬁer. speciﬁcally propose model called sdvae encodes input data disentangled representation non-interpretable representation category information directly utilized regularize disentangled representation equation constraint. enhance feature learning ability proposed incorporate reinforcement learning relieve lack data. dynamic framework capable dealing image text data corresponding encoder decoder networks. extensive experiments image text datasets demonstrate effectiveness proposed framework. abundant data generated online every greatly advanced machine learning data mining computer vision communities. however manual labeling large dataset time labor consuming. sometimes even requires domain knowledge. results majority data limited labels. therefore semi-supervised learning utilizes labeled unlabeled data model training attracting increasing attention existing semi-supervised models generally categorized categories i.e. discriminative model generative model graph-based model combined model categories among various semi-supervised models proposed semisupervised generative models based variational auto-encoder shown strong performance image classiﬁcation text classiﬁcation effectiveness semisupervised learning comes efﬁciency posterior distribution estimation powerful ability feature extracting text data image data adapt semi-supervised learning semi-supervised vaes typically composed three main components encoder network decoder classiﬁer application encoder decoder classiﬁer implemented using various models e.g. networks though classiﬁer plays vital role achieving semi-supervised goal introduces extra parameters learn. limited labeled data optimal choice introduce parameters semisupervised learning memorize limited data large quantities parameters namely overﬁting. therefore paper investigate directly incorporate limited label information without introducing classiﬁer achieve goal semi-supervised learning time reduce number parameters learned. particular investigate following challenges without introducing classiﬁer incorporate label information semi-supervised learning? effectively label information representation learning vae? attempt solve challenges propose novel semi-supervised learning model named semi-supervised disentangled variational auto-encoder sdvae adopts conditions better representation learning ability vae. unlike existing semi-supervised vaes utilize classiﬁers sdvae encodes input data disentangled representation noninterpretable representation category information directly utilized regularize disentangled representation equation constraint. labeled data limited labeled information affects model much. remedy this change equation constraints reinforcement learning format helps objective gain category information heuristics. inverse auto-regression also applied improve latent variable learning. proposed framework ﬂexible deal image text data choosing corresponding encoder decoder networks. main contributions paper lagrangian multiplier used penalize deviation constraint kl||pθ) given λkl||pθ) reduces original variational autoencoder problems proposed kingma however closer target mathematical description fact information latent variable tighter lower bound condition loose constraint decoder introduced. empirical results show condition performs better original vae. thus paper condition basic model. apart classiﬁcation loss label information added objective function facing labeled data. however paper discriminative information added scratch equation constrained proposed order highlight contribution labeled data. term extract latent feature observed data called encoder generally. minimizing divergence approximate true posterior distribution non-negative ﬁxed minimizing kl||pθ equivalent maximizing rewrite ﬁrst term reconstruction error second term divergence prior posterior values play different roles approximation. introduce details next section. practice usually main error term regarded regularization enforce close relatively small. constrain divergence term small component gain tighter lower bound goal transformed maximize order incorporate label information latent representation assume latent representation divided parts i.e. disentangle variable non-interpretable variable. disentangle variable captures categorical information used prediction task. therefore label information constrain disentangled variable. non-interpretable variable vectors comprised dimensions combine uncertain information data. simplicity notation denote disentangled variable denote non-interpretable representation. encoder rewritten assume disentangled variable noninterpretable variable independent condition i.e. reasonable assumption given categorical information dependent thus captures categorical information independent given means seldom information category information validated experiment part. encoder non-interpretable representation encoder disentangle representation. based assumptions written eqφqφ represents reconstruction error given variable denote kl||p) kl||p) respectively. equation categorical information extracted data i.e. captured disentangled variable partial labels given directly label information regularize capturing categorical information many ways regularize inspired work equation constraints elbo equation constraint enforce disentangled representation close label information work consider ways constraint elbo discussed below. observed label information encoder disentangle variable popular loss function supervised learning doesn’t introduce parameters. therefore choose loss function regularizing disentangled variable name method semi-supervised disentangled adding loss function objective function sdvae-i given drawback sdvae-i obviously training results depend number labeled data heavily. however semi-supervised learning usually small size labeled data available. thus hard disentangle variable capture category information. remedy this inspired idea equation constraints expressed reinforcement learning elbo seen reward equation constraint. disentangle variable acts agent decides output category information. seen eq.. finally constant number added bias. parameter changed followed however terms take effect labeled data. make drawback another term log-likelihood expectation disentangle variable added information entropy calculated labeled data unlabeled data. helps reduce large variance disentangle information. objective function changed eq.. different latent variables extracted data directly make posterior inference ﬂexible enhance ability disentangle representation highdimension space inverse autoregressive applied sdvae-i sdvae-ii. chain initialized output encoder. together random sample latent variable calculated update chain lstm shown eq.. models trained end-to-end using mini-batch adam optimizer training algorithm summarized algorithm line initialize parameters. line line sample mini-batch encode input data line line apply iaf. update parameters line line time labeled data unlabeled data. furthermore assume variables independent. however previous works extract latent variable data. label information label variable infers shared parameters infers directly. then based different assumptions differences previous works mathematics. elbo independent latent variable inferences written different latent variable inference. furthermore ignore assumption difference facing labeled data previous works objective function special case label missing previous works apply marginal posterior inference label information shown eq.. paper inference latent variable inference shown eq.. section conduct experiments validate effectiveness proposed framework. speciﬁcally want answer following questions disentangled representation able capture categorical information? noninterpretable variable helpful data reconstruction? proposed framework effective semi-supervised learning? answer questions conduct experiments image text datasets respectively. image domain choose widely used benchmark datasets evaluating effectiveness sdvae i.e. mnist svhn mnist data samples train data samples test set. svhn data samples train data samples test set. datasets contain categories. convolutional layers used extract features images fully connected layers used convert features non-interpretable variable disentangle variable. decoder network composed fully connected layers latent features back images. dropout applied encoder decoder networks. ﬁrst experiment explore disentangle variable non-interpretable variable perform image reconstruction. experiment conducted mnist dataset. training data randomly select data samples labeled data remainings unlabeled. dimension disentangle variable category number dimension ﬁrst train model learn parameters. trained model learn latent representation test data. learning representations mask turn affect reconstruction input image. sample results shown fig.. also t-sne visualize testing data. results four models shown fig.. mainly captures categorical information little inﬂuence reconstruction task. speciﬁcally fig. images class clustered together implying disentangled representation captures categorical information. addition cluster sdvae-i gives worst visualization clusters intersections sdvae-i&iaf sdvae-ii&iaf give better visualization suggests sdvae-i&iaf sdvaeii&iaf better capturing categorical information. fig. masked still reconstructs input image well indicating appropriate reconstruction. explore variable takes effect image reconstruction range certain dimension properties image reconstruction different dimensions italic controlling bold controlling transform control style controlling etc. seen images fig. left right. furthermore conduct experiments test proposed models semi-supervised learning mnist. randomly select points training labeled data varied rest training data used unlabeled data. compare state-of-the-art supervised semi-supervised classiﬁcation algorithms used experiments conducted times average accuracy standard deviation showed table note performances compared methods too. table proposed model sdvae-ii&iaf performs best classiﬁcation makes least classiﬁcation errors small part labeled data. although sdvae-i performs good proposed models still achieve state-of-the-art results. validate observation also conduct semisupervised learning svhn another popularly used dataset. svhn training samples test samples. among training data randomly select data samples labeled data rest unlabeled data. results shown table similarly observe sdvaeii&iaf gives best performance. application text data encoder also convolutional neural networks different case image data convolutional neural networks referring parallelized together. extracting feature word level extracting feature character level. decoder applied conditioned lstm given follows conditional lstm vanilla lstm except current variable replaced concatenation latent variable techniques dropout batch normalized utilized encoder decoder networks. randomly select samples training labeled data others unlabeled training. similarly t-sne visualize disentangle variable non-interpretable variable proposed model test data unlabeled data. results showed fig. left ﬁgure fig. disentangle representation clearly separate positive negative samples non-interpretable representation cannot i.e. data points clusters interleaved other. suggests disentangle representation captures categorical information well seldom categorical information non-interpretable variable. figure left ﬁgure t-sne distribution noninterpretable variable right ﬁgure t-sne distribution disentangle variable correspondingly. different categories different colors number. conduct semi-supervised classiﬁcation text dataset using representation learned previous experiments tuning model. similarly compare state-of-the-art semi-supervised learning algorithms. average test error rate reported table results that sdvae-ii&iaf outperforms compared methods implies effectiveness proposed framework semi-supervised learning; reinforcement learning performance increases suggests components contribute model. objective function proper ﬁnding depicted eq.. results different values shown fig. results better small value leads rich information latent variable also gets better reconstruction error. described before large value kl-divergence also cause overﬁtting underﬁtting model. however case reconstruction error sign good performance. model structure chain built according results different length shown right ﬁgure fig.. ﬁgure good chain long long iaf. rces good together klds latent variable isvery unstable. contrary stable increase divergence stable decrease reconstruction error length chain means good reconstruction latent variable captures useful information. also validated results sdvaei&iaf sdvae-ii&iaf. thus experiments length default. decide parameter sdvae-ii made grid search text data image data. image data experiment conducted svhn dataset labeled samples. experimental result ranges ranges shown fig.. text data experiment conducted imdb data labeled samples. experimental result range shown fig.. semi-supervised semi-supervised learning attracting increasing attention lots works proposed works divided discriminative models generative models graph based models combined model effectiveness deep generative models capturing data distribution semi-supervised models based deep generative models generative adversarial network variational auto-encoder become popular. semi-vae incorporates learned latent variable classiﬁer improves performance greatly. ssvae extends semi-vae sequence data also demonstrates effectiveness semi-supervised learning text data. aforementioned semi-supervised parametric classiﬁer increases burden learn parameters given limited labeled data. proposed framework incorporates label information directly disentangled representation thus avoids parametric classiﬁer. variants great potential image text mining various models based proposed improve performance example apply condition gave tighter lower bound. similarly introduce importance weighting also tries give tighter bound. consider stein based sampling minimize divergence. rewrite evidence lower bound objective decomposition give clear explanation term. extend ﬂexible posterior inference introduced improves lot. work propose models extract disentangle variable non-interpretable variable data time. disentangle variable designed capture category information thus relieves classiﬁers semisupervised learning. non-interpretable variable designed reconstruct data. experiments show could even reﬂect certain textual features italic bold transform style hand writing digital data reconstruction. variables cooperate well performs functions sdvae. improves model effectively basis sdvae-i sdvae-ii. especially which sdvaeii&iaf achieves state-of-the-art results image data text data semi-supervised learning tasks.", "year": 2017}