{"title": "Poincaré Embeddings for Learning Hierarchical Representations", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, while complex symbolic datasets often exhibit a latent hierarchical structure, state-of-the-art methods typically learn embeddings in Euclidean vector spaces, which do not account for this property. For this purpose, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space -- or more precisely into an n-dimensional Poincar\\'e ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We introduce an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincar\\'e embeddings outperform Euclidean embeddings significantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability.", "text": "representation learning become invaluable approach learning symbolic data text graphs. however complex symbolic datasets often exhibit latent hierarchical structure state-of-the-art methods typically learn embeddings euclidean vector spaces account property. purpose introduce approach learning hierarchical representations symbolic data embedding hyperbolic space precisely n-dimensional poincaré ball. underlying hyperbolic geometry allows learn parsimonious representations symbolic data simultaneously capturing hierarchy similarity. introduce efﬁcient algorithm learn embeddings based riemannian optimization show experimentally poincaré embeddings outperform euclidean embeddings signiﬁcantly data latent hierarchies terms representation capacity terms generalization ability. learning representations symbolic data text graphs multi-relational data become central paradigm machine learning artiﬁcial intelligence. instance word embeddings wordvec glove fasttext widely used tasks ranging machine translation sentiment analysis. similarly embeddings graphs latent space embeddings nodevec deepwalk found important applications community detection link prediction social networks. embeddings multi-relational data rescal transe universal schema used knowledge graph completion information extraction. typically objective embedding methods organize symbolic objects similarity embedding space reﬂects semantic functional similarity. purpose similarity objects usually measured either distance inner product embedding space. instance mikolov embed words inner product maximized words co-occur within similar contexts text corpora. motivated distributional hypothesis i.e. meaning words derived contexts appear. similarly hoff embed social networks distance social actors minimized connected network. reﬂects homophily property found many real-world networks i.e. similar actors tend associate other. although embedding methods proven successful numerous applications suffer fundamental limitation ability model complex patterns inherently bounded dimensionality embedding space. instance nickel showed linear embeddings graphs require prohibitively large dimensionality model certain types relations. although non-linear embeddings mitigate problem complex graph patterns still require computationally infeasible embedding dimensionality. consequence method exists able compute embeddings large graph-structured data social networks knowledge graphs taxonomies without loss information. since ability express information precondition learning generalization therefore important increase representation capacity embedding methods realistically used model complex patterns large scale. work focus mitigating problem certain class symbolic data i.e. large datasets whose objects organized according latent hierarchy property inherent many complex datasets. instance existence power-law distributions datasets often traced back hierarchical structures prominent examples power-law distributed data include natural language scale-free networks social semantic networks similarly empirical analysis adcock indicated many real-world networks exhibit underlying tree-like structure. exploit structural property learning efﬁcient representations propose compute embeddings euclidean hyperbolic space i.e. space constant negative curvature. informally hyperbolic space thought continuous version trees naturally equipped model hierarchical structures. instance shown ﬁnite tree embedded ﬁnite hyperbolic space distances preserved approximately base approach particular model hyperbolic space i.e. poincaré ball model well-suited gradient-based optimization. allows develop efﬁcient algorithm computing embeddings based riemannian optimization easily parallelizable scales large datasets. experimentally show approach provide high quality embeddings large taxonomies without missing data. moreover show embeddings trained wordnet provide state-of-the-art performance lexical entailment. collaboration networks also show poincaré embeddings successful predicting links graphs outperform euclidean embeddings especially dimensions. remainder paper organized follows section brieﬂy review hyperbolic geometry discuss related work regarding hyperbolic embeddings. section introduce poincaré embeddings discuss compute them. section evaluate approach tasks taxonomy embedding link prediction networks predicting lexical entailment. hyperbolic geometry non-euclidean geometry studies spaces constant negative curvature. instance associated minkowski spacetime special relativity. network science hyperbolic spaces started receive attention well-suited model hierarchical data. instance consider task embedding tree metric space structure reﬂected embedding. regular tree branching factor nodes level nodes level less equal hence number children grows exponentially distance root tree. hyperbolic geometry kind tree structure modeled easily dimensions nodes exactly levels root placed sphere hyperbolic space radius nodes less levels root located within sphere. type construction possible hyperbolic disc area circle length grow exponentially radius. figure example. intuitively hyperbolic spaces thought continuous versions trees vice versa trees thought \"discrete hyperbolic spaces\" similar construction possible circle length disc area grow linearly quadratically regard euclidean geometry. instead necessary increase dimensionality embedding model increasingly complex hierarchies. number parameters increases lead computational problems terms runtime memory complexity well overﬁtting. properties hyperbolic space recently considered model complex networks. instance kleinberg introduced hyperbolic geometry greedy routing geographic communication networks. similarly boguñá proposed hyperbolic embeddings internet topology perform greedy shortest path routing embedding space. krioukov developed framework model complex networks using hyperbolic spaces discussed instance dimensional hyperbolic space constant curvature length circle geodesics poincaré disk growth poincaré distance figure negative curvature distance points increases exponentially closer boundary. growth poincaré distance relative euclidean distance norm embedding regular tree connected nodes spaced equally apart typical properties heterogeneous degree distributions strong clustering emerges assuming underlying hyperbolic geometry networks. adcock proposed measure based gromov’s δ-hyperbolicity characterize tree-likeness graphs. machine learning artiﬁcial intelligence hand euclidean embeddings become popular approach learning symbolic data. instance addition methods discussed section paccanaro hinton proposed ﬁrst embedding methods learn relational data. recently holographic complex embeddings shown state-of-the-art performance knowledge graph completion. relation hierarchical representations vilnis mccallum proposed learn density-based word representations i.e. gaussian embeddings capture uncertainty asymmetry. given information hierarchical relations form ordered input pairs vendrov proposed order embeddings model visual-semantic hierarchies words sentences images. following interested ﬁnding embeddings symbolic data distance embedding space reﬂects semantic similarity. assume exists latent hierarchy symbols organized. addition similarity objects intend also reﬂect hierarchy embedding space improve existing methods ways inducing appropriate bias structure embedding space learning parsimonious embeddings superior generalization performance decreased runtime memory complexity. however assume direct access information hierarchy e.g. ordered input pairs. instead consider task inferring hierarchical relationships fully unsupervised instance necessary text network data. reasons motivated discussion section embed symbolic data hyperbolic space contrast euclidean space exist multiple equivalent models beltrami-klein model hyperboloid model poincaré half-plane model. following base approach poincaré ball model well-suited gradient-based optimization. seen easily distance function poincare ball equation differentiable. hence model optimization algorithm needs maintain constraint embeddings. models hyperbolic space however would difﬁcult optimize either form distance function constraints introduce. instance hyperboloid model constrained points distance function beltrami-klein model requires compute location ideal points boundary unit ball. particular open d-dimensional unit ball denotes euclidean norm. poincaré ball model hyperbolic space corresponds riemannian manifold i.e. open unit ball equipped riemannian metric tensor boundary ball denoted corresponds sphere part hyperbolic space represents inﬁnitely distant points. geodesics circles orthogonal figure illustration. seen equation distance within poincaré ball changes smoothly respect location locality property poincaré distance ﬁnding continuous embeddings hierarchies. instance placing root node tree origin would relatively small distance nodes euclidean norm zero. hand leaf nodes placed close boundary poincaré ball distance grows fast points norm close one. furthermore please note equation symmetric hierarchical organization space solely determined distance points origin. self-organizing property equation applicable unsupervised setting hierarchical order objects speciﬁed advance text networks. remarkably equation allows therefore learn embeddings simultaneously capture hierarchy objects well similarity since single hierarchical structure already represented dimensions poincaré disk typically used represent hyperbolic geometry. method instead poincaré ball main reasons first many datasets text corpora multiple latent hierarchies co-exist always modeled dimensions. second larger embedding dimension decrease difﬁculty optimization method good embedding allows degrees freedom optimization process. compute poincaré embeddings symbols {xi}n interested ﬁnding assume given problem-speciﬁc loss function embeddings {θi}n encourages semantically similar objects close embedding space according poincaré distance. estimate solve optimization problem since poincaré ball riemannian manifold structure optimize equation stochastic riemannian optimization methods rsgd rsvrg particular denote tangent space point furthermore denote riemannian gradient denote euclidean gradient using rsgd parameter updates minimize equation form denotes retraction onto denotes learning rate time hence minimization equation require riemannian gradient suitable retraction. since poincaré ball conformal model hyperbolic space angles adjacent vectors identical angles euclidean space. length vectors however might differ. derive riemannian gradient euclidean gradient sufﬁcient rescale inverse poincaré ball metric tensor i.e. since scalar matrix inverse trivial compute. furthermore since equation fully differentiable euclidean gradient easily derived since symmetric partial derivative derived analogously. retraction operation combination riemannian gradient corresponds well-known natural gradient method furthermore constrain embeddings remain within poincaré ball projection seen equations algorithm scales well large datasets computational memory complexity update depends linearly embedding dimension. moreover algorithm straightforward parallelize methods hogwild updates sparse collisions unlikely large-scale data. addition optimization procedure found following training details helpful obtaining good representations first initialize embeddings randomly uniform distribution causes embeddings initialized close origin second found good initial angular layout helpful good embeddings. reason train initial \"burn-in\" phase reduced learning rate η/c. combination initializing close origin improve angular layout without moving towards boundary. experiments duration burn-in epochs. section evaluate quality poincaré embeddings variety tasks i.e. embedding taxonomies link prediction networks modeling lexical entailment. compare poincaré distance deﬁned equation following distance functions euclidean cases include euclidean distance euclidean distance symmetric expect requires large dimensionality model hierarchical structure data. translational asymmetric data also include score function proposed bordes modeling large-scale graph-structured data. score function also learn global translation vector training. note translational score function asymmetry information nature embedding problem symmetric distance order indicates hierarchy elements. instance case is-a relations taxonomies. poincaré distance euclidean distance could randomly permute order obtain identical embedding case translational score function. such fully unsupervised applicable hierarchical information available. table experimental results transitive closure wordnet noun hierarchy. highlighted cells indicate best euclidean embeddings well poincaré embeddings acheive equal better results. bold numbers indicate absolute best results. ﬁrst experiments interested evaluating ability poincaré embeddings embed data exhibits clear latent hierarchical structure. purpose conduct experiments transitive closure wordnet noun hierarchy settings reconstruction evaluate representation capacity embed fully observed data reconstruct embedding. reconstruction error relation embedding dimension measure capacity model. link prediction test generalization performance split data train validation test randomly holding observed links. links validation test include root leaf nodes links would either trivial predict impossible predict reliably. since using transitive closure hypernymy relations form directed acyclic graph hierarchical structure directly visible data inferred. transitive closure wordnet noun hierarchy consists nouns hypernymy relations. data learn embeddings settings follows observed hypernymy relations noun pairs. learn embeddings symbols related objects close embedding space. particular minimize loss function negative examples training randomly sample negative examples positive example. equation interpreted soft ranking loss related objects closer objects didn’t observe relationship. choice loss function motivated fact don’t want push symbols belonging distinct subtrees arbitrarily apart subtrees might still close. instead want farther apart symbols observed relation. evaluate quality embeddings commonly done graph embeddings observed relationship rank distance among ground-truth negative examples i.e. among d)}. reconstruction setting evaluate ranking nouns dataset. record mean rank well mean average precision ranking. results experiments shown table seen poincaré embeddings successful embedding large taxonomies regard representation capacity generalization performance. even compared translational embeddings information structure task poincaré figure two-dimensional poincaré embeddings transitive closure wordnet mammals subtree. ground-truth is-a relations original wordnet tree indicated blue edges. poincaré embedding achieves mean rank subtree. embeddings show greatly improved performance using embedding smaller order magnitude. furthermore results poincaré embeddings link prediction task robust regard embedding dimension. attribute result structural bias poincaré embeddings could lead reduced overﬁtting kind data clear latent hierarchy. figure show additionally visualization two-dimensional poincaré embedding. purpose clarity embedding trained mammals subtree wordnet. next evaluated performance poincaré embeddings link prediction networks. since edges complex networks often explained latent hierarchies nodes interested beneﬁts poincaré embeddings terms representation size generalization performance. performed experiments four commonly used social networks astroph condmat grqc hepph. networks represent scientiﬁc collaborations exists undirected edge persons co-authored paper. networks model probability edge proposed krioukov fermi-dirac distribution hyperparameters. here corresponds radius around point points within radius likely edge parameter speciﬁes steepness logistic function inﬂuences average clustering well degree distribution cross-entropy loss learn embeddings sample negatives section evaluation split dataset randomly train validation test set. hyperparameters tuned method validation set. table lists score poincaré euclidean embeddings test hyperparameters best validation score. additionally list reconstruction performance without missing data. translational embeddings applicable datasets consist undirected edges. seen poincaré embeddings perform well datasets especially low-dimensional regime outperform euclidean embeddings. interesting aspect poincaré embeddings allow make graded assertions hierarchical relationships hierarchies represented continuous space. test property hyperlex gold standard resource evaluating well semantic models capture graded lexical entailment quantifying degree type ratings scale using noun part hyperlex consists rated noun pairs evaluated well poincaré embeddings reﬂect graded assertions. purpose used poincaré embeddings obtained section embedding wordnet dimensionality note embeddings speciﬁcally trained task. determine extent is-a true used score function here term acts penalty lower embedding hierarchy i.e. higher norm hyperparameter determines severity penalty. experiments using equation scored noun pairs hyperlex recorded spearman’s rank correlation ground-truth ranking. results experiment shown table seen ranking based poincaré embeddings clearly outperforms state-of-the-art methods evaluated methods table preﬁxed also wordnet basis therefore comparable. embeddings also achieved state-of-the-art accuracy wbless evaluates non-graded lexical entailment. paper introduced poincaré embeddings learning representations symbolic data showed simultaneously learn similarity hierarchy objects. furthermore proposed efﬁcient algorithm compute embeddings showed experimentally poincaré embeddings provide important advantages euclidean embeddings hierarchical data first poincaré embeddings enable parsimonious representations whats allows learn high-quality embeddings large-scale taxonomies. second excellent link prediction results indicate hyperbolic geometry introduce important structural bias embedding complex symbolic data. third state-of-the-art results predicting lexical entailment suggest hierarchy embedding space corresponds well underlying semantics data. main focus work evaluate general properties hyperbolic geometry embedding symbolic data. future work intend expand applications poincaré embeddings instance multi-relational data also derive models tailored speciﬁc applications word embeddings. furthermore shown natural gradient based optimization already produces good embeddings scales large datasets. expect full riemannian optimization approach increase quality embeddings lead faster convergence. references aaron adcock blair sullivan michael mahoney. tree-like structure large social information networks. data mining ieee international conference pages ieee antoine bordes nicolas usunier alberto garcía-durán jason weston oksana yakhnenko. translating embeddings modeling multi-relational data. advances neural information processing systems pages guillaume bouchard sameer singh theo trouillon. approximate reasoning capabilities low-rank vector spaces. aaai spring syposium knowledge representation reasoning integrating symbolic neural approaches mikhael gromov. hyperbolic groups. essays group theory pages springer aditya grover jure leskovec. nodevec scalable feature learning networks. proceedings sigkdd international conference knowledge discovery data mining pages zellig harris. distributional structure. word peter hoff adrian raftery mark handcock. latent space approaches social network analysis. journal american statistical association douwe kiela laura rimell ivan vulic stephen clark. exploiting image generality lexical entailment detection. proceedings annual meeting association computational linguistics pages robert kleinberg. geographic routing using hyperbolic space. infocom ieee international conference computer communications. ieee pages ieee dmitri krioukov fragkiskos papadopoulos maksim kitsak amin vahdat marián boguná. george miller christiane fellbaum. wordnet electronic lexical database maximilian nickel volker tresp hans-peter kriegel. three-way model collective learning multi-relational data. proceedings international conference machine learning icml pages maximilian nickel xueyan jiang volker tresp. reducing rank relational factorization models including observable patterns. advances neural information processing systems pages maximilian nickel lorenzo rosasco tomaso poggio. holographic embeddings knowledge graphs. proceedings thirtieth aaai conference artiﬁcial intelligence pages bryan perozzi rami al-rfou steven skiena. deepwalk online learning social representations. proceedings sigkdd international conference knowledge discovery data mining pages sebastian riedel limin andrew mccallum benjamin marlin. relation extraction matrix factorization universal schemas. proceedings naacl-hlt pages théo trouillon johannes welbl sebastian riedel éric gaussier guillaume bouchard. complex embeddings simple link prediction. proceedings international conference machine learning icml york city june pages julie weeds daoud clarke jeremy refﬁn david weir bill keller. learning distinguish hypernyms co-hyponyms. proceedings international conference computational linguistics coling pages dublin city university association computational linguistics hongyi zhang sashank reddi suvrit sra. riemannian svrg fast stochastic optimization riemannian manifolds. advances neural information processing systems pages", "year": 2017}