{"title": "Automatic Rule Extraction from Long Short Term Memory Networks", "tag": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "abstract": "Although deep learning models have proven effective at solving problems in natural language processing, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns. In this paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of representative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM.", "text": "although deep learning models proven effective solving problems natural language processing mechanism come conclusions often unclear. result models generally treated black boxes yielding insight underlying learned patterns. paper consider long short term memory networks demonstrate approach tracking importance given input lstm given output. identifying consistently important patterns words able distill state lstms sentiment analysis question answering representative phrases. representation quantitatively validated using extracted phrases construct simple rule-based classiﬁer approximates output lstm. neural network language models especially recurrent neural networks stanused dard tools natural translation sutskever question answering hewlett long short term memory hochreiter schmidhuber architecture become basic building block neural nlp. although lstm’s often core state systems operation well understood. besides basic desire scientiﬁc viewpoint clarify workings often case important understand machine learning algorithm made particular choice. moreover lstm’s costly production compared discrete models lookup tables pattern matching. work describe novel method visualizing importance speciﬁc inputs determining output lstm. demonstrate that searching phrases consistently important importance scores used extract simple phrase patterns consisting words trained lstm. phrase extraction ﬁrst done general document classiﬁcation framework different sentiment analysis datasets. demonstrate also specialized complex models applying wikimovies recently introduced question answer dataset. concretely validate extracted patterns input rules-based classiﬁer approximates performance original lstm. lines related work visualizing lstms. first hendrik karpathy analyse movement gate activations sequence. karpathy able identify co-ordinates correspond semantically meaningful attributes whether text quotes along sentence word however cell co-ordinates harder interpret particular often obvious activations inputs important speciﬁc outputs. another approach emerged literature alikaniotis denil bansal word document looking norm derivative loss function respect embedding parameters word. bridges high-dimensional cell state low-dimensional outputs. techniques generalapplicable visualizing importance sets input coordinates output coordinates differentiable function. work describe techniques designed around structure lstm’s show give better results setting. recent line work hewlett rajpurkar miller focused neural network techniques extracting answers directly documents. previous work focused knowledge bases techniques questions logical forms suitable querying them. although effective within domain inevitably incomplete thus unsatisfactory solution general problem questionanswering. wikipedia contrast enough information answer broader array questions easy query. originally introduced miller wikimovies dataset consists questions movies paired wikipedia articles. present novel decomposition output lstm product factors term product interpreted contribution particular word. thus assign importance scores words according contribution lstm’s prediction past years lstms become core component neural systems. given sequence word embeddings lstm processes word time keeping track cell state vectors contain information sentence word computed function using updates show capture notion importance word lstm’s output. however terms fail account information contributed word affected lstm’s forget gates words consequently empirically found importance scores approach often yield considerable amount false positives. nuanced approach obtained considering additive decomposition equation term interpreted contribution cell state word iterating equation ftct− it˜ct suggests natural deﬁnition alternative score corresponding augmenting terms products forget gates reﬂect upstream changes made initially processing word introduce technique using variable importance scores extract phrases trained lstm. search phrases consistently provide large contribution prediction particular class relative classes. utility patterns validated using input rules based classiﬁer. simplicity focus binary classiﬁcation case. phrase reasonably described predictive whenever occurs causes document labelled particular class labelled other. importance scores introduced correspond contribution particular words class predictions used score potential patterns looking pattern’s average contribution prediction given class relative classes. precisely given collection documents {{xij}nd given phrase compute scores classes well combined score class high strong signal class likewise propose score function order search high scoring representative phrases provide insight trained lstm denote class corresponding phrase. practice number phrases large feasibly compute score all. thus approximate brute force search step procedure. first construct list candidate phrases searching strings consecutive words importance scores threshold experiments then score rank candidate phrases much smaller phrases. extracted patterns section used construct simple rules-based classiﬁer approximates output original lstm. given document list patterns sorted descending score given classiﬁer sequentially searches pattern within document using simple string matching. ﬁnds pattern classiﬁer returns associated class given ignoring lower ranked patterns. resulting classiﬁer interpretable despite simplicity retains much accuracy lstm used build implemented models torch using default hyperparameters weight initializations. wikimovies documents questions pre-processed multiple word entities concatenated single word. given question relevant articles found ﬁrst extracting question rarest entity returning list wikipedia articles containing words. pre-deﬁned splits train validation test sets containing questions respectively. word hidden representations lstm dimension wikimovies yelp stanford sentiment treebank. models optimized using adam kingma default learning rate using early stopping validation set. rule extraction using gradient scores product reward function replaced sum. datasets found normalizing gradient scores largest gradient improved results. ﬁrst applied document classiﬁcation framework different sentiment analysis datasets. originally introduced zhang yelp review polarity dataset obtained yelp dataset challenge train test sets size task binary prediction whether review positive negative reviews relatively long average length words. also used binary classiﬁcation task stanford sentiment treebank socher less data train/dev/test sizes done sentence level much shorter document lengths. report results table seven different models. report state results prior work using convolutional neural networks; zhang yelp. also report lstm baselines competitive state along three different pattern matching models described above. also report prior results using words features naive bayes. additive cell decomposition pattern equals outperforms cell-difference patterns handily beat gradient results. coincides empirical observations regarding information contained within importance measures validates introduced measure. differences measures become pronounced yelp longer document sizes provide opportunities false positives. large wordvec zhang cnn-multichannel naive bayes socher lstm cell decomposition pattern matching cell-difference pattern matching gradient pattern matching although pattern matching algorithms underperform methods emphasize pure performance goal would expect simple model. rather fact method provides reasonable accuracy piece evidence addition qualitative evidence given later word importance scores extracted patterns contain useful information understanding actions lstm. although document classiﬁcation comprises sizeable portion current research natural language processing much recent work focuses complex problems models. section examine wikimovies recently introduced question answer dataset show simple modiﬁcations approach adapted problem. wikimovies dataset consisting questions movies paired relevant wikipedia articles. constructed using pre-existing dataset movielens paired templates extracted simplequestions dataset bordes open-domain question answering dataset based freebase. selected wikipedia articles movies identifying movies omdb associated article title match kept title ﬁrst section article. given question task read relevant articles extract answer contained somewhere within text. dataset also provides list entities containing possible answers. propose simpliﬁed version recent work given pair question document ﬁrst compute embedding question using lstm. then word document augment word embedding computed question embedding. equivalent adding additional term linear question embedding gate equations allowing patterns lstm absorbs directly conditioned upon question hand. introduce simple modiﬁcations useful adapting pattern extraction framework speciﬁc task. first order deﬁne classiﬁcations problems search over treat entity within document separate binary classiﬁcation task corresponding predictor given classiﬁcation problems rather search space possible phrases restrict ending entity question. also distinguish patterns starting beginning document introduce entity character pattern vocabulary matched entity. template examples seen below table extracted list patterns rules-based classiﬁer search positive examples return answer entity matched highest ranked positive pattern. report results different models tables show results miller key-value memory network representations information extraction text next report results lstm described section finally show results using three variants pattern matching algorithm described section using patterns extracted using additive decomposition difference cells approaches gradient importance scores discussed section performance reported using accuracy possible answers i.e. hits metric. shown table lstm model surpasses prior state nearly moreover automatic pattern matching model approximates lstm less error surprisingly small simple model falls within prior state art. similarly sentiment analysis observe clear ordering results across question categories cell decomposition scores providing best performance followed cell difference gradient scores. present extracted patterns sentiment tasks wikimovies question categories table patterns qualitatively sensible providing validation approach. increased size yelp dataset allowed longer phrases extracted relative sst. actor movie director movie writer movie movie movie year movie writer movie actor movie director movie genre movie votes movie rating movie language movie tags deﬁnitely come back again. love love love place great food great service. highly recommended deﬁnitely coming back overall great experience love everything about hidden gem. worst customer service ever horrible horrible horrible won’t back disappointed place never back there worth money recommend place riveting documentary real charmer funny touching well worth time journey heart emotional wallop pleasure watch whole family cast uniformly superb comes heart best ﬁlms year surprisingly funny deeply satisfying pretentious mess plain worst year disappointingly generic fart jokes banal dialogue poorly executed waste time weak script dullard platitudes never catches tries hard acting untalented artistes derivative horror lackluster adaptation charles dickens’ adapted journalist written western starring starring afﬂeck movie stars stars french icelandic ﬁnnish russian danish bengali dutch original german zuluczech estonian mandarin ﬁlipino hungarian great subject movie hollywood squandered opportunity using prop warmed-over melodrama kind choreographed mayhem director john built career story loses bite last-minute happy ending that’s even less plausible rest picture. often-deadly boring strange reading classic whose witty dialogue treated bafﬂing casual approach. gorgeous mind-blowing breath-taking mess although approach able extract sensible patterns achieve reasonable performance still approximation algorithm lstm. table present examples instances lstm able correctly classify sentence algorithm along pattern used algorithm. ﬁrst glance extracted patterns sensible gets done witty dialogue phrases you’d expect positive review movie. however placed broader context particular reviews cease predictive. demonstrates that although work useful ﬁrstorder approximation still additional relationships lstm able learn data. prediction accuracy rules-based classiﬁer provides quantitative validation relative merits visualizations qualitative differences also insightful. table provide side-by-side comparison different measures. discussed before difference cells technique fails account updates resulting word affected lstm’s forget gates word initially processed answer. consequently empirically found without interluding forget gates dampen cell movements variable importance scores noisier additive cell decomposition approach. additive cell decomposition identiﬁes phrase stars’ well actor’s name aqib khan important sensible conclusion. moreover vast majority words labelled importance score corresponding irrelevant. hand difference cells approach yields widely changing importance scores challenging interpret. terms noise gradient measures seem somewhere middle. patterns broadly consistent observed provide qualitative validation metrics. paper introduced novel method visualizing importance speciﬁc inputs determining output lstm. searching phrases consistently provide large contributions able distill trained state lstms ordered representative phrases. quantitatively validate extracted phrases performance simple rules-based classiﬁer. results shown general document classiﬁcation framework specialized complex recently introduced question answer dataset. introduced measures provide superior predictive ability cleaner visualizations relative prior work. believe represents exciting paradigm analysing behaviour lstm’s. table comparison importance scores acquired three different approaches conditioning question west west starred actors?. bigger darker means important. daniel hewlett alexandre lacoste llion jones illia polosukhin andrew fandrianto matthew kelcey david berthelot. wikireading novel large-scale language understanding task wikipedia. association computational linguistics alexander miller adam fisch jesse dodge amir-hossein karimi antoine bordes jason weston. key-value memory networks directly reading documents. empirical methods natural language processing richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts potts. recursive deep models semantic compositionality sentiment treebank. emnlp last dogmen western adventure written directed murphy thunderbolt hong kong action starring jackie chan bloody bloody bible amerihorror comedy platter", "year": 2017}