{"title": "Byzantine-Tolerant Machine Learning", "tag": ["cs.DC", "cs.LG", "cs.NE", "math.OC", "stat.ML"], "abstract": "The growth of data, the need for scalability and the complexity of models used in modern machine learning calls for distributed implementations. Yet, as of today, distributed machine learning frameworks have largely ignored the possibility of arbitrary (i.e., Byzantine) failures. In this paper, we study the robustness to Byzantine failures at the fundamental level of stochastic gradient descent (SGD), the heart of most machine learning algorithms. Assuming a set of $n$ workers, up to $f$ of them being Byzantine, we ask how robust can SGD be, without limiting the dimension, nor the size of the parameter space.  We first show that no gradient descent update rule based on a linear combination of the vectors proposed by the workers (i.e, current approaches) tolerates a single Byzantine failure. We then formulate a resilience property of the update rule capturing the basic requirements to guarantee convergence despite $f$ Byzantine workers. We finally propose Krum, an update rule that satisfies the resilience property aforementioned. For a $d$-dimensional learning problem, the time complexity of Krum is $O(n^2 \\cdot (d + \\log n))$.", "text": "growth data need scalability complexity models used modern machine learning calls distributed implementations. today distributed machine learning frameworks largely ignored possibility arbitrary failures. paper study robustness byzantine failures fundamental level stochastic gradient descent heart machine learning algorithms. assuming workers byzantine robust without limiting dimension size parameter space. ﬁrst show gradient descent update rule based linear combination vectors proposed workers tolerates single byzantine failure. formulate resilience property update rule capturing basic requirements guarantee convergence despite byzantine workers. ﬁnally propose krum update rule satisﬁes resilience property aforementioned. d-dimensional learning problem machine learning received attention past years. applications range images classiﬁcation ﬁnancial trend prediction disease diagnosis gaming driving major companies currently investing machine learning technologies support businesses roughly speaking machine learning consists giving computer ability improve solves problem quantity quality information short computer list internal parameters called parameter vector allows computer formulate answers several questions picture?. according many correct incorrect answers provided speciﬁc error cost associated parameter vector. learning process updating parameter vector order minimize cost. increasing amount data involved well growing complexity models learning schemes require computational resources. consequence industry-grade machine-learning implementations distributed example google reportedly used processors train image classiﬁer however distributing computation several machines induces higher risk failures including crashes computation errors. worst case system undergo byzantine failures i.e. completely arbitrary behaviors machines involved. practice failures stalled processes biases data samples distributed among processes. classical approach mask failures distributed systems state machine replication protocol requires however state transitions applied processes. case distributed machine learning constraint seen ways either processes agree sample data based update local parameter vectors agree parameter vector updated. case sample data transmitted process perform heavyweight computation update local parameter vector. entails communication computational costs defeat entire purpose distributing work. case processes check chosen update parameter vector indeed computed correctly real data byzantine failures easily prevent convergence learning algorithm. neither solutions satisfactory realistic distributed machine learning setting. fact learning algorithms today rely core component namely stochastic gradient descent whether training neural networks regression matrix factorization support vector machines cases cost function depending parameter vector minimized based stochastic estimates gradient. distributed implementations typically take following form single parameter server charge updating parameter vector worker processes perform actual update estimation based share data access speciﬁcally parameter server executes synchronous rounds which parameter vector broadcast workers. turn worker computes estimate update apply parameter server aggregates results ﬁnally update parameter vector. today aggregation typically implemented averaging variants byzantine worker force parameter server choose arbitrary vector even large amplitude direction vectors. clearly byzantine worker prevent classic averaging-based approach converge. choosing appropriate update vectors proposed workers turns challenging. non-linear distance-based choice function chooses among proposed vectors vector closest everyone else might look appealing. distance-based choice tolerates single byzantine worker. byzantine workers collude helping selected moving barycenter vectors farther correct area. formulate byzantine resilience property capturing suﬃcient conditions parameter server’s choice tolerate byzantine workers. essentially guarantee cost decrease despite byzantine workers require parameter server’s choice point average direction gradient statistical moments bounded homogeneous polynomial moments correct estimator gradient. ensure resilience property consider majority-based approach looking approach robust byzantine workers propose vectors correct area exponential computational cost prohibitive. interestingly combining intuitions majority-based distance-based methods choose vector somehow closest local computation time dimension parameter vector. simplicity presentation ﬁrst introduce version krum function selects vector. discuss method iterated leverage contribution single correct worker. paper organization. section recalls classical model distributed sgd. section proves linear combinations resilient even single byzantine worker introduces concept -byzantine resilience. section introduce krum function compute computational cost prove -byzantine resilience. section analyze convergence distributed using krum function. section discuss krum iterated leverage contribution workers. finally discuss related work open problems section cost function dataset. byzantine worker proposes vector note that since communication synchronous parameter server receive vector value given byzantine worker parameter server acts received default value parameter server computes vector applying deterministic function vectors received. refer choice function parameter server. parameter server updates parameter vector using following equation figure gradient estimates computed correct workers distributed around actual gradient cost function byzantine worker propose arbitrary vector sgd-based learning algorithms used today choice function consists computing average input vectors. lemma states linear combination vectors tolerate single byzantine worker. particular averaging robust byzantine failures. proof. byzantine worker proposes vector note parameter server could cancel eﬀects byzantine behavior setting example requires means detect worker byzantine. following deﬁne basic requirements appropriate robust choice function. intuitively choice function output vector real gradient precisely vector points steepest direction cost function optimized. expressed lower bound scalar product vector condition technical states moments controlled moments gradient estimator bounds moments classically used control eﬀects discrete nature dynamics condition allows transfer control choice function. deﬁnition -byzantine resilience). angular value integer independent identically distributed random vectors random vectors possibly dependent vi’s. choice function said -byzantine resilient vector introduce krum choice function which show satisﬁes -byzantine resilience condition. barycentric choice function fbary deﬁned vector kfbary vik. lemma howdeﬁne choice function order select among vi’s vector including correct ones thus would close real gradient. however byzantine workers propose vectors large enough move total barycenter away correct vectors remaining byzantine worker proposes barycenter. since barycenter always minimizes squared distance last byzantine worker certain vector chosen parameter server. situation depicted figure words since choice function takes account vectors including remote ones byzantine workers collude force choice parameter server. prevent arbitrary vectors proposed byzantine workers selected gradients computed correct workers area byzantine workers collude propose vectors arbitrarily remote area thus allowing another byzantine vector dimensional vectors proof. parameter server computes squared distances parameter server sorts distances sums ﬁrst values thus computing score vi’s takes additional term required minimum score negligible relatively proposition states that gradient estimator accurate enough krum function -byzantine-resilient angle depends ratio deviation gradient. krum function selects correct vector proof fact relatively easy since probability distribution correct vector gradient estimator core diﬃculty occurs krum function selects byzantine vector distribution vector completely arbitrary even depend correct vectors. general sense part proof reminiscent median technique median scalar values always bounded values proposed correct workers. extending observation multi-dimensional trivial. notice chosen byzantine vector score greater score correct worker. allows derive upper bound distance real gradient. upper bound involves distances correct correct neighbor vectors distances correct byzantine neighbor vectors. explained above ﬁrst term relatively easy control. second term observe correct vector neighbors non-neighbors. particular distance neighbor bounded correct correct vector distance. words manage control distance chosen byzantine vector real gradient upper bound involving distances vectors proposed correct workers. extent workers computing gradient estimates mini-batches indeed averaging gradient estimates mini-batch divides deviation squared root size mini-batch. proof. without loss generality assume byzantine vectors occupy last positions list arguments i.e. index correct refers vector among vn−f index byzantine refers vector among index denote number correct indices following proposition considers non-convex cost function. context non-convex optimization even centralized case generally hopeless proving parameter vector tends local minimum. many criteria used instead. follow prove parameter vector almost surely reaches region sense explained below. conditions conditions non-convex convergence analysis condition slightly stronger condition corresponding states that beyond certain horizon cost function convex enough sense direction gradient suﬃciently close direction parameter vector condition however states gradient estimator used correct workers accurate enough i.e. local standard deviation small relatively norm gradient. course norm gradient tends zero near e.g. extremal saddle points. actually ratio take advantage noise gradient estimator bias choice parameter server. therefore proposition interpreted follows presence byzantine workers parameter vector almost surely reaches basin around points right-hand side previous inequality summand convergent series. quasimartingale convergence theorem shows sequence converges almost surely turn shows sequence converges almost surely sake simplicity deﬁned krum function selects vector among vectors proposed. fact parameter server could avoid wasting contribution workers selecting vectors instead. achieved instance selecting proposition random d-dimensional vectors random vectors possibly dependent vi’s. assume η√d· kgk. then large enough m-krum function -byzantine resilient deﬁned ﬁrst glance byzantine-resilient machine problem address paper related multi-dimensional approximate agreement results d-dimensional approximate agreement cannot applied context following reasons assume vectors proposed instance agreement bounded least correct workers propose vector would require redundant work setting; importantly requires local computation worker cost seems reasonable small dimensions e.g. mobile robots meeting space becomes real issue context machine learning high billion case complexity krum function closer approach recently proposed authors assume bounded gradient work important step towards byzantine-tolerant machine learning. however study deals parameter vectors dimension one. authors tackle multi-dimensional situation using iterated approximate byzantine agreement reaches consensus asymptotically. however achieved ﬁnite possible environmental states cannot used continuous context stochastic gradient descent. present work oﬀers many possible extensions. first question whether bound tight remains open question tolerate asynchrony byzantine workers. second shown scheme forces parameter vector reach made smaller also remains open. third m-krum function iterates -krum function times multiplying overall computation complexity. alternative select ﬁrst vectors computing score krum function. proving -byzantine-resilience alternative remains open.", "year": 2017}