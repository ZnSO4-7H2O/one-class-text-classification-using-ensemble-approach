{"title": "Neurogenesis Deep Learning", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "Neural machine learning methods, such as deep neural networks (DNN), have achieved remarkable success in a number of complex data processing tasks. These methods have arguably had their strongest impact on tasks such as image and audio processing - data processing domains in which humans have long held clear advantages over conventional algorithms. In contrast to biological neural systems, which are capable of learning continuously, deep artificial networks have a limited ability for incorporating new information in an already trained network. As a result, methods for continuous learning are potentially highly impactful in enabling the application of deep networks to dynamic data sets. Here, inspired by the process of adult neurogenesis in the hippocampus, we explore the potential for adding new neurons to deep layers of artificial neural networks in order to facilitate their acquisition of novel information while preserving previously trained data representations. Our results on the MNIST handwritten digit dataset and the NIST SD 19 dataset, which includes lower and upper case letters and digits, demonstrate that neurogenesis is well suited for addressing the stability-plasticity dilemma that has long challenged adaptive machine learning algorithms.", "text": "general-purpose feature detectors shallow layers network focus situations case. dnns’ features known specialized deeper layers network therefore presumably less robust classes data. work focus inputs trained network ﬁnds difﬁcult represent. regard addressing problem continuous learning reality dnns robust concept drift data processed changes gradually time transfer learning trained model repurposed operate different domain. unlike developing visual cortex exposed varying inputs many years data used train dnns typically limited scope thereby diminishing applicability networks encode information statistically distinct training set. impact training data limitations relatively minor concern cases application domain change however domains sampled data unpredictable changes quickly seen cell phone camera value static deep network quite limited. mechanism brain maintained selective regions hippocampus permissive birth neurons throughout one’s lifetime process known adult neurogenesis speciﬁc function neurogenesis memory still debated clearly provides hippocampus unique form plasticity present regions less exposed concept drift. process biological neurogenesis complex observations neurons preferentially recruited response behavioral novelty neurons gradually learn encode information consider beneﬁts neurogenesis exploring whether artiﬁcial neurons facilitate learning novel information deep networks preserving previously trained information. accomplish this consider speciﬁc illustrative example mnist handwritten digit dataset larger nist dataset includes handwritten digits well upper lower case letters. autoencoder initially trained subset dataset’s classes continuous adaptation occurs abstract—neural machine learning methods deep neural networks achieved remarkable success number complex data processing tasks. methods arguably strongest impact tasks image audio processing data processing domains humans long held clear advantages conventional algorithms. contrast biological neural systems capable learning continuously deep artiﬁcial networks limited ability incorporating information already trained network. result methods continuous learning potentially highly impactful enabling application deep networks dynamic data sets. here inspired process adult neurogenesis hippocampus explore potential adding neurons deep layers artiﬁcial neural networks order facilitate acquisition novel information preserving previously trained data representations. results mnist handwritten digit dataset nist dataset includes lower upper case letters digits demonstrate neurogenesis well suited addressing stability-plasticity dilemma long challenged adaptive machine learning algorithms. machine learning methods powerful techniques statistically extracting useful information data throughout modern society. particular deep learning deep neural network methods proven successful part ability utilize large volumes unlabeled data progressively form sophisticated hierarchical abstractions information dl’s training processing mechanisms quite distinct biological neural learning behavior algorithmic structure somewhat analogous visual processing stream mammals progressively deeper layers cortex appear form abstracted representations sensory information acquired retina dnns typically trained once either large amount labelled data large amount unlabeled data followed smaller amount labeled data used ﬁnetune network particular function handwritten digit classiﬁcation. training paradigm often expensive requiring several days large computing clusters ideally fully trained network continue prove useful long duration even application domain changes. dnns found successes transfer learning learning remaining class. results demonstrate neurogenesis hippocampus-inspired intrinsic replay enables learning classes minimal impairment original representations challenge conventional approaches continue train existing network novel data without structural changes. ﬁeld machine learning transfer learning addresses problem utilizing existing trained system dataset containing objects different kind. past years researchers examined different ways transferring classiﬁcation capability established networks tasks. recent approaches taken horizontal approach transferring layers rather ﬁnely grained vertically oriented approach dynamically creating eliminating individual nodes layer. neurogenesis proposed enable acquisition novel information minimizing potential disruption previously stored information indeed neurogenesis similar processes shown beneﬁt number studies using shallow neural networks although studies typically focused conventional transfer learning opposed continuous adaptation learning considered here. adaptive architecture calandra shows applied data unseen trained network approach hinges incrementally re-training deep belief networks whenever concept drift emerges monitored stream data operates within constant memory bounds. utilize generative capability dbns provide training samples previously learned classes. class conditional sampling trained networks biological inspiration well historical artiﬁcial neural network implementations yosinski evaluated transfer capability high-level layer reuse speciﬁc dnns transferring learning increased recipient network performance though closer target task base task better transfer. transferring speciﬁc layers could actually cause performance degradation however. likewise kandaswamy used layer transfer means transfer capability convolutional neural networks stacked denoising transferring capability resulted reduction overall computation time lower classiﬁcation errors. papers ﬁxed-sized dnns except additional output nodes classes demonstrate features early layers general features later layers thus transferable classes. neurogenesis deep learning algorithm neurogenesis brain provides motivation creating dnns adapt changing environments. here introduce concept neurogenesis deep learning process incorporating nodes level existing enable network adapt environment changes. consider speciﬁc case adding nodes pre-train stacked deep although approach extend types dnns well. type neural network designed encode data decoded produce reconstructions minimal error. goal many algorithms learn ﬁlters feature detectors complexity specialization features increases deeper network layers. although successive layers feature detectors could require exponential expansion nodes guarantee information preserved progresses sophisticated representations practice deep typically much manageable number features using training process select features best describe training data. however guarantee representations deeper layers sufﬁcient losslessly encode novel information representative original training set. latter case believe useful previously suggested biological neurogenesis addresses similar coding challenge brain ﬁrst step algorithm occurs data points fail appropriately reconstructed trained network. reconstruction error computed level stacked determine level’s representational capacity considered insufﬁcient given application. parameterized weights biases activation function described input output encode layers followed decode layers. data sample’s high assumption level examination contain rich enough nodes accurately reconstruct sample. therefore stands reason sufﬁciently high warrants addition feature detector second step algorithm adding training nodes occurs critical number input samples fail achieve adequate representation level network. node also added previous level added nodes. process require labels relying entirely quality sample’s representation computed reconstruction. high nodes added level userspeciﬁed maximum number nodes. nodes fig. illustration processing mnist digits faithfully reconstruct originally trained digit fails reconstructing novel digit nodes added levels enables reconstruct level arrows show inputs reconstructed various depths. trained using nodes level reconstruction outliers. words training nodes reconstructions errors gradients weight updates calculated function uses entire nodes current level within single hidden layer order disturb existing feature detectors encoder weights connected nodes updated level consideration. decoder weights connected existing feature detectors allowed change slightly learning rate divided ﬁnal step algorithm intended stabilize network’s previous representations presence newly added nodes. involves training nodes level data replayed samples previously seen classes network trained. samples classes original data longer exists created using encoding reconstruction capability current network process call intrinsic replay process analogous observed dynamics within brain’s hippocampus memory consolidation appears neural regions hippocampus replay neuronal sequences originally experienced learned behaviors explorations effort strengthen stabilize newly acquired information alongside previously encoded information. method involves storing class-conditional statistics layer encoding network cholesky decomposition requires operations dimension performed class trained network. high-level representations retrieved sampling normal distribution described statistics leveraging decoding network data points previously trained classes reconstructed. fig. illustration intrinsic replay process used ndl. original data presented trained network results high-level representations top-most layer encoder. average entries cholesky decomposition covariance matrix hidden layer stored class replayed values desired given class samples drawn randomly normal distribution deﬁned class’s stored statistics. then using ae’s reconstruction pathway digits stored class approximated. evaluated datasets mnist nist datasets. nist dataset downsampled original pixel images however otherwise normalize characters within classes variation scale location within frame much greater mnist data. mnist dataset deep pre-trained stacked layered manner subset dataset classes training without without conducted unseen data classes. initially trained digits statistically representative digits then learning incrementally performed remaining digits. used -layer inspired hinton’s network mnist reduced ------- since subset digits used initial training. experiment training samples class presented once. nist dataset trained digit classes alone learning performed incrementally letters order evaluate impact nist dataset without potentially complicating factor training data used replaying classes. initial used nist dataset also inspired hinton’s mnist network difference number highestlevel features. used instead high-level features input n-layer trained data classes d–du− class data vector per-level thresholds vector per-level maximum nodes allowed odes maximum number samples allowed regloball axoutliers learning rate plasticity odesn nodes odesn train outliers update encoder weights connected nodes update decoder weights stability train aestablet rain using update weights outliers du|regloball nout |outliers| odes odes odesn illustrate process mnist data ﬁrst trained deep encode subset mnist classes. then nodes added neurogenesis trained network needed encode remaining digit. initial size illustrative example determined follows. calandra’s work ---- classiﬁer trained initially digits presented digits training together samples generated examined subsets digits initial training figure illustrates digits appear contain complete digit features seen quality reconstructions compared training although limited training sets yield impaired reconstructions novel digits. chose focus initial training digits digits represent smallest features pair digits. then continuous learning simulated progressively expanding number encountered classes adding samples remaining digits sequence class time. calandra network shown overcapacity digits virtue subsequent ability learn digits. suspect overcapacity hinton’s network therefore start network roughly size assumption neurogenesis grow network sufﬁcient learn remaining digits individually presented training. thus size initial prior neurogenesis -------. accordingly trained -network using training samples stacked denoising training ready address drifting inputs ndl. classes digits presented following order notably procedure strictly concept drift transfer learning rather designed examine capability network learn novel inputs maintaining stability previous information begins presenting samples class level identifying ‘outlier’ samples user-speciﬁed threshold. then nodes added level entire level pre-trained shl-ae. initially weights connected newly added nodes allowed updated full learning rate. encoder weights connected nodes allowed change decoder weights nodes allowed change learning rate divided step relates notion plasticity biological neurogenesis. brieﬂy training nodes stabilization step takes place entire level trained shl-ae using training samples classes seen network calculating samples class additional nodes added either enough samples falls threshold user-speciﬁed maximum number nodes reached current level. neurogenesis complete level weights connecting next level trained using samples classes. process repeats succeeding level using outputs previous encoding layer. capable reconstructing images class addition current previous classes results experiments mnist data showed established network trained digits enlarged neurogenesis represent digits guided level stacked compared network created three control networks control size enlarged without network trained ﬁrst subset digits retrained without intrinsic replay samples single digit time control continuous learning original network using using intrinsic replay control size enlarged ndl+ir network trained ﬁrst subset digits retrained samples single digit time using intrinsic replay generate samples previously trained classes throughout experiment figure shows network built upon ndl+ir slightly outperforms learning ﬁxed network notably ndl+ir outperforms straight learning reconstruction across digits ability represent data well preserving ability represent previously trained digits latter point important getting trained network learn information particularly challenging getting preserve information quite difﬁcult. note ﬁnal size unknown prior neurogenesis. network size increased based network exposed information possible value using method determine effective size. original size -------. figure shows grows classes presented neurogenesis gaining representational capacity classes learned. ‘cl+ir’ control network initially identical size neurogenesis network ‘ndl+ir’ initially trained digits learned represent remaining mnist digits time order presented neurogenesis network size ﬁxed. figure shows reconstructed images class learned ‘cl+ir’ figure shows comparable images ‘ndl+ir’ network trained accommodate mnist digits. trained digits without provides marginal improvement reconstruction ability learning digits; without likewise fails improve reconstruction though training makes reconstruction partial networks useful; improves overall reconstruction previously trained digits; improves along improved partial network reconstructions; full network reconstructions networks progressive training digits; neurogenesis contribution network size ndl+ir networks. fig. reconstructions digits pre-trained networks learning progressive classes. networks using conventional learning able acquire digits show ability maintain representations recently trained digits networks using able acquire digits show superior reconstructions previously encountered digits even digits trained earlier row) networks mis-reconstructed digits unseen classes digits appear belong previously trained class expected. notably ‘cl+ir’ reconstructions digits previously seen classes often mis-reconstructed recently seen classes. contrast ‘ndl+ir’ networks stable representations previously encoded data minimal disruption past classes information acquired. suggests adding neurons network exposed novel information advantageous maintaining stability dnn’s previous representations. applying nist dataset presents challenges evaluating neurogenesis performance number classes. figure shows effect learning class comparing initial class network trained digits learning letters ﬁnal classes learned. line segment downward slope indicates ﬁnal less initial clear observation learning classes intrinsic replay results smaller learning without neurogenesis classes. addition ﬁnal ndl+ir lower initial even classes used train original implies ultimate built neurogenesis richer feature detectors resulting better representation classes. another observation that general initial cl+ir network lower initial ndl+ir network. reason original ndl+ir network smaller ﬁxed cl+ir network. figure shows improvement class representation beginning ndl+ir figures show progression time growing ﬁnal network. neurons added earlier neurogenesis process later. novel classes presented feature learned representation capability improves classes. eventually need additional neurons diminishes. figure reveals particularly lacking feature detectors necessary good representation class levels. figure clear class also lacking feature detectors time presented learning need neurogenesis previous classes. fig. neurogenesis contribution networks trained digits only. experiments conducted newly presented classes randomly ordered. plot shows average number neurons added progessively class standard deviation error bars. value model continuously adapt changing data challenging quantify. here notionally quantify value machine learning algorithm given time considered tradeoff beneﬁt computation provides user costs algorithm generation model itself associated run-time costs computation. typically consists time physical energy space required computation performed. machine learning applications must consider lifetime algorithm appropriate amortize model’s build costs. algorithm design desirable minimize cost terms; however dominant cost differ depending extent real-world data changes. consider neurons order synapses. example cost building model scale performing operations training samples training well-regularized appropriately model. result dominate algorithm’s cost unless lifetime model offset polynomial difference description illustrates need extend model’s lifetime inexpensive manner minimizes data required adapt model future use. presented adaptive algorithm using concept neurogenesis extend learning established presented data classes. algorithm adds neurons level data samples accurately represented network determined high focus paper proof concept continuous learning dnns adapt changing application domains. several elements algorithm sought optimize deserve consideration. instance optimal number samples unknown affect computational cost associated use. elements need considered better ways establishing using thresholds developing method determine number outliers allow neurogenesis. considered network growing size neurogenesis adaptation obtainable larger network ﬁxed size restricting learning rate subset neurons needed later time. evaluated algorithm datasets gray-scale objects blank backgrounds look forward application additional datasets including natural color imagery. ultimately anticipate several signiﬁcant advantages neurogenesis-like method adapting existing networks incorporate data particularly given suitable capabilities. ﬁrst relates costs application domains. ability adapt information extend model’s useful lifetime real-world situations possibly substantial amounts. extending model’s lifetime increases duration amortize costs developing model case build cost often vastly outpaces runtime operational costs trained feed-forward network. result continuous adaptation potentially make cost effective domains signiﬁcant concept drift. admittedly method describe added processing cost neurogenesis process required intrinsic replay; however cost likely amount constant factor increase processing costs still signiﬁcantly lower costs associated repeatedly retraining original training data. valuable particularly cases bulk storage data permitted costs restrictions. much community focused cases extensive unlabeled training data technique provide solutions training data time limited data expected arrive continuously. furthermore considered stark change data landscape network exposed exclusively novel classes. realworld applications novel information encountered gradually. slower drift would likely require neurogenesis less often would equally useful needed. finally escaped algorithm present emulating adult neurogenesis within cortical-like circuit whereas adult mammals substantial neurogenesis appear sensory cortices networks similar juvenile developmental visual systems network exposed limited extent information eventually encounter. presumably takes many nodes layer trains much larger broader data requirement neurogenesis diminish. situation predict levels neurogenesis eventually diminish zero early network ability represent broad level features prove sufﬁcient even novel data encountered whereas neurogenesis always remain useful deepest network layers comparable medial temporal lobe hippocampus areas cortex. indeed work illustrates incorporation neural developmental adult plasticity mechanisms staggering network development layer conventional dnns likely continue offer considerable beneﬁts. acknowledgments work supported sandia national laboratories’ laboratory directed research development program hardware acceleration adaptive neural algorithms grand challenge. sandia multi-mission laboratory managed operated sandia corporation wholly owned subsidiary lockheed martin corporation u.s. department energy’s national nuclear security administration contract deac-al. cire¸san meier schmidhuber transfer learning latin chinese characters deep neural networks neural networks international joint conference ieee appleby kempermann wiskott role additive neurogenesis synaptic plasticity hippocampal memory model grid-cell like input plos comput biol vol. chambers conroy network modeling adult neurogenesis shifting rates neuronal turnover optimally gears network learning according novelty gradient journal cognitive neuroscience vol. chambers potenza hoffman miranker simulated apoptosis/neurogenesis regulates learning memory capabilities adaptive neural networks neuropsychopharmacology vol. wiskott rasch kempermann functional hypothesis adult hippocampal neurogenesis avoidance catastrophic interference dentate gyrus hippocampus vol. calandra raiko deisenroth pouzols learning deep belief networks non-stationary streams international conference artiﬁcial neural networks. springer carr jadhav frank hippocampal replay awake state potential substrate memory consolidation retrieval nature neuroscience vol. kandaswamy silva alexandre santos improving deep neural network performance reusing features trained transductive transference international conference artiﬁcial neural networks. springer", "year": 2016}