{"title": "Hidden Tree Markov Networks: Deep and Wide Learning for Structured Data", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "The paper introduces the Hidden Tree Markov Network (HTN), a neuro-probabilistic hybrid fusing the representation power of generative models for trees with the incremental and discriminative learning capabilities of neural networks. We put forward a modular architecture in which multiple generative models of limited complexity are trained to learn structural feature detectors whose outputs are then combined and integrated by neural layers at a later stage. In this respect, the model is both deep, thanks to the unfolding of the generative models on the input structures, as well as wide, given the potentially large number of generative modules that can be trained in parallel. Experimental results show that the proposed approach can outperform state-of-the-art syntactic kernels as well as generative kernels built on the same probabilistic model as the HTN.", "text": "ieee. personal material permitted. permission ieee must obtained uses current future media including reprinting/republishing material advertising promotional purposes creating collective works resale redistribution servers lists reuse copyrighted component work works. appear proceedings ieee symposium series computational intelligence abstract—the paper introduces hidden tree markov network neuro-probabilistic hybrid fusing representation power generative models trees incremental discriminative learning capabilities neural networks. forward modular architecture multiple generative models limited complexity trained learn structural feature detectors whose outputs combined integrated neural layers later stage. respect model deep thanks unfolding generative models input structures well wide given potentially large number generative modules trained parallel. experimental results show proposed approach outperform state-of-the-art syntactic kernels well generative kernels built probabilistic model htn. deep learning revolution strongly rooted ability efﬁciently learning informative neural representations effectively inform predictor part model starting large scale complex often noisy data. produced breakthrough performances several ﬁelds computer science machine vision speech recognition natural language understanding. speciﬁc application ﬁelds often associated data specialized nature images machine vision sequences speech text processing. recently noted increasing attention deep learning community complex type data naturally describes hierarchical treestructured data. trees thought compound objects made atomic entities represented knowledge encoded node labels bound together hierarchical relationships represented tree edges. interpretation mind clearly note effective representation information associated node cannot determined considering node isolation. rather needs take consideration surrounding context represented nodes target vertex linked challenge that within scenario data samples trees varying structure size. hence learning model needs adaptive respect variations. deep learning approaches tree-structured data developed mostly variations long short term memory original cell architecture designed process sequences extended model different parsing directions well larger contextual dependencies fact deep lstm-based approaches basically implementing speciﬁc instance general approach tree structured data processing proposed late nineties inspired work relative density networks paper take completely different approach proposing neuro-probabilistic hybrid model allows learn effective encodings discriminative structural knowledge using generative tree models immersed neural architecture used perform ﬁnal predictive task model strongly rooted characterizing concepts deep learning modularity network profoundly layered architecture organizes information hierarchical form. extended wide sense allows concatenate parallel large number generative structural feature detectors. intended small complexity allowing many small easily trainable structural detectors place huge monolithic generative model generally quite harder train perform inference onto. show architectural choices need taken order make feature detectors tuned different structural properties experimentally assess effect discriminatively training generative models within neural architecture. results highlight approach outperform methods imbuing discriminative power generative models e.g. deﬁnition generative kernels modular structure network allows apply wide range performance training optimization tricks might prove effective dealing large scale problem. instance single generative models pre-trained unsupervised fashion embedded neural architecture gradient descent used reﬁne parameters based supervised information. hand modularity network allows straightforwardly parallelize computations generative models instance resorting minibatching remainder paper organized follows section reviews background adaptive processing treestructured data placing particular focus formalization generative models underlying proposed approach. section introduces proposed model name hidden tree markov network section provides experimental assessment comparing state-of-the-art kernels trees section concludes paper. paper deals models learn predictive tasks labelled datasets whose constituents tree-structured samples whose size connectivity vary among data points. purpose paper consider classiﬁcations tasks want associate input tree class label. formalize notation used throughout paper consider dataset labeled rooted tree connected acyclic graph consisting nodes single vertex denoted root nodes connected exactly simple path. index used denote n-th tree dataset structures omitted notational simplicity context clear. term used denote generic node whose direct ancestor called parent denoted deﬁnition node maximum parent variable number direct descendants l-th child node denoted chl. note assume trees ﬁnite maximum outdegree i.e. maximum number children node. node without children called leaf leaves n-th tree denoted finally vertex tree associated label d-dimensional vector. literature adaptive processing tree-structures quite rich comprising kernel-based generative neural approaches. foundational work providing reference framework processing tree structured data proposed unifying approach deal data neural probabilistic perspective. recently deep learning wave re-discovered recursive neural networks processing trees. particular proposed recursive neural network basically implemented specialized version general framework within parse tree classiﬁcation task emotion recognition. work several followed typically proposing recursive variants long short term memory e.g. bottom-up lstm top-down lstm alternative approach learning structured data forward using kernel functions deﬁne similarity measures trees upon support vector machines built solve classiﬁcation/regression problems. several kernel functions proposed past-years deal structured data early survey available many syntactic type class tree kernels degree matching trees determined counting number common substructures among trees various approaches literature basically differentiate identify composing substructures weigh structural matches. subset tree kernel instance counts number matching proper subtrees subtree kernel restricts matching complete subtrees computational efﬁciency. elastic tree kernels hand allow matching nodes different labels matching substructures built combining subtrees descendants. partial tree kernel relaxes allow partial productions parse-tree grammar basically allowing perform partial matching subtrees cost increased computational complexity. recent comparative analysis syntactic tree kernels found another class adaptive approaches trees based generative models based hidden markov state formulation brieﬂy reviewed next section basic building block upon build solution. particular paper proposes hybrid respect approaches literature. fuse architecture ability generative models extracting descriptive probability distributions tree spaces ﬂexibility efﬁciency incremental learning ability neural approaches. further experimental analysis show hybrid reaches higher classiﬁcation performances state-of-the-art kernels syntactical built generative models used approach. hidden tree markov models allow modeling probability distributions spaces trees generalizing hidden markov model approach sequential domain learning hidden generative process labeled trees regulated hidden state variables modeling structural context node determining emission label. literature refer mainly types generative processes associated top-down bottom-up parsing directions. approaches characterized different context propagation strategies that practice induce different conditional independence relationships leading different representational capabilities. following focus version htmm shown conditional dependence relationships introduced context allow general capture discriminant details tree structures respect context learning parameters bu-htmm addressed expectation-maximization problem applied logarithm likelihood obtained marginalizing unknown hidden state assignment summing trees dataset. resulting likelihood follows details learning algorithm found summary batch learning algorithm based twostage iterative procedure allows efﬁciently compute solution log-likelihood maximization problem. e-step estimates posterior indicator variables introduced completed log-likelihood while m-step exploits posteriors update model parameters posterior estimation critical part algorithm efﬁciently computed message passing upwards downwards structure nodes’ dependency graph exact parameterization generative model depends number factors including stationariety assumption taken form state emission distribution. however often multinomial type hence model parameters quite straightforwardly members multinomial tables resulting constrained maximization typical problem wants address htmm tree classiﬁcation. generative setting task typically addressed training different probabilistic model class then test tree classspeciﬁc models queried tree assigned class whose corresponding model highest likelihood generated unfortunately generative approach typically yields poor classiﬁcation performances learning model supplied information might help discriminating different classes approaches exists problem introducing class information probabilistic model using perform class inference however even models cannot reach classiﬁcation performance discriminative approaches cannot easily generalized regression problems. hand structural knowledge inferred htmm models used inform discriminative methods e.g. building kernels exploit underlying generative model. instance case fisher tree kernel fig. unfolding bu-htmm structure example tree generating directed graphical model comprising visible label nodes well hidden markov state nodes ﬁgure shows example emission distribution node transition distribution node subtree. bu-htmm implements generative process composes child subtrees node tree recursive fashion. practice bu-htmm recursive model whose associated graphical model different tree obtained unfolding structure target tree described figure shows example unfolding. realize this rely hidden state variables associated state transition dynamics follows direction generative process. speciﬁcally observed tree modeled hidden state variables following indexing observed nodes assuming values discrete hidden states direction generative process modeled state transition probability assuming node conditionally independent rest tree joint hidden state direct descendants qchl observed. complete speciﬁcation model figure assume label node completely speciﬁed hidden state emission distribution problem formulation becomes computationally impractical trees binary since size joint conditional transition distribution order node outdegree. addressed introducing scalable switching parent approximation factorizes mixture pairwise child-parent transitions. resulting buhtmm joint distribution recall denotes leaves tree prior state distribution summation term corresponds factorization using switching parent latent variable independent following section show different approach problem making generative tree models discriminative plugging hierarchical neural network ultimately realizing neuro-probabilistic hybrid model tree-structured data. differently kernelbased solution above approach separate phase generative model htmm) learning discriminative predictor instead generative models trained discriminatively altogether neurons performing network prediction. predictive accuracy generative models classiﬁcation tasks limited generative training style algorithm basically amounts learning class-conditional distributions class positive examples class. lack information negative class examples allow models develop sufﬁciently discriminative representation class boundaries resulting poor classiﬁcation performances. several approaches attempted addressing aspect within context structured data processing. input-driven generative models instance provide introducing discriminative information training single probabilistic model predict output class given input structure parameterizes model distributions. context sequential data alphanet approach proposed instead attach layer softmax neurons receiving input likelihood class conditional hmms. showed train hybrid model cross-entropy maximization reaching superior classiﬁcation performances respect purely generative training. later introduced relative density combining hmms additional layers hidden comparator neurons trainable backpropagation. motivated approaches above introduce hidden tree markov network tree structured data whose architecture sketched fig. core sits generative model trees buhtmm described section ii-b available instances represented round boxes bottom fig. bu-htmms unfold structure current input tree produce result log-likelihood value module representing probability current sample generated m-th bu-htmm. values interpreted outputs modules combined pairwise successive layer hidden contrastive neurons. pair bu-htmm activation function hyperbolic tangent. contrastive neuron acts comparator different generative models whose likelihood weighted ﬁxed synaptic weight equal either depending module. models confronted pairwise opposite signs expect develop different responses structures training time. words bu-htmm seen detector structural properties trees. comparator neurons expect different bu-htmm models become tuned different structural features data. contrastive neurons turn interpreted detectors abstract structural feature following deep learning idea structuring knowledge hierarchical fashion level information input abstract features close network output. generative models ﬁxed weight matrix whose element output layer combined fully connected layer softmax neurons adaptive weight matrix softmax output computed expected following switching parent probability l-th child. terms zuijl latent indicator variables node state joint observation node state l-th child state respectively details). free parameters m-th generative model. given formulation bu-htmm proceed derivation gradients needed minimize respect appropriate application chain rule calculus. sparing details tedious derivations obtain following gradients labelled sample using indicator variable zero otherwise. derivative m-th model log-likelihood equal expected value taken posterior distribution indicator variables zuijl yields following parameter-speciﬁc gradients completing model derivation model neuro-probabilistic hybrid deep given comprises level recursive models whose depth grows input structure time expect wide since allows large number structural feature detectors small complexity place fewer large htmms generally harder train. ground truth binary variable class zero otherwise. introduced term explicitly denote dependencies model parameters crossentropy minimization obtained taking derivatives respect parameters include adaptive output weights well parameters distributions bu-htmm models. ﬁrst unconstrained optimization problem second deals optimization sum-to-one constraints nature bu-htmm parameters essentially elements multinomial distributions although exist approaches solve constrained optimization problem ﬁeld hidden markov models survey) generally effective introduce reparametrization markov model learning addressed unconstrained loss minimization problem. rewrite bu-htmm likelihood highlight distribution parameters follows computed using standard e-step pass buhtmm referred upwards-downwards algorithm details. last step concludes derivation gradients model whose parameters updated using gradient-based algorithm stochastic gradient descent minibatch etc. following section show experimental assessment model based update i.e. generic parameter time sample tree provide experimental assessment model publicly available benchmarks tree-data classiﬁcation spanning different application areas including structures different properties table summarizes main characteristics datasets used experimental assessment. ﬁrst benchmark concerns classiﬁcation formatted documents corpus used inex competition dataset characterized large sample size large number unbalanced classes; trees generally shallow large outdegree. standard splits training test sets available dataset roughly half total samples used training. second benchmarks concerns classiﬁcation molecular structure glycans represented rooted trees nodes stand mono-saccharides edges stand sugar bonds. consider datasets kegg/glycan database referred leukemia cystic data benchmarks differs considerably inex task binary small number samples available; trees small small outdegree. third experiments deals parse trees representing english propositions dow-jones news articles associated semantic information. employ version propbank dataset introduced includes sample section propbank comprising training trees validation examples well test samples extracted section benchmark deﬁnes binary classiﬁcation problem unbalanced class distribution percentage positive examples roughly explored different conﬁgurations varying number hidden tree markov models well number hidden states given considerable differences sizing datasets structural properties trees considered different choices hyperparameters depending benchmark hand. particular inex propbank considered glycans task tested number tested hidden states determined following guidelines original paper bottom-up markov tree model model hyperparameters chosen model selection inex performed -fold cross validation training split propbank data used predeﬁned validation set. glycans tasks hand results obtained stratiﬁed -fold using available standard partitions. fold used different random initializations hidden markov models distributions. training model performed stochastic gradient descent described section exponentially decaying learning rate nesterov momentum model-selected conﬁguration dataset evaluated external test except glycans tasks results literature provided mean roc-auc -folds table summarizes results application datasets described previous section. here predictive performance model compared state-of-the-art generative tree kernels built hidden markov tree models used denoted table classiﬁers realized support vector classiﬁcation using publicly available libsvm software described cross-validation procedure using validation scheme used applied generative kernels select number hidden states value cost parameter csvm following values additionally table provides reference results popular syntactic tree kernels literature classification performance external test comparing tree kernels configuration selected cross-validation. performance inex task expressed classification accuracy propbank results highlight outperformed reference kernels task exception inex kernel still state performance runner-up model classiﬁcation accuracy hand performance signiﬁcantly higher propbank data increases score notable points second best model using many generative models small complexity compared generative models high complexity used tendency favour many smaller models complex htmm also evident glycans task best performing model. seems able challenge state-of-the-art syntactic kernels exploiting ability generative tree models capturing structural representations data effective approach. better assess effectiveness respect exploiting underlying generative models performed additional experiment involving inex here focusing tradeoff classiﬁcation accuracy model complexity deﬁned terms number number hidden states well terms number generative models employed. choice inex motivated non-trivial number classes i.e. generative setting underlying means kernel using htmm deﬁne feature space. instead varied number generative models figure shows result analysis showing clear advantage terms classiﬁcation performance conﬁgurations test. order reach competitive performances needs larger hidden state space using models. hand able cope lesser models c.f. performance models used well generative models reduced state spaces. particular achieves competitive performances little hidden states generative model. seems conﬁrm initial intuition training generative models part deep neural architecture obtain discriminatively-tuned structural feature detectors training generative models isolation fusing contribution classiﬁcation fig. plot accuracy-complexity tradeoff inex dataset compared using underlying generative model number generative models ﬁxed class number curves denote results introduced deep learning approach adaptive processing tree data based modular architecture comprising several parallel generative models detectors structural features. detectors learned data incorporating discriminative information training hidden markov tree models backpropagation error generated neural output layer. contrastive layer sparsely connected neurons ﬁxed binary weights serves force differentiation feature detectors confronting predicted likelihoods pushing apart inverted signs contrastive weights. experimental analysis highlights ability model learning encoding structural information effective target predictive task. particular experimental results show outperforms generative kernels based underlying generative models. further outperform state-of-the-art syntactic tree kernels. lays basic architecture integrating generative models different representation capabilities model allowing learn richer discriminative structural feature encodings. particular shown integration information top-down bottom-up bacciu micheli sperduti compositional generative mapping tree-structured data part bottom-up probabilistic modeling trees ieee trans. neural netw. learning syst. vol. bacciu micheli sperduti generative multiset kernel structured data proc. international conference artiﬁcial neural networks ser. lncs vol. springer bacciu micheli alessandro adaptive tree kernel multinomial generative topographic mapping proc. ieee international joint conference neural networks ieee bridle training stochastic model recognition algorithms networks lead maximum mutual information estimation parameters advances neural information processing systems bacciu micheli alessandro integrating bi-directional contexts generative kernel trees proc. ieee international joint conference neural networks ieee htmm yield superior predictive performance approaches isolation. ongoing extension model considers comprising bottom-up top-down models. another ongoing development concerns application processing complex general classes graphs. fact possible modular structure perform different parallel visits graph using neural layers integrate information extracted visits generative models. finally currently studying modular architecture exploited parallelize parameters ﬁtting cards using minibatching work supported italian ministry education university research project list-it author gladly acknowledges dell-emc nvidia donating dell server nvidia gpus used perform experimental analysis. socher perelygin chuang manning potts recursive deep models semantic compositionality sentiment treebank proceedings conference empirical methods natural language processing. stroudsburg association computational linguistics october moschitti efﬁcient convolution kernels dependency constituent syntactic trees proc. european conference machine learning ser. ecml’. springer", "year": 2017}