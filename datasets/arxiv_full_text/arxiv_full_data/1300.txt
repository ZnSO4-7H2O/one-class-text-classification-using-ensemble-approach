{"title": "Towards the Limit of Network Quantization", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Network quantization is one of network compression techniques to reduce the redundancy of deep neural networks. It reduces the number of distinct network parameter values by quantization in order to save the storage for them. In this paper, we design network quantization schemes that minimize the performance loss due to quantization given a compression ratio constraint. We analyze the quantitative relation of quantization errors to the neural network loss function and identify that the Hessian-weighted distortion measure is locally the right objective function for the optimization of network quantization. As a result, Hessian-weighted k-means clustering is proposed for clustering network parameters to quantize. When optimal variable-length binary codes, e.g., Huffman codes, are employed for further compression, we derive that the network quantization problem can be related to the entropy-constrained scalar quantization (ECSQ) problem in information theory and consequently propose two solutions of ECSQ for network quantization, i.e., uniform quantization and an iterative solution similar to Lloyd's algorithm. Finally, using the simple uniform quantization followed by Huffman coding, we show from our experiments that the compression ratios of 51.25, 22.17 and 40.65 are achievable for LeNet, 32-layer ResNet and AlexNet, respectively.", "text": "network quantization network compression techniques reduce redundancy deep neural networks. reduces number distinct network parameter values quantization order save storage them. paper design network quantization schemes minimize performance loss quantization given compression ratio constraint. analyze quantitative relation quantization errors neural network loss function identify hessian-weighted distortion measure locally right objective function optimization network quantization. result hessian-weighted k-means clustering proposed clustering network parameters quantize. optimal variable-length binary codes e.g. huffman codes employed compression derive network quantization problem related entropy-constrained scalar quantization problem information theory consequently propose solutions ecsq network quantization i.e. uniform quantization iterative solution similar lloyd’s algorithm. finally using simple uniform quantization followed huffman coding show experiments compression ratios achievable lenet -layer resnet alexnet respectively. deep neural networks emerged state-of-the-art ﬁeld machine learning image classiﬁcation object detection speech recognition natural language processing machine translation substantial progress neural networks however comes high cost computations hardware resources resulting large number parameters. example krizhevsky came deep convolutional neural network consisting million parameters imagenet competition followed deeper neural networks even larger numbers parameters e.g. simonyan zisserman large sizes deep neural networks make difﬁcult deploy resource-limited devices e.g. mobile portable devices network compression great interest recent years reduce computational cost memory requirements deep neural networks. interest paper mainly curtailing size storage network parameters particular focus network size compression reducing number distinct network parameters quantization. besides network quantization network pruning studied network compression remove redundant parameters permanently neural networks matrix/tensor factorization low-rank approximation investigated well efﬁcient representations neural networks smaller number parameters consequently save computations moreover similar network quantization low-precision network implementation examined vanhoucke courbariaux anwar gupta extremes low-precision neural networks consisting binary ternary parameters found courbariaux rastegari note different types network compression techniques employed other. related work investigation paper found gong conventional quantization method using k-means clustering employed network quantization. conventional approach however proposed little consideration impact quantization errors neural network performance loss effort optimize quantization procedure given compression ratio constraint. paper reveal suboptimality conventional method newly design quantization schemes neural networks. particular formulate optimization problem minimize network performance loss quantization given compression ratio constraint efﬁcient quantization methods neural networks. derived performance loss quantization neural networks quantiﬁed approximately hessian-weighted distortion measure. then hessian-weighted k-means clustering proposed network quantization minimize performance loss. identiﬁed optimization problem network quantization provided compression ratio constraint reduced entropy-constrained scalar quantization problem optimal variable-length binary coding employed quantization. efﬁcient heuristic solutions ecsq proposed network quantization i.e. uniform quantization iterative solution similar lloyd’s algorithm. alternative hessian proposed utilize function second moment estimates gradients adam stochastic gradient descent optimizer used training. advantage using alternative computed training obtained training additional cost. shown proposed network quantization schemes applied quantizing network parameters layers together once rather layer-by-layer network quantization gong follows investigation hessian-weighting handle different impact quantization errors properly within layers also across layers. moreover quantizing network parameters layers together even avoid layer-by-layer compression rate optimization. rest paper organized follows. section deﬁne network quantization problem review conventional quantization method using k-means clustering. section discusses hessian-weighted network quantization. entropy-constrained network quantization schemes follow section finally experiment results conclusion found section section respectively. consider neural network already trained pruned employed ﬁne-tuned quantization. network pruning employed parameters network subject quantization. pruned networks focus quantization unpruned parameters. goal network quantization quantize network parameters order reduce size storage minimizing performance degradation quantization. network quantization network parameters grouped clusters. parameters cluster share quantized value representative value cluster belong quantization lossless binary coding follows encode quantized parameters binary codewords store instead actual parameter values. either ﬁxed-length binary coding variable-length binary coding e.g. huffman coding employed end. suppose total parameters neural network. quantization parameter assumed bits. quantization partition network parameters clusters. network parameters cluster number bits codeword assigned network parameters cluster lookup table decode quantized observe compression ratio depends number clusters also sizes clusters lengths binary codewords assigned them particular variable-length code used encoding quantized values. ﬁxed-length codes however codewords length i.e. ⌈log thus compression ratio reduced function number clusters i.e. assuming given. provided network parameters {wi}n quantize k-means clustering partitions disjoint sets denoted minimizing mean square quantization error follows first although k-means clustering minimizes msqe imply k-means clustering minimizes performance loss quantization well neural networks. k-means clustering treats quantization errors network parameters equal importance. however quantization errors network parameters degrade performance signiﬁcantly others. thus minimizing loss quantization neural networks needs take dissimilarity account. second k-means clustering consider compression ratio constraint. simply minimizes distortion measure given number clusters i.e. clusters. however suboptimal variable-length coding follows since compression ratio depends number clusters also sizes clusters assigned codeword lengths them determined binary coding scheme employed after clustering. therefore optimization network quantization given compression ratio constraint need take impact binary coding account i.e. need solve quantization problem actual compression ratio constraint imposed speciﬁc binary coding scheme employed clustering. section analyze impact quantization errors neural network loss function derive hessian-weighted distortion measure relevant objective function network quantization order minimize quantization loss locally. moreover analysis propose hessian-weighted k-means clustering network quantization minimize performance loss quantization neural networks. consider general non-linear neural network yields output input vector consisting trainable network parameters network; total number trainable parameters network. loss function loss deﬁned objective function minimize average expected output input cross entropy mean square error typical examples loss function. given training data xtrain optimize network parameters solving following problem e.g. approximately using stochastic gradient descent method mini-batches square matrix consisting second-order partial derivatives called hessian matrix hessian. assume loss function reached local minima training. local minima gradients zero i.e. thus ﬁrst term right-hand side neglected third term right-hand side also ignored assumption average loss function approximately quadratic local minimum finally simplicity approximate hessian matrix diagonal matrix setting off-diagonal terms zero. then follows local minimum diagonal elements hessian i.e. hii’s non-negative thus summation always additive implying average loss function either increases stays same. therefore performance degradation quantization neural network measured approximately hessian-weighted distortion shown discussion hessian-weighted distortion measure found appendix call hessian-weighted k-means clustering. observe give larger penalty network parameter deﬁning distortion measure clustering second-order partial derivative larger order avoid large deviation original value since impact loss function quantization expected larger parameter. hessian-weighted k-means clustering locally optimal minimizing quantization loss ﬁxed-length binary coding follows compression ratio solely depends number clusters shown section similar conventional k-means clustering solving optimization easy lloyd’s algorithm still applicable efﬁcient heuristic solution problem hessian-weighted means used cluster centers instead non-weighted regular means. recall interested diagonal elements hessian. efﬁcient computing diagonal hessian presented becker based back propagation method similar back propagation algorithm used computing ﬁrst-order partial derivatives computing diagonal hessian order complexity computing gradients. hessian computation network quantization performed completing network training. data used compute hessian either reuse training data data e.g. validation data set. observed experiments even using small subset training validation data sufﬁcient yield good approximation hessian network quantization. although efﬁcient obtain diagonal hessian discussed previous subsection hessian computation free. order avoid additional hessian computation propose alternative metric instead hessian. particular consider neural networks trained adam optimizer propose function second moment estimates gradients alternative hessian. adam algorithm computes adaptive learning rates individual network parameters ﬁrst second moment estimates gradients. compare adam method newton’s optimization method using hessian notice second moment estimates gradients adam method like hessian newton’s method. observation leads function second moment estimates gradients alternative hessian. advantage using second moment estimates adam method computed training obtain training additional cost. makes hessian-weighting feasible deep neural networks millions parameters. note similar quantities found used optimization methods using adaptive learning rates e.g. adagrad adadelta rmsprop propose quantizing network parameters layers neural network together taking hessian-weight account. layer-by-layer quantization examined previous work however e.g. larger number bits assigned convolutional layers fully-connected layers implies heuristically treat convolutional layers importantly. follows fact impact quantization errors performance varies signiﬁcantly across layers; layers e.g. convolutional layers important others. concern exactly address hessian-weighting. hessian-weighting properly handles different impact quantization errors within layers also across layers thus employed quantizing layers network together. impact quantization errors vary substantially across layers within layers. thus hessian-weighting show beneﬁt deeper neural networks. note hessianweighting still provide gain even layer-by-layer quantization since address different impact quantization errors network parameters within layer well. recent neural networks getting deeper e.g. szegedy deep neural networks quantizing network parameters layers together even advantageous since avoid layer-by-layer compression rate optimization. optimizing compression ratios jointly across individual layers requires exponential time complexity respect number layers. total number possible combinations compression ratios individual layers increases exponentially number layers increases. section investigate solve network quantization problem constraint compression ratio. designing network quantization schemes want minimize performance loss also want maximize compression ratio. section explored quantify minimize loss quantization. section investigate take compression ratio account properly optimization network quantization. quantizing network parameters clustering lossless data compression variable-length binary coding followed compressing quantized values. optimal codes achieve minimum average codeword length given source. entropy theoretical limit average codeword length symbol achieve lossless data compression proved shannon known optimal codes achieve limit overhead less integer-length codewords allowed. optimal coding also called entropy coding. huffman coding entropy coding schemes commonly used source distribution provided estimated. follows optimization problem complex solve arbitrary variablelength binary code since average codeword length arbitrary. however identify simpliﬁed optimal codes e.g. huffman codes assumed used. particular optimal coding closely achieves lower limit average source code length i.e. entropy approximately entropy quantized network parameters clustering given |ci|/n ratio number network parameters cluster number network parameters moreover assuming b/c. summary assuming optimal coding employed clustering approximately replace compression ratio constraint entropy constraint clustering output. network quantization problem translated quantization problem entropy constraint called entropy-constrained scalar quantization information theory. efﬁcient heuristic solutions ecsq proposed network quantization following subsections i.e. uniform quantization iterative solution similar lloyd’s algorithm k-means clustering. shown gish pierce uniform quantizer asymptotically optimal minimizing mean square quantization error random source reasonably smooth density function resolution becomes inﬁnite i.e. number clusters asymptotic result leads come simple efﬁcient network quantization scheme follows note hessian-weighted mean instead non-weighted mean computing cluster centers second step order take beneﬁt hessian-weighting. performance comparison uniform quantization non-weighted mean uniform quantization hessian-weighted mean found appendix although uniform quantization straightforward method never shown literature actually efﬁcient quantization schemes neural networks optimal variable-length coding e.g. huffman coding follows. note uniform quantization always good; inefﬁcient ﬁxed-length coding also ﬁrst shown paper. another scheme proposed solve ecsq problem network quantization iterative algorithm similar lloyd’s algorithm k-means clustering. although iterative solution complicated uniform quantization section ﬁnds local optimum given discrete source. iterative algorithm solve general ecsq problem provided chou derive similar iterative algorithm solve ecsq problem network quantization. main difference method chou minimize hessian-weighted distortion measure instead non-weighted regular distortion measure optimal quantization. detailed algorithm discussion found appendix section presents experiment results proposed network quantization schemes three exemplary convolutional neural networks lenet mnist data resnet cifar- data alexnet imagenet ilsvrc- data set. experiments summarized follows evaluate performance proposed network quantization methods withnetwork pruning. pruned model need store values unpruned parameters also respective indexes original model. index information compute index differences unpruned network parameters original model compress huffman coding finally evaluate performance network quantization schemes using hessian alternative used instead discussed section retrain considered neural networks adam optimizer obtain second moment estimates gradients training. then square roots second moment estimates instead hessian evaluate performance. second experiment network quantization schemes cifar- data pre-trained -layer resnet -layer resnet consists parameters total achieves accuracy. pruned model prune original network parameters ﬁne-tune rest. third evaluate network quantization schemes alexnet imagenet ilsvrc- data obtain pre-trained alexnet caffe model achieves top- accuracy. pruned model prune parameters ﬁne-tune rest. ﬁne-tuning adam optimizer used order avoid computation hessian utilizing alternative however pruned model recover original accuracy ﬁne-tuning adam method; top- accuracy recovered pruning ﬁne-tuning able better pruned model achieving original accuracy pruning retraining iteratively however used here. ﬁrst present quantization results without pruning -layer resnet figure accuracy -layer resnet plotted average codeword length network parameter quantization. ﬁxed-length coding employed proposed hessian-weighted k-means clustering method performs best expected. observe hessian-weighted k-means clustering yields better accuracy others even ﬁne-tuning. hand huffman coding employed uniform quantization iterative algorithm ecsq outperform hessian-weighted k-means clustering k-means clustering. however ecsq solutions underperform hessian-weighted k-means clustering even k-means clustering ﬁxed-length coding employed since optimized optimal variable-length coding. figure accuracy versus average codeword length network parameter network quantization huffman coding ﬁne-tuning lenet -layer resnet hessian computed samples square roots second moment estimates gradients used instead hessian alternative. figure shows performance hessian-weighted k-means clustering hessian computed small number samples observe even using hessian computed small number samples yields almost performance. also show performance hessian-weighted k-means clustering alternative hessian used instead hessian explained section particular square roots second moment estimates gradients used instead hessian using alternative provides similar performance using hessian. table summarize compression ratios achieve different network quantization methods pruned models. original network parameters -bit ﬂoat numbers. using simple uniform quantization followed huffman coding achieve compression ratios lenet -layer resnet alexnet respectively marginal performance loss. observe loss compressed alexnet mainly pruning. here also compare network quantization results ones note layer-bylayer quantization k-means clustering evaluated quantization schemes including k-means clustering employed quantize network parameters layers together paper investigates quantization problem network parameters deep neural networks. identify suboptimality conventional quantization method using k-means clustering newly design network quantization schemes minimize performance loss quantization given compression ratio constraint. particular analytically show hessian used measure importance network parameters propose minimize hessianweighted quantization errors average clustering network parameters quantize. hessianweighting beneﬁcial quantizing network parameters together since handle different impact quantization errors properly within layers also across layers. furthermore make connection network quantization problem entropyconstrained data compression problem information theory push compression ratio limit information theory provides. efﬁcient heuristic solutions presented i.e. uniform quantization iterative solution ecsq. experiment results show proposed network quantization schemes provide considerable gain conventional method using k-means clustering particular large deep neural networks. sajid anwar kyuyeon hwang wonyong sung. fixed point optimization deep convolutional neural networks object recognition. ieee international conference acoustics speech matthieu courbariaux yoshua bengio jean-pierre david. binaryconnect training deep neural networks binary weights propagations. advances neural information processing systems suyog gupta ankur agrawal kailash gopalakrishnan pritish narayanan. deep learning limited numerical precision. proceedings international conference machine learning song huizi william dally. deep compression compressing deep neural networks pruning trained quantization huffman coding. arxiv preprint arxiv. yong-deok eunhyeok park sungjoo taelim choi yang dongjun shin. compression deep convolutional neural networks fast power mobile applications. arxiv preprint arxiv. alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems vadim lebedev yaroslav ganin maksim rakhuba ivan oseledets victor lempitsky. speeding-up convolutional neural networks using ﬁne-tuned cp-decomposition. arxiv preprint arxiv. baoyuan wang hassan foroosh marshall tappen marianna pensky. sparse convolutional neural networks. proceedings ieee conference computer vision pattern recognition michael mozer paul smolensky. skeletonization technique trimming network relevance assessment. advances neural information processing systems mohammad rastegari vicente ordonez joseph redmon farhadi. xnor-net imagenet classiﬁcation using binary convolutional neural networks. arxiv preprint arxiv. olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein imagenet large scale visual recognition challenge. international journal computer vision tara sainath brian kingsbury vikas sindhwani ebru arisoy bhuvana ramabhadran. lowrank matrix factorization deep neural network training high-dimensional output targets. ieee international conference acoustics speech signal processing christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. proceedings ieee conference computer vision pattern recognition christian szegedy vincent vanhoucke sergey ioffe jonathon shlens zbigniew wojna. rethinking inception architecture computer vision. arxiv preprint arxiv. tijmen tieleman geoffrey hinton. lecture .-rmsprop divide gradient running average recent magnitude. coursera neural networks machine learning zichao yang marcin moczulski misha denil nando freitas alex smola song ziyu wang. deep fried convnets. proceedings ieee international conference computer vision diagonal approximation hessian simpliﬁes optimization problem well solution network quantization. simpliﬁcation comes performance loss. conjecture loss approximation small. reason contributions off-diagonal terms always additive summation small value. however diagonal terms non-negative therefore contributions always additive. verify conjecture paper since solving problem without diagonal approximation complex; even need compute whole hessian matrix also costly. observe relation hessian-weighted distortion measure quantization loss holds model objective function approximated quadratic function respect parameters quantize model. hence quantization methods proposed paper minimize hessian-weighted distortion measure speciﬁc neural networks generally applicable quantization parameters model whose objective function locally quadratic respect parameters approximately. finally consider interactions quantization retraining formulation section analyze expected loss quantization assuming retraining focus ﬁnding optimal network quantization schemes minimize performance loss. experiments however ﬁne-tune quantized values recover loss quantization improve performance. compare uniform quantization non-weighted mean uniform quantization hessianweighted mean figure shows uniform quantization hessian-weighted mean slightly outperforms uniform quantization non-weighted mean. figure accuracy versus average codeword length network parameter network quantization huffman coding ﬁne-tuning -layer resnet uniform quantization nonweighted mean uniform quantization hessian-weighted mean used. heuristic iterative algorithm solve method lagrange multipliers network quantization presented algorithm similar lloyd’s algorithm k-means clustering. difference partition network parameters assignment step. lloyd’s algorithm euclidean distance minimized. ecsq individual lagrangian cost function i.e. minimized instead includes quantization error expected codeword length entropy coding.", "year": 2016}