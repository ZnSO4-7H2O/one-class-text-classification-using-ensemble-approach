{"title": "Towards a Visual Turing Challenge", "tag": ["cs.AI", "cs.CL", "cs.CV", "cs.GL", "cs.LG"], "abstract": "As language and visual understanding by machines progresses rapidly, we are observing an increasing interest in holistic architectures that tightly interlink both modalities in a joint learning and inference process. This trend has allowed the community to progress towards more challenging and open tasks and refueled the hope at achieving the old AI dream of building machines that could pass a turing test in open domains. In order to steadily make progress towards this goal, we realize that quantifying performance becomes increasingly difficult. Therefore we ask how we can precisely define such challenges and how we can evaluate different algorithms on this open tasks? In this paper, we summarize and discuss such challenges as well as try to give answers where appropriate options are available in the literature. We exemplify some of the solutions on a recently presented dataset of question-answering task based on real-world indoor images that establishes a visual turing challenge. Finally, we argue despite the success of unique ground-truth annotation, we likely have to step away from carefully curated dataset and rather rely on 'social consensus' as the main driving force to create suitable benchmarks. Providing coverage in this inherently ambiguous output space is an emerging challenge that we face in order to make quantifiable progress in this area.", "text": "language visual understanding machines progresses rapidly observing increasing interest holistic architectures tightly interlink modalities joint learning inference process. trend allowed community progress towards challenging open tasks refueled hope achieving dream building machines could pass turing test open domains. order steadily make progress towards goal realize quantifying performance becomes increasingly difﬁcult. therefore precisely deﬁne challenges evaluate different algorithms open tasks? paper summarize discuss challenges well give answers appropriate options available literature. exemplify solutions recently presented dataset question-answering task based real-world indoor images establishes visual turing challenge. finally argue despite success unique ground-truth annotation likely step away carefully curated dataset rather rely ’social consensus’ main driving force create suitable benchmarks. providing coverage inherently ambiguous output space emerging challenge face order make quantiﬁable progress area. recently witness tremendous progress machine perception language understanding tasks. progress ﬁelds inspired researchers build holistic architectures challenging grounding natural language generation image/video image-to-sentence alignment recently presented question-answering problems paper argue visual turing test open domain task question-answering based real-world images resemblances famous turing test deviates attempts discuss challenges together tools benchmark different models task. typically measure progress ﬁeld quantifying performance different methods carefully crafted benchmarks. crowdsourcing combination machine learning approaches served well generate curated datasets unique ground truth scale complexity openness task grows quest crafting good benchmarks also becomes difﬁcult. first interpreting evaluating answer system becomes increasingly difﬁcult ideally would rely human judgement. want objective metrics evaluate automatically large scale. second establishing evaluation methodology assigns scores large output domain challenging system based ontologies limited coverage. third mimic human response deal inherent ambiguities human judgement stem issues like binding reference frames social conventions. instance reports question answering task real-world images even human answers inconsistent. obviously cannot problem humans rather argues inherent ambiguities task. competing methods validated true annotations truth task even human answers cannot completely agree other? instead seeking unique true answer suggest look ’social consensus’ takes multiple human answers different interpretations question account. enables incorporate ’agreement’ humans directly metric. although idea entirely believe sits core building open holistic challenges. exemplify ﬁndings daquar dataset demonstrating different challenges present dataset. hope exposition helpful towards building public visual turing challenge generate discussion agreeable evaluation procedure designing systems address open domain tasks. paper holistic architecture machine learning architecture designed work task fuses least modalities e.g. language vision. external world part task accessible holistic learner sensors either human world machine world models aspects human world. strive holistic open tasks grounding question-answering based images need deal large gamut challenges. section distilled discuss prominent ones order guide discussion. vision language scalability perception natural language understanding crucial parts holistic reasoning ground representation external world therefore serve common reference point machines humans. human conceptualization divides percepts different instances categories well spatio-temporal concepts. architectures mimicking reproducing space human concepts need capture diversity therefore scale thousands concepts concept ambiguity number categories grows semantic boundaries become fuzzy hence ambiguities inherently introduced instance sometimes overlook difference ’night stand’ ’cabinet’ ’armchair’ ’sofa’. therefore reasonable expect holistic architectures create alternative hypotheses external world inference. also relates gradual category membership human perception portrayed prototype theory attributes human concepts limited object categories also include attributes genders colors states often concepts cannot learned rather contextualized associated noun. e.g. white white elephant surly different white white snow. ambiguity reference resolution reliably answering questions challenging even humans. quality answer depends ambiguous latent notions reference frames intentions understood depending cultural bias context object-centric observer-centric even world-centric frames reference moreover even unclear ’with’ ’beneath’ ’over’ mean. seems least difﬁcult symbolically deﬁne terms predicates. holistic learning inference encompassing aforementioned aspects shown current research directions show promise adapting symbolic-based approaches vector-based approaches represent meaning. common sense knowledge turns questions solely answered access common sense knowledge high reliability. instance which object table used cutting? already narrows likely options signiﬁcantly correct answer probably knife scissors. questions like which hand teacher chin? require mixture vision language. understand question holistic learner needs ﬁrst detect person ﬁgure person teacher understand gender person detect chin understand ’left’ ’right’ side ﬁnally relates ’her’ ’teacher’. however different parts common sense knowledge used different modality. ’object cutting’ seeing affordance object cannot learnt solely images. hand things often co-occur together stand visual-based common sense knowledge. instance expect scissor inside small plastic never wall window. common sense knowledge help holistic machine learning architectures either fulﬁll task limit hypothesis space hence reduce computational complexity search problem. deﬁning benchmark dataset quantifying performance argue question answering based visual input task signiﬁcantly differ grounding problem unique advantages towards deﬁning challenge dataset. prominently latter ﬁnding mapping linguistic fragments physical world whereas question answering task end-to-end system necessarily want enforce constraints penalty internal representation holistic learner. sense grounding latent sub-task holistic learner needs solve evaluated finally argue establishing benchmark dataset based question answering task similar turing test tractable. learning grounding asks exhaustive symbolic-based annotations world question answering needs textual annotations aspects question refers daquar challenging large dataset question answering task based real-world images. images present real-world indoor scenes questions unconstrained natural language sentences. daquar’s language scope beyond nouns tuples typical recognition datasets other linguistically rich datasets either tackle images consider constrained domain suitable learning embedding/image-sentence retrieval language generation section discuss isolation different challenges reﬂected daquar. vision language machine world daquar represented images questions content. daquar contains different nouns question answers altogether extract nouns questions). consider nouns singular form questions still categories. current state-of-the-art semantic segmentation methods nyu-depth dataset discriminate object categories much fewer needed. daquar also contains parts speech colors spatial prepositions grounded moreover ambiguities naturally emerge grained categories exist daquar. instance ’night stand’ ’stool’ ’cabinet’ sometimes refer thing. also variation naming colors among annotations. questions rely heavily spatial concepts different frame reference. daquar includes various challenges related natural language understanding. semantic representation needs work large number predicates questions substantial length possible language errors questions. common sense knowledge daquar includes questions reliably answered using common sense knowledge. instance which object table used cutting? already provides strong non-visual cues cutting object. answers questions what desk front scissors? improved search space reasonable restricted. moreover annotators hypothesize missing parts object based common sense. believe common sense knowledge interesting venue explore daquar. question answering task question answering task also understanding hidden intentions questioner grounding sub-goal solve. authors treat grounding latent variable question answering task. others modeled pragmatic effects question answering task approaches never shown work less constrained environments. automation evaluating answers complex tasks answering questions requires quite deep understanding natural language involved concepts hidden intentions questioner. ideal impractical metric would manually judge every single answer every architecture individually. since infeasible seeking automatic approximation evaluate different holistic architectures scale. ambiguity complex tasks interested inherently ambiguous. ambiguities stem cultural bias different frame reference ﬁned grained categorization. implies multiple interpretations question possible hence many correct answers. coverage since multiple ways expressing concept automatic performance metric take equivalence class among answers consideration assigning similar scores members class. attempts alleviate issue deﬁning similarity scores lexical databases approaches however lacks coverage cannot assign similarity terms represented structure. wups scores exemplify aforementioned requirements illustrating wups score automatic metric quantiﬁes performance holistic architectures proposed metric motivated development ’soft’ generalization accuracy takes ambiguities different concepts account membership measure i-th question answers produced architecture human respectively represented bags words. authors proposed using similarity membership measure wups score. choice suffers aforementioned coverage problem whole metric takes human interpretation question account. future directions deﬁning metrics recent work provides several directions towards improving scores. deal ambiguities stem different readings question collecting human answers question propose based that generalizations wups score. ﬁrst call interpretation metric runs many human answers takes maximal score machine answer high similar least human answer. however many human answers also rank higher machine answers ’socially agreeable’ measuring agree human answers. done averaging multiple human answers. call second extension consensus metric. problem coverage potentially alleviated vector based representations answers. although case coverage issues less problematic understand concerns score dependent training data used build representation. hand abundance textual data recent improvements vector based approaches consider valid alternative similarities based ontologies. experimental scenarios many cases success challenging learning problems accelerated external data training e.g. object detection believe visual turing challenge consists sub-task prohibited auxiliary data understand holistic learners generalize limited challenging data established setup. hand limit artiﬁcial restrictions building next generation holistic learners. therefore open sub-tasks permissible another sources training stated including additional vision language resources synthetic data curated questions. goal contribution sparkle discussions benchmarking holistic architectures complex open tasks. identify particular challenges holistic tasks exhibit exemplify manifested recent question answering challenge judge competing architectures measure progress task suggest several directions improve existing metrics discuss different experimental scenarios. acknowledgement would like thank michael stark comments draft. palmer verbs semantics lexical selection. acl. miller g.a. wordnet lexical database english. cacm fellbaum wordnet. wiley online library pennington socher manning c.d. glove global vectors word representation. emnlp.", "year": 2014}