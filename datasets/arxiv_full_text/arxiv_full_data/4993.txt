{"title": "Intrinsically Motivated Goal Exploration Processes with Automatic  Curriculum Learning", "tag": ["cs.AI", "cs.LG"], "abstract": "Intrinsically motivated spontaneous exploration is a key enabler of autonomous lifelong learning in human children. It allows them to discover and acquire large repertoires of skills through self-generation, self-selection, self-ordering and self-experimentation of learning goals. We present the unsupervised multi-goal reinforcement learning formal framework as well as an algorithmic approach called intrinsically motivated goal exploration processes (IMGEP) to enable similar properties of autonomous learning in machines. The IMGEP algorithmic architecture relies on several principles: 1) self-generation of goals as parameterized reinforcement learning problems; 2) selection of goals based on intrinsic rewards; 3) exploration with parameterized time-bounded policies and fast incremental goal-parameterized policy search; 4) systematic reuse of information acquired when targeting a goal for improving other goals. We present a particularly efficient form of IMGEP that uses a modular representation of goal spaces as well as intrinsic rewards based on learning progress. We show how IMGEPs automatically generate a learning curriculum within an experimental setup where a real humanoid robot can explore multiple spaces of goals with several hundred continuous dimensions. While no particular target goal is provided to the system beforehand, this curriculum allows the discovery of skills of increasing complexity, that act as stepping stone for learning more complex skills (like nested tool use). We show that learning several spaces of diverse problems can be more efficient for learning complex skills than only trying to directly learn these complex skills. We illustrate the computational efficiency of IMGEPs as these robotic experiments use a simple memory-based low-level policy representations and search algorithm, enabling the whole system to learn online and incrementally on a Raspberry Pi 3.", "text": "intrinsically motivated spontaneous exploration enabler autonomous lifelong learning human children. allows discover acquire large repertoires skills self-generation self-selection self-ordering self-experimentation learning goals. present unsupervised multi-goal reinforcement learning formal framework well algorithmic approach called intrinsically motivated goal exploration processes enable similar properties autonomous learning machines. imgep algorithmic architecture relies several principles self-generation goals parameterized reinforcement learning problems; selection goals based intrinsic rewards; exploration parameterized time-bounded policies fast incremental goal-parameterized policy search; systematic reuse information acquired targeting goal improving goals. present particularly efﬁcient form imgep uses modular representation goal spaces well intrinsic rewards based learning progress. show imgeps automatically generate learning curriculum within experimental setup real humanoid robot explore multiple spaces goals several hundred continuous dimensions. particular target goal provided system beforehand curriculum allows discovery skills increasing complexity stepping stone learning complex skills show learning several spaces diverse problems efﬁcient learning complex skills trying directly learn complex skills. illustrate computational efﬁciency imgeps robotic experiments simple memory-based low-level policy representations search algorithm enabling whole system learn online incrementally raspberry keywords intrinsically motivated exploration; unsupervised multi-goal reinforcement learning; intrinsic motivation; curiosity-driven learning; automatic generation goals; curriculum learning; learning progress; robotics; modular representations extraordinary property natural intelligence children capacity lifelong autonomous learning. processes autonomous learning infants several properties fundamentally different many current machine learning systems. among capability spontaneously explore environments driven intrinsic motivation discover learn tasks problems imagine select crucially engineer externally imposing target goal learn hand providing curriculum learning providing ready-to-use database training examples. rather children self-select objectives within large potentially open-ended space goals imagine collect training data physically practicing goals. particular explore goals organized manner attributing values interestingness evolve time allowing self-deﬁne learning curriculum called developmental trajectory developmental sciences self-generated learning curriculum allows infants avoid spending much time goals either easy difﬁcult focusing goals right level complexity right time. within process learned goals/problems often stepping stones discovering solve goals increasing complexity. thus explicitly guided ﬁnal target goal mechanisms allow infants discover highly complex skills biped locomotion tool would extremely difﬁcult learn would focused goals start rewards goals typically rare deceptive. essential component organized spontaneous exploration intrinsic motivation system also called curiosity-driven exploration system last decade series computational robotic models intrinsically motivated exploration learning infants developed opening theoretical perspectives neuroscience psychology ideas allowed simulate predict important properties infant spontaneous exploration ranging vocal development object affordance tool learning ﬁrst idea infants might select experiments maximize intrinsic reward based empirical learning progress generates automatically developmental trajectories progressively complex tasks practiced learned used stepping stones complex skills. second idea beyond selecting actions states based predictive learning progress provide powerful organize intrinsically motivated exploration select goals i.e. self-generated reinforcement learning problems based measure control learning progress here intrinsic reward empirical improvement towards solving problems/goals happening lower-level policy search mechanisms generate physical actions. efﬁciency goal exploration processes leverages fact data collected targeting goal informative better solutions goals beyond neuroscience psychology believe models open perspectives artiﬁcial intelligence. particular algorithmic architectures intrinsically motivated goal exploration shown allow efﬁcient acquisition repertoires high-dimensional motor skills automated curriculum learning several robotics experiments includes example learning omnidirectional locomotion learning multiple ways manipulate single complex ﬂexible object article ﬁrst present formal framework called unsupervised multi-goal reinforcement learning well formalization intrinsically motivated goal exploration processes compact general previous models. then present experimentation implementations processes complex robotic setup multiple objects associated multiple spaces parameterized reinforcement learning problems robot learn certain objects tools manipulate objects. analyze discuss curriculum learning automated unsupervised multi-goal exploration process compare trajectory exploration learning spaces problems generated mechanisms hand-designed learning curriculum exploration targeting single space problems random motor exploration. let’s consider machine produce behaviours executing stochastic policies parameterized context characterizing current state environment generated stochastically distribution µenvpcq. consider policies πθpat` stt` attq stochastic black-boxes produce time-bounded behavioural trajectories denoted tstp at¨¨¨ stend atendu dynamics νenvpτ environment. assume terminate time time policy started timeout used meta-controller stop policy timeout reached features) characterizing behaviour machine environment execution policy terminates starting context descriptors outcome vector characterize properties behavioural trajectory finally assume machine capable sample goals space reinforcement learning problems parameterized goal/problem deﬁned goal-parameterized reward function speciﬁes reward received agent tries solve goal/problem context policy observes outcome main assumption given context policy observed outcome agent compute words given context policy outcome agent receives partial reward function rcθoτ compute reward experiment solving problem rcθoτppq oτq. another understand rewards given goal problem solve agent compute speciﬁc reward function giving reward/ﬁtness previous experiments solving particular problem rppc oτq. thus framework sampling goal/problem equivalent sampling reward function example illustrates generality form goals enabling express complex objectives simply depend observation state policies might depend several aspects entire behavioural trajectories. importantly framework assume goals/problems solvable. example robotics experiments below representation goal spaces provided engineer large parts goal spaces solvable machine know beforehand goals solvable relative difﬁculty. given spaces problems contexts policies well environment machine explore environment’s dynamics learn stochastic meta-policy solve well possible problems contexts order evaluate well machine learned solve diverse problems diverse contexts deﬁne test distribution problems deﬁne unsupervised multi-goal reinforcement learning problem problem learning meta-policy minimal loss minimal amount experiments term \"unsupervised\" tests problems unknown machine learning. actually engineer provides mathematical operators machine algorithmically generate parameterized reward functions measurements/outcomes behaviours. interesting avenue future work consider representation learning algorithms learning parameterization goal spaces. problems problem spaces deﬁnition test distribution interested evaluating competence learning agent particular problem space case sample problems spaces uniform particularities comparison concept reward function often used literature. ﬁrst particularity reward functions computed based outcome executing policy context thus consider whole behaviour machine environment execution rewards markovian considers perspective lower level state transitions associated execution i.e. atq. second particularity since computation reward internal machine computed time experiment problem particular problem agent trying solve. consequently machine experiments policy context observes stores results experiment memory later self-generates problems compute associated rewards rppc rppc rpipc information improve goals property essential enables direct reuse data collected learning solve problem solving later problems leveraged curriculum learning intrinsically motivated goal exploration processes autonomous robot playing ball. consider wheeled robot move around environment walls ball push. sensory system robot allows measure encode vector current position speed ball well distances walls several directions. robot produce behaviours executing policies encoded recurrent neural network weights parameterized network takes input current value sensor measurements outputs motor speed values roll-out starting context roll-out vector encoding current position speed robot ball robot executing movement time time tend records sequence motor commands observed sensor measurements ...pstend atendq. data robot computes outcome executing descriptors/features let’s consider example descriptors vector encoding translation ball time tend minimal distance walls trajectory energy used robot generating trajectory let’s consider space problems robot could sample from problem/goal consists trying behaviour context ball translates vector maximizing minimal distance wall minimizing energy spent relative weights components problem. thus space problems parameterized example problem problem staying away walls problem problem translating ball vector minimizing energy spent constraint distance walls. simple case consider outcomes would simply translation ball roll-out problems/goals would consist target translations case goal literally target conﬁguration environment roll-out parameterized rewards function rgpc directly implemented function distance target translation observed translation executing context rgpc e´}dg´d}. case would need formal space parameterized problems would isomorphic space outcomes. however considering parameterized space problems allows sample learn solve families general problems example above. cases unsupervised multi-goal reinforcement learning problem consists exploring environment collect learning data efﬁcient learning good inverse model space goals contexts. approach could taken perform exploration perform experiments sampling values given executing observing corresponding outcome. sampling random active using many active learning methods literature developed learn actively forward models world dynamics however argued example many real world situations large areas space values either producing meaningful outcomes redundant example parameters robot might produce movement robot even touch ball. consequence certain manifolds space useful learn produce certain diversity outcomes. reason another approach proven efﬁcient previous experimental studies formalized section intrinsically motivated goal exploration processes idea imgeps explore environment repeatedly sampling goal problem space current inverse model best predicted reach goal current context optimization algorithm constrained time budget improve best current solution. interestingly resulting experiment produce learning data improve current best solutions problems/goals enables transfer learning among goals/problems enables system robust sampling goals might difﬁcult learn initially simply impossible learn furthermore also provides framework goals sampled actively example sampling often goals system currently making learning progress. below present formalization imgeps show experiments generate learning curriculum goals progressively automatically sampled learned order increasing complexity. exploration problem versus inverse model learning problem several technical challenges need addressed learn meta-policy considers database exemplars available learner initially challenge devise learning algorithms detect leverage regularities database learn given test dataset generate policy parameters rppj maximal average problems test dataset encoding probabilistic inverse model spaces problems contexts space policies call problem learning database problem inverse model learning here evaluation performance inverse model learning algorithm average reward problems test dataset learning possibly considering speed convergence learning process. considers framework autonomous learning cannot assume database learning exemplars already available learner learner needs explore collect database meta-policy learned either concurrently incrementally later batch manner. article focus second problem believe fundamental autonomous learning architectures present family algorithmic processes exploration called intrinsically motivated goal exploration processes aims collecting learning exemplars properties diversity suited learn good inverse models space problems however algorithmic processes require inverse modeling step experiments implement simple algorithms incremental inverse model learning. furthermore evaluation exploration processes considers relation number roll-outs policies environment quality learned meta-policy implementing inverse model. efﬁcient exploration processes processes allow learn high-quality minimal number roll-outs environment. environments inﬁnity problems/goals unknown difﬁculty explored reward information problems sparse becomes challenging decide action policies explore next. next section presents goal exploration processes approach organize exploration contexts. present intrinsically motivated goal exploration processes algorithmic architecture aims address unsupervised multi-goal reinforcement learning problem. algorithmic architecture instantiated many particular algorithms contains slots several low-level policy learning algorithms used however algorithmic implementations share several general principles deﬁne concept intrinsically motivated goal exploration processes learning machine self-generate sample goals abstract parameterized reinforcement learning problems; goals typically deﬁned complex time-extended functions behavioural outcomes; modular case several goal spaces sample from; goal sampling made using intrinsic rewards; particularly efﬁcient intrinsic measure based estimating empirical learning progress towards selfgenerated goals learning loops running parallel exploration loop uses parameterized time-bounded policies fast incremental goal-parameterized policy search enabling fast efﬁcient search good solutions diverse sets problems; exploitation loop uses data collected exploration loop consolidate learning process possibly using slow batch learning algorithms; section deﬁne general algorithmic architecture solve unsupervised multi-goal reinforcement learning problem intrinsically motivated goal exploration processes architecture sense composed several modules working together multiple implementations possible. give several variants implementations architecture section require environment context space distribution µenvpcq require policy parameter space outcome space trajectory distribution νenvpτ require problem parameter space goal-parameterized reward function require initial knowledge initializemetapolicy π\u0001pθ initializeexplorationmetapolicy initializegoalpolicy launch asynchronously following loops exploration loop loop observe context choose goal based intrinsic rewards e.g. using infer policy parameters maximize expected rgpc using exploration/exploitation trade-off possibly computing problem-speciﬁc reward function giving reward/ﬁtness previous experiments solve current goal problem rgpcj execute roll-out observe trajectory tsttend attendu νenvpτ compute outcome trajectory goal-parameterized reward function rcθoτ computed reward/ﬁtness current experiment solving problem rcθoτppq compute current reward associated solving goal problem update achieved updategoalpolicy updateknowledge target meta-policy training loop updatemetapolicy achieved using online batch training several algorithmic ingredients used intrinsically motivated goal exploration processes. goal exploration learning agent iteratively samples parameterized problem space problems sets goal interaction goal exploration loop. loop learner ﬁrst infers best current parameters current meta-policy π\u0001pθ several sampling strategies used discussed below. then roll-out lower-level policy executed allowing observation behavioural trajectory measurement outcome data allows compute reward associated goal compute intrinsic reward evaluating interestingness choice based comparison past experiences; update goal exploration policy intrinsic reward; update meta-policy fast incremental learning algorithm depends particular implementation imgep update learning database then asynchronously learning database used learn target meta-policy online batch learning algorithm also depends particular implementation imgep. goal exploration goal problem chosen iteration. inﬁnite continuous high-dimensionality makes choice goal trivial question. indeed even goal-parameterized reward function rcθoτ gives information ﬁtness policy solve problems policy chosen problem solve mind thus rcθoτ give much information problems execution another policy chosen targeting another goal intrinsic rewards provide mean agent self-estimate expected interest exploring goals learning solve problems intrinsic reward signal associated chosen goal based heuristic outcome novelty progress reducing outcome prediction error progress competence solve problems based intrinsic rewards different goals different contexts choice goal context goal policy typically implemented contextual multi-armed bandit maximize future intrinsic rewards. here intrinsic rewards based measuring competence progress towards self-generated goals shown particularly efﬁcient learning repertoires high-dimensional robotics skills fig. schematic representation possible learning curves exploration preference agent intrinsic rewards based learning progress. figure schematic representation possible learning curves different problems associated exploration preference agent intrinsic rewards based learning progress. left plot schematic learning curves associated imaginary problems axis represent competence agent solve problem axis training time problem. blue orange green curves represent learnable problems agent’s competence increases training different rates saturates long training time. purple curve represents problem agent always competence progress. curve learning curve unlearnable problem stochastic outcomes e.g. trying predict outcome random dice. right exploration preference agent learning progress heuristic explore problems deﬁned learning curves. exploration preference based progress computed time derivative competence agent normalized progress problems beginning exploration agent makes progress problem blue prefers train preference shift towards problem orange make less progress problem blue progressively shift problem green. agent making progress problem purple choose explore problem noisy estimated learning progress. exploration meta-policy target meta-policy goal exploration loop main objective consists experiments allow collect data cover well space problems exploration meta-policy π\u0001pθ learned used output distribution policies interesting execute gather information solving self-generated problem/goal context achieve objective collecting interesting data exploration meta-policy must fast incremental updates. maximize coverage space problems precise targeting goals less crucial capacity update meta-policy quickly incrementally. thus memory-based learning methods well adapted within loop contrary target policy training loop aims learn meta-policy purpose used exploitation mode later meta-policy asked solve precisely possible problems maximum reward. training meta-policy done asynchronously data collected goal exploration loop allows training algorithms slower possibly batch might allow better generalize e.g. using gaussian mixture models support vector regression neural networks. differences justify fact imgeps general different representations learning algorithms learning meta-policies two-level learning scheme similarities complementary learning systems theory used account organization learning mammalian brains particular case problem space modular agent learn goal policy problem space choice goals becomes hierarchical sense agent ﬁrst chooses problem space explore goal space policy particular goal corresponding goal policy levels choice seen learning curves different problem spaces exploration preference based learning progress space computed average progress across problems launch asynchronously following loops exploration loop loop observe context choose goal space based intrinsic rewards e.g. using choose goal infer policy parameters maximize expected rgpc using exploration/exploitation trade-off possibly computing problem-speciﬁc reward function giving reward/ﬁtness previous experiments solve current goal problem rgpcj execute roll-out observe trajectory tsttend attendu νenvpτ compute outcome trajectory goal-parameterized reward function rcθoτ computed reward/ﬁtness current experiment solving problem rcθoτppq compute current reward associated solving goal problem update achieved γkpg updategoalpolicy updategoalspacepolicy updateknowledge target meta-policy training loop updatemetapolicy achieved using online batch training order benchmark different learning algorithms complex realistic environment continuous policy outcome spaces designed real robotic setup composed humanoid front joysticks used tools objects. show running experimental setup video. code available open-source together shapes printed objects. robotic setup platforms ﬁrst poppy torso robot mounted front joysticks second platform poppy ergo robot controlled right joystick push ball controls lights sounds. poppy robust accessible open-source printed robotic platform figure robotic setup. left poppy torso robot mounted front joysticks. right full setup poppy ergo robot controlled right joystick tennis ball arena changes lights sounds. robotic left joints. position joints time deﬁned action bounds deﬁned probability self-collide still reach large volume even left behind left shoulder extent. framework dynamical movement primitives generate smooth joint trajectories given motor parameters. joints controlled starting rest position joint parameterized weights weight basis functions weight representing position joint trajectory given provided agent dmps generates policy roll-out outputting smooth -steps trajectory atendu joints executed translate trajectory robotic hand producing roll-out goes back rest position. tools toys analogical joysticks reached left moved direction. position joysticks controls poppy ergo robotic follows. left joystick control variable. ergo robot motors moves hardwired synergies allow control rotational speed extension. right joystick left-right axis controls speed rotation ergo robot around center second platform means pushing right joystick right small angle move ergo towards right small speed pushing joystick higher angle increase ergo’s rotational speed. ergo rotation angle bounded r´π; reset every iterations. right joystick backward-forward axis controls extension ergo joystick rest position ergo stays rest position. right joystick moved forward ergo extends away center using motors comes back joystick released. yellow tennis ball freely moving blue arena slightly sloped ball always comes close center movement. ball tracked camera retrieve position polar coordinates speed ball controls intensity light circle around arena. finally ball touches border arena sound produced varied pitch depending ball rotation angle. learning agent’s point view \"things\" objects special status objects tools manipulate objects discovered agent. distractors several objects included environment agent cannot interact. agent initially aware objects can’t controlled. objects moving randomly independently agent noise variable added time step objects static right hand robot disabled experiment camera recording ball trajectory blue circular arena yellow out-of-reach button also out-of-reach lamp distractor objects reset roll-out. trajectory outcomes choosing motor command agent observes current context conﬁguration objects scene then chosen command translates dmps motor trajectory left executed seconds. assume perceptual system providing trajectories objects scene. learning system focus here. successive states environment deﬁned follows. first trajectory hand computed forward model position. states joystick ergo read sensors position ball retrieved camera. states intensity light pitch sound computed ball position speed. state represents concatenation states objects time variables vector trajectory deﬁned sequence actions states environment seconds tstp stend atendu. deﬁne outcome corresponding object -steps sampling trajectory movement. outcome space corresponding hand outcome space corresponding left joystick right joystick ergo ball respectively light sounds outcome space corresponding random distractors static objects movement agent computes outcome concatenation outcomes corresponding object goals agent deﬁned particular outcomes reach space goals thus particular trajectories object many goals actually solvable e.g. moving hand right left quickly moving ball high speed. outcome space deﬁnes goal space framework unsupervised multi-goal reinforcement learning problem modular problem space initial experience void. context space represents position object movement. policy parameter space parameter deﬁnes action sequence dynamical movement primitive framework. execution policy context leads trajectory tstp stend atendu robot actions environmental states. outcomes computed samples objects’ trajectories movement concatenation outcomes. outcome space object deﬁnes particular goals agent reach trajectories agent give object. space thus problem space deﬁne reward associated solving problem context parameters outcome ´||p ok||k. norm ||.||k euclidean norm divided maximal distance rewards comparable across problem spaces. given context policy outcome reward computed agent time experience oτq. interested uniform loss lpπq meta-policy equation study four variants modular goal exploration processes plus control random agents. random motor babbling control condition agents choose random policy iteration. assess long random exploration would take discover reward information improving towards given problems. four following algorithms agents choose random policies iterations random model babbling model babbling particular model-based implementation intrinsically motivated modular goal exploration process agent learns model goal space goal space corresponds trajectory space object. model providing inverse function which given current context desired goal gives policy estimated best reach goal context. goal policy hierarchical sense agent ﬁrst chooses model train goal space explore particular goal goal space random model babbling choice goal space particular goal random γkpg always uniform distributions. meta-policies π\u0001pθ memory-based initialized uniform distributions. bootstrapping experience random policies implemented fast nearest neighbor search kd-tree. given goal problem context agent chooses previously executed policy corresponding context-outcome closest current i.e. minimizes prppc c||q. exploration meta-policy used explore policies deﬁned meta-policy plus gaussian exploration noise π\u0001pθ diagonal covariance matrix weights meta-policies implementation algorithmic variants possible implementations meta-policies described appendix single goal space algorithm similar algorithm chosen goal space always outcome space ball always chooses chooses random goals deﬁne condition study agents would learn case engineer cares ability robot push ball gives goal space robot. fixed curriculum algorithm similar curriculum engineered hand agents choose goal spaces seventh total number iterations sequence active model babbling active model babbling variant implementation architecture goal space policy tries maximize empirical estimation learning progress agent estimates learning progress globally problem space iteration context observed goal space chosen random goal sampled then iterations agent uses π\u0001pθ generate exploration policy update progress estimation. uses without exploration generate updates learning progress estimation estimated progress reaching estimate learning progress made reach current goal agent compares outcome outcome obtained previous context goal similar finally implements non-stationary bandit algorithm sample goal spaces. bandit keeps track running average intrinsic rewards associated current goal space probability samples random space probability uses soft maximization probability sample proportional expp otherwise. bandit variant strategic bandit trial iterations random condition independent trials condition. first show agents explored different outcome spaces depending condition study structure learning problem looking details agents condition succeeded information problems space exploring another ﬁnally analyze intrinsic rewards goal space policy condition amb. trial iteration takes raspberry computers running learning algorithm control torso ergo robots. implementation learning algorithms online incremental inverse models nearest neighbors search whole learning energy-efﬁcient experimental computational energy cost paper exploration outcome spaces order estimate performance different learning algorithms compute loss given test distribution measures accuracy learned meta-policy choose good parameters solve problems contexts sampled training execution policies allowed produce outcomes outcome corresponds solution particular kok. diverse outcomes diverse problems meta-policy solve good reward. example agent managed move left joystick different directions outcomes corresponding left joystick diverse meta-policy able reproduce movements evaluated problem moving left joystick given direction. good proxy measuring loss thus measure diversity outcomes produced exploration outcome space deﬁne measure exploration outcome space percentage cells explored discretization outcome space variables. instance outcome space corresponding hand corresponding hand’s position time points seconds movement. space variables discretized bins variable count number cells occupied time points previous iterations. similarly exploration outcome space corresponding object variables measured bins discretization space show results exploration measure learning fig. random condition generates good diversity hand outcome space explore outcome spaces. results condition depend trials trials agent succeeded discover move object next exploration stage allowed exploration budget current stage giving good exploration results trials showing exploration spaces. condition gives equal lower exploration results random spaces agent always focuses ball outcome space even know reach joysticks. exploration experimental computational energy cost paper total energy consumption processing units experiments paper kwh. similarly experimental robotic energy cost paper total energy consumption robots’ actuators experiments paper kwh. estimate carbon cost running experiments gco-eq. figure exploration outcome spaces exploration measure percentage cells explored discretization outcome space. provide mean trials condition learning iterations discoveries order understand structure learning problem look details agents condition succeeded information problems space exploring another space. fig. shows proportion iterations goal given space allowed move left joystick right joystick ergo robot. first easiest reachable object exploring random policies choosing goals hand outcome space left joystick reached iterations versus almost objects. also discover right joystick efﬁcient choose goals left joystick hand choosing random policies similarly shows discover move ergo much efﬁcient choose goals right joystick hand left joystick results tells good strategy discover objects explore different outcome spaces sequence easiest movements right joystick discovered explore ergo discovered etc. structure thus transfer learning problem sense different problems independent exploring solutions allows discover part solutions others. figure transfer solutions problem spaces. show proportion iterations allowed reach left joystick reach right joystick move ergo robot depending current chosen goal space condition intrinsic rewards active model babbling fig. shows running average intrinsic rewards computed bandit algorithm agents condition depending problem space goals chosen. solution discovered control object intrinsic reward stays object means agent rarely choose train solve problems spaces. estimated learning progress space drives choice space explore thus leads developmental sequence exploring easier complex problems avoiding loose time complex problems. figure intrinsic rewards condition amb. graph shows learning progress estimated agent condition problem spaces iterations. learning progress heuristic leads sequence exploration focus easier complex problems. early models intrinsically motivated reinforcement learning used drive efﬁcient exploration context target tasks rare deceptive rewards context computational modelling open-ended unsupervised autonomous learning humans reviews historical development methods links cognitive sciences neuroscience found several lines results shown intrinsically motivated exploration learning mechanisms particularly useful context learning solve reinforcement learning problems sparse deceptive rewards. example several state-of-the-art performances deep reinforcement learning algorithms letting machine learn solve complex video games achieved complementing extrinsic rewards intrinsic reward pushing learner explore improving predictions world dynamics even radical approach solving problems rare deceptive extrinsic rewards completely ignore extrinsic rewards machine explore environment sole purpose learning predict consequences actions learning control self-generated goals generate novel outcomes shown example allow robots learn tool learn play video games without ever observing extrinsic reward. approaches intrinsically motivated exploration used intrinsic rewards value visited actions states measuring novelty improvement predictions provide e.g. recently however organizing intrinsically motivated exploration higher level goals sampling goals according measures competence progress proposed shown efﬁcient contexts high-dimensional continuous action spaces strong time constraints interaction environment several strands research robotics presented algorithms instantiate intrinsically motivated goal exploration processes using different terminologies contextual policy search formulated within evolutionary computation perspective however previous approaches formalized general framework unsupervised multi-goal reinforcement learning considered intrinsically motivated exploration multiple spaces goals allow formation learning curriculum. preliminary investigation modular goal exploration processes presented however prior work presented particular implementations imgeps without formalizing general formal framework presented here. proposed methods framed imgeps however considered notions goals restricted reaching states direct sensory measurements consider goal-parameterized rewards computed goal used different intrinsic rewards evaluate algorithms robotic setups. notion auxiliary tasks also related imgeps sense allows learner acquire tasks rare rewards adding several objectives increase density information obtained environment another line related work proposed theoretical framework automatic generation problem sequences machine learners however focused theoretical considerations experiments abstract problems. machine learning concept curriculum learning often used context training neural networks solve prediction problems. many approaches used hand-designed learning curriculum recently shown learning progress could used automate intrinsically motivated curriculum learning lstms however approaches considered curriculum learning sets reinforcement learning problems central imgep framework assumed pre-existence database learning exemplars sample from. recent related work studied intrinsic rewards based learning progress could also used automatically generate learning curriculum discrete sets reinforcement learning problems consider high-dimensional modular parameterized problems. concept \"curriculum learning\" also called \"developmental trajectories\" prior work computational modelling intrinsically motivated exploration particular topic intrinsically motivated goal exploration paper provides formal framework unsupervised multi-goal reinforcement learning problem expressed problem structure appearing different settings. formalized corresponding algorithmic architecture leverages structure efﬁcient exploration learning. designed ﬁrst real robotic experiment intrinsicallymotivated humanoid robot discovers complex continuous high-dimensional environment succeeds explore learn scratch objects used tools objects. evaluated different variants intrinsically motivated goal exploration processes showed variants hand-design curriculum learning allowed discover complex skills. furthermore agent monitors learning progress intrinsic rewards autonomously develops learning sequence curriculum easier complex tasks. also comparison agents exploring ball problem space versus spaces shows engineer specify target problems wants robot solve would efﬁcient also explore possible intrinsic goals develop skills serve stepping stones solve target problems. chosen evaluate several implementations imgeps real robotic setup provides real world constraints available simulations stringent time constraints make impossible tune parameters algorithms running many pre-experiments well realistic complex noise. however also prevented much experiments would need disentangle learning performances conditions amb. currently running trials different conditions. also current limitation setup suppose agents already perceptual system allowing track objects well spaces representations encode transformations. future work study representation learning methods could bootstrap representations scene pixels unsupervised manner.", "year": 2017}