{"title": "Differentiable Scheduled Sampling for Credit Assignment", "tag": ["cs.CL", "cs.LG", "cs.NE", "I.2.7; I.2.6"], "abstract": "We demonstrate that a continuous relaxation of the argmax operation can be used to create a differentiable approximation to greedy decoding for sequence-to-sequence (seq2seq) models. By incorporating this approximation into the scheduled sampling training procedure (Bengio et al., 2015)--a well-known technique for correcting exposure bias--we introduce a new training objective that is continuous and differentiable everywhere and that can provide informative gradients near points where previous decoding decisions change their value. In addition, by using a related approximation, we demonstrate a similar approach to sampled-based training. Finally, we show that our approach outperforms cross-entropy training and scheduled sampling procedures in two sequence prediction tasks: named entity recognition and machine translation.", "text": "demonstrate continuous relaxation argmax operation used create differentiable approximation greedy decoding sequence-tosequence models. incorporating approximation scheduled sampling training procedure well-known technique correcting exposure bias–we introduce training objective continuous differentiable everywhere provide informative gradients near points previous decoding decisions change value. addition using related approximation demonstrate similar approach sampled-based training. finally show approach outperforms cross-entropy training scheduled sampling procedures sequence prediction tasks named entity recognition machine translation. sequence-to-sequence models demonstrated excellent performance several tasks including machine translation summarization dialogue generation image captioning however standard cross-entropy training procedure models suffers well-known problem exposure bias cross-entropy training always uses gold contexts states contexts encountered training match encountered test time. issue addressed using several approaches incorporate awareness decoding choices training optimization. include reinforcement learning imitation learning beam-based approaches paper focus simplest implement least computationally expensive approaches scheduled sampling stochastically incorporates contexts previous decoding decisions training. scheduled sampling empirically successful training objective drawback procedure directly incorporates greedy decisions time step objective discontinuous parameter settings previous decisions change value. result gradients near points non-informative scheduled sampling difﬁculty assigning credit errors. particular gradient provide information useful distinguishing local errors without future consequences cascading errors serious. here propose novel approach based scheduled sampling uses differentiable approximation previous greedy decoding decisions inside training objective incorporating continuous relaxation argmax. result end-to-end relaxed greedy training objective differentiable everywhere fully continuous. making objective continuous points previous decisions change value approach provides gradients respond cascading errors. addition demonstrate related approximation reparametrization sample-based training yield stochastic gradients lower variance standard scheduled sampling. experiments different tasks machine translation named entity recognition show approach outperforms cross-entropy training standard scheduled sampling effective rectify exposure bias candifferentiate cascading errors lead sequence decisions local errors benign effects. speciﬁcally scheduled sampling focuses learning optimal behavior current step given ﬁxed decoding decision previous step. previous decision largely responsible current error training procedure difﬁculty adjusting parameters accordingly. following machine translation example highlights credit assignment issue step model prefers word ‘barks’ after incorrectly predicting ‘dog’ step correct error scheduled sampling procedure would increase score ‘purrs’ step conditioned fact model predicted ‘dog’ step ideal learning behaviour. ideally model able backpropagate error step source problem occurred step ‘dog’ predicted instead ‘cat’. lack credit assignment training result discontinuity objective function used scheduled sampling illustrated figure denote ground truth target symbol step embedding representation word hidden state seqseq decoder step standard crossentropy training deﬁnes loss step hi−)) scheduled sampling uses loss hi−)) refers model’s prediction previˆ step. here model prediction ˆyi− obtained argmaxing output softmax layer. hence addition intermediate hidden states ﬁnal softmax scores previous model prediction ˆyi− depends model parameters ideally backpropagated through unlike gold target symbol independent model parameters. however argmax operation discontinuous thus training objective exhibits discontinuities parameter settings previous decoding decisions change value change points represent discontinuities gradients undeﬁned effect correcting earlier mistake training procedure approaches point essentially hidden. approach described detail next section attempt problem incorporating continuous relaxation argmax operation scheduled sampling procedure order form approximate fully continuous objective. relaxed approximate objective depicted figure blue purple lines depending temperature parameter tradessmoothness quality approximation. section explain detail continuous relaxation greedy decoding build fully continuous training objective. also introduce related approach sample-based training. word vocabulary output score word previous step embedding passed next step. operation relaxed replacing indicator function peaked softmax function hyperparameter deﬁne soft argmax procedure another variant scheduled sampling pass sampled embedding softmax distribution previous step current step instead argmax. expected enable better exploration search space optimization added randomness hence result robust model. section discuss review approximation gumbel reparametrization trick module sample-based decoder. approximation proposed maddison jang showed soft argmax operation introduced used reducing variance stochastic gradients sampling softmax distributions. unlike soft argmax approach fully continuous approximation sampling operation result much informative gradients compared naive scheduled sampling procedure. gumbel reparametrization trick shows sampling categorical distribution refactored sampling simple distribution followed deterministic transformation follows sampling independent gumbel noise element categorical distribution typically done transforming sample uniform distribution −log adding componentwise unnormalized score element ﬁnally taking argmax vector. using argmax softening procedure above arrive approximation reparametrization trick mitigates gradient’s variance introduced sampling. approximation ‘concrete’ approximation softmax sampling relaxation scheduled sampling sample-based decoder. discuss details next section. note original motivation based removing discontinuity strictly apply sampling procedure still yields stochastic gradient sampling gumbel distribution. however approach conceptually related greedy relaxations since here soft argmax reparametrization reduces gradient variance yield informative training signal. intuitively approach results gradient loss aware sampling procedure compared naive scheduled sampling hence carries forward information decisions made previous steps. empirical results discussed later show similar gains greedy scenario. differentiable relaxed decoders argmax relaxation introduced above recipe fully differentiable greedy decoder designed produce informative gradients near change points. ﬁnal training network scheduled sampling relaxed greedy decoding shown figure instead conditioning current hidden state argmax embedding previous step ˆei− α-soft argmax embedding ¯ei− deﬁned section removes discontinuity original greedy scheduled sampling objective passing linear combination embeddings dominated argmax next step. figure this different using expected softmax embedding approach approximates actual sampling process instead linearly weighting embeddings softmax probabilities illustrates effect varying increases closely approximate greedy decoder. standard scheduled sampling minimize cross-entropy based loss time step. hence computational complexity approach comparable standard seqseq training. discuss section mixing model predictions randomly ground truth symbols training annealing probability using ground truth epoch results better models stable training. result training reliant annealing schedule important hyperparameters ground truth mixing probability parameter used approximating argmax function. output prediction time step still output hard argmax depicted figure case scheduled sampling sample-based training–where decisions sampled rather chosen greedily conduct experiments using related training procedure. instead using soft argmax soft sample embedding ˜ei− deﬁned section apart difference training carried using procedure. approximation-aware gormley training conceptually related focuses variational decoding procedures. hoang also propose continuous relaxations decoders focused developing better inference procedures. grefenstette successfully soft approximation argmax neural stack mechanisms. finally ranzato experiment similarly motivated objective fully continuous found performed worse standard training. perform experiments machine translation named entity recognition data dataset preprocessing data splits ranzato named entity recognition conll shared task data german language provided data splits. perform preprocessing data.the output vocabulary length ner. implementation details seqseq model simple attention mechanism bidirectional lstm encoder lstm decoder seqseq model lstm encoder lstm decoder ﬁxed attention mechanism deterministically attends input token decoding output hence involve learning attention parameters. hyperparameter tuning start training actual ground truth sequences ﬁrst epoch decay probability selecting ground truth token inverse sigmoid epochs decay strength parameter also tuned different values explore effect varying exponentially epochs. table report results best performing conﬁguration decay parameter parameter validation set. account variance across randomly started runs multiple random restarts systems evaluated always used best validation score calculate test performance. comparison report validation test metrics tasks table bleu respectively. ‘greedy’ table refers scheduled sampling soft argmax decisions ‘sample’ refers corresponding reparametrized sample-based decoding scenario. compare approach baselines standard cross-entropy loss minimization seqseq models standard scheduled sampling procedure report results variants approach ﬁxed parameter throughout training procedure vary exponentially number epochs fixed attention refers scenario bidirectional lstm encoder representation source sequence token time step decoding time step instead using linear combination input sequences weighted according attention parameters standard attention mechanism based models. three approaches improve standard cross-entropy based seqseq training. moreover approaches using continuous relaxations outperform standard scheduled sampling best results obtained relaxed greedy decoder annealed yielded gain standard seqseq baseline gain standard scheduled sampling. obtain best results relaxed sample-based decoder yielded gain bleu standard seqseq gain bleu standard scheduled sampling. observe reparametrized samplebased method although fully continuous endto-end unlike soft greedy approach results good performance tasks particularly might effect stochastic exploration search space output sequences training hence expect beneﬁt sampling much larger search space associated also observe annealing results good performance suggests smoother approximation loss function initial stages training helpful guiding learning right direction. however experiments noticed table effect different schedules scheduled sampling ner. decay strength parameter. higher corresponds gentler decay schedules. always refers case predictions previous predictions always passed inputs next step. computational complexity approach comparable standard seqseq training. however instead vocabulary-sized lookup approach requires matrix multiplication. practically observed hardware models tasks similar speeds suggests approach leads accuracy gains without compromising run-time. moreover shown table observe gradual decay mixing probability consistently compared favorably aggressive decay schedules. also observed ‘always sample’ case relaxed greedy decoding never ground truth inputs worked well resulted unstable training reckon effect large difference search space associated positive results indicate mechanisms credit assignment useful added models ameliorate exposure bias. further results suggest continuous relaxations argmax operation used effective approximations hard decoding training. marc’aurelio ranzato sumit chopra michael auli wojciech zaremba. sequence level training recurrent neural networks. international conference learning representations. st´ephane ross geoffrey gordon drew bagnell. reduction imitation learning structured prediction no-regret online learning. aistats. volume page alexander rush sumit chopra jason weston. neural attention model abstractive sentence summarization. empirical methods natural language processing. iulian serban alessandro sordoni yoshua bengio aaron courville joelle pineau. building end-to-end dialogue systems using generative hieraaai’ proarchical neural network models. ceedings thirtieth aaai conference artiﬁcial intelligence. erik tjong sang fien meulder. introduction conll- shared task language-independent named entity recognition. proceedings seventh conference natural language learning hlt-naacl -volume association computational linguistics pages kelvin jimmy ryan kiros kyunghyun aaron courville ruslan salakhutdinov richard zemel yoshua bengio. show attend tell neural image caption generation visual attention. icml. volume pages references daniel andor chris alberti david weiss aliaksei severyn alessandro presta kuzman ganchev slav petrov michael collins. globally normalized transition-based neural networks. association computational linguistics. dzmitry bahdanau philemon brakel kelvin anirudh goyal ryan lowe joelle pineau aaron courville yoshua bengio. actor-critic algorithm sequence prediction. international conference learning representations. dzmitry bahdanau kyunghyun yoshua bengio. neural machine translation jointly learning align translate. international conference learning representations. samy bengio oriol vinyals navdeep jaitly noam shazeer. scheduled sampling sequence prediction recurrent neural networks. advances neural information processing systems. pages mauro cettolo niehues sebastian st¨uker luisa bentivogli marcello federico. report iwslt evaluation campaign iwslt proceedings international workshop spoken language translation hanoi vietnam. daum´e daniel marcu. learning search optimization approximate large margin proceedings methods structured prediction. international conference machine learning. pages matthew gormley mark dredze jason eisner. approximation-aware dependency parsing belief propagation. transactions association computational linguistics edward grefenstette karl moritz hermann mustafa suleyman phil blunsom. learning advances transduce unbounded memory. neural information processing systems. pages chris maddison andriy mnih whye teh. concrete distribution continuous reinternalaxation discrete random variables. tional conference learning representations.", "year": 2017}