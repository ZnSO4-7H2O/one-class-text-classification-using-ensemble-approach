{"title": "Domain Adaptation and Transfer Learning in StochasticNets", "tag": ["cs.CV", "stat.ML"], "abstract": "Transfer learning is a recent field of machine learning research that aims to resolve the challenge of dealing with insufficient training data in the domain of interest. This is a particular issue with traditional deep neural networks where a large amount of training data is needed. Recently, StochasticNets was proposed to take advantage of sparse connectivity in order to decrease the number of parameters that needs to be learned, which in turn may relax training data size requirements. In this paper, we study the efficacy of transfer learning on StochasticNet frameworks. Experimental results show ~7% improvement on StochasticNet performance when the transfer learning is applied in training step.", "text": "fig. training test error stl- dataset. blue lines represent performance result stochasticnet transferred knowledge purple lines represent performance result baseline stochasticnet. observed stochasticnet transferred knowledge achieves signiﬁcant improvement performance compared baseline stochasticnet. discussion figure demonstrates training test errors baseline stochasticnet well stochasticnet transferred knowledge. observed stochasticnet transferred knowledge provides signiﬁcant improvements classiﬁcation accuracy compared baseline stochasticnet decrease test error promising preliminary results demonstrate efﬁcacy transfer learning stochasticnets highly motivate future deeper exploration area boosting performance stochasticnets situation insufﬁcient training data domain interest. acknowledgments work supported natural sciences engineering research council canada canada research chairs program ontario ministry research innovation. authors also thank nvidia hardware used study nvidia hardware grant program.", "year": 2015}