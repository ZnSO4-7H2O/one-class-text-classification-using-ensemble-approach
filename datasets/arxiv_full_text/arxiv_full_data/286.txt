{"title": "Learning to generalize to new compositions in image understanding", "tag": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "abstract": "Recurrent neural networks have recently been used for learning to describe images using natural language. However, it has been observed that these models generalize poorly to scenes that were not observed during training, possibly depending too strongly on the statistics of the text in the training data. Here we propose to describe images using short structured representations, aiming to capture the crux of a description. These structured representations allow us to tease-out and evaluate separately two types of generalization: standard generalization to new images with similar scenes, and generalization to new combinations of known entities. We compare two learning approaches on the MS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show, Attend and Tell), and a simple structured prediction model on top of a deep network. We find that the structured model generalizes to new compositions substantially better than the LSTM, ~7 times the accuracy of predicting structured representations. By providing a concrete method to quantify generalization for unseen combinations, we argue that structured representations and compositional splits are a useful benchmark for image captioning, and advocate compositional models that capture linguistic and visual structure.", "text": "recurrent neural networks recently used learning describe images using natural language. however observed models generalize poorly scenes observed training possibly depending strongly statistics text training data. propose describe images using short structured representations aiming capture crux description. structured representations allow tease-out evaluate separately types generalization standard generalization images similar scenes generalization combinations known entities. compare learning approaches ms-coco dataset state-of-the-art recurrent network based lstm simple structured prediction model deep network. structured model generalizes compositions substantially better lstm times accuracy predicting structured representations. providing concrete method quantify generalization unseen combinations argue structured representations compositional splits useful benchmark image captioning advocate compositional models capture linguistic visual structure. training models describe images natural language embodies fundamental problems language image understanding. allows ground meaning language visual data figure motivating task learning generalize compositions entities images reﬂected descriptions. image represented subject-relation-object tuple. compositional split testing performed novel compositions entities observed training namely images matching given assigned either training testing. language compositionality understand rich visual scenes. recently deep neural networks successfully used task results inspiring impressive became clear aftermath analyzing results current approaches suffer fundamental issues. first generalization poor images describing scenarios seen training time. second evaluating descriptions challenging strong language models generate sensible descriptions missing espaper propose address issues focusing structured representations image descriptions. ﬁrst step simple structured representations consisting subject-relationobject triplets reducing full sentences representation focus composition entities image. main advantages. first allows quantify quality model predictions directly using accuracy predictions. second allows partition data model tested combinations included training set. allows evaluate compositional generalization unseen scenarios illustrated figure partition ms-coco dataset using compositional split compare state-of-the-art recurrent attention model show-attend-and-tell structured prediction model built deep cnn. recurrent model achieves similar performance traditional ms-coco split. however achieves accuracy structured model tested partitioning requires generalization combinations. generalizing novel compositions observation separate kinds generalization interest generating image descriptions. ﬁrst generalizing images class routinely evaluated including current data split ms-coco challenge second type focus concerned generalizing scenarios akin transfer zeroshot learning learning extended semantically-similar classes. importantly generalization crux learning complex scenes since language visual scenes compositional resulting exponentially large possible descriptions. hence goal learning describe images would properly quantify generalization combinations known entities relations. practice ﬁrst image descriptions short open-ie style phrases form subjectrelation-object partition examples test training sets share common images triplets compositional split natural test generalization short utterances natural language since small training could used train large possible combinations test time. overlap scenarios training test still occur synonymy hypothesize partitioning leads much stronger need generalization. jointly predict triplet train structured-prediction model deep convolutional network. first image analyzed produce candidate bounding boxes labels similar classiﬁer trained large dataset without ﬁne-tuning current data. structured model deep network used structured minimizing hinge loss between predicted ground-truth triplets. speciﬁcally model learns score function triplets decomposed scalar weights learned algorithm. here score assigned subject score assigned object score assigned relation binary feature subject relation similarly fro. details model appendix better understanding signals useful prediction experimented multiple variants model potentials details section experiments data evaluated image captioning ms-coco data currently standard benchmark evaluating image captioning models parsed ms-coco descriptions triplets ﬁrst constructing dependency parse trees description using manually-constructed patterns extract triplets description. finally word stemmed. removing descriptions without sros yielded unique pairs analyzing structured phrases images naturally involves grounding entities speciﬁc image locations. datasets like visual-genome ms-coco provide human-marked bounding boxes many entities. here goal able generalize entities larger datasets instead inferred bounding boxes using pre-trained deep-network localizer limited nouns vocabulary frequent nouns selecting entities localizable. vocabulary relations relations yielding triplets. vocabulary visual entity recognition used localizer fully overlap vocabulary captions. instance term appear captions terms bull calf} obtain high scores localizer. match vocabularies followed procedure zitnick appendix details. mapping used select images whose predicted entities matched entities captions. image several bounding boxes label selected highest score. also removed duplicate triplets image triplets subject object bounding box. keeping images bounding boxes subject object left pairs unique images stateof-the-art attention model caption generation re-trained decoder layers predict triplets soft-attention. hyper-parameters tuned maximize accuracy evaluation learning rate weight decay importantly also controlled model capacity tuning embedding dimensionality lstm dimensionality section remaining parameters implementation provided stochastic conditional draw based training distribution draw based training distribution ptrain ptrain. baseline designed capture gain attributed bigram statistics. frequent triplet predict consisting frequent subject frequent relation frequent object based training distribution. construction compositional split constructed frequent full triplet training appear test set. evaluation procedure test candidate pairs bounding boxes image. pair candidate triplets ranked scores compared ground-truth triplets compute precisionk image. images ground-truth since associated descriptions. image captioning bleu score common metric. here sroaccuracy equivalent bleu- single-term evaluation metric handle semantic smearing namely case image described several ways semantically adequate using different words hence counted errors. issue often addressed representing words continuous semantic spaces. keeping paper focused leave outside current evaluations experimented cross-validation procedures. first coco split used train-test split provided ms-coco restricted images sros second compositional split applied unique triplets create fold cross validation split. object subject appear train moved test training triplets object subject class appeared test triplets moved train set. subject object appearing less times removed training set. images used across approaches. fraction images sometimes deviates since triplets images others. figure table show average precisionk across images comparing ssvm sa&t test training performance. panel methods trained tested mscoco split. ssvm/conv model wins precision sa&t model achieves test precision baselines frequent dataset table triplet appear data yielding accuracy. figure comparing sa&t ssvm/conv. ms-coco split. compositional split. sa&t overﬁts strongly ssvm compositional split. error bars denote standard error mean across folds. test .%). importantly sa&t dramatically fails combinations large generalization apparent difference precision training test test precision baselines generalization often over-ﬁtting controlled reducing model capacity. therefore tested sa&t different capacities varying number parameters expected training error decreased number parameters. importantly test error decreased point started rising over-ﬁtting. ms-coco split sa&t best test error better ssvm model compositional split signiﬁcantly worse. words wide range lstm parameters still generalize well compositional split. importantly number examples experiments well within range dataset sizes sa&t originally used time ssvm model limited bigram potentials unable memorize triplets lstm model conclude merely reducing capacity sa&t model sufﬁciently effective control overﬁtting compositional case. better understanding signals useful prediction compared multiple variants ssvm model using different features r-node potential inputs details potentials appendix since images described myriad ways manually sampled random predictions ssvm model assess true model accuracy. every prediction answered questions exist image reasonable description image. cases ssvm produced exists image cases reasonable description image. compositional aspects language images recently explored approached visual task breaking questions substructures re-using modular networks. combined subjects objects relationships graph structure image retrieval. learned spatial relations generating descriptions based template. modelled synthetic scenes generated using crf. dataset combinations entities modelled crf. developed ways match sentences images space meaning parametrized subject-verbobject triplets structured model closely related recently trained model leverages language priors semantic embeddings predict subject-relation-object tuples. performance model unseencompositions subset test exhibits large generalization gap. finally generalization objects often achieved smearing semantically-related entities stochastic conditional frequent object node potential potential deﬁned similarly relation node potential relation node trained separate stage using train-test folds follows. multiclass trained predict features subject object bounding boxes. inference time score assigns relation given image. input features experiments used subject object one-hot-vector both. one-hot-vector features. spatial features following paper main contributions. first highlight role generalization combinations known objects vision-to-language problems propose experimental framework measure compositional generalization. second existing state-of-the-art image captioning models generalize poorly combinations compared structured-prediction model. future work plan extend approach full captions handle deeper semantic structures including modiﬁers adjectives more. subject node potential learned sparse linear transformation matrix localizer vocabulary caption entities vocabulary bases empirical joint probability training data. example learned weighted combination likelihood scores localizer gives classes bull calf}. spatial features normalized zero mean unit variance. pairwise feature fsr. potential bigram probability combination estimated training data similarly fro. creating dataset selected images visual entities mapped terms captions. since vocabulary visual entity recognition differs vocabulary captions estimated mapping locaizer vocabulary caption terms following procedure zitnick speciﬁcally computed labels predicted localizer bounding boxes nouns sro. considered top- matches vocabulary word manually pruned outliers removed data sample caption terms match bbls. step results entities.", "year": 2016}