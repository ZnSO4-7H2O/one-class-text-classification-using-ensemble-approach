{"title": "Structured Low-Rank Matrix Factorization with Missing and Grossly  Corrupted Observations", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Recovering low-rank and sparse matrices from incomplete or corrupted observations is an important problem in machine learning, statistics, bioinformatics, computer vision, as well as signal and image processing. In theory, this problem can be solved by the natural convex joint/mixed relaxations (i.e., l_{1}-norm and trace norm) under certain conditions. However, all current provable algorithms suffer from superlinear per-iteration cost, which severely limits their applicability to large-scale problems. In this paper, we propose a scalable, provable structured low-rank matrix factorization method to recover low-rank and sparse matrices from missing and grossly corrupted data, i.e., robust matrix completion (RMC) problems, or incomplete and grossly corrupted measurements, i.e., compressive principal component pursuit (CPCP) problems. Specifically, we first present two small-scale matrix trace norm regularized bilinear structured factorization models for RMC and CPCP problems, in which repetitively calculating SVD of a large-scale matrix is replaced by updating two much smaller factor matrices. Then, we apply the alternating direction method of multipliers (ADMM) to efficiently solve the RMC problems. Finally, we provide the convergence analysis of our algorithm, and extend it to address general CPCP problems. Experimental results verified both the efficiency and effectiveness of our method compared with the state-of-the-art methods.", "text": "recovering low-rank sparse matrices incomplete corrupted observations important problem machine learning statistics bioinformatics computer vision well signal image processing. theory problem solved natural convex joint/mixed relaxations certain conditions. however current provable algorithms suﬀer superlinear per-iteration cost severely limits applicability large-scale problems. paper propose scalable provable structured low-rank matrix factorization method recover low-rank sparse matrices missing grossly corrupted data i.e. robust matrix completion problems incomplete grossly corrupted measurements i.e. compressive principal component pursuit problems. speciﬁcally ﬁrst present small-scale matrix trace norm regularized bilinear structured factorization models cpcp problems repetitively calculating large-scale matrix replaced updating much smaller factor matrices. then apply alternating direction method multipliers eﬃciently solve problems. finally provide convergence analysis algorithm extend address general cpcp problems. experimental results veriﬁed eﬃciency eﬀectiveness method compared state-of-the-art methods. keywords compressive principal component pursuit robust matrix completion robust principal component analysis low-rank matrix recovery completion recent years recovering low-rank sparse matrices severely incomplete even corrupted observations received broad attention many diﬀerent ﬁelds statistics bioinformatics machine learning computer vision signal image processing areas data analyzed often high dimensionality brings great challenges data analysis digital photographs surveillance videos text documents. fortunately high-dimensional data observed intrinsic dimension often much smaller dimension ambient space high-dimensional data principal component analysis popular analysis tools recover low-rank structure data mainly simple implement solved eﬃciently eﬀective many real-world applications face recognition text clustering. however main challenges faced observed data often contaminated outliers missing values small linear measurements address issues many compressive sensing rank minimization based techniques methods proposed robust low-rank sparse matrix decomposition lrsd) low-rank matrix completion many applications recover matrix small number observed entries example collaborative ﬁltering recommender systems. problem often called matrix completion missing entries outliers presented arbitrary location measurement matrix. matrix completion used wide range problems collaborative ﬁltering structure-from-motion click prediction recommendation face reconstruction applications would like recover low-rank sparse matrices corrupted data. example face images person corrupted glasses shadows classical cannot address issue least-squares ﬁtting sensitive gross outliers. recovering low-rank matrix presence outliers extensively studied often called rpca lrsd. rpca problem successfully applied many important applications latten semantic indexing video surveillance image alignment general applications also simultaneously recover low-rank sparse matrices small sets linear measurements called compressive principal component pursuit principle problems mentioned exactly solved high probability mild assumptions hybrid convex program involving l-norm trace norm minimization. recent years many techniques algorithms low-rank matrix recovery completion proposed theoretical guarantees derived however provable algorithms exploit closed-form expression proximal operator trace norm involves singular value decomposition hence high computational cost even applicable solving large-scale problems. address issue propose scalable robust bilinear structured factorization method recover low-rank sparse matrices incomplete corrupted data small linear measurements formulated follows regularization parameter klk∗ trace norm low-rank matrix rm×n i.e. singular values rm×n sparse error matrix given linear measurements underdetermined linear operator linear projection operator denotes loss function l-norm loss l-norm loss functions. unlike existing robust low-rank matrix factorization approaches method takes account fact observation contaminated additive outliers missing data i.e. robust matrix completion problems also identify low-rank sparse noisy components incomplete grossly corrupted measurements i.e. cpcp problems. also present robust bilateral factorization framework cpcp problems repetitively calculating large matrix replaced updating much smaller factor matrices. verify convincing experimental results eﬃciency eﬀectiveness method. propose scalable structured framework simultaneously recover low-rank sparse matrices cpcp problems. imposing orthogonality constraint convert original cpcp models smaller-scale matrix trace norm regularized problems respectively. fact optimal solution i.e. values unobserved locations zero reformulate proposed problem replacing linear projection operator constraint simple equality one. remainder paper organized follows. review background related work section section propose scalable trace norm regularized models cpcp problems. develop eﬃcient admm algorithm solving problems extend solve cpcp problems section provide theoretical analysis algorithm section report empirical results section conclude paper section distributed according haar measure probabilistically independent sign. minimizer problem unique equal probability least positive numerical constants. theorem states commensurately small number measurements suﬃcient accurately recover low-rank sparse matrices high probability. indeed entire space model degenerates following rpca problem denotes given observations. several algorithms developed solve convex optimization problem ialm lrsd although models convex optimization problems algorithms converge globally optimal solution involve iteration suﬀer high computational cost many eﬀorts towards fast computation partial approximate performance existing methods still unsatisfactory many real applications. address problem propose scalable provable robust bilinear factorization method missing grossly corrupted observations. matrix factorization useful tools scientiﬁc computing high dimensional data analysis decomposition decomposition nmf. paper robust bilinear factorization aims smaller low-rank matrices following discuss solve models worth noting rpca problem viewed special case problem entries corrupted matrix directly observed. next section mainly develop eﬃcient alternating direction method multipliers solver solving non-convex problem also worth noting although algorithm produce diﬀerent estimations estimation stable guaranteed theorems convexity problems according discussion above clear method scalable method cpcp problems. compared convex algorithms common rpca cpcp methods computational complexity impractical solving relatively large-scale problems method linear complexity scales well handle large-scale problems. understand better superiority method compare relate several popular robust low-rank matrix factorization methods. clear model special case trace norm regularized model moreover models used focus desired low-rank matrix. sense viewed special cases model major diﬀerence used factorizations used paper. factorizations also makes update operation highly scalable modern parallel architectures regarding complexity clear schemes similar theory computational complexity. however experimental results section algorithm usually runs much faster accurate methods following bilinear spectral regularized matrix factorization formulation similar models model section propose eﬃcient alternating direction method multipliers solving problem extend solving cpcp problem provide convergence analysis algorithm section recently shown literature admm eﬃcient convex non-convex programming problems various applications. also refer recent survey recently exploited applications admm. eﬃciently solving problem assume without loss generality unknown entries yk/αk. fact optimal solution given matrix pkvk speed-up calculation introduce idea uses decomposition instead svd. resulting iteration step formulated follows orthogonal basis range space i.e. although optimal solution iterative scheme equivalent solve equivalent analysis provided section moreover factorizations also makes update scheme highly scalable modern parallel architectures summarizing analysis above obtain admm scheme solve problem outlined algorithm algorithm essentially gauss-seidel-type scheme admm update strategy jacobi version admm easily implemented well suited parallel distributed computing hence particularly attractive solving largescale problems addition expected output algorithm also accelerated adaptively changing eﬃcient strategy increase iteratively algorithm easily used solve rpca problem entries corrupted matrix directly observed. moreover introduce adaptive rank adjusting strategy algorithm section algorithms important parameters. small cause underﬁtting large estimation error; large cause overﬁtting large deviation underlying low-rank matrix fortunately several works provided matrix rank estimation strategies compute good value rank involved matrices. thus relatively large integer addition provide scheme dynamically adjust rank parameter scheme starts overestimated input i.e. ⌊.r⌋. following decease aggressively dramatic change estimated rank product ukvt detected based eigenvalue decomposition usually occurs iterations. speciﬁcally calculate eigenvalues shown following theorem. rd×d|mt ukok vkok. u∗kt ukvt remark proof theorem found appendix since lagrange function determined product diﬀerent values essentially equivalent long product kvk∗ kv∗k∗. meanwhile detailed proofs lemma following lemma theorems found appendix lemma ensures feasibility solution assessed. paper want show possible prove local optimality solution produced algorithm number iterations needed algorithm stop deﬁned also discuss time complexity algorithm. problem main running time algorithm consumed performing small matrix size decomposition matrix pkvk matrix multiplications. time complexity performing time complexity decomposition matrix multiplications total time complexity algorithm solving problems framework introduced robust matrix factorization general many possible extensions methodology. section outline novel result methodology extension consider important low-rank matrix completion. space limit refrains fully describing development give readers enough details understand applications. similar algorithm present eﬃcient admm scheme solve matrix completion problem algorithm also easily used solve low-rank matrix factorization problem entries given matrix observed. evaluate eﬀectiveness eﬃciency method cpcp problems text removal background modeling face reconstruction collaborative ﬁltering. experiments intel core running windows main memory. ﬁrst conduct experiment considering simulated task artiﬁcially generated data whose goal remove generated text image. ground-truth image size rank equal data matrix. image short phase text form plays role outliers. fig. shows image together clean image outliers mask. fairness rank algorithms times true rank underlying matrix. input data generated setting randomly selected pixels image missing entries. compare method state-of-the-art methods including sparcs regl bf-alm regularization parameter √max regl stopping tolerance algorithms section. results obtained diﬀerent methods visually shown fig. outlier detection accuracy error low-rank component recovery also presented. low-rank matrix recovery concerned methods including sparcs regl bfalm outperform visually also quantitatively. outlier detection seen method signiﬁcantly performs better methods. short signiﬁcantly outperforms regl bf-alm sparcs terms low-rank matrix recovery spare outlier identiﬁcation. moreover running time sparcs regl bf-alm .sec .sec .sec .sec .sec respectively. evaluate robustness method respect regularization parameter given rank variations. conduct experiments artiﬁcially generated data illustrate outlier detection accuracy error lowrank component recovery sparcs regl method given rank sparcs regl method chosen regularization parameter regl chosen grid notice bf-alm regl achieve similar results provide results former following. average error results independent runs shown figs. method performs much robust sparcs regl respect given rank. moreover method much robust regl regularization parameter experiment test method real surveillance videos object detection background subtraction rpca plus matrix completion problem. background modeling crucial task motion segmentation surveillance videos. video sequence satisﬁes low-rank sparse structures background frames controlled factors hence exhibits low-rank property foreground detected identifying spatially localized sparse residuals test method four color surveillance videos bootstrap lobby hall mall databases. data matrix consists ﬁrst frames size since original videos colors ﬁrst reshape every frame video long column vector collect columns data matrix size moreover input data generated setting randomly selected pixels frame missing entries. fig. illustrates background extraction results bootstrap data ﬁrst fourth columns represent input images missing data second ﬁfth columns show low-rank recoveries third sixth columns show sparse components. clear background eﬀectively extracted method regl grasta notice sparcs could yield experimental results databases memory. moreover decomposition results method especially recovered low-rank components slightly better regl grasta. also report running time table times faster grasta times faster regl. shows method good scalability address large-scale problems. also test method face reconstruction problems incomplete corrupted face data small linear measurements respectively. face database used part extended yale face database large corruptions. figure background extraction results diﬀerent algorithms bootstrap data ﬁrst second last rows show recovered low-rank sparse images grasta regl respectively. face images often decomposed low-rank part capturing face appearances diﬀerent illuminations sparse component representing varying illumination conditions heavily shadows. resolution images pixel values normalized pixel values used form data vectors dimension input data generated setting randomly selected pixels image missing entries. fig. shows original reconstructed images regl average computational time algorithms people’s faces presented. observed performs better methods visually also eﬃciently eﬀectively eliminates heavy noise shadows simultaneously completes missing entries. words achieve latent features underlying original images regardless observed data corrupted outliers missing values. moreover implement challenging problem recover face images incomplete line measurements. considering computational burden projection operator resize original images normalize pixel values form data vectors dimension following input data subspace generated randomly dimension .mn. fig. illustrates reconstructed images cpcp respectively. clear cpcp eﬀectively remove shadows faces images simultaneously successfully recover low-rank sparse components reduced measurements. collaborative ﬁltering technique used recommender systems main purposes predict unknown preference user unrated items according similar users similar items. order evaluate method matrix completion experiments conducted three widely used recommendation system data sets movielensk ratings movielensm ratings movielensm ratings. randomly split data sets training testing sets ratio training testing experimental results reported independent runs. also compare method softimpute optspace lmafit state-of-the-art manifold optimization methods scgrass rtrmc parameters default values compared algorithms. root mean squared error evaluation measure deﬁned total number ratings testing denotes ground-truth rating user item denotes corresponding predicted rating. average rmse three data sets reported independent runs shown table results shown table that ﬁxed ranks matrix factorization methods including scgrass rtrmc lmafit method except optspace usually perform better convex trace norm minimization methods soft-impute. moreover bilinear factorization method trace norm regularization http//www.math.nus.edu.sg/~mattohkc/nnls.html http//www.stat.columbia.edu/~rahulm/software.html http//web.engr.illinois.edu/~swoh/software/optspace/ http//lmafit.blogs.rice.edu/ http//www-users.cs.umn.edu/~thango/ http//perso.uclouvain.be/nicolas.boumal/rtrmc/ figure face reconstruction results cpcp ﬁrst column show original images second third columns show low-rank sparse components obtained cpcp last columns show low-rank sparse components obtained rbf. consistently outperforms matrix factorization methods including optspace scgrass rtrmc lmafit trace norm minimization methods soft-impute. conﬁrms robust bilinear factorization model trace norm regularization reasonable. furthermore also analyze robustness method respect parameter changes given rank regularization parameter movielensm data shown fig. method robust parameter variations. comparison also show results related methods scgrass lmafit optspace rtrmc varying ranks diﬀerent regularization parameters fig. clear that increasing number given ranks rmse scgrass lmafit rtrmc becomes dramatically increases method increase slightly. conﬁrms bilinear matrix factorization model trace norm regularization signiﬁcantly reduce over-ﬁtting problems matrix factorization. scgrass rtrmc optspace spectral regularization models respectively method performs robust optspace scgrass rtrmc terms regularization parameter moreover method easily used incorporate side-information paper proposed scalable robust bilinear structured factorization framework cpcp problems. unlike existing robust low-rank matrix factorization methods proposed method address large-scale problems also solve low-rank sparse matrix decomposition problems incomplete corrupted observations. ﬁrst presented smaller-scale matrix trace norm regularized models cpcp problems respectively. developed eﬃcient admm algorithm solve rpca problems analyzed suboptimality solution produced algorithm. finally extended algorithm solve cpcp problems. experimental results real-world data sets demonstrated superior performance method comparison state-of-the-art methods terms eﬃciency eﬀectiveness. proof sketch lemma similar ﬁrst prove boundedness multipliers variables algorithm analyze convergence algorithm prove boundedness ﬁrst give following lemmas. lemma real hilbert space endowed inner product corresponding norm ∂kxk denotes subgradient. kyk∗ kyk∗ dual norm norm lemma αkbyk+ calculated kpωk λkvk∗ kpωk λkuvtk∗ kpωk λku∗tk∗ hby∗ u∗ti kpωk λku∗tk∗ hby∗ u∗ti kpωk λku∗tk∗ hby∗ u∗ti kpωk λku∗tk∗ hby∗ u∗ti. lemma kpωk∞ kuvt −u∗t kd−u∗t", "year": 2014}