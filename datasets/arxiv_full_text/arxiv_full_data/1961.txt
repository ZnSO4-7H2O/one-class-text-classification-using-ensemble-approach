{"title": "A Fast Factorization-based Approach to Robust PCA", "tag": ["cs.CV", "cs.AI", "stat.ML"], "abstract": "Robust principal component analysis (RPCA) has been widely used for recovering low-rank matrices in many data mining and machine learning problems. It separates a data matrix into a low-rank part and a sparse part. The convex approach has been well studied in the literature. However, state-of-the-art algorithms for the convex approach usually have relatively high complexity due to the need of solving (partial) singular value decompositions of large matrices. A non-convex approach, AltProj, has also been proposed with lighter complexity and better scalability. Given the true rank $r$ of the underlying low rank matrix, AltProj has a complexity of $O(r^2dn)$, where $d\\times n$ is the size of data matrix. In this paper, we propose a novel factorization-based model of RPCA, which has a complexity of $O(kdn)$, where $k$ is an upper bound of the true rank. Our method does not need the precise value of the true rank. From extensive experiments, we observe that AltProj can work only when $r$ is precisely known in advance; however, when the needed rank parameter $r$ is specified to a value different from the true rank, AltProj cannot fully separate the two parts while our method succeeds. Even when both work, our method is about 4 times faster than AltProj. Our method can be used as a light-weight, scalable tool for RPCA in the absence of the precise value of the true rank.", "text": "nuclear norm denoting i-th largest singular value |sij| norm. theoretically mild conditions exactly separate true rank number algorithms developed solve including singular value thresholding accelerated proximal gradient versions augmented lagrange multipliers based approaches exact inexact among algorithms based state-of-the-art ones solving need compute svds matrices iteration. improve efﬁciency another based algorithm adopts propack package solves partial svds instead full svds. however still computationally costly large. despite elegant theory convex rpca major drawbacks underlying matrix incoherence guarantee data grossly corrupted results underlying true ones; nuclear norm lead biased estimation rank combat drawbacks ﬁxes rank hard constraint uses non-convex rank approximation accurately approximate rank needs solve full svds. need solve partial svds signiﬁcantly reduces complexity compared computation full svd; example altproj complexity however known priori fail recover correctly. reduce complexity enhance scalability alleviate dependence knowledge paper propose factorization-based model rpca decomposed rd×k rn×k rk×k model relaxes requirement priori knowledge rank assuming upper bounded scalable algorithms developed optimize models efﬁciently. acceptance paper came attention also proposed factorization-based approach problem. however model distinct model variants uses explicit rank constraint matrix factorization case abstract—robust principal component analysis widely used recovering low-rank matrices many data mining machine learning problems. separates data matrix low-rank part sparse part. convex approach well studied literature. however state-of-the-art algorithms convex approach usually relatively high complexity need solving singular value decompositions large matrices. non-convex approach altproj also proposed lighter complexity better scalability. given true rank underlying rank matrix altproj complexity size data matrix. paper propose novel factorization-based model rpca complexity upper bound true rank. method need precise value true rank. extensive experiments observe altproj work precisely known advance; however needed rank parameter speciﬁed value different true rank altproj cannot fully separate parts method succeeds. even work method times faster altproj. method used light-weight scalable tool rpca absence precise value true rank. principal component analysis fundamental technique exploratory data analysis widely used many data mining tasks. given data matrix rd×n classic seeks best rank-k approximation complexity well known sensitive outliers. combat drawback past decade number approaches robust proposed including alternating minimization random sampling techniques multivariate trimming others recently type rpca method emerged become popular assumes separated parts i.e. low-rank sparse solving following problem factorization provides natural upper bound rank low-rank component upper bound used relax stringent requirement knowledge true rank altproj algorithm. capture sparse structure adopt norm obtain sparsity. thus consider following objective function many applications true rank known. presence prior knowledge however information present lack desired capability recovering rank arbitrary used. situation propose marry advantages convex ﬁxed-rank rpca approaches yielding following optimization problem balancing parameter. even case knowledge precise value available proper still chosen because worst case min. oftentimes domain information application hand upper bound obtained. shown nuclear norm approximate true rank well dominant singular values matrix nonconvex rank approximations help improve learning performance here adopt log-determinant rank approximation obtain following rpca model ground truth rank known uses non-convex rank approximation case ground truth rank unknown. variants model differ starkly considers second case simply uses nuclear norm. summarize contributions paper follows propose factorization-based model rpca allowing recovery low-rank component without priori knowledge true rank. absence true rank non-convex rank approximation adopted approximate rank matrix accurately nuclear norm. efﬁcient alm-type optimization algorithms developed scalability contrasted altproj whose cost difference important large. empirically extensive experiments testify effectiveness model algorithms quantitatively qualitatively various applications. deﬁned norms column vectors matrix. matrix large singular values nuclear norm accurate approximation rank. non-convex rank approximations considered number applications subspace clustering non-convex rank approximation also used rpca replaces nuclear norm non-convex rank approximation i-th largest singular value approaches usually need solve svds. matrix involved large computation general intensive. reduce complexity rpca several approaches attempted. example algproj uses non-convex alternating minimization techniques rpca admits complexity. uses factorization approach section formulate fast factorization-based rpca model data sparse component lowrank approximation rk×k core matrix rd×k rn×k satisfying identity matrix proper size. seen rest backgrounds. case ﬁrst last datasets respectively. dataset data matrix constructed treating vectorized frames columns. following test f-ffp u-ffp cases according whether known. case f-ffp altproj. terminate methods iterations x−l−sf reached. ialm data sets fast convergence good visual quality. balancing parameter chosen theoretical altproj default parameters used. f-ffp ialm. without speciﬁcation parameter settings remain throughout paper. numerical results reported table observed ialm separates sparsely fails recover rank f-ffp altproj recover properly. f-ffp generates sparse altproj datasets. methods competitive ﬁtting error. however possible f-ffp obtain accurate ﬁtting iterations time provided ialm altproj. furthermore f-ffp needs least amount time datasets. case speciﬁed tight upper bound based domain knowledge video. test datasets compare uffp altproj ialm. u-ffp chosen show numerical results table observed u-ffp still able recover true rank whereas altproj fails case. besides time cost u-ffp increases slightly less second datasets time cost altproj increases seconds. also show video frames figure visual results figure observed backgrounds recovered ialm still shadows; altproj separates foreground background well known backgrounds clean unknown; fffp u-ffp successfully separate foreground background video. learning face important topic pattern recognition; however shadows make challenging. often shadows face images varying lighting conditions therefore crucial handle shadows peculiarities http//perception.ir.a-star.edu.sg/bk model/bk index.html http//limu.ait.kyushu-u.ac.jp/dataset/en/ http//wordpress-jodoin.dmi.usherb.ca/dataset/ http//research.microsoft.com/en-us/um/people/jckrumm/wallﬂower/testimages.htm. respectively. derivations optimization similar summarize optimization algorithm here deﬁne left right singular vectors matrix thin deﬁne operator pdiag{σ∗ evaluate proposed model algorithms consider three important applications foreground-background separation shadow removal face images anomaly detection. compare algorithms state-of-theart methods including ialm altproj make propack package solve svds efﬁciency. experiments section conducted using matlab dual-core intel xeon linux server memory. purpose reproductivity provide codes website. foreground-background separation detect moving objects interesting activities scene remove background video sequence. task datasets listed ﬁrst column table among ﬁrst contain single background http//perception.csl.illinois.edu/matrix-rank/sample code.htmlrpca. http//www.personal.psu.edu/nsa/codes.html. https//www.researchgate.net/publication/ codes icdm highway altproj ialm f-ffp altproj ialm f-ffp altproj ialm f-ffp altproj ialm f-ffp altproj ialm f-ffp bootstrap altproj ialm f-ffp altproj ialm f-ffp altproj ialm f-ffp altproj ialm f-ffp altproj ialm f-ffp altproj ialm f-ffp altproj ialm f-ffp altproj ialm f-ffp altproj ialm f-ffp altproj ialm f-ffp saturations face images improve learning capability face image data. handled rpca clean images reside low-rank subspace shadows correspond sparse components. following extended yale dataset test. persons dataset choose ﬁrst subjects. subject heavily highway altproj u-ffp altproj u-ffp altproj u-ffp altproj u-ffp altproj u-ffp pedestrian bootstrap altproj u-ffp altproj u-ffp altproj u-ffp altproj u-ffp fountain altproj u-ffp altproj u-ffp altproj u-ffp altproj u-ffp altproj u-ffp altproj u-ffp corrupted images size taken varying lighting conditions. image vectorized column data matrix. natural assume face images person reside rank space hence first consider case known. following ialm altproj f-ffp applied subject quantitative visual results shown table figure respectively. observed figure altproj f-ffp successfully remove shadows face images. majority shadows removed ialm still remain. table altproj f-ffp recover lowrank component exactly rank ialm recovers higher rank. then consider case unknown. following setting apply altproj u-ffp subject. quantitative visual results shown table figure respectively. observed table u-ffp similar visual results f-ffp altproj appears unable remove shadows completely. quantitatively shown table altproj gives higher rank uffp gives true rank. besides proposed u-ffp fastest among methods. regarded outliers. anomaly detection identify kinds outliers. usps dataset contains images hand-written digits size following among images select ﬁrst images last construct data matrix size regarding vectorized image column. since number greater former dominant digit images latter outliers. true rank examples selected images shown figure observed besides quite different majority. therefore anomaly detection case detect also anomaly ‘’s. applying fffp columns correspond anomalies contain relatively larger values. norm measure every column show values figure outliers identiﬁed ﬁnding columns highest bars. ease visualization vanish values smaller figure corresponding digits shown figure outliers include several unusual ‘’s. figure foreground-background separation highway light switch- videos corresponding figure respectively. left right results ialm altproj f-ffp altproj u-ffp respectively. every panels grouped together bottom recovered background foreground respectively. figure shadow removal results subjects eyaleb data. parts left original image rest recovered clean images shadows ialm altproj f-ffp respectively. given number images subject form low-dimensional subspace. image signiﬁcantly different majority images regarded outlier; besides fewer images another subject kanade robust norm factorization presence outliers missing data alternative convex programming computer vision pattern recognition cvpr ieee computer society conference vol. ieee john wiley croux haesbroeck principal component analysis based robust estimators covariance correlation matrix inﬂuence functions efﬁciencies biometrika vol. wright ganesh peng robust principal component analysis exact recovery corrupted low-rank matrices convex optimization advances neural information processing systems leow cheng zhang background recovery ﬁxed-rank robust principal component analysis computer analysis images patterns. springer netrapalli niranjan sanghavi anandkumar jain non-convex robust advances neural information processing systems active subspace toward scalable low-rank learning neural computation vol. caramanis sanghavi robust outlier pursuit advances neural information processing systems peng kang cheng subspace clustering using log-determinant rank approximation proceedings sigkdd international conference knowledge discovery data mining. respectively. test computation time increases uniformly sample partition frames sampling rate .··· keep pixels frame. test relationship time respect frames down-sample frame different rates varying .··· keep spatial information frame. f-ffp u-ffp iterations repeated runs report average time figure observed time cost increases essentially linearly fffp u-ffp. paper propose factorization-based rpca model. non-convex rank approximation used minimize rank ground truth unknown. alm-type optimization developed solving model. model algorithms admit scalability data dimension sample size suggesting potential real world applications. extensive experiments testify effectiveness scalability proposed model algorithms quantitatively qualitatively.", "year": 2016}