{"title": "Ensemble representation learning: an analysis of fitness and survival  for wrapper-based genetic programming methods", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "Recently we proposed a general, ensemble-based feature engineering wrapper (FEW) that was paired with a number of machine learning methods to solve regression problems. Here, we adapt FEW for supervised classification and perform a thorough analysis of fitness and survival methods within this framework. Our tests demonstrate that two fitness metrics, one introduced as an adaptation of the silhouette score, outperform the more commonly used Fisher criterion. We analyze survival methods and demonstrate that $\\epsilon$-lexicase survival works best across our test problems, followed by random survival which outperforms both tournament and deterministic crowding. We conduct a benchmark comparison to several classification methods using a large set of problems and show that FEW can improve the best classifier performance in several cases. We show that FEW generates consistent, meaningful features for a biomedical problem with different ML pairings.", "text": "model respect objective function mind many variants proposed embed linear regression and/or local search program leading beer models high-level takeaway success methods hybridize best focus computational eort parts modeling process known np-hard namely tasks feature selection construction task feature construction also known feature engineering representation learning well-motivated since central factor aecting quality model derived ability data representation facilitate learning paper focuses supervised classication task goal mapping associates vector attributes class labels using paired examples goal feature engineering representation p-dimensional feature mapping classier accurately classies samples gp-based approaches representation learning include evolving single features decision trees coupling program recent work advocated refer ensemble approach treats entire population program representing transformation form proposed methods feed population output linear regression model make predictions. ml-specic nature previous approaches motivates development general feature engineering wrapper method wrapper-based ensemble method feature engineering unlike previous approaches allows learning algorithm scikit-learn format used estimation. demonstrated regression several pairings including lasso linear nonlinear support vector regression k-nearest neighbors central ability evolve features single population introduction \u0003-lexicase survival produces uncorrelated population behavior. wrapper-based ensemble approach under-studied presents challenges evolutionary computation standpoint namely need individuals population complement facilitating learning method paired. goal paper abstract recently proposed general ensemble-based feature engineering wrapper paired number machine learning methods solve regression problems. here adapt supervised classication perform thorough analysis tness survival methods within framework. tests demonstrate tness metrics introduced adaptation silhouee score outperform commonly used fisher criterion. analyze survival methods demonstrate \u0003-lexicase survival works best across test problems followed random survival outperforms tournament deterministic crowding. conduct benchmark comparison several classication methods using large problems show improve best classier performance several cases. show generates consistent meaningful features biomedical problem dierent pairings. reference format william cava jason moore. ensemble representation learning analysis tness survival wrapper-based genetic programming methods. proceedings genetic evolutionary computation conference berlin germany july pages. introduction traditional genetic programming applied classication and/or regression individual programs assume roles feature selection transformation model prediction evaluated ability make accurate estimations and/or predictions. exibility evolving structure parameters model comes heavy computational cost mitigated instead uses fast machine learning method optimize parameters permission make digital hard copies part work personal classroom granted without provided copies made distributed prot commercial advantage copies bear notice full citation page. copyrights third-party components work must honored. uses contact owner/author. gecco berlin germany copyright held owner/author. ----//.... test evaluating ability several survival tness techniques framework supervised classication. addition whereas previously demonstrated side-byside comparisons default methods robustly analyze whether general produce beer models existing techniques hyper-parameter optimization every method considered. paper contains four main contributions. first presents much-needed analysis tness survival methods ensemblebased representation learning currently lacking eld. second focuses classication task focus previous methods framework. presents robust comparisons methods including previously proposed method also focuses feature learning. contribution analyze biomedical problem able correctly identify nonlinear underlying structure data across pairings thereby showing usefulness learning readable data representations. pair several well-known classiers analysis logistic regression support vector classication random forests present overview section including description several tness survival methods tested. review related work thoroughly section including distinguishing wrapper lter approaches well single multiple ensemble representations features results experiments comparison methods shown section discussion conclusions following section methods components summarized figure learning process begins method original data. maintains internal validation evaluate models guarantees returned model cross-validation tness least good initial data representation produce. initializes population feature transformations seeded features initial model non-zero coecients. generation model trained produce ˆy). selection step entry point information method quality current representation. methods admit regularization feature importance scores apply selective pressure population eliminating individuals corresponding coecient feature importance zero model. feature importance measured using gini importance share feature selection role. selection remaining individuals figure used produce ospring sub-tree crossover point mutation. diers previous ensemble representation learning approaches incorporates crossover variation instead strict mutation. figure diagram showing main steps few. denotes starting population; population selection; ospring produced crossover mutation; population conducting survival φ❕φo. fitness compare three tness metrics experimental analysis section contrast traditional tness engineered feature must measure individual’s ability separate data classes rather predictive capacity since model. simple approach assessing feature quality look coecient determination using \u0003-lexicase survival designed promote feature diversity inuence ability population eectively produce representation training step. include random survival tests control eect unguided search. deterministic crowding niching mechanism ospring compete parent similar eqn. child dene similarity correlation spring. case mutation parent similarity comparison necessary. although traditionally steady state algorithm implementation generational. children take place parent surviving population beer tness. algorithm produces niches population maintain diverse features. \u0003-lexicase survival survival technique adapted \u0003lexicase selection few. \u0003-lexicase selection turn adaptation lexicase selection continuous-valued problems. lexicase selection works pressuring individuals population solve unique subsets training samples shiing selective pressure cases dicult terms population performance. \u0003-lexicase survival diers \u0003-lexicase selection removes individuals selected step remaining selection pool adds survivors next generation. iteration \u0003-lexicase survival proceeds follows getsurvivors related work feature construction received considerable aention implementations falling single feature multiple feature ensemble categories. single feature representations aempt evolve single solution engineered feature multiple feature representations encode candidate feature transformations individual individual multi-output estimate case separate model trained outputs program resulting output used assign tness individual. ensembles recent approach designed reduce computational complexity model individual. ensemble approaches instead single model output entire population. ensemble-like approach treats individual population single features treats ensemble output population among ensemble methods shares common evolutionary feature synthesis uses successful wrapper-based approach incorporates feature selection information seems appropriate since binary classication capture correlation feature change imposes additional however multiclass classication constraint feature rewarding increasing direction class label values. certain problems imposed tness pressure warranted general case want assume order class labels relative distance feature meaningful. instead want reward features separate samples dierent classes cluster samples within classes. mean belonging class label i.e. standard deviation. fisher criterion gives measure average pairwise separation between dispersion within classes however provide ne-grained information distance specic samples transformation. aempt extract information include silhouee score comparisons. like eqn. silhouee score assesses feature quality combining withinclass variance distance neighboring classes. captures tightness cluster overlap nearest cluster. silhouee score single sample dened here samples class label samples next nearest class takes account pairwise square distances within class separation neighboring classes other. euclidean distance metric used. aggregate tness engineered feature average silhouee score samples survival unlike typical populations model-based surviving individuals assessed together estimation therefore benet chosen work well together. fact many pairings depend co-linearity features including svc. test four methods achieving cooperation tournament survival deterministic crowding \u0003-lexicase survival random survival. tournament survival agnostic population structure selecting survivors simply picks individual tournament best tness survive. meanwhile deterministic crowding routine. unlike pairs exclusively lasso uses three population partitions incorporate crossover individuals. motivated hypotheses pairing best treated like hyper-parameter method existing diversity-preserving selection methods successfully adapted purposes ensemble-based feature survival. note previous work consider eect tuning proposed algorithm approaches compared vital step algorithm comparisons application real-world problems. experimental setup conduct separate sets experiments. described section designed compare tness survival methods combination dierent methods hyperparameters. results experiment choose tness survival method second experiments. second experiments described section benchmark comparison several methods larger classication problems. datasets used comparison freely available penn machine learning benchmark repository. comparison methods evaluate few’s performance comparison approaches gaussian na¨ıve bayes multi-feature method derived couples multi-feature representation nearest centroid classier information implementations refer methods evaluated classication problems vary numbers classes samples features seen table ensure robust comparisons include hyper-parameter optimization training phase method. grid search hyper-parameters method using -fold cross-validation training choose parameters. model best average cross validation accuracy training evaluated test set. process repeated shued train/test splits data. aempt control dierent possible hyper-parameter combinations methods hyper-parameters population output type oat); depth population size generations selection method length none regularization coecient penalty elastic net); epochs regularization coecient kernel estimators minimum weight fraction leaf features spliing criterion weights table classication data sets used paper tuning comparison methods stands gametes data sets named according number epistatic loci number attributes noise fraction heterogeneity fraction analcatdata authorship analcatdata cyyoung coil w-a-.h w-a-.h german hill valley noise hill valley without noise magic mfeat fourier mfeat pixel molecular biology promoters monk optdigits parity+ schizo texture vowel yeast hyper-parameters considered include population size method. expressed function number features data output type features bool) feature depth. floating point outputs outputs {and important note tuning method considered paired few. result experiment compares relative eects learning representation default method tuning hyper-parameters methods. results tness survival methods compared tuning datasets figures respectively. tness metric comparisons yield unexpected results. fisher criterion outperformed silhouee score problems surprisingly silhouee score outperform tness metric either; across problems pairings signicant dierence performance aside new-thyroid. surprising given hypothesis section class label assumptions implicit would make less suited classication multiple labels. according evidence conjunction lower complexity tness criterion benchmark comparison. \u0003-lexicase survival produces accurate classiers deterministic crowding tournament random survival across problems pairings. signicantly correlated higher test accuracy according t-test signicantly outperforms tournament deterministic crowding according pairwise wilcoxon tests correcting multiple comparisons. \u0003-lexicase survival also outperforms random survival auto new-thyroid ties problems random survival performs strongly compared tournament deterministic crowding survival outperforming methods problems. results motivate \u0003-lexicase survival benchmark comparison. test accuracies method comparisons benchmark datasets shown boxplot form figure mean rankings summarized figure across problems performance varies generally producing highest test accuracy. whereas generally well problems excels also well cases underperforms likely few’s ability tune method paired. problems stand particularly amenable feature engineering w-a-.h hill valley without noise parity+. three problems well-known containing strong interactions features helps explain observed increase performance few. terms mean rankings across problems generates best classiers among methods tested followed closely friedman test rankings post-hoc analysis reveals signicantly outperform across problems expected computation time higher methods wrapper-based approach. quicker performance explained implementation compared few’s python implementation well mgp’s consistently fast pairing. show models generated single runs w-a-.h table using genetics problem generated using gametes simulation tool consists aributes noise interact epistatically meaning must considered together infer correct class models correctly identify interaction features problem few’s transformation provides essential knowledge required figure comparison tness denitions tuning data sets. subplot presents dierent data set; x-axis corresponds paired learner boxplots represent accuracy scores obtained silhouette score figure comparison survival algorithms tuning data sets. subplot presents dierent data set; x-axis corresponds paired learner boxplots represent accuracy scores using dierent survival methods. discussion conclusion results suggest useful technique supervised classication problems. performs best average among algorithms tested include optimized models. result provides evidence methods data representation inuence algorithm performance much than parameter seings algorithms. although hasn’t tested here likely including hyper-parameter optimization methods paired tuning step would show even greater gains performance baseline approach. also performs beer multiple feature approach uses pairing. despite few’s runtime tests complexity analysis suggests well-positioned large datasets comparison feature construction techniques. whereas techniques like polynomial feature expansion scale poorly number features n-degree polynomial) techniques like kernel transformations scale poorly numbers samples scales independently features dataset linearly quadratically population size. observations warrant investigation large datasets. references soha ahmed mengjie zhang lifeng peng bing xue. multiple feature construction eective biomarker identication classication using genetic programming. proceedings annual conference genetic evolutionary computation. hp//dl.acm.org/citation.cfm?id= ignacio arnaldo krzysztof krawiec una-may o’reilly. multiple regression genetic programming. proceedings conference genetic evolutionary computation. press doihp//dx.doi.org/. ignacio arnaldo una-may o’reilly kalyan veeramachaneni. building predictive models feature synthesis. press doihp//dx. doi.org/./. yoshua bengio aaron courville pascal vincent. representation learning review perspectives. ieee transactions paern analysis machine intelligence hp//ieeexplore.ieee.org/xpls/ all.jsp?arnumber= rich caruana alexandru niculescu-mizil. empirical comparison supervised learning algorithms. proceedings international conference machine learning. hp//dl.acm.org/citation.cfm?id= vin´ıcius veloso melo. kaizen programming. gecco proceedings genetic evolutionary computation conference. press doihp//dx.doi.org/./. dean foster howard karlo justin aler. variable selection hard. proceedings conference learning eory. //www.jmlr.org/proceedings/papers/v/foster.pdf jerome friedman trevor hastie robert tibshirani. elements statistical learning. vol. springer series statistics springer berlin. //statweb.stanford.edu/∼tibs/book/preface.ps helmuth spector matheson. solving uncompromising problems lexicase selection. ieee transactions evolutionary computation doihp//dx.doi.org/./tevc.. hitoshi taisuke sato. genetic programming local hill-climbing. technical report etl-tr--. electrotechnical laboratory umezono tsukuba-city ibaraki japan. hp//www.cs.ucl.ac.uk/sta/w.langdon/p/ papers/iba gplhc.pdf michael kommenda gabriel kronberger stephan winkler michael aenzeller stefan wagner. eects constant optimization nonlinear least squares minimization symbolic regression. gecco companion proceeding eenth annual conference companion genetic evolutionary computation conference companion. amsterdam netherlands doihp//dx.doi.org/doi./. table example solutions gmt-w-a-h problem using decision tree logistic regression pairings. identies correct underlying epistatic interaction features cases. model either simple decision tree split order estimated importances logistic regression model four terms. krzysztof krawiec. genetic programming-based construction features machine learning knowledge discovery tasks. genetic programming evolvable machines hp//link.springer.com/article/./ william cava omas helmuth spector kourosh danai. genetic programming epigenetic local search. gecco proceedings genetic evolutionary computation conference. press doihp//dx.doi.org/./. william cava jason moore. general feature engineering wrapper machine learning using \u0003-lexicase survival. european conference genetic programming. springer hps//link.springer.com/chapter/. /---- ./---- william cava spector kourosh danai. epsilon-lexicase selection regression. gecco proceedings genetic evolutionary computation conference. york hp//dx.doi.org/./. cava william silva sara vanneschi leonardo spector moore jason genetic programming representations multi-dimensional feature learning biomedical classication. european conference applications evolutionary computation. springer hps//link.springer. com/chapter/./---- ./---- samir mahfoud. niching methods genetic algorithms. ph.d. disserta trent mcconaghy. fast scalable deterministic symbolic regression technology. genetic programming eory practice springer hp//link.springer.com/chapter/./---- mohammed muharram george smith. evolutionary constructive induction. ieee transactions knowledge data engineering hp//ieeexplore.ieee.org/xpls/abs all.jsp?arnumber= fabian pedregosa varoquaux alexandre gramfort vincent michel bertrand irion olivier grisel mathieu blondel peter preenhofer weiss vincent dubourg others. scikit-learn machine learning python. journal machine learning research hp//www.jmlr.org/ papers/v/pedregosaa.html sara silva luis mu˜noz leonardo trujillo vijay ingalalli mauro castelli leonardo vanneschi. multiclass classicatin rough multidimensional clustering. genetic programming eory practice xiii. vol. springer arbor mahew smith larry bull. genetic programming genetic algorithm feature construction selection. genetic programming evolvable machines hp//link.springer.com/article/./ s--- spector. assessment problem modality dierential performance lexicase selection genetic programming preliminary report. proceedings fourteenth international conference genetic evolutionary computation conference companion. hp//dl.acm.org/citation.cfm?id= robert tibshirani trevor hastie balasubramanian narasimhan gilbert chu. diagnosis multiple cancer types shrunken centroids gene expression. proceedings national academy sciences doihp//dx.doi.org/./pnas. ryan urbanowicz kiralis nicholas sinno-armstrong tamra heberling jonathan fisher jason moore. gametes fast direct algorithm generating pure strict epistatic models random architectures. biodata mining hps//biodatamining.biomedcentral.com/articles/ ./--- ˇzegklitz petr poˇs´ık. symbolic regression algorithms built-in linear regression. arxiv. hp//arxiv.org/abs/. arxiv", "year": 2017}