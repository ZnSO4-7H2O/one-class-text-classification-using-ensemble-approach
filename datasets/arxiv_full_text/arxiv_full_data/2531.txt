{"title": "Bayesian Optimization with Dimension Scheduling: Application to  Biological Systems", "tag": ["stat.ML", "cs.AI", "cs.LG", "math.OC"], "abstract": "Bayesian Optimization (BO) is a data-efficient method for global black-box optimization of an expensive-to-evaluate fitness function. BO typically assumes that computation cost of BO is cheap, but experiments are time consuming or costly. In practice, this allows us to optimize ten or fewer critical parameters in up to 1,000 experiments. But experiments may be less expensive than BO methods assume: In some simulation models, we may be able to conduct multiple thousands of experiments in a few hours, and the computational burden of BO is no longer negligible compared to experimentation time. To address this challenge we introduce a new Dimension Scheduling Algorithm (DSA), which reduces the computational burden of BO for many experiments. The key idea is that DSA optimizes the fitness function only along a small set of dimensions at each iteration. This DSA strategy (1) reduces the necessary computation time, (2) finds good solutions faster than the traditional BO method, and (3) can be parallelized straightforwardly. We evaluate the DSA in the context of optimizing parameters of dynamic models of microalgae metabolism and show faster convergence than traditional BO.", "text": "adepartment computing imperial college london binra laboratory environmental biotechnology france cdepartment chemical engineering imperial college london *m.deisenroth r.misenerimperial.ac.uk; abstract bayesian optimization data-efﬁcient method global black-box optimization expensive-to-evaluate ﬁtness function. typically assumes computation cost cheap experiments time consuming costly. practice allows optimize fewer critical parameters experiments. experiments less expensive methods assume simulation models able conduct multiple thousands experiments hours computational burden longer negligible compared experimentation time. address challenge introduce dimension scheduling algorithm reduces computational burden many experiments. idea optimizes ﬁtness function along small dimensions iteration. strategy reduces necessary computation time ﬁnds good solutions faster traditional method parallelized straightforwardly. evaluate context optimizing parameters dynamic models microalgae metabolism show faster convergence traditional bayesian optimization data-efﬁcient method global black-box optimization expensive-to-evaluate ﬁtness function. working hypothesis experiments expensive e.g. terms time money computations relatively cheap. applied wide variety problems including tuning critical parameters deep neural networks learning controller parameters walking robots automatic algorithm conﬁguration balance exploration exploitation optimization uses gaussian processes model posterior distribution ﬁtness functions available experiments similar experimental design acquisition function applied posterior ﬁtness functions suggest next experiment. applications common theme performance degrades high dimensions and/or large data amount observation points scaling gaussian processes surrogate functions. practice limited optimizing parameters experiments since predictions/evaluations scale linearly number dimensions cubically number data points integrate dynamic models biological processes allow test biological hypotheses running fewer costly real-world experiments paper considers estimating biological parameters minimizing squared error model experimental data points propose efﬁcient parameter estimation dynamic microalgae metabolism model forcing function based light exposure nitrate input; experimental data collected measurable outputs including lipids carbohydrates carbon organic biomass nitrogen organic biomass chlorophyll. method general applied process model. several timescales collecting microalgae metabolism data experiment lacour take days model simulation baroukh runs fraction second. traditionally applied functions expensive evaluation costs e.g. running experiment objective paper testing biological hypotheses; speciﬁcally interested running simulation model many times parameter estimation. parameter estimation dynamic systems alternatively proposed using deterministic global optimization heuristics using gradient-based methods. problems size practically speaking range current deterministic global optimization technology left consider heuristics gradient-based approaches. prefer bayesian optimization heuristics genetic algorithms allow immediately study sensitivity model response surface. gradient-based methods directly beneﬁt proposed bayesian optimization approach since warm start gradient optimization promising initialization points. evaluating microalgae metabolism model application quickly leads data sized standard cannot manage. address problem introduce bayesian optimization dimension scheduling algorithm distributes training data across many containing training data subset dimensions. iteration subset dimensions sampled probability distribution reﬂects importance corresponding parameters utility acquisition function optimized respect subset. beneﬁts faster computational performance iteration considers relatively small number prior experiments. context consider regression problem unknown ﬁtness/utility function. gaussian likelihood accounts independently identically distributed measurement noise objective infer latent utility function training data {xi}n {yi}n deﬁned collection random variables ﬁnite number gaussian distributed. fully speciﬁed mean function covariance function hyperparameters allows encode high-level structural assumptions e.g. differentiability. without loss generality assume prior mean function trained ﬁnding hyper-parameters {ψσε} maximize log-marginal likelihood rn×n kernel matrix. given hyper-parameters training test input posterior predictive distribution corresponding function value gaussian mean variance given places prior ﬁtness function serves probabilistic surrogate model instead optimizing require many function evaluations optimizes acquisition function depends posterior predictive distribution decide next evaluation point experiment yields additional training point updates model. updating observation repeats cycle convergence upper bound total number experiments. summarized alg. approaches toward scaling dimensions. additive method developed kandasamy variant similar contribution assumes objective function decomposed operating mutually disjoint dimensions functions decomposability assumption functions optimized separately. expedites typical approaches explicitly breaking dependence functional domains. although kandasamy show method applicable non-additive functions that case parameter estimation problem good results using abo. introduce shares goal cutting computation time abandons assumption functional decomposability particular idea mutually disjoint parameter domains. algorithm extension traditional algorithm summarized alg. initialization corresponds standard procedure observed function values sampled based initial samples corresponding observations argminx stores best known argument best objective value stores best objective value. samples coordinates probability vector represents relative importance coordinates. purpose apply available data proportional eigenvalue magnitude every iterations. samples random dimension optimize z-coordinates clamping coordinates current best values. optimization model possesses input dimensions z-coordinates exists spawn model relevant coordinates initial samples training data. optimization optimize acquisition function related obtain perform experiment observed function value better objective value update xn+. note effectively update dimensions parameter values data point added i.e. data size models increase. focus parameter estimation dynamic models microalgae metabolism objective minimize weighted squared errors model simulations real world experimental data. weights chosen mean kind experimental data kind measurement contributes equally error. model simulation evaluated fraction second wide bounds number dimensions creates large search area prohibits random sampling methods. compare traditional tested different model variants experiment runs model. model variant e.g. represents different biological hypothesis microalgae metabolism varying number parameters possible parameter bounds. experiment iterations randomly sampled data squared exponential kernel expected improvement acquisition function. cases chose dimension size dsa. figure motivates using dimension diagramming average results experiments model note average running time objective values consistently better dimension table summarizes model details results runs column stands dimensions model. cases terminated lower objective value traditional exceptions arise limitations discussed next section. figure presents sample running best objective values. traditional method tends improve less frequently optimization process progresses. method constant changes dimensions tends improve objective value frequently achieves overall lower objective value. random sampling performance uniformly superior traditional seen table algorithm completes experiment average ﬁfth computation time figure shows average computation time experiment runs models. performance increase stems reduced number dimensions reduced number since algorithm never uses data cannot assert global algorithm convergence. issue convergence could addressed example hybrid solution function space sampled near optimal solutions dsa. addresses subset problems speciﬁc conditions. algorithm faces similar challenges increase number total dimensions gp’s accuracy would suffer since observation points would spread thinly many gps. current implementation maximum number equivalent number permutations subset used dsa. however equally relevant pruning beneﬁcial limit number models. marginal likelihood could used purpose. advantages bayesian optimization dimension scheduler traditional bayesian optimization fairly straightforward code parallelization increased performance multicore systems. traditional bayesian optimization method parallelized certain extent. work gaussian process models distributed systems allows scale larger data sets. solvers parallelized dividing search space different processes. approaches provide compartmental parallelization whole process still sequential. parallelization process much simpler current solver modules. solution lies distribution across many processes manager process communicate between child-process objective function. manager would assign different iteration points process. process would contain solver based iteration number process receives process maximizes acquisition function process returns solution. manager would evaluate solution return process update start iteration. identiﬁed performance issue traditional experimentation time signiﬁcantly less optimization time therefore developed dsa. case microalgae model out-performed traditional terms computation time best objective value. could applied models relatively quick computational model high dimensionality requires efﬁcient parameter estimation. improvement parallel implementation reduction based marginal likelihood would lead even greater performance beneﬁts; parallelization potential suggests algorithm able manage dimensions traditional acknowledgments work supported engineering physical sciences research council royal academy engineering research fellowship r.m. references baroukh mu˜noz-tamayo j.-p. steyer bernard drum framework metabolic modeling non-balanced growth. application carbon metabolism unicellular microalgae. plos calandra seyfarth peters deisenroth bayesian optimization learning gaits uncertainty.", "year": 2015}