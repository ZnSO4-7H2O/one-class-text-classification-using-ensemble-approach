{"title": "Maximing the Margin in the Input Space", "tag": ["cs.AI", "cs.LG", "I.2.6; I.5.1"], "abstract": "We propose a novel criterion for support vector machine learning: maximizing the margin in the input space, not in the feature (Hilbert) space. This criterion is a discriminative version of the principal curve proposed by Hastie et al. The criterion is appropriate in particular when the input space is already a well-designed feature space with rather small dimensionality. The definition of the margin is generalized in order to represent prior knowledge. The derived algorithm consists of two alternating steps to estimate the dual parameters. Firstly, the parameters are initialized by the original SVM. Then one set of parameters is updated by Newton-like procedure, and the other set is updated by solving a quadratic programming problem. The algorithm converges in a few steps to a local optimum under mild conditions and it preserves the sparsity of support vectors. Although the complexity to calculate temporal variables increases the complexity to solve the quadratic programming problem for each step does not change. It is also shown that the original SVM can be seen as a special case. We further derive a simplified algorithm which enables us to use the existing code for the original SVM.", "text": "propose novel criterion support vector machine learning maximizing margin input space feature space. criterion discriminative version principal curve proposed hastie criterion appropriate particular input space already well-designed feature space rather small dimensionality. deﬁnition margin generalized order represent prior knowledge. derived algorithm consists alternating steps estimate dual parameters. firstly parameters initialized original svm. parameters updated newton-like procedure updated solving quadratic programming problem. algorithm converges steps local optimum mild conditions preserves sparsity support vectors. although complexity calculate temporal variables increases complexity solve quadratic programming problem step change. also shown original seen special case. derive simpliﬁed algorithm enables existing code original svm. generalization error independent dimensionality. practice however original sometimes gives small margin input space metric feature space usually quite diﬀerent input space. situation undesirable particular input space already well-designed feature space using prior knowledge. paper gives learning algorithm maximize margin input space. diﬃculty getting explicit form margin input space classiﬁcation boundary curved vertical projection sample point boundary always unique. solve problem linear approximation techniques. derived algorithm basically consists iterations alternating stages follows estimate projection point solve quadratic programming optimal parameter values. dual structure appears frameworks algorithm variational bayes. much related work principal curve proposed hastie principal curve ﬁnds curve ‘center’ points input space. derived algorithm gradient-descent type newton-like; hence investigate convergence property. shown derived algorithm always converges global optimum converges local optimum mild conditions. interesting relations original also shown original seen special case algorithm; number support vectors increase much original svm. algorithm veriﬁed simple simulations. construct m-dimensional input corresponding output using ﬁnite number samples consider linear classiﬁer sgn] feature input hilbert space weight margin input space deﬁned minimum distance sample points classiﬁcation boundary input space. since classiﬁcation boundary forms complex curved surface distance cannot obtained explicit form signiﬁcantly projection point boundary unique. here metric input space necessary euclidean. riemannian metric deﬁned enables represent many kinds prior knowledge. example invariance patterns implemented form. another example fisher information matrix natural metric input space parameter space probability distribution. although distance theoretically preferable measured length geodesic riemannian space causes computational diﬃculty. formulation since need distance sample point another point computationally feasible distance sample point another point quadratic norm order solve optimization problem start solution original update solution iteratively. kinds linearization technique kernel trick described next section derivative deﬁned ∂k/∂x. groups parameters here parameters linear coeﬃcients estimate projection point forms base functions. initialized corresponding parameters original parameters initialized since constraint nonlinear linearize around approximate solution solution current step. linearization simpliﬁes problem also enables derive dual problem. order avoid calculation mapping high dimensional hilbert space applies kernel trick inner product replaced symmetric positive deﬁnite kernel function easy complete algorithm consider update approximate value projection point initialized otherwise convergent solution precise want. good approximates solution given reﬁne iteratively projection point point critical point distance separating boundary i.e. local minimum local maximum saddle point. equilibrium state stable point local maximum saddle point. proof straightforward show point equillibrium state iteration step point critical point projection without loss generality assume uniform metric case update rule invariant metric transformation. consider behavior around critical point suﬃciently small vector show mapped separating hypersurface small step iteration. therefore consider case computationally intensive treatment would usually necessary several steps considered unstablity local minima occurs small region relatively size update causes another problem assumed section bases. however bases based need based ˆxi. solve problem although ˆωnew bases s.v. restrict bases support vectors preserve sparsity bases. proposition proved basically proposition fact linearization almost exact small perturbation case modify algorithm slowing equilibrium state stable margin locally optimal. however don’t simulation case local minimum unstable expected rare. another problem algorithm iteration step always increase margin monotonically. although usually faster gradient type algorithms algorithm sometimes improve solution original all. original seen special case algorithm annealing technique example updating temporal variables parameters gradually initial values. however simplicity crude method simulation follows repeat several steps algorithm choose best solution gives largest estimated value margin. complexity algorithm need space time complexity calculate temporal variables computation kernel function original requires space time. calculation pararellized easily. complexity diﬀerent comparatively small. variables calculated complexity same. therefore calculation temporal variables comparative time proposed algorithm comparative original svm. algorithm heavy large simpliﬁed algorithm shown section section give simulation result artiﬁcial data sets order verify proposed algorithm examine basic performance. training samples test samples randomly drawn positive negative distribution gaussian mixture components uniformly distributed centers ﬁxed spherical variance kernel function used spherical gaussian kernel metric taken euclidean figure show example results original proposed algorithm case margin value increases simulation repeated sets samples diﬀerent random numbers. estimated margins input space original proposed algorithm shown ﬁgure crude algorithm described previous section cases among runs cannot improve margin original svm. ratios margin distributed results indicates margin input space eﬃcient improve generalization performance average cases cannot reduce generalization error even margin input space increases. noisy situation hard margin classiﬁer often overﬁts samples. several possibitilities incorporate soft margin give simple one. soft margin derived introducing slack variables optimization problem. soft constraint form don’t update ﬁrst second steps algorithm necessary more. simpliﬁcation makes algorithm little simpler terms vanish. however consider simpliﬁcation. shown relation original original derived since causes many temporal variables maintain ˆgi. terms related ˆbi’s vanish. proposed learning algorithm kernel-based classiﬁer maximizes margin input space. derived algorithm consists alternating optimization foot perpendicular linear coeﬃcient parameters. dual structure appears frameworks algorithm variational bayes principal curve. many issues studied algorithm example analyzing generalization performance theoretically ﬁnding eﬃcient algorithm reduces complexity converges stably. also interesting issue extend framework problems classiﬁcation regression. paper assumed kernel function given ﬁxed. recently several techniques criteria choose kernel function proposed extensively. expect techniques much knowledge original incorporated framework. applying algorithm real world data also important.", "year": 2002}