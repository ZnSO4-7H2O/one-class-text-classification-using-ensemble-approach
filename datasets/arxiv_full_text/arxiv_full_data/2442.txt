{"title": "Better Optimism By Bayes: Adaptive Planning with Rich Models", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "The computational costs of inference and planning have confined Bayesian model-based reinforcement learning to one of two dismal fates: powerful Bayes-adaptive planning but only for simplistic models, or powerful, Bayesian non-parametric models but using simple, myopic planning strategies such as Thompson sampling. We ask whether it is feasible and truly beneficial to combine rich probabilistic models with a closer approximation to fully Bayesian planning. First, we use a collection of counterexamples to show formal problems with the over-optimism inherent in Thompson sampling. Then we leverage state-of-the-art techniques in efficient Bayes-adaptive planning and non-parametric Bayesian methods to perform qualitatively better than both existing conventional algorithms and Thompson sampling on two contextual bandit-like problems.", "text": "squandered myopic forms planning provably over-optimistic thompson sampling fails account risk task performs poorly. experimental results highlight fact bayesoptimal behavior adapts exploration strategy function cost horizon uncertainty non-trivial way. also consider extension model case general subtasks including subtasks themselves small mdps paper organized follows ﬁrst discuss modelbased bayesian reinforcement learning outline existing planning algorithms case show thompson sampling’s over-optimism deleterious. next introduce exploration-exploitation domain motivates statistical model class mdps shared structure across sequences tasks. provide empirical results version domain uses real data coming popular supervised learning problem along simulated extension. finally discuss related work. consider bayes-adaptive planner starts prior models environmental dynamics progressively receives data controlled interaction environment updates posterior distribution models using bayes-rule actions intended maximize ext= γtrt] discount factor random reward obtained time uncertain world requires balancing exploration exploitation. here discount factor plays crucial role arbitrating relative importance future rewards. general warrant much exploration future exploitation heavily downweighted. opposite true clear illustration γ-dependent exploration-exploitation bayesian policies found gittins indices optimal strategy integrates current belief could transformed light even though refer exploration exploitation actions never actually labeled bayesian setting interpretation actions whose consequences uncertain certainly valuable computational costs inference planning conﬁned bayesian model-based reinforcement learning dismal fates powerful bayes-adaptive planning simplistic models powerful bayesian non-parametric models using simple myopic planning strategies thompson sampling. whether feasible truly beneﬁcial combine rich probabilistic models closer approximation fully bayesian planning. first collection counterexamples show formal problems over-optimism inherent thompson sampling. leverage state-of-theart techniques efﬁcient bayes-adaptive planning non-parametric bayesian methods perform qualitatively better existing conventional algorithms thompson sampling contextual bandit-like problems. computer power increases statistical methods improve increasingly rich range variety probabilistic models world. models embody inductive biases allowing appropriately conﬁdent inferences drawn limited observations. domain beneﬁt markedly models planning control models arbitrate exquisite balance safe exploration lucrative exploitation. general powerful solution balancing involves forward-looking bayesian planning face partial observability treats exploration-exploitation trade-off optimization problem squeezing greatest beneﬁt choice. unfortunately notoriously computationally costly particularly complex models leaving open possibility might justiﬁed compared heuristic approaches perform similarly much reduced computational cost instance treating tradeoff learning problem regret setting focusing asymptotic requirement discover optimal solution motivation paper demonstrate practical power bayesian planning. show that despite arduous optimization problem sample-based planning approximations excel rich models realistic settings challenging exploration-exploitation task derived real dataset even data generated prior. contrast show beneﬁts bayesian inference possible future data. resulting policy well known solution augmented markov decision process whose details defer suppl. material section finding exact bayes-optimal policy computationally intractable even tiny state spaces since augmented state space either continuous discrete potentially unbounded; transitions augmented require integration full posterior. although operation trivial closedform simple probabilistic models intractable rich models. common solution approximate inference methods markov chain monte carlo snugly common heuristic planning problem side-stepped sampling posterior planning myopically. describe method called thompson sampling section below show panacea. potentially powerful class approximate solutions capable handling large state spaces complex models involves sample-based forwardsearch methods. algorithms sparse sampling bamcp plan myopically; approximate bayes-adaptive planning directly albeit computational cost. unclear integrate methods approximate mcmc approaches however recent algorithmic developments provide practical approximate inference schemes perform sample-based planning sophisticated models. thompson sampling thompson sampling myopic planning method selects actions step drawing single sample dynamics posterior distribution greedily solving corresponding sampled mdp; choosing optimal action current state. though heuristically myopic perspective bayes-adaptivity computationally cheap proven empirically theoretically perform well various domains mentioned well complex e.g. bayesian nonparametric models case handled mcmc sampling intuitively generates optimistic values unknown parts posterior entropy samples large. forces agent visit regions. however show deriving optimism exploration always beneﬁcial consider simple particularly pernicious classes counter-example; failure modes illustrated results section below. actions going left right. source reward either end. agent starts middle knows everything except delivers reward; mdps prior probability episode terminates reward obtained. figure illustration. critically transition changes belief end. step samples chains heads sample suggests rewarding. since depends unbiased coin effectively performing random walk probability moving either direction takes time reach much worse linear time bayes-optimal policy commits given direction tie-breaking ﬁrst step maintains direction chain. might ascribe failure fact developed multi-armed bandits lack temporally extended structure. duly adapted setting goal controlling expected regret. instance psrl algorithm inspired bayesian samples current posterior executes optimal policy several steps exploring bypasses ts’s lack commitment example still problematic discounted objectives illustrated example boss algorithm complicated construction combines multiple posterior samples examples illustrate similar issue kind optimism generates exploration. non-myopic planning forward-search bayesian planning avoids myopia integrating evolution possible future beliefs. sample-based forwardsearch planning algorithms sparse sampling perform integrations generally able deal approximate inference schemes necessary handle rich probabilistic models. bayes-adaptive monte-carlo planning algorithm forward-search sample-based bayes-adaptive planning algorithm based pomcp guaranteed converge bayes-optimal solution even combined mcmc-based inference despite lack ﬁnite-time guarantees displays good empirical performance number tasks. bamcp compounds advantages sparse-sampling increase search efﬁciency. shares samples taken posterior; combines many samples search tree able plan less myopically. critically like pomcp bamcp involves root sampling samples generated current history distribution ﬁltered forward. beliefs need updated step search tree thus search horizon number simulations bamcp requires samples posterior belief update instead samples many belief updates. reasons chose bamcp forward-search planning algorithm paper. completeness bamcp algorithm speciﬁed supplementary material; refer details. huge range possible models complex domains. understanding apply whole subject right. here adopt strategy successful areas statistical modeling namely using bayesian non-parametric model permits complexity scale observations accumulate carefully parameterizing structure likely repeat. section consider rich non-parametric task extension contextual bandit problem. however although solving wholly artiﬁcial task revealing differences different methods planning says little performance real cases data generated model. thus ﬁrst motivate rich model generalization realistic exploration task. mushroom exploration task mushroom dataset repository contains instances gilled mushrooms different species agaricus lepiota family described discrete attributes whether mushroom poisonous edible build based data follows point time agent faced attributes random mushroom dataset choose whether ignore ignoring mushroom consequence; eating edible mushroom rewarding eating poisonous mushroom incurs large cost illustrated figure agent provided initial ’free’ observations attributes edibility mushrooms. problem conventionally thought terms supervised learning. however since agent allowed ignore mushroom actually akin contextual bandit task however case unlike past work contextual bandits early rewards valuable later ones characterized discount factor critical difference explofigure illustration mushroom exploration domain rows represent sequence mushrooms value different attributes displayed column. agent mushroom obtain reward/cost choose ignore example sample sequence tasks section left columns indicate context variables right columns indicate rewards observed task left true clustering tasks displayed. dictionary context variables composed different colors case. formally mushroom consists sequence mushroom tasks parametrized parameter vector contains scalar generate context single scalar parameter generate subtask dynamic denoting dynamics described follows. states form mushroom eaten task otherwise. choosing exit action aexit increments ﬁrst state component updates context observation components. choosing action aeat updates simple statistical model aspect mushroom task joint statistics characteristics danger mushrooms. truth matter data actually unclear; therefore stringent test planning algorithm whether possible perform well based vague likely incentration parameter unknown inferred data inﬂuencing inﬂuenced exploration. results reported figure three different numbers ’free’ examples. surprising result bayesadaptive agent manages obtain positive return starting data despite mismatch true data generative model. demonstrates abstract prior information structure guide exploration successfully. given exactly statistical model fails match performance; speculate overoptimism investigate section supp. material initial data provided free reduce prior uncertainty improve performance large margin return remains inferior bayes-adaptive agent conditions. purposes comparison also considered simpler discriminative statistical model namely bayesian logistic regression suggested contextual bandits. figure shows results applying α−ucb context. worse logistic regression model crpbased model; demonstrates added beneﬁts prior captures many aspects data datapoints. α−ucb algorithm despite good performance long-run large datasets optimistic perform well discounted objectives. mushroom task seen sequence subtasks share structure whose order agent cannot control. domains adaptive medical treatments patient understood subtask handling customer interactions making decisions drill different geological locations. section consider generalized version domains characteristic form shared structure. further addressing environments actually drawn model study planning absence model mis-match. generalization allow multiple arms subtask using notation section parameter vector generate concontains scalar parameters text scalar parameters generate actual task dynamics generative model identical otherwise choices agent particular task either leave subtask next; pull arms previously pulled. states form steps. performance worse commits policy regular expected outcome since psrl takes time integrate react observations. accurate model. best agent assumes general non-parametric model allows substantial underlying complexity true model adapts ongoing characterization function evidence data observed employ particularly popular non-parametric model called chinese restaurant process dirichlet process mixture postulates mushrooms come possibly inﬁnite number mixture components. concentration parameter random variables cluster assignments. base measure dirichlet process assumed symmetric dirichlet prior hyperparameter together conjugate observation model allows relatively straightforward inference schemes collection vectors contains parameters corresponding mixture component task parameters particular drawn ﬁrst choosing mixture component according using corresponding parameters sample component inﬁnitestate inﬁnite-horizon derived generative process sampling inﬁnite sequence tasks patching together. data time consist mushroom attributes labels observed including current mushroom subtask posterior distribution dynamics obtained straightforwardly posterior past future since uniquely characterized mushroom data context dimension maximum number values attribute dimensions data. implies possible conﬁgurations mushrooms assumed model. since known generic hyperprior gamma. results mushroom task stress mushroom data really generated process assumed previous section makes task somerealistic. indeed agent lacks prior data maximizing return highly challenging. randomly eating mushrooms sample dataset particularly strategy cost asymmetry edible poisonous mushrooms. natural point comparison policy ignoring mushrooms leads neutral return figure exploration-exploitation results mushroom dataset steps discounted return bamcp model section including hyperparameter inference. either starting scratch prior prior plus initial random data size datapoints observed steps. discounted return α−ucb using bayesian logistic regression model task setting averaged runs. exact setting experiments portrayed figure context cues arms task possible values function domain −−}. drew mdps different values concentration parameter agent assumed know generative structure mdp; considered cases knew true value generic hyperprior gamma learn. seen contextual bandit task shared structure modeled crp. difference usual deﬁnition contextual bandit here arms known reward give option playing multiple arms context addition unlike algorithm existing work contextual bandit rarely exploits unsupervised learning context affords even label obtained. many extensions possible including complex intra-task dynamics general forms shared structure; however focus planning rather modeling. results synthetic data investigate behaviour performance bayesian agents acting tasks sampled non-parametric model above. reward mapping implies arms since values equally likely priori. thus again strategy πexit always exiting subtasks fair comparison value myopic planner based posterior mean never explore gaining value useful adaptive strategy able obtain mean return least concentrate metrics computed ﬁrst steps agent environment discounted rewards number times agent decides skip subtask trying arms second metric relates safe exploration aspect task; sometimes optimism unwarranted likely lead negative outcomes even taking account long-term consequences potential information gain. figure performance bamcp thompson sampling non-parametric bandit task terms discounted return concentration parameter known algorithm varies x-axis. average number subtasks ignored algorithm different environmental conditions. plotted value averaged runs steps bamcp task. figure shows performance function concentration parameter known. concentration parameter small different mixture components making easy case little uncertainty identity mixture components observations therefore little uncertainty outcome pull. limit cluster exist domain essentially degenerates form multinomial multiarmed bandit problem. grows identity given task’s cluster becomes uncertain aliasing grows safe exploration becomes challenging. learning slower regime simply parameter values acquire. every cluster different; would prevent kind generalization bayes-optimal policy skip every subtask figure shows bamcp adapts explorationexploitation strategy according structure environment; small values justify risk exploring incurring costs optimism progressively disappears gets larger. translates positive return generalization feasible despite marginal negative expected cost return close costs cannot avoided. hand suffers overoptimism across board leading poor discounted returns especially number mixture components large. intuition ts’s poor performance comes considering extreme case subtasks sampled different cluster. here past experience provides little information value arms current cluster; thus discovering values likely help future. however samples single conﬁguration mostly informed prior situation likely results least arms positive outcome then incorrectly picks putatively positive rather exiting. myopic sample-based exploration strategies bayesian boss would suffer similar forms unwarranted optimism since also rely sampling posterior samples according greedily challenging scenario arises known agent. bayesian agent starts uninformative prior order infer value data also takes account planning belief hyperparameter changes time. figure observe bayes-adaptive agent conservative explores safely unknown. expected results lower returns however robustness increased uncertainty shown modest difference. many researchers considered powerful statistical models context sequential decision-making including exploration-exploitation settings non-parametric models considered context control emphasis modeling data rather planning. authors consider factored mdps whose transitions modeled using bayesian networks. demonstrate advanfigure performance bamcp various values concentration parameter without hyperparameter inference. learned bayes-adaptive policy avoids subtasks manages maintain similar level performance despite uncertainty averaged runs simulations time-step. discount parameter tages appropriate prior capture existing structure true dynamics least case problems safe exploration arise. planning propose online monte-carlo algorithm approximate sampling scheme however forward-search conducted depth small branching factor presumably limiting beneﬁts bayes-adaptivity. consider inﬁnite combining bayes-adaptive planning approximate inference possible mdps. however class models quite speciﬁc particular domain consider. hierarchical dirichlet process used allow unbounded number states pomdp infer size state space data referred ipomdp model. model used online forward-search planning scheme albeit rather limited depth tested modestly-sized problems. gaussian processes employed infer models dynamics limited data excellent empirical performance. however uncertainty captures explicitly used exploration-exploitation-sensitive planning. addressed heuristic planning based uncertainty reduction. generally task reminiscent case active classiﬁcation active learning ultimately aims accurate classiﬁer labeling budget concerned completely different metric namely discounted remodel-based bayesian often viewed attractive hopeless particularly high-dimensional noisy domains. previously shown bamcp efﬁcient combination extensions monte-carlo tree search bayes-adaptive planning computationally viable powerful. showed alternative overoptimistic myopic planning methods thompson sampling severe problems bamcp avoids explicit lookahead computations. attempt scale bayes-adaptive planning real domains proposed contextual-bandit benchmark domain derived mushroom dataset associated bayesian non-parametric model challenging exploration-exploitation domain demonstrated feasibility advantages using bayes-adaptive fully bayesian agent. various ways improve planning. along generic ideas adaptive adjustments roll-out policy exerts strong inﬂuence performance bamcp would interesting think radical departures function approximation within search tree based histories possible future histories. remaining open problem understand classes domains truly beneﬁt computations bayes-adaptive planning served right simpler exploration-exploitation approach. indeed come examples additional computation barely matters gains vanishingly small focused planning; means challenge opened success bamcp modeling. non-parametric model shared structure amongst sub-tasks readily generalizable many domains including ones equivalent arms mdps radical extension would something closer indian buffet process whole collection subtasks also share measure structure; lead solutions collaboration among expert solvers.", "year": 2014}