{"title": "Symmetry-invariant optimization in deep networks", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "Recent works have highlighted scale invariance or symmetry that is present in the weight space of a typical deep network and the adverse effect that it has on the Euclidean gradient based stochastic gradient descent optimization. In this work, we show that these and other commonly used deep networks, such as those which use a max-pooling and sub-sampling layer, possess more complex forms of symmetry arising from scaling based reparameterization of the network weights. We then propose two symmetry-invariant gradient based weight updates for stochastic gradient descent based learning. Our empirical evidence based on the MNIST dataset shows that these updates improve the test performance without sacrificing the computational efficiency of the weight updates. We also show the results of training with one of the proposed weight updates on an image segmentation problem.", "text": "recent works highlighted scale invariance symmetry present weight space typical deep network adverse effect euclidean gradient based stochastic gradient descent optimization. work show commonly used deep networks max-pooling sub-sampling layer possess complex forms symmetry arising scaling based reparameterization network weights. propose symmetry-invariant gradient based weight updates stochastic gradient descent based learning. empirical evidence based mnist dataset shows updates improve test performance without sacriﬁcing computational efﬁciency weight updates. also show results training proposed weight updates image segmentation problem. stochastic gradient descent workhorse optimization deep networks well-known form uses euclidean gradients varying learning rate optimize weights deep network. regard recent work brought light simple scale invariance properties symmetries weight space commonly used deep networks possess. symmetries invariance reparameterizations weights imply although loss function remains invariant euclidean gradient varies based chosen parameterization. particular euclidean gradient scales inversely scaling variable leads different trajectories different reparameterizations weights training process ∗this work initiated author department electrical engineering computer science university li`ege li`ege belgium visiting department engineering university cambridge cambridge although issues raised recently precursor methods early work amari proposed natural gradients tackle weight space symmetries neural networks. idea compute steepest descent direction weight update manifold deﬁned symmetries direction update weights. euclidean gradient direction ignores symmetries longer steepest descent direction. recently pascanu bengio proposed second order method using natural gradients deep networks. natural neural networks hand deﬁne reparameterization network weights euclidean natural gradient based updates block-diagonal approximation fisher information matrix approximate natural metric motivate proposed reparameterization works ollivier deﬁne several metrics also based approximations fisher information matrix hessian loss function perform scaleinvariant optimization. above-mentioned proposals either computationally expensive implement need modiﬁcations architecture. hand optimization manifold symmetries topic much research provides guidance simpler metric choices show paper paper analysis commonly used networks shows exists complex forms symmetries affect optimization hence need deﬁne simpler weight updates take account invariances. accordingly look ways resolving symmetries. result geometric viewpoint manifold search space. proposed symmetry-invariant updates numerically efﬁcient implement. even though focus paper algorithms noted updates proposed table readily extended ﬁrst second order batch algorithms paper builds upon extend recent work section analyze weight space symmetries exist deep architecture commonly classiﬁcation layer composed fully connected network followed relu non-linearity max-pooling-sub-sampling step. analyze extension architecture output fully connected network batch normalized shown signiﬁcantly speed optimization architectures convolutional form currently used practical problems image segmentation e.g. section discusses manifold optimization techniques address symmetries propose simple weight updates give motivation behind those. proposed updates shown table finally numerical experiments discussed sections stochastic gradient descent algorithms proposed updates implemented matlab manopt codes available http//bamdevmishra.com/ codes/deepnetworks. keep exposition simple consider layer deep architecture shown figure layer arch typical components commonly found convolutional neural networks multiplication trainable weight matrix elementwise rectiﬁcation relu max-pooling stride sub-sampling. ﬁnal layer trainable soft-max classiﬁer predicts probabilities relevant classes. arch additional batch normalization layer compared arch analysis section readily extends deeper architectures components arch arch. rows weight matrices correspond ﬁlters layers respectively. dimension corresponds input dimension layer. example mnist digits dataset input dimensional vector ﬁlters layers dimensionality dimension corresponds trainable class vector. network figure ﬁlters layer positive scalar diag diag operator creates diagonal matrix argument placed along diagonal. noted repeating elements along diagonal comes max-pooling operation. reparameterizations changes intermediate outputs layer subsequently effect layer αβsm diag. reparameterize class vectors matrix hence weights classiﬁer parameters reparameterized shown leaves loss unchanged. therefore exists continuous symmetries reparameterizations leave loss function unchanged. noted analysis differs authors deal simpler case wherein scalar reparameterization free. emphasized reparameterβ izations left side reparameterization acts right side difference arch arch introduction batch normalization layer arch figure shows network. idea behind layer reduce change distribution input features different layers course optimization speed convergence. accomplished normalizing feature layers zero-mean unit variance mini-batch. separate trainable scale shift applied resulting features obtain respectively. effectively models distribution features gaussians whose mean variance learnt training. empirical results show normalization signiﬁcantly improves convergence experiments also support result. zero-mean unit-variance normalization elements allows complex symmetries exist network. consider following reparameterizations diag diag elements real number. loss invariant reparameterization weights seen following similar derivation shown arch. noted additional parameters used arch proposed left unchanged. unfortunately euclidean gradient weights used standard weight update invariant reparameterizations weights possible arch arch. seen simple example function invariant transformation non-zero scalar i.e. equivalently euclidean gradient clear euclidean gradient invariant reparameterizations i.e. scales inversely scaling variable consequently optimization trajectory vary signiﬁcantly based chosen parameterization. hand scale-invariant gradient scales proportionally scaling variable. issue resolved either deﬁning suitable non-euclidean gradient invariant reparameterizations placing appropriate constraints ﬁlter weights show following section propose ways resolving symmetries arise deep architectures. first follow approach equip search space non-euclidean metric resolve symmetries present. second break symmetries forcing ﬁlter weights unit-norm manifold. case updates simple implement stochastic gradient descent setting manifolds proposed updates shown layer deep network. however updates readily extended deeper architectures. consider weight vector euclidean squared length small incremental vector connecting given matrix positive deﬁnite matrix. identity matrix coordinate system euclidean. steepest descent direction loss function metric given euclidean gradient riemannian gradient metric consequently ﬁrst order weight update form current weight euclidean gradient updated weight learning rate. therefore resolve symmetries discussed section propose novel riemannian metric takes trace square matrix diag operator creates diagonal matrix argument placed along diagonal diag operator extracts diagonal elements argument matrix. noted deﬁned separately weights invariant reparameterizations shown arch arch. noted acts right side case motivation behind metric choice comes classical notion right left invariances differential geometry restricted diagonal elements. show invariance proposed metric consider invariance arch example. therefore squared length left unchanged considered reparameterization similar derivations show metric invariant reparameterizations term proposed metric collectively scaled metric scaled metric equally applicable invariant metric arch possesses symmetries shown another resolve symmetries exist arch arch constrain weight vectors oblique manifold i.e. ﬁlter fully connected layers constrained unit euclidean norm. equivalently impose constraints diag steepest descent direction loss unit-norm manifold computed euclidean gradient rie˜ mannian gradient unit-norm manifold effectively normal component euclidean gradient i.e. subtracted result tangential component. following tangential direction takes update manifold pulled back manifold retraction operation finally update weight unit-norm manifold form proposed weight updates based scaled metric based unit-norm constraint used stochastic gradient descent setting experiments described following section. emphasized proposed updates numerically efﬁcient implement. euclidean gradients computed efﬁciently using gradient back-propagation proposed symmetry-invariant updates loss function arch arch type networks shown table updated weight learning rate partial derivatives loss respect respec tively operator orth normalizes rows input argument. linear projection operation projects arbitrary matrix onto tangent space oblique manifold element speciﬁcally deﬁned z−diag)w diag operator creates diagonal matrix argument placed along diagonal diag operator extracts diagonal elements argument matrix. additional parameters used arch updated proposed noted convergence analysis manifolds follows developments train four layer deep arch arch networks perform digit classiﬁcation mnist dataset. dataset training images testing images. architectures features layer. digit images rasterized dimensional vector input network. input pre-processing performed. weights layer drawn standard gaussian ﬁlter unit-normalized. soft-max class vectors also drawn standard gaussian class vector unit-normalized. stochastic gradient descent based optimization widely used technique training deep networks. three different weight updates compare within ﬁrst-order framework scaled metric unit-norm balanced b-sgd uses euclidean updates wherein starting values ﬁlters class vectors unit-normalized. bsgd also studied benchmark algorithm choose mini-batch size samples. choose base learning rate training experimental network. select optimal learning rate create validation images training testing. train network learning rate using randomly chosen images training epochs. start epoch training randomly permuted mini-batches sampled sequence ensuring training sample used within epoch. record error validation measured error validation sample candidate base learning rate. candidate rate corresponds lowest validation error selected used training network full training set. repeat process learning rate selection training network full training times three weight update strategies. runs measure mean variance test error. ignore small proportion runs validation error diverged. training network well known protocols annealing decaying learning rate; bold-driver protocol exponential decay protocol. exponential decay protocol choose decay factor epoch. network protocols three different weight update strategies training runs combination thus totaling sixty training runs. training full dataset choose randomly chosen samples training remaining samples validation. train minimum epochs maximum epochs. bold driver protocol used terminate training training error less validation error increases respect measured epochs earlier successive validation error measurements differ less mean standard deviation test error various training combinations tabulated table quantitative ﬁgures observe arch performs signiﬁcantly better arch. emphasizes ability batch normalization improve performance arch results characterized high mean large standard deviation values three weight updates. however clear indication superiority protocol other. bold driver protocol improves performance b-sgd worsens performance four layer deep network. exponential decay protocol produces best result combination weight updates better performance four layer deep network. good performance explained fact allows better gradient back-propagation norm ﬁlters constrained unit norm. arch network includes batch normalization helps improve gradient back-propagation constraining scales input feature maps layer. beneﬁcial result adding layer clearly visible results perform much better b-sgd bold driver protocol. mean standard deviation test error lowest four layer deep networks considered. bold driver protocol performs better exponential decay protocol considered weight updates. annealing protocol seems suitable sufﬁcient regularization place arch show mean trajectories test error training epochs four layer deep arch arch figure arch show results using exponential decay protocol arch show results using bold driver protocol. practice using bold driver training terminated based stopping criterion epochs. table trajectories four layer deep arch arch networks result lowest mean test error. standard deviation band shown along mean trajectory dotted lines. best results obtained using arch weight updates bold driver learning rate annealing protocol. absorb symmetries present arch help improve performance achieved standard batch normalization alone. also seen regularize weights network training without introducing hyper-parameters e.g. weight decay term. quantitative results show layer deep network bold driver protocol performs better using b-sgd update training four layer deep network exponential decay learning rate. also noteworthy observe performance difference four layer deep arch network large. raises question future research whether networks necessarily deep made shallower better optimization. apply proposed weight updates table training segnet deep convolutional network proposed road scene image segmentation multiple classes network although convolutional possesses symmetries analyzed arch network trained epochs camvid figure using proposed weight update shown table training segnet quality predictions compared ground truth indicates successful training network. training images. predictions trained segnet sample test images dataset seen figure qualitative results indicate usefulness analysis symmetry-invariant weight updates larger networks arise practice. highlighted symmetries exist weight space currently popular deep neural network architectures. shown symmetries handled well stochastic gradient descent optimization framework either designing appropriate non-euclidean metric imposing unit-norm constraint ﬁlter weights. strategies take account manifold structure weights network reside lead symmetry-invariant weight updates. empirical results show test performance improved using proposed symmetry-invariant weight updates even modern architectures. future research direction would exploit techniques deep convolutional neural networks used practical applications. vijay badrinarayanan roberto cipolla supported sponsorship toyota motor europe belgium. bamdev mishra supported fnrs research fellow scientiﬁc responsibility rests authors. badrinarayanan handa cipolla segnet deep convolutional encoder-decoder architecture robust semantic pixel-wise labelling. technical report arxiv. badrinarayanan mishra cipolla understanding symmetries deep networks. technical report arxiv. accepted nips workshop optimization machine learning neyshabur salakhutdinov srebro path-sgd path-normalized optimization deep neural networks. advances neural information processing systems accepted publication.", "year": 2015}