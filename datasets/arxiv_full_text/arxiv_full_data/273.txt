{"title": "Guiding Reinforcement Learning Exploration Using Natural Language", "tag": ["cs.AI", "cs.CL", "cs.LG", "stat.ML"], "abstract": "In this work we present a technique to use natural language to help reinforcement learning generalize to unseen environments. This technique uses neural machine translation, specifically the use of encoder-decoder networks, to learn associations between natural language behavior descriptions and state-action information. We then use this learned model to guide agent exploration using a modified version of policy shaping to make it more effective at learning in unseen environments. We evaluate this technique using the popular arcade game, Frogger, under ideal and non-ideal conditions. This evaluation shows that our modified policy shaping algorithm improves over a Q-learning agent as well as a baseline version of policy shaping.", "text": "humans also extremely proﬁcient generalizing many states often language aids endeavor. work human language learn human-like state abstractions enhance reinforcement learning unseen environments. that neural machine translation techniques—speciﬁcally encoder-decoder networks—to learn generalized associations natural language behavior descriptions state/action information. model thought model generalized action advice augment state interactive machine learning algorithm make effective unseen environments. work choose modify policy shaping interactive machine learning algorithm learns human critique evaluate technique using arcade game frogger. speciﬁcally evaluate technique performs base q-learning algorithm version policy shaping uses demonstrations examples policy critique task learning unseen frogger maps variety situations summarize main contributions paper follows show neural machine translation used create generalized model action advice show model used augment policy shaping enable reinforcement learning agents better learn unseen environments perform evaluation method arcade game frogger several previously unseen maps using unreliable synthetic oracles meant simulate human trainers. work primarily related bodies artiﬁcial intelligence research interactive machine learning knowledge transfer reinforcement learning. interactive machine learning algorithms knowledge provided human teachers help train machine learning models. allows human experts help train intelligent agents thus enabling agents learn faster would left learn own. typically human teachers interact agent providing either demonstrations correct behavior work present technique natural language help reinforcement learning generalize unseen environments. technique uses neural machine translation speciﬁcally encoder-decoder networks learn associations natural language behavior descriptions state-action information. learned model guide agent exploration using modiﬁed version policy shaping make effective learning unseen environments. evaluate technique using popular arcade game frogger ideal non-ideal conditions. evaluation shows modiﬁed policy shaping algorithm improves q-learning agent well baseline version policy shaping. interactive machine learning algorithms seek augment machine learning human knowledge order enable intelligent systems better make decision complex environments. algorithms allow human teachers directly interact machine learning algorithms train learn tasks faster would able own. typically humans interact systems either providing demonstrations positive behavior intelligent agent learn from providing online critique agent explores environment. techniques proven effective sometimes difﬁcult trainers provide required demonstrations critique. demonstrations require trainer possess in-depth prior knowledge system environment trainers provide hundreds instances feedback agent begins utilize issue compounded considers training must occur environment agent ﬁnds work seek reduce burden human trainers using natural language enable interactive machine learning algorithms better generalize unseen environments. since language primary ways humans communicate using language train intelligent agents come naturally human teachers using demonstrations critique. proposed approach natural language instruction also need given online agent learning. allowing instruction given directly critique agent’s behavior work seeks improve upon methods enabling algorithms learn natural language addition demonstrations critique. work using natural language augment machine learning algorithms. much work done using natural language instructions help reinforcement learning agents complete tasks efﬁciently. early works area focused learning mappings instructions speciﬁc control sequences learning environments previous work language used mainly used instruct complete speciﬁc task speciﬁc environment. words language state tightly coupled. main work differs work seeking language abstraction tool. work focuses exploring language used help reinforcement learning agents transfer knowledge unseen environments. recent work examined language help reinforcement learning agents environmentagnostic way. example work done using high-level task speciﬁcations engineer environmentagnostic reward functions improve learning also techniques sentiment analysis used bias agent exploration improve learning unseen environments techniques however require additional information environment descriptions object types environment always readily available. technique relaxes requirement using neural machine translation learn relationships natural language action/state descriptions parts state space. work closely related involves using deep q-learning identify language representations help reinforcement learning agents learn unseen environments technique however also requires knowledge environment provided order learn representations. technique require additional information provided domain author state annotations generated human teachers. reinforcement learning reinforcement learning technique used solve markov decision process tuple possible world states possible actions transition function reward function discount factor reinforcement learning ﬁrst learns policy deﬁnes actions taken state. work q-learning uses q-value estimate expected future discounted rewards taking action state agent explores environment q-value updated take account reward agent receives state. paper boltzmann exploration select actions reinforcement learning agent take training. using boltzmann exploration probability agent choosing particular action training calculated eq/τ temperature constant controls whether agent prefer random exploration exploration based current q-values. policy shaping paper build upon policy shaping framework technique incorporates human critique reinforcement learning. unlike techniques reward shaping policy shaping considers critique signal evaluates whether action taken state desirable rather whether resulting state desirable. policy shaping utilizes human feedback maintaining critique policy calculate probability action taken given state according human feedback signal. during learning probability agent takes action calculated combining critique policy used policy shaping generated examining consistent feedback certain actions are. action receives primarily positive negative critique critique policy reﬂect greater lower probability respectively explore action during learning. encoder-decoder networks encoder-decoder networks used frequently areas machine translation learn convert sets input sequences desired output sequences. work encoder-decoder networks translate state/action descriptions written natural language machineunderstandable state/action information natural language describes. example input network could natural language describing layout grid environment action taken speciﬁc state desired output network would speciﬁc state action representation used learning agent. networks encoder network decoder network. generative architecture component neural networks work conjunction learn translate input sequence output sequence this encoder network ﬁrst learns encode input vector ﬁxed length context vector context vector meant encode important aspects input sequence decoder producing desired output sequence. vector used input second component network decoder learns iteratively decode vector target output setting learning problem particular thought vector encodes high-level concept information help decoder construct general state representations input sequence. mentioned previously primary disadvantages interactive machine learning humans must retrain agent whenever encounters environment. address issue show encoder-decoder network used learn language-based critique policy. high level overview technique seen figure technique works ﬁrst humans generate annotated states actions interacting single learning environment ofﬂine thinking aloud actions performing. annotations used train encoder-decoder network create language-based critique model. model queried agent exploring environments receive action advice guide towards states potentially high rewards. done even learning agent encounters states explicitly seen used train language-based critique model. steps discussed greater detail below. acquiring human feedback typically training agent using critique requires large amount consistent online feedback order build critique policy model human feedback speciﬁc problem. words human trainer would normally required watch agent learning provide feedback used real time improve agent’s performance. critique normally comes form discrete positive negative feedback signal associated given state action. provides agent little opportunity generalize unseen environments since feedback tightly coupled state information. address this technique uses natural language means generalize feedback across many possibly unknown states. training encoder-decoder model general critique policy refer language-based critique policy enables agent receive action advice potential state ﬁnds types data order create policy examples actions taken environment natural language describing action taken. information gathered humans interact agent’s learning environment providing natural language descriptions behavior. many ways humans potentially provide state action annotations. instance humans could provide full episode behavior along behavior annotations. also possible humans provide incomplete trajectories even simply examples single actions along natural language annotations. regardless collected state/action demonstrations provided human stored later used positive feedback signal language used help generalize feedback signal many states. example process consider environment agent tasked dodging obstacles. assume environment learning agent move four cardinal direction down left right. order gather necessary data learn feedback policy human trainer presented different obstacle initializations tasked providing short behavior examples navigating them. trainer could provide feedback action took fact. example feedback might description dodging obstacle coming beside describing action move obstacle approaching side agent. training encoder-decoder network annotations generated possible directly associate language information state action information. paired data used train encoderdecoder network. speciﬁcally natural language descriptions used inputs network network tasked reconstructing state action associated description. asking network reconstruct state action network learns identify common elements shared similar inputs importantly elements input natural language sequences relate certain regions output sequence state action information. enables network learn high-level concept information enables generalize natural language advice unseen states. utilizing language-based critique policy ultimate goal work language-based critique policy speed learning unseen environments. that language-based critique policy conjunction policy shaping. recall reinforcement learning agent using policy shaping makes decisions using distinct pieces information probability performing action based action’s current q-value probability performing action based human critique policy. here language-based critique policy learned early take place standard critique policy normally used policy shaping. language-based critique policy used framework must able calculate probability performing action state even never seen before. whenever agent encounters state query language-based critique policy probability performing action state given natural language input; however must ﬁrst determine piece feedback feedback used training applicable current state. natural language utterance training action agent perform current state calculate probability model reconstructing agent’s current state performing said action. think probability well utterance describes performing speciﬁc action speciﬁc state. whichever utterance associated action results overall largest probability used network input create action distribution. create action distribution calculate following original policy shaping algorithm critique policy constantly updated agent learning. since language-based critique policy trained ofﬂine opportunity update itself lead agent blindly following poor feedback. address this make parameter equation control much weight place knowledge extracted language-based critique policy. practice found algorithm performs well initialized small value increases course learning. cause agent begin learning trusting language-based critique policy shift towards relying experience time goes allows evaluate technique examine performs training vitual agents play arcade game frogger. speciﬁcally seek show using natural language augment policy shaping enables reinforcement learning agents speed learning unknown environments. section discuss evaluation compare agents trained technique agents trained using q-learning algorithm well agent trained using baseline policy shaping algorithm access behavior observations. frogger experiments arcade game frogger test domain. chose frogger discrete environment still quite complex speciﬁc mechanics environment. learning agent’s goal environment move bottom environment navigating obstacles world. world obstacles move following pattern. obstacles alternating rows move space either left right every time step. moving outside bounds getting falling water result agent’s death imposes reward penalty reaching goal earns agent reward move taken environment result small reward pentaly environment agent take actions move down left right choose nothing. test technique three different frogger environments. environments shown figures differ based obstacle density. speciﬁcally evaluate performance maps spaces chance containing obstacle chance containing obstacle chance containing obstacle. addition evaluate agent performance environments different conditions deterministic condition actions execute normally stochastic condition actions chance executing normally chance agent’s action fails executes different action instead. data collection since human teachers generate natural language used training possible mistakes made. therefore important examine effect imperfect teachers technique. difﬁcult control type error using actual humans experiments simulated human oracles generate required training observations natural language. create behavior traces required training trained reinforcement learning agents random starting positions move forward dodging obstacles training figure chose speciﬁc task help eliminate map-speciﬁc strategies learned using agent trained navigate complete environment. help eliminate mapspeciﬁc strategies states recorded training examples used remainder experiments encompass grid surrounding agent. done help prevent encoder-decoder network making spurious associations natural language annotations potentially unrelated regions state space. since using simulated humans generate state action traces grammar create natural language annotations system require. grammar constructing following technique used technique uses natural language utterances generated humans create grammar variances human language preserved codiﬁed. order produce sentence grammar must ﬁrst provided state action information grammar identiﬁes appropriate grammar rule construct natural language sentence. grammar constructed grammar rule produce large number unique sentences. using information grammar produce natural language sentence describing states actions. introducing uncertainty natural language annotations generated grammar. speciﬁcally create different language-based critique models using following simulated teachers teachers correct grammar rule provide feedback time random rule provide feedback time teachers correct grammar rule provide feedback time random rule provide feedback time allow evaluate robust technique potential errors contained natural language feedback provided human trainers. feel also helps mitigate regularity often present using synthetic grammars. important note training sets generated using behavior traces. makes performance resulting critique policies directly comparable. error injected training duplicate training examples removed. provide evidence variability grammar respect training sets also examined often sentences repeated training set. training seen sentence comprised total training contained total training examples. average sentence repeated time. training seen sentence comprised training examples. average sentence repeated time. disparity training size duplicate training examples removed errors introduced. training using dataset train encoder-decoder network ephocs. speciﬁcally embedding encoder-decoder network attention comprised two-layer recurrent neural networks composed long shortterm memory cells containing hidden units embedding size mentioned previously network learns translate natural language descriptions state action information. experiments network learns translate natural language generated grammar provided state action information case grid surrounding agent well action performed state. evaluation experiments using four intelligent agents. ﬁrst baseline q-learning agent access human feedback refer q-learning agent. second agent trained using policy shaping refer observation-based critique agent. agent access state action information used train language-based critique policy uses positive action feedback signal. help guard poor examples training agent also uses parameter control much weight given action examples agent’s experience. ﬁnal agents trained using technique refer language-based critique accuracy agent language-based critique accuracy agent depending training used generate feedback oracle agent used learning. agents evaluated three unseen frogger maps deterministic stochastic conditions described previously. test cases meant simulate agent performs simple (determindeterministic test case agent trained episodes. stochastic test case agent trained episodes. test cases learned policy evaluated every episodes total cumulative reward earned episode averaged total runs. results results agent deterministic frogger maps seen figure results agent stochastic frogger maps seen figure language-based critique agent observation-based critique agent tested several initial values schedules increasing parameters. graphs show best results achieved experiments. seen figure language-based critique agents converge much faster q-learning agent observation-based critique agent. interestingly accurate language-based critique agent outperformed accurate language-based critique agent map. important note observation-based critique agent also consistently outperforms q-learning agent meaning using observations still provides beneﬁt training unseen environments. stochastic versions test frogger maps. experiments language-based critique agents outperform agents used testing. contrary performance deterministic environments accurate language-based critique agent accurate language-based critique agent performed similarly environments. also interesting note language-based critique agents ones converge training episodes. observation-based critique agent outperforms q-learning agent. similar performance deterministic environments shows access behavior observations still provides amount beneﬁt generalizing behavior unseen environments. discussion ﬁrst thing note across test cases language-based critique agents either outperformed observation-based critique agent baseline q-learning agent. shows natural language provides knowledge useful generalizing unseen environments cannot obtained simply looking past observations. addition shows technique robust complexity learning environment well language error present human trainers. signiﬁcant result provides evidence encoder-decoder network identify relevant features natural language annotations used training even dataset contains large amount noise. differences performance especially pronounced deterministic stochastic maps well deterministic stochastic maps. cases language-based critique agents drastically outperformed baselines. consistently positive results across deterministic stochastic environments shows powerful tool language respect generalizing knowledge across many types environments. result needs discussed performance accurate language-based critique agent respect accurate language-based critique agent deterministic maps. cases accurate agent outperformed accurate agent contrary intuition accurate agent consistently outperform accurate agent additional error contained latter training set. hypothesize behavior explained model overﬁtting causing erratic behavior cases. encoder-decoder network overﬁtting part training possible increased error introduced accurate agent regularizing effect network allowed better generalize states found maps. results experiments provide strong evidence technique effective utilizing language help learn generalizable knowledge technique without limitations. first experiments made several simplifying assumptions necessary order control variance accompanies human teachers. using grammar able control amount variance present annotations used training. attempted mitigate encoding grammar large amount variance training language-based critique model using training sets containing natural language error naturally occurring language likely contain variation present grammar. addition natural language explanations used train language-based critique model used annotate single actions. typically humans provide explanations actions context larger goal-based behavior trajectory level individual actions. improve system enable learn state/action explanations varying levels granularity. also explored technique applied discrete environments. using discrete environment makes easy associate natural language annotations state action information. done continuous environment would much difﬁcult determine state action associated certain natural language annotations. finally explored technique used generalize unseen environments within domain. unclear however technique could used transfering knoweldge agents learning similar tasks different domains. language powerful tool humans generalize knowledge across large number states. work explore language used augment machine intelligence give intelligent agents expanded ability generalize knowledge unknown environments. specifically show neural machine translation techniques used give action advice reinforcement learning agents generalizes across many different states even seen before. experiments shown generalized model advice enables reinforcement learning agents quickly learn unseen environments. addition technique gives human teachers another train intelligent agents. ability augment human demonstration critique human feedback potential signiﬁcantly reduce amount effort required order train intelligent agents. makes task training intelligent agents approachable potential human trainers. even possible task could crowdsourced future drastically reducing effort part individual trainer making types agent training methods appealing. work hope help bring language barrier exists humans intelligent agents. removing barrier hope enable humans transfer complex knowledge intelligent agents allow learn even complex tasks complex unknown environments.", "year": 2017}