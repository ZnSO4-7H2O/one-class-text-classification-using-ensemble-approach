{"title": "Teaching Deep Convolutional Neural Networks to Play Go", "tag": ["cs.AI", "cs.LG", "cs.NE"], "abstract": "Mastering the game of Go has remained a long standing challenge to the field of AI. Modern computer Go systems rely on processing millions of possible future positions to play well, but intuitively a stronger and more 'humanlike' way to play the game would be to rely on pattern recognition abilities rather then brute force computation. Following this sentiment, we train deep convolutional neural networks to play Go by training them to predict the moves made by expert Go players. To solve this problem we introduce a number of novel techniques, including a method of tying weights in the network to 'hard code' symmetries that are expect to exist in the target function, and demonstrate in an ablation study they considerably improve performance. Our final networks are able to achieve move prediction accuracies of 41.1% and 44.4% on two different Go datasets, surpassing previous state of the art on this task by significant margins. Additionally, while previous move prediction programs have not yielded strong Go playing programs, we show that the networks trained in this work acquired high levels of skill. Our convolutional neural networks can consistently defeat the well known Go program GNU Go, indicating it is state of the art among programs that do not use Monte Carlo Tree Search. It is also able to win some games against state of the art Go playing program Fuego while using a fraction of the play time. This success at playing Go indicates high level principles of the game were learned.", "text": "figure example capturing pieces white’s stones upper left connected adjacency form single group black places stone indicated grid point group surrounded meaning longer empty grid points adjacent entire group removed board figure example positions game moves passed moves passed right panel seen white gaining control territory center board black gaining inﬂuence left right edges. mastering game remained long standing challenge ﬁeld modern computer systems rely processing millions possible future positions play well intuitively stronger ‘humanlike’ play game would rely pattern recognition abilities rather brute force computation. following sentiment train deep convolutional neural networks play training predict moves made expert players. solve problem introduce number novel techniques including method tying weights network ‘hard code’ symmetries expect exist target function demonstrate ablation study considerably improve performance. ﬁnal networks able achieve move prediction accuracies different datasets surpassing previous state task signiﬁcant margins. additionally previous move prediction programs yielded strong playing programs show networks trained work acquired high levels skill. convolutional neural networks consistently defeat well known program indicating state among programs monte carlo tree search. also able games state playing program fuego using fraction play time. success playing indicates high level principles game learned. ancient deeply strategic board game notable board games human experts still comfortably ahead computer programs terms skill. predicting moves made expert players interesting challenging machine learning task give brief introduction rules defer comprehensive account rules. number different rulesets subtly differ moves illegal game scored focus generalities player game usually played grid based board. board typically starts empty game start stones pre-placed board give player starting advantage. black plays ﬁrst placing black stone grid point chooses. white places white stone unoccupied grid point play continues manner. players pass instead placing stone case turn skipped opponent make second move. stones canmoved placed however player capture group opponent’s stones surrounding group stones. case surrounded group removed board shown figure broadly speaking objective capture many grid points board possible either occupying surrounding stones. game played players pass turn case players come agreement player control grid points game scored. capturing mechanic possible create inﬁnite ‘loops’ play players repeatedly capture replace pieces. rulesets include rules prevent occurring. simplest version rule called simple-ko rule states players cannot play moves would recreate position existed previous turn. rulesets contain stronger versions rule called super-ko rules prevent players recreating previously seen position. figure shows example board positions could occur game played. state computer programs fuego pachi crazystone achieve skill level strong amateur players still behind professional level play. difﬁculty computers domain relative board games chess often attributed things. first large number possible moves. players possible starting moves. board ﬁlls number possible moves decreases expected remain hundreds late game. contrast chess number possible moves might stay around ﬁfty. second good heuristics assessing player winning found. chess examining pieces player left board plus heuristics assess player better position serve good estimator winning. counting number stones player poor indicator winning proven difﬁcult build effective rules estimating player stronger position. ﬁculties. mcts algorithms evaluate positions using simulated ‘playouts’ game played completion current position assuming players move randomly follow cheap best move heuristic. many playouts carried assumed good positions ones program winner majority them. finding improvements strategy seen great deal recent work main source progress building playing programs. recent survey mcts algorithms survey modern playing systems. human experts rely heavily pattern recognition playing expert players gain strong intuitions parts board fall whose control best moves consider glance without needing mentally simulate possible future positions. provides sharp contrast typical computer algorithms simulate thousands possible future positions make minimal pattern recognition. gives reason think developing pattern recognition algorithms might missing element needed close performance computers humans. particular pattern recognition systems could provide ways combat high branching factor might possible prune many possible moves based patterns current position. would result system analyzes much ‘human like’ manner eliminating candidate moves based learned patterns within board ‘thinking ahead’ remaining promising moves. work train deep convolutional neural networks detect strong moves within position training move prediction task predicting expert human players would choose move faced particular position. outside playing move prediction interesting machine learning task right. expect target function highly complex since fair assume human experts think complex non-linear ways choose moves. also expect target function non-smooth minor change position could expected dramatically alter moves likely played next. properties make learning task challenging however argued acquiring ability learn complex non-smooth functions particular importance comes solving properties also motivated attempt deep learning approach argued deep learning well suited learning complex non-smooth functions deep learning allowed machine learning algorithms approach human level pattern recognition abilities image previous work move prediction typically made feature construction shallow neural networks. feature construction approach involves characterizing legal move large number features. features include many ‘shape’ features take value stones around move question match predeﬁned conﬁguration stones otherwise. stone conﬁgurations small nearest three squares large entire board. large numbers stone conﬁgurations harvested ﬁnding commonly occurring stone conﬁgurations training data. shape features augmented hand crafted features distance move question edge board whether making move capture lose stones distance previous moves ect. finally model trained rank legal moves based features. work following approach includes depending complexity model used researchers seen accuracies move prediction strong amateur players. best algorithm know achieved using features manner non-linear ranking model. attempts neural networks move prediction completed before. work trained neural network predict expert moves using hand constructed features preprocessing techniques reduce dimensionality data layer neural network predicted whether particular move professional player would made not. network able achieve accuracy test professional games. work closely resembles work done layer convolutional networks trained move prediction. networks typically included convolution layer ﬁfteen ﬁlters followed another convolutional layer ﬁlter. layer zero-padded input output shape. softmax function applied ﬁnal layer output interpreted probabilities expert player would place stone different grid points. achieved accuracy using network took current board position previous moves input. using ensemble accuracy reached work differ several important ways. much deeper networks propose several novel position encoding schemes network designs improve performance. important strategy tying weights within network ‘hard code’ particular symmetries expect exist good move prediction functions. also careful previously made moves input. motivations choosing first classiﬁers trained using previous moves input might come rely simple heuristics like ‘move near area previous moves made’ instead learning evaluate positions based current stone conﬁguration. might improve accuracy fundamental motivation whether classiﬁer capture knowledge ability borrow knowledge experts looking past moves cheapens objective. game theoretic perspective previous moves inﬂuence current best move information needed play well. secondly using previous move likely reduce performance comes playing stand alone player. play opponent network liable make much worse moves would made experts therefore coming rely assumption previous moves made experts likely yield results. potential problem also noted work also ﬁrst perform evaluation across datasets providing opportunity compare classiﬁers trained datasets differ terms playing abilities move prediction accuracy. several works mentioned analyze comment strength move predictor program stand alone player. researchers found neural network consistently defeated conclude ‘...belief conﬁrmed local move predictor sufﬁcient play strong game.’ work also reports move predictor beaten researchers report players estimated move predictor ranking report rates another computer opponent. give formal results state systems make strong stand alone playing programs. general seems past approaches move prediction resulted programs much skill. done networks trained take input representation current position output probability distribution grid points board interpreted probability distribution possible places expert player could place stone. testing probability given grid points would illegal move either occupied stone simple-ko rule zero remaining outputs renormalized. follow encoding current position binary matrices. ﬁrst matrix ones indicating location stones player play second matrix ones marking opponent’s stones are. depart additionally encoding presence ‘simpleko constraint’ present third matrix. simple-ko constraints refers grid points current player allowed place stone simpleko rule. simple-ko constraints reasonably sparse. dataset professional games moves made simple-ko constraint present. however simple-ko constraints often featured tactics hypothesize still important include input. elect encode move constraints beyond ones created simple-ko rule meaning constraints stemming superko rules rare harder detected rulesetdependent less prominent tactics. thus input three channels height width following well work found useful feature tried encoding board channels stones encoded based number ‘liberties’ number empty squares opposing player would need occupy capture stone. case channels encode current player’s pieces liberties channels opponent’s pieces channel marks simple-ko constraints before. classiﬁer trained predict players choose pass move. passing extremely rare throughout game practically never beneﬁcial pass move thus passing mainly used game signal willingness game. means players usually pass beneﬁcial moves left played situation players agree game over. modeling situation occurs beyond scope work. effective networks composed many convolutional layers. since input size found important zero input convolutional layer stop size outputs higher layers becoming exceedingly small. brieﬂy experimented variations found zero-padding layer ensure layer’s output shape reasonable choice. general using fully connected layer effective convolutional layer used however found performance networks using convolutional fully connected layer decreased networks increased size. networks increased size using fully connected layer network became unhelpful. thus architectures many convolutional layers followed single fully connected layer. experience using rectiﬁer activation function slightly effective using tanh activation function. limited primarily running time almost cases increasing depth number ﬁlters network increased accuracy. implies limit achieved merely scaling network. found using many smaller convolutional ﬁlters deep networks efﬁcient terms trading runtime accuracy using fewer larger ﬁlters. brieﬂy experimented non-convolutional networks found much harder train often requiring epochs training approximate second order gradient descent methods getting worse results. makes think convolutional architecture important well task. neural networks described ﬁrst convolutional layer zero input. current board representation zeros represent empty grid points zero padding results board ‘appearing’ surrounded empty grid points. words ﬁrst layer cannot capture differences stones next edge empty grid point. solution reserve additional channel encode bounds squares padded input. scheme completely empty eighth channel added input. then instead zero padding input input padded ones around eighth channel padded zeros around channels. allows network distinguish bound squares empty ones. experimented padding input ones channel used opponent’s pieces zeros channels found give worse results. grid points board legal moves either already occupied stone rules. therefore points eliminated possible places expert move priori. testing account this knowledge present network training. informal experiments show classiﬁer able learn avoid predicting illegal moves close accuracy still speculate accounting knowledge training might beneﬁcial simplify function classiﬁer trying learn. accomplish zero outputs layer illegal apply softmax operator make predictions backprop gradient across outputs learning. figure visualization weights randomly sampled channels randomly sampled convolution ﬁlters layer convolutional neural network trained gogod dataset. network trained without reﬂection preservation enforced. seen even without weight tying ﬁlters column learned acquire symmetric property. effect even stronger weight visualization board reﬂected across either diagonal axis game sense changed. transformed position could played identical manner original position would result scores player. therefore would expect that board reﬂected classiﬁer’s output reﬂected manner. exploiting insight would train various combinations reﬂections training data increasing number data points could train eight folds. tactic comes serious cost ﬁnal network took four days train epochs. increasing data eight folds means would require almost month train number epochs. better route ‘hard wire’ reﬂectional preserving property network. done carefully tying weights property exists layer. convolutional layers means tying weights convolutional ﬁlter invariant reﬂections. words enforcing weights symmetrical around horizontal vertical diagonal dividing lines. illustration kinds weights produces found figure desired effect consider application convolutional ﬁlter input channel. weights convolutional ﬁlter index square patch input index column input figure applying convolutional ﬁlter upper left patch input upper right patch input reﬂected across y-axis hard encode horizontal reﬂection preservation need ensure means must patch/weight top-to-bottom right-to-left manner. pre-activation output ﬁlter applied xijwij. input reﬂected horizontally patch inputs reﬂected located opposite side vertical axis. assuming convolutional ﬁlter applied dense manner convolutional ﬁlter applied reﬂected patch. necessary sufﬁcient application convolutional ﬁlter reﬂected patch pre-activation value before meet goal layer preserve horizontal reﬂections applied input. thus xiwij xij. thus require short convolutional ﬁlter symmetric around vertical axis. illustrated figure similar proof built vertical diagonal reﬂections. fully connected layers property also encoded using weight tying. first arrange outputs layer square shape. output unit refer weight given input unit. output unit horizontal reﬂection refer weight given input output pre-activation value aijxij height width input also assumed square shape. achieve invariance horizontal reﬂections require pre-activation value equal input reﬂected across vertical axis therefore that ready application image processing often expect target function invariant horizontal reﬂections. minor adjustment could make layer invariant reﬂections rather reﬂection preserving meaning input reﬂected output change. referring figure matter ensuring could build network lower layers preserve horizontal reﬂections ﬁnal layer invariant horizontal reﬂections. resulting network invariant horizontal reﬂections half number parameters untied network. provides account expected reﬂectional invariance property without double amount training data. test network datasets ﬁrst games disk dataset consisting professional games. games played variety rulesets long time controls. second dataset consists games played high ranked players server games played japanese rules slightly lower player rankings generally much faster time controls. datasets previous work ﬁeld typically used either other. available data gogod dataset select games dataset number positionmoves pairs dataset trained roughly equal. yields million move-positions pairs dataset. partition datasets test train validation sets containing position-move pairs disjoint games testing validation remaining training. validation monitor learning experiment hyperparameters. found vanilla gradient descent effective task although found important anneal learning rate towards learning. dropout part would increase training time part overﬁtting primarily problem. also regularization. convolutional fully connected layers biases initialized zero weights drawn normal distribution mean standard deviation figure computation single output unit weights input computation output unit opposite side vertical axis weights input reﬂected across y-axis. hard encode horizontal reﬂection preservation need ensure easy examining equations shown means require ect. thus need demonstrated figure similar proof built vertical diagonal reﬂections applied biases layer. tactics also readily generalize cases multiple channels input output. encoding reﬂection invariance ties weight another weight accounting three possible reﬂections weight tied weights. likewise weight tying convolutional ﬁlters reduced number parameters ﬁlter approximately eighth. thus reduced number parameters network eight folds. averaging weights requires computation experience minor cost relative forward backward propagating batches training data. alternative method enforcing horizontal reﬂection preservation would apply ﬁlters half input reﬂect ﬁlters apply half input. would allow meeting requirement without tying weights experimented approach. table ablation study. train medium scale network consisting four convolutional layers fully connected layer gogod dataset excluding different features. networks judged based accuracy average probability assign correct move average rank give correct move relative possible moves test set. liberty encoding reﬂection preserving techniques useful suggested techniques improve average rank average probability. test design choices made performed ablation study. study done ‘medium scale’ network allow multiple experiments conducted quickly. network convolutional layer ﬁlters three convolutional layers ﬁlters fully connected layer. networks trained mini-batch gradient descent batch size using learning rate epochs epochs took nvidia gpu. results table reﬂection preserving technique extremely effective leaving dropped accuracy almost liberty encoding scheme improved performance essential leaving dropped performance remaining optimizations less dramatic impact still contributed non-trivially. together additions increased overall accuracy increased accuracy relative using liberties encoding table results layer dcnn train test gogod dataset. rank refers average rank expert’s move given probability refers average probability assigned expert’s move. figure negative likelihood validation training layer dcnn trained gogod dataset. vertical lines indicate learning rate annealed. improvement validation less slowed halt despite using epochs training. table results layer dcnn train test dataset. rank refers average rank expert’s move given probability refers average probability assigned expert’s move fully connected layer. network used optimizations enumerated previous section. network trained seven epochs learning rate epochs epoch batch size took roughly four days single nvidia gpu. figure shows learning speed measured validation set. network trained evaluated gogod dataset shown table table respectively. knowledge best reported result gogod dataset using ensemble shallow networks best dataset using feature engineering latent factor ranking results completed recent versions datasets much compared results surpassed results margins respectively. additionally done without using previous moves input. analyze impact using feature report accuracy networks dropped feature removed making seem like networks relied heavily feature. also examine gogod test accuracy network function number moves completed game figure accuracy increases predictable opening moves decreases complex middle game increases board ﬁlls number possible moves decreases towards table rates three dcnns level using chinese japanese rules. matchup games played. gogod refer full scale network trained named dataset gogod small refers exclude none network section even smaller dcnn able consistently defeat figure accuracy allowed ﬁxed number guesses gogod test set. sharp increase accuracy occurs given additional guesses indicates that neural network’s best guess wrong right answer often among next guesses. however occasions right move ranked best move network. game. finally examine accurate network allowing network make multiple guesses test figure encouraging note that network’s highest ranking move incorrect second third highest ranked move often correct. however times expert move among ranked moves network. clear exactly well human expert would perform task seems likely human expert would practically always able guess another expert would move given guesses. thus think dcnn reached human level performance task. networks trained successful move predictors necessary mean strong players. potential problems. first during game opponent classiﬁer itself liable make moves create positions uncommon games professionals. since networks tested trained kinds positions table winrates playing layer dcnn trained gogod dataset fuego chinese rules. fuego given seconds move four gigabytes threads intel i-mq processor. pondering turned off. matchup games played. able even games opponent indicates high degree skill acquired. guarantee continue perform well occurs. second even classiﬁer able predict expert player level move time moves extremely poor could still terrible player. test strength networks players played computer programs feugo test ﬁnal network trained data gogod data smaller ‘exclude none’ network section results found table table fuego respectively. complication dcnns capability pass turn. therefore game indeﬁnitely networks eventually good moves play start making suicidal moves thus losing game. work around issue allow fuego resign additionally dcnn pass turn whenever opponent thus ending game. games scored using opposing engine’s scoring function. results promising. even though networks playing using ‘zero step look ahead’ policy using fraction computation time opponents still able play better take games away fuego. settings might play around ranking fuego implies networks achieving ranking approximately kyu. human player reaching ranking would normally require years study. indicates sophisticated knowledge game acquired. also indicates great potential program integrates information produced network. smaller network trained comfortably defeated despite less accurate previous work move prediction. thus seems likely choice previous move input helped move predictors generalize well predicting expert player’s moves playing stand alone players. might also attributed deep learning based approach. deep learning algorithms shown particular beneﬁt sample distributions relates situation since networks viewed learning play biased sample positions. network trained gogod dataset performed slightly better trained dataset. might expect since dataset contains many games speed liable lower quality. work introduced application deep learning problem predicting moves made expert players. contributions also include number techniques helpful task including powerful weight tying technique take advantage symmetries expect exist target function. networks state move prediction despite using previous moves input play impressive amount skill even though future positions explicitly examined. great deal could done extend work. scaling likely effective. limited size networks keep training time using data larger networks likely increase accuracy. completed preliminary exploration hyperparameter space think better network architectures could found. curriculum learning integration reinforcement learning techniques might provide avenues improvement. excellent skill achieved minimal computation could allow approach used strong fast playing mobile application. obvious next step integrate dcnn full ﬂedged playing system. example dcnn could parallel mcts program used provide high quality priors strongest moves consider are. system would ﬁrst bring sophisticated pattern recognitions abilities playing strong potential ability surpass current computer programs. references araki nobuo yoshida kazuhiro tsuruoka yoshimasa tsujii jun’ichi. move prediction maximum entropy method. computational intelligence games bengio yoshua bastien fr´ed´eric bergeron arnaud boulanger-lewandowski nicolas breuel thomas chherawala youssouf cisse moustapha cˆot´e myriam erhan dumitru eustache jeremy deep learners beneﬁt out-of-distribution examples. aistats browne cameron powley edward whitehouse daniel lucas simon cowling peter rohlfshagen philipp tavener stephen perez diego samothrakis spyridon colton simon. survey monte carlo tree search methods. computational intelligence games enzenberger markus muller martin arneson broderick segal richard. fuego open-source framework board games engine based monte carlo tree search. computational intelligence games hinton geoffrey srivastava nitish krizhevsky alex sutskever ilya salakhutdinov ruslan improving neural networks preventing co-adaptation feature detectors. arxiv preprint arxiv.", "year": 2014}