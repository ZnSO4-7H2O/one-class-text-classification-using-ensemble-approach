{"title": "Spontaneous Symmetry Breaking in Neural Networks", "tag": ["stat.CO", "cs.AI", "cs.CV", "cs.LG"], "abstract": "We propose a framework to understand the unprecedented performance and robustness of deep neural networks using field theory. Correlations between the weights within the same layer can be described by symmetries in that layer, and networks generalize better if such symmetries are broken to reduce the redundancies of the weights. Using a two parameter field theory, we find that the network can break such symmetries itself towards the end of training in a process commonly known in physics as spontaneous symmetry breaking. This corresponds to a network generalizing itself without any user input layers to break the symmetry, but by communication with adjacent layers. In the layer decoupling limit applicable to residual networks (He et al., 2015), we show that the remnant symmetries that survive the non-linear layers are spontaneously broken. The Lagrangian for the non-linear and weight layers together has striking similarities with the one in quantum field theory of a scalar. Using results from quantum field theory we show that our framework is able to explain many experimentally observed phenomena,such as training on random labels with zero error (Zhang et al., 2017), the information bottleneck, the phase transition out of it and gradient variance explosion (Shwartz-Ziv & Tishby, 2017), shattered gradients (Balduzzi et al., 2017), and many more.", "text": "propose framework understand unprecedented performance robustness deep neural networks using ﬁeld theory. correlations weights within layer described symmetries layer networks generalize better symmetries broken reduce redundancies weights. using parameter ﬁeld theory network break symmetries towards training process commonly known physics spontaneous symmetry breaking. corresponds network generalizing without user input layers break symmetry communication adjacent layers. layer decoupling limit applicable residual networks show remnant symmetries survive non-linear layers spontaneously broken. lagrangian non-linear weight layers together striking similarities quantum ﬁeld theory scalar. using results quantum ﬁeld theory show framework able explain many experimentally observed phenomena training random labels zero error information bottleneck phase transition gradient variance explosion shattered gradients many more. deep neural networks used image recognition tasks great success. ﬁrst kind alexnet many neural architectures proposed achieve start-of-the-art results image processing time. notable architectures include inception residual networks understanding inner working deep neural networks remains difﬁcult task now. discovered training progress ceases goes information bottleneck learning rate decreased suitable amount network goes phase transition. deep networks appear able regularize able train randomly labeled data zhang zero training error. gradients deep neural networks behaves white noise layers balduzzi many unexplained phenomona. found framework explain aforementioned results among many others. recent work showed ensemble behavior binomial path lengths resnets explained taylor series expansion ﬁrst order decoupling limit. found series approximation generates symmetry breaking layer reduces redundancy weights leading better generalization. resnet contain symmetry breaking layers architecture. suggest resnets able break symmetry communication layers. another recent work also employed taylor expansion investigate resnets show remnant symmetry subgroup survive relu layer weights thought scalar ﬁelds covariant remnant symmetry. emergence scalar ﬁeld intriguing perhaps troubling. scalar ﬁeld admit negative hessian eigenvalues loss minimum. historically paradox solved promoting scalar ﬁeld theory quantum ﬁeld theory incorporating einstein’s special relativity apply neural networks? illustrate residual network behaves like quantum ﬁeld theory. layer decoupling limit norm change feature representation layer upper bound deﬁnition layer decoupling. similar relativity speed particle bounded speed light. residual networks shown network decomposed subnetworks consisting possible paths layers veit quantum mechanics propagation particle points paths connecting points. residual network described quantum ﬁeld theory scalar ﬁeld justiﬁed. fact presence negative hessian eigenvalues drives phase transitions deep neural networks eigenvalues also observed organization paper follows. background deep neural networks ﬁeld theory given section section shows remnant symmetries exist neural network weights approximated scalar ﬁeld. experimental results conﬁrm theory given section review ﬁeld theory given appendix explicit example spontaneous symmetry breaking shown appendix show necessary sufﬁcient conditions preserving symmetry. explicitly include symmetry transformations equation investigate effects caused symmetry transformation input subsequent layers. suppose transformation matrix group note parameters estimated. write dependence obtained transformation input qtxt weights symmetry group covariant qtyt. requires conditions satisﬁed. first qtwtq− existence inverse trivial group second commutativity rtqt qtrt. example group afﬁne transformations statistical learning requires loss function minimized. written form mutual information training error kullback-leibler divergence. section approximate loss function continuum limit samples layers. deﬁne loss deﬁnition orthogonal group orthogonal group group matrices conditions obtain series expansion around minimum first since invariant. term series expansion must invariant suppose orthogonal group qtw. wiwi invariant. wiwi invariant einstein summation convention used coefﬁcient ﬁxed hamiltonian kinetic energy higher order terms negligible decoupling limit. model robust higher order terms neglected well. lagrangian density wiwi absorbed without loss generality. precisely lagrangian scalar ﬁeld ﬁeld theory. standard results scalar ﬁeld theory found appendix account effect learning rate employ results thermal ﬁeld theory identify temperature learning rate phenomenon profound implication. responsible phase transition neural networks generates long range correlation input output. details ﬁeld theory found appendix section show spontaneous symmetry breaking occurs neural networks. first show learning deep neural networks considered solely breaking symmetries weights. show non-linear layers preserve symmetries across nonlinear layers. show weight pairs adjacent layers within layer approximately invariant remnant symmetry leftover non-linearities. assume weights scalar ﬁelds invariant orthogonal group experimental results show deep neural networks undergo spontaneous symmetry breaking. theorem deep feedforward networks learn breaking symmetries proof operator representing sequence layers network formed applying repeatedly ai)xin. suppose symmetry group afﬁne aff. xout lxin xout computed single afﬁne transformation contains nonlinearity symmetry explicitly broken nonlinearity layers learn generalized representation input. kinetic term invariant transformation obtain invariance ∂twi replaced covariant derivative dtwi invariant dtwi) ∂twi) αb)i qbq−. ﬁelds introduced invariance responsible spontaneous symmetry breaking focus paper. consider further. theorem relu reduces symmetry invariant subgroup proof suppose denotes relu operator output acts input max. invariant invariant note transformed negative value passed relu already. loss function invariant need predicted output covariant similarly invariant loss rate require covariant following theorem shows pair weights adjacent layers considered invariant power series expansion. theorem neural network weights adjacent layers form approximate invariant suppose neural network consists afﬁne layers followed continuous non-linearity weights layer transform qtwtq− remnant symmetry qtrt rtqt. wtwt− considered invariant loss rate. recall ﬁrst layer corresponds after. therefore considered scalar ﬁeld remnant symmetry. remnant symmetry exact general. sigmoid functions approximation. crucial feature remnant symmetry continuous strong correlation inputs outputs generated spontaneous symmetry breaking. state goldstone theorem ﬁeld theory without proof. case adhere case remnant symmetry othogonal group invariant invariant. write lagrangian deep feedforward network weights responsible spontaneous symmetry breaking. standard ﬁeld theory results apply deep neural networks. review ﬁeld theory given appendix formalism spontaneous symmetry breaking given appendix spontaneous symmetry breaking splits weight deviations sets different behaviors. weights zero eigenvalues spectrum dominated small frequencies correlation function. weights hessian eigenvalues weights symmetry broken. appendix standard calculation ﬁeld theory shows correlation functions weights form gradient variance explosion prove results found shwartz-ziv tishby figure showed variance weight gradients grow orders magnitude near training. claim long range correlations generated spontaneous symmetry breaking. means gradient variance spontaneous symmetry breaking blows observed shwartz-ziv tishby recall correlation function corresponds weight deviaspace means feature representations product weights independent input. together large correlation across layers means feature maps highly correlated output. precisely behaviors observed shwartz-ziv tishby robustness deep neural networks neural networks resilient overﬁtting. recall ﬂuctuation weights arise sampling noise. measure model robustness. small value denotes weights’ resistance sampling noise. network overﬁt weights would sensitive sampling error. spontaneous symmetry breaking weights zero eigenvalues obey klein-gordon equation robustness model. zhang referred phenomenon implicit regularization. independent data distribution. therefore network learn data random labels also observed zhang identity mapping outperforms skip connections result residual unit’s output small. residual units decoupled leading small easier spontaneous symmetry breaking occur skip connection across residual units breaks additional symmetry. suppose identity respectively. output residual untis neither satisﬁes covariance observed orhan pitkow leads weight eigenvalues larger non-residual networks owing higher oscillation frequency correlation function. spontaneous symmetry breaking generates long range correlations weights feature representations across input samples layers feature representations almost independent input highly correlated output. result shown figure shwartz-ziv tishby further spontaneous symmetry breaking causes ﬂuctuation weights grow dramatically reﬂected increase variance gradients training shown figure shwartz-ziv tishby work solved puzzling mysteries deep learning showing deep neural networks undergo spontaneous symmetry breaking. ﬁrst attempt describe neural network scalar quantum ﬁeld theory. shed light many unexplained phenomenon observed experiments. observation deep networks learn random training labels error well generalized models many zero valued weights phenomenon feature maps becomes highly correlated output early layers direct consequences spontaneous symmetry breaking experimental results conﬁrm theory. also experiments validates theory. observation gradients highly oscillatory across layers network without skip connections oscillation reduced resnets conﬁrm form correlation function equation believe work provides understanding deep neural network motivate experimental theoretical investigations. shown predict performance deep network considering symmetry layers. gives practitioners section state relevant results ﬁeld theory without proof. lagrangian mechanics ﬁelds equations motion ﬁelds solution euler-lagrange equation result principle least action. action q|k| correlation function dominated values therefore hππi hand shown hσσi damped weight eigenvalues |m|. singularity correlation function means language group theory. symmetry broken elements orthogonal matrices independent continous symmetries number continuous broken example showed corresponds ﬁelds. even though formulated ﬁeld theory based decoupling limit resnets result inﬁnite correlation general applied even decoupling limit valid. direct result spontaneous symmetry breaking. state goldstone theorem without proof.", "year": 2017}