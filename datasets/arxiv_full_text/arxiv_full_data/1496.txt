{"title": "DeepStory: Video Story QA by Deep Embedded Memory Networks", "tag": ["cs.CV", "cs.AI", "cs.CL"], "abstract": "Question-answering (QA) on video contents is a significant challenge for achieving human-level intelligence as it involves both vision and language in real-world settings. Here we demonstrate the possibility of an AI agent performing video story QA by learning from a large amount of cartoon videos. We develop a video-story learning model, i.e. Deep Embedded Memory Networks (DEMN), to reconstruct stories from a joint scene-dialogue video stream using a latent embedding space of observed data. The video stories are stored in a long-term memory component. For a given question, an LSTM-based attention model uses the long-term memory to recall the best question-story-answer triplet by focusing on specific words containing key information. We trained the DEMN on a novel QA dataset of children's cartoon video series, Pororo. The dataset contains 16,066 scene-dialogue pairs of 20.5-hour videos, 27,328 fine-grained sentences for scene description, and 8,913 story-related QA pairs. Our experimental results show that the DEMN outperforms other QA models. This is mainly due to 1) the reconstruction of video stories in a scene-dialogue combined form that utilize the latent embedding and 2) attention. DEMN also achieved state-of-the-art results on the MovieQA benchmark.", "text": "question-answering video contents achieving human-level significant intelligence involves vision language real-world settings. demonstrate possibility agent performing video story learning large amount cartoon videos. develop video-story learning model i.e. deep embedded memory networks reconstruct stories joint scene-dialogue video stream using latent embedding space observed data. video stories stored long-term memory component. given question lstm-based attention model uses long-term memory recall best question-story-answer triplet focusing specific words containing information. trained demn novel dataset children’s cartoon video series pororo. dataset contains scene-dialogue pairs .-hour videos fine-grained sentences scene description story-related pairs. experimental results show demn outperforms models. mainly reconstruction video stories scene-dialogue combined form utilize latent embedding attention. demn also achieved state-of-the-art results movieqa benchmark. introduction question-answering problem important research intelligence many computational models proposed past decades. focused knowledge representation reasoning based natural language processing many hand-crafted syntactic semantic features recently deep learning methods started outperform traditional methods text domain using convolutional neural networks long short-term memory attention based deep models methods different previous approaches require feature engineering exploit large amount training data. performance improvements continuously extended image tasks however results video domain lagged compared text image settings. still methods datasets address video story used probablistic concept graph. built story learning models separately scenes dialogues videos fused final answer predictions model i.e. averaging answer prediction scores. late fusion sometimes performance degradation understanding video stories requires scene dialogue information together separately. point paper provides contributions video story problem. first construct novel large-scale video story dataset-pororoqa children’s popular cartoon videos series ‘pororo’. pororoqa properties make suitable test video story characteristics cartoon videos series simple clear coherent story structure small environment compared videos like dramas movies. dataset provides high-quality scene descriptions allow high-level video analysis. dataset consists video scene-dialogue pairs created videos hours total length fine-grained descriptive sentences scene descriptions multiple choice questions video stories. question coupled five possible answers; correct four incorrect answers provided human annotators. plan release dataset community. second propose video story learning model deep embedded memory networks demn reconstructs stories joint stream video scene-dialogue combining scenes dialogues sentence forms using latent embedding space. video stories stored long-term memory component read written given pair word level attention-based lstm evaluates best answer creating question-story-answer triplets using long-term memory focusing specific keywords. processes pass three modules learned supervised learning setting. test model different datasets pororoqa movieqa compare results various story models including human models memory networks. pororo think hides behind tree? pororo thinks loopy can’t find him. pororo thinks loopy know count. pororo thinks loopy counting till fifty. pororo thinks loopy playing game. pororo thinks loopy already found friends. game pororo friends playing? pororo friends playing hide seek. pororo friends sing song. pororo friends playing cricket. pororo friends making snow angles. pororo friends going sledding. experimental results show demn achieves state-of-the-art performances datasets reconstruction video stories scene-dialogue combined form using latent embedding attention. task statement figure shows overview video story task. regard video e.g. episode cartoon video series compositional material consisting video scenes v={vi}…|v| dialogues l={li}…|l| sequence image frames natural language sentence containing dialogue. work paired i.e. x={}…|x| |x|=|v|=|l|. also assume externally available sources included video available video story task. external information source ground-truth scene descriptions description multiple sentences describing scene video considering videos real environment fine-grained annotations model learn relationships ground-truth descriptions scenes videos training retrieve relevant descriptions test time newly observed scene. also formulate video story task multi-choice task. video story-related questions answer sentence question. question story video hypothesis answer sentence consists multiple choice answer sentences a={ar}…k thus model choose correct answer sentence among possible answer sentences given question video words given scoring function goal pick correct answer sentence maximize dataset contains pairs movie stories provides various information sources including video clips descriptive sentences. however suitable testbed video story certain points movieqa considered scenes videos always provided tasks questions answered using linguistic information sources dialogues script plot. descriptive sentences movieqa often contain contextual information available within provided video clip i.e. cohesion scenes descriptive sentences. reasons created benchmark allows high-level video analysis high-quality large amounts descriptive sentences simple clear coherent story structure. pororoqa dataset simple nature cartoon images used exploring high-level reasoning required solve imageqa similar cartoon images cartoon videos simple story structure small environment compared videos movies dramas. particular cartoon series kids properties similar events repeated number characters background small. famous cartoon video series children called ‘pororo’ consists episodes. episode different story minutes average length amount total running time hours. main characters entire video. size vocabulary ratio type action pororo egg? person lives forest? abstract main event pororo day? detail little penguin wear? method crong introduce himself? reason pororo take home? location small village situated? statement dinosaur first? causality happens crong reaches yes/no pororo took shelter poby? time collection recruited workers different workers participated making descriptive sentences. asked watch videos creating asked make questions video stories correct answer four wrong answers question. descriptive sentences given annotators. next gave context question relevant scene-dialogue pair video. words question relevant scene-dialogue pair contains information answer. excluding pairs follow guidelines e.g. vague irrelevant ones where they? ‘how many trees videos? obtained pairs. average number episode i.e. video average numbers words question answer figure shows guildleines given workers. table shows examples statistics types questions. dataset comparison compare dataset existing public video datasets table best knowledge pororoqa highest number videoqa well first video dataset coherent storyline throughout dataset. plan release pororoqa dataset community contribution. selecting episode watching youtube video please make story-related english. please keep mind sure watch video making data. please select scene-subtitle pair likely match question write qas. questions localized video contents. please provide correct answer question people would agree four wrong answers deception. answer complete sentence correct grammar. minimum number words sentence four. \"lunch\" \"pororo crong lunch\" please avoid vague terms. \"when pororo there?\" \"when pororo loopy's house?\" \"what pororo doing?\" \"what pororo crong crying house?\" please avoid completely unrelated questions. \"how earth\" please avoid image-specific questions. \"how many trees video?\" please avoid creating duplicate questions episode. please character names follows. figure instructions shown creators. sentences collection descriptive sentences collected website. made videos scene-dialogue pairs visible human annotators. annotators could provide data directly site viewing videos scene-dialogue pairs. converted scene animated displayed site. asked human annotators amazon mechanical turk visit site concretely describe scene multiple sentences following guidelines. total descriptive sentences collected human annotators. average number sentences words scene description table shows advantage descriptions movieqa; descriptions well associated visual stories. evaluation randomly selected samples dataset recruited persons score question video story ideas extended model understands video stories joint stream modalities scene dialogue gives attention specific pieces evidence answer correctly. figure shows structure proposed model demn video story demn takes video x={}…|x| input scene dialogue task passes three modules described below. video story understanding module main objective module reconstruct video stories form sentences scene-dialogue streams observed videos. training time module learns scene embedding matrix dialogue embedding matrix test time module transforms scene-dialogue pair video story following deep residual networks encoder-decoder deep model compute visual-linguistic features pair input scene-dialogue pair define video story concatenation output video stories input video i.e. s={si}…|x| ={êi li}…|x|. means concatenation. example ‘there three friends ground. friends talking house.’ story stored long-term memory component e.g. table. truth description pairs training dataset learn first train scene embedding matrix using combination hinge rank loss dot-product score trained achieve higher dot-product score embedded representation scene scene training video dataset vector aggregation image features computed frame vtr. used average pooling sized -layer residual networks activations corresponding dialogue feature vector computed skip-thought vectors pre-trained using wikipedia dialogue corpus ‘pororo’ cartoon videos feature vector contrastive dialogue sentence vtr. deep models computing features scenes dialogues test time. embedding matrix trainable parameters pre-trained scene-descriptive sentence pairs mpii-md dataset stochastic gradient descent train margin fixed training time. trained compute combined vector pair summing embedded scene vector vtrtm representation corresponding dialogue ltr. then train embedding i.e. ctrtm ground-truth description achieves higher dot-product score scores contrastive description vectors note denoted vectors i.e. normalized unit length. story selection module function module recall best video story contains answer information question module reads list stories {si}…|x| input video long-term memory scores story matching highest scoring relevant story retrieved with experimental setup split episodes ‘pororo’ videos training validation test number pairs training validation test evaluation methods accuracy mean reciprocal rank used evaluate story selection module models value informs average inverse rank correct relevant story among video story model experiments pororoqa intend measure human performance pororoqa task performance existing story models performance comparison proposed model story models. performances evaluated ablation experiments possible input combinations briefly describe human experiments comparative models model setting. lstm baseline models used challenge video story task extended models replacing image input video scene input adding extra input models linguistic sources videos dialogues descriptions. represent language used bag-of-word average pooling wordvec skip-thoughts vectors. linguistic features fused visual features calculated average pooling -layer residual networks activations. memory networks end-to-end memory networks memory networks end-to-end memory networks initially proposed text story video story task models extended separately built story models using scenes dialogues fused results last components models. visual story models retrieved descriptions proxy scenes like model. demn evaluated demn modes i.e. without attention. used linear neural networks alternative scoring functions also demn memory networks retrieve descriptions ablation experiments involving used ground-truth descriptions instead. results report human performances pororoqa task. first table shows human performances experiments. videos important majority questions. information provided human function scores match pair output module fuses question relevant story. example ‘what friends ground? three friends ground. friends talking house’. appropriate answer answer a={ar}…k. similar story selection module module scores match pair answer sentence highest scoring answer sentence selected with scoring function matches pair. scoring function handle long sentences word level attention-based model used scoring functions model builds embeddings sequences tokens x={xi}…|x| y={yi}…|y| measure closeness cosine similarity. video story question answer sentence. model encodes token using bidirectional lstm calculates sentence vector averaging output token vectors bidirectional lstm side. token vector multiplied softmax weight determined exp) training train modules fully supervised setting. question training data associated list scene-dialogue pairs {}…|x| video belongs respective judgements correctly relevant otherwise. also associated list answer sentences {ar}…k judgments correct answer otherwise. setting relevant scene-dialogue pair correct answer considered data instance triplets means description retrieved video story understanding module. subscript index correctly relevant scene-dialogue pair i.e. training performed hinge rank loss triplets table accuracies pororoqa task. stands question dialogue scene ground-truth description respectively. ablation experiments memory networks variants using used ground-truth descriptions scenes retrieved ones scores denoted parentheses. figure qualitative results demn pororoqa tasks. scene dialogue correspond scene dialogue contain story selected model. story means story reconstructed video story understanding module. show first frame scenes. performance significantly improved. found useful information descriptions answer questions dialogues. overall humans achieved high accuracy remaining rows table show performances models. unlike images visualqa models failed solve visual features also difficulty using sequences long dialogues descriptions. end-to-end memory networks supervisions questions relevant scene-dialogue labels performances lower memory networks. memory networks similar performances demn attention story modality however using linguistic stories visual stories together demn able achieve better performances. combined video story reconstruction improved scores thus accuracies. using attention demn showed best performance models. however still room performance improvement comparing differences performance humans performance table accuracies movieqa task. demn achieved state-of-the-art scores videoqa mode. rand. means accuracy model nearly sscb convolutional neural networks-based model model experiments movieqa benchmark movieqa benchmark dataset provides movies multiple choices qas. report accuracies demn table time submission paper demn achieved state-of-the-art results validation test video mode. understand scenes used description mpii-md assume reason relatively performance movieqa unlike pororoqa many different story structures make optimization difficult. concluding remarks proposed video story model demn video story dataset-pororoqa. pororoqa simple coherent story-structured videos high-quality scene descriptions. demonstrated potential model showing state-of-the-art performances pororoqa movieqa. future work explore methods curriculum learning help optimize complex story structures using pororoqa. aishwarya agrawal jiasen stanislaw antol margaret mitchell lawrence zitnick dhruv batra devi parikh. visual question answering. proceedings ieee conference computer vision pattern recognition andrea frome greg corrado jonathon shlens samy bengio jeffrey dean marc’ aurelio ranzato tomas mikolov. devise deep visual-semantic embedding model. proceedings advances neural information processing systems akira fukui dong park daylen yang anna rohrbach trevor darrell marcus rohrbach. multimodal compact bilinear pooling visual question answering visual grounding. proceedings conference empirical methods natural language processing jung-woo kyung-min byoung-tak zhang. automated construction visual-linguistic knowledge concept learning cartoon videos. proceedings aaai conference artificial intelligence eduard hovy laurie gerber hermjakob chin-yew deepak ravichandran. toward semantics-based answer pinpointing. proceedings human language technology conference kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition ryan kiros ruslan salakhutdinov richard zemel. unifying visual-semantic embeddings multimodal neural language models proceedings transactions association computational linguistics ryan kiros yukun ruslan salakhutdinov richard zemel antonio torralba raquel urtasun sanja fidler. skip-thought vectors. proceedings advances neural information processing systems anna rohrbach marcus rohrbach annemarie friedrich sikandar amin mykhaylo andriluka manfred pinkal bernt schiele. coherent multi-sentence video description variable level detail. proceedings german confeence pattern recognition anna rohrbach marcus rohrbach niket tandon bernt schiele. dataset movie description. proceedings ieee conference computer vision pattern recognition sainbayar sukhbaatar arthur szlam jason weston fergus. end-to-end memory networks. proceedings advances neural information processing systems ming cicero santos bing xiang bowen zhou. lstm-based deep learning models non-factoid answer selection. international conference learning representations workshop makarand tapaswi yukun rainer stiefelhagen antonio torralba raquel urtasun sanja fidler. movieqa understanding stories movies question-answering. proceedings ieee conference computer vision pattern recognition atousa torabi christopher hugo larochelle aaron courville. using descriptive video services create large data source video annotation research. arxiv preprint arxiv.v wang eric nyberg. long short-term memory model answer sentence selection question answering proceedings association computational linguistics ting yong rui. msr-vtt large video description dataset bridging video language. proceedings ieee conference computer vision pattern recognition lawrence zitnick devi parikh. bringing semantics focus using visual abstraction. proceedings ieee conference computer vision pattern recognition", "year": 2017}