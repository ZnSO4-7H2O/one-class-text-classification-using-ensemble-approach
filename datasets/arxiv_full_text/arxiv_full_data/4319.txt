{"title": "Permutation-equivariant neural networks applied to dynamics prediction", "tag": ["cs.CV", "stat.ML"], "abstract": "The introduction of convolutional layers greatly advanced the performance of neural networks on image tasks due to innately capturing a way of encoding and learning translation-invariant operations, matching one of the underlying symmetries of the image domain. In comparison, there are a number of problems in which there are a number of different inputs which are all 'of the same type' --- multiple particles, multiple agents, multiple stock prices, etc. The corresponding symmetry to this is permutation symmetry, in that the algorithm should not depend on the specific ordering of the input data. We discuss a permutation-invariant neural network layer in analogy to convolutional layers, and show the ability of this architecture to learn to predict the motion of a variable number of interacting hard discs in 2D. In the same way that convolutional layers can generalize to different image sizes, the permutation layer we describe generalizes to different numbers of objects.", "text": "introduction convolutional layers greatly advanced performance neural networks image tasks innately capturing encoding learning translation-invariant operations matching underlying symmetries image domain. comparison number problems number diﬀerent inputs type’ multiple particles multiple agents multiple stock prices etc. corresponding symmetry permutation symmetry algorithm depend speciﬁc ordering input data. discuss permutation-invariant neural network layer analogy convolutional layers show ability architecture learn predict motion variable number interacting hard discs convolutional layers generalize diﬀerent image sizes permutation layer describe generalizes diﬀerent numbers objects. problems high degree symmetry create large ineﬃciencies machine learning algorithms take symmetries account. problem symmetries algorithm must independently learn pattern multiple times over leading massive increase amount training data time required creating many opportunities spurious biases creep approach handle level data removing redundant symmetries input aligning things canonical form. examples range things like data augmentation sift features spatial transformer networks. attentional models also implicitly aligning location attention features rather coordinates input data. possibility build desired symmetry functional form network level. example convolutional neural networks respect translation invariance. shared ﬁlter applied point image space aggregated hierarchy pooling operations ﬁnal classiﬁcation result behaves locally translation-invariant scale. kind convolutional approach extended domains symmetries. example graph convolutional networks allow convolution operations performed local neighborhoods arbitrary graphs general framework constructing kinds speciﬁed invariances proposed authors generalized convolutional operations aﬃne invariant computations showed construct similar things arbitrary symmetry spaces. permutation symmetry work along line done specifying expansion terms neural applied speciﬁc symmetry functions estimation potentials initio quantum mechanics calculations. addition uses pooling instances construct permutationinvariant summary statistics diﬀerent data distributions. instead invariance generates result change transformations input might want obtain property called ‘equivariance’. case goal make ﬁnal outcome constant despite application transformations rather create structure input output network behave under transformation. invariance targets function form equivariance targets function form useful things image segmentation applying rotation input image change pixel classes change positions classes occur general recipe constructing equivariant convolutions proposed technique used generate convolutional layer equivariant degree rotations mirror symmetry. paper we’re interested making networks predict future dynamics sets similar interacting objects example learning predict synthesize future dynamics particles observing trajectories. could something common theme constructing group-invariant group-equivariant functions perform pooling combinations symmetry transforms. case symmetry group applying transformation associated symmetry shuﬄes around members group remove member. means long calculation applied identically members symmetry group output invariant transformations applied input. equivariance constructed similar fashion combining members input space pooling respect transformations applied two. permutation equivariance existing work diﬀer interactions elements considered. variadic networks implement permutation equivariance addition kind mean-ﬁeld approximation interactions elements otherwise single-element neural networks recent work convolutions also provides equivariant layer interchangeable objects summation maximization pairs objects used construct equivariant functions. approaches common kind wrapper guarantees inﬂuence rest elements particular member pooled invariant generally structure outside wrapper made complex structure within wrapper made simple considering interacting particles diﬃculties interaction potentials relatively speaking fairly complex functions. apply pooling operation lose ability track exactly particles interacted order contribute given hidden layer feature. sense might require excessive number layers reconstruct correct interactions network learn hidden variables encode interaction also store kind indexing information neighbors ability wrap function pooling operation output invariant quite general though possible complex function inside interaction multi-layer neural network. fig. structure permutational layer. output calculated function separately combines correspondingly indexed input inputs summed weights used combinations. along line deriving eﬀective equations motions complex particles animals swarm players sport apply abstract level something relationships variations diﬀerent sectors economy interaction speciﬁc stocks. associated symmetry sort data permutation symmetry looking methods build permutation equivariant neural networks. present numerical experiments assessing feasibility performance various approaches problem show combination multi-layer embedded functions inside pooling permutation invariant wrapper achieve signiﬁcantly higher accuracy predicting dynamics interacting hard discs. furthermore networks using combination demonstrate ability generalize diﬀerent number particles seen training also handle cases particles identical including auxiliary random label feature. matrices parameters element-wise nonlinearity summation replaced anything appropriate properties elements case instead stack input elements together apply dense neural network pool output network applied choices second element. single layer takes combinations input elements generates output permutation invariance wrapper around arbitrarily complex function central idea paper; here we’ll refer ’permutational layer’ layers stacked form complex network interestingly terms pairwise selections input necessary consider interactions overall. hand method would ﬁrst glance cost construct full symmetry space associated permutations input set. missing looking interactions? note similar situation traditional convolutional neural networks. general convolutional layer would ﬁlters size input image would require require computations evaluate. however restricting ﬁlters ﬁxed size local nature cost reduced back we’re somewhat similar full symmetry space constructed taking combinations pairwise exchanges consider ﬁnite number pairwise exchanges sort local neighborhood. case viewed point view method pairwise layer equivalent sort nearest-neighbor ﬁlter symmetry space. image domain would restrict receptive ﬁeld quite strongly stacking multiple layers resolution causes ﬁlter sizes add. case permutation symmetry elements fig. example network architecture involving permutational layers. inputs permutational layer rank- tensor containing objects input features. within permutational layer neural network inputs outputs features taken pairs objects. result output features objects. layers stacked capture multi-object interactions. somewhat analogous approaches image domain apply ’convolutions’ ﬁlter size order increase complexity operations done particular scale example inception architecture. also recent experiments showing factoring convolutions spatial part local part advantageous. construct permutation equivariant layer similar form described sense pairs elements chosen input results aggregated indices second element. however work apply matrix multisimulation generate training trajectory. task network take positions velocities discs point time input predict positions velocities timesteps later compare performance various network architectures task trained trajectories length networks implemented lasagne theano. train using adam optimizer learning rate schedule number training steps. cases inputs network discs outputs predicted values compare three dense architectures eﬀectively calculate changes coordinates directly predicting future coordinates) four architectures using permutational layers. network architectures examine shown fig. case permutational layer networks stack three layers case. however compare results layers contain single matrix multiplication followed relu nonlinearity versus cases layers contain -layer dense neural networks. also compare diﬀerence using average pooling aggregate pairwise combinations versus using pooling. increasing complexity single permutational layer works better adding additional permutational layers suggesting capturing precise form inter-particle interactions important accurate prediction capturing complex multi-body interactions. furthermore pooling appears perform better generalize signiﬁcantly better average pooling. training results shown table methods appear less converge rate dense architectures perform signiﬁcantly worse task. furthermore increasing depth dense netfig. network architectures investigate hard disc dynamics task. compare dense networks diﬀerent sizes without ﬁnal skip connection well -layer permutational network without ﬁnal skip connection. also consider version perm-skip- pooling permutational layers instead average pooling. densely connected information percolates pairwise interactions quickly. output -body interaction generates latent features could thought containing information pairs particles. however next layer eﬀectively read information diﬀerent pairs meaning output hidden feature layers contain information interaction sets particles. upper bound continues grow exponentially network depth. course information hidden features likely simple information single speciﬁc particle rather kind population average weighted depends features receiving particle. network learn construct population average relevant receiving particle separately becomes much easier express locally relevant summary statistics rather common mean ﬁeld. table generalization perm-skip- network diﬀerent numbers discs. corresponds training case column test case. entries average mean-squared-error unseen sequences dynamics. pooling version appears signiﬁcantly better generalization ability diﬀerent numbers discs system true dynamics strongly determined nearest-neighbor interactions contribute usefulness pooling compared average pooling. discs trained averaged unseen sequences. results shown table numbers objects within factor training case network appears retain predictive ability outside range network’s performance rapidly degrades. network’s generalization ability fails relate choice pooling used deﬁning permutational layer structure. average pooling order aggregate diﬀerent interactions types pooling would also permutation invariant would extract diﬀerent types statistics population objects average pooling conceals information absolute number particles present compared un-normalized pooling might focus something closer nearest-neighbor interactions. restrict ourselves average pooling focus present permutational layer forms pooling would reasonable architectural choices order optimize generalization performance. varying number objects training another possible strategy. weak permutation invariance another consideration kind problem objects always identical. part objects data elements swapped latent features make objects behave somewhat diﬀerently. features known must learned data. features must learned persistent given works beyond certain point makes asymptotic convergence worse. cases using skip connection beginning calculation improves performance. direct inspection predicted trajectories show velocities harder predict positions moments collision walls discs diﬃcult points resolve accurately. predicted trajectories versus actual trajectories shown fig. also network generate ﬁctitious trajectory taking predictions next input state generate overall sequence. accuracy compared real trajectory quickly decays overall resultant conﬁgurations qualitatively consistent idea objects hard discs overlaps avoided discs reﬂect another appear transfer momentum eachother appears though model captured least qualitative statistical regularities motion hard discs suﬃcient generalize extended trajectories. example synthesized trajectory generated using perm-skip--max architecture seen https//youtu.be/sdwiwrrqu. permutational networks deﬁned operation applies uniformly inputs much like convolution ﬂexible size input however guarantee generalization statement action network still welldeﬁned number input objects. test property taking networks trained discs using predict motion diﬀerent numbers discs. measure mean squared error perm-skip- perm-skip--max networks’ predictions using predict diﬀerent number however functionality added back preserving generalization permutation invariant parts system. done assigning additional features input data object labels. could simple unique vector associated object network must learn associate latent properties could advanced. example could low-dimensional embedding learned individual object trajectories using e.g. tsne siamese network. test idea train perm-skip- system composed discs radius discs radius compare results network versus also provide disc arbitrary -component random vector label. base network obtains average mean-squared-error training signiﬁcantly worse identical discs case. hand augmented network obtains performance un-augmented algorithm achieves identical discs. even simple sort auxiliary labeling allow non-permutationinvariant information injected back permutation invariant architecture retaining beneﬁts places permutation invariance appropriate. course sort random label unlikely network would generalize diﬀerent numbers discs random labels choice made test network could learn heterogeneities own. could example directly label discs radius instead generalize permutation equivariant network architectures used variety cases inputs outputs interchangeable objects. considered problem type learning predict trajectories sets interacting objects. note sources complexity network must learn handle. complexity arising choices complicated subsets elements multi-element interactions. complexity arising form interaction itself. problem dynamics prediction interactions often captured well pairwise approximation speciﬁc form interactions complicated function properties particles. this simple linear functions pairs particles must struggle capture something precise position collision discs. handle this observed idea taking direct pairwise selection elements create permutation equivariance general wrapper around sort function combining elements linear function elements eachother. wrapper embed deep neural network order learn model form complicated interaction potentials. result architecture capture complex interactions accurately readily generalizes diﬀerent numbers non-identical objects. future investigation would interesting examine problems domain permutation invariance closely related symmetries whether complexity pairwise interactions complexity multi-body relationships dominant factor performance problems. code network architectures layers described paper along numerical experiments hard disc dynamics trained perm-skip-max network available https//github.com/ arayabrain/permutationalnetworks. chollet arxiv preprint arxiv. dieleman schlter raﬀel olson snderby nouri maturana thoma battenberg kelly fauw heilman almeida mcfee weideman takcs rivaz crall sanders rasul french degrave lasagne first release.", "year": 2016}