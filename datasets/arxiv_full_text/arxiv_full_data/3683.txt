{"title": "A New Optimal Stepsize For Approximate Dynamic Programming", "tag": ["math.OC", "cs.AI", "cs.LG", "cs.SY", "stat.ML"], "abstract": "Approximate dynamic programming (ADP) has proven itself in a wide range of applications spanning large-scale transportation problems, health care, revenue management, and energy systems. The design of effective ADP algorithms has many dimensions, but one crucial factor is the stepsize rule used to update a value function approximation. Many operations research applications are computationally intensive, and it is important to obtain good results quickly. Furthermore, the most popular stepsize formulas use tunable parameters and can produce very poor results if tuned improperly. We derive a new stepsize rule that optimizes the prediction error in order to improve the short-term performance of an ADP algorithm. With only one, relatively insensitive tunable parameter, the new rule adapts to the level of noise in the problem and produces faster convergence in numerical experiments.", "text": "approximate dynamic programming proven wide range applications spanning large-scale transportation problems health care revenue management energy systems. design eﬀective algorithms many dimensions crucial factor stepsize rule used update value function approximation. many operations research applications computationally intensive important obtain good results quickly. furthermore popular stepsize formulas tunable parameters produce poor results tuned improperly. derive stepsize rule optimizes prediction error order improve short-term performance algorithm. relatively insensitive tunable parameter rule adapts level noise problem produces faster convergence numerical experiments. approximate dynamic programming emerged powerful tool solving stochastic optimization problems inventory control emergency response health care energy storage revenue management sensor management recent research used solve large-scale ﬂeet management problem variables time period millions dimensions state variable energy resource planning problem time periods applications operations research especially demanding often requiring sequential solution linear nonlinear integer programming problems. algorithm limited hundred iterations important good solution quickly possible process hinges stepsize controls information merged existing estimates. many problems diﬃcult solve curse dimensionality. reason tradition dating back bellman’s earliest work solving equation approximately. ﬁeld evolved variety names including approximate dynamic programming neuro-dynamic programming reinforcement learning major class methods known approximate value iteration observation value bootstrapped approximation downstream value used update approximation. generic procedure computing observation given iteration algorithm. intend illustrate concept constructing practice summation also approximated. expectation within operator avoided using concept post-decision state enables compute modiﬁed version exceptionally quickly. makes approximate value iteration particularly useful online applications since easy implement. stepsize note uses statistical bootstrapping estimate value depends statistical approximation deﬁning characteristic approximate value iteration proven successful broad classes operations research applications. reinforcement learning community uses closely related algorithm known q-learning uses similar bootstrapping scheme learn value state-action pair. approximate value iteration q-learning stepsize plays roles. first smooths eﬀects noise observations second determines much weight placed rewards dual role stepsize speciﬁc bootstrapping-based methods whose ease makes natural approach largescale applications operations research rate convergence crucial. e.g. examples applications. method using stepsize update value state-action pair based ﬁeld stochastic approximation; thorough treatments ﬁeld. ﬁrst apply theory show convergence algorithm stepsize rule satisﬁes certain conditions. general practice shows strong bias toward simple rules easy code. example stepsize rule eﬀect averaging observations ˆvn. fact rule satisﬁes necessary theoretical conditions convergence made kind default rule however literature acknowledged rule produce slow convergence; contributions paper derive theoretical bounds providing insights weakness rule. reason many practitioners slow initial convergence problems volatile non-convergent estimates limit. also easy construct problems single constant work poorly. solves inventory problem spare parts high-volume spare part remain inventory days low-volume part remain inventory hundreds days. small stepsize work poorly low-volume part large stepsizes fail dampen noise appropriate high-volume parts. applications require stochastic stepsize rules computed adaptively error previous prediction estimate. methods include stochastic gradient rule delta-bar-delta rule variants kalman ﬁlter detailed survey deterministic stochastic stepsizes given additional references main challenge faced methods prediction error diﬃcult estimate general often resulting highly volatile stepsizes large numbers tunable parameters. recent work adopts diﬀerent approach based relative frequency visits diﬀerent states heavily tied on-policy learning whereas practical implementations often oﬀ-policy learning promote exploration cases literature largely ignores dependence observation previous value function approximation arguably deﬁning feature approximate value iteration. instance algorithm viewed biasadjusted kalman ﬁlter assumes independent observations. approach problem stepsize selection studying single state action. model radically streamlines behaviour general retains features problems crucial stepsize performance namely bias-variance tradeoﬀ dependence observations. model make following contributions derive easily computable convergent upper lower bounds time required convergence demonstrating rate convergence slow rule almost never used adp. derive closed-form easily computable stepsize rule optimal single-state single-action problem. ﬁrst stepsize rule account dependence observations adp. formula requires tuning easy apply general multi-state problem. analyze convergence properties stepsize rule. show stall declines zero limit. ﬁrst optimal stepsize provably properties. present numerical comparisons stepsizes general setting demonstrate that popular competing strategies sensitive tunable parameters rule robust fairly insensitive single parameter. last property vital practical importance allowing developers focus approximation strategies without concern poor performance poorly tuned stepsize formula. section deﬁnes optimality stepsize illustrates need optimal stepsize rule theoretically demonstrating poor performance single-state single-action problem. section derives optimal stepsize rule approximate value iteration problem shows used general setting. section presents numerical sensitivity analysis rule single-state problem. finally sections present numerical results general examples. section lays stylized model used analysis deﬁnes optimality stepsize setting. section motivates need optimal stepsize showing commonly used stepsize produces unusably slow convergence model. refer quantity inside expectation prediction error. recall serves observation value state prediction error squared diﬀerence observation current estimate value. prediction error standard objective optimal stepsize rule used reinforcement learning used e.g. stochastic gradient methods kalman ﬁltering signal processing main challenge faced researchers that general dynamic program cannot solved closed form. reason error-minimizing stepsize algorithms adopt gradient descent approach stepsize adjusted based estimate derivative respect αn−. resulting stepsize algorithms longer optimal exhibit volatile behaviour early stages. many require extensive tuning. also seek minimize prediction error adopt diﬀerent approach. instead approximating general case consider stylized dynamic program single state single action closed-form solution. setting course general issues exhibit much complex behaviour streamlined single-state single-action model. however stylized model still subject issues provide insight resolved general case. main advantage oﬀered model allows address issues using closed-form solution optimal stepsize explicitly capturing relationship bias-variance tradeoﬀ dependence observations. able adapt solution single-state problem general dynamic programs brieﬂy note allow observation random. high level allows view single-state single-action problem stand-in inﬁnite-horizon steady state. recall well-known property markov decision processes policy produced basic value iteration update converges optimal policy probability state converges steadystate distribution result unconditional expectation contribution earned iteration approaches constant denote again single-state single-action model cannot capture complexity general allows distill large class simple elegant archetype capturing behaviours common class. research error-minimizing stepsizes motivated poor practical performance simple stepsize rules. among these notable produces provably convergent estimates value function thus persists literature kind default rule evidenced recent e.g. theoretical worstcase convergence rate stepsize known slow derive bounds easier compute demonstrate rule unusably slow even stylized single-state single-action model section consider model simplicity assume discussion rewards deterministic. algorithm performs badly deterministic case generally expect perform even worse allowed random since increasing noise generally slows convergence. brieﬂy summarize results give numerical illustration; full technical details found appendix. numerical illustration enters multiplicative factor bounds true value function well. thus free parameter. results plotted log-scale figure grows large upper lower bounds figure show number iterations reaches optimal. lower bound value gives upper bound number iterations needed upper bound gives lower bound iterations needed. near already require iterations causing diﬃculty applications requiring signiﬁcant amount time iteration. then grows larger require least iterations impractical almost application. grows number iterations needed least that simple problem approximate value iteration stepsize converges slowly impractical inﬁnite horizon applications particularly discount factor close behaviour likely seen complex inﬁnite horizon problems also undiscounted ﬁnite horizon problems. remainder paper studies stepsize rule optimal single-state single-action used analysis. section derive stepsize rule optimal approximate value iteration problem given study convergence properties section however special case theoretical tractability ultimate goal obtain algorithm applied general dynamic program. extension explained section general form stepsize given finally section considers extension ﬁnite-horizon problems. numerator denominator fraction include covariance terms. knowledge ﬁrst stepsize literature explicitly account dependence observations. furthermore formula includes close section showing formula behaves correctly special cases. rewards collect deterministic estimate simply adding discounted rewards converge optimal stepsize rule. process stationary i.e. simply estimating using known optimal stepsize rule common convergence proofs. automatically imply a.s. convergence produce convergence single-state single-action model. begin showing bias term stepsize formula converges zero; proof given appendix. prove proposition limn→∞ theorem limn→∞ proof. enough show apply together proposition show every convergent subsequence must converge zero using proof contradiction. first suppose subsequence satisfying limk→∞ combining proposition along theorem important practical well theoretical implications. lower bound provided proposition ensures stepsize decline quickly. bound provided many standard rules including beneﬁt rule designed minimize prediction error faster convergence still guaranteed avoid risk stalling. guarantee theorem stepsize asymptotically approach zero particularly valuable applications interested policy values themselves. example ﬁnance value function used estimate price option. ﬂeet management application value functions used estimate marginal value truck drivers. applications essential algorithm produce tight estimates values. discuss adapted general dynamic program. ﬁrst step consider extension single-state model unknown. case estimate unknown quantities smoothing observations plugging estimates expression optimal stepsize ﬁrst glance appears problem needing secondary stepsize calculate optimal one. however important note secondary stepsize required estimate parameters distribution one-period reward unlike sequence value function approximations one-period reward single-state problem stationary straightforwardly estimated random rewards collected time period. true signiﬁcance easily extended general case replace random reward many states actions. one-period reward earned taking action state sequence rewards depends policy used visit states; approximate value iteration policy change time thus making sequence rewards non-stationary. however discussed section basic value-iteration update eventually converges optimal policy meaning expected one-period reward earned state converges single system-wide constant suggests that general suﬃcient keep single system-wide estimate rather store state-dependent estimates. hand quantities related bias variance value function approximation. suggests that general state-dependent. example q-learning algorithm separate approximation state-action pair leading state-dependent stepsize. figure describes example implementation osavi classic ﬁnite-state ﬁnite-action generic algorithm used lookup table approximation. complex problem employ state aggregation method would store diﬀerent block aggregation structure. memory cost similar procedure recursively updated quantities stored estimated parameter. general setting suggest using constant stepsize e.g. avoid giving equal weight early observations taken transient period reached steady state probability state still changing policy. numerical work suggests performance sensitive choice νn−. finally brieﬂy note convergence analysis section mostly carries general case. first bound proposition still holds almost surely since proof holds arbitrary values even change iterations. bounds proposition also hold since proofs functional forms recursive updates hold arbitrary stepsize sequence. consequently theorem still holds a.s. long sample-based approximations explode inﬁnity subsequence. approximations type convergence also sense. proof uses logic proof proposition think symmetric matrix updated recursively using elements λn−. matrix starts diagonal increases covariances gradually expand main diagonal outward. next repeat analysis section solve inﬁnite-horizon case reduces original formula ﬁnitehorizon formula requires store parameters form matrix potential incur substantially greater computational cost. beneﬁt optimally vary stepsize unknown adapt approximation procedure outlined section replace unknown values ﬁrst study performance stepsize rule instance single-state singleaction problem. allows obtain insights sensitivity performance respect diﬀerent problem parameters. considered normally distributed rewards mean standard deviation discount factor. optimal value problem policies used initial approximation. furthermore sample-based parameters policies initialized zero throughout parts study. five diﬀerent stepsize rules implemented; brieﬂy describe follows. algorithm like osavi stepsize minimizes form prediction error scalar signal processing problem assumes observations independent. secondary stepsize rule ¯νn− used estimate bias value function approximation tunable parameter. stepsize behaves like rule early iterations quickly converges limit point behaves like constant stepsize rule. tends happen within approximately iterations. experiments used issue tuning discussed section mcclain’s rule viewed slightly sophisticated version constant stepsize. tunable parameter. value yielded good performance choice problem parameters. however harmonic stepsize sensitive choice tunable parameter highly problem dependent. expect good convergence hundred iterations order work quite well. hand anticipate running algorithm millions iterations might choose order higher. issue discussed section performed logarithm stepsize. used tunable parameter. also considered polynomial stepsize consistently underperformed rules listed above omitted subsequent analysis. constant rule yielded results similar mcclain’s rule also omitted. figure shows value objective function achieved stepsize rule iterations. osavi rule consistently achieves best performance however harmonic stepsize properly tuned performs comparably. bakf mcclain rules level around objective value data point figure average outer loop simulations. noted that idbd displays slowest convergence early eventually overtakes bakf mcclain’s rule continues exhibit improvement late iterations. single-state setting found less sensitive tunable parameter harmonic rule also produced less volatile stepsizes bakf. examine performance rule multi-stage problems later contrast benchmarks fairly sensitive tunable parameters. discuss context single-state problem allows examine tuning issues minimal number problem inputs. begin considering approximate bakf rule uses secondary stepsize ¯νn− estimate bias figure shows eﬀect varying ¯νn− objective value achieved bakf that constant value secondary stepsize clear tradeoﬀ performance early late iterations. smaller values ¯νn− result better performance long worse performance short run. terms quality approximation smaller constants cause slower convergence stable estimates. long run. case constant stepsize. furthermore rule produces close performance osavi. however general many diﬀerent rewards constant stepsize better able handle transient phase mdp. reason focus primarily constant values study. even declining secondary stepsize bakf rule outperformed osavi simple constant secondary stepsize results diﬀerent values ¯νn− indicate bakf quite sensitive choice ¯νn−. figure suggests osavi relatively insensitive choice secondary stepsize. lines figure represent performance osavi values ranging high changes much smaller eﬀect performance osavi varying ¯νn− bakf rule. small values yield slightly poorer performance little diﬀerence furthermore objective value achieved osavi declines zero constant value whereas bakf always levels constant secondary stepsize. conclude osavi robust bakf requires less tuning secondary stepsize. figure shows sensitivity mcclain’s rule choice tunable parameter eﬀect similar eﬀect using diﬀerent constant values ¯νn− figure smaller values give better late-horizon performance worse early-horizon performance. harmonic rule analyzed figure good choice problem particular parameter values chosen. larger values consistently worse smaller values eﬀective early iterations. however yields good performance best competing stepsize rules. however best value highly problem-dependent. figure shows continues perform well increased even achieves slightly lower objective value approximate optimal rule later iterations although osavi performs noticeably better early iterations. however figure shows becomes best value discount factor increased optimal rule retuned figure results shown interestingly appears optimal choice sensitive discount factor signal-to-noise ratio. conclude based figures best choice harmonic stepsize rule sensitive parameters problem best choice problem setting perform poorly diﬀerent problem. contrast figure shows osavi relatively insensitive tunable parameter. simple value yields good results settings considered. claim osavi robust alternative several leading stepsize rules. also tested general osavi rule section synthetic states actions state generated following manner. state action probability reward generated uniformly probability generated uniformly randomly picked states reachable. state generated number bssx bssx bssx probability making transition transition probability state reachable zero. manner obtained sparse high-value states leading variety value function. used value iteration compute true optimal value stepsize implemented together following oﬀ-policy approximate value iteration algorithm. value function approximation deﬁned state-action pair q-learning algorithm. upon visiting state action chosen uniformly random. then state simulated transition probabilities mdp. compute brieﬂy discuss reasoning behind design. policy uses value function approximation make decisions also implicitly depend stepsize used update approximation. stepsize aﬀects policy aﬀects sequence visited states turn aﬀects calculation future stepsizes. ensuring good states visited suﬃciently often important practical performance algorithms issue quite separate problem stepsize selection outside scope paper. sought decouple stepsize policy randomly generating states actions. algorithm iterations stepsize rules section performance iterations evaluated follows. policy takes action maxx state calculate value probability transitioning policy reward obtained following state then calculate true value function obtained value iteration. gives suboptimality policy average quantity simulations consisting iterations learning algorithm. simulations standard errors performance measure negligible relative magnitude omitted subsequent ﬁgures discussion. compared following stepsize rules mcclain’s rule harmonic rule bakf rule secondary stepsize osavi secondary stepsize idbd experimented several orders magnitudes found produced good performance although diﬀerence magnitudes relatively small. also made stepsizes state-dependent order achieve quicker convergence. example harmonic rule number times state visited given iterations. parameters used osavi bakf also chosen state-dependent. figure shows average suboptimality achieved stepsize rule time iterations. that discount factors harmonic rule achieves best performance early slows considerably later iterations. osavi achieves best performance second half time horizon margin victory clearly pronounced conclude osavi yields generally competitive performance. harmonic rule tuned perform well performance quite sensitive value particularly visible figure secondary parameter osavi tuned wish observe single constant value suﬃcient produce competitive performance. finite-horizon problems introduce dimension relative size learning bias versus noise contribution depends time period. result optimal stepsize behaviour changes time. ﬁnite-horizon case oﬀ-policy algorithm before update calculated equations backward dynamic programming; following values evaluate policies. performance measure suboptimality policy induced value function approximation averaged states. used section horizon discount factor compared approximate version ﬁnite-horizon osavi rule mcclain’s rule harmonic rule αn−t rules achieved best performance previous experiments easily applied ﬁnite-horizon problem. before stepsizes made state-dependent. figure shows average suboptimality stepsize rule. harmonic rule competitive osavi overall. however performance slows later iterations figure furthermore outperforms osavi midlate iterations osavi largely closed continues improve harmonic rule slows down. finally figure shows magnitude stepsize αn−t produced osavi formula simple synthetic states reachable transition probabilities normalized i.i.d. samples distribution. purpose illustrate behaviour optimal stepsize diﬀerent osavi identical rule would expect since last time horizon. assume observations stationary corollary applies. values earlier time horizon optimal stepsize steadily increases largest values αn−t takes long time observations propagate backward across time horizon need larger stepsizes time ensure observations eﬀect. note that earlier time periods osavi goes period exploration settling curve closely approximated suitably calibrated choice ﬁnite-horizon problem diﬀerent time period. last part experimental study demonstrates osavi used conjunction problem state space continuous. present stylized inventory problem generic resource bought sold spot market held inventory interim. basic structure problem appears applications ﬁnance energy inventory control water reservoir management deliberately abstract particular setting wish keep focus stepsize rule test generic setting required. state variable generic inventory problem contains dimensions. denotes amount resource currently held inventory denotes current spot price resource. assume take values representing percentage total inventory capacity action represents decision inventory sell current stock assume sell total capacity time step increments thus actions problem. resource variable discrete assume spot price continuous follows geometric ornstein-uhlenbeck process standard price model ﬁnance areas. minor modiﬁcations problem could also changed exogenous supply process could draw satisfy demand. important aspect continuous makes impossible solve every state. furthermore even given state computing expectation diﬃcult transition next state depends continuous random variable. reasons approach problem using approximate dynamic programming discrete value function approximation. address issue continuous transition next state post-decision state concept introduced discussed extensively given state decision post-decision state given equations value function approximation used lookup table log-price discretized intervals width thus table contained total entries entry initialized large value keeping recommendation sec. optimistic initial estimates. however approximation used discretized state space experiments simulated using continuous price process. price discretized calls lookup table. important detail discrete value function approximation still solving original continuous problem. price process instantiated mean-reversion parameter volatility prices thus around sharp spikes possible. before pure exploration policy action chosen uniformly random. also before simulate future state price process order compute next state actually visited algorithm generated randomly secondary stepsize idbd osavi secondary stepsize evaluate performance stepsize rule iterations ﬁxed approximation simulated total reward obtained making decisions form maxx ﬁniteinﬁnite-horizon settings. quantity averaged sample paths. figure reports performance approximation obtained using diﬀerent stepsize rules. since objective maximize revenue higher numbers y-axis represent better quality. figure shows performance osavi inﬁnite-horizon setting. larger size inventory problem require several thousand iterations order obtain improvement target policy speciﬁed iterations osavi consistently yields improvement value target policy. analogously figure section also compared osavi harmonic rule ﬁnite-horizon setting; results shown figure inﬁnite-horizon setting several thousand iterations required improvement observed osavi consistently outperforms best version harmonic. solved exactly additional techniques post-decision state variable necessary deal continuous state spaces diﬃcult expectations. even streamlined form considered here inventory problem features continuous price variable value state depends behavior mean-reverting stochastic diﬀerential equation. fact osavi retains advantages stepsize rules setting encouraging sign. proposed mathematical framework analyzing stepsize selection approximate dynamic programming. analysis based stylized model single-state singleaction mdp. used model derive rate convergence results popular stepsize rule. even stylized problem approximate value iteration converges slowly rule virtually unusable inﬁnite-horizon applications. underscores importance stepsize selection general dynamic programming problems. derived optimal stepsize minimizing prediction error value function approximation single-state model. knowledge stepsize ﬁrst take account covariance observation make value state approximation value property inherent approximate value iteration. furthermore able compute closed-form expression prediction bias single-state single-action case considerably simplifying task estimating quantity general case. rule easily extended general setting ﬁniteinﬁnite-horizon. tested stepsize rule several leading deterministic stochastic rules. single-state single-action case consistently outperform stepsize rules. competing rules tuned yield competitive performance also sensitive choice tuning parameter. hand stepsize rule robust displaying little sensitivity parameter used estimate one-period reward. also tested stepsize rule general discretestate well complex problem. found osavi performs competitively rules ﬁniteinﬁnite-horizon settings. conclude stepsize rule good alternative leading stepsizes. conclusion reﬂects particular experiments chose run. important remember deterministic stepsizes harmonic rule ﬁnely tuned particular problem resulting better performance adaptive rule present. strength rule however ability adjust evolution value function approximation well relative lack sensitivity tuning. equation ¯vn+ approximated derivative respect ﬁrst step extend deﬁnition natural numbers onto positive reals piecewise linear interpolation. deﬁne provides useful bound asymptotic convergence negative however becomes large upper bound becomes larger trivial upper bound shown lemma bound longer useful. obtain useful bounds theorem broader range increase chosen. increasing also increases tightens bound across left-hand side converges zero right-hand side follows however impossible because limit numerator denominator contain strictly positive term δ∗). terms section provide additional background approach discuss relation bakf rule rule originally presented name however optimal dynamic programming refer alternate name bias-adjusted kalman ﬁlter given bakf rule designed signal processing problem sequence independent observations unknown means common variance unknown means estimated usual exponential smoothing technique bakf rule particularly relevant study also chooses stepsize minimize expected squared error prediction. bakf osavi prediction error squared diﬀerence mean observation estimate. cases resulting optimal stepsize contains term representing bias approximation term representing variance. crucial diﬀerence follows. bakf designed general signal processing problem goal track scalar moving signal. work applies computational formula bakf application fact derivation bakf uses general setting whose main assumptions violated adp. first bakf assumes observations used smoothing independent case approximate value iteration. rather guiding principle approximate value iteration bootstrap observations approximations impossibility obtaining unbiased estimates unknown value function. contrast osavi makes additional modeling assumption observations constructed according thus prediction error recast form reﬂects speciﬁc structure adp. viewed special case bakf derivation additional structure imposed problem provides important improvements bakf. first osavi explicitly incorporates dependence observation approximation. dependence crucial updating structure handled bakf. second bias term bakf unknown practice work advocates using sample-based approximations quantity giving rise second non-stationary estimation problem. hand special structure assumed general also need approximate however interpret average one-period reward earned following optimal policy steady-state quantity stationary thus easier estimate bias. also section secondary estimation procedure less sensitive secondary stepsize osavi bakf. conclude discussion noting that hand bakf provides generality appropriate general signal processing problem. hand speciﬁc context approximate value iteration observations constructed bootstrapping thus inherently biased dependent osavi captures speciﬁc structure leading improved performance. authors grateful topology atlas forum several helpful discussions. research supported part afosr contracts fa--- fa-- fa--- contracts cmmi- iis- iis- contract n--- center dynamic data analysis.", "year": 2014}