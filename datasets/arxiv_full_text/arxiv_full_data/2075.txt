{"title": "Testing Hypotheses by Regularized Maximum Mean Discrepancy", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Do two data samples come from different distributions? Recent studies of this fundamental problem focused on embedding probability distributions into sufficiently rich characteristic Reproducing Kernel Hilbert Spaces (RKHSs), to compare distributions by the distance between their embeddings. We show that Regularized Maximum Mean Discrepancy (RMMD), our novel measure for kernel-based hypothesis testing, yields substantial improvements even when sample sizes are small, and excels at hypothesis tests involving multiple comparisons with power control. We derive asymptotic distributions under the null and alternative hypotheses, and assess power control. Outstanding results are obtained on: challenging EEG data, MNIST, the Berkley Covertype, and the Flare-Solar dataset.", "text": "data samples come different distributions? recent studies fundamental problem focused embedding probability distributions sufﬁciently rich characteristic reproducing kernel hilbert spaces compare distributions distance embeddings. show regularized maximum mean discrepancy novel measure kernel-based hypothesis testing yields substantial improvements even sample sizes small excels hypothesis tests involving multiple comparisons power control. derive asymptotic distributions null alternative hypotheses assess power control. outstanding results obtained challenging data mnist berkley covertype flare-solar dataset. homogeneity testing important problem statistics machine learning. tests whether samples drawn different distributions. relevant many applications instance schema matching databases speaker identiﬁcation popular two-sample tests like kolmogorovsmirnov cramer-von-mises capable capturing statistical information densities high frequency features. non-parametric kernel-based statistical tests maximum mean discrepancy enable obtain greater power density based methods. applicable euclidean spaces also groups semigroups structures strings graphs bioinformatics robotics problems etc. consider regularized version address hypothesis testing. distributions compared simultaneously face multiple comparisons setting statistical methods exist deal issue multiple test correction given prescribed global signiﬁcance threshold comparisons however corresponding threshold comparison becomes small greatly reduces power test. situations wants retain null hypothesis tests small conservative. main contribution deﬁnition regularized method. regularization term rmmd allows control power test statistic. regularizer provably optimal maximal power; need ﬁne-tuning user. rmmd improves higher power especially small sample sizes preserving advantages mmd. power control enables look true sets null distributions among signiﬁcant ones challenging multiple comparison tasks. provide experimental evidence good performance challenging electroencephalography dataset artiﬁcially generated periodic gaussian data mnist covertype datasets. also assess power control asymptotic relative efﬁciency test. paper organized follows. section elaborate hypothesis testing deﬁne maximum mean discrepancy metric. describe homogeneity testing extend multiple comparisons. section deﬁne rmmd hypothesis testing compare kernel fisher discriminant analysis assess power control are. additional empirical justiﬁcation test various datasets presented section statistical hypothesis test method which based experimental data aims decide whether hypothesis true false alternative hypothesis level signiﬁcance test represents probability rejecting assumption true type error occurs reject although holds. power statistical test usually deﬁned desirable property statistical test prescribed global signiﬁcance level power equals population limit. divide discussion hypothesis testing topics homogeneity testing multiple comparisons. embedding probability distributions reproducing kernel hilbert spaces yields linear method takes information higher order statistics account characteristic kernels injectively probability distribution onto mean element corresponding rkhss. distance mean elements rkhs known deﬁnition given following theorem two-sample test investigates whether samples generated distribution. testing used measure distance embedded probability distributions rkhs. besides calculating distance measure need check whether distance signiﬁcantly different zero. this asymptotic distribution distance measure used obtain threshold values extract statistically signiﬁcant cases. perform hypothesis test null hypothesis alternative samples drawn distributions result close enough zero accept indicates distributions coincide; otherwise alternative assumed hold. threshold asymptotic distribution empirical -quantile distribution statistically signiﬁcant. test determines means bootstrap procedure. statistical analysis data typically needs testing many hypotheses. multiple comparisons multiple testing problem arises evaluate several statistical hypotheses simultaneously. overall type error denote type error single comparison multiple testing scenario. maintaining prescribed signiﬁcance level multiple comparisons yields stringent nevertheless many studies used without correction. several statistical techniques developed control dunn-ˆsid´ak method independent comparisons multiple testing signiﬁcance level obtained decreases probability type error increases power test decreases. requires control correcting tackle problem control deﬁne hypothesis test based rmmd higher power mmd-based test next section. compare distributions multiple testing problem approaches one-vs-all pairwise comparisons. one-vs-all case distribution compared distributions family thus distributions require comparisons. pairwise case pair distributions compared cost main contribution paper novel regularization measure called rmmd. regularization aims provide test statistics greater power erdogmus principe showed parzen window estimation renyi entropy rmmd obtain statistical test greater power penalizing term µqh. formulate rmmd empirical estimator follows non-negative regularization constants. simplicity consider many application however introduce prior knowledge complexity distributions choosing modiﬁed jensen-shanon divergence corresponding rmmd deﬁned second sample {yi}n population counterparts i.e. population mean element population covariance operator deﬁned probability measure covp call between-distribution covariance. pooled covariance operator denoted derive distribution test statistics null hypothesis homogeneity implies consistency test guaranteed form distribution assume {xi}n independent samples respectively k+k−k−k−h k+κqk denotes convergence distribution. without loss generality assume proofs hold even based hoeffding theorem theorem serﬂing prove following theorem increase power rmmd-based test need decrease variance theorem following theorem used obtain maximal power setting give ﬁxed hyper-parameter—no need user tuning. optimal value decreases variance simultaneously ﬁxed deﬁned changed variance according theorem gretton null hypothesis test statistics degenerates. corresponds theorem large sample sizes null distribution approaches distribution inﬁnite weighted independent random variables weights equal eigenvalues within-distribution covariance operator denote test statistics based i.i.d. random variables scaling factor. harchaoui introduced kernel fisher discriminant analysis homogeneity test regularizing within-distribution covariance operator. maximum fisher discriminant ratio deﬁnes test statistic. empirical kfda test statistic denoted kfda analyze asymptotic behaviour statistics null hypothesis harchaoui consider situations regarding regularization parameter held ﬁxed obtaining limit distribution similar tends zero slower n−/. ﬁrst situation test statistic converges thus test statistics based kfda normalizes kfda weights random variables using covariance operator regularizer. comparison sensitive information higher order moments bigger weights second situation test statistics converges kfda variance function theorem precise analytical normal distribution obtains higher power rmmd. divergence kfda estimation distribution null hypothesis looses accuracy affect power. contrast kfda rmmd consistent since divergence null hypothesis happen more. rmmd generalized form test statistics based obtain moreover minimizing variance normal distribution obtain best power thus hyper-parameter ﬁxed without requiring tuning user. comparison kfda rmmd require restrictive constraints obtain high power. also results higher power kfda cases small sample size. speed power convergence kfda slower number samples rmmd convergence test statistic normal distribution enables fast consistent straightforward estimation null distribution within time without need using estimation method. results power comparison tests reported section assess power control asymptotic relative efﬁciency. criterion shows rmmd better test statistic obtains higher power rather kfda smaller sample size. relative efﬁciency enables select effective statistical test quantitatively test statistics compared. necessary sample size test statistics achieve power signiﬁcance level denoted relative efﬁciency statistical test respect statistical test given test statistic considered better smaller means needs lower sample size obtain power given authors assessed power control means analysis local alternatives work large sample size tends inﬁnity. article focus attention small sample size case challenging. section compute kfdarmmd rmmd kfda using artiﬁcial datasets types kernels obtain smaller rmmd rather kfda mmd. means rmmd gives higher power much smaller sample size. results different data sets reported table figure figure experimentally shown outperform many traditional two-sample tests generalized wald-wolfowitz test generalized kolmogorov-smirnov test hall-tajvidi biau-gy¨orf test. shown kfda outperforms hall-tajvidi test test. select hall traditional baseline methods compare rmmd kfda mmd. experimentally evaluate utility proposed hypothesis testing method present results various artiﬁcial real-world benchmark datasets. proposed method used testing homogeneity structured data advantage traditional two-sample tests. artiﬁcially generated distributions locally compact abelian groups applied rmmd-test decide whether samples come distributions not. suppose ﬁrst sample drawn uniform distribution unit interval. sample drawn perturbed uniform distribution density sin. higher perturbation frequencies becomes harder discriminate since distributions periodic nature characteristic kernel tailored periodic domain coshmod samples distribution type error computed comparing prediction ground truth repetition. average results runs. signiﬁcance level perform experiment kfda hall. powers homogeneity test comparing mentioned methods reported table periodic. best power achieved rmmd expected results kernel methods better traditional ones. since selection kernel critical choice kernel-based methods also investigated usage different kernel replaced previous kernel hyperparameter. report best results achieved periodic table reader referred detailed study kernels. also report results problem comparing -dimensional gaussian distributions samples zero mean vector covariance matrix respectively. dataset referred gaussian table investigation effect kernel selection tuning parameters showed best results achieved kernels parameters obtain supreme value mmd. reported results agree. results kernel-based test statistics improved kernel justiﬁcation parameter tuning cases rmmd outperform kfda mmd. instance result periodic kernel tuned hyper-parameter better ﬁrst periodic kernel without hyper-parameter gaussian kernel-processed datasets median distance data points provided best results. used -fold cross validation procedure tune parameters experiment. effect changing power simulated tests ﬁrst testing similarity uniform distribution second cases best power obtained results slightly differ theoretical value relatively small sample sizes used tests. samples larger sizes obtained maximal power results depicted figure assure statistical test aggressive rejecting null hypothesis reported results type error rmmd kfda different sample sizes figure samples supposed drawn used gaussian kernel variance equals medium distance data points. results averaged runs conﬁdence interval obtained replicates. rmmd obtains zero type error smaller sample sizes results kfda comparable. mmdkfda assess power control test statistics also compared kfdarmmd uniform distribution alternative obtained smaller rmmd rather kfda mmd. means rmmd gives higher power fewer samples. table shows results averaged runs periodic data figure depicts detailed results type error rmmd kfda based different sample sizes ares also calculated complex tasks. consider ﬁrst sample drawn uniform distribution unit area. sample drawn perturbed uniform distribution density sinsin. increasing values discrimination becomes harder range changes call problems puni puni respectively. best results statistical kernel-based methods achieved using characteristic kernel tailored periodic domain tuned using -fold cross validation procedure. results reported table show much smaller values rmmd rather kfda mmd. figure shows detailed results type error rmmd kfda based different sample sizes different frequencies displayed figure rmmd obtains robust result zero type error samples different frequencies. instead kfda need much larger samples difﬁcult cases larger obtain power one. moving synthetic data standard benchmarks tested method three datasets mnist dataset handwritten digits covertype dataset forest cover types flare-solar dataset compare performance rmmd kfda using pairwise approach testing differences distributions classes table average results runs. family wide level rmmd-based test achieves higher power methods recorded four subjects performing visual task. checkerboard presented subject’s left visual ﬁeld. refer details data collection preprocessing. learning task subject signal distributions assigned electrodes. data contain fig. probability density function puni left probability density function puni right. increases probability density function looks similar uniform distribution discrimination becomes difﬁcult test statistics. instances dimensional feature vector distribution. goal hypothesis testing disambiguate signals recorded electrodes corresponding early visual cortex rest. difﬁcult signal-to-noise ratio similarity patterns electrodes. moreover high number electrodes makes experiment good candidate assess multiple comparison part method. one-vs-all approach normalized distribution electrode compared normalized combined distribution electrodes. rmmd gaussian kernel used hypothesis test. parameter gaussian kernel median distance data points. results hypothesis test reject null hypothesis conﬁrm dissimilarity distributions electrodes. results pairwise approach rmmd depicted figure neuroscientists usually subjectively assess results obtained imaging techniques inferred machine learning. instance current experiment expectation electrodes fig. results rmmd hypothesis tests data recorded electrodes subject bottom respectively. categorized electrodes recognized methods related visual task colored. region categorized together means imaging techniques multiple comparisons. electrodes area confused belonging high noise. figure describes categorization electrodes. assess results quantitatively means false discovery rates using following fdrs compare results rmmd fig. reference image electrodes shown left. categorized electrodes four groups follows electrodes corresponding visual cortex region interest peripheral electrodes wrongly detected noise electrodes left visual cortex often detected noise interrelation brain areas remaining electrodes. right results rmmd quantitatively compared based fdrs deﬁned text. smallest robust fdrs obtained rmmd. novel regularized maximum mean discrepancy kernel-based test statistic generalizing test. proved rmmd overpowers kfda; power consistency obtained higher rate. power control makes rmmd good hypothesis test multiple comparisons especially crucial case small sample sizes. contrast kfda convergence rmmdbased test statistics normal distribution null alternative hypotheses yields fast straightforward rmmd estimates. experiments goldstandard benchmarks data yield state results. work partially funded nano-resolved multi-scale investigations human tactile sensations tissue engineered nanobio-sensors grant state representation reward based learning spiking neuron models psychophysics grant. fukumizu sriperumbudur gretton sch¨olkopf characteristic kernels groups semigroups. advances neural information processing systems. koller schuurmans bengio bottou vol. curran hook gretton borgwadt rasch sch¨olkopf smola kernel method two-sample-problem. advances neural information processing systems. sch¨olkopf platt hofmann vol. press cambridge gretton fukumizu song sch¨olkopf smola kernel statistical test independence. advances neural information processing system. platt c.d. koller singer roweis vol. press cambridge hall tajvidi permutation tests equality distributions high-dimensional settings. biometrika vol. harchaoui bach moulines testing homogeneity kernel ﬁsher discriminant analysis. advances neural nikitin asymptotic efﬁciency non-parametric tests. cambridge university press renyi measures entropy information. proc. fourth berkeley symp math statistics probability scheff´e analysis variance. wiley york serﬂing approximation theorems mathematical statistics. wiley york smola gretton song sch¨olkopf hilbert space embedding distributions. algorithmic learning theory. sriperumbudur gretton fukumizu k.lanckerietg. sch¨olkopf injective hilbert space embeddings probability measures. proceedings annual conference learning theory. servedio zhang omnipress madison walsh multiple comparisons bonferroni corrections false discovery rates. lecture notes vol. w¨achter biegler implementation primal-dual interior point ﬁlter line search algorithm large-scale nonlinear", "year": 2013}