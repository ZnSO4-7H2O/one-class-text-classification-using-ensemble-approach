{"title": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "abstract": "We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural network-based Seq2Seq learning and propose a new model called CopyNet with encoder-decoder structure. CopyNet can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of CopyNet. For example, CopyNet can outperform regular RNN-based model with remarkable margins on text summarization tasks.", "text": "address important problem sequence-to-sequence learning referred copying certain segments input sequence selectively replicated output sequence. similar phenomenon observable human language communication. example humans tend repeat entity names even long phrases conversation. challenge regard copying seqseq machinery needed decide perform operation. paper incorporate copying neural networkbased seqseq learning propose model called copynet encoderdecoder structure. copynet nicely integrate regular word generation decoder copying mechanism choose subsequences input sequence proper places output sequence. empirical study synthetic data sets real world data sets demonstrates efﬁcacy copynet. example copynet outperform regular rnn-based model remarkable margins text summarization tasks. recently neural network-based sequence-tosequence learning achieved remarkable success various natural language processing tasks including limited machine translation syntactic parsing text summarization dialogue systems seqseq essentially encoder-decoder model encoder ﬁrst transform input sequence certain representation transform representation output sequence. adding attention mechanism seqseq ﬁrst proposed automatic alignment machine translation signiﬁcant improvement performance various tasks different canonical encoderdecoder architecture attention-based seqseq model revisits input sequence form dynamically fetches relevant piece information based mostly feedback generation output sequence. paper explore another mechanism important human language communication called copying mechanism. basically refers mechanism locates certain segment input sentence puts segment output sequence. example following dialogue turns observe different patterns subsequences response copied input utterance canonical encoder-decoder variants attention mechanism rely heavily representation meaning might sufﬁciently inaccurate cases system needs refer sub-sequences input like entity names dates. contrast copying mechanism closer rote memorization language processing human being deserving different modeling strategy neural network-based models. argue beneﬁt many seqseq tasks elegant uniﬁed model accommodate understanding rote memorization. towards goal propose copynet capable regular generation words also operation copying appropriate segments input sequence. despite seemingly hard operation copying copynet trained end-toend fashion. empirical study synthetic datasets real world datasets demonstrates efﬁcacy copynet. encoder-decoder rnn-based encoder-decoder successfully applied real world seqseq tasks ﬁrst sutskever encoder-decoder framework source sequence converted ﬁxed length vector encoder i.e. {ht} states so-called context vector dynamics function summarizes hidden states e.g. choosing last state hts. practice found gated alternatives lstm often perform much better vanilla ones. attention mechanism ﬁrst introduced seqseq release burden summarizing entire source ﬁxed-length vector context. instead attention uses dynamically changing context decoding process. natural option represent weighted source hidden states i.e. function shows correspondence strength attention approximated usually multi-layer neural network note source sentence encoded bi-directional making hidden state aware contextual information ends. cognitive perspective copying mechanism related rote memorization requiring less understanding ensuring high literal ﬁdelity. modeling perspective copying operations rigid symbolic making difﬁcult soft attention mechanism integrate fully differentiable neural model. section present copynet differentiable seqseq model copying mechanism trained end-to-end fashion gradient descent. encoder bi-directional used transform source sequence series hidden states equal length hidden state corresponding word representation source hts} considered short-term memory later accessed multiple ways generating target sequence score functions generate-mode copy-mode respectively normalization term shared eψc. shared normalization term modes basically competing softmax function rendering different canonical deﬁnition mixture model also pictorially illustrated figure score mode calculated prediction copying generation assume vocabulary out-of-vocabulary word. addition another words unique words source sequence xts}. since contain words copying sub-sequence enables copynet output words. nutshell instance-speciﬁc vocabulary source normalization term equals c|st− considering exist multiple positions source sequence. practice often concentrated location among multiple appearances indicating prediction closely bounded location words. sense performs type read similar attentive read however higher precision. remainder paper referred selective read. speciﬁcally designed copy mode pinpointing precision corresponding naturally bears location source sequence encoded hidden state. discussed section particular design potentially helps copy-mode covering consecutive sub-sequence words. source hybrid addressing hypothesize copynet uses hybrid strategy fetching content combines content-based location-based addressing. addressing strategies coordinated decoder managing attentive read selective read well determining enter/quit copy-mode. semantics word location encoded hidden states properly trained encoder rnn. judging experiments attentive read copynet driven semantics language model therefore capable traveling freely even across long distance. hand copynet enters copy-mode selective read often guided location information. result selective read often takes rigid move tends cover consecutive words including unks. unlike explicit design hybrid addressing neural turing machine copynet subtle provides archixj rdh×ds non-linear activation function considering non-linear transformation help project semantic space. empirically also found using tanh non-linearity worked better linear transformation used following experiments. calculating copy-mode score hidden states hts} represent word source sequence xts} since bidirectional encodes content also location information hidden states location informaton important copying note probabilities equal considering multiple source symbols decoding naturally appear source sequence appears source. state update copynet updates decoding state previous state previous symbol context vector following generic attention-based seqseq model. however minor changes yt−−→st path copying mechanism. speciﬁcally represented source sequence copy-mode contribute mixture model gradient less encourage copy-mode; otherwise copy-mode discouraged competition shared normalization term practice cases mode dominates. regular symbols vocabulary size shown table below rule produce number instances replacing variables randomly generated subsequences vocabulary. create types rules including task learn seqseq transformation training instances. dataset designed study behavior copynet handling simple rigid patterns. since strings repeat random also viewed extreme cases rote memorization. experimental setting select artiﬁcial rules dataset rule instances generated split training testing compare accuracy copynet encoderdecoder without attention fair comparison bi-directional encoder another decoder seqseq models hidden layer size word embedding dimension size beam search testing. prediction considered provides simple moving step right speciﬁcally assuming selective read concentrates word update−−−→st state-update operation acts location location+ making favor +)th word prediction predict−−−→ copy-mode. leads −−−−→ζ state uphandling out-of-vocabulary words although hard verify exact addressing strategy directly strong evidence empirical study. saliently properly trained copynet copy fairly long segment full words despite lack semantic information representation. provides natural extend effective vocabulary include words source. although change small seems quite signiﬁcant empirically alleviating problem. indeed many applications much words target side example proper nouns essentially replicates source side. learning although copying mechanism uses hard operation copy source choose paste generate symbols vocabulary copynet fully differentiable optimized end-to-end fashion using backpropagation. given batches source target sequence {x}n objectives minimize negative log-likelihood superscripts index instances. since probabilistic model observing target word mixture generate-mode copy-mode need additional labels modes. network learn coordinate modes data. specifically particular word found clear table copynet signiﬁcantly outperforms rule-types except indicating copynet effectively learn patterns variables accurately replicate rather long subsequence symbols proper places.this hard enc-dec difﬁculty representing long sequence high ﬁdelity. difﬁculty alleviated attention mechanism. however attention alone seems inadequate handling case strict replication needed. closer look reveals decoder dominated copy-mode moving subsequence replicate switch generate-mode leaving area showing copynet achieve rather precise coordination modes. text summarization automatic text summarization aims condensed representation capture core meaning original document. recently formulated seqseq learning problem essentially gives abstractive summarization since summary generated based represencontrast extractive tation document. summarization extracts sentences phrases original text fuse summaries therefore making better overall structure original document. sense copynet summarization lies somewhere categories since part output summary actually extracted document fused together possibly words generate-mode. dataset evaluate model recently published lcsts dataset large scale dataset short text summarization. dataset collected news medias sina weibo including pairs chinese. shown table part manually rated quality following setting part training subset part scored testing set. experimental setting copynet based character word word-based variant word-segmentation obtained jieba. vocabulary size respectively much smaller models variants embedding dimension size hidden layers following evaluate test performance commonly used rouge- rouge- rouge-l compare models essentially canonical encoder-decoder variant attention. figure examples copynet lcsts compared context. word segmentation applied input words underlined. highlighted words words copy-mode probability higher generate-mode. also provide literal english translation document golden copynet omitting context since language broken. one. possible explanation wordbased model even much larger vocabulary still large proportion oovs large number entity names summary data mistakes word segmentation. copynet ability handle words copying mechanism performs however slightly better word-based variant. case study shown figure make following interesting observations summary copynet words copy-mode summary usually still ﬂuent; copynet tends cover consecutive words original document often puts together segments away other indicating sophisticated coordination content-based addressing location-based addressing; copynet handles words really well generate acceptable summary document many oovs even summary often contains many words. contrast canonical rnn-based approaches often fail cases. quite intriguing copynet often important parts document behavior characteristics extractive summarization often generate words connect words showing aspect abstractive summarization. single-turn dialogue experiment follow work neural dialogue model proposed test copynet single-turn dialogue. basically neural model learns generate response user’s input given pairs training instances. experimental setting create datasets ds-i ds-ii slot ﬁlling collected patterns. main difference datasets ﬁlled substrings training testing ds-ii overlaps ds-i sampled pool. dataset instances training testing. compare copynet canonical rnnsearch character-based model conﬁguration section compare copynet rnnsearch ds-i ds-ii terms top- top- accuracy estimating respectively chance top- top- matching golden. since often many good responses input accuracy appears closer real world setting. shown table copynet signiﬁcantly outperforms rnnsearch especially ds-ii. suggests introducing copying mechanism helps dialogue system master patterns dialogue correctly identify correct parts input often proper nouns replicate response. since ﬁlled substrings overlaps ds-ii performance rnnsearch drops signiﬁcantly cannot handle words unseen training data. contrast performance copynet drops slightly learned slots copying mechanism relies less representation words. figure examples testing ds-ii shown input text golden outputs rnnsearch copynet. words rectangles unseen training set. highlighted words words copy-mode probability higher generate-mode. green cirles cross given based human judgment whether response appropriate. rest answers smoothly generatemode. note decoding sequence exactly standard still correct regarding meanings. contrast although rnnsearch usually generates answers right formats fails catch critical entities three cases difﬁculty brought unseen words. related work work partially inspired recent work pointer networks pointer mechanism used predict output sequence directly input. addition difference application cannot predict outside input sequence copynet naturally combine generating copying. copynet also related effort solve problem neural machine translation. luong introduced heuristics postprocess translated sentence using annotations source sentence. contrast copynet addresses problem systemic end-to-end model. however copynet copies exact source words output cannot directly applied machine translation. however copying mechanism naturally extended types references except input sequence help applications heterogeneous source target secopying mechanism also viewed carrying information next stage without nonlinear transformation. similar ideas proposed training deep neural networks classiﬁcation tasks shortcuts built layers direct carrying information. recently noticed parallel efforts towards modeling mechanisms similar related copying. cheng lapata devised neural summarization model ability extract words/sentences source. gulcehre proposed pointing method handle words summarization contrast copynet general limited speciﬁc task words. moreover softmaxcopynet ﬂexible gating related work handling mixture modes ability adequately model content copied segment. conclusion future work proposed copynet incorporate copying sequence-to-sequence learning framework. future work extend idea task source target heterogeneous types example machine translation. acknowledgments work supported part china national project alessandro sordoni michel galley michael auli chris brockett yangfeng margaret mitchell jian-yun jianfeng bill dolan. neural network approach context-sensitive generation conversational responses. arxiv preprint arxiv.. oriol vinyals łukasz kaiser terry slav petrov ilya sutskever geoffrey hinton. grammar foreign language. advances neural information processing systems pages", "year": 2016}