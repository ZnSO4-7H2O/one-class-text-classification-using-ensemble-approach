{"title": "Sequential Short-Text Classification with Recurrent and Convolutional  Neural Networks", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Recent approaches based on artificial neural networks (ANNs) have shown promising results for short-text classification. However, many short texts occur in sequences (e.g., sentences in a document or utterances in a dialog), and most existing ANN-based systems do not leverage the preceding short texts when classifying a subsequent one. In this work, we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts. Our model achieves state-of-the-art results on three different datasets for dialog act prediction.", "text": "recent approaches based artiﬁcial neural networks shown promising results short-text classiﬁcation. however many short texts occur sequences existing ann-based systems leverage preceding short texts classifying subsequent one. work present model based recurrent neural networks convolutional neural networks incorporates preceding short texts. model achieves state-of-the-art results three different datasets dialog prediction. short-text classiﬁcation important task many areas natural language processing including sentiment analysis question answering dialog management. many different approaches developed short-text classiﬁcation using support vector machines rule-based features combining svms naive bayes building dependency trees conditional random fields several recent studies using anns shown promising results including convolutional neural networks recursive neural networks however short texts usually appear sequence therefore using information preceding short texts improve classiﬁcation accuracy. previous works sequential short-text classiﬁcation mostly based non-ann approaches hidden markov models maximum entropy naive bayes inspired performance ann-based systems non-sequential short-text classiﬁcation introduce model based recurrent neural networks cnns sequential short-text classiﬁcation evaluate dialog classiﬁcation task. dialog characterizes utterance dialog based combination pragmatic semantic syntactic criteria. accurate detection useful range applications speech recognition automatic summarization model achieves state-of-the-art results three different datasets. model comprises parts. ﬁrst part generates vector representation short text using either architecture discussed section figure second part classiﬁes current short text based vector representations current well preceding short texts presented section figure figure four instances two-layer feedforward used predicting probability distribution classes short-text stands short text vector rnn/cnn architecture generates left right history sizes corresponds non-sequential classiﬁcation case. short-text representation given short text length represented sequence m-dimensional word vectors used model produce n-dimensional short-text representation symbols tanh refer elementwise sigmoid hyperbolic tangent functions element-wise multiplication. pooling layer sequence vectors output layer combined single vector represents short-text using following mechanisms last mean pooling. last pooling takes last vector i.e. mean pooling averages vectors i.e. pooling takes element= wise maximum cnn-based short-text representation using ﬁlter rh×m height convolution operation consecutive word vectors starting word outputs scalar feature relu perform convolution operations different ﬁlters denote resulting features whose dimensions comes distinct ﬁlter. repeating convolution operations sequential short-text classiﬁcation n-dimensional short-text representation given architecture short text sequence. sequence si−d−d two-layer feedforward predicts class short text. hyperparameters history sizes used ﬁrst second layers respectively. datasets experimental setup datasets evaluate model dialog classiﬁcation task using following datasets dstc dialog state tracking challenge followed number utterances parenthesis. training model trained minimize negative loglikelihood predicting correct dialog acts utterances train using stochastic gradient descent adadelta update rule gradient descent step weight matrices bias vectors word vectors updated. regularization dropout applied pooling layer early stopping used validation patience epochs. table accuracy different architectures history sizes setting report average computed runs. sequential classiﬁcation outperforms non-sequential classiﬁcation overall model outperformed lstm model datasets albeit small margin except swda. also tried variant lstm model gated recurrent units results generally lower lstm. word vectors pretrained glove twitter mrda swda choices yielded best results among publicly available wordvec glove senna rnnlm word vectors. effects history sizes short-text class representations respectively presented table lstm models. models increasing keeping improved performances percentage points. conversely increasing keeping yielded better results performance increase less pronounced incorporating sequential information short-text representation level effective class representation level. using sequential information shorttext representation level class representation level help cases even lower performances. hypothesize shorttext representations contain richer general information class representations larger dimension. class representations convey additional information shorttext representations likely propagate errors previous misclassiﬁcations. table compares results state-of-theart. overall model shows competitive results requiring human-engineered features. rigorous comparisons challenging draw many important details text preprocessing train/valid/test split vary many studies fail table accuracy models methods literature. majority class model predicts frequent class. graphical model naive bayes memory-based learning models features derived transcribed words well previous predicted dialog acts except naive bayes. interlabeler agreement could obtained swda. lstm models presented results test accuracy highest accuracy validation set. article presented ann-based approach sequential short-text classiﬁcation. demonstrate adding sequential information improves quality predictions performance depends sequential information used model. model achieves state-of-theart results three different datasets dialog prediction.", "year": 2016}