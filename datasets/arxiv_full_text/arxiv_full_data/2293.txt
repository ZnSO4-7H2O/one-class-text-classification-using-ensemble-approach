{"title": "Isolating Sources of Disentanglement in Variational Autoencoders", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We decompose the evidence lower bound to show the existence of a term measuring the total correlation between latent variables. We use this to motivate our $\\beta$-TCVAE (Total Correlation Variational Autoencoder), a refinement of the state-of-the-art $\\beta$-VAE objective for learning disentangled representations, requiring no additional hyperparameters during training. We further propose a principled classifier-free measure of disentanglement called the mutual information gap (MIG). We perform extensive quantitative and qualitative experiments, in both restricted and non-restricted settings, and show a strong relation between total correlation and disentanglement, when the latent variables model is trained using our framework.", "text": "believes encouraging independence latent variables causes disentanglement. however strong evidence linking factorial representations disentanglement. part attributed problems evaluation procedure itself. traversals latent representation illustrate disentanglement qualitatively proper statistical measures disentanglement show subtle differences models robustness hyperparameter settings. contribute learning disentangled representations ﬁrst showing widely used objective learning representations evidence lower bound rewritten contain terms describing independence latent variables mutual information latent variables observed data based this analyze β-vae objective propose improved β-tcvae requires additional hyperparameters training. evaluate method propose mutual information principled information-theoretic quantity measures disentanglement compared β-vae βtcvae reliable discover features exhibit stronger correlation factorial disentangled representations discuss existing works either learning disentangled representations without supervision evaluating learned representations. problems inherently related since improvements learning algorithms require evaluation metrics sensitive subtle details stronger evaluation metrics reveal deﬁciencies decompose evidence lower bound show existence term measuring total correlation latent variables. motivate β-tcvae reﬁnement state-ofthe-art β-vae objective learning disentangled representations requiring additional hyperparameters training. propose principled classiﬁer-free measure disentanglement called mutual information perform extensive quantitative qualitative experiments restricted non-restricted settings show strong relation total correlation disentanglement latent variables model trained using framework. learning disentangled representations without supervision difﬁcult open problem. disentangled variables generally considered contain combination interpretable semantic information ability independently generative process. proper expression disentanglement subject philosophical debate many believe factorial representation statistically independent variables good starting point representation distills information compact form oftentimes semantically meaningful useful variety tasks instance found representations generalizable robust adversarial attacks. many state-of-the-art methods learning disentangled representations based increasing penalty parts existing objective. instance chen claim mutual information latent variables observed data encourage latent variables becoming interpretable. tangentially higgins higgins metric deﬁned accuracy vc-dimension linear classiﬁer achieve identifying ﬁxed ground truth factor. speciﬁcally ground truth factors {vk}k training data point aggregation samples drawn i.i.d. random vectors ﬁxed value classiﬁcation target drawback method lack axis-alignment detection. believe truly disentangled model contain latent variable related factor. means include axis-alignment detection mnih proposes using argminj varq majority-vote classiﬁer. classiﬁer-based disentanglement metrics tend produce varying results depending hyperparameters. higgins mnih metrics loosely interpreted measuring reduction entropy observed. section show possible directly measure mutual information principled information-theoretic quantity used latent distributions provided efﬁcient estimation exists. hoffman johnson analyzed elbo decomposition showcasing term quantiﬁes criterion section introduce reﬁned decomposition showing terms describing criteria appear elbo. following notation decomposition hoffman johnson identify training example unique integer index deﬁne uniform random variable relate note sampled using intermediate data variational autoencoder variational autoencoder latent variable model pairs top-down generative model bottom-up inference network. instead directly performing maximum likelihood estimation intractable marginal loglikelihood training done optimizing tractable evidence lower bound would like optimize lower bound averaged empirical distribution decoder encoder parameterized deep neural networks. certain families distributions gradients estimated using reparameterization trick. kingma welling additionally show properly trained disentangle emotion pose simple face images latent manifold. higgins argue factorial latent representations also encouraged become independent. simple penalization shown capable obtaining models high degree disentanglement image datasets. however isn’t made explicit penalizing dkl||p) factorial prior lead learning latent variables exhibit disentangled transformations data samples. infogan infogan variant generative adversarial network encourages interpretable latent representation maximizing mutual information observation small subset latent variables. approach relies optimizing lower bound intractable mutual information. true underlying generative factors known reason believe factors disentangled possible create supervised evaluation metric. many proposed classiﬁer-based metrics decomposition corresponds increasing weight three terms. speciﬁcally means objective encourages total correlation simultaneously penalizes index-code mutual information. hypothesize total correlation main reason β-vae achieves empirical success learning disentangled representations. however importantly emphasize β-vae penalizes index-code mutual information weight cannot obtain total correlation without also obtaining index-code encourages model discard information latent variables resulting poor latent variables generative model. reasoning resonates observed behavior hyperparameter requires careful tuning large values typically fail practice possible assign different penalty weights terms instead propose simple alternative β-vae objective penalize total correlation term. penalize index-code since interested obtaining useful latent-variable model. moreover neither penalize dimensionwise believe interesting learning disentangled representations. allows train latent-variable models encourage independent aggregated posterior increase chances obtaining disentangled representation. note different penalization coefﬁcients used terms decomposition keep coefﬁcients ensure valid lower bound likelihood. data points. furthermore deﬁne refer aggregated posterior following makhzani notation decompose term assuming factorized decomposition shown denotes dimension latent variable similar decomposition hoffman johnson refer index-code mutual information index-code mutual information data variable latent variable based empirical data distribution fact viewed consistent biased estimator mutual information true data generating distribution expectation index-code lower bound. since empirical data distribution subtlety higher index-code would imply latent variables sufﬁcient information distinguishing empirical samples. chen argued higher mutual information lead disentanglement even proposed completely disregarding term referred total correlainformation theory tion many generalizations mutual information random variables naming unfortunate actually measure dependence variables. argue heavier penalty term induces disentangled representation since intuitively term measures difference actual joint distribution aggregate posterior ideal factorized joint. refer dimension-wise term mainly prevents individual latent dimensions deviating corresponding priors. acts complexity penalty aggregate posterior reasonably follows minimum description length formulation elbo. table comparison prior metrics proposed detects axis-alignment unbiased hyperparameter settings generally applied latent distributions provided efﬁcient estimation exists. measure notion disentanglement empirical generative process ground truth latent factors known. often semantically meaningful scalar attributes data sample. instance photographic portraits generally contain disentangled factors pose lighting condition attributes face skin tone gender face width etc. though ground truth factors provided still possible evaluate disentanglement using known factors. propose metric based empirical mutual information latent variables ground truth factors. insight empirical mutual information latent variable ground truth factor estimated using joint distribution deﬁned eppq]. assuming underlying factors generating process known empirical data samples mutual information information-theoretic quantity able describe relationship regardless parameterization. higher mutual information implies contains information mutual information maximal exists invertible relationship furthermore quantized mutual information bounded entropy such normalized mutual information i/h. decomposed expression requires evaluation depends entire empirical dataset. such undesirable compute exactly training. discuss possible estimate stochastically using minibatch samples without added hyperparameters. first notice na¨ıve monte carlo approximation based minibatch samples likely underestimate intuitively seen viewing mixture distribution data index indicates mixture component. randomly sampled component close whereas would large component came from. much better sample component weight probability appropriately. note minibatch estimator consistent variance biased since expectation lower bound. despite bias note trick crucial enabling training advantageous absence additional hyperparameters compared methods require training auxiliary neural nets difﬁcult compare disentangling algorithms without proper metric. prior works resorted visualizing effects change representation. tedious cannot used measure robustness approach. furthermore metric robust hyperparameters properties robustness properly attributed algorithm. existing classiﬁer-based metrics simple classiﬁer reduce amount variance necessity designing dataset hyperparameter-free bias metric value propose infogan modifying generative adversarial networks objective encourage high mutual information small subset latent variables observed data. motivates removal index-code term explored recent works however investigations generative modeling also claim penalized mutual information information bottleneck encourages compact disentangled representations means describe properties disentangled representations factorial representations motivated many sometimes total correlation even equated measurement disentanglement. similar vein non-linear independent component analysis studies problem inverting generative process assuming independent latent factors. instead perfect inversion maximizing mutual information learned representation ground truth factors. simple priors encourage interpretability means warping complex factors simpler manifolds. recently mnih also explored penalizing total correlation variational autoencoder; however explicitly acknowledge existence term already elbo factorial priors. furthermore minibatch importance sampling allows adding arbitrary weights term decomposition require additional hyperparameters whereas learning algorithm mnih requires auxiliary discriminator network estimate total correlation term. best knowledge ﬁrst show strong relation between factorial representations disentanglement perform series quantitative qualitative experiments showing β-tcvae consistently achieve higher scores compared state-of-the-art methods β-vae infogan. furthermore possible compute average maximal formulation defends important cases. ﬁrst case related rotation factors. latent variables axis-aligned variable contain decent amount information regarding factors. heavily penalizes unaligned variables indication entanglement. second case related compactness representation. latent variable reliably models ground truth factor unnecessary latent variables also informative factor. summarized table metric detects axis-alignment generally applicable meaningful factorized latent distribution including vectors multimodal categorical structured distributions. because metric limited whether mutual information estimated. efﬁcient estimation mutual information ongoing research topic simple estimator sufﬁcient datasets use. better capture subtle differences models compared existing metrics. systematic experiments analyzing existing metrics placed supplementary materials. focus discussing learning disentangled representations unsupervised manner. nevertheless note inverting generative processes known disentangled factors weak supervision pursued many. goal case perfect inversion distill simpler representation although explicitly main motivation many unsupervised generative modeling frameworks explored disentanglement learned representations prior β-vae shown successful disentanglement limited settings factors variation higher values mutual information penality βvae strong hinders usefulness latent variables. however β-tcvae higher values consistently results models higher disentanglement score relative β-vae. disentangled representation achievable learning algorithms chances obtaining representation typically isn’t clear. unsupervised learning disentangled representation high variance since disentangled labels provided training. understand robustness algorithm show plots depicting quartiles score distribution various methods figure used β-vae β-tcvae based modes figure infogan used continuous latent codes noise variables. settings chosen following suggested chen also added instance noise stabilize training. despite best efforts unable train infogan disentangle dsprites dataset. likely pixels input images take binary values required instance noise trick even train. general median score highest β-tcvae close highest score achieved methods. despite best half β-tcvae runs achieving high scores half still perform poorly. modeling faces β-tcvae exist low-score outliers although scores still higher median score achieved infogan. total correlation previously conjectured lead disentanglement provide concrete evidence β-tcvae learning algorithm satisﬁes property. figure shows scatter plot total correlation disentanglement metric varying values trained dsprites faces datasets averaged random initializations. models trained β-tcvae correlation average average figure compared β-vae β-tcvae creates disentangled representations preserving better generative model data increasing shaded regions show conﬁdence intervals. higher better metrics. first analyze performance proposed βtcvae metric restricted setting ground truth factors uniformly independently sampled. paint clearer picture robustness learning algorithms aggregate results multiple experiments visualize effect initialization adam learning rate used train models. perform quantitative evaluations datasets dataset shapes dataset synthetic faces ground truth factors summarized table dsprites faces also contain types shapes identities respectively treated noise evaluation. since β-vae β-tcvae objectives lower bounds standard elbo would like effect training modiﬁcation. choice affects learning algorithms train using range values. trade-off density estimation amount disentanglement measured shown figure figure scatter plots average value larger circles indicate higher average β-tcvae makes better total correlation scores reach higher disentanglement. note log-scale though pearson correlation coefﬁcients estimated without using log. models trained β-vae lower correlation. dsprites data higher values β-vae seems stop decreasing among latent variables disentanglement decreases. meanwhile β-vae achieves lower total correlation β-tcvae faces dataset possibly indication latent variables regressing towards prior becoming inactive. general degree total correlation β-tcvae creates better disentangled model. also strong evidence hypethsis β-vae large values decreases index-code mutual information latent variables leading worse generative model without increased chances learning disentangled representation. notion disentanglement exist even underlying generative process samples factors non-uniformly dependently. many real datasets exhibit behavior conﬁgurations factors sampled others violating statistical independence assumption. disentangling factors variation case corresponds ﬁnding generative model latent factors independently perturb generated result even bias sampling procedure. general β-tcvae problem ﬁnding correct factors variation dataset interpretable factors variation found prior work even though independence assumption violated. start dataset factors test β-tcvae using sampling distributions varying degrees correlation dependence. take datset synthetic faces lighting factor. joint distributions factors test summarized figure includes varying degrees sampling figure β-tcvae higher chance obtaining disentangled representation β-vae even presence sampling bias. samples non-zero probability joint distributions; likely sample times likely least likely sample. bias. speciﬁcally conﬁguration uses uniform independent factors; uses factors non-uniform marginals uncorrelated independent; uses uncorrelated dependent factors; uses correlated dependent factors. method train models compute mig. reduction three factors enough give reasonable results β-vae β-tcvae. infogan used continuous codes noise variables. plots showing quartiles conﬁguration shown figure possible train disentangled model conﬁgurations chances obtaining overall lower exist sampling bias. across conﬁgurations βtcvae superior β-tcvae infogan large difference median scores conﬁgurations. chairs figure shows traversals latent variables depict interpretable property generating chairs. β-vae shown capable learning ﬁrst four properties azimuth size style backrest. however style change learned β-vae seem consistent chairs. β-tcvae learn additional interpretable figure qualitative comparisons celeba. traversal ranges shown parentheses. attributes manifested direction latent variable show one-sided traversal. semantically similar variables β-vae shown comparison. properties material chair rotation swivel chairs. properties subtle likely require higher index-code mutual information lower penalization index-code β-tcvae helps ﬁnding properties. celeba figure shows attributes discovered β-tcvae without supervision traverse large range show effect generalizing represented semantics variable. representation learned β-vae entangled nuances shown generalizing probability regions. instance difﬁculty rendering complete baldness narrow face width whereas β-tcvae shows meaningful extrapolation. extrapolation gender attribute β-tcvae shows focuses gender-speciﬁc facial features whereas β-vae entangled many irrelevances face width. ability generalize beyond ﬁrst standard deviations prior mean implies β-tcvae model generate rare samples bald mustached females. present decomposition elbo propose βtcvae trained stochastically using minibatch importance sampling. evaluate approach propose classiﬁerfree disentanglement metric called mig. show βtcvae superior β-vae infogan learning disentangled representations. particular able discover factors subtle β-vae. alemi alexander fischer dillon joshua murphy kevin. deep variational information bottleneck. international conference learning representations alemi alexander poole fischer dillon joshua saurous murphy kevin. information-theoretic analysis deep latent-variable models. arxiv preprint arxiv. aubry mathieu maturana daniel efros alexei russell bryan sivic josef. seeing chairs exemplar part-based alignment using large dataset models. cvpr bengio yoshua courville aaron vincent pascal. representation learning review perspectives. ieee transactions pattern analysis machine intelligence burgess christopher higgins irina arka matthey loic watters nick desjardins guillaume lerchner alexander. understanding disentangling beta-vae. learning disentangled representations perception control workshop chen duan houthooft rein schulman john sutskever ilya abbeel pieter. infogan interpretable representation learning information maximizing generative adversarial nets. advances neural information processing systems cian eastwood christopher williams. framework quantitative evaluation disentangled representations. international conference learning representations glorot xavier bordes antoine bengio yoshua. domain adaptation large-scale sentiment classiﬁcation deep learning approach. proceedings international conference machine learning goodfellow pouget-abadie jean mirza mehdi bing warde-farley david ozair sherjil courville aaron bengio yoshua. generative adversarial nets. advances neural information processing systems higgins irina matthey loic arka burgess christopher glorot xavier botvinick matthew mohamed shakir lerchner alexander. beta-vae learning basic visual concepts constrained variational framework. international conference learning representations hoffman matthew johnson matthew elbo surgery another carve variational evidence lower bound. workshop advances approximate bayesian inference nips karaletsos theofanis belongie serge r¨atsch gunnar. bayesian representation learning oracle constraints. international conference learning representations kulkarni tejas whitney william kohli pushmeet tenenbaum josh. deep convolutional inverse graphics network. advances neural information processing systems kumar abhishek sattigeri prasanna balakrishnan avinash. variational inference disentangled latent concepts unlabeled observations. arxiv preprint arxiv. makhzani alireza shlens jonathon jaitly navdeep goodfellow frey brendan. adversarial autoencoders. iclr workshop international conference learning representations matthey loic higgins irina hassabis demis lerchner alexander. dsprites disentanglement testing sprites dataset. https//github.com/deepmind/dsprites-dataset/ paysan pascal knothe reinhard amberg brian romdhani sami vetter thomas. face model pose illumination invariant face recognition. advanced video signal based surveillance avss’. sixth ieee international conference ieee radford alec metz luke chintala soumith. unsupervised representation learning deep convolutional generative adversarial networks. arxiv preprint arxiv. reed scott sohn kihyuk zhang yuting honglak. learning disentangle factors variation manifold interaction. international conference machine learning reshef david reshef yakir finucane hilary grossman sharon mcvean gilean turnbaugh peter lander eric mitzenmacher michael sabeti pardis detecting novel associations large data sets. science siddharth paige brooks meent jan-willem desmaison alban torr philip learning disentangled representations semi-supervised deep generative models. advances neural information processing systems sønderby casper kaae caballero jose theis lucas wenzhe husz´ar ferenc. amortised inference image super-resolution. international conference learning representations thomas valentin pondard jules bengio emmanuel sarfati marc beaudoin philippe meurs marie-jean pineau joelle precup doina bengio yoshua. arxiv preprint independently controllable features. arxiv. vedantam ramakrishna fischer huang jonathan murphy kevin. generative models visually grounded imagination. international conference learning representations mutual information estimation inference network compute mutual information assuming model ppq. speciﬁcally compute every pair latent variable ground truth factor reduce variance perform stratiﬁed sampling samples value estimate sample perform stratiﬁed sampling computation time estimatation procedure depends dataset size general done minutes datasets experiments. bound tight model make zero exist invertible function hand mutual information maximal know high conditional entropy suggests metric meaningful measuring much information retains regardless parameterization distributions. minibatch importance sampling first minibatch indices element sampled i.i.d. sampled batch instance denote probability sampled minibatch elements ﬁxed rest sampled i.i.d. gives also performed experiments factorial normalizing ﬂexible prior. dimension normalizing depth parameters trained maximize β-tcvae objective. multi-modal distributions. preliminary experiments found signiﬁcant improvement using factorial gaussian prior decided include paper. figure able capture subtle differences. relation learned variables ground truth factors plotted best β-tcvae β-vae dsprites dataset according metric shown. corresponds ground truth factor column latent variable. plots show relationship latent variable mean versus ground truth factor active latent variables shown. position color blue indicates high value indicates value. colored lines indicate object shape oval green square blue heart. interestingly latent variables rotation peaks oval peaks square suggesting models produced compact code observing object rendered certain degrees rotation. believe metric also invariant hyperparameters. instance existence hyperparameters prior metrics means different hyperparameter values result different metric outputs. additionally even stable classiﬁer always outputs accuracy given dataset creation dataset classiﬁer-based metrics still problematic. aggregated inputs used higgins mnih depend batch size difﬁcult tune leads inconsistent metric values. fact empirically metrics informative small figure plots higgins metric fully trained vaes. increases aggregated inputs become quantized. increase accuracy metric also reduces models making hard discriminate similarly performing models. relative ordering models also preserved different values. give insight capturing show β-tcvae experiments scores near quantized values mig. general gives scores entangled representations even variables axis-aligned. shows clearer pattern scoring position scale less rotation. likely latent variables rotation. unsupervised setting ground truth rotation impossible differentiate ovals squares latent variable simply learns value. evident plots latent variables describing rotation many-to-one. existence factors redundant values downside using scoring mechanism factors appear simple datasets dsprites. note type plot show whole picture. speciﬁcally mean latent variables shown uncertainty latent variables not. mutual information computes reduction uncertainty observing factor uncertainty important cannot easily plotted. changes explained reduction uncertainty even though plots look similar. score near representations appear axis-aligned disentangled. higher scores likely reducing entropy fully match ground truth latent variables would mixture dirac deltas would high dimension-wise factorized gaussian. figure entangled representations relatively high higgins metric correctly scores low. higgins metric tends overly optimistic compared metric. relationships ground truth factors learned latent variables shown controversial models shown dots. colored line indicates different shape sample traversals latent variables model depend rotation clearly mirror other. figure shows model single point based metrics. general metrics agree disentangled models; however metric falls quickly comparatively. fact higgins metric tends output inﬂated score inability detect subtle differences lack axis-alignment. example look controversial models disagreed upon metrics controversial model shown figure dot. metric ranks model better models higgins metric ranks better models. inspecting relationship latent units ground truth factors scale factor seems disentangled position factors axis aligned latent variables rotation appear mirror slight difference. rows figure show traversals corresponding latent variables rotation. clearly simply rotate opposite direction. since higgins metric enforce single latent variable inﬂuence factor mistakenly assigns higher disentanglement score model. note many models near black ﬁgure exhibit similar behavior. controversial models model ordered either metric model assigned unique integer deﬁne controversial model maxα higher rank implies disentanglement. models higgins metric believes highly disentangled believes not. figure shows controversial models. figure controversial models. brackets indicate rank models higgins metric. instance controversial model shown ranked better model higgins metric believes better models. super.__init__ self.output_dim output_dim self.conv nn.convd self.bn nn.batchnormd self.conv nn.convd self.bn nn.batchnormd self.conv nn.convd self.bn nn.batchnormd self.conv nn.convd self.bn nn.batchnormd self.conv nn.convd self.bn nn.batchnormd self.conv_z nn.convd self.act nn.relu super.__init__ self.conv nn.convtransposed self.bn nn.batchnormd self.conv nn.convtransposed self.bn nn.batchnormd self.conv nn.convtransposed self.bn nn.batchnormd self.conv nn.convtransposed self.conv nn.convd self.bn nn.batchnormd self.conv nn.convd self.bn nn.batchnormd self.conv nn.convd self.bn nn.batchnormd self.conv nn.convd self.bn nn.batchnormd self.conv nn.convd self.bn nn.batchnormd self.conv_z nn.convd self.conv_d nn.convd self.act nn.leakyrelu super.__init__ self.conv nn.convtransposed self.bn nn.batchnormd self.conv nn.convtransposed self.bn nn.batchnormd self.conv nn.convtransposed self.bn nn.batchnormd self.conv nn.convtransposed self.bn nn.batchnormd self.conv nn.convtransposed self.bn nn.batchnormd self.conv_final nn.convtransposed self.act nn.leakyrelu super.__init__ self.output_dim output_dim self.conv nn.convd self.bn nn.batchnormd self.conv nn.convd self.bn nn.batchnormd self.conv nn.convd self.bn nn.batchnormd self.conv nn.convd self.bn nn.batchnormd self.conv nn.convd self.bn nn.batchnormd self.conv_z nn.convd self.act nn.relu super.__init__ self.conv nn.convtransposed self.bn nn.batchnormd self.conv nn.convtransposed self.bn nn.batchnormd self.conv nn.convtransposed self.bn nn.batchnormd self.conv nn.convtransposed self.bn nn.batchnormd self.conv nn.convtransposed self.bn nn.batchnormd self.conv_final_mu nn.convtransposed self.act nn.relu", "year": 2018}