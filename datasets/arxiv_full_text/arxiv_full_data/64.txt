{"title": "End-to-End Attention-based Large Vocabulary Speech Recognition", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "abstract": "Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs). Most of these systems contain separate components that deal with the acoustic modelling, language modelling and sequence decoding. We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Network (RNN) that performs sequence prediction directly at the character level. Alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the RNN. For each predicted character, the attention mechanism scans the input sequence and chooses relevant frames. We propose two methods to speed up this operation: limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames, thereby reducing source sequence length. Integrating an n-gram language model into the decoding process yields recognition accuracies similar to other HMM-free RNN-based approaches.", "text": "many current state-of-the-art large vocabulary continuous speech recognition systems hybrids neural networks hidden markov models systems contain separate components deal acoustic modelling language modelling sequence decoding. investigate direct approach replaced recurrent neural network performs sequence prediction directly character level. alignment input features desired character sequence learned automatically attention mechanism built rnn. predicted character attention mechanism scans input sequence chooses relevant frames. propose methods speed operation limiting scan subset promising frames pooling time information contained neighboring frames thereby reducing source sequence length. integrating n-gram language model decoding process yields recognition accuracies similar hmm-free rnn-based approaches. deep neural networks become popular acoustic models state-of-the-art large vocabulary speech recognition systems however systems components hidden markov models gaussian mixture models n-gram language models predecessors. combinations neural networks statistical models often referred hybrid systems. typical hybrid system deep neural network trained replace gaussian mixture model emission distribution predicting input frame likely state. state labels obtained trained gmm-hmm system used perform forced alignment. words two-stage training process required older approach still used starting point. obvious downside hybrid approach acoustic model directly trained minimize ﬁnal objective interest. investigate neural lvcsr models trained direct approach replacing hmms attention-based recurrent sequence generators trained end-to-end sequence prediction. recently work end-to-end neural network lvcsr systems shown promising results. neural network model trained connectionist temporal classiﬁcation achieved promising results wall street journal corpus similar setup used obtain state-of-the-art results switchboard task well models trained predict sequences characters later combined word level language model. furthermore language model implemented ctc-speciﬁc weighted finite state transducer decoding accuracies competitive dnn-hmm hybrids obtained time direction neural network research emerged deals models learn focus attention speciﬁc parts input. systems type shown promising results variety tasks including machine translation caption generation handwriting synthesis visual object classiﬁcation phoneme recognition work investigate application attentionbased recurrent sequence generator part end-to-end lvcsr system. start system proposed make following contributions show training long sequences made feasible limiting area explored attention range promising locations. reduces total training complexity quadratic linear largely solving scalability issue approach. already proposed name windowing used spirit clockwork hierarchical gating introduce recurrent architecture successively reduces source sequence length pooling frames neighboring time. system propose neural network sequences speech frames sequences characters. whole system differentiable trained directly perform task hand still divided different functional parts work together learn encode speech signal suitable feature representation decode representation sequence characters. used rnns encoder decoder parts system. decoder combines attention mechanism attention-based recurrent sequence generator able learn alignment input output. therefore ﬁrst discuss rnns subsequently combined attention mechanisms perform sequence alignment. quite research recurrent neural networks speech recognition probably explained large extent elegant deal sequences variable length. given sequence feature vectors standard computes corresponding sequence hidden state vectors using matrices trainable parameters represent connection weights network vector trainable bias parameters. function often non-linear squashing function like hyperbolic tangent applied element-wise input. hidden states used features serve inputs layer performs task like classiﬁcation regression. given output layer objective optimize differentiable gradient objective respect parameters network computed backpropagation time. like feed-forward networks rnns process discrete input data representing -hot-coding feature vectors. used statistical model sequences labels. that trained predict probability next label conditioned part sequence already processed. sequence labels trained provide conditional distribution next label using matrix trainable connection weights vector bias parameters softmaxi exp. likelihood complete sequence given distribution used generate sequences either sampling distribution choosing likely labels iteratively. equation deﬁnes simplest however practice usually advanced equations deﬁne dependency ht−. famous examples so-called recurrent transitions long short term memory gated recurrent units designed better handle longterm dependencies. work simpler architecture easier implement efﬁciently. hidden states computed using following equations obtain model uses information future frames past frames pass input data recurrent neural networks opposite directions concatenate hidden state vectors. recurrent neural network type often referred bidirectional rnns. finally shown better results speech recognition tasks obtained stacking multiple layers recurrent neural networks simply done treating sequence state vectors input sequence next pile. figure shows example bidirectional rnns stacked construct deep architecture. many challenging tasks involve inputs outputs variable length. examples machine translation speech recognition input output variable length; image caption generation captions variable lengths. encoder-decoder networks often used deal variable length input output sequences encoder network transforms input intermediate representation. decoder typically uses representation order generate outputs sequences described work deep birnn encoder. thus representation sequence birnn state vectors standard deep birnn sequence long input bottom-most layer context speech recongnition means every recordings. found decoder decoder network system attention-based recurrent sequence generator subsection introduces arsgs explains motivation behind choice arsg study. rnns process generate sequential data length sequence hidden state vectors always equal length input sequence. learn alignment sequences model distribution clear functional dependency arsg produces output sequence element time simultaneously aligning generated element encoded input sequence composed additional subnetwork called ‘attention mechanism’. attention selects temporal locations input sequence used update hidden state make prediction next output value. typically selection elements αtlhl called attention weights require figure schematic representation parameter matrices parameter vectors denotes convolution stands previous state component arsg. explain works starting shows weights obtained normalizing scores etl. illustrated score depends previous state content respective location vector socalled convolutional features name convolutional comes convolution along time axis used compute matrix comprises feature vectors attention mechanism described combines information three sources decide focus step decoding history contained content candidate location focus previous step described attention weights αt−. shown making attention location-aware using equations deﬁning crucial reliable behaviour long input sequences. disadvantage approach complexity training procedure since weights computed pairs input output positions. paper showcases windowing approach reduces complexity decoding work apply windowing training stage well. namely constrain attention mechanism consider positions range median interpreted context distribution. values deﬁne much window expands left right respectively. modiﬁcation makes training signiﬁcantly faster. apart speedup brings windowing also helpful starting training procedure. experience becomes increasingly harder train arsg completely scratch longer input sequences. found providing rough estimate desired alignment early training stage effective quickly bring network parameters good range. speciﬁcally forced network choose positions range numbers smin smax vmin vmax roughly estimated training number leading silent frames training utterances smin smax speaker speed i.e. ratio transcript encoded input lengths vmin vmax. aimed make windows narrow possible keeping invariant character pronounced within window resulting sequence windows quickly expanding still sufﬁcient quickly move network random initial mode often aligned characters single location audio data. note median-centered windowing could used purpose since relies quality previous alignment deﬁne window one. although arsg construction implicitly learns output symbol depends previous ones transcriptions training utterances typically insufﬁcient learn high-quality language model. reason investigate arsg integrated language model. main challenge speech recognition word-based language models used whereas arsg models distribution character sequences. weighted finite state transducer framework build character-level language model word-level one. wfst ﬁnite automaton whose transitions weight input output labels. deﬁnes cost transducing fig. schematic representation attention-based recurrent sequence generator. time step combines hidden state input vectors compute attention weights αtl. subsequently hidden state prediction output label computed. input sequence output sequence considering pathes corresponding sequences input output labels. composition operation used combine fsts deﬁne different levels representation characters words case. compose language model finite state transducer lexicon simply spells letters word. speciﬁcally build min) deﬁne log-probability character sequences. push weights towards starting state help hypothesis pruning decoding. tunable parameters. last term important without component dominates cost minimized short sequences. note criterion decoding proposed network. integrating arsg beam-search decoding easy share property current state depends previous input symbol. therefore simple left-to-right beam search algorithm similar described approximate value minimizes determinization becomes impractical moderately large fsts trigram model shipped wall street journal corpus handle non-deterministic fsts assume weights logarithmic semiring compute total logprobability paths corresponding character preﬁx beam. probability quickly recomputed character added preﬁx. build character-level model n-gram model advocated. note research described paper carried independently without communication authors aforementioned works. popular method train networks perform sequence prediction connectionist temporal classiﬁcation used great success phoneme recognition characterbased lvcsr allows recurrent neural networks predict sequences shorter input sequence summing possible alignments output sequence input module. summation done using dynamic programming similar forward backward passes used inference hmm. approach output labels conditionally independent given alignment output sequences. context speech recognition means network lacks language model greatly boosts system performance added trained network extension transducer combines rnns sequence transduction system network similar network runs time-scale input sequence separate models probability next label output label conditioned previous ones. like inference done dynamic programming method similar backward-forward algorithm hmms taking account constraints deﬁned rnns. unlike transduction systems also generate output sequences longer input. transducers state-of-the-art results phoneme recognition timit dataset recently matched asrg network transducer arsg approaches roughly equivalent capabilities. approaches implicit language model learnt jointly rest network. main difference approaches arsg alignment explicitly computed network opposed dealing distribution alignments transducer. hypothesize difference might major impact development methods. finally must mention recently published works partially overlap content paper. encoder-decoder character-based recognition model quite similar ours. particular work pooling birnn layers also proposed. also using fsts trained evaluated models wall street journal corpus training done hour long sentences. input features used mel-scale ﬁlterbank coefﬁcients together energy. dimensional features extended ﬁrst second order temporal derivatives obtain total feature values frame. evaluation done eval evaluation set. hyperparameter selection performed set. language model integration used closed vocabulary setup bigram trigram language model provided data set. text preprocessing leaving distinct labels characters apostrophe period dash space noise end-of-sequence tokens. model used layers forward backward units encoder layers reading every second hidden states network therefore encoder reduced utterance length factor centered convolution ﬁlter width used attention mechanism extract single feature previous step alignment described used rough estimate proper alignment ﬁrst training epoch described section training restarted windowing described section. window parameters corresponds considering large second long span audio data step taking account pooling done layers. training adadelta hyperparameters continued log-likelihood stopped improving. finally annealed best model terms restarting training explained section used beam search minimize combined cost deﬁned ﬁnished terminated sequences cheaper non-terminated sequence beam found. sequence considered terminated ended special end-of-sequence token network trained generate transcript. measure best performance used beam size however brought relative improvement beam size used parameter settings language model without one. necessary asymmetric window attention decoding large speciﬁcally reduced without trick cost could inﬁnitely minimized looping across input utterance penalty jumping back time included high enough. results experiments gathered table model shows performance superior systems external language model used. improvement adding external language model however much larger ctc-based systems. ﬁnal peformance model better reported worse language models used. major difference arsg approaches language model implicitly learnt latter. indeed sequence model explained literally contained arsg subnetwork. believe reason greater performance arsg-based system external used. however implicit language model trained relatively small corpus transcripts containing less million characters. reported rnns overﬁt corpora size experiments combat overﬁtting well. using weight clipping brought consistent performance gain change picture. reasons hypothesize overﬁtting internal language model main reasons model reach performance level reported network used. table character error rate word error rate scores setup wall street journal corpus comparison results literature. note results directly comparable networks predicting phonemes instead characters since phonemes easier targets. said treat advantage arsg supports joint training language model rest network. contains approximately hours training data overﬁtting might less issue corpora containing hundreds even thousands hours annotated speech. language model trained large text corpus could integrated arsg beginning training using states language model additional input arsg. suppose would block incentive memorizing training utterances thereby reduce overﬁtting. addition extra n-gram model would required. note similar idea already proposed machine translation. work showed encoder-decoder network attention mechanism used build lvcsr system. resulting approach signiﬁcantly simpler dominating hmm-dnn fewer training stages much less auxiliary data less domain expertise involved. combined trigram language model system shows decent although state-of-the-art performance. present methods improve computational complexity investigated model. first propose pooling time birnn layers reduce length encoded input sequence. second propose windowing training ensure decoder network performs constant number operations output character. together methods facilitate application attention-based models large-scale speech recognition. unlike networks model intrinsic languagemodeling capability. furthermore potential trained jointly external language model. investigations direction likely part future work. authors would like acknowledge support following agencies research funding computing support national science center nserc calcul qu´ebec compute canada canada research chairs cifar. bahdanau also thanks planet intelligent systems gmbh yandex.", "year": 2015}