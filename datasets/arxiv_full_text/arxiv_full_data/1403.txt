{"title": "Deep Convolutional Neural Network Design Patterns", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Recent research in the deep learning field has produced a plethora of new architectures. At the same time, a growing number of groups are applying deep learning to new applications. Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet). Here we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures. In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files is available at https://github.com/iPhysicist/CNNDesignPatterns). We hope others are inspired to build on our preliminary work.", "text": "recent research deep learning ﬁeld produced plethora architectures. time growing number groups applying deep learning applications. groups likely composed inexperienced deep learning practitioners bafﬂed dizzying array architecture choices therefore older architecture attempt bridge mining collective knowledge contained recent deep learning research discover underlying principles designing neural network architectures. addition describe several architectural innovations including fractal fractalnet network stagewise boosting networks taylor series networks hope others inspired build preliminary work. many recent articles discuss architectures neural networking especially regarding residual networks larsson zhang huang although literature covers wide range network architectures take high-level view architectures basis discovering universal principles design network architecture. discuss original design patterns could beneﬁt inexperienced practitioners seek incorporate deep learning various applications. paper addresses current lack guidance design deﬁciency prompt novice rely standard architecture e.g. alexnet regardless architecture’s suitability application hand. abundance research also opportunity determine elements provide beneﬁts speciﬁc contexts. universal principles deep network design exist? principles mined collective knowledge deep learning? architectural choices work best given context? architectures parts architectures seem elegant? design patterns ﬁrst described christopher alexander regards architectures buildings towns. alexander wrote timeless quality architecture lives quality enabled building based universal principles. basis design patterns resolve conﬂict forces given context lead equilibrium analogous ecological balance nature. design patterns highly speciﬁc making clear follow ﬂexible adapted different environments situations. inspired alexander’s work gang four applied concept design patterns architecture object-oriented software. classic computer science book describes patterns resolve issues prevalent software design requirements always change. inspired previous works architectures articulate possible design patterns convolutional neural network architectures. design patterns provide universal guiding principles take ﬁrst steps deﬁning network design patterns. overall enormous task deﬁne design principles neural networks applications limit paper cnns canonical application image classiﬁcation. however recognize architectures must depend upon application deﬁning ﬁrst design pattern; design pattern architectural structure follows application addition principles allowed discover gaps existing research articulate novel networks units hope rules thumb articulated valuable experienced novice practitioners preliminary work serves stepping stone others discover share additional deep learning design patterns. best knowledge little written recently provide guidance understanding appropriate architectural choices. book neural networks tricks trade contains recommendations network models without reference vast amount research past years. perhaps closest work szegedy authors describe design principles based experiences. much research studied neural network architectures unaware recent survey ﬁeld. unfortunately cannot justice entire body work focus recent innovations convolutional neural networks architectures particular residual networks recent family variants. start network networks describes hierarchical network small network design repeatedly embedded overall architecture. szegedy incorporated idea inception architecture. later authors proposed modiﬁcations original inception design similar concept contained multi-scale convolution architecture meantime batch normalization presented unit within network makes training faster easier. introduction residual networks papers suggested skip connections. skip connections proposed raiko highway networks gating mechanism decide whether combine input layer’s output showed networks allowed training deep networks. dropin technique also trains deep networks allowing layer’s input skip layer. concept stochastic depth drop-path method introduced huang residual networks introduced authors describe network imagenet challenge. able extend depth network tens hundreds layers improve network’s performance. authors followed another paper investigate identity mappings help report results network thousand layers. research community took notice architecture many modiﬁcations original design soon proposed. inception-v paper describes impact residual connections inception architecture compared results results updated inception design. resnet resnet paper suggests duel stream architecture. veit provided understanding residual networks ensemble relatively shallow networks. authors illustrated residual connections allow input follow exponential number paths architecture. time fractalnet paper demonstrated training deep networks symmetrically repeating architectural pattern. described later found symmetry introduced paper intriguing. similar vein convolutional neural fabrics introduces three dimensional network usual depth network ﬁrst dimension. wide residual networks demonstrate simultaneously increasing depth width leads improved performance. swapout layer dropped skipped used normally combined residual. deeply fused nets proposes networks multiple paths. weighted residual networks paper authors recommend weighting factor output convolutional layers gradually introduces trainable layers. convolutional residual memory networks proposes architecture combines convolutional residual network lstm memory mechanism. residual residual networks authors propose adding hierarchy skip connections input skip layer module number modules. densenets introduces network module densely connected; output layer input layers module. multi-residual paper authors propose expanding residual block width-wise contain multiple convolutional paths. appendix describes close relationship many residual network variants. reviewed literature speciﬁcally extract commonalities reduce designs fundamental elements might considered design patterns. seemed clear reviewing literature design choices elegant others less particular patterns described paper following architectural structure follows application proliferate paths strive simplicity increase symmetry pyramid shape over-train cover problem space incremental feature construction normalize layer inputs several researchers pointed winners imagenet challenge successively used deeper networks szegedy simonyan zisserman also apparent imagenet challenge multiplying number paths network recent trend illustrated progression alexnet inception resnets. example veit show resnets considered exponential ensemble networks different lengths. design pattern proliferate paths based idea resnets exponential ensemble networks different lengths. proliferates paths including multiplicity branches architecture. recent examples include fractalnet xception decision forest convolutional networks scientists embraced simplicity/parsimony centuries. simplicity exempliﬁed paper striving simplicity achieving state-of-the-art results fewer types units. design pattern strive simplicity suggests using fewer types units keeping network simple possible. also noted special degree elegance fractalnet design attributed symmetry structure. design pattern increase symmetry derived fact architectural symmetry typically considered sign beauty quality. addition symmetry fractalnets also adheres proliferate paths design pattern used baseline experiments section essential element design patterns examination trade-offs effort understand relevant forces. fundamental trade-off maximization representational power versus elimination redundant non-discriminating information. universal convolutional neural networks activations downsampled number channels increased input ﬁnal layer exempliﬁed deep pyramidal residual networks design pattern pyramid shape says overall smooth downsampling combined increase number channels throughout architecture. another trade-off deep learning training accuracy versus ability network generalize non-seen cases. ability generalize important virtue deep neural networks. regularization commonly used improve generalization includes methods dropout drop-path noted srivastava dropout improves generalization injecting noise architecture. believe regularization techniques prudent noise injection training improves generalization design pattern over-train includes training method network trained harder problem necessary improve generalization performance inference. design pattern cover problem space training data another improve generalization related regularization methods cover problem space includes noise data augmentation random cropping ﬂipping varying brightness contrast like. common thread throughout many successful architectures make layer’s easier. deep networks example single layer needs incrementally modify input partially explains success residual networks since deep networks layer’s output likely similar input; hence adding input layer’s output makes layer’s incremental. also concept part motivation behind design pattern extends beyond that. design pattern incremental feature construction recommends using short skip lengths resnets. recent paper showed experiment using identity skip length network depth ﬁrst portion network trained. design pattern normalize layer inputs another make layer’s easier. normalization layer inputs shown improve training accuracy underlying reasons clear batch normalization paper attributes beneﬁts handling internal covariate shift authors streaming normalization express might otherwise. feel normalization puts layer’s input samples equal footing allows back-propagation train effectively. research wide resnets shown increasing number channels improves performance additional costs extra channels. input data many benchmark datasets channels design pattern input transition based common occurrence output ﬁrst layer signiﬁcantly increases number channels examples increase channels/outputs ﬁrst layer imagenet alexnet inception resnets intuitively makes sense increase number channels ﬁrst layer allows input data examined many ways clear many outputs best. here trade-off cost versus accuracy. costs include number parameters network directly affects computational storage costs training inference. design pattern available resources guide layer widths based balancing costs application’s requirements. choose number outputs ﬁrst layer based memory computational resources desired accuracy. multiple branches three methods used combine outputs; concatenation summation maxout. seems different researchers favorites hasn’t motivation using preference another. section propose rules deciding combine branches. summation common ways combining branches. design pattern summation joining joining performed summation/mean. summation preferred joining mechanism residual networks allows branch compute corrective terms rather entire approximation. difference summation mean best understood considering drop-path residual network input skip connection always present summation causes layers learn residual hand networks several branches branch dropped using mean preferable keeps output smooth branches randomly dropped. researchers seem prefer concatenation design pattern down-sampling transition recommends using concatenation joining increasing number outputs pooling. down-sampling pooling using stride greater good combine branches concatenate output channels hence smoothly accomplishing joining increase number channels typically accompanies down-sampling. maxout used competition locally competitive networks competitive multi-scale networks liao carneiro design pattern maxout competition based maxout choosing activations contrast summation mean activations cooperating; here competition winner. example branch composed different sized kernels maxout useful incorporating scale invariance analogous pooling enables translation invariance. first recommended combining summation/mean concatenation maxout joining mechanisms differing roles within single architecture rather typical situation used throughout. next design pattern proliferate branches modify overall sequential pattern modules fractalnet architecture. instead lining modules maximum depth arranged modules fractal pattern shown named fractal fractalnet network exchange depth greater number paths. drop-path introduced huang works randomly removing branches iteration training though path doesn’t exist network. symmetry considerations opposite method named freeze-path. instead removing branch network training freeze weights though learning rate zero. similar idea proposed recurrent neural networks potential usefulness combining drop-path freeze-path named freeze-drop-path best explained non-stochastic case. figure shows example fractal fractalnet architecture. let’s start training leftmost branch figure drop-path branches. branch train quickly since relatively parameters compared entire network. next freeze weights branch allow next branch right active. leftmost branch providing good function approximation next branch works produce small corrective term. since next branch contains layers previous branch corrective term easier approximate original function network attain greater accuracy. continue process left right train entire network. used freeze-drop-path ﬁnal/bottom join architecture figure named stagewise boosting networks analogous stagewise boosting idea boosting neural networks architecture new. appendix discuss implementation tested. taylor series expansions classic well known function approximation method since neural networks also function approximators short leap fofs sbns consider branches network terms taylor series expansion. hence taylor series implies squaring second branch summation joining unit analogous second order term expansion. similarly third branch would cubed. call taylor series networks precedence idea literature polynomial networks multiplication networks training. results original fractalnet given ﬁrst table baseline. ﬁrst four rows table figure compare test accuracy original fractalnet architectures architectures modiﬁcations advocated design patterns. ﬁrst modiﬁcation concatenation instead fractal-joins downsampling locations networks. results cifar- cifar- kernels better performance competitive maxout multiple scales. figure also illustrates experiment replacing pooling average pooling throughout architecture changes training proﬁle. cifar- training accuracy rises quickly plateaus lags behind original fractalnet ends better ﬁnal performance implies average pooling might signiﬁcantly reduce length training behavior provides evidence cooperative average pooling might preferable competitive pooling. table figure compare test accuracy results architectural innovations described section architecture ends similar ﬁnal test accuracy fractalnet architectures behind learning rate dropped. however clear figures architectures train quickly fractalnet. network best trains quickly fractalnet achieves similar accuracy. freeze-drop-path questionable since ﬁnal performance lags behind fractalnet leaving exploration suitable applications architectures future work. paper describe convolutional neural network architectural design patterns discovered studying plethora architectures recent deep learning papers. hope design patterns useful experienced practitioners looking push state-of-the-art novice practitioners looking apply deep learning applications. exists large expanse potential follow work effort primarily focused residual networks classiﬁcation hope preliminary work inspire others follow architectural design patterns recurrent neural networks deep reinforcement learning architectures beyond. authors want thank numerous researchers upon whose work design patterns based especially larsson making code publicly available. work supported naval research laboratory base program. guosheng xiaojiang peng yongxin yang timothy hospedales jakob verbeek. frankenstein learning deep face representations using small data. arxiv preprint arxiv. yani ioannou duncan robertson darko zikic peter kontschieder jamie shotton matthew brown antonio criminisi. decision forests convolutional networks models in-between. arxiv preprint arxiv. yangqing evan shelhamer jeff donahue sergey karayev jonathan long ross girshick sergio guadarrama trevor darrell. caffe convolutional architecture fast feature embedding. proceedings international conference multimedia matthew johnson-roberson charles barto rounak mehta sharath nittur sridhar vasudevan. driving matrix virtual worlds replace human-generated annotations real world tasks? arxiv preprint arxiv. jonathan krause benjamin sapp andrew howard howard zhou alexander toshev duerig james philbin fei-fei. unreasonable effectiveness noisy data ﬁne-grained recognition. arxiv preprint arxiv. alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems david krueger tegan maharaj j´anos kram´ar mohammad pezeshki nicolas ballas rosemary anirudh goyal yoshua bengio hugo larochelle aaron courville zoneout regularizing rnns randomly preserving hidden activations. arxiv preprint arxiv. qianli liao kenji kawaguchi tomaso poggio. streaming normalization towards simpler biologically-plausible normalizations online recurrent learning. arxiv preprint arxiv. tsung-yu aruni roychowdhury subhransu maji. bilinear models ﬁne-grained visual recognition. proceedings ieee international conference computer vision olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein imagenet large scale visual recognition challenge. international journal computer vision leslie smith emily hand timothy doster. gradual dropin layers train deep neural networks. proceedings ieee conference computer vision pattern recognition nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. proceedings ieee conference computer vision pattern recognition christian szegedy vincent vanhoucke sergey ioffe jonathon shlens zbigniew wojna. rethinking inception architecture computer vision. arxiv preprint arxiv. architectures mentioned section commonly combine outputs layers using concatenation along depth axis element-wise summation element-wise average. show latter special cases former weight-sharing enforced. likewise show skip connections considered introducing additional layers network share parameters existing layers. residual network variants reformulated standard form many variants equivalent. ﬁlter three dimensions spatial dimensions along convolution occurs third dimension depth. input channel corresponds different depth ﬁlter layer. result ﬁlter considered consist slices convolved input channel. results convolutions added together along bias produce single output channel. output channels multiple ﬁlters concatenated produce output single layer. outputs several layers concatenated behavior similar single layer. however instead ﬁlter spatial dimensions stride padding ﬁlter different structure. function within network though cases same. fact standard layer ﬁlters shape considered special case concatenating outputs multiple layer types. summation used instead concatenation network considered perform concatenation enforce weight-sharing following layer. results ﬁrst summing several channels element-wise convolving ﬁlter slice output yields result convolving slice channels performing element-wise summation afterwards. therefore enforcing weight-sharing ﬁlter slices applied channel inputs share weight results behavior identical summation form similar concatenation highlights relationship two. batch normalization rendered irrelevant. details architecture-speciﬁc manipulations summations averages described section ability express depth-wise concatenation element-wise element-wise mean variants other architectural features recent works combined within single network regardless choice combining operation. however concatenation expressivity therefore strictly better others. summation allows networks divide network’s task. also trade-off number parameters expressivity layer; summation uses weight-sharing signiﬁcantly reduce number parameters within layer expense amount expressivity. different architectures expressed similar fashion changes connections themselves. densely connected series layers pruned resemble desired architecture skip connections zeroing speciﬁc ﬁlter slices. operation removes dependency output speciﬁc input channel; done channels given layer connection layers severed. likewise densely connected layers turned linearly connected layers preserving layer dependencies; skip connection passed intermediate layers. ﬁlter introduced input channel passing through ﬁlter performs identity operation given input channel. existing ﬁlters intermediate layers zeroed slices input introduce dependencies. arbitrarily connected layers turned standard form. certainly recommend representation actual experimentation introduces ﬁxed parameters. merely describe illustrate relationship different architectures. representation illustrates skip connections effectively enforce speciﬁc weights intermediate layers. though restriction reduces expressivity number stored weights reduced number computations performed decreased network might easily trainable. implementations caffe using cuda experiments node cluster nvidia titan black gpus memory dual intel xenon cpus node. used cifar cifar- datasets classes respectively. caffe code prototxt ﬁles publicly available https//github.com/iphysicist/cnndesignpatterns. started fractalnet implementation baseline described larsson used three column module shown figure experiments replaced fractal-join concatenation downsampling locations. experiments modiﬁed kernel sizes module combined branches maxout. fractalnet module shown figure architecture consists sequential modules. fractal fractalnet architecture uses module overall fractal design figure rather original sequential one. limited investigation realization left study designs future work. followed fractalnet implementation regards dropout dropout rate module depending depth module architecture. choice dropout rates found experimentation better values possible. local drop-path rate fractal-joins ﬁxed identical fractalnet implementation. freeze-drop-path introduces four parameters. ﬁrst whether active branch chosen stochastically deterministically. chosen stochastically random number generated active branch assigned based interval falls deterministically parameter user number iterations cycle branches caffe implementation freeze-drop-path unit bottom input speciﬁed ﬁrst assigned branch next branch branch etc. next parameter indicates proportion iterations branch active relative branches. ﬁrst type interval uses square branch number assign interval length branch active gives update iterations higher numbered branches. next type gives amount iterations branch. experiments showed ﬁrst interval type works better used obtained results section addition experiments showed stochastic option works better deterministic option used section results. stagewise boosting network’s architecture architecture except branches combined fractal-join combined branch freezedrop-path join. reason combining branches came ﬁrst experiments; branches separate performance deteriorated branch frozen branch active. hindsight weights branch path also branch path modiﬁed training branch taylor series network architecture addition squaring branch combined activations freeze-drop-path join. experiments trained epochs. since training used gpus batchsize epochs amounted iterations. adopted learning rate fractalnet implementation started dropped learning rate factor epochs", "year": 2016}