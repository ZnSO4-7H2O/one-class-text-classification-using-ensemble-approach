{"title": "Modulating early visual processing by language", "tag": ["cs.CV", "cs.CL", "cs.LG"], "abstract": "It is commonly assumed that language refers to high-level visual concepts while leaving low-level visual processing unaffected. This view dominates the current literature in computational models for language-vision tasks, where visual and linguistic input are mostly processed independently before being fused into a single representation. In this paper, we deviate from this classic pipeline and propose to modulate the \\emph{entire visual processing} by linguistic input. Specifically, we condition the batch normalization parameters of a pretrained residual network (ResNet) on a language embedding. This approach, which we call MOdulated RESnet (\\MRN), significantly improves strong baselines on two visual question answering tasks. Our ablation study shows that modulating from the early stages of the visual processing is beneficial.", "text": "commonly assumed language refers high-level visual concepts leaving low-level visual processing unaffected. view dominates current literature computational models language-vision tasks visual linguistic inputs mostly processed independently fused single representation. paper deviate classic pipeline propose modulate entire visual processing linguistic input. speciﬁcally introduce conditional batch normalization efﬁcient mechanism modulate convolutional feature maps linguistic embedding. apply pre-trained residual network leading modulated resnet architecture show signiﬁcantly improves strong baselines visual question answering tasks. ablation study conﬁrms modulating early stages visual processing beneﬁcial. human beings combine processing language vision apparent ease. example natural language describe perceived objects able imagine visual scene given textual description. developing intelligent machines impressive capabilities remains long-standing research challenge many practical applications. towards grand goal witnessed increased interest tasks intersection computer vision natural language processing. particular image captioning visual question answering visually grounded dialogue systems constitute popular example tasks large-scale datasets available. developing computational models language-vision tasks challenging especially open question underlying tasks fuse/integrate visual textual representations? extent process visual linguistic input separately stage fuse them? equally important fusion mechanism use? paper restrict attention domain visual question answering natural testbed fusing language vision. task concerns answering open-ended questions images received signiﬁcant attention research community current state-of-the-art systems often following computational pipeline illustrated ﬁrst extract high-level image features imagenet pretrained convolutional network obtain language embedding using figure overview classic pipeline language vision modalities independently processed classic pipeline propose directly modulate resnet processing language. recurrent neural network word-embeddings. high-level representations fused concatenation element-wise product tucker decomposition compact bilinear pooling processed downstream task hand. attention mechanisms often used questions attend speciﬁc spatial locations extracted higher-level feature maps. main reasons recent literature focused processing modality independently. first using pretrained convnet feature extractor prevents overﬁtting; despite large training hundred thousand samples backpropagating error downstream task weights layers often leads overﬁtting. second approach aligns dominant view language interacts high-level visual concepts. words view thought pointers high-level conceptual representations. best knowledge work ﬁrst fuse modalities early stages image processing. parallel neuroscience community exploring extent processing language vision coupled evidence accumulates words visual priors alter visual information processed beginning precisely observed signals related low-level visual features modulated hearing speciﬁc words language people hear ahead image activates visual predictions speed image recognition process. ﬁndings suggest independently processing visual linguistic features might suboptimal fusing early stage help image processing. paper introduce novel approach language modulate entire visual processing pre-trained convnet. propose condition batch normalization parameters linguistic input approach called conditional batch normalization inspired recent work style transfer beneﬁt scales linearly number feature maps convnet impacts less parameters greatly reducing risk over-ﬁtting. apply pretrained residual network leading novel architecture refer modern. show signiﬁcant improvements datasets vqav guesswhat? stress approach general fusing mechanism applied multi-modal tasks. summarize contributions three fold propose conditional batch normalization modulate entire visual processing language brieﬂy outline residual networks current top-performing convolutional networks ilsvrc classiﬁcation competition. contrast precursor convnets constructs representation layer resnet iteratively reﬁnes representation adding residuals. modiﬁcation enables train deep convolutional networks without suffering much vanishing gradient problem. speciﬁcally resnets built residual blocks denotes outputted feature map. refer ficwh denote input sample feature location residual function composed three convolutional layers fig. original resnet paper detailed overview residual block. group blocks stacked form stage computation representation dimensionality stays identical. general resnet architecture starts single convolutional layer followed four stages computation. transition stage another achieved projection layer halves spatial dimensions doubles number feature maps. several pretrained resnets available including resnet- resnet- resnet- differ number residual blocks stage. convolutional layers resnets make batch normalization technique originally designed accelarate training neural networks reducing internal co-variate shift given mini-batch {fi···}n examples normalizes feature maps training time follows constant damping factor numerical stability trainable scalars introduced keep representational power original network. note convolutional layers mean variance computed batch spatial dimensions module output non-linear activation function. inference time batch mean variance varb replaced population mean variance often estimated exponential moving average batch mean variance training. brieﬂy recap common obtain language embedding natural language question. formally question sequence length token taken predeﬁned vocabulary transform token dense word-embedding learned look-up table. task limited linguistic corpora common concatenate pretrained glove vectors word embeddings. sequence embeddings recurrent neural network produces sequence state vectors popular transition functions like long-short term memory cell gated recurrent unit incorporate gating mechanisms better handle long-term dependencies. work lstm cell transition function. finally take last hidden state embedding question denote throughout rest paper. section introduce conditional batch normalization show modulate pretrained resnet. idea predict batch normalization language embedding. ﬁrst focus single convolutional layer batch normalization module pretrained scalars available. would like directly predict afﬁne scaling parameters language embedding starting training procedure parameters must close pretrained values recover original resnet model poor initialization could signiﬁcantly deteriorate performance. unfortunately difﬁcult initialize network output pretrained reasons propose predict change frozen original scalars straightforward initialize neural network produce output zero-mean small variance. one-hidden-layer predict deltas question embedding feature maps within layer finally batch normalization stress freeze resnet parameters including during training. fig. visualize difference computational original batch normalization proposed modiﬁcation. explained section resnet consists four stages computation subdivided several residual blocks. block apply three convolutional layers highlighted fig. computationally efﬁcient powerful method modulate neural activations; enables linguistic embedding manipulate entire feature maps scaling down negating them shutting etc. parameters feature total number parameters comprise less total number parameters pre-trained resnet. makes scalable method compared conditionally predicting weight matrices evaluate proposed conditional batch normalization tasks. next section outline tasks describe neural architectures experiments. source code experiments available https//github.com/guesswhatgame. hyperparameters also provided appendix visual question answering task consists open-ended questions real images. answering questions requires understanding vision language commonsense knowledge. paper focus vqav dataset contains questions images. baseline architecture ﬁrst obtains question embedding lstm-network detailed section image extract feature maps last layer resnet- input size feature maps size incorporate spatial attention mechanism conditioned question embedding pool spatial dimensions. formally given feature maps fi··· question embedding obtain visual embedding follows denotes concatenating vectors. hidden layer relu activations whose parameters shared along spatial dimensions. visual question embedding fused element-wise product follows denotes element-wise product trainable weight matrices trainable bias. linguistic perceptual representations ﬁrst projected space equal dimensionality tanh non-linearity applied. fused vector computed element-wise product representations. joined embedding ﬁnally predict answer distribution linear layer followed softmax activation function. described architecture study impact using several stages resnet. approach combined existing architecture also apply modern state-of-the-art network speciﬁcally network replaces classic attention mechanism advanced included glimpses image features noticeably modern modulates entire visual processing pipeline therefore backpropagates convolutional layers. requires much memory using extracted features. feasibly experiments today’s hardware conduct experiments paper resnet-. training procedure select most-common answers training cross-entropy loss distribution provided answers. train training early-stopping validation report accuracies test-dev using evaluation script provided guesswhat? cooperative two-player game players image rich visual scene several objects. player oracle randomly assigned object scene. object known player questioner whose goal locate hidden object asking series yes-no questions answered oracle full dataset composed binary question/answer pairs images. interestingly guesswhat? game rules naturally leads rich variety visually grounded questions. opposed vqav dataset dataset contains commonsense questions answered without image. paper focus oracle task form visual question answering answers limited applicable. speciﬁcally oracle take input incoming question image target object object described category spatial location object crop. outline neural network architecture reported original guesswhat? paper first crop initial image using target object bounding object rescale square. extract activation last convolutional layer relu pre-trained resnet-. also embed spatial information crop within image extracting -dimensional vector location bounding wbox hbox denote width height bounding respectively. convert object category dense category embedding using learned look-up table. finally lstm encode current question concatenate embeddings single vector feed input single hidden layer outputs ﬁnal answer distribution using softmax layer. report results state-of-the-art architectures namely multimodal compact bilinear pooling network mutan approaches employ bilinear pooling mechanism fuse language vision embedding respectively using random projection tensor decomposition. addition re-implement model described section benchmarking state-of-the-art models train training proceed early stopping validation report accuracy test report best validation accuracy outlined methods task table. note input images size compare modern baselines improve performance interestingly ﬁnetuning batch norm parameters signiﬁcantly improves accuracy another signiﬁcant performance jump condition batch normalization question input improves baseline almost accuracy points state-of-the-art models images size also include results baseline architecture larger images. seen table nearly matches state results modern rely speciﬁc attention mechanism combine proposed method architecture observe outperforms state-of-the-art model half point. please note select latter requires fewer weight parameters stable train. note presented results resnet- models rely extracted image embedding resnet-. sake comparison baseline models extracted image embedding resnet-. also advanced architecture observe performance gains approximately accuracy points. guesswhat? report best test errors outlined method oracle task guesswhat? table ﬁrst compare results feed crop selected object model. observe trend vqa. error performs better either ﬁne-tuning ﬁnal block batch-norm parameters turn improve using features note relative improvement much bigger guesswhat? vqa. therefore also investigate performance methods include spatial category information. observe ﬁnetuning last layers parameters improve performance modern improves best reported test error points error. figure t-sne projection feature maps resnet modern. points colored according answer type vqa. whilst clusters features modern successfully modulates image feature towards speciﬁc answer types. modern fine tuning experiments modern outperforms methods update resnet parameters demonstrates important condition language representation. modern also outperforms stage tasks shows performance gain modern increased model capacity. conditional embedding provided baselines oracle task guesswhat? authors observed best test error obtained providing object category spatial location. model including features object crop actually deteriorates performance error. means baseline fails extract relevant information images handcrafted features. therefore oracle answer correctly questions requires spatial information object category. baseline model embedding crop generic resnet help even ﬁnetune stage contrast applying modern helps better answer questions test error drops points. ablation study investigate impact modulating layers resnet. report results table interestingly observe performance slowly decreases apply exclusively later stages. stress best performance it’s important modulate stages computational resources limited recommend apply last stages. visualizing representations order gain insight proposed fusion mechanism compare visualizations visual embeddings created baseline model modern. ﬁrst randomly picked unique image/question pairs validation vqa. trained modern model extract image features attention mechanism modern compare extracted resnet- features ﬁnetune resnet- ﬁrst decrease dimensionality average pooling spatial dimensions feature subsequently apply t-sne embeddings. color points according answer type provided dataset show visualizations models appendix interestingly observe answer types spread image features ﬁnetuned features. contrast representations modern cleanly grouped three answer types. demonstrates modern successfully disentangles images representations answer type likely ease later fusion process. ﬁnetuning models cluster features direct link clusters answer type. results indicate modern successfully learns representation differs classic ﬁnetuning strategies. appendix visualize feature disentangling process stage stage. possible spot sub-clusters t-sne representation fact correspond image question pairs similar modern related recent work vqa. majority proposed methods similar computational pipeline introduced first extract high-level image features imagenet pretrained convnet independently processing question using rnn. work focused level fusing mechanism language visual vectors. instance shown improve upon classic concatenation element-wise product tucker decomposition bilinear pooling exotic approaches another line research investigated role attention mechanisms authors propose co-attention model visual language embeddings proposes stack several spatial attention mechanisms. although attention mechanism thought modulating visual features language stress mechanism high-level features. contrast work modulates visual processing start. modern inspired conditional instance normalization successfully applied image style transfer. previous methods transfered image style network showed styles could compressed single network sharing convolutional ﬁlters learning style-speciﬁc normalization parameters. notable differences work. first uses non-differentiable table lookup normalization parameters propose differentiable mapping question embedding. second predict change normalization parameters pretrained convolutional network keeping convolutional ﬁlters ﬁxed. parameters including transposed convolutional ﬁlters trained. best knowledge ﬁrst paper conditionally modulate vision processing using normalization parameters. paper introduce conditional batch normalization novel fusion mechanism modulate layers visual processing network. speciﬁcally applied pre-trained resnet leading proposed modern architecture. approach motivated recent evidence neuroscience suggesting language inﬂuences early stages visual processing. strengths modern incorporated existing architectures experiments demonstrate signiﬁcantly improves baseline models. also found important modulate entire visual signal obtain maximum performance gains. paper focuses text images modern extended neural architecture dealing modalities sound video. broadly could also applied modulate internal representation deep network respect embedding regardless underlying task. instance signal modulation batch norm parameters also beneﬁcial reinforcement learning natural language processing adversarial training tasks. authors would like acknowledge stimulating research environment sequel lab. thank vincent dumoulin helpful discussions conditional batch normalization. acknowledge following agencies research funding computing support chistera iglu cper nord-pas calais/feder data advanced data science technologies nserc calcul québec compute canada canada research chairs cifar. thank nvidia providing access dgx- machine used work.", "year": 2017}