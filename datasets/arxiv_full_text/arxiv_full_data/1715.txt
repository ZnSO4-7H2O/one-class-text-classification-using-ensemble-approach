{"title": "A Method for Stopping Active Learning Based on Stabilizing Predictions  and the Need for User-Adjustable Stopping", "tag": ["cs.LG", "cs.CL", "stat.ML", "I.2.6; I.2.7; I.5.1; I.5.4; G.3"], "abstract": "A survey of existing methods for stopping active learning (AL) reveals the needs for methods that are: more widely applicable; more aggressive in saving annotations; and more stable across changing datasets. A new method for stopping AL based on stabilizing predictions is presented that addresses these needs. Furthermore, stopping methods are required to handle a broad range of different annotation/performance tradeoff valuations. Despite this, the existing body of work is dominated by conservative methods with little (if any) attention paid to providing users with control over the behavior of stopping methods. The proposed method is shown to fill a gap in the level of aggressiveness available for stopping AL and supports providing users with control over stopping behavior.", "text": "performance terms f-measure. seen ﬁgure issue stop early useful generalizations still made wind lower performing system stop late useful generalizations made wind wasting human annotation effort. terms aggressive conservative used throughout rest paper describe behavior stopping methods. conservative methods tend stop right figure conservative sense they’re careful risk losing signiﬁcant amounts fmeasure even means annotating many examples necessary. aggressive methods hand tend stop left figure aggressively trying reduce unnecessary annotations. survey existing methods stopping active learning reveals needs methods widely applicable; aggressive saving annotations; stable across changing datasets. method stopping based stabilizing predictions presented addresses needs. furthermore stopping methods required handle broad range different annotation/performance tradeoff valuations. despite this existing body work dominated conservative methods little attention paid providing users control behavior stopping methods. proposed method shown level aggressiveness available stopping supports providing users control stopping behavior. active learning reduce annotation costs generated considerable interest recently realize savings annotation efforts enables must mechanism knowing stop annotation process. figure intended motivate value stopping right time. x-axis measures number human annotations requested ranges y-axis measures research conducted ﬁrst author applicability several leading methods restricted used certain situations e.g. can’t used base learners select points certain batch sizes etc. instability leading methods work well datasets completely break datasets either stopping late wasting enormous amounts annotation effort stopping early losing large amounts f-measure. paper presents stopping method based stabilizing predictions addresses areas provides user-adjustable stopping behavior. essential idea behind method test predictions recently learned models examples don’t labeled stop predictions stabilized. main advantages method that requires additional labeled data it’s widely applicable ﬁlls need method aggressively save annotations stable performance provides users control aggressively/conservatively stop section discusses related work. section explains stabilizing predictions stopping criterion detail. section evaluates method discusses results. section concludes. laws sch¨utze present stopping criteria based gradient performance estimates gradient conﬁdence estimates. technique gradient performance estimates applicable probabilistic base learners used. gradient conﬁdence estimates method generally applicable method denoted tables figures measures rate change model conﬁdence window recent points gradient falls threshold stopped. margin exhaustion stopping criterion developed svms says stop remaining unlabeled examples outside current model’s margin denoted tables figures. ertekin developed similar technique stops number support vectors saturates. equivalent margin exhaustion experiments method shown explicitly tables figures. since svms compare margin exhaustion evaluation section. unlike method margin exhaustion applicable margin-based methods svms can’t used base learners maximum entropy naive bayes others. schohn cohn show experiments margin exhaustion tendency stop late. further conﬁrmed experiments section conﬁdence-based stopping criterion says stop model conﬁdence consistently drops. pointed stopping criterion based assumption learner/feature representation incapable fully explaining examples. however assumption often violated performance method suffers stopping criteria reported max-conf method indicates stop conﬁdence model unlabeled example exceeds threshold. context margin-based methods maxconf boils simply generalization margin exhaustion method. min-err reported superior max-conf says stop accuracy recent model current batch queried examples exceeds threshold proposes multicriteria-based stopping handle setting threshold min-err. refuse stop raise min-err threshold classiﬁcation changes remaining unlabeled data consecutive actively learned models current min-err threshold satisﬁed. denote multi-criteria-based strategy reported work better min-err isolation zwh. seen sometimes min-err indeed stops later desired must stop least late min-err does. susceptibility stopping late shown emprically section also applicable setups select examples small batches. stop active learning point annotations stop providing increases performance perhaps straightforward separate labeled data stop performance begins level set. problem requires additional labeled data counter original reason using ﬁrst place. hypothesis sense stop looking predictions consecutively learned models examples don’t labeled. won’t know predictions correct stabilized. predictions stabilized hypothesize performance models stabilized vice-versa ensure aggressive approach saving annotations. checks stabilization predictions examples called stop don’t labeled. since stabilizing predictions stop going used indication model stabilization occurred stop ought representative types examples encountered application time. conﬂicting factors deciding upon size stop use. hand small desirable checked quickly. hand large desired ensure don’t make decision based isn’t representative application space. compromise factors chose size it’s important allow examples stop queried active learner selects highly informative ruling could hurt performance. preliminary experiments made stop distinct unlabeled points made available querying performance qualitatively curve translated fmeasure points. therefore allow points stop selected essential idea compare successive models’ predictions stop stabilized. simple deﬁne agreement between models would measure percentage points models make predictions. however experimental results separate development dataset show cutoff agreement stop sensitive dataset used. different datasets different levels agreement expected chance simple percent agreement doesn’t adjust this. measurement agreement human annotators received signiﬁcant attention context drawbacks using percent agreement recognized alternative metrics proposed take chance agreement account. survey several agreement metrics presented. agreement metrics form kappa statistic measures agreement expected chance modeling coder separate distribution governing likelihood assigning particular category. formally kappa deﬁned equafound kappa robust parameter doesn’t require tuning moving dataset. separate development dataset kappa cutoff worked well. experiments current paper used agreement cutoff kappa zero tuning performed. section cutoff delivers robust results across folds datasets. kappa cutoff captures intensity agreement must occur conclude stop. though intensity cutoff excellent default advantages method giving users option vary intensity cutoff users control aggressive behave. explored section another give users control stopping behavior give control longevity agreement must maintained concludes stop. simplest implementation would check recent model previous model stop agreement exceeds intensity cutoff. however independent wanting provide users longevity control ideal approach because there’s risk models could happen highly agree next model highly agree them. therefore propose using average agreements window recent pairs models. call recent model previous model window size average agreements mn−. separate development data window size worked well. experiments current paper used longevity window size zero tuning performed. section longevity default delivers robust results across folds datasets. furthermore section shows varying longevity requirement provides users anlever controlling aggressively behave. experimental setup evaluate stabilizing predictions stopping method multiple datasets text classiﬁcation named entity recognition tasks. datasets freely publicly available used many past works. text classiﬁcation publicly available spam corpora spamassassin corpus used trec spam corpus trecp/ham described corpora task binary classiﬁcation task perform -fold cross validation. also reuters dataset particular reuters- distribution modapte split. since document belong category category treated separate binary classiﬁcation problem consistent results reported largest categories. datasets newsgroups newsgroup article classiﬁcation webkb page classiﬁcation datasets. webkb four largest categories. datasets binary features every word occurs training data least three times. used bydate version dataset downloaded http//people.csail.mit.edu/jrennie/newsgroups/. version recommended since makes cross-experiment comparison easier since randomness selection train/test splits. svms deliver high performance datasets employ svms base learner bulk experiments selection points query approach used selecting points closest current hyperplane. svmlight training svms. smaller datasets batch size used initial training size larger datasets batch size used initial training size main results table shows results datasets. dataset report average number annotations requested stopping methods well average f-measure achieved stopping methods. facts worth keeping mind. first numbers table averages therefore sometimes methods could similar average numbers annotations wildly different average f-measures second sometimes method higher average number annotations lower better evaluation metrics would reﬁned measures annotation effort number annotations annotations require amount effort annotate lacking reﬁned model datasets number annotations experiments. average f-measure method lower average number annotations. caused because ﬁrst fact mentioned numbers averages and/or also caused less more phenomenon active learning often less data higher-performing model learned data; ﬁrst reported subsequently observed many others methods except unstable sense least dataset major failure either stopping late wasting large numbers annotations stopping early losing large amounts f-measure it’s always clear evaluate stopping methods tradeoff value extra f-measure versus saving annotations clearly known different different applications users. last point deserves discussion. cases clear stopping method best. example wkb-project method saves annotations highest f-measure. method performs best ner-dna? arguments reasonably made best case depending exactly annotation/performance tradeoff promising direction research stopping methods develop user-adjustable stopping methods stop aggressively user’s annotation/performance preferences dictate. task-dataset trec-spam newsgroups spamassassin ner-protein ner-dna ner-celltype reuters wkb-course wkb-faculty wkb-project wkb-student average table methods stopping dataset average number annotations automatically determined stopping points average f-measure automatically determined stopping points displayed. bold entries statistically signiﬁcantly different average simply unweighted macro-average datasets. ﬁnal column represents standard fully supervised passive learning entire training data. much others known perform consistently conservative manner users pick stopping criterion that’s suitable particular annotation/performance valuation. purpose ﬁlls stopping criteria seem conservative sense deﬁned section hand aggressive stopping criterion less likely annotate data needed. second avenue providing user-adjustable stopping single stopping method adjustable. section shows intensity longevity provide levers used control behavior controlled fashion. ious criteria graph active learning curve help visualize methods perform. figure shows graph representative fold. x-axis measures number human annotations requested far. y-axis measures performance terms f-measure. vertical lines various stopping methods would stopped hadn’t continued simulation. ﬁgure reinforces illustrates seen table namely stops aggressively existing criteria able doesn’t make sense show graph average cross validation average number annotations stopping point cross learning curve completely misleading point. consider method stops early late times. additional experiments subsection conducted least computationally demanding dataset spamassassin. results tables show varying intensity cutlongevity requirement respectively enable user control stopping behavior. methods enable user adjust stopping controlled fashion areas future work include combining intensity longevity methods controlling behavior; developing precise expectations change behavior corresponding changes intensity longevity settings. results table show results different stop sizes. even random selection stop small sp’s performance holds fairly steady. plus fact random selection stop sets size worked across folds datasets table show practice perhaps simple heuristic choosing fairly large random points works well. nonetheless think size necessary depend dataset factors feature representation principled methods determining size and/or makeup stop area future work. example construction techniques table controlling behavior stopping intensity. kappa intensity levels -fold average number annotations automatically determined stopping points -fold average f-measure automatically determined stopping points displayed spamassassin dataset. table controlling behavior stopping longevity. window length longevity levels -fold average number annotations automatically determined stopping points -fold average f-measure automatically determined stopping points displayed spamassassin dataset. could developed create stop sets high representativeness density example possibility cluster examples begins make sure stop contains examples clusters. another possibility greedy algorithm stop iteratively grown iteration center mass stop feature space computed example unlabeled pool maximally feature space center mass selected inclusion stop set. could useful efﬁciency also ensure adequate representation task space. latter accomtable methods stopping maximum entropy base learner. stopping method average number annotations automatically determined stopping point average f-measure automatically determined stopping point displayed. bold entries statistically signiﬁcantly different margin exhaustion method shown since can’t used non-margin-based learner. ﬁnal column represents standard fully supervised passive learning entire training data. table investigating sensitivity stop size. stop sizes fold average number annotations automatically determined stopping points -fold average fmeasure automatically determined stopping points displayed spamassassin dataset. advantages it’s widely applicable table shows results using maximum entropy models base learner during results reinforce conclusions experiments performing aggressively statistically signiﬁcant differences performance sp’s favor. figure shows graph representative fold. effective methods stopping crucial realizing potential annotation savings enabled survey existing stopping methods identiﬁed three areas improvements called for. stopping method based stabilizing predictions addresses three areas widely applicable stable aggressive saving annotations. empirical evaluation existing methods informative evaluating criteria also informative demonstrating difﬁculties rigorous objective evaluation stopping criteria different annotation/performance tradeoff valuations. opens future area work user-adjustable stopping. potential avenues enabling user-adjustable stopping single criterion adjustable suite methods consistent differing levels aggressiveness/conservativeness users pick suit annotation/performance tradeoff valuation. substantially widens range behaviors existing methods users choose from. also sp’s behavior adjusted user-controllable parameters. andrew mccallum kamal nigam. comparison event models naive bayes text classiﬁcation. proceedings aaai- workshop learning text categorization. greg schohn david cohn. less more active learning support vector machines. proc. international conf. machine learning pages morgan kaufmann francisco jingbo eduard hovy. active learning word sense disambiguation methods addressing class imbalance problem. proceedings joint conference empirical methods natural language processing computational natural language learning pages jingbo huizhen wang eduard hovy. multi-criteria-based strategy stop active learning data annotation. proceedings international conference computational linguistics pages manchester august. coling organizing committee.", "year": 2014}