{"title": "WRPN: Wide Reduced-Precision Networks", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "For computer vision applications, prior works have shown the efficacy of reducing numeric precision of model parameters (network weights) in deep neural networks. Activation maps, however, occupy a large memory footprint during both the training and inference step when using mini-batches of inputs. One way to reduce this large memory footprint is to reduce the precision of activations. However, past works have shown that reducing the precision of activations hurts model accuracy. We study schemes to train networks from scratch using reduced-precision activations without hurting accuracy. We reduce the precision of activation maps (along with model parameters) and increase the number of filter maps in a layer, and find that this scheme matches or surpasses the accuracy of the baseline full-precision network. As a result, one can significantly improve the execution efficiency (e.g. reduce dynamic memory footprint, memory bandwidth and computational energy) and speed up the training and inference process with appropriate hardware support. We call our scheme WRPN - wide reduced-precision networks. We report results and show that WRPN scheme is better than previously reported accuracies on ILSVRC-12 dataset while being computationally less expensive compared to previously reported reduced-precision networks.", "text": "computer vision applications prior works shown efﬁcacy reducing numeric precision model parameters deep neural networks. activation maps however occupy large memory footprint training inference step using mini-batches inputs. reduce large memory footprint reduce precision activations. however past works shown reducing precision activations hurts model accuracy. study schemes train networks scratch using reduced-precision activations without hurting accuracy. reduce precision activation maps increase number ﬁlter maps layer scheme matches surpasses accuracy baseline full-precision network. result signiﬁcantly improve execution efﬁciency speed training inference process appropriate hardware support. call scheme wrpn wide reduced-precision networks. report results show wrpn scheme better previously reported accuracies ilsvrc- dataset computationally less expensive compared previously reported reduced-precision networks. promising approach lower compute memory requirements convolutional deeplearning workloads numeric precision algorithms. operating lower precision mode reduces computation well data movement storage requirements. efﬁciency beneﬁts many existing works propose low-precision deep neural networks even -bit ternary mode -bit binary mode however majority existing works low-precision dnns sacriﬁce accuracy baseline full-precision networks. further prior works target reducing precision model parameters primarily beneﬁts inference step batch sizes small. improve execution efﬁciency accuracy low-precision networks reduce precision activation maps model parameters increase number ﬁlter maps layer. call networks using scheme wide reduced-precision networks scheme compensates surpasses accuracy baseline full-precision network. although number compute operations increases increase number ﬁlter maps layer compute bits required operation fraction required using full-precision operations wrpn offers better accuracies computationally less expensive compared previously reported reduced-precision networks. report results alexnet batch-normalized inception resnet- ilsvrc- dataset. -bits sufﬁcient training deep wide models achieving similar better accuracy baseline network. -bit activation -bit weights accuracy at-par baseline full-precision. making networks wider operating -bit precision close accuracy previously reduced-precision quantization scheme hardware friendly allowing efﬁcient hardware implementations. evaluate efﬁciency beneﬁts low-precision operations titan arria- fpga asic. fpga asic deliver signiﬁcant efﬁciency gain operations cannot take advantage low-precision operations. prior works proposing reduced-precision networks work precision weights activation maps occupy larger memory footprint using mini-batches inputs. using mini-batches inputs typical training dnns cloudbased batched inference figure shows memory footprint activation maps ﬁlter maps batch size changes different networks resnet- resnet-) training inference steps. figure memory requirements feed forward convolutional deep neural network. orange boxes denote weights blue boxes activations green boxes gradient-maps batch-size increases ﬁlter reuse across batches inputs activation maps occupy signiﬁcantly larger fraction memory compared ﬁlter weights. aspect illustrated figure shows memory requirements canonical feed-forward hardware accelerator based system training activation maps weight tensors allocated device memory forward pass along memory gradient maps backward propagation. total memory requirements training phase memory required activation maps weights maximum input gradient maps maximum back-propagated gradients inference memory allocated input output feature maps required single layer memory allocations reused layers. total memory allocation inference maximum maximum required across layers plus w-tensors. batch sizes more activations start occupy total memory footprint training. overall reducing precision activations weights reduces memory footprint bandwidth storage also simplifying requirements hardware efﬁciently support operations. based observation activations occupy memory footprint compared weights reduce precision activations speed training inference steps well memory requirements. however straightforward reduction precision activation maps leads signiﬁcant reduction model accuracy conduct sensitivity study reduce precision activation maps model weights alexnet running ilsvrc- dataset train network scratch. table reports ﬁndings. top- single-precision accuracy accuracy binary weights activations similar reported data-point table using technique data points collected using quantization scheme runs hyper-parameters training carried number epochs baseline network. consistent results reported prior works quantize weights activations ﬁrst last layer. that general reducing precision activation maps weights hurts model accuracy. further reducing precision activations hurts model accuracy much reducing precision ﬁlter parameters. quite effective alexnet lower precision weights lose accuracy. however scheme effective networks like resnet inception. re-gain model accuracy working reduced-precision operands increase number ﬁlter maps layer. although number compute operations increase widening ﬁlter maps layer bits required compute operation fraction required using full-precision operations. result appropriate hardware support signiﬁcantly reduce dynamic memory requirements memory bandwidth computational energy speed training inference process. widening ﬁlter maps inspired wide resnet work depth network reduced width layer increased wide resnet requires re-design network architecture. work maintain depth parameter baseline network widen ﬁlter maps. call approach wrpn wide reduced-precision networks. practice scheme simple effective starting baseline network architecture change width ﬁlter without changing network design parameter hyper-parameters. carefully reducing precision simultaneously widening ﬁlters keeps total compute cost network at-par baseline cost. table reports accuracy alexnet double number ﬁlter maps layer. doubling ﬁlter maps alexnet -bits weights -bits activations exhibits accuracy at-par full-precision networks. operating -bits weights -bits activations surpasses baseline accuracy binary weights activations better accuracy xnornet doubling number ﬁlter maps alexnet’s compute operations grow compared baseline full-precision network however using reduced-precision operands overcompute complexity fraction baseline. example operands weights activations number ﬁlters reduced-precision alexnet total compute cost full-precision baseline also experiment widening factors. widening ﬁlters -bits activation precision -bits weight precision still at-par baseline accuracy. wide ﬁlters least -bits weight -bits activation precision required accuracy match baseline full-precision wide accuracy. further table shows widening ﬁlters needs lower precision least -bits total compute cost baseline compute cost. thus trade-off widening reducing precision network parameters. work trade-off higher number compute operations aggressively reducing precision operands involved operations sacriﬁcing model accuracy. apart beneﬁts reduced precision activations mentioned earlier widening ﬁlter maps also improves efﬁciency underlying gemm calls convolution operations since compute accelerators typically efﬁcient single kernel consisting parallel computation large data-structures opposed many small sized kernels study scheme applies deeper networks. this study resnet- batchnormalized inception similar trends particularly -bits weight -bits activations continue provide at-par accuracy baseline. tensorflow tensorpack evaluations ilsvrc- train dataset analysis. resnet- ﬁlters modular layers shortcut connections ﬁlter bank width changes depth increases. pre-activation variant resnet baseline top- accuracy resnet- implementation using single-precision -bits data format binarizing weights activations layers except ﬁrst last layer network gives top- accuracy binarizing resnet reorder layer used hyper-parameters learning rate schedule baseline network. reference resnet- xnor-net full-precision network also interesting note top- accuracy single-precision alexnet lower top- accuracy binarized resnet- experimented doubling number ﬁlters layer reduce precision activations weights. table shows results analysis. doubling number ﬁlters -bits precision weights activations beats baseline accuracy -bits activations -bits weights top- accuracy at-par baseline. reducing precision -bits weights activations degrades accuracy compared baseline. binarizing weights activations wide ﬁlters top- accuracy worse baseline full-precision network cost baseline network. widening ﬁlters binarizing weights activations reduces wide network cost full-precision baseline network. although -bits precision seems enough wide networks advocate -bits activation precision -bits weight precision. ternary weights multipliers adders instead. additionally conﬁguration loss accuracy. further accuracy degradation tolerable even binary circuits efﬁcient hardware implementation saving bandwidth weights activations compared full-precision networks. gains realized simpler hardware implementation lower compute cost compared baseline networks. applied wrpn scheme batch-normalized inception network network includes batch normalization layers variant googlenet convolutional ﬁlters replaced convolutions wide ﬁlters. table shows results analysis. using -bits activations -bits weight doubling number ﬁlter banks network produces model almost at-par accuracy baseline single-precision network wide network binary weights activations within full-precision baseline network. adopt straight-through estimator approach work quantizing real number k-bits ordinality quantized numbers mathematically small ﬁnite would zero gradients respect inputs. method circumvents problem deﬁning operator arbitrary forward backward operations. prior works using approach deﬁne operators quantize weights based expectation weight tensors. instance uses threshold scaling factor layer quantize weights ternary domain. scaling factors learned parameters. xnor-net binarizes weight tensor computing sign tensor values scaling mean absolute value output channel weights. dorefa uses single scaling factor across entire layer. quantizing weights k-bits dorefa uses tf.clip_by_val using tensorﬂow quantizing activation tensor values constrain values within range step followed quantization step real number quantized k-bit number. given input real-valued weights activation tensor quantized versions. reserved sign-bit case weight values hence quantized values. thus weights stored interpreted using signed data-types activations using un-signed data-types. appropriate afﬁne transformations convolution operations done using quantized values followed scaling ﬂoating-point constants binary weights approach binarized weight value computed based sign input value followed scaling mean absolute values. binarized activations formulation quantize gradients maintain weights reduced precision format. convolution operation using wrpn forward pass training involves matrix multiplication k-bits signed k-bits unsigned operands. since gradient values -bits ﬂoating-point format backward pass involves matrix multiplication operation using -bits k-bits operand gradient weight update. hard clipping tensors range maps efﬁciently min-max comparator units hardware opposed using transcendental operations long latency operations. dorefa schemes involve division operation computing maximum value input tensor. floating-point division operation expensive hardware computing maximum tensor operation. additionally quantization parameters static require learning involve back-propagation like approach. avoid costly operations propose simpler quantization scheme practice effective performance energy efﬁciency could achieve low-precision compute operation highly depends hardware runs operations. study efﬁciency low-precision operations various hardware targets fpga asic. evaluate wrpn nvidia titan pascal fpga intel arria-. collect performance numbers previously reported analysis well experiments. fpga implement accelerator architecture shown figure prototypical accelerator design used various works asic core accelerator consists systolic array processing elements perform matrix vector operations along on-chip buffers well off-chip memory management unit. conﬁgured support different precision operates ternary values optimized include adder since need multiplier case. binary implemented using xnor bitcount. design targets arria- fpga. asic study synthesize design using intel process technology obtain area energy estimates. figure summarize analysis. figure shows efﬁciency improvements using ﬁrst-order estimates efﬁciency computed based number bits used operation. method would expect efﬁcient respectively however practice efﬁciency gains reducing precision depend whether underlying hardware take advantage low-precisions. baseline. provides ﬁrst-class support operations able take advantage lower precisions. contrary fpga take advantage precisions since amenable implementations fpga’s reconﬁgurable fabric. figure shows performance improvements track well ﬁrst-order estimates figure fact fpga improvements exceed ﬁrst-order estimate. reducing precision simpliﬁes design compute units lower buffering requirements fpga board. compute-precision reduction leads signiﬁcant improvement throughput smaller hardware designs shorter circuit delay figure shows performance performance/watt reduced-precision operations fpga. fpga performs quite well precision operations. terms performance/watt fpga better lower precisions. asic allows truly customized hardware implementation. asic study provides insights upper bound efﬁciency beneﬁts possible low-precision operations. figure show improvement performance energy efﬁciency various low-precision asic relative baseline ﬁgures show going lower precision offers orders magnitude efﬁciency improvements. summary fpga asic well suited wrpn approach. wide wrpn approach requires total operations original network. however lower precision operation better efﬁciency fpga asic. hence wrpn delivers overall efﬁciency win. reduced-precision dnns active research area. reducing precision weights efﬁcient inference pipeline well studied. works like binary connect ternary-weight networks ﬁne-grained ternary quantization target reducing precision network weights still using full-precision activations. accuracy almost always degraded quantizing weights. alexnet imagenet loses top- accuracy. schemes like ﬁne-tuning quantize network weights sacriﬁce accuracy much applicable training networks scratch. shows promising results -bits precision. xnor-net dorefa target training well. targets weight quantization only works targeting activation quantization hurt accuracy. xnor-net approach reduces top- accuracy dorefa quantizing weights activations -bit further xnor-net requires re-ordering layers scheme work. recent work targets low-precision activations reports accuracy non-multiples operand values introduces hardware inefﬁciency memory accesses longer dram cache-boundary aligned end-to-end run-time performance aspect unclear using complicated quantization schemes. target end-to-end training inference using simple quantization method reducing precision without loss accuracy. best knowledge work ﬁrst study reduced-precision deep wide networks show accuracy at-par baseline precision -bits activations -bits weights. report state accuracy wide binarized alexnet resnet still lower compute cost. present wide reduced-precision networks scheme dnns. scheme numeric precision weights activations signiﬁcantly reduced without loss network accuracy. result contrast many previous works reduced-precision activations detrimentally impact accuracy; speciﬁcally -bit weights -bit activations sufﬁcient match baseline accuracy across many networks including alexnet resnet- batchnormalized inception. achieve result quantization scheme increasing number ﬁlter maps reduced-precision layer compensate loss information capacity induced reducing precision. motivate work observation full-precision activations contribute signiﬁcantly memory footprint full-precision weight parameters using mini-batch sizes common training cloud-based inference; furthermore reducing precision activations weights compute complexity greatly reduced wrpn quantization scheme computation precision activations weights hardware friendly making viable deeply-embedded system deployments well cloud-based training inference servers compute fabrics low-precision. compare titan arria- fpga asic implementations using wrpn show scheme increases performance energy-efﬁciency iso-accuracy across each. overall reducing precision allows custom-designed compute units lower buffering requirements provide signiﬁcant improvement throughput.", "year": 2017}