{"title": "Deep Convolutional Networks on Graph-Structured Data", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Deep Learning's recent successes have mostly relied on Convolutional Networks, which exploit fundamental statistical properties of images, sounds and video data: the local stationarity and multi-scale compositional structure, that allows expressing long range interactions in terms of shorter, localized interactions. However, there exist other important examples, such as text documents or bioinformatic data, that may lack some or all of these strong statistical regularities.  In this paper we consider the general question of how to construct deep architectures with small learning complexity on general non-Euclidean domains, which are typically unknown and need to be estimated from the data. In particular, we develop an extension of Spectral Networks which incorporates a Graph Estimation procedure, that we test on large-scale classification problems, matching or improving over Dropout Networks with far less parameters to estimate.", "text": "deep learning’s recent successes mostly relied convolutional networks exploit fundamental statistical properties images sounds video data local stationarity multi-scale compositional structure allows expressing long range interactions terms shorter localized interactions. however exist important examples text documents bioinformatic data lack strong statistical regularities. paper consider general question construct deep architectures small learning complexity general non-euclidean domains typically unknown need estimated data. particular develop extension spectral networks incorporates graph estimation procedure test large-scale classiﬁcation problems matching improving dropout networks less parameters estimate. recent times deep learning models proven extremely successful wide variety tasks computer vision acoustic modeling natural language processing core success lies important assumption statistical properties data namely stationarity compositionality local statistics present natural images video speech. properties exploited efﬁciently convnets designed extract local features shared across signal domain. thanks this able greatly reduce number parameters network respect generic deep architectures without sacriﬁcing capacity extract informative statistics data. similarly recurrent neural nets trained temporal data implicitly assume stationary distribution. think data examples signals deﬁned low-dimensional grid. case stationarity well deﬁned natural translation operator grid locality deﬁned metric grid compositionality obtained downsampling equivalently thanks multi-resolution property grid. however exist many examples data lack underlying low-dimensional grid structure. example text documents represented bags words thought signals deﬁned graph whose nodes vocabulary terms whose weights represent similarity measure terms co-occurence statistics. medicine patient’s gene expression data viewed signal deﬁned graph imposed regulatory network. fact computer vision audio main focus research efforts deep learning represent special case data deﬁned extremely simple lowdimensional graph. complex graphs arising domains might higher dimension statistical properties data deﬁned graphs might satisfy stationarity locality compositionality assumptions previously described. type data dimension deep learning strategies reduced learning fully-connected layers parameters regularization carried weight decay dropout graph structure input known introduced model generalize convnets using learning complexity similar convnet demonstrated simple lowdimensional graphs. work interested generalizing convnets high-dimensional general datasets importantly setting graph structure known priori. context learning graph structure amounts estimating similarity matrix complexity therefore wonder whether graph estimation followed graph convolutions offers advantages respect learning directly data fully connected layers. attempt answer question experimentally establish baselines future work. explore approaches areas application possible apply convolutional networks before text categorization bioinformatics. results show method capable matching outperforming large fully-connected networks trained dropout using fewer parameters. main contributions summarized follows extend ideas large-scale classiﬁcation problems speciﬁcally imagenet consider general setting prior information graph structure available propose unsupervised supervised graph estimation strategies combination supervised graph convolutions. rest paper structured follows. section reviews similar works literature. section discusses generalizations convolutions graphs section addresses question graph estimation. finally section shows numerical experiments large scale object recogniton text categorization bioinformatics. several works explored architectures using so-called local receptive ﬁelds mostly applications image recognition. particular proposes scheme learn group together features based upon measure similarity obtained unsupervised fashion. however attempt exploit weight-sharing strategy. recently proposed generalization convolutions graphs graph laplacian. identifying linear translation-invariant operator grid counterpart general graph view convolutions family linear transforms commuting laplacian. combining commutation property rule localized ﬁlters model requires parameters feature map. however construction requires prior knowledge graph structure shown simple low-dimensional graphs. recently introduced shapenet another generalization convolutions non-euclidean domains based geodesic polar coordinates successfully applied shape analysis allows comparison across different manifolds. however also requires prior knowledge manifolds. graph similarity estimation aspects also extensively studied past. instance studies estimation graph statistical point view identiﬁcation certain graphical model using -penalized logistic regression. also considers problem learning deep architecture series haar contractions learnt using unsupervised pairing criteria features. work builds upon introduced spectral networks. recall deﬁnition main properties. spectral network generalizes convolutional network graph fourier transform turn deﬁned generalization laplacian operator grid graph laplacian. input vector seen signal deﬁned graph nodes. deﬁnition similarity matrix representing undirected graph d−/w graph laplacian eigenvectors graph convolution input signals ﬁlters deﬁned x∗gg represents point-wise product. here unitary matrix plays role fourier transform several ways computing graph laplacian paper choose normalized version wij. note case represents lattice deﬁnition recover discrete laplacian operator also note laplacian commutes translation operator diagonalized fourier basis. follows eigenvectors given discrete fourier transform matrix. recover classical convolution operator noting convolutions deﬁnition linear operators diagonalize fourier domain learning ﬁlters graph thus amounts learning spectral multipliers extending convolution inputs multiple input channels straightforward. signal input channels locations apply transformation channel multipliers however feature need convolutional kernels typically restricted small spatial support independent number input pixels enables model learn number parameters independent order recover similar learning complexity spectral domain thus necessary restrict class spectral multipliers corresponding localized ﬁlters. purpose seek express spatial localization ﬁlters terms spectral multipliers. grid smoothness frequency domain corresponds spatial decay since algorithm train graph convolution layer given matrix interpolation kernel weights forward pass fetch input batch gradients w.r.t outputs compute interpolated weights ˜wff image speech applications order reduce complexity model often useful trade spatial resolution feature resolution representation becomes deeper. purpose pooling layers compute statistics local neighborhoods average amplitude energy maximum activation. layers deﬁned graph providing equivalent notion neighborhood. work construct neighborhoods different scales using multi-resolution spectral clustering consider average max-pooling standard convolutional network architectures. whereas recognition tasks non-euclidean domains considered might prior knowledge graph structure input data many real-world applications knowledge. thus necessary estimate similarity matrix data constructing spectral network. paper consider possible graph constructions unsupervised measuring joint feature statistics another supervised using initial network proxy estimation. unsupervised graph estimation given data rl×n number samples number features simplest approach estimating graph structure data consider distance features given i-th column correlations typically sufﬁcient reveal intrinsic geometrical structure images effects higher-order statistics might non-negligible contexts especially presence sparsity. indeed many situations pairwise euclidean distances might suffer unnormalized measurements. several strategies variants exist gain robustness instance replacing euclidean distance z-score square-correlation mutual information. distance used build gaussian diffusion kernel computed distance corresponding k-th nearest neighbor feature deﬁnes kernel whose variance locally adapted around feature point opposed variance shared. main advantage require labeled data. therefore possible estimate similarity using several datasets share features example text classiﬁcation. discussed previous section notion feature similarity well deﬁned depends choice kernel criteria. therefore context supervised learning relevant statistics input signals might correspond imposed similarity criteria. thus interesting feature similarity best suits particular classiﬁcation task. particularly simple approach fully-connected network determine feature similarity. given training normalized features rl×n labels initially train fully connected network layers weights using standard relu activations dropout. extract ﬁrst layer features rn×m number ﬁrst-layer hidden features consider distance dsup gaussian kernel interpretation supervised criterion extract collection linear measurements best serve classiﬁcation task. thus features similar network decides similarly within linear measurements. constructions seen distilling information learnt ﬁrst network kernel. general case assumptions made dimension graph amounts extracting parameters ﬁrst learning stage moreover assume low-dimensional graph structure dimension parameters extracted projecting resulting kernel leading directions. finally observe could simply replace eigen-basis obtained diagonalizing graph laplacian arbitrary unitary matrix optimized back-propagation together rest parameters model. report results strategy although point learning complexity fully connected network parameters number layers input dimension). order measure performance spectral networks real-world data explore effect graph estimation procedure conducted experiments three datasets text categorization computational biology computer vision. experiments done using torch machine learning environment custom cuda backend. based spectral network architecture classical convolutional network namely interleaving graph convolution relu graph pooling layers ending fully connected layers. noted above training spectral network requires matrix multiplication input output feature perform graph fourier transform compared efﬁcient fast fourier transform used classical convnets. found training spectral networks large numbers feature maps time-consuming therefore chose experiment mostly architectures fewer feature maps smaller pool sizes. found performing pooling beginning network especially important reduce dimensionality graph domain mitigate cost expensive graph fourier transform operation. section adopt following notation descibe network architectures denotes graph convolution layer feature maps denotes graph pooling layer stride pool size denotes fully connected layer hidden units. results also denote number free parameters network pnet number free parameters estimating graph pgraph. used reuters dataset described consists training test sets containing documents mutually exclusive classes. document represented log-normalized words common non-stop words. baseline used fullyconnected network hidden layers consisting hidden units regularized dropout. chose hyperparameters performing initial experiments validation consisting onetenth training data. speciﬁcally number subsampled weights learning rate used pooling rather average pooling. also found using adagrad made training faster. architectures trained using hyperparameters. since experiments computationally expensive train models full convergence. enabled explore model architectures obtain clearer understanding effects graph construction. note architectures designed factor ﬁrst hidden layer fully connected network across feature maps subsampled graph trading resolution graph domain resolution across feature maps. number inputs last fully connected layer always fully-connected network. idea reduce number parameters ﬁrst layer network avoiding much compression second layer. note increase tradeoff resolution graph domain across features reaches point performance begins suffer. especially pronounced unsupervised graph estimation strategies. using supervised method network much robust factorization ﬁrst layer. table compares test accuracy fully connected network gc-p-fc network. figure .-left shows factorization lower layer beneﬁcial regularizing effect. merck molecular activity challenge computational biology benchmark task predict activity levels various molecules based distances bonds different atoms. experiments used dataset samples features. chose dataset challenging relatively dimensionality made spectral networks tractable. baseline architecture used network hidden layers regularized using dropout weight decay. used hyperparameter settings data normalization recommended paper. before used one-tenth training tune hyperparameters network. task found subsampled weights worked best average pooling performed better pooling. since task predict continuous variable networks trained minimizing root mean-squared error loss. following measured performance computing squared correlation predictions targets. designed architectures factor ﬁrst hidden layers fully-connected network across feature maps subsampled graph left second layers unchanged. before unsupervised graph estimation strategies yield signiﬁcant drop performance whereas supervised strategy enables network perform similarly fully-connected network much fewer parameters. indicates able factor lower-level representations retain useful information classiﬁcation task. figure .-right shows test performance models trained. note merck datasets test samples assayed different time samples training thus distribution features typically different training test sets. therefore test performance signiﬁcantly noisy function train performance. however effect different graph estimation procedures still clear. experiments graph construction relied estimation data. measure inﬂuence graph construction compared ﬁlter learning graph frequency domain performed experiments imagenet dataset graph already known namely grid. spectral network thus convolutional network whose weights deﬁned frequency domain using frequency smoothing rather imposing compactly supported ﬁlters. training performed exactly figure except linear transformation fast fourier transform. network consisted convolution/relu/max pooling layers feature maps followed fully-connected layers hidden units regularized dropout. trained versions network classical convolutional network spectral network weights deﬁned frequency domain interpolated using spline kernel. networks trained epochs imagenet dataset input images scaled accelerate training. models yield nearly identical performance. interstingly spectral network learns faster convnet ﬁrst part training although networks converge around time. requires investigation. convnet architectures base appeal success ability produce highly informative local statistics using learning complexity avoiding expensive matrix multiplications. motivated consider generalizations high-dimensional unstructured data. statistical properties input satisfy stationarity composotionality spectral networks learning complexity order convnets. general setting prior knowledge input graph structure known model requires estimating similarities operation making model deeper increase learning complexity much general fully connected architectures. moreover contexts feature similarities estimated using unlabeled data model less parameters learn labeled data. however results demonstrate extension poses signiﬁcant challenges although learning complexity requires parameters feature evaluation forward backward requires multiplication graph fourier transform costs operations. major difference respect traditional convnets require fourier implementations convnets bring complexity thanks speciﬁc symmetries grid. open question whether approximate eigenbasis general graph laplacians using givens’ decompositions similar fft. experiments show input graph structure known priori graph estimation statistical bottleneck model requiring general graphs m-dimensional graphs. supervised graph estimation performs signiﬁcantly better unsupervised graph estimation based low-order moments. furthermore veriﬁed architecture quite sensitive graph estimation errors. supervised setting step viewed terms bootstrapping mechanism initially unconstrained network self-adjusted become localized weightsharing. finally statistical assumptions stationarity compositionality always veriﬁed. situations constraints imposed model risk reduce capacity reason. possibility addressing issue insert fully connected layers input spectral layers data transformed appropriate statistical model. another strategy left future work relax notion weight sharing introducing instead commutation error graph laplacian puts soft penalty transformations commute laplacian instead imposing exact commutation case spectral net.", "year": 2015}