{"title": "Feedforward Sequential Memory Neural Networks without Recurrent Feedback", "tag": ["cs.NE", "cs.CL", "cs.LG"], "abstract": "We introduce a new structure for memory neural networks, called feedforward sequential memory networks (FSMN), which can learn long-term dependency without using recurrent feedback. The proposed FSMN is a standard feedforward neural networks equipped with learnable sequential memory blocks in the hidden layers. In this work, we have applied FSMN to several language modeling (LM) tasks. Experimental results have shown that the memory blocks in FSMN can learn effective representations of long history. Experiments have shown that FSMN based language models can significantly outperform not only feedforward neural network (FNN) based LMs but also the popular recurrent neural network (RNN) LMs.", "text": "introduce structure memory neural networks called feedforward sequential memory networks learn long-term dependency withusing recurrent feedback. proposed fsmn standard feedforward neural networks equipped learnable sequential memory blocks hidden layers. work applied fsmn several language modeling tasks. experimental results shown memory blocks fsmn learn effective representations long history. experiments shown fsmn based language models signiﬁcantly outperform feedforward neural network based also popular recurrent neural network lms. machine learning methods applied model sequential data text speech video important take advantage long-term dependency. traditional approaches explored capture long-term structure within sequential data using recurrent feedbacks regular recurrent neural networks lstm-based models. rnns learn carry complicated transformations data extended periods time store memory weights network. therefore gaining popular sequential data modeling tasks. recently different rnns also surge constructing neural computing models varying forms explicit memory units example proposed memory networks employ memory component read written proposed neural turing machines improve memory neural networks coupling external memory resources learn sort small numbers well symbolic manipulation tasks. work proposed simpler structure memory neural networks namely feedforward sequential memory networks learn long-term dependency sequential data without using recurrent feedback. fsmn extend standard feedforward neural networks introducing memory blocks hidden layers. different rnns overall fsmns remain feed-forward structure learned much efﬁcient stable ways rnns lstms. work evaluated performance fsmn language modeling tasks penn tree bank english wiki large text compression benchmark tasks fsmn based language models signiﬁcantly outperform standard fnn-lms also popular recurrent neural network signiﬁcant margin. feedforward sequential memory network standard feedforward neural network single multiple memory blocks hidden layer. instance figure shows fsmn memory block added second hidden layer. given sequence coefﬁcients form n-dimension learnable vector a··· activation function furthermore shown figure next hidden layer viewpoint signal processing memory block fsmn regarded highorder ﬁnite impulse response ﬁlter recurrent layer rnns namely viewed ﬁrst-order inﬁnite impulse response ﬁlter figure obviously vector regarded coefﬁcients n-order ﬁlter. know ﬁlters compact ﬁlters. however ﬁlters difﬁcult implement. cases ﬁlters become unstable ﬁlters always stable. moreover learning iir-ﬁlter-like rnns requires so-called back-propagation time signiﬁcantly increases computational complexity learning also causes problems gradient vanishing exploding hand proposed fir-ﬁlter-like fsmns efﬁciently learned using standard back-propagation procedure. therefore learning fsmns stable efﬁcient rnns. goal language modeling predict next word text sequence given previous words. explain implement fsmns task. fsmn standard feedforward neural network except additional memory blocks. show memory block efﬁciently implemented sentence-by-sentence matrix multiplications suitable mini-batch based stochastic gradient descent method running gpus. suppose n-order ﬁlter coefﬁcients memory block denoted a··· an}. given sentence consisting words construct therefore sequential memory operations whole sequence computed matrix multiplication similarly extend idea mini-batch composed sentences compute sequential memory representation sentences mini-batch follows backward pass except weights network also need calculate gradients used update ﬁlter coefﬁcients calculate gradients using standard back-propagation algorithm. therefore computation fsmns formulated large matrix multiplications efﬁciently conducted gpus. result fsmn based computational complexity standard training much efﬁcient rnn-lms. evaluated fsmns benchmark tasks penn treebank corpus words following setup large text compression benchmark ltcb enwik dataset composed ﬁrst bytes enwiki-pages-articles.xml. split three parts training validation test sets. limit vocabulary size ltcb replace out-of-vocabulary words <unk>. fsmns hidden units employ rectiﬁed linear activation function i.e. max. nets initialized based normalized initialization without using pre-training. mini-batch size ltcb tasks respectively. initial learning rate weights ﬁlter coefﬁcients respectively kept ﬁxed long perplexity validation decreases least that continue epochs training learning rate halved epoch. task also momentum weight decay avoid overﬁtting. ﬁrst evaluated performance fsmn-lms task. trained fsmn input context window previous words used predict next word. fsmn contains linear projection layer hidden layers memory block ﬁrst hidden layer. memory block -order ﬁlter. table summarized perplexities test various language models. ltcb task trained several baseline systems n-gram using modiﬁed kneser-ney smoothing without count cutoffs; several traditional feedforward nnlms different model sizes input context windows iii) rnn-lm hidden layer nodes using toolkit nd-order fofe based fnnlm different hidden layer sizes. moreover examined fsmn based different architectures. trained -hidden-layer fsmn memory block ﬁrst hidden layer second hidden layer both. order ﬁlter experiments. table summarized perplexities ltcb test various models. experimental results table table shown fsmn based signiﬁcantly outperform baseline higher-order feedforward neural network fofe-based well popular rnn-based quite signiﬁcant margin. work proposed novel neural network architecture namely feedforward sequential memory networks fir-ﬁlter-like memory blocks hidden layer standard feedforward neural networks. experimental results language modeling tasks shown fsmn effectively learn long term history. future work apply model sequential data modeling tasks acoustic modeling speech recognition.", "year": 2015}