{"title": "Visualizing Natural Language Descriptions: A Survey", "tag": ["cs.CL", "cs.AI", "cs.CV", "cs.GR", "cs.HC"], "abstract": "A natural language interface exploits the conceptual simplicity and naturalness of the language to create a high-level user-friendly communication channel between humans and machines. One of the promising applications of such interfaces is generating visual interpretations of semantic content of a given natural language that can be then visualized either as a static scene or a dynamic animation. This survey discusses requirements and challenges of developing such systems and reports 26 graphical systems that exploit natural language interfaces and addresses both artificial intelligence and visualization aspects. This work serves as a frame of reference to researchers and to enable further advances in the field.", "text": "natural language interface exploits conceptual simplicity naturalness language create high-level user-friendly communication channel humans machines. promising applications interfaces generating visual interpretations semantic content given natural language visualized either static scene dynamic animation. survey discusses requirements challenges developing systems reports graphical systems exploit natural language interfaces addresses artiﬁcial intelligence visualization aspects. work serves frame reference researchers enable advances ﬁeld. imagination irrefutable part mankind’s social-cognitive processes visualspatial skills memory access learning creativity communication. people rely capability creating sharing communicating imaginations daily activities. general approaches share imaginations explicit approach implicit approach. former imaginations directly realized using visualization techniques sketching painting computer-aided tools. approach objective nature results accurate description given imagination. however requires high-level visualization skills would take years practice learning. example case computer animation vast variety design tools available. tools provide designers user-friendly graphical user interface follows dominant approach human-computer interactions known windows icons menus pointer model. learning tools even professional designers tedious labor intensive time-consuming requiring learn utilize complex graphical interfaces. furthermore migrating speciﬁc tool another would require learning interfaces scratch. learning process faces sophistication case scripting interfaces graphical apis game engines. learning curve even slower novice users need create simple animation applications. implicit approach hand subjective carried representing imaginations natural language descriptions. approach speaker writer shares imaginations verbal channel audience perceives reconstructs imaginations based internal mental states. hence unique imagination result diﬀerent realizations. simplicity naturalness describing imaginations verbal channel explicit approach considered dominant mechanism sharing imaginations interpersonal communications. motivates researchers develop systems directly convert natural language descriptions target visualizations. natural language interface exploits conceptual simplicity naturalness language create high-level user-friendly communication channel humans machine. interface used generate visual interpretations semantic content given natural language visualized either static scene dynamic animation. nevertheless current technical diﬃculties allow machines completely capture deep semantics embedded within natural languages. diﬃculties root characteristics natural languages semi-structured ambiguous context-sensitive subjective. recent research comprehensive user studies performed compare performance natural language interfaces conventional animation design tasks terms control creativity learning measures. results suggest terms high-level control virtual objects animation design natural language interfaces outperform guis whereas terms spatial motion control simpler guis. also concluded using guis increases creativity micro-level design macro-level design natural language interfaces eﬃcient higher versatility. also shown natural language interfaces signiﬁcantly reduce learning time design time. according study good strategy develop hybrid interface integrates interfaces lets user decide diﬀerent manipulations. three general requirements identiﬁed developing systems. ﬁrst requirement associated computer graphics. system visualizing natural language descriptions requires visualization engine realize ﬁnal interpretation language. fortunately current software hardware technologies computer graphics highly advanced generate natural visualizations real time. thus requirement pose challenges. second requirement related understanding natural language. natural language interface must able disambiguate description discover hidden semantics within convert formal knowledge representation. requirement even limited system present fundamental challenge. third requirement designing integrated architecture. designing system capable integrating natural language interface visualization purposes requires tackling profound technical challenges diﬀerent conceptual operational levels. system requires integration artiﬁcial intelligence techniques natural language understanding knowledge representation planning spatio-temporal reasoning computer graphics techniques real-time rendering action synchronization behavior-based modeling deformation consistent manner. considering above-mentioned requirements main challenges developing systems process disambiguating descriptions expressed natural language capturing deep semantics embedded within surface syntax converting discovered semantics representation processed software. process relies hierarchy sub-processes including limited morphological analysis stemming lemmatization; syntactic analysis part-of-speech tagging syntactic parsing named-entities recognition anaphora resolution; semantic analysis word disambiguation capturing predicate-argument structures role labeling; discourse analysis. natural language interface visualization purpose disambiguate descriptions based scene arrangements capture semantics associated scene layout spatio-temporal constraints parameterized actions people communicate assume target audience priori knowledge context hence elaborate also omit common-sense facts assume audience gaps inferring implicit knowledge challenge current computer software. furthermore challenging task derive meaningful interpretations spatio-temporal relations descriptions world model. refers formal representation information computer software utilize perform complex tasks. representation support insertion update querying operations target knowledge. also represent concepts entities relations constraints uncertainty etc. system designed convert natural language descriptions visual representation requires component represent discovered semantics decide actions taken. also reasoning mechanism embedded within component help system derive implicit knowledge available knowledge. designing component trivial task. semantics represented high-level concepts within component eventually need grounded low-level graphical objects visual features transformations relations. mapping process involves decomposing high-level concepts low-level graphical instructions running serial parallel parametrizing instructions. automating process research goals. scalable system couple high-level semantic processing low-level action decomposition consistent manner. also exploit data-driven techniques generalize unseen scenarios. gathering required tools repositories lexical considering interdisciplinary nature visualization systems natural language interfaces categorize literature several points view. terms design methodology systems classiﬁed rule-based data-driven multiagent systems. another possible classiﬁcation based system behavior divides systems reactive deliberative systems. also possible classify literature based utilized language understanding approach syntactic analysis knowledgebase scheme however none classiﬁcations address graphical aspects. similarly categorize literature based graphical aspects ignore intelligent aspects systems. order consistent classiﬁcation scheme address diﬀerent aspects research works categorize literature based generated output. using scheme classify literature three categories text-to-picture text-to-scene text-to-animation conversion systems. noteworthy throughout article term text interchangeably refers oral written form language utterance. regard verbal commands issued operator textual scripts provided user textual content within pages treated text. authors’ knowledge concerned article ﬁrst comprehensive overview systems approaches associated visualizing natural language descriptions. surprisingly despite scientiﬁc industrial merit many studies carried direction. among existing works solid contributions ﬁeld. survey discusses requirements challenges developing systems reports graphical systems exploit natural language interfaces addresses artiﬁcial intelligence visualization aspects. work serves frame reference researchers enable advances ﬁeld. introduced system elaborate system inputs outputs design methodology architecture implementation language processes graphical processes intelligent processes resources discuss advantages disadvantages well. article organized follows. section provides concise terminology computational linguistics. section overviews text-to-picture conversion systems investigates example systems. section discusses text-to-scene conversion systems elaborates seven systems. section provides comprehensive overview text-to-animation conversion systems. section discusses overall restrictions developed systems provides potential solutions possible directions future studies. section concludes article. considering interdisciplinary nature article possibly attract audiences diﬀerent ﬁelds computational linguistics human-computer interactions artiﬁcial intelligence computer graphics. provide readers self-contained article section provides concise terminology computational linguistics follows. text-to-picture conversion probably simplest method visualizing natural language descriptions treats problem mapping natural language descriptions visual representation data-driven image retrieval ranking problem tries solve using foundations commercial web-based image search engines. approach descriptive terms constituents represent main concepts text extracted using text-mining techniques bag-ofwords named entities n-gram models. assumed annotated dataset images available. case automatic annotation common collect repository multimodal information containing images text co-occurring text around images annotate them. web-based image retrieval systems process carried exploiting surrounding text images text appearing within html tags. extracted text tokenized subset terms selected scored determine weighted annotations corresponding images extracted concepts matched image annotations subset images retrieved ranked given concept based predeﬁned similarity measures. finally concept retrieved images highest rank illustrated order corresponding concepts appear text. approach inherits solid theoretical foundations search engines. also exploiting statistical information retrieval rather natural language understanding text-to-picture conversion approach computationally eﬃcient however result expected visualization three main reasons cannot capture deep semantics embedded within natural language descriptions visualization restricted available images cannot interpolate inbetween visual information. approach main focus survey hence systems discussed. story picturing engine addresses mapping process given textual story representative pictures focusing quantifying image importance pool images. system receives input stories vermont mostly rural state. countryside cozy feeling place ranks related available images accordingly output. system pipeline three processes follows. first descriptor keywords extracted story. purpose stop-words eliminated using manually crafted dictionary subset remaining words selected based combination bag-of-words model named-entity recognition. utilized bag-ofwords model uses wordnet determine polysemy count number senses given word words. among them nouns adjectives adverbs verbs polysemy count selected descriptor keywords. naive named-entity recognizer used extract proper nouns names places people based beginning letter words. images contain least keyword named entity retrieved local annotated image database. next step pipeline estimate similarity pairs images based visual lexical features calculated based linear combination integrated region matching distance wordnet hierarchy. finally images ranked based mutual reinforcement method ranked images retrieved. system basically image search engine gets given description query retrieves ranks related images. despite good accuracy performance story picturing engine retrieves picture given story ignores many aspects temporal spatial relations. goal system augment human-human human-computer communications adding visual modality natural language channel contrast story picturing engine system associates diﬀerent picture extracted phrase presents story sequence related pictures. treats textto-picture conversion problem optimization process tries optimize likelihood extracted phrases images placement given input description. extract phrases system ﬁrst eliminates stop words uses pos-tagger extract nouns proper nouns adjectives. words logistic regression model decide probability picturability based google counts image counts. then textrank algorithm applied computed probabilities keywords selected used form phrases. image selection process based matching extracted phrases image annotations. matching success matched images retrieved. otherwise image segmentation clustering algorithm applied image likely associated query phrase. ultimately retrieved pictures positioned based three constraints including minimum overlap centrality important pictures closeness pictures regarding closeness associated phrases. despite superiority system story picturing engine still inherits drawbacks text-to-picture systems results stilted visualizations. possible improve visualization directly create scene rather showing representative pictures. approach known text-to-scene conversion paradigm lets system elaborate background layout lighting objects poses relative sizes spatial relations features cannot addressed using text topicture conversion systems text-to-scene conversion system words speciﬁc tags carry visual information others. noun proper-noun tags usually associated objects agents places words tags exploited retrieve three-dimensional models model repositories. adjective usually associated object features words utilized alter object attributes color relative size etc. preposition mostly associated spatial relations verbs usually determine actions poses articulated models avatar pointing object. text-to-scene approach generate elaborated uniﬁed visualization given descriptions within single static scene coherent realization comparison text-to-picture approach. nevertheless faces challenges mentioned section designing components. also generated scene static address neither dynamics temporal relations useful visualizing single episode. section overview seven text-to-scene conversion systems. natural language driven image generation early projects generating static scenes natural language descriptions uses restricted form input language basically simple regular expression. main focus nalig investigate relationship spatial information prepositions italian phrases. accepted form phrases system follows using regular expression nalig understand inputs book table. also handle ambiguities within phrases infer simple implicit spatial arrangements using taxonomical rules object supports object deﬁne relations existing objects. rules deﬁned based state conditions containment constraints structural constraints supporting rules. example given input branch roof system infer tree near house branch roof. addition spatial arrangements nalig also utilizes statics infer object support another object based physical equilibrium. nalig restricted system support user interactions ﬂexible inputs spatial relations. language-based placement system rule-based spatial-manipulation system inspired cognitive linguistics. generates static scenes direct manipulation spatial arrangements rigid objects using restricted subset natural language. using restricted grammar able objects hang object one. also disambiguate simple spatial relations wall ﬂoor. system consists simple parser implemented designed restricted input language rendering engine visualize static environment. syntax restricted language form regular expression follows. denotes placement verb speciﬁes type positioning object manipulated. system deﬁnes placement verbs including hang. represents object placed whereas represents reference object. system contains objects walls rugs objects tables lamps already included virtual world. hence user limited pre-existing objects. preposition indicates spatial relation diﬀerent groups spatial relations above/below left/right deﬁned system. kleene plus operator lets system handle compound spatial relations reference objects. example input command ﬂoor front picture lamp decomposed system objects annotated names used match geometric information models corresponding objects. placement carried using axis-aligned bounding boxes objects facilitate determining surface interiors objects. simple failure handling mechanism also used handle non-existent locations. comparison nalig advantages ﬂexible allowed inputs spatial arrangements object repository object manipulations. nevertheless inherits disadvantages nalig restricted terms input language interactions. also focuses spatial relations ignores clues within scene description used infer implicit knowledge. wordseye designed generate scenes containing environment objects characters attributes poses kinematics spatial relations. system input textual descriptions include information actions spatial relations object attributes. system consists main components including linguistic analyzer scene depicter. linguistic analyzer equipped tagger statistical parser. early version analyzer implemented common lisp later mica parser exploited well. parses input text constructs dependency structure represents dependencies among words facilitate semantic analysis. structure utilized construct semantic representation objects actions relations represented terms semantic frames words noun tags associated objects associated hyponyms hypernyms acquired using wordnet. spatial relations captured using predeﬁned spatial patterns based dependency structure. words verb associated parametrized functions indicate eﬀects verbs. recent development lexical knowledge extracted wordnet framenet semi-manually reﬁned construct scenario-based lexical knowledge resource essentially lexical knowledgebase tailored represent lexical common-sense knowledge text-to-scene conversion purposes knowledge sblr represented vignet extension framenet consists intermediate frames called vignettes bridge semantic semantic frames framenet low-level graphical frames. vignet also contains implicit knowledge restricted environments kitchen. knowledge remedy missing common-sense facts within natural language descriptions acquired manual descriptions pictures gathered amazon mechanical turk online crowd-sourcing framework data collection using human intelligence task collected corpus processed using naive text processing techniques populate vignet extracted vignettes depiction module converts semantic frames low-level graphical speciﬁcations. purpose uses depiction rules convert objects actions relations attributes extracted semantic representation realizable visual counterpart. geometric information objects manually tagged attached models. component also employs transduction rules solve implicit conﬂicting constraints positioning objects scene incremental manner. soon layout completed static scene rendered using opengl. semantic database consists nouns verbs whereas visual database consists models images. diﬀerent features models geometric shape type ﬂexibility embeddability manually annotated. instance objects long thin vertical base annotated stem. wordseye also contains large rules including spatial rules depiction rules transduction rules. example contains three rules kicking action whose ﬁring strengths evaluated based type object kicked. wordseye utilized thousand on-line users create static scenes. although achieved good degree success allowed input language describing scenes stilted another problem wordseye interactive exploit user’s feedbacks. also interface provided adding knowledge rules system would require hard-wiring system. automatic visualization descriptive texts generates static scenes descriptive text emphasizing spatial relations naturalness generated scene. consists layers including automatic scene graph generation layer object arranging layer. former responsible processing extracting embedded information within text generating scene graph information. utilizes gate open-source text processing tool pre-processor tasks lemmatizing nouns tagging generating dependency structures. reﬁnes pre-processed text segmenting blocks based punctuation marks coordination conjunctions assigns meta-data prepositions nouns within text ignores rest. prepositions grouped according semantic similarities. example under below beneath prepositions classiﬁed group. meta-data contain word role position text quantity corresponding model. next step directed graph constructed node represents object edge represents preposition. constructed graph pruned merging redundant nodes. graph provides eﬃcient data structure traversing spatial relations. object arranging layer uses described graph render scene. assigns axis-aligned bounding object applies distance rotation heuristics standardizing scales orientations dependent supporting objects. rotation heuristic ensures dependent object faces supporter. also applies little randomness achieve untidy appearance. heuristics result natural appearance scene. avdt focuses naturalness generated layusing manually crafted heuristics proper analysis spatial relations hence results natural-looking scenes comparison wordseye. also contrary wordseye avdt deal linguistic cycles allows comfortable inputs. example shown wordseye fails visualize sentence table vase whereas avdt successfully realize general avdt inherits problems wordseye stilted input lack interactivity relying hand-crafted rules. contrary previous systems system developed stanford university infers implicit relations partially supports interactive scene manipulation active learning. system scene template generated input text converted geometric graph utilized render static scene. scene template constructed input text using stanford corenlp language processing tool graph objects vertices relations edges. objects recognized detecting nouns considered visualizable according wordnet. words adjective within noun phrases extracted identify attributes objects. spatial relations extracted using pre-deﬁned patterns. natural language descriptions usually contain common-sense facts spatial arrangements. alleviate challenge system uses conditional probability model object occurrences hierarchy priors exploits bayes’s rule infer implicit spatial arrangements. inferred knowledge inserted scene template graph. example sample input text cake table infer cake plate plate table. geometric graph contains model instances correspond objects within scene template associated spatial arrangements. graph used directly render static scene. system also supports interactive scene manipulation active learning. user objects deﬁned positions remove existing objects. also select object within scene annotate system modiﬁes probabilistic model support hierarchy observing users design scenes. example user asks system table system increases probability co-occurrence table probability table supporting cup. system surpasses previous text-to-scene conversion systems terms adaptive behavior interactivity. however terms language understanding richness model repository wordseye avdt outperform system. systems mentioned utilize textual clues either pre-deﬁned rules models learned corpora. trend text-to-scene conversion systems learning associations textual visual clues. research works follow approach used text-to-picture conversion systems focusing learning visual features available image database extracting associations visual textual features automate visualization process.however contrary text-to-picture conversion systems associations position objects within static scene rather selecting representative pictures. attribit designed help users create visual content using subjective attributes dangerous airplane. system provides user parts model interest helps assembling components construct plausible model. exploits crowd-sourcing presents volunteers models diﬀerent parts objects airplane wings asks compare every pair models using adjectives. ranks associations components gathered attributes using support vector machine classiﬁer. learned model used along directly capture semantic attributes provide user corresponding parts model. promising data-driven system developed microsoft research center introduced system learns visual features abstract scenes extracts semantic information corresponding corpus learns associations extracted visual features semantics generate scenes based unseen natural language descriptions. similar attribit system utilizes gathering training dataset. exploits conditional random field extract objects occurrences attributes positions. extracting visual features system extracts semantics form predicate tuples using semantic role analysis scene generation system learns associations predicate tuples visual features based co-occurrences using highest mutual information uses associations generate scene. system purely data driven learns generate static scenes observing available scenes corresponding descriptors. however support online learning. authors used system simple scenario children’s playground. therefore clear whether approach generate satisfactory scenes scenarios well. system also lacks strong semantic analysis capturing general dependencies. text-to-animation paradigm adds dynamics static scenes realizes temporal relations extra layer towards naturalness generated visualization. paradigm addition linguistic analysis performed text-to-scene conversion systems visual verbs within text captured parametrized grounded virtual actions manipulations within digital world. action parametrization challenge case general-domain systems requires inference knowledge trajectories targets intermediate actions moreover text-to-animation conversion system constraint network expanded capture spatio-temporal constraints rather static spatial constraints. words objects enter exit scene spatial relations among vary simulation time proceeds. approach visualize imaginations natural aforementioned approaches. section overview text-to-animation conversion systems. noteworthy classify systems user controls embodied agents natural language commands text-to-animation systems. reason similar conventional text-to-animation conversion systems systems manipulate environment based verbal descriptions commands well. diﬀerence that systems user manipulates world embodied agents rather directly manipulating objects. shrldu developed winograd massachusetts institute technology pioneer systems integrated computer graphics. also early systems used deep semantic parsing. shrldu consists simulated robotic manipulator equipped intelligent controller operates within virtual world. world contains blocks diﬀerent shapes sizes colors. robotic perform three actions blocks including moving block location grasping block ungrasping block. robot manipulates environment according restricted given natural language commands. shrldu implemented micro-planner lisp programming languages. architecture consists four modules including language analyzer planner dialogue manager graphical engine. language analyzer operates based systematic grammar view language. validates syntactic analysis semantic clues acquired environment parsing process. instance ambiguous command pyramid block ﬁrst recognizes pyramid possible noun phrase checks world model determine whether unique pyramid exists. based observation decides whether block part noun phrase. planner component used plan sequence feasible actions reach goal state deﬁned input command. utilizes backward chaining algorithm considering preconditions actions plan sequence manipulations. example given world model block blue block user asks pyramid blue block robot ﬁrst grasp block move random location ungrasp grasp pyramid move blue block ﬁnally ungrasp interesting feature shrldu dialogue manager enables answer simple queries regarding world conﬁguration history actions taken. also command clariﬁcation case ambiguities input command acknowledge accomplishment tasks. despite restricted grammar operational environment naive dialogue manager shrldu inspired many systems. parameterized action representation developed university pennsylvania framework controlling virtual humans using natural language commands context-sensitive fashion main focus develop comprehensive knowledge representation scheme reﬂect input commands agents’ behaviors. structure contains applicability condition start results participants semantics path purpose termination duration manner sub-actions parent action previous action concurrent action next action representation applicability condition boolean expression indicates feasibility action given agent. start result indicate states time stamps given action performed agent beginning termination action. participants refer agent executing current passive objects related action. semantics include boolean pre-conditions post-conditions action must satisﬁed agent perform action. also embed motion force action applied. path denotes start points direction distance motion. purpose determines whether current action satisfy conditions trigger another action termination determines condition terminating current action. structure also contains pointers pars including parent next previous concurrent actions. execution architecture implemented python reactive framework designed handle representation. architecture consists components including language converter database manager execution engine agent process visualizer. language converter parses input commands using xtag parser uses naive string matching algorithm corresponding objects agents database database manager. also captures verbs adjectives order construct corresponding representation given verbal command. execution engine synchronizes actions using universal clock passes received structures corresponding agent processes. also controls visualizer update environment. active agent within virtual world assigned agent process handles queue pars executed visualizer uses opengl render virtual world inhabitants based received commands execution engine. architecture relies shallow parsing rather attempting capture deep semantics. also support deliberative planning essential generating plans complicated goals. lack interactivity another drawback architecture. carsim domain-speciﬁc system developed generating simple animations accidents based swedish accident reports collected news articles narratives victims oﬃcial transcriptions oﬃcers. consists main modules including information extraction module visualization module. information extraction module analyzes input text converts triplet representation ¡src¿ denotes scene objects weather represents road objects cars collisions happened accident. module utilizes granska tagger tagging input text uses small lexicon regular expressions extract named entities street names. also exploits local dictionary extracted wordnet discover action verbs. light domain-speciﬁc ontology combined classiﬁer trained using small example reports employed extract events textual description accidents. ontology also utilized solve co-references. visualization module utilizes animation planner graphical engine render planned animation. animation planner exploits naive greedy algorithm plan animation considering constraints initial positions initial directions trajectories. planning algorithm support backtracking thus cannot optimal plans. constraints addressed using small spatial temporal rules. initial direction position inferred directly input report propagated objects whose initial condition explicitly system mostly focuses practical aspects rather theoretical arguments. shown fair degree success limited domain. lacks solid mechanism harvest information user interactions feedbacks. also contain strong object repository lexical resources. scriptviz aims replace manual storyboard drawing automatic dynamic scene generation motion-picture production process. capable analyzing screenplays written well-formed sentences animating corresponding objects agents actions. system consists three interacting modules including language understanding module high-level planner scene generator. language understanding module uses apple parser derive syntactical structure input text. separates clauses based conjunctions extracts actions verbs recognizes objects proper nouns. verb proper noun matched actions objects using naive binary matching mechanism respectively. high-level planning module generates action plans based information received language understanding module. planning process completed within four consecutive phases. first oﬄine plan outline extracted plan database respect objects actions detected input script. states objects agents collected virtual environment. information used decide feasibility actions according current conﬁguration environment. case feasible action parameters oﬄine plan result represented using structure scene generator assigns resulted corresponding agent updates states renders scene real time. scriptviz implemented java uses opengl graphical engine. support interactive modiﬁcation generated animation embed lexical common-sense resources. also limited model repository scene layout options. limitations result weak visualizations given scripts. furthermore clear kind actions agents perform treatment articulated bodies discussed work. confucis multi-modal text-to-animation conversion system generate animation single input sentence containing action verb synchronize speech. basically narrator system developed animating human characters peripheral narrator agent storytelling actions. confucis address temporal relations actions performed virtual humans. utilizes h-anim standard modeling animating virtual humans. supports synchronization facial expressions parallel animation upper lower body human models confucis consists knowledgebase language processor media allocator animation engine text-to-speech engine narrator synchronizer. knowledgebase contains lexicon parser visual database. visual database contains limited models action animations. language processor uses connexor functional-dependency grammar parser wordnet lexical conceptual structure database parse input sentence capture semantics carries. media allocator exploits acquired semantics generate representation three modalities including animation speech narration. animation engine uses generated visual database generate animation. text-to-speech narrator modules also generate speech initialize narrator agent respectively. finally synchronizer integrates modalities vrml later used render animation. main challenges text-to-animation conversion system deﬁning sub-actions result high-level action. hypothetical scenario assume input sentence john hits paul bottle john distance paul bottle table distance john. realize input sentence plausible animation system exploit planner schedule intermediate actions john walks toward table picks bottle walks toward paul hits bottle. confucius addresses challenge using hand-crafted sub-actions turn restrict animation predeﬁned actions also limited number sentences input restricted format input sentences user restricted expressing intended description. confucius interactive sense user modify generated animation. scenemaker collaborative multi-modal system designed pre-visualizing scenes given scripts facilitate movie production process. system successor confucis system exploits underlying language processing multi-modal animation generation tools. scenemaker expands confucis adding common-sense knowledge genre speciﬁcation emotional expressions capturing emotions scripts. users edit generated animation online mobile devices. scenemaker consists layers including user interface mobile device scene production layer running server. user interface receives screenplay user provides animation script scene editor edit generated animation. scene production layer contains three components operating serial manner including understanding module reasoning module visualization module. understanding module performs text analysis using confucis platform. reasoning module uses wordnetaﬀect extension wordnet conceptnet interpret context manage emotions plan actions. visualization module fetches corresponding models music database generates speech sets camera lighting conﬁguration. despite adding interactions alleviating input restrictions scenemaker inherits ﬂaws confucis terms action deﬁnition. noteworthy could snapshots resulting animation published articles. system designed generate motion virtual agents using motion animations stored within motion database carry task captures pre-deﬁned action verbs including intransitive transitive ditransitive verbs input using local dictionary. system exploits motion framesan extension case frames focusing semantic valence knowledge representation scheme consists agent motion instrument target contact position direction initial posture adverbs modify motion. assumed characters objects motion frames manually predeﬁned user. workﬂow system follows. first input sentence parsed using stanford corenlp tool small rules dictionary utilized extract query frames temporal constraints. query frames motion frames extracted input matched motion database. temporal constraints determine whether actions serial parallel. system uses extracted temporal constraints create rough schedule actions searches motion database motion clips match query frames. motion database consists manually annotated atomic motions represented motion frames. atomic actions combined create complex actions. matching process done consecutive steps. first query frame matched motion frames database based actions agents. retrieved candidates ranked using weighted similarity measure based target instrument initial posture adverbs. system generate intermediate motions locomotion grabbing instrument. ultimately predeﬁned scene information retrieved atomic motions integrated order animate motions. system relies oﬄine motion database makes diﬃcult handle unseen motions. also clear atomic motion clips fused generate compound motions. another disadvantage system limited language processing capabilities. last least imposes high volume workload users assuming characters objects motion frames manually predeﬁned users. intelligent virtual environment language learning domainspeciﬁc multi-modal virtual reality system consists embodied conversational agents designed improve speaking listening skills non-native users english. ivell implements scenarios airport shopping mall learners speak domain-speciﬁc agents immigration agent manipulating virtual world using haptic robot. agents alter diﬃculty level conversation automatically evaluating user’s linguistic proﬁciency. agent consists abstract layer embodied layer. abstract layer consists language interpreter user evaluator fuzzy knowledgebase haptic interpreter language generator action coordinator. language interpreter lemmatizes parses inputs using opennlp tool matches results deterministic ﬁnite automata capture user’s intentions. user evaluator uses weighted model score user’s proﬁciency. knowledgebase light domain-speciﬁc fuzzy ontology keeps knowledge predeﬁned tasks. haptic interpreter maps low-level force position vectors acquired haptic robot high-level perceptions. language generator generates answers diﬀerent diﬃculty levels based knowledge extracted knowledgebase. action coordinator synchronizes graphical actions haptic actions output speech whose score closest user’s proﬁciency level. embodied layer contains speech recognizer text-to-speech engine haptic interface avatar controller. system developed c.net uses autodesk dmax dvia virtools model render environment. diﬀerent modules within system communicate synchronously tcp/ip protocol provides system distributed processing capabilities. ivell interactive system adapt interactions based user’s proﬁciency level. also utilizes natural language generator augment interactions. moreover asks user’s help able understand utterance. nevertheless uses limited approach capture semantics. also similar previous systems uses limited hardwired actions. early text-to-animation synthesis systems story driven animation system introduced japanese system consists three modules including story understanding stage directing action generating modules implemented prolog lisp programming languages. system input restricted unambiguous text contain sentences describing actions time sequence. story understanding module performs syntactic semantic analyses. however original article explain applied techniques. also uses assumption-based reasoning adds simple implicit assertions story. stage direction module exploits simple heuristics position actors background based extracted information generated assertions. action generating module uses model descriptions motion descriptions. limited simple articulated ﬁgures primitive joint motions deﬁned combined create simple animation. attempts create interactive interface animating stages simple stories described restricted sentences. stage includes objects attributes simple spatial relations. spatial relations captured using regular expressions represented format. utilizes xml-based knowledgebase parametrize extracted properties stage. knowledgebase contains visual descriptions objects attributes spatial relations. information extracted input text knowledgebase integrated representation converted vrml format. vrml animated within java applet lets user manipulate stage using mouse commands. despite simplicity restricted nature provides users cross-platform functionalities. interactive e-hon japanese multi-modal storytelling system designed facilitating interactions parents children animating explaining diﬃcult concepts simpler form using content. system uses japanese morphological analyzer lexicon extract time space weather objects actions story. extracted information matched lookup tables including background table action table. time space weather matched background table provide animation appropriate static background. objects actions matched action table used retrieve corresponding recorded animation database. system mostly relies lookup tables binary matching algorithms severely limits capability semantic analyses. semi-automatic system developed rhodes university generates animations given annotated ﬁction texts. basic assumption system characters objects environment conﬁguration spatial relations character transitions text annotated well-formed structure advance. uses annotations characters objects query model database. system exploits query expansion mechanism using wordnet enhance possibility ﬁnding proper models. also uses annotated spatial information construct spatio-temporal constraint network. provides users interface alter constraint network increase artistic aspects generated animation. layout constraints satisﬁed using incremental greedy algorithm. system developed python extracted models environment rendered using blenderd. system lets user modify animation manipulating constraints provides robust model matching scheme using query expansion mechanism. hand requires annotated ﬁction texts input labor-intensive tedious task. data-driven system developed university melbourne attempts train classiﬁer ground high-level verbs low-level graphical tasks. carry task extracts verb features collocation features semantic role features scripts. also extracts binary spatial features virtual stage. linguistic visual features used train maximum entropy classiﬁer decide next graphical action. despite interesting approach co-training semantic stage features fails provide proper means interaction. webanimation multi-modal pedagogical system uses content related recipes create online animation teach users cook. converting content animation done within three steps including extracting relevant text capturing semantics animating actions. relevant recipe information located traversing html tags analyzed using phoenix parser extracted instructions mapped actions captured ingredients associated objects. domain-speciﬁc ontology utilized match actions graphical representation. ingredients also matched graphical models. finally user-created screenplay used synchronize animation monologue explaining recipe. system user craft screenplay interferes pedagogical purpose. also considering noise-prone nature content clear well system behave mining useful content. vistd domain-speciﬁc system creating animation historical naval battles narratives provided users. system uses manually populated ship speciﬁcation database temporal database. temporal database populated narrative analyzer extracts time date structures using regular expressions. retrieved information databases converted vrml format. vistd designed restricted way. similar nalig detect simple syntactic structures utilizes small dictionary. diﬀerent approach relies service-oriented multi-agent design methodology proposed models agents using nlpingenias multi-agent system based ingenias framework nlpingenias exploits natural language descriptions model agents supports user-in-the-loop disambiguation descriptions. acquired agent models alice rapid prototyping environment generating virtual environments render world agents. system approaches text-to-animation conversion problem software engineering point view. exploits agile software development using existing platforms rather struggling theoretical sophistications. hand restricted limitations building blocks cannot tailor meet speciﬁc requirements. adaptive animation generation system introduced hassani system multi-agent data-driven system utilizes statistical content mining techniques extracting attribute values objects relative sizes velocities. system consists three interacting agents including information retrieval agent cognitive agent language processing agent. cognitive agent contains knowledgebase planner decide actions. also interacts visualization interface terms high-level visual operations perceptions. figure evolution text-to-scene conversion systems terms lexical ﬂexibility grammatical ﬂexibility action diversity spatial diversity object diversity. authors mostly focus information retrieval agent elaborate language processing agent. reported accuracy retrieved results promising. however results provided simulating solar system clear whether generalize scenarios well. language processing agent employs regular expressions extracting embedded information query generation. elaborated systems including text-to-picture conversion systems text-toscene conversion systems text-to-animation conversion systems. evolution text-to-picture conversion systems identiﬁed main directions. systems evolved terms extracting text-image associations. early systems exploit associations text image annotations. later associations augmented fusing visual features semantic features. systems also evolved terms output. early system provides users representative picture whereas successor system provides users images ordered based temporal input descriptions. future text-to-picture conversion systems improve exploiting better semantic processing image processing association learning techniques. however limited pictures results enhance dramatically comparison current systems. investigate evolution text-to-scene conversion systems terms measures including lexical ﬂexibility grammatical ﬂexibility action diversity spatial diversity object diversity. lexical grammatical ﬂexibility measures related ﬂexibility input language whereas three measures determine quality output. measures deﬁned based likert scale distinct values including evolution timeline text-to-scene conversion systems illustrated figure figure evolution text-to-animation conversion systems terms lexical ﬂexibility grammatical ﬂexibility action diversity spatial diversity temporal diversity object diversity structure improve nalig wordseye longer enhanced. trend represents current technical diﬃculties understanding natural language. evolution action diversity follows similar trend. large number possible actions practical craft them. hand learning actions associating action verbs challenge current machine vision techniques. terms spatial object diversity wordseye almost achieved good performance relying huge object database large number hand-crafted spatial rules. spatial relations limited hand crafted. important observation current data driven systems outperform rule-based systems. probably data-driven systems used feasibility studies whereas rule based systems wordseye commercialized which turn provide required resources crafting many rules possible. moreover authors’ knowledge concerned useful dataset available purpose. investigate evolution text-to-animation conversion systems similar approach adding temporal diversity measure measures used evaluate text-to-scene conversion systems. evolution timeline illustrated figure surprisingly shown ﬁgure trend indicates text-to-animation conversion systems improved much since shrldu. systems improve terms object diversity spatial diversity using similar approaches taken systems wordseye. also temporal relations limited spatial relations measure improved well. nevertheless text-toanimation conversion systems inherit challenges related actions input language. conclude text-to-scene text-to-animation conversion systems signiﬁcantly improve machine vision language understanding methods improved. fortunately advances deep convolutional neural networks long short-term memory neural networks gradually enhancing areas research respectively. summarize discussed systems table type system indicates whether text-to-picture text-to-scene text-to-animation conversion system. interactivity indicates whether provides user means manipulate generated output whereas adaptive characteristic refers system’s capability extracting information given system priori. system uses data-driven techniques content mining active learning crowd-sourcing association learning considered adaptive. interface type text speech pointer system domain determines whether built general custom purposes. syntactic semantic analyses indicate approaches system utilizes analyze text. finally methodology knowledgebase determine paradigm system built exploited knowledge resource respectively. shown table terms system behavior systems interactive adaptive. behavioral information reveals fundamental research works carried direction. system designed visualizing natural language descriptions interactive adaptive. considering current technical challenges designing complete natural language understanding component system harvest relevance feedbacks modiﬁcations performed user evolve incremental manner. system disambiguate input text collaboration user well. also considering huge amount common-sense information required system practical gather information manually. hence datadriven methods active learning integrated systems. moreover possible pre-determine possible actions user system. actions range character shooting horse galloping hills. address challenge system able detect capture motions actions learning dynamics features actions re-target agents. potential challenging approach would using machine vision techniques learn actions online annotated multi-media content youtube. tasks currently bottleneck systems. however developments end-toend learning paradigms especially combination deep learning reinforcement learning shown potentially promising results applied mitigate mentioned challenges terms interface type systems utilize mouse interactions utilize speech. suggested study reported better hybrid interface consisting natural language mouse interactions. also considering current advances speech recognition technology simpler speech rather typed text. terms domain systems general-domain systems whereas type system story picturing engine text-to-picture synthesizer wordseye avdt stanford university nalig attribit microsoft university melbourne shrldu confucis scenemaker scriptviz kyushu institute carsim ivell sdas adaptive animation interactive e-hon vistd webanimation rhodes university ingenias-based system domain interface general typed text general typed text general typed text general typed text typed text general text pointer general general typed text general typed text general typed text typed text general speciﬁc typed text general typed text typed text general general typed text general typed text speciﬁc typed text typed text general speech+pointer speciﬁc general typed text speciﬁc typed text typed text general speciﬁc typed text speciﬁc typed text speciﬁc typed text typed text speciﬁc general typed text system story picturing engine text-to-picture synthesizer pos-tagging wordseye avdt stanford university nalig attribit microsoft university melbourne shrldu confucis scenemaker scriptviz kyushu institute carsim ivell sdas adaptive animation interactive e-hon vistd webanimation rhodes university ingenias-based system regular expression statistical parsing statistical parsing statistical parsing regular expression pos-tagging pos-tagging statistical parsing dependency parsing semantic parsing dependency parsing lexical dependency parsing lexical statistical parsing statistical parsing pos-tagging dependency parsing none none statistical parsing unknown unknown regular expression none none regular expression none regular expression statistical parsing ontology none regular expression none regular expression statistical parsing active learning data-driven wordnet data-driven rule-based rule-based rule-based data-driven wordnet rule-based data-driven data-driven data-driven rule-based rule-based wordnet+conceptual database rule-based wordnet-aﬀect+conceptnet rule-based rule-based rule-based wordnet+ontology rule-based multi-agent rule-based data-driven wordnet none rule-based none rule-based ontology rule-based rule-based wordnet rule-based multi-agent designed domain speciﬁc. nevertheless percentage reﬂect completeness systems. example even though carsim domain-speciﬁc system outperforms general-domain systems terms language processing semantic analysis. except cases designing general-domain systems resulted systems restrictions. systems ignore adaptive interactive behaviors heavily rely hard-wiring rules. summarized table systems designed based data-driven approach follow multi-agent paradigm rule-based systems. surprisingly general-domain systems designed following rule-based approach turn prevents supporting appropriate degrees generalizability. successful system ignore neither priori knowledge provided experts chunks knowledge acquired diﬀerent online resources. system also able distribute tasks among diﬀerent agents support cross-platform service-oriented models. therefore practical general natural language visualizer requires integration priori knowledge extracted knowledge process distributing tasks among agents terms syntactic analyses systems exploit regular expressions utilize tagging employ syntactic parsing. among general-domain systems syntactic parsing exploit tagging resulting loss constituent information regular expressions essentially ignore syntactic information. latter methods cannot provide proper syntactic analyses comparison parsing techniques. therefore general-domain systems suﬀer deﬁciency. terms semantic analyses systems rely naive matching keywords equivalent ignoring semantics. hand systems exploit shallow semantic analyses. shockingly general-domain systems follow approach. furthermore systems knowledgebase ontologies semantic analysis whereas exploit user-in-the-loop semantic analysis. rest systems rely shallow semantic analysis follows association analysis dependency analysis semantic role analysis. finally terms using knowledgebase lexicons ontologies systems completely ignore resources. ratio general-domain systems. words great fraction general-domain systems require commonsense knowledge equipped knowledge resources. fact highlights another fundamental problem current systems simply ignore knowledge resources hence cannot infer unpredicted situations. among systems utilizing sort knowledge resources exploit wordnet lexicon. identify main problems current systems. ﬁrst problem associated current technical challenges natural language understanding knowledge representation common-sense knowledge implicit knowledge action learning. surprise second problem rooted fact current systems appreciate neither available resources available techniques hence meet expected requirements natural language visualizer. ﬁrst problem possibly alleviated using end-to-end learning algorithms near future. deep learning shown promising results machine vision object recognition speech recognition language modeling deep reinforcement learning shown promising results action learning remedy second problem comarticle discussed requirements challenges developing systems capable visualizing descriptions expressed natural language. reported systems elaborated methodology; implementation; natural language processing aspects including morphological syntactic semantic analyses; knowledgebase; lexicons; components; computer graphics aspects rendering model repositories; pros cons systems. conclude that addition current technical challenges natural language understanding providing common-sense knowledge inferring implicit knowledge action learning systems introduced literature appreciate neither available resources available techniques hence meet expected requirements natural language visualizer. predict that using end-to-end learning algorithms current challenges developing systems mitigated foreseeable future.", "year": 2016}