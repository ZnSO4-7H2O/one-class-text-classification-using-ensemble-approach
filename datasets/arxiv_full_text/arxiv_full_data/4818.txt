{"title": "A Unified Approach for Learning the Parameters of Sum-Product Networks", "tag": ["cs.LG", "cs.AI"], "abstract": "We present a unified approach for learning the parameters of Sum-Product networks (SPNs). We prove that any complete and decomposable SPN is equivalent to a mixture of trees where each tree corresponds to a product of univariate distributions. Based on the mixture model perspective, we characterize the objective function when learning SPNs based on the maximum likelihood estimation (MLE) principle and show that the optimization problem can be formulated as a signomial program. We construct two parameter learning algorithms for SPNs by using sequential monomial approximations (SMA) and the concave-convex procedure (CCCP), respectively. The two proposed methods naturally admit multiplicative updates, hence effectively avoiding the projection operation. With the help of the unified framework, we also show that, in the case of SPNs, CCCP leads to the same algorithm as Expectation Maximization (EM) despite the fact that they are different in general.", "text": "present uniﬁed approach learning parameters sum-product networks prove complete decomposable equivalent mixture trees tree corresponds product univariate distributions. based mixture model perspective characterize objective function learning spns based maximum likelihood estimation principle show optimization problem formulated signomial program. construct parameter learning algorithms spns using sequential monomial approximations concave-convex procedure respectively. proposed methods naturally admit multiplicative updates hence effectively avoiding projection operation. help uniﬁed framework also show that case spns cccp leads algorithm expectation maximization despite fact different general. sum-product networks deep graphical model architectures admit exact probabilistic inference linear time size network similar traditional graphical models main problems learning spns structure learning parameter learning. parameter learning interesting even know ground truth structure ahead time; structure learning depends parameter learning better parameter learning often lead better structure learning. poon domingos gens domingos proposed generative discriminative learning algorithms parameters spns. high level approaches view spns deep architectures apply projected gradient descent optimize data log-likelihood. several drawbacks associated pgd. example projection step hurts convergence algorithm often lead solutions boundary feasible region. also contains additional arbitrary parameter projection margin hard well practice. authors also mentioned possibility applying algorithms train spns viewing nodes spns hidden variables. presented update formula without details. however update formula given incorrect ﬁrst pointed corrected paper take different perspective present uniﬁed framework treats special cases learning parameters spns. prove complete decomposable equivalent mixture trees tree corresponds product univariate distributions. based mixture model perspective precisely characterize functional form objective function based network structure. show optimization problem associated learning parameters spns based principle formulated signomial program exponentiated gradient viewed ﬁrst order approximations signomial program suitable transformations objective function. also show signomial program formulation equivalently transformed difference convex functions formulation objective function program naturally expressed difference convex functions. formulation allows develop efﬁcient optimization algorithms learning parameters spns based sequential monomial approximations concave-convex procedure respectively. proposed approaches naturally admit multiplicative updates hence effectively deal positivity constraints optimization. furthermore uniﬁed framework also show cccp leads algorithm despite approaches different general. although mainly focus based parameter learning mixture model interpretation also helps develop bayesian learning method spns cccp viewed different levels convex relaxation original hence framework also provides intuitive compare four approaches. conduct extensive experiments benchmark data sets compare empirical performance cccp. experimental results validate theoretical analysis cccp best among approaches showing converges consistently faster stability three methods. furthermore cccp boost performance learnspn showing achieve results comparable state-of-the-art structure learning algorithms using spns much smaller network sizes. simplify discussion main idea uniﬁed framework focus attention spns boolean random variables. however framework presented general easily extended discrete continuous random variables. ﬁrst deﬁne notion network polynomial. denote indicator variable returns otherwise. deﬁnition unnormalized probability distribution boolean random vector network polynomial multilinear function indicator variables summation possible instantiations boolean random vector sum-product network boolean variables rooted computes network polynomial leaves univariate indicators boolean variables internal nodes either product. node computes weighted children product node computes product children. scope node deﬁned variables indicators among node’s descendants. node terminal node indicator variable scope else v∈ch scope. complete node children scope root node xn}. paper focus complete decomposable spns. complete decomposable node deﬁnes network polynomial corresponds sub-spn rooted network polynomial denoted network polynomial deﬁned root computed recursively children. probability normalization constant computed spns setting values leaf nodes i.e. leads efﬁcient joint/marginal/conditional inference spns. introducing ﬁrst introduce geometric programming strict subclass monomial deﬁned function domain restricted positive orthant coefﬁcient positive exponents properties posynomials positivity allows transform posynomial standard form convex program since posynomials convex functions general. however effectively transform convex problem using logarithmic transformation trick multiplicative coefﬁcients monomial also objective/constraint function form except multiplicative constant inside monomial restricted positive i.e. take real value. although difference seems small huge difference computational perspective. negative multiplicative constant monomials invalidates logarithmic transformation trick frequently used result cannot reduced convex programs believed hard solve general section show parameter learning problem spns based principle formulated sequence optimal monomial approximations combined backtracking line search concave-convex procedure tackle space constraints refer interested readers supplementary material proof details. introduce notion induced trees spns show every complete decomposable interpreted mixture induced trees induced tree corresponds product univariate distributions. perspective understood huge mixture model effective number components mixture determined network structure. method describe ﬁrst method interpreting mixture distribution method result exponentially smaller mixture section details. remark. although focus attention boolean random variables simplicity discussion illustration thm. extended case univariate distributions leaf nodes continuous discrete distributions countably inﬁnitely many values e.g. gaussian distributions poisson distributions. simply replace product univariate univariate distribution also note possible unique induced trees share guaranteed different. shortly thm. implies joint distribution represented essentially mixture model potentially exponentially many {xn}n components mixture. remark. four theorems prove fact ensemble mixture trees tree computes unnormalized distribution total number unique trees network cardinality depends structure component simple product univariate distributions. illustrate theorems simple example fig. figure complete decomposable mixture induced trees. double circles indicate univariate distributions different colors used highlight unique induced trees; induced tree product univariate distributions zhao show every complete decomposable equivalent bipartite bayesian network layer hidden variables layer observable random variables. number hidden variables bipartite bayesian network equal number nodes naive expansion bayesian network mixture model lead huge mixture model components number nodes complement theory show complete decomposable essentially mixture trees effective number unique induced trees given note depends network structure often much smaller without loss generality assuming layers nodes alternating layers product nodes height however exponentially many trees recursively merged combined overall network size still tractable. let’s consider likelihood function computed binary random variables model parameters input vector model parameters edge weights every node collect together long vector corresponds number edges emanating nodes deﬁnition probability distribution induced computed corollary weights input vector iwd∈tt indicator variable whether t-th induced tree not. monomial corresponds exactly unique induced tree statement direct corollary thm. thm. thm. deﬁnition network polynomial know multilinear function indicator variables. corollary works complement characterize functional form network polynomial terms follows likelihood function expressed ratio posynomial functions. show optimization problem based using deﬁnition corollary problem rewritten nonconvex general essentially hard solve computational perspective however despite hardness general objective function formulation spns special structure i.e. ratio posynomials makes design efﬁcient optimization algorithms possible. ﬁrst-order methods viewed approximating applying logarithmic transformation objective function only. although signomial program objective function expressed ratio posynomials. hence still apply logarithmic transformation trick used geometric programming objective function variables optimized. concretely exp∀d take objective function; becomes equivalent maximize following objective without constraint transforming log-space naturally guarantee positivity solution iteration hence transforming constrained optimization problem unconstrained optimization problem without sacriﬁce. terms convex functions transformation. hence transformed objective function expressed difference convex functions called function helps design efﬁcient algorithms solve problem based general idea sequential convex approximations nonlinear programming. let’s consider linearization terms order apply ﬁrst-order methods transformed space. compute gradient respect different components view node intermediate function network polynomial apply chain rule back-propagate gradient. differentiation respect root node network differentiation network polynomial respect partial function node computed passes network bottom-up pass evaluates values partial functions given current input top-down pass differentiates network polynomial respect partial function. following evaluation-differentiation passes gradient objective function computed furthermore although computation conducted results fully expressed terms suggests practice need explicitly construct follows approximating best linear function equivalent using best monomial approximation signomial program leads sequential monomial approximations original formulation iteration linearize terms form optimal monomial function terms additive update leads multiplicative update since exp) backtracking line search determine step size update iteration. sequential monomial approximation fails structure problem learning spns. propose another approach based concave-convex procedure fact objective function expressed difference convex functions. high level cccp solves sequence concave surrogate optimizations convergence. many cases maximum concave surrogate function solved using convex solvers result efﬁciency cccp highly depends choice convex solvers. however show suitable transformation network compute maximum concave surrogate closed form time linear network size leads efﬁcient algorithm learning parameters spns. also prove convergence properties algorithm. consider objective function maximized convex function concave function. linearize convex part concave function convexity ∇yft result following properties always hold cccp updates iteration solving unless already maxy maxy case generalized ﬁxed point found algorithm stops. easy show iteration cccp always note also computing log-likelihood input therefore bounded monotone convergence theorem limk→∞ exists sequence converges. discuss compute closed form solution maximization concave surrogate since differentiable concave ﬁxed sufﬁcient necessary condition maximum leads system nonlinear equations hard solve closed form. however change variable considering locally normalized weights solution easily computed. described transformed equivalent normal locally normalized weights bottom pass follows note derivation fvi/fs /∂fvi treated constants hence absorbed since ij∀j constrained locally normalized. order obtain solution edge weight sufﬁcient statistics include three terms evaluation value differentiation value previous edge weight obtained passes network input thus computational complexity obtain maximum concave surrogate interestingly leads update formula algorithm despite fact cccp start different perspectives. show limit points sequence guaranteed stationary points theorem limiting points limk→∞ stationary point summarize four algorithms highlight connections differences table although mainly discuss batch version algorithms four algorithms easily adapted work stochastic and/or parallel settings. conduct experiments benchmark data sets various domains compare evaluate convergence performance four algorithms cccp data sets widely used assess different spns task density estimation. features data sets binary features. spns used comparisons cccp trained using learnspn discard weights returned learnspn random weights initial model parameters. random weights determined random seed four algorithms. detailed information datasets spns used experiments provided supplementary material. implement four algorithms c++. algorithm maximum number iterations absolute difference training log-likelihood consecutive steps less algorithms stopped. combine backtracking line search weight shrinking coefﬁcient learning rates initialized three methods. projection margin learning rate backtracking line search cccp. smoothing parameter cccp avoid numerical issues. show fig. average log-likelihood scores training data sets evaluate convergence speed stability cccp. clearly cccp wins large margin convergence speed solution quality. furthermore among four algorithms cccp stable guarantee log-likelihood decrease iteration. shown fig. training curves cccp smooth three methods almost cases. experiments also clearly show cccp often converges iterations. hand since ﬁrst-order methods. stable often achieves better solutions large data sets also converges faster surprisingly performs worse cases quite unstable despite fact admits multiplicative updates. hook shape curves data sets e.g. kosarak projection operations. table average log-likelihoods test data. highest log-likelihoods highlighted bold. shows statistically better log-likelihoods cccp shows statistically worse log-likelihoods cccp. signiﬁcance measured based wilcoxon signed-rank test. data nltcs msnbc plants audio jester netﬂix accidents retail pumsb-star computational complexity update four algorithms. constant involved term cccp slightly larger three algorithms calls cccp. however practice cccp often takes less time three algorithms takes fewer iterations converge. list detailed running time statistics four algorithms data sets supplementary material. combine cccp tuning procedure structure learning algorithm learnspn compare state-of-the-art structure learning algorithm id-spn concretely keep model parameters learned learnspn initialize cccp. update model parameters globally using cccp tuning technique. normally helps obtain better generative model since original parameters learned greedily locally structure learning algorithm. validation log-likelihood score avoid overﬁtting. algorithm returns parameters achieve best validation log-likelihood score output. learnspn id-spn publicly available implementations provided original authors default hyperparameter settings. experimental results reported table. shown table cccp learnspn always helps improve model performance. optimizing model parameters data sets boost learnspn achieve better results state-of-the-art id-spn data sets original learnspn outperforms id-spn data set. note sizes spns returned learnspn much smaller produced id-spn. hence remarkable tuning parameters cccp achieve better performance despite fact models smaller. fair comparison also list size spns returned id-spn supplementary material. show network polynomial posynomial function model parameters learning parameter maximum likelihood yields signomial program. propose convex relaxations solve analyze convergence properties cccp learning spns. extensive experiments conducted evaluate proposed approaches current methods. also recommend combining cccp current structure learning algorithms boost modeling accuracy. proof. argue contradiction tree must exist node parent means exist least paths connect root denote last node common preﬁx paths. construction know must exist since paths start root node also claim otherwise paths overlap other contradicts assumption multiple parents. shows paths represented common preﬁx shared paths since last common node. construction process deﬁned def. know children recall node def. takes child hence claim must product node since children paths indicate scope scope scope scope scope scope leading scope scope scope contradiction decomposability product node hence long complete decomposable must tree. completeness trivially satisﬁed node child also straightforward verify satisﬁes decomposability induced subgraph decomposable. proof. first scope scope root also root shows least indicator leaves otherwise scope root node strict subset scope root node furthermore variable indicator leaves. observed fact child collected node i¯xi appear simultaneously ancestor i¯xi guaranteed exist tree structure however leads contradiction fact decomposable. result exactly indicator product univariate distributions. speciﬁcally product indicator variables case boolean input variables. already shown tree product nodes multiple children. follows functional form must monomial edge weights contribute monomial. combing above know root node children sub-spn unique induced trees def. total number unique induced trees root product node children total number unique induced trees easy check objective function constraint function signomials. equivalence optimal value achieved choose also optimal solution otherwise feasible combined constraint function contradicts optimality direction solution achieves optimal value claim also optimal value otherwise exists feasible since also feasible contradicts optimality transformation make problem easier solve. rather destroys structure i.e. objective function ratio posynomials. however equivalent transformation reveal insights intrinsic complexity optimization problem indicates hard solve efﬁciently guarantee achieving globally optimal solution. discussed sequence function values converges limiting point. however fact alone necessarily indicate converges stationary point imply sequence converges zangwill’s global convergence theory successfully applied study convergence properties many iterative algorithms frequently used machine learning including generalized alternating minimization also cccp also apply zangwill’s theory combine analysis show following theorem proof. zangwill’s global convergence theory iterative algorithms show convergence case. showing proof need ﬁrst introduce notion point-toset mapping output mapping deﬁned set. formally point-to-set deﬁned power suppose equipped norm respectively. point-to-set said imply closed point-to-set said closed closed every point concept closedness point-to-set setting reduces continuity restrict output singleton every possible input i.e. point-to-point mapping. furthermore assume obtained local normalization update guarantee {wk} solution maxy unique. fact inﬁnitely many solutions nonlinear equations. however deﬁne above returns solution convex program dimensional hyper cube. hence case reduces point-to-point deﬁnition closedness point-to-set reduces notion continuity point-to-point map. deﬁne stationary point hence need verify continuity show this ﬁrst characterize functional form ∂fvi used inside claim node ∂fvi again posynomial function graphical illustration given fig. explain process. also derived rules product rules used top-down differentiation. speciﬁcally product node parents network assumed ∂fvj nodes differentiation respect given ∂fvi reach figure graphical illustration ∂fvi partial derivative respect posynomial product edge weights lying path root network polynomials nodes children product nodes path shown numerator denominator posynomial functions posynomial functions continuous functions order show also continuous need guarantee denominator degenerate posynomial function i.e. denominator possible input vector recall stationary point hence boundary dimensional hyper cube hence component. immediately leads result continuous since ratio strictly positive posynomial functions. verify third property zangwill’s global convergence theory. iteration cccp following cases consider zangwill’s global convergence theory conclude limit points {wk} converges monotonically stationary point remark technically need choose ensure continuity initial positive multiplicative update renormalization ensure ﬁnite steps ints. theoretically limit possible components become however practice numerical precision ﬂoat numbers computers possible ﬁnite update steps components become practical implementation recommend small positive number smooth components iterations cccp. smoothing hurt monotonic property cccp happens close early stopping obtain solution interior remark thm. implies limiting point sequence {wk} must must converge stationary point log-likelihood function stationary point. thm. imply sequence {wk} guaranteed converge. studies convergence property general cccp procedure. strong conditions i.e. strict concavity surrogate function contraction mapping possible show sequence {wk} also converges. however none conditions hold case. fact general inﬁnitely many ﬁxed points i.e. equation inﬁnitely many solutions also ﬁxed value least solution inﬁnitely many solutions. properties spns make generally hard guarantee convergence sequence {wk} give simple example illustrate hardness spns fig. consider applying cccp procedure learn parameters given fig. three instances choose initial parameter weights indicator variables shown fig. assignment probability simplex equally optimal terms likelihood inputs. example uncountably inﬁnite equal solutions invalidates ﬁnite solution requirement given order show convergence important {wk} convergence {αk} desired locations log-likelihood surface practice equally good log-likelihood sufﬁce inference/prediction task. worth point theorem imply convergence sequence i.e. limits stationary points also present negative subsequences example fig. invalidates application zangwill’s global convergence theory analysis case. convergence rate general cccp still open problem studied convergence rate unconstrained bound optimization algorithms differentiable objective functions problem special case. conclusion depending curvature cccp exhibit either quasi-newton behavior superlinear convergence ﬁrst-order convergence. show experiments cccp normally exhibits fast superlinear convergence rate compared sma. cccp special cases general framework known majorization-maximization. show case spns algorithms coincide other i.e. lead update formulas despite fact start totally different perspectives. brieﬂy review current approach training spns using projected gradient descent another related approach exponentiated gradient optimize optimizes log-likelihood projecting intermediate solution back positive orthant gradient update. since constraint open need manually create closed projection operation well deﬁned. feasible choice project \u0001∀d} assumed small. avoid projection direct solution exponentiated gradient method ﬁrst applied online setting latter successfully extended batch settings training convex models. admits multiplicative update iteration hence avoids need projection pgd. however mostly applied convex setting clear whether convergence guarantee still holds nonconvex setting. table statistics data sets models. number variables modeled network size network number parameters estimated network. means ratio training instances times number variables number parameters.", "year": 2016}