{"title": "Adaptive Regularization for Weight Matrices", "tag": ["cs.LG", "cs.AI"], "abstract": "Algorithms for learning distributions over weight-vectors, such as AROW were recently shown empirically to achieve state-of-the-art performance at various problems, with strong theoretical guaranties. Extending these algorithms to matrix models pose challenges since the number of free parameters in the covariance of the distribution scales as $n^4$ with the dimension $n$ of the matrix, and $n$ tends to be large in real applications. We describe, analyze and experiment with two new algorithms for learning distribution of matrix models. Our first algorithm maintains a diagonal covariance over the parameters and can handle large covariance matrices. The second algorithm factors the covariance to capture inter-features correlation while keeping the number of parameters linear in the size of the original matrix. We analyze both algorithms in the mistake bound model and show a superior precision performance of our approach over other algorithms in two tasks: retrieving similar images, and ranking similar documents. The factored algorithm is shown to attain faster convergence rate.", "text": "abstract learning distributions algorithms weight-vectors arow recently shown empirically achieve state-of-the-art performance various problems strong theoretical guaranties. extending algorithms matrix models pose challenges since number free parameters covariance distribution scales dimension matrix tends large real applications. describe analyze experiment algorithms learning distribution matrix models. ﬁrst algorithm maintains diagonal covariance parameters handle large covariance matrices. second algorithm factors covariance capture inter-features correlation keeping number parameters linear size original matrix. analyze algorithms mistake bound model show superior precision performance approach algorithms tasks retrieving similar images ranking similar documents. factored algorithm shown attain faster convergence rate. many machine learning tasks involve models form matrix. important example consider problem linear metric learning dissimilarity between pair samples measured using mahalanobis distance parametrized positive semi-deﬁnite matrix. second important example matrix model obtained learning multiple linear classiﬁers regularized jointly like case object recognition many classes. linear matrix models common metric multiclass learning broader class vector linear model popular choice many domains since provide good balance simplicity scalability performance. methods generate linear classiﬁers data ﬂourished past decade including svmimportantly learning linear models recently shown modeling second order information models references therein) using information training improves convergence rate learning algorithms well performance resulting classiﬁers. effective methods developed primarily handling vector models designed handle matrix models. ﬁrst sight problems involve learning matrices could handled directly using methods developed learning vectors including second order methods described above. practice however matrix models often pose challenge scalability since memory runtime complexity scale quadratically data dimensionality modeling second order interactions between features therefore require parameters limiting methods relatively dimensional data. paper study second-order methods learning matrix models test context similarity learning. describe aroma online algorithm learns distribution matrix models. since maintaining full covariance matrix parameters would feasible large dimensions describe models capture part covariance structure. ﬁrst describe simple model diagonal covariance matrix. model scales well large matrices fails model correlations features could crucial applications. context metric similarity learning aroma used learn distribution metrics instead single metric. evaluate aroma tasks retrieving images documents evaluating similarity objects. aroma variants outperform competing methods large gap. additionally involved variant convergence faster methods evaluated. know makes state-of-the-art method extensively studied task linear similarity learning. notation work often consider bilinear form rm×n. given matrix denote vector generated stacking columns matrix using operator write bilinear form note element-wise product vectors elements matrix vector denote number non-zero elements vector known norm. given square matrices rm×m rn×n denote kronecker product matrix size composed blocks block λijω. finally refers equation longer version manuscript provided online. focus problem learning linear similarity measure pairs objects form similarity measure related metric learning models form square matrices becomes equivalent vectors constant -norm. interestingly similarity measure symmetric even deﬁned objects different dimensions general allows learn measure relatedness objects different domains like images sounds images text importantly vectors representing query object sparse contain elements computing similarity score takes kqkp operations instead dense vectors. triplets objects triplet containing query object candidate objects known object related query object importantly relative similarity learning setup assume exists absolute numerical level similarity object query learner access training therefore assumes weaker type supervision making easier collect labeled data either human raters collecting indirect data association object pairs. example pages ranked similarity third page number users visiting within session. formally goal learn bi-linear similarity scoring function parametrized rm×n total ordering induced similarity function objects would consistent partial ordering information given query similar model recently studied different contexts speciﬁcally develop online algorithm allows rank objects similarity query object like online prediction algorithms online retrieval algorithms work rounds. round algorithm receives triplet composed query possible out∈ algorithm outputs single comes indicating outcome better given query. receives correct answer updates model. follows describe online algorithms minimize loss modeling distribution matrix models ﬁrst review previous work learning distributions vector models. models. speciﬁcally arow maintains gaussian distribution vectors denoted rd×d. mean encodes knowledge algorithm weight features used make predictions. covariance captures notion conﬁdence weights used training effective learning rate features different statistics. arow motivated tasks natural language processing many features rare features frequent. arow online algorithm works rounds. i-th round algorithm receives input employs current model make prediction {±}. receives true label suffers loss finally algorithm updates prediction rule using pair proceeds next round. naive approach model uncertainty matrices would linearity ranking function write inner product vectors dimension viewed simply learning vectors dimension transforming matrix model vector original arow algorithm vectors applied. unfortunately approach requires maintain mean parameters vector size full covariance matrix size even moderate dimension values size full covariance matrix cannot stored memory. instance dimension vectorized model full covariance matrix requires parameters. designing second order algorithms matrices thus requires model covariance compact way. discuss develop compact representations learning algorithms diagonal covariance factorized covariance. arow shown attain state-of-the-art performance many problems performance analyzed full covariance matrices diagonal covariance matrices next section entire paper lift arow matrices maintaining memory speed efﬁciency. online classiﬁcation learning online retrieval algorithms work rounds. round algorithm receives triplet composed query possible out∈ algorithm outputs single comes indicating outcome better given query. receives correct answer updates model. simplicity assume ﬁrst outcome always preferable namely given algorithm rank denote rm×n covariance matrix maintains element feature thus diagonal-like although rectangular shape. identify update notation used matrix-similarity measures cases expect second term ﬁrst square-root term small values function close zero. unlike vector-variant analysis required input features sparse. instead require inputs query sparse inputs difference objects sparse necessarily both. second approach model distribution similarity matrices based factorizing covariance matrix captures separately correlations input output describe second algorithm deﬁnition matrix-variate normal distribution deﬁnition random matrix rm×n said matrix variate normal distribution mean matrix rm×n covariance matrix rm×m rn×n symmetric matrix variate normal distributions denoted derive algorithm revisiting objective arow compute three terms objective model. ﬁrst term obtain divergence matrix-variate normal distributions call algorithm d-aroma diagonaladaptive regularization matrix models summarized fig. memory required d-aroma space needed store time complexity operations involve componentwise operations proceeding describe next algorithm state mistake bound d-aroma. rounds algorithm made prediction mistake example indices algorithm made update mistake occurred. then proof omitted lack space similar spirit analysis section orabona crammer their analysis expect bound small either combination feature query feature output differil small) combination useful prediction small. feature combinations fall understand theorem matrices thought second order moments objects queries respectively. observe matrices identity matrix weighted outer products objects queries. ﬁrst term bound detailed derivation update steps given long version yields second algorithm named faroma summarized fig. using woodbury identity follows psd. worth comparing update fig. update arow updates share formal structure different constants. arow uses parameter denominator fλi−qi. assuming aroma uses mr/q furthermore lower value λi−qi λi−qi larger value effective parameter mr/q λi−qi turn reduces effect update. extreme case λi−qi ωi−. intuitively algorithm decrease total variance examples observed. variance already variance related query λi−qi need reduce variance related output vice versa. following symmetry observations also hold update f-aroma uses total memory store mean matrix covariance matrices time complexity also since involves addition elements matrices. note d-aroma f-aroma asymptotic complexity later requires storage manipulation matrix. dimensions differ signiﬁcantly complexity f-aroma larger d-aroma f-aroma scales quadratically d-aroma scales linearly either parameters. conclude section mistake bound similar theorem analysis applies algorithm fig. minor changes. first assumes mistake driven version algorithm namely algorithm makes update mistake occurs. condition update therefore wi−pi instead wi−pi. second evaluated diagonal factored aroma data sets. first learned semantic similarity pairs images caltech- dataset second learned similarity measure pairs text documents using -newsgroups data collected lang tasks used standard -fold cross validation report precision test set. ﬁrst tested aroma image similarity task using caltech dataset. dataset consists images obtained google image search picsearch.com. images assigned categories evaluated humans order ensure image quality relevance. allow direct comparisons previous literature used classes. represent image using sparse code based patch descriptors. speciﬁcally features extracted dividing image overlapping square patches describing patch edge color histograms. edge histograms used uniform local binary patterns estimate texture histogram patch considering differences intensity circular neighborhoods centered pixel. used uniform patterns means circle radius considered centered block bins corresponding uniform sequences merged. patterns concatenated color histograms. form sparse code patch descriptors mapped codewords using dictionary trained large images using k-means. then patch representations collected represent image sparse code. local descriptor represented discrete index called visterm image represented bag-ofvisterms vector components related presence absence visterm assignment weight visterm image according tfidf weights. approach found successful grangier bengio chechik used -sized codebook median non-zero values image maximum aligned eigenvectors associated small values columns aligned eigenvectors necessarily both. property holds input space arow also second order perceptron. faroma property holds subspaces queries objects. next second term bound small either matrices skewed. function concave. similar property holds also theorem required features either spaces would sparse non-informative. property required hold spaces both. figure experiments image similarity using caltech. left precision function images. aroma iterations middle precision images traced training. right sensitivity aroma regularizer average precision iterations. class labels oasis online similarity model based ranking cost across triplets similar setup studied used estimate added beneﬁt using covariance distribution addition mean aroma does. itml/lego online approach succeeds maintain proper metric learning efﬁcient lmnn large margin nearest neighbor early large margin metric learning methods euclidean distance equivalent using identity matrix left panel fig. compares precision obtained d-aroma f-aroma competing methods. diagonal factorized aroma perform similarly slightly higher performance factored aroma. methods signiﬁcantly better methods head ranked images. ranked image aroma improves precision second best approach middle panel fig. traces precision test training showing convergence achieved iterations. beginning daroma slightly better f-aroma later faroma converged faster. right panel fig. demonstrates aroma largely robust choice regularizer less change precision across three orders magnitude second experiments studied problem learning similarity measure pairs text documents. task numerous applications ﬁnding content related given text document. dataset documents divided classes documents class. documents considered similar share class labels. lected terms conveyed high information identity class using infogain criterion selected features normalized using tf-idf represented document words. newsgroups website proposes split data train test sets. repeated splitting times based sizes proposed splits evaluated learned similarity measures using ranking criterion. view every document test query rank remaining test documents similarity scores computed precision ranked documents. computed mean average precision widely used measure information retrieval community averages different values left panel fig. shows precision ranked similar document. clearly aroma methods outperform itml oasis large. middle panel fig. traces precision progresses learning iterations. f-aroma achieves higher precision diagonal aroma learning iterations fact converges faster. d-aroma reaches level iterations. interestingly aroma learns much faster oasis takes oasis times steps precision precision gain preserved across large regime values shown right panel fig. figure newsgroups. left precision aroma compared oasis itml aroma results obtained iterations. oasis results obtained iterations robust choice middle precision images function training iterations total iterations. right sensitivity aroma regularizer average precision images iterations. tasks model covariance matrix distribution using linear number parameters. diagonal-aroma likely superior variance individual features large relative feature dependencies factoredaroma expected superior data strong correlations across features caltech data. factored-aroma also converged faster. ojala pietikainen maenpaa multiresolution gray-scale rotation invariant texture classiﬁcaieee tran. pattern tion local binary patterns. analysis mach. intelligence", "year": 2012}