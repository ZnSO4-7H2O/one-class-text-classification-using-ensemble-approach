{"title": "Gaussian Process Networks", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "In this paper we address the problem of learning the structure of a Bayesian network in domains with continuous variables. This task requires a procedure for comparing different candidate structures. In the Bayesian framework, this is done by evaluating the {em marginal likelihood/} of the data given a candidate structure. This term can be computed in closed-form for standard parametric families (e.g., Gaussians), and can be approximated, at some computational cost, for some semi-parametric families (e.g., mixtures of Gaussians).  We present a new family of continuous variable probabilistic networks that are based on {em Gaussian Process/} priors. These priors are semi-parametric in nature and can learn almost arbitrary noisy functional relations. Using these priors, we can directly compute marginal likelihoods for structure learning. The resulting method can discover a wide range of functional dependencies in multivariate data. We develop the Bayesian score of Gaussian Process Networks and describe how to learn them from data. We present empirical results on artificial data as well as on real-life domains with non-linear dependencies.", "text": "paper address structure continuous procedure structures. done data given candidate computed closed-form families imated computational semi-parametric sians). present probabilistic sian process priors. priors semi­ parametric nature learn almost noisy functional relations. using bitrary priors directly lihoods structure method discover dependencies bayesian works describe present well real-life domains pendencies. part motivation applies structure ology problems transcription transcribes sequence cent technical biologists genes experiment ated experiments thousands help understand various aspects pression levels biological several particular quantitative might think gene either \"suppressed\" modes. experience ever shows discretization information rectly represent variables. decomposition structure. move efficiently adding removing dure greedy hill-climbing performs local change results reaches dure necessarily perform well practice. procedures search learning mixture models however lems. exact computation family cannot done closed form. instead resort approximations tion this requires parameters tion problem space many local maxima. thus practice running time learn mixture components. kernel methods performance parameter. over-smoothed. sure data over-fit usually cross-validation testing. mann tresp leave-one-out dure. addition score different network setting. hofmann tresp suggest cross-validated family. loss family incur data. summarize family parameters tion estimate score family. approximation reasonable hyperparameters sharply maximum. situations termined better approximation posterior gaussian however requires posterior probability therefore score family given perform conjugate gradi­ ascent search parameters. eval­ uation point search requires compute determinant thus computational naive implementations. peated iteration step. practice figure family scores plots shown dependencies. different network models no-dependency network \"true\" network \"opposite direction\" network. linear score family noise added. case noise units standard deviations network models data set. kernel method assigns variable learns distribution \"delta\" kernel method bounded learn distributions sharp. another option treat varicovariance lengthscales tion hardly changes directions. tion future space learned lengthscales ficient procedure. using gaussian process putation costly. currently method analyze learning dbns influences would able understand controls generating might learn depends would give clue effects presence", "year": 2013}