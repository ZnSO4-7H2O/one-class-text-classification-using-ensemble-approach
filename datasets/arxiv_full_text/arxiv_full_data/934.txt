{"title": "Massively Multitask Networks for Drug Discovery", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "Massively multitask neural architectures provide a learning framework for drug discovery that synthesizes information from many distinct biological sources. To train these architectures at scale, we gather large amounts of data from public sources to create a dataset of nearly 40 million measurements across more than 200 biological targets. We investigate several aspects of the multitask framework by performing a series of empirical studies and obtain some interesting results: (1) massively multitask networks obtain predictive accuracies significantly better than single-task methods, (2) the predictive power of multitask networks improves as additional tasks and data are added, (3) the total amount of data and the total number of tasks both contribute significantly to multitask improvement, and (4) multitask networks afford limited transferability to tasks not in the training set. Our results underscore the need for greater data sharing and further algorithmic innovation to accelerate the drug discovery process.", "text": "massively multitask neural architectures provide learning framework drug discovery synthesizes information many distinct biological sources. train architectures scale gather large amounts data public sources create dataset nearly million measurements across biological targets. investigate several aspects multitask framework performing series empirical studies obtain interesting results massively multitask networks obtain predictive accuracies signiﬁcantly better single-task methods predictive power multitask networks improves additional tasks data added total amount data total number tasks contribute signiﬁcantly multitask improvement multitask networks afford limited transferability tasks training set. results underscore need greater data sharing algorithmic innovation accelerate drug discovery process. discovering treatments human diseases immensely complicated challenge. prospective drugs must attack source illness must satisfying restrictive metabolic toxicity constraints. traditionally drug discovery extended process takes years move start ﬁnish high rates failure along way. suitable target identiﬁed ﬁrst step drug discovery process ﬁnding. given druggable target pharmaceutical companies screen millions drug-like compounds effort attractive molecules optimization. screens often automated robots expensive perform. virtual screening attempts replace augment high-throughput screening process computational methods machine learning methods frequently applied virtual screening training supervised classiﬁers predict interactions targets small molecules. variety challenges must overcome achieve effective virtual screening. rates experimental screens result imbalanced datasets require special handling effective learning. instance care must taken guard unrealistic divisions active inactive compounds information leakage strong similarity active compounds furthermore paucity experimental data means overﬁtting perennial thorn. overall complexity virtual screening problem limited impact machine learning drug discovery. achieve greater predictive power learning algorithms must combine disparate sources experimental data across multiple targets. deep learning provides ﬂexible paradigm synthesizing large amounts data predictive models. particular multitask networks facilitate information sharing across different experiments compensate limited data associated particular experiment. million experimental measurements targets. demonstrate multitask networks trained collection achieve signiﬁcant improvements baseline machine learning methods. show adding tasks data yields better performance. effect diminishes data tasks added appear plateau within collection. interestingly total amount data total number tasks signiﬁcant roles improvement. furthermore features extracted multitask networks demonstrate transferability tasks contained training set. finally presence shared active compounds moderately correlated multitask improvement biological class target not. machine learning rich history drug discovery. early work combined creative featurizations molecules off-the-shelf learning algorithms predict drug activity state moved reﬁned models inﬂuence relevance voting method combines low-complexity neural networks k-nearest neighbors bayesian belief networks repurpose textual information retrieval methods virtual screening related work uses deep recursive neural networks predict aqueous solubility extracting features connectivity graphs small molecules deep learning made inroads drug discovery recent years notably merck kaggle competition teams given pre-computed molecular descriptors compounds experimentally measured activity targets asked predict activity molecules held-out test set. winning team used ensemble models including multitask deep neural networks gaussian process regression dropout improve baseline test nearly winners contest later released technical report discusses multitask networks virtual screening additional work merck analyzed choice hyperparameters training singlemultitask networks showed improvement random forest models merck kaggle result received skepticism cheminformatics drug discovery communities major concerns raised sample size small gains predictive accuracy small justify increase complexity. preparing work workshop paper released also used massively multitask networks virtual screening work curated dataset biological targets million associated data points trained multitask network. network tasks fewer data points emphasis work considerably different; report highlights performance gains multitask networks focused disentangling underlying causes improvements. another closely related work proposed collaborative ﬁltering virtual screening employed multitask networks kernel-based methods multitask networks however consistently outperform singletask models. within greater context deep learning draw upon various strands recent thought. prior work used multitask deep networks contexts language understanding multilanguage speech recognition best-performing networks draw upon design patterns introduced googlenet winner ilsvrc models trained datasets gathered publicly available data. datasets divided four groups pcba dud-e tox. pcba group contained experiments pubchem bioassay database group contained challenging datasets speciﬁcally designed avoid common pitfalls virtual screening dud-e group contained datasets designed evaluation methods predict interactions proteins small molecules datasets used recent data challenge contained experimental data targets relevant drug toxicity prediction. used training data challenge test released constructed collection. total datasets contained experimental data points compounds. details dataset groups given table appendix details individual datasets biological target categorization. community used metrics derived receiver operating characteristic curve evaluate model performance. recall curve binary classiﬁer plot true positive rate false positive rate discrimination threshold varied. individual datasets interested area curve global measure classiﬁcation performance generally collection datasets consider mean median k-fold-average auck deﬁned classiﬁer trained folds dataset tested fold completeness include appendix alternative metric called enrichment widely used cheminformatics literature note many performance metrics exist literature; lack standard metrics makes difﬁcult direct comparisons previous work. neural network nonlinear classiﬁer performs repeated linear nonlinear transformations input. represent input i-th layer network transformation performed respectively weight matrix bias i-th layer nonlinearity transformations ﬁnal layer network simple linear classiﬁer softmax predicts probability input label number possible labels w··· weight vectors. learned training backpropagation algorithm multitask network attaches softmax classiﬁers task ﬁnal layer pounds whose physical properties cause interference experimental measurements allow promiscuous interactions many targets. notable exception group processed consideration pathologies used extended connectivity ﬁngerprints generated rdkit featurize molecule. molecule decomposed fragments—each centered non-hydrogen atom—where fragment extends radially along bonds neighboring atoms. fragment assigned unique identiﬁer collection identiﬁers molecule hashed ﬁxed-length vector construct molecular ﬁngerprint. ecfp ﬁngerprints commonly used cheminformatics applications especially measure similarity compounds number molecules failed featurization process used training networks. appendix details. traditional approach model evaluation ﬁxed training validation test sets. however imbalance present datasets means performance varies widely depending particular training/test split. compensate variability used stratiﬁed kfold cross-validation; fold maintains active/inactive proportion present unsplit data. remainder paper note choose explicit validation set. several datasets collection actives feared selecting speciﬁc validation would skew results. consequence suspect choice hyperparameters affected information leakage across folds. however networks appear highly sensitive hyperparameter choice consider leakage serious issue. networks overﬁtting data. discussed section datasets small fraction positive examples. single hidden layer multitask network table dataset associated parameters. total number positives tens hundreds overﬁtting number parameters major issue absence strong regularization. reducing number parameters speciﬁc dataset motivation pyramidal architecture. pyramidal networks ﬁrst hidden layer wide second narrow hidden layer dimensionality reduction similar motivation implementation convolutions googlenet architecture wide lower layer allows complex expressive features learned narrow layer limits parameters speciﬁc task. adding dropout pyramidal networks improved performance. also trained singletask versions best pyramidal network understand whether design pattern works well less data. table indicates models outperform vanilla singletask networks substitute multitask training. results variety alternate models presented appendix. investigated sensitivity results sizes pyramidal layers running networks combinations hidden layer sizes across architectures means medians shifted showing larger changes range note performance sensitive choice learning rate number training steps. appendix details data. previous section demonstrated massively multitask networks improve performance single-task models. section seek understand multitask performance affected increasing number tasks. priori three reasonable growth curves constructed trained series multitask networks datasets containing tasks. datasets contain ﬁxed held-in tasks consists randomly sampled collection investigate performance multitask networks various hyperparameters compare several standard machine learning approaches. table shows highlights experiments. best multitask architecture signiﬁcantly outperformed simpler models including hypothetical model whose performance dataset matches best single-task model every model trained performed extremely well dud-e datasets making comparisons models dud-e uninformative. reason exclude dud-e subsequent statistical analysis. however remove dud-e training altogether adversely affected performance datasets theorize dude helped regularize classiﬁer avoid overﬁtting. table median -fold-average aucs various models. model sign test last column estimates fraction datasets model superior pmtnn wilson score interval derive conﬁdence interval fraction. non-neural network methods trained using scikit-learn implementations basic hyperparameter optimization. also include results hypothetical best single-task model provide stronger baseline. details cross-validation training procedures given appendix. logistic regression random forest single-task neural pyramidal stnn max{lr stnn pstnn} -hidden layer multitask neural pyramidal multitask neural pcba three datasets. datasets correspond unique targets obvious analogs remaining collection. training collection superset preceding collection tasks added randomly. network series computed mean -fold-averageauc tasks held-in collection. repeated experiment times different choices random seed. figure plots results experiments. shaded region emphasizes average growth curve black dots indicate average results different experimental runs. ﬁgure also displays lines associated held-in dataset. note several datasets show initial dips performance. however datasets show subsequent improvement achieves performance superior single-task baseline. within limits current dataset collection distribution figure agrees either plateau still climbing. mean performance held-in still increasing tasks hypothfigure held-in growth curves. y-axis shows change compared single-task neural network architecture colored curve multitask improvement given held-in dataset. black dots represent means across held-in datasets experimental additional tasks randomly selected. shaded curve mean across combinations datasets experimental runs. previous section studied effects adding tasks investigate relative importance total amount data total number tasks. namely better many tasks small amount associated data small number tasks large amount associated data? constructed series multitask networks tasks. previous section tasks randomly associated networks cumulative manner networks contained held-in tasks described previous section. tasks chosen associated largest datasets collection containing data points. note tasks belonged pcba group. trained series networks multiple times data points sampled non-held-in tasks. perform sampling given task data points present ﬁrst stage appeared second data points present second stage appeared third decided larger datasets could sample meaningfully across entire range. combinations tasks data points realized; instance enough data train -task network additional data points. repeated experiment times using different random seeds. figure shows results experiments. x-axis tracks number additional tasks y-axis displays improvement performance held-in relative multitask network trained held-in data. total amount data ﬁxed tasks consistently yields improvement. similarly number tasks ﬁxed adding additional data consistently improves performance. results suggest total amount data total number tasks contribute signiﬁcantly multitask effect. features extracted layer network represent information useful many tasks. consequently sought determine transferability features tasks training set. held data sets growth curves calculated section used learned weights points along growth curves initialize single-task networks held-out datasets ﬁne-tuned. results training networks shown figure first note many datasets performed worse baseline initialized -held-in-task networks. further datasets never exhibited positive effect multitask initialization. transfer learning negative. figure multitask beneﬁt increasing tasks data independently. figure added randomly selected tasks ﬁxed held-in set. stratiﬁed random sampling scheme applied additional tasks order achieve ﬁxed total numbers additional input examples white points indicate mean experimental runs meanauc initial network trained held-in datasets. color-ﬁlled areas error bars describe smoothed conﬁdence intervals. stronger multitask networks trained data. large multitask networks exhibited better transferability average effect even datasets auc. hypothesize extent generalizability determined presence absence relevant data multitask training set. biological context datasets implies active compounds contain information inactive compounds; inactive compound inactive many reasons active compounds often rely similar physical mechanisms. hence shared active compounds good measure dataset similarity. moderate correlation logodds-mean-auc note correlation present mean-auc y-coordinate hypothesize portion multitask effect determined shared active compounds. dataset likely beneﬁt multitask training shares many active compounds datasets collection. figure multitask improvement compared active occurrence rate point ﬁgure represents particular dataset x-coordinate mean across active compounds y-coordinate difference log-oddsmean-auc multitask single-task models. gray bars indicate standard deviations around means. moderate correlation reasons discussed section excluded dud-e analysis. figure shows relationship multitask improvement target classes. before report multitask improvement terms log-odds exclude dud-e datasets. qualitatively target class beneﬁted multitask training. nearly every target class realized gains suggesting multitask framework applicable experimental data multiple target classes. mentioned section many cases tasks identical targets. compared multitask improvement duplicate unique tasks. distributions substantial overlap average logodds improvement slightly higher duplicated tasks since duplicated targets likely share many active compounds improvement consistent correlation seen secfigure held-out growth curves. y-axis shows change compared single-task neural network architecture colored curve result initializing single-task neural network weights networks section computing mean across experimental runs. datasets included training original networks. shaded curve mean across combinations datasets experimental runs black dots represent means across held-out datasets experimental additional tasks randomly selected. figure plots multitask improvement measure dataset similarity call active occurrence rate active compound dataset aoriα deﬁned number additional datasets compound also active respectively values k-th fold dataset multitask single-task models logit log-odds reduces effect outliers emphasizes changes baseline high. note reasons discussed section dud-e excluded analysis. observed multitask effect stronger datasets others. consequently investigated possible explanations discrepancy found presence shared active compounds moderately correlated multitask improvement biological class target not. also possible multitask improvement results accurately modeling experimental artifacts rather speciﬁc interactions targets small molecules. believe case demonstrated strong improvement thoroughlycleaned datasets. efﬁcacy multitask learning directly related availability relevant data. hence obtaining greater amounts data critical importance improving state art. major pharmaceutical companies possess vast private stores experimental measurements; work provides strong argument increased data sharing could result beneﬁts all. data maximize beneﬁts achievable using current architectures order algorithmic progress occur must possible judge performance proposed models previous work. disappointing note published applications deep learning virtual screening distinct datasets directly comparable. remains future research establish standard datasets performance metrics ﬁeld. another direction future work study small molecule featurization. work possible featurization exist many others. additional performance also realized considering targets well small molecules featurization. another line research could improve performance using unsupervised learning explore much larger segments chemical space. although deep learning offers interesting possibilities virtual screening full drug discovery process remains immensely complicated. deep learning—coupled large amounts experimental data—trigger revolution ﬁeld? considering transformational effect methods ﬁelds optimistic future. b.r. supported fannie john hertz foundation. s.k. supported smith stanford graduate fellowship. also acknowledge support particular latter award funded american recovery reinvestment figure multitask improvement across target classes. xcoordinate lists series biological target classes represented dataset collection y-coordinate difference log-odds-mean-auc multitask single-task models. note dud-e datasets excluded. classes ordered total number targets target classes fewer members merged miscellaneous. tion however sign tests single-task multitask models duplicate unique targets gave signiﬁcant highly overlapping conﬁdence intervals respectively; recall meaning intervals given caption table together results suggest signiﬁcant information leakage within multitask networks. consequently results analysis unlikely signiﬁcantly affected presence duplicate targets dataset collection. work investigated massively multitask networks virtual screening. gathered large collection publicly available experimental data used train massively multitask neural networks. networks achieved signiﬁcant improvement simple machine learning algorithms. explored several aspects multitask framework. first demonstrated multitask performance improved addition tasks; performance still climbing tasks. next considered relative importance introducing data tasks. found additional data additional tasks contributed signiﬁcantly multitask effect. next discovered multitask learning afforded limited transferability tasks contained training set. effect universal required large amounts data even apply. references abdo ammar chen beining mueller christoph salim naomie willett peter. ligand-based virtual screening using bayesian networks. journal chemical information modeling collobert ronan weston jason. uniﬁed architecture natural language processing deep neural networks multitask learning. proceedings international conference machine learning deng hinton geoffrey kingsbury brian. types deep neural network learning speech recognition related applications overview. acoustics speech signal processing ieee international conference ieee lusci alessandro pollastri gianluca baldi pierre. deep architectures deep learning chemoinformatics prediction aqueous solubility drug-like molecules. journal chemical information modeling junshui sheridan robert liaw andy dahl george svetnik vladimir. deep neural nets method quantitative structure-activity relationships. journal chemical information modeling nair vinod hinton geoffrey rectiﬁed linear units improve restricted boltzmann machines. proceedings international conference machine learning pedregosa fabian varoquaux ga¨el gramfort alexandre michel vincent thirion bertrand grisel olivier blondel mathieu prettenhofer peter weiss dubourg vincent scikit-learn machine learning python. journal machine learning research rohrer sebastian baumann knut. maximum unbiased validation data sets virtual screening based pubchem bioactivity data. journal chemical information modeling swamidass joshua azencott chlo´e-agathe tingwan gramajo hugo tsai shiou-chuan baldi pierre. inﬂuence relevance voting accurate interpretable virtual high throughput screening method. journal chemical information modeling szegedy christian yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru vanhoucke vincent rabinovich andrew. arxiv preprint going deeper convolutions. arxiv. unterthiner thomas mayr andreas ¨unter klambauer steijaert marvin wenger j¨org ceulemans hugo hochreiter sepp. deep learning opportunity virtual screening. mysinger michael carchia michael irwin john shoichet brian directory useful decoys enhanced better ligands decoys better benchmarking. journal medicinal chemistry wang yanli xiao jewen suzek tugba zhang jian wang jiyao zhou zhigang lianyi karapetyan karen dracheva svetlana shoemaker benjamin pubchem’s bioassay database. nucleic acids research pcba datasets dose-response assays performed ncats chemical genomics center downloaded pubchem bioassay using following search limits totalsidcount activesidcount chemical conﬁrmatory dose-response target single ncgc. limits correspond search query small molecule doseresponse ncgc. note dud-e datasets especially susceptible artiﬁcial enrichment artifact dataset construction procedure. data point collection associated binary label classifying either active inactive. description datasets given table datasets cover wide range target classes assay types including cell-based vitro experiments. datasets duplicated targets marked asterisk pcba datasets compounds labeled active considered inactive missing data pubchem bioassay and/or featurization errors data points compounds used evaluation models; failure rates dataset group shown table group suffered especially high failure rates likely relatively large number metallic otherwise abnormal compounds supported rdkit package. counts given table include missing data. graphical breakdown datasets target class shown figure datasets used held-in held-out analyses repeated table table respectively. extension treatment task similarity text generated heatmap figure show pairwise intersection datasets collection. characteristics datasets immediately apparent target class enzyme enzyme enzyme viability viability viability miscellaneous transcription factor transcription factor viability miscellaneous gpcr protease channel signalling enzyme enzyme enzyme signalling enzyme miscellaneous protein-protein interaction gpcr protein-protein interaction protein-protein interaction protein-protein interaction miscellaneous enzyme enzyme protein-protein interaction enzyme enzyme enzyme enzyme enzyme miscellaneous enzyme enzyme enzyme transcription factor enzyme target class transcription factor miscellaneous miscellaneous gpcr protease miscellaneous enzyme enzyme promoter promoter enzyme enzyme protein kinase protease protein-protein interaction enzyme enzyme gpcr protein kinase enzyme enzyme protein-protein interaction protein-protein interaction transcription factor viability promoter miscellaneous enzyme miscellaneous transcription factor enzyme miscellaneous protein-protein interaction enzyme enzyme enzyme enzyme enzyme enzyme enzyme transcription factor enzyme enzyme target class protein-protein interaction protein-protein interaction promoter enzyme transcription factor enzyme promoter miscellaneous signalling signalling promoter miscellaneous miscellaneous gpcr promoter miscellaneous enzyme protease signalling miscellaneous enzyme miscellaneous viability viability viability protein kinase miscellaneous protein-protein interaction channel channel miscellaneous miscellaneous enzyme enzyme enzyme enzyme protease gpcr gpcr protein kinase transcription factor protein kinase enzyme receptor target class transcription factor miscellaneous protein-protein interaction protein-protein interaction protein-protein interaction protein kinase protease protease protease gpcr gpcr transcription factor transcription factor transcription factor enzyme transcription factor transcription factor transcription factor miscellaneous promoter miscellaneous miscellaneous miscellaneous gpcr protein kinase protease enzyme enzyme protease gpcr gpcr protein kinase atad mitochondrial membrane potential signalling adenosine receptor tyrosine-protein kinase angiotensin-converting enzyme acetylcholinesterase adenosine deaminase adam beta- adrenergic receptor beta- adrenergic receptor serine/threonine-protein serine/threonine-protein aldose reductase beta-lactamase androgen receptor target serine/threonine-protein kinase braf carbonic anhydrase caspase- cyclin-dependent kinase catechol o-methyltransferase cytochrome cytochrome macrophage colony stimulating factor receptor c-x-c chemokine receptor type peptide deformylase -beta-hydroxysteroid dehydrogenase dipeptidyl peptidase dopamine receptor dihydrofolate reductase epidermal growth factor receptor erbb estrogen receptor alpha coagulation factor coagulation factor fatty binding adipocyte fibroblast growth factor receptor fk-binding protein protein ferase/geranylgeranyltransferase type alpha subunit farnesyl diphosphate synthase glucocorticoid receptor glucocerebrosidase glutamate receptor ionotropic glutamate kainate histone deacetylase histone deacetylase human immunodeﬁciency virus type integrase human immunodeﬁciency virus type protease human immunodeﬁciency virus type reverse transcriptase hmg-coa reductase hexokinase type enzyme transcription factor transcription factor transcription factor transcription factor enzyme enzyme enzyme enzyme protease protein kinase transcription factor enzyme protein kinase target insulin-like growth factor receptor enoyl- reductase leukocyte adhesion glycoprotein lfa- alpha tyrosine-protein kinase kinesin-like protein stem cell growth factor receptor thymidine kinase protein kinase beta tyrosine-protein kinase leukotriene hydrolase kinase-activated protein kinase mineralocorticoid receptor hepatocyte growth factor receptor kinase c-jun n-terminal kinase kinase alpha matrix metalloproteinase dual speciﬁcity mitogen-activated protein kinase kinase nitric-oxide synthase neuraminidase phospholipase group poly polymerase- phosphodiesterase cyclooxygenase- cyclooxygenase- serine/threonine-protein purine nucleoside phosphorylase ppara protein-tyrosine phosphatase transformylase muscle glycogen phosphorylase dihydroorotate dehydrogenase renin rho-associated protein kinase retinoid receptor alpha thrombin trypsin tryptase beta- thymidylate synthase urokinase-type plasminogen activator vascular endothelial growth factor receptor serine/threonine-protein inhibitor apoptosis protein target class gpcr miscellaneous enzyme gpcr miscellaneous protein kinase protease protease transcription factor aryl hydrocarbon receptor promoter figure pairwise dataset intersections. value element position corresponds fraction dataset contained dataset thin black lines used indicate divisions dataset groups. figure multitask performance duplicate unique targets. outliers omitted clarity. notches indicate conﬁdence interval around median computed iqr/ table enrichment scores models reported table value median across datasets group mean k-fold enrichment values. enrichment alternate measure model performance common virtual drug screening. enrichment deﬁnition roughly enrichment factor better random model’s predictions are. figure graphical representation data table text. notches indicate conﬁdence interval around median computed iqr/ occasionally notch limits beyond quartile markers producing folded down effect boxplot. paired t-tests relative pmtnn across non-dud-e datasets gave multitask networks table trained learning rate batch size steps using stochastic gradient descent. weights initialized zero-mean gaussian standard deviation bias initialized experimented higher learning rates found pyramidal networks sometimes failed train however effect vanished lower learning rate. models trained simultaneous replicas sharing gradient updates cases used many pyramidal single-task networks trained settings steps. vanilla single-task networks trained learning rate steps. networks used figure figure trained learning rate epochs plus constant million steps. constant factor introduced observed smaller multitask networks required epochs larger networks stabilize. networks figure trained pyramidal single task architecture weights initialized weights networks represented figure trained steps learning rate noted main text datasets collection contained many inactive active compounds. ensure actives given adequate importance training weighted actives dataset total weight equal number inactives dataset table pyramid sensitivity analysis. median -fold-average-auc values given several variations pyramidal architecture. attempt avoid problem training failures layer becoming zero early training learning rate ﬁrst steps steps. table descriptions additional models. mtnn multitask neural net. auxiliary heads refers attachment independent softmax units task hidden layers unless otherwise marked assume training steps. -hidden layer mtnn auxiliary heads attached hidden layers steps -hidden layer mtnn steps -hidden layer mtnn steps pyramidal deep reconnected pyramidal deep pyramidal mtnn connected pyramidal mtnn connected pyramidal mtnn dropout steps pyramidal mtnn dropout learning rate table median -fold-average values additional models. sign test conﬁdence intervals paired t-test p-values relative pmtnn table calculated across non-dud-e datasets.", "year": 2015}