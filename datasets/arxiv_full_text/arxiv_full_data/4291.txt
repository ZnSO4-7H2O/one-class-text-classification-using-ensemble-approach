{"title": "Training Spiking Neural Networks for Cognitive Tasks: A Versatile  Framework Compatible to Various Temporal Codes", "tag": ["q-bio.NC", "cs.NE", "stat.ML"], "abstract": "Conventional modeling approaches have found limitations in matching the increasingly detailed neural network structures and dynamics recorded in experiments to the diverse brain functionalities. On another approach, studies have demonstrated to train spiking neural networks for simple functions using supervised learning. Here, we introduce a modified SpikeProp learning algorithm, which achieved better learning stability in different activity states. In addition, we show biological realistic features such as lateral connections and sparse activities can be included in the network. We demonstrate the versatility of this framework by implementing three well-known temporal codes for different types of cognitive tasks, which are MNIST digits recognition, spatial coordinate transformation, and motor sequence generation. Moreover, we find several characteristic features have evolved alongside the task training, such as selective activity, excitatory-inhibitory balance, and weak pair-wise correlation. The coincidence between the self-evolved and experimentally observed features indicates their importance on the brain functionality. Our results suggest a unified setting in which diverse cognitive computations and mechanisms can be studied.", "text": "conventional modeling approaches found limitations matching increasingly detailed neural network structures dynamics recorded experiments diverse brain functionalities. another approach studies demonstrated train spiking neural networks simple functions using supervised learning. here introduce modified spikeprop learning algorithm achieved better learning stability different activity states. addition show biological realistic features lateral connections sparse activities included network. demonstrate versatility framework implementing three well-known temporal codes different types cognitive tasks mnist digits recognition spatial coordinate transformation motor sequence generation. moreover find several characteristic features evolved alongside task training selective activity excitatory-inhibitory balance weak pair-wise correlation. coincidence self-evolved experimentally observed features indicates importance brain functionality. results suggest unified setting diverse cognitive computations mechanisms studied. keywords spiking neural network supervised learning temporal code sparse coding neural dynamics multiple tasks simplified models rate-based computation reflect little reality biological neural systems. therefore bridging needs unified approach compatible advanced tools deep learning able incorporate essential neuronal dynamics. spiking neural networks widely used simulation model neuroscience field adopt biological models neuron synapses. conventionally snns constructed model observed neural activities facilitate explaining underlying mechanisms–. main obstacle training functional snns spike events make neuronal states incompatible standard gradient-based learning methods studies around problem regarding snns pseudo rate-based models another approach spikeprop considered spike times state variables derived differentiable relationship input output spike times .the spikeprop approach developed focusing learning algorithms snns shown equivalent rate-based counterparts simple tasks however efforts training still solving real-world problems replicating human-level cognitive functionalities. find limitations remained current development first exist algorithms learn exact spike times rather various temporal codes potentially suitable certain tasks; second previous studies rarely explore rich network structures neural dynamics observed biological neural networks facilitate neural computation. paper adopt spiking neural network model basic structural dynamical characteristics neural system. modified spikeprop learning rule improved stability introduced. errors assigned spikes spike timing backpropagation. conditions efficient learning snns discussed regulation methods introduced support stable learning neural activity. introduced three temporal codes suitable different top-down viewpoint brain performs large variety functions ranging perception motion execution higher cognitive processes reasoning emotions; viewing bottom diverse brain activities mainly carried neurons transmitting transforming spikes within neural circuits. brain organizes spiking neural dynamics meaningful computation resolved. nonetheless analysis modeling neural computation assume ratebased models neuron represents information average firing rate–. although rate-based approaches straightforward evidences accumulated importance exact spike timing neural processing network dynamics accordingly number temporal coding theories developed synfire chains polychronization rank order coding predictive spike coding remains challenging confirm theories modeling studies hardness handcrafting complex cognitive functions model networks. learning essential element achieve functionality biological artificial neural networks. however endeavors implementing biologically realistic learning rules functional spiking neural networks achieved limited success hand gradient-based learning artificial neural networks especially deep architectures achieved considerable success various tasks moreover deep neural networks begun imitate cognitive functionalities memory attention solving tasks despite remarkable progresses deep learning rarely given significant feedbacks brain works. discrepancy believe stems facts first recent developments deep learning mainly guided insights mathematics rather neuroscientific findings; second highly training spiking neural networks using gradient methods first proposed spikeprop. following idea assume errors propagated neuron spikes. output error associated given temporal code first decide error propagation updating rule discuss cost function different temporal codes. network assumed allto-all connection ensure generality learning rule. notably network parameters thresholds membrane time constants also trainable using learning rule. however considered training synaptic weights axonal conduction delays updated using fixed look-up table. respect given partial derivative cost function spikes spike according chain rule compute kinds cognitive tasks. then build feedforward neural network solve three simple cognitive tasks. specific adopt rank-order code learn digits recognition; relative spike time code transform point position coordinates; synaptic current code generate motor sequences. three experiments explore different network dynamic modes synchronization sustained spiking activity. addition biological realistic characteristics lateral connections sparse spikes introduced model network several neural activity characteristics trained network investigated. training analyzing three functional networks took step relating connectivity activity computational functionalities demonstrated versatility spiking neural networks. part error defined cost function. errors spikes obtained iteratively apply equation last spike first spike. assigned error spike update synaptic weights conduction delays delay. equation indicated synaptic current linear convolution incoming spike times prototype synaptic kernel. addition biological synapses diverse dynamics based contained transmitters receptors. first experiments paper model synapses fast dynamics. third experiment considered synapses combination fast dynamics slow dynamics. figure shows four postsynaptic potentials induced different synaptic receptor combinations. typical spike generation dynamics present model depicted figure figure description spiking neural network model. postsynaptic potentials evoked synapses different dynamics. spike generation dynamics. membrane potential reached threshold linearly summation postsynaptic potentials membrane potential reset vreset. description neuron model lstm framework. different spiking order result different computation graph neural network. another important realization rank-order coding encode information order spikes omit spike timing. discrete temporal code suitable categorical variables spike order represents class. define cost function above introduced temporal codes based assumption ctc. however neural system often needs generate signals continuously evolve time motor sequences. previous studies proposed pass filters transform discrete spike trains continuous time dependent variables. here define kind read-out neuron summate psps evoke spikes. then define synaptic current code uses membrane potential network output notably spike timing spike order strictly constrained synaptic current code. therefore timing individual spikes variable long populational activity retains output current. computation applying gradient-based learning rule spiking neural network implies spike timing represent information equivalently continuous variables ann. addition neural dynamics challenging issue neural computation represent variety signals prototyped spikes. various temporal codes proposed different neural signals different activity modes. section adopt three temporal codes correspondent three kind tasks. basic form temporal code encode information exact timing spikes. temporal code utilizes full representation capacity spikes implemented spikeprop kind algorithms. however requirement access spike times makes hard neural network interpret. alleviate requirement narrow time window temporal code. fact abundant experimental studies suggest cortical neurons tend spike coherently attending mental tasks based experimental named communication coherence proposes neuron groups effectively communicate gamma-band oscillatory synchronization scenario neuron evoke spike short time window. network’s state guaranteed stable training. connection weights randomly initialized excitatory inhibitory synapses loosely balanced. training however excitatory inhibitory synapses learn tightly canceling increase representation accuracy. consequently firing rate network decrease membrane potentials driven closely rest potential. hence regulation needed counter fluctuation firing rate. another important requirement efficient learning stable gradients propagation. gradients errors vanish propagation learning extremely slow deep layers. hand learning unstable gradients explode propagation. using similar gradient-based rule learning also suffer unstable gradient problems. indeed slow unstable convergences known issues spikeprop kind learning algorithms. unlike learning however find gradient explosion problem critical training. excitation-inhibition balance tends become tighter synapse weights tend disperse training gradients also tend grow intrinsic spike generation mechanism. conventional methods alleviate gradient explosion problem gradient clip solve half problem. gradient explosion effects error backpropagation also effects spike forward propagation pushes network chaotic realm small noises would cause large changes spikes chaotic networks cannot utilize spike timing represent information even ideal learning rule could network certain dataset. addition stable propagation spikes guarantee stable gradients case anns make matters worse gradient likely explode membrane potential reaches threshold extremely slow although find avoid gradient explosion completely reasonably alleviate problem regulate raise rate membrane potential. biological neurons adaptive currents prohibit slowly ramping membrane potential depolarization. inspired fact figure example propagation synchronous spikes backpropagation errors. synchronous spikes stably propagate multiple layers small disturb input cause large change spikes later layers gradient explosion condition. example gradient explosion error backpropagation spikes. section describe weight initialization several regulation methods implemented network. initialized excitatory different connections differently different effects spike initiation. example strong feedforward inhibitory synapses tend cause silent neurons problem strong lateral excitatory synapses lead avalanche phenomenon. consequently feedforward connections excitatory synapses drawn provided several unique characteristics. firstly gain inputs determined static weights activation function. seen spike generation process figure effectiveness input spike dependent combination synaptic dynamics dynamical state postsynaptic neuron. property better interpreted analog spiking neuron long short-term memory unit input gate synaptic currents determined synaptic weights synapse dynamics; forget gate impact input continuously diminished neuron membrane’s leaky conductance; output gate information inputs transmitted output spike membrane potential reached threshold. original lstm unit spiking neuron model selectively transmit information based history inputs. extreme information spiking neuron processed temporal relation sparse all-or-none spikes. meanwhile spiking neuron much simpler lstm unit computational complexity. secondly spiking neural network dynamical system hence computations unfolded time. moreover mentioned above represents information recent history inputs. consequently structure computation graph dynamically determined spike timing also container information result training spiking neural network different inputs equivalent forge multiple ‘effective’ networks shared weights. requirements efficient learning computation previous works training spiking neural network using gradient-based algorithms often report occasionally fails slow converge especially large data sets. machine learning community concluded general requirements efficiently training gradient-based algorithms. requirements rarely discussed previous spiking neural network studies. below discuss closely related conditions also apply learning snns mean variance neuron’s states proper range. gradient neuron’s states stable propagation. artificial neural networks various normalization algorithms proposed guarantee proper mean variance activation. reason keep activation efficient gradient propagation avoid pathological curvature covariate shift activation artificial neural network analog firing rate biological neurons. neural networks using temporal code however different definition needed descript output states. define standard deviation neuronal population’s spike times variance output define mean spike count mean output. definition consistent definition studies synchronous spikes. spiking neurons leaky nature neural dynamics spike time differences represent information variance spike times must fall proper range transmit information efficiently. addition spiking neural networks also could suffer pathological curvature covariate shift problems training reason artificial networks. furthermore delete spikes change computation graph network. hence stable spike count training fundamental requirement efficient learning. recently activation function called selu proposed drive mean variance activation fixed attractor neuroscience many studies synchrony spike propagation acknowledged spiking neurons also similar properties studies often focus stable propagation synfire chains spikes wider dispersion propagate stably well however synchronous spike mode; initialized excitatory synapses inhibitory synapses last experiment support sustained spikes. addition synapses cannot grow infinitely biological constrains weight-dependent factor original update rule control limit synapses weight. furthermore stability network’s firing rate critical efficient learning drifting away training. hence introduce homeostatic rule synaptic weights regulate firing rate representation schemes achieved designing sophisticated receptive fields encoding neuron using linear filter unnecessary simple demonstration. simplified version rank-order code named time-to-first spike code employed encode classification result. defined output neurons neuron assigned preferred digit class. output considered correct first spike neuron’s preferred class matches image label. network consists neurons structure shown figure neurons relay layers regulated spike average spikes/trail. addition poisson spikes neuron background noise trained noiseless network comparison. epochs training mini-batch size training without noise converge faster background noise training background noise robust test neural system needs perform various kinds mental tasks neural assemblies specialized diverse functionalities. however cortical networks different brain areas share similar microcircuit structure. fact indicated neural network capable learn multiple tasks modular structure. section train simple feedforward neural network learn three task coordinate transformation task movement generation task. general network structure shown figure different layers connected feed-forwardly. addition lateral connections pervasive cortical networks neurons within layer also inter-connected. connections initialized random conduction delay delays constrained within training. descriptions network shown table network configuration several simplifications ease training network consist homogeneous neurons; neuron excitatory inhibitory synapses; network defined dense connections layers. simulation based brian simulator simulation time step experiments adam optimizer present three tasks covered small portion mental tasks brain performs. addition simplified task description network structure necessarily reflect full biological reality. nevertheless work provides insight learning computation capacity spiking neural network. classification task creatures constantly required discriminate different objects make appropriate decisions based sensory stimuli. tasks fall realm classification problem machine learning. demonstrate spiking neural network’s capacity tasks trained feedforward network recognize handwriting digits mnist dataset mnist dataset contains labeled grayscale images handwriting digits training labeled digits testing. relative time code encode image input input neuron encoding grayscale pixel. efficient directions origin) rotate coordinate around center certain angle spatial organization coordinate inspired coordinate system hippocampus different biological system relative time code encode position coordinate axis assigned neuron linear filter. besides defined neurons encode rotation angle preferred angle uniformly range network consist neurons neurons first second relay layers respectively neurons regulated spike average rate spikes/trail. trained network randomly generated positions within unit distance center random rotation angles ranging training mean spike time error reached mean error estimated position transformation errors uniformly distributed coordinate space tend larger outer areas likely fewer neurons dedicated encode outer areas. figure training corrdination transformation task. scheme coordiante coordiante transforamtion task point origins light blue vector axises linear filtered position caculated project point axises. coordinate transformation achieved rotate point angle around center. transformation errors output positions. unbalanced distribution motif structures connections layer layer first three motifs shown graph connection represent positive weights blue connections represent negative weights. distribution pair-wise correlation neurons within layers. next investigate structure activity trained network. experimental studies shown neural network structures specialized proportion different motif structures significantly deviated chance level trained functional network also find motif structures consist much higher portion others hence specific motif structures highly likely computational observed phenomenon cortical network weak pair-wise correlation globally synchronous spiking neurons phenomenon also arises spontaneously training functional network even though intentionally modelled weak pair-wise correlation suggested facilitate efficient coding work confirmed hypothesis bottom-up approach. also measured receptive field neurons relay layers shown organized patterns receptive fields neurons layers quite similar. closer inspection receptive fields calculate standard deviation spike triggered input /output. figure training minst dataset. basic structure simple feedforward network. evolution error rate learning noise noiseless conditions. weight distribution training. examples learned features minst cognition task features layer layer neurons estimated first-spike triggered average features layer layer neurons estimated spike triggered average features layer layer neurons estimated fsta random stimulus. next investigate learned features hidden layer neurons using spike triggered average kind methods. conventional method neuroscience examine neuron’s receptive field conduct three different procedures examine receptive features firstly calculate neuron’s applying original digit images stimulus. secondly first-spike triggered average neuron calculated average images cause neuron spike first. finally calculate first-spike triggered average randomized stimulus pixel randomly chosen pixels corresponding position images. shown figure neurons network developed global features feature characterizes certain digit. comparison features calculated fsta shows spike timing selective spike count. addition also notice neurons response randomized image stimuli much lower firing rate digit images phenomenon indicates neurons trained exclusively response certain features. confirm effect learning network activity. shuffled postsynaptic weights within neuron trained network stimulate network digit images randomized images respectively. shuffled network responds kinds stimulus spikes indistinctively. shows network’s activity vary greatly different detailed connections even overall configuration remains same. result questioned validity conventional modeling approaches reproduce neural activity observed experiments randomly generated connection weights. coordinate transformation task many cognitive tasks require brain represent outside world internal models consist properties vary continuously. predicting transforming continuum properties neural systems analog regression problems machine learning. coordinate transformation object’s position velocity common task animals robots. demonstrate snn’s capacity regression problems train feedforward network perform coordinate transformation task defined coordinate system axes example network activity output motor sequence response motor command. series motor sequences generated network. amplitude uniformly varied frequency uniformly varied average positive membrane potential group relay layer neurons. represent excitatory projection neurons blue represent inhibitory projection neurons. amplitude output varied fixed amplitude frequency output varied cycles/ms. trained network iterates random samples mini-batch typical execution shown figure update weights second relay layer output neuron network implement liquid state machine however find updating weights deeper layers increased speed capacity learning. addition synapse network contains fast slow dynamics characterized separate weights. slow synaptic dynamics provide long-term channel propagate errors large time-scale signal short time-scale spike fast synaptic dynamics manage transient spike dynamics correct residual errors. notably another recently proposed method also used synapses fast slow dynamics learn motions differ work method uses fast synaptic dynamics reference learn motion update slow synaptic weights. aims works also different aims transform certain input time-varying signal certain output time-scale work aims transform synchrony commands series motion outputs investigate network dynamics different motion execution measured mean positive membrane potential groups relay layer neurons project excitatory inhibitory outputs respectively. figure fixed frequency motion varied amplitude shows relative phase neuron group activities changed accordingly. figure fixed amplitude motion changed frequency expected oscillation frequency neurons also followed frequency desired motion. similar first experiment result also demonstrated overall neural dynamics precisely determined exact spike times. spiking neural networks useful tool neuroscience modeling neural dynamics. using gradient-based rule demonstrated snns capable learning different functions different temporal codes. proposed recently synaptic plasticity mechanisms achieved figure developed feature selectivity relay neurons coordinate transformation task. example receptive field neurons relay layer. color represent rotation angle dark area represent transformed point coordination. example receptive field neurons relay layer. distributions rotation angles stimulus responded neurons brain often required process transform signals different time-scales. tasks often involve responding particular temporal input sequences responding delay responding temporally complex output. demonstrate neural network’s computational capacity multitime scale transformation tasks train network execute series motions according synchrony commands. desired motion output defined sinusoidal function frequency duration motion. experiment designed last trail. output designed maintain standstill first execute motion output second relative spike time encode input command neurons neurons encode amplitude neurons encode frequency uniformly varied preferred stimulus defined parameter output encoded synaptic current coding neuron receiving projections layer. synchronous commands propagated relay layer transformed time-varying signal sustained activity relay layer. unlike previous configurations lateral connections second relay layer initialized excitatory synapses inhibitory synapses support sustained activity output form error back-propagation admittedly exact relation synaptic plasticity gradient backpropagation still needs study. safe however trained spiking neural network provide valuable insights neural coding neural computation regardless learning rule. three tasks demonstrated simple covered functionalities classification internal representation long short time-scale transformation essential complex mental tasks. neural network model considerably simplified well reflect organization real cortical networks. nevertheless many important features cortical network evolved training specialized receptive fields motif structures tight excitation-inhibition balance weak pair-wise correlation spikes spontaneous development features suggests necessary functionality neural networks. hence building functional network training provide investigate role network features brain functions. addition found experiments overall activities trained network diverge greatly response meaningful random stimulus. selective activities selective synchronization among brain areas typical properties brain. work suggested phenomena might result specific connectivity learned specific tasks rather special organization brain. therefore training biological learning could also indispensable step modeling neural behaviors. additionally proposed recently recurrent neural networks could versatile tools neuroscience research trained rnns snns complementary other rnns efficient describing large-scale activities snns describe details transient dynamics exact spike activities. altogether training provide powerful enhancement current modeling study neural computation cognitive functions. alongside training three tasks spotted three important requirements efficient learning modified network model learning rule accordingly. first requirement proper spike time variance hand spike times need contain enough entropy represent rich information temporal codes. hand relevant neurons need spike within effective time window confined postsynaptic spike generation dynamics. proper weight initialization regulation shown spike time variance selforganized within proper range snns. second requirement proper stable spiking rate even though network encodes information temporal codes stable spikes still necessary effective information transmission stable learning. hand also flexible enough allow irrelevant neurons cease spike certain stimulus conditions. addition spike sparseness also preferable nonlinear neural computations hence introduced spiking rate regulation method keep neurons stable firing rate. third requirement stable gradient propagation gradient vanishing would cause inefficiency learning gradient explosion would cause instability learning network activity. gradient vanishing problem benign snns lateral connections introduced threshold rise rate membrane potential avoid gradient explosion caused unreliable spikes. however stable gradients training still fully guaranteed model studies needed solve problem. notably experiments based spikeprop kind learning using neuron networks. linear additive requirement gradientbased rule posed strong constrain development. another thread studies local error assignment rules need exact gradients developed difference target propagation random back-propagation synthetic gradients studies currently focused anns rate-code based snns. however rate code consist codes based-on relative spike times since neuron high firing rate expected spike early. hence expect extend rules networks transient spiking dynamics studies. fields neuroscience artificial intelligence long intertwined history. recently researchers field called collaborations fields main study demonstrate common language bridging neuroscience deep learning. thus hand introduce advance technics deep learning train biological neural network models. hand also intrinsic properties extend capability machine learning systems. remark computation likened neuron model lstm cell. notably model extremely simplified version spiking neuron. complicate biological neuron models different realizations gates lstm analogy. example input gate also controlled membrane potential models conductance-based synapses; recently proposed soft threshold model allows output change gradually voltage-dependent gate function. essential difference snns rnns however dynamical evolvements snns mostly constrained within neuron cell dynamics rnns defined network property. additional single neuron computation renders spiking neural network ability dynamically routing information flow spiking activities .this property applicable scale microcircuit large neural assemblies. small portion neurons active given time brain specific communication established synchronizations ability flexibly routing information flow brain considered fundamental brain’s multitask capability work introduced synapse model fast slow dynamics transform spike signal different time-scales. studies introduce heterogeneous intrinsic neuronal properties adaptive currents dendritic nonlinearities project spike timing information higher order temporal spaces. shown experiments spiking activity support different function modules different temporal codes. combined flexibility information routing believe large-scale spiking neural networks biological network dynamics diverse temporal codes good candidate general systems. furthermore configuration energy efficient engineering perspective. rather communicate globally update cycle spiking neurons need communicate others internal computation evoked meaningful event. adopting event-driven computing architecture neuromorphic computer chips truenorth spinnaker simulate millions neurons real time relatively energy assumption. properties make snns preferable real life applications.", "year": 2017}