{"title": "Unsupervised Monocular Depth Estimation with Left-Right Consistency", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Learning based methods have shown very promising results for the task of depth estimation in single images. However, most existing approaches treat depth prediction as a supervised regression problem and as a result, require vast quantities of corresponding ground truth depth data for training. Just recording quality depth data in a range of environments is a challenging problem. In this paper, we innovate beyond existing approaches, replacing the use of explicit depth data during training with easier-to-obtain binocular stereo footage.  We propose a novel training objective that enables our convolutional neural network to learn to perform single image depth estimation, despite the absence of ground truth depth data. Exploiting epipolar geometry constraints, we generate disparity images by training our network with an image reconstruction loss. We show that solving for image reconstruction alone results in poor quality depth images. To overcome this problem, we propose a novel training loss that enforces consistency between the disparities produced relative to both the left and right images, leading to improved performance and robustness compared to existing approaches. Our method produces state of the art results for monocular depth estimation on the KITTI driving dataset, even outperforming supervised methods that have been trained with ground truth depth.", "text": "figure depth prediction results kitti bottom input image ground truth disparities result. method able estimate depth thin structures street signs poles. understanding shape scene single image independent appearance fundamental problem machine perception. many applications synthetic object insertion computer graphics synthetic depth field computational photography grasping robotics using depth human body pose estimation robot assisted surgery automatic conversion film accurate depth data cameras also crucial self-driving cars expensive laser-based systems often used. humans perform well monocular depth estimation exploiting cues perspective scaling relative known size familiar objects appearance form lighting shading occlusion combination top-down bottom-up cues appears link full scene understanding ability accurately estimate depth. work take alternative approach treat automatic depth estimation image reconstruction problem training. fully convolutional model require depth data instead trained synthesize depth intermediate. learns predict pixel-level correspondence pairs rectified stereo images known camera baseline. existing methods also address problem several limitations. example fully differentiable making training suboptimal image formation models learning based methods shown promising results task depth estimation single images. however existing approaches treat depth prediction supervised regression problem result require vast quantities corresponding ground truth depth data training. recording quality depth data range environments challenging problem. paper innovate beyond existing approaches replacing explicit depth data training easier-to-obtain binocular stereo footage. propose novel training objective enables convolutional neural network learn perform single image depth estimation despite absence ground truth depth data. exploiting epipolar geometry constraints generate disparity images training network image reconstruction loss. show solving image reconstruction alone results poor quality depth images. overcome problem propose novel training loss enforces consistency between disparities produced relative left right images leading improved performance robustness compared existing approaches. method produces state results monocular depth estimation kitti driving dataset even outperforming supervised methods trained ground truth depth. depth estimation images long history computer vision. fruitful approaches relied structure motion shape-from-x binocular multi-view stereo. however techniques rely assumption multiple observations scene interest available. come form multiple viewpoints observations scene different lighting conditions. overcome limitation recently surge number works pose task monocular depth estimation supervised learning problem methods attempt directly predict depth pixel image using models trained offline large collections ground truth depth data. methods enjoyed great success date scale large output resolutions improve upon methods novel training objective enhanced network architecture significantly increases quality final results. example result algorithm illustrated fig. method fast takes order milliseconds predict dense depth image modern gpu. specifically propose following contributions network architecture performs end-to-end unsupervised monocular depth estimation novel training loss enforces left-right depth consistency inside network. evaluation several training losses image formation models highlighting effectiveness approach. addition showing state results challenging driving dataset also show model generalizes three different datasets including outdoor urban dataset collected ourselves make openly available. large body work focuses depth estimation images either using pairs several overlapping images captured different viewpoints temporal sequences assuming fixed camera static scene changing lighting approaches typically applicable input image available scene interest. focus works related monocular depth estimation single input image assumptions scene geometry types objects present made. vast majority stereo estimation algorithms data term computes similarity pixel first image every pixel second image. typically stereo pair rectified thus problem disparity estimation posed search problem pixel. recently shown instead using hand defined similarity measures treating matching supervised learning problem training function predict correspondences produces superior results also shown posing binocular correspondence search multi-class classification problem advantages terms quality results speed instead learning matching function mayer introduced fully convolutional deep network called dispnet directly computes correspondence field images. training time attempt directly predict disparity pixel minimizing regression training loss. dispnet similar architecture previous end-to-end deep optical flow network scenes approaches typically synthetic data training. synthetic data becoming realistic e.g. still requires manual creation content every application scenario. single-view monocular depth estimation refers problem setup single image available test time. saxena proposed patch-based model known maked first over-segments input image patches estimates location orientation local planes explain patch. predictions plane parameters made using linear model trained offline dataset laser scans predictions combined together using mrf. disadvantage method planar based approximations e.g. difficulty modeling thin structures predictions made locally lack global context required generate realistic outputs. instead hand-tuning unary pairwise terms convolutional neural network learn them. another local approach ladicky incorporate semantics model improve pixel depth estimation. karsch attempt produce consistent image level predictions copying whole depth images training set. drawback approach requires entire training available test time. eigen showed possible produce dense pixel depth estimates using scale deep network trained images corresponding depth values. unlike previous work single image depth estimation rely hand crafted features initial oversegmentation instead learn representation directly pixel values. several works built upon success approach using techniques crfs improve accuracy changing loss regression classification using robust loss functions incorporating strong scene priors case related problem surface normal estimation again like previous stereo methods approaches rely high quality pixel aligned ground truth depth training time. perform single depth image estimation train added binocular color image instead requiring ground truth depth. recently small number deep network based methods novel view synthesis depth estimation proposed require ground truth depth training time. flynn introduced novel image synthesis network called deepstereo generates views selecting pixels nearby images. training relative pose multiple cameras used predict appearance held-out nearby image. appropriate depths selected sample colors neighboring images based plane sweep volumes. test time image synthesis performed small overlapping patches. requires several nearby posed images test time deepstereo suitable monocular depth estimation. deepd network also addresses problem novel view synthesis goal generate corresponding right view input left image context binocular pairs. using image reconstruction loss method produces distribution possible disparities pixel. resulting synthesized right image pixel values combination pixels scan line left image weighted probability disparity. disadvantage image formation model increasing number candidate disparity values greatly increases memory consumption algorithm making difficult scale approach bigger output resolutions. work perform comparison deepd image formation model show algorithm produces superior results. closest model spirit concurrent work garg like deepd method train network monocular depth estimation using image reconstruction loss. however image formation model fully differentiable. compensate perform taylor approximation linearize loss resulting objective challenging optimize. similar recent work e.g. model overcomes problem using bilinear sampling generate images resulting fully differentiable training loss. propose fully convolutional deep neural network loosely inspired supervised dispnet architecture mayer posing monocular depth estimation image reconstruction problem solve disparity field without requiring ground truth depth. however minimizing photometric loss result good quality image reconstructions poor quality depth. among terms fully differentiable training loss includes left-right consistency check improve quality synthesized depth images. type consistency check commonly used post-processing step many stereo methods e.g. incorporate directly network. section describes single image depth prediction network. introduce novel depth estimation training loss featuring inbuilt left-right consistency check enables train image pairs without requiring supervision form ground truth depth. given single image test time goal learn function predict per-pixel scene depth ˆd=f. existing learning based approaches treat supervised learning problem color input figure loss module outputs left right disparity maps loss combines smoothness reconstruction left-right disparity consistency terms. module repeated four different output scales. convolution up-convolution bilinear sampling up-sampling skip connection. images corresponding target depth values training. presently practical acquire ground truth depth data large variety scenes. even expensive hardware laser scanners imprecise natural scenes featuring movement reflections. alternative instead pose depth estimation image reconstruction problem training. intuition that given calibrated pair binocular cameras learn function able reconstruct image other learned something shape scene imaged. specifically training time access images corresponding left right color images calibrated stereo pair captured moment time. instead trying directly predict depth attempt find dense correspondence field that applied left image would enable reconstruct right image. refer reconstructed image ˜ir. similarly also estimate left image given right =ir. assuming images rectified corresponds image disparity scalar value pixel model learn predict. given baseline distance cameras camera focal length trivially recover depth predicted disparity ˆd=bf/d. high level network estimates depth inferring disparities warp left image match right one. insight method simultaneously infer disparities using left input image obtain better depths enforcing consistent other. network generates predicted image backward mapping using bilinear sampler resulting fully differentiable image formation model. illustrated fig. na¨ıvely learning generate right image sampling left smooth disparities prefers predicted left right disparities consistent. main terms contains left right image variant left image convolutional layers. appearance matching loss training network learns generate image sampling pixels opposite stereo image. image formation model uses image sampler spatial transformer network sample input image using disparity map. uses bilinear sampling output pixel weighted four input pixels. contrast alternative approaches bilinear sampler used locally fully differentiable integrates seamlessly fully convolutional architecture. means require simplification approximation cost function. disparity smoothness loss encourage disparities locally smooth penalty disparity gradients depth discontinuities often occur image gradients similar weight cost edge-aware term using image gradients left-right disparity consistency loss produce accurate disparity maps train network predict left right image disparities given left view input convolutional part network. ensure coherence introduce left-right disparity consistency penalty part model. cost attempts make left-view disparity equal projected right-view disparity figure sampling strategies backward mapping. na¨ıve sampling produces disparity aligned target instead input. corrects this suffers artifacts. approach uses left image produce disparities images improving quality enforcing mutual consistency. produce disparities aligned right image however want output disparity align input left image meaning network sample right image. could instead train network generate left view sampling right image thus creating left view aligned disparity alone works inferred disparities exhibit ‘texture-copy’ artifacts errors depth discontinuities seen fig. solve training network predict disparity maps views sampling opposite input images. still requires single left image input convolutional layers right image used training enforcing consistency disparity maps using novel left-right consistency cost leads accurate results. fully convolutional architecture inspired dispnet features several important modifications enable train without requiring ground truth depth. network composed main parts encoder decoder please supplementary material detailed description. decoder uses skip connections encoder’s activation blocks enabling resolve higher resolution details. output disparity predictions four different scales double spatial resolution subsequent scales. even though takes single image input network predicts disparity maps output scale left-to-right right-to-left. training loss table comparison different image formation models. results kitti stereo training disparity images training kitti dataset cityscapes model left-right consistency performs best improved addition cityscapes data. last shows result model trained tested input images instead test time network predicts disparity finest scale level left image resolution input image. using known camera baseline focal length training convert disparity depth map. also estimate right disparity training used test time. compare performance approach supervised unsupervised single view depth estimation methods. train rectified stereo image pairs require supervision form ground truth depth. existing single image datasets lack stereo pairs suitable evaluation. instead evaluate approach using popular kitti dataset. evaluate image formation model compare variant algorithm uses original deepd image formation model modified deepds added smoothness constraint. also evaluate approach without left-right consistency constraint. network implemented tensorflow contains million trainable parameters takes order hours train using single titan dataset thousand images epochs. inference fast takes less frames second image including transfer times gpu. please supplementary material code details. optimization weighting different loss components possible output disparities constrained dmax using scaled sigmoid non-linearity dmax image width given output scale. result multi-scale output typical disparity neighboring pixels differ factor scale correct this scale disparity smoothness term scale equivalent smoothing level. thus =./r downscaling factor corresponding layer respect resolution input image passed network. non-linearities network used exponential linear units instead commonly used rectified liner units found relus tended prematurely predicted disparities intermediate scales single value making subsequent improvement difficult. following replaced usual deconvolutions nearest neighbor upsampling followed convolutions. trained model scratch epochs batch size using adam used initial learning rate kept constant first epochs halving every epochs end. initially experimented progressive update schedules lower resolution image scales optimized first. however found optimizing four scales stable convergence. similarly identical weighting loss scale found weighting differently unstable convergence. experimented batch normalization found produce significant improvement ultimately excluded data augmentation performed fly. flip input images horizontally chance taking care also swap images correct position relative other. also added color augmentations chance performed random gamma brightness color shifts sampling uniform distributions ranges gamma brightness color channel separately. resnet sake completeness similar also show variant model using resnet encoder rest architecture parameters training procedure staying identical. variant contains million trainable parameters indicated resnet result tables. post-processing order reduce effect stereo disocclusions create disparity ramps left side image occluders final post-processing step performed output. input image test time also figure qualitative results kitti eigen split. ground truth velodyne depth sparse interpolate visualization purposes. method better resolving small objects pedestrians poles. compute disparity horizontally flipped image flipping back disparity obtain disparity aligns disparity ramps located right occluders well right side image. combine disparity maps form final result assigning first left image using last right disparities central part final disparity average final post-processing step leads better accuracy less visual artifacts expense doubling amount test time computation. indicate results using result tables. present results kitti dataset using different test splits enable comparison existing works. form dataset contains rectified stereo pairs scenes typical image pixels size. kitti split first compare different variants method including different image formation models different training sets. evaluate high quality disparity images provided part official kitti training covers total scenes. remaining scenes contain images keep training rest evaluation. disparity images much better quality reprojected velodyne laser depth values models inserted place moving cars. models result ambiguous disparity values transparent surfaces windows issues object boundaries models perfectly align images. addition maximum depth present kitti dataset order meters maximum predictions networks value. results computed using depth metrics along d-all disparity error kitti metrics measure error meters ground truth percentage depths within threshold correct value. important note measuring error depth space ground truth given disparities leads precision issues. particular non-thresholded measures sensitive large errors depth caused prediction errors small disparity values. erties trained scratch network architecture ours deepd image formation model performs poorly. fig. deepd produces plausible image reconstructions output disparities inferior ours. loss outperforms deepd baselines addition left-right consistency check increases performance measures. fig. illustrate zoomed comparisons clearly showing inclusion leftright check improves visual quality results. results improved first pre-training model additional training data cityscapes dataset containing training stereo pairs captured various cities across germany. dataset brings higher resolution image quality variety compared kitti similar setting. cropped input images keep image removing reflective hoods input. interestingly model trained cityscapes alone perform well numerically. likely difference camera calibration datasets clear advantage fine-tuning data related test set. eigen split able compare existing work also test split images proposed covers total scenes. remaining scenes contain images keep training rest evaluation similarly generate ground truth depth images reproject points viewed velodyne laser left input color camera. aside producing depth values less pixels input image errors also introduced rotation figure comparison method without leftright consistency. consistency term produces superior results object boundaries. results shown without post-processing. table results kitti using split eigen training kitti dataset cityscapes predictions generated left right images instead left input images. fair comparison compute results relative correct image. provided source code eigen results computed relative velodyne instead camera. garg results taken directly paper. results except crop also show results crop maximum evaluation distance. last rows computed uncropped ground truth. also implemented stereo version model fig. network’s input concatenation left right views. perhaps unsurprisingly stereo models outperforms monocular network every single metric especially d-all disparity measure seen table model trained epochs becomes unstable trained longer. illustrate method generalize datasets compare several fully supervised methods maked test maked consists rgb/depth pairs stereo images thus method cannot train data. network trained cityscapes dataset despite dissimilarities datasets content camera parameters still achieve reasonable results even beating metric three. different aspect ratio maked dataset evaluate central crop images. table compare output similarly cropped results methods. case kitti dataset results would likely improved relevant training data. qualitative comparison related methods shown fig. numerical results good baselines qualitatively compare favorably supervised competition. finally illustrate examples model generalizing datasets figure using model trained cityscapes tested camvid driving dataset accompanying video supplementary material despite differences location image characteristics camera calibration model still figure image reconstruction error kitti. methods output plausible right views deepd image formation model without smoothness constraints produce valid disparities. velodyne motion vehicle surrounding objects also incorrect depth readings occlusion object boundaries. fair methods crop evaluate input image resolution. exception garg al.’s results results baseline methods recomputed given authors’s original predictions ensure scores directly comparable. produces slightly different numbers previously published ones e.g. case predictions evaluated much smaller depth images baseline methods bilinear interpolation resize predictions correct input image size. table shows quantitative results example outputs shown fig. algorithm outperforms existing methods including trained ground truth depth data. pre-training cityscapes dataset improves results using kitti alone. even though left-right consistency check postprocessing improve quality results still artifacts visible occlusion boundaries pixels occlusion region visible images. explicitly reasoning occlusion training could improve issues. worth noting depending large baseline camera depth sensor fully supervised approaches also always valid depth pixels. method requires rectified temporally aligned stereo pairs training means currently possible existing single-view datasets training purposes e.g. however possible fine-tune model application specific ground truth depth data. finally method mainly relies image reconstruction term meaning specular transparent surfaces produce inconsistent depths. could improved sophisticated similarity measures presented unsupervised deep neural network single image depth estimation. instead using aligned ground truth depth data rare costly exploit ease binocular stereo data captured. novel loss function enforces consistency predicted depth maps camera view training improving predictions. results superior fully supervised baselines encouraging future research require expensive capture ground truth depth. also shown model generalize unseen datasets still produce visually plausible depth maps. future work would like extend model videos. current depth estimates performed independently frame adding temporal consistency would likely improve results. would also interesting investigate sparse input alternative training signal finally model estimates pixel depth would interesting also predict full occupancy scene acknowledgments would like thank david eigen ravi garg laina fayao providing data code recreate baseline algorithms. also thank stephan garbin skills peter hedman latex magic. grateful epsrc funding engd centre ep/g/ projects ep/k/ ep/k/. table results maked dataset methods marked supervised ground truth depth data maked training set. using standard metric errors computed depth less meters central image crop. produces visually plausible depths. also captured frame dataset frames second taken urban environment wide angle consumer stereo camera. finetuning cityscapes pre-trained model dataset produces visually convincing depth images test captured camera different please video supplementary material results. references abadi agarwal barham brevdo chen citro corrado davis dean devin tensorflow large-scale machine learning heterogeneous distributed systems. arxiv preprint arxiv. cordts omran ramos rehfeld enzweiler benenson franke roth schiele. cityscapes dataset semantic urban scene understanding. cvpr eigen fergus. predicting depth surface normals semantic labels common multi-scale convolutional architecture. iccv geiger lenz urtasun. ready autonomous driving? kitti vision benchmark suite. cvpr godard hedman brostow. multi-view reconstruction highly specular surfaces uncontrolled environments. mayer h¨ausser fischer cremers dosovitskiy brox. large dataset train convolutional networks disparity optical flow scene flow estimation. cvpr woodham. photometric method determining surface orientation multiple images. optical engineering girshick farhadi. deepd fully automatic d-to-d video conversion deep convolutional neural networks. eccv", "year": 2016}