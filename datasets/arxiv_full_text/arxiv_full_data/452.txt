{"title": "Automatic Open Knowledge Acquisition via Long Short-Term Memory Networks  with Feedback Negative Sampling", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "Previous studies in Open Information Extraction (Open IE) are mainly based on extraction patterns. They manually define patterns or automatically learn them from a large corpus. However, these approaches are limited when grasping the context of a sentence, and they fail to capture implicit relations. In this paper, we address this problem with the following methods. First, we exploit long short-term memory (LSTM) networks to extract higher-level features along the shortest dependency paths, connecting headwords of relations and arguments. The path-level features from LSTM networks provide useful clues regarding contextual information and the validity of arguments. Second, we constructed samples to train LSTM networks without the need for manual labeling. In particular, feedback negative sampling picks highly negative samples among non-positive samples through a model trained with positive samples. The experimental results show that our approach produces more precise and abundant extractions than state-of-the-art open IE systems. To the best of our knowledge, this is the first work to apply deep learning to Open IE.", "text": "previous studies open information extraction mainly based extraction patterns. manually deﬁne patterns automatically learn large corpus. however approaches limited grasping context sentence fail capture implicit relations. paper address problem following methods. first exploit long shortterm memory networks extract higher-level features along shortest dependency paths connecting headwords relations arguments. path-level features lstm networks provide useful clues regarding contextual information validity arguments. second constructed samples train lstm networks without need manual labeling. particular feedback negative sampling picks highly negative samples among non-positive samples model trained positive samples. experimental results show approach produces precise abundant extractions state-of-the-art open systems. best knowledge ﬁrst work apply deep learning open open information extraction task involves taking sentences extracting arguments relations them. open systems extract information form triple n-tuple. consider following input sentence ‘boeing announced open system extract <boeing; announced; asb> <boeing; announced asb; <boeing; announced; asb; open successfully applied many tasks question answering knowledge base population ontology extension major difference traditional open domain dependency. traditional requires pre-deﬁned relations whereas open not. open represents relations words sentence. paradigm removes domain dependency extending relation whole word-sets. thus possible open scale web. previous open systems adopt main approaches. ﬁrst approach involves manually deﬁning extraction patterns relationships arguments. reverb showed simple parts-of-speech patterns cover majority relationships. gamallo kraken manually deﬁne extraction rules dependency parse trees. second approach involves automatically learning dependency-based extraction patterns large corpus. methods adopting second approach include ollie renoun although previous open systems used many studies systems extract relations represented explicitly sentence. example previous systems relation ‘capital’ ‘vilnius’ ‘lithuania’ following sentences ‘the countries ofﬁcially vilnius capital lithuania’; ‘the geographical midpoint europe north lithuania's capital vilnius’; ‘vilnius capital lithuania residence grand duke’. explicit relations accompany text snippets strong clues regarding relation however previous open systems fail relation implicitly represented sentence returned lithuania lived capital vilnius death’. unlike explicit relations implicit relation captured merely textual patterns. extracting implicit relations involves deeper understanding context sentence. paper propose novel open system automatically extracts features using long short-term memory networks. bidirectional recurrent architecture lstm units automatically extracts higher-level features along shortest dependency paths connecting headwords relations arguments. paths contain informative words relevant ﬁnding proper arguments relation extracted features grasp contextual information without superﬂuous information. because prevalent datasets training open systems propose methods constructing training samples. particular feedback negative sampling selects highly negative samples among non-positive samples decreases disagreements positive negative samples. procedure constructing training fully automatic. require manual labeling. experimental results show proposed system produces times correct extractions including implicit relations higher precision state-of-the-art open systems. remainder paper organized follows. section describes types relations system aims extract. section deﬁnes open tasks argument detection preposition classiﬁcation. section describes procedure automatically constructing training set. sections provide detailed explanations neural network architectures argument detection preposition classiﬁcation respectively. section describes triples extracted outputs argument detection preposition classiﬁcation. section describes experimental settings shows evaluaﬁrst type relation verb-mediated relation. relation type verb phrase. often forms n-ary relation. consider example ‘boeing announced relation ‘announced’ arguments ‘boeing’ ‘the asb’ n-ary relation represented n-tuple <boeing; announced; asb; however because binary relation core concept semantic ontological n-ary relation must converted binary relations. conversion involves handling problem incomplete relations. example merely spanning pairs arguments triples <boeing; announced; asb> <boeing; announced <the asb; announced however relation <boeing; announced omits critical information—namely ‘the asb’—and fails complete relation ‘boeing’ appropriate triple without loss information <boeing; announced ‘the asb’ patient ‘announce’ case <the asb; announced appropriate triple <the asb; announced note restore complete passive form rather announced sufﬁcient indicating passive form downward application. another type relation noun-mediated relation. relation type noun phrase. described yahya noun-mediated relation attribute argument. consider example board meadows bank independent bank nevada’. triple noun-mediated relation <meadows bank; independent bank nevada>. triple ‘nevada’ target attribute independent bank’ ‘meadows bank’ value attribute. ‘be’ relation phrase order specify meaning attribute resulting <meadows bank; independent bank nevada>. unlike verb-mediated relations conversion noun-mediated n-ary relation binary relations merely involves spanning pairs arguments. null. argument detection classiﬁes path verb argn noun preposition classiﬁcation ﬁnds appropriate preposition arg. figure preposition classiﬁcation selects ‘in’ appropriate preposition ‘announced’ highly precise tuple extraction christensen leveraged semantic role labeling n-ary relations used extract highly precise tuples verbmediated relations. assign predicate argn labeled word roles respectively. word preposition apply assignment child retaining lemma preposition preposition classiﬁcation. consider following example addition french open nadal singles titles output ‘predicate nadal titles am-dis am-tmp assignment extracts tuple ‘rel nadal titles argn addition argn minimize tuple-extraction errors extract tuples sentences highest conﬁdence scores. deﬁne dependency-based extraction patterns extract highly precise tuples nounmediated relations patterns applied subgraphs dependency parse tree. circles arrows patterns represent words dependency relations subgraph respectively. preposition classiﬁcation retain lemma word circle circle pattern retain ‘of’. pattern matched assign words circles respectively. example dependency parse tree sentence ‘the agency located gaborone capital botswana’ seventh pattern figure matched assignment extracts tuple ‘rel capital gaborone botswana’. like verb-mediated tuple extraction figure argument detection preposition classiﬁcation given following sentences ‘boeing announced ‘meadows bank independent bank nevada’. simplicity specify dependency relations. deﬁne open tasks argument detection preposition classiﬁcation. given sentence detecting argument involves regarding certain word headword relation classifying words whether proper headwords arguments relation. input classiﬁer takes shortest dependency path connecting arg. denote path path. considering shortest dependency path connecting words concentrate informative words useful understanding relation words example figure ‘boeing’ ‘asb’ ‘the’ irrelevant determining whether proper argument ‘announced’. deﬁne four classes argument detection argn null. verb-mediated relation agent patient relation respectively argn denotes arguments. nounmediated relation value target relation respectively. classify argn case noun-mediated relation. finally null denotes term argument. figure argument detection classiﬁes path path path argn respectively. paths classiﬁed lowing constraints sentence contains lemma relation; headwords relations arguments connected linear dependency path; triples verb-mediated relations path length less seven. example acquired seed triple tuple ‘rel capital gaborone botswana’ arguments linked dbpedia entities ‘gaborone’ ‘botswana’. retrieved corpus found ‘now prime minister bechuanaland khama continued push botswana's independence newly established capital gaborone’. augmentation produces pairs cannot covered highly precise tuples. label path path path pairs highly precise tuples argn respectively. labeled paths comprise positive samples argument detection. also label path path noun prepositions comprise samples preposition classiﬁcation. samples previous stages merely indicate paths argn. describe paths null possible option negative sampling regard non-positive paths negative ones. however risks treating uncaptured positive paths negative ones. example found path uncaptured positive path label argn sentence ‘their presumption rebuffed afﬁrmed moses' uniqueness lord spoke face face’ strategy addressing problem begins observation features highly negative path contains positive path positive path contains similar positive path negative path example figure path path path highly negative paths. contain path positive node positive path. observation describe feedback negative sampling algorithm rationale behind algorithm follows model trained positive samples assigns high conﬁdence goal training augmentation sentences representing relations highly precise tuples patterns failed capture. similar ollie renoun augmentation process based seed-based distant supervision arguments seed triple appear sentence relation likely appear sentence. augmentation begins converting tuple triples. tuples verb-mediated relation convert tuple <arg; rel; arg> <arg; rel; argn> <arg; rel; argn>. tuples noun-mediated relation converted <arg; rel; arg>. among converted triples acquire seeds satisfying following constraints arguments proper nouns cardinal numbers; arguments proper noun properly linked entities dbpedia lemma relation ‘be’ ‘do’. dbpedia spotlight entity linking. seed triple sentences containing linked entities arguments proper noun surface forms arguments cardinal number. distant supervision hypothesis often erroneous include folalgorithm feedback negative sampling input non-positive paths empty negative samples model trained positive samples prediction score threshold foreach non-positive path positive paths also assigns high conﬁdence non-positive paths features. non-positive path obtain clues regarding features. path features model assigns high prediction score certain class. regard path negative sample score exceeds certain threshold. figure describes architecture neural network used argument detection. time-step network acquires input vector node path. input vector concatenation vectors following features word dependency relation named entity. leverage word embeddings trained large corpus unsupervised manner. word embeddings capture syntactic semantic information words based context corpus. pre-trained word embeddings english wikipedia corpus skip-gram model wordvec acquired word embedding matrix mword rdimword×|w| words. dependency relations provide essential information regarding syntactic structure sentence. however prevailing method pre-training dependency relation embeddings. work randomly initialized dependency relation embedding matrix mpos rdimpos×|p| mdep rdimdep×|d| sets tags dependency labels respectively. ﬁnetuned supervised manner backpropagation training. named entity recognition classiﬁes word pre-deﬁned semantic category. thus acquire semantic types words categories belong again named entity embedding matrix rdimne×|n| randomly initialized updated back-propagation training. recurrent neural network obtains previous hidden state time-step creates maintains internal memory. doing process arbitrary sequences inputs. however traditional rnns wellknown problems vanishing exploding gradients. input sequence long gradient either decay grow exponentially. long short-term memory units ﬁrst introduced hochreiter schmidhuber order tackle problem adaptive gating mechanism. among many lstm variants selected lstm peephole connections spirit gers schmidhuber furthermore forward backward directional recurrent lstm layer bi-directional architecture makes predictions based information past future. obtain bi-directional output vector rdiml time-step vector forward backward operation picks salient features along sequence vectors produce single vector longer related length sequence. subsequently fully connected layer non-linearly transforms path-level feature vector learn complex features. select hyperbolic tangent activation function obtain higher-level feature vector hhigher rdimh finally softmax output layer projects hhigher vector dimensions equivalent number classes. softmax operation applied obtain vector hout elements representing conditional probability class. neural network used preposition classiﬁcation almost model used previous section. modiﬁcations preposition classiﬁcation model. first penultimate fully connected layer model. directly connect pooling layer softmax output layer. second modiﬁcation number output classes. number classes preposition classiﬁcation depends number prepositions appear positive samples. prepositions positive samples additional class nonprepositions neural network model preposition classiﬁcation hout triple extraction begins aligning prediction results deﬁned extraction template alignment produces incomplete triples arguments relations incomplete phrases. span dependents aligned words argn ensure triples contain sufﬁcient information sentences. previous open systems assign score extracted triple. score used indicate degree correctness since extracted triples always correct. deﬁne scoring function below. extracted triple sentence dependency parsing conﬁdence score args arguments prob conditional probability softmax output. since errors path propagated ﬁnal extraction scoring function mean conditional probabilities arguments weighted dependency parsing conﬁdence score sentence. number extractions system extracted implicit relations extracting implicit relations requires analyzing context sentences rather merely setting boundaries split relations arguments sentences. despite relatively small proportion implicit relations among correct extractions indeed worth extracting contributed abundant extractions. compared system model trained samples without augmented training found extractions made properly learning relations augmented training set. open systems apart proposed system failed extract implicit relations. open heuristically converted outputs produce extractions. high reliance missed implicit relations failed capture. manner similar augmented training ollie automatically constructed training samples seed-based distant supervision. however ollie converted dependency paths connecting headwords relations arguments pattern templates. consequently ollie failed extract complex features sentences. reverb assumed arguments relations appear consecutively sentence. although assumption often correct unsuitable extracting implicit relations. next analyzed system beneﬁts bi-directional lstm networks compared sets extractions extractions model trained samples highly precise tuples extractions method using highly precise tuple extraction former contained times correct extractions latter set. moreover quality extractions considered ﬁrst contained extractions precise. analyzed effect augmenting training comparing models model trained augmented samples samples highly precise tuples figure shows augmented training contributed production times correct extractions slight evaluation settings crawled news articles randomly sampled sentences evaluation. because open extracts totally relations sentences ground-truth extractions. reason natural choice performance metric calculate precision number extractions. common metric previous open studies. extractions manually annotated correctness sorted according score descending order. system output extractions scores order clarify evaluation results. compared system three widely used open systems open ollie reverb. unlike open ollie system determine whether extractions factual. thus considered extractions open ollie comparison withdistinguishing factuality extractions. convert unary relations binary relations discarded unary relations open proposed system produced extractions open systems achieved highest precision areas regarding number extractions speciﬁcally proposed system produced times correct extractions open ollie reverb respectively. feedback negative sampling random sampling non-positive paths feedback negative sampling achieved higher precision overall. loss precision random sampling non-positive paths disagreements between positive negative samples. also analyzed input feature contributed extraction performance baseline model word feature input. added dependency relation named entity features one-by-one. higher precision achieved features combined compared word feature used. notably dependency relation feature boosted precision considerably. combining four features precision increased added advantage expanding total number correct extractions. analyzed incorrect extractions investigated source errors. according analysis errors incorrect dependency parsing. proposed system acquires dependency path input errors dependency parsing propagated throughout system. among incorrect extractions withdependency parsing errors errors argument detection preposition classiﬁcation. novel open system lstm networks produced precise abundant extractions state-of-the-art open systems. particular proposed system extracted implicit relations unlike open systems. advantages proposal stem contributions bi-directional recurrent architecture lstm units enabling extraction higher-level features containing contextual information sentence; feedback negative sampling reduces disagreements positive negative samples. best knowledge ﬁrst work apply deep learning open figure best result achieved model trained positive samples augmented training negative samples feedback negative sampling features word dependency relation named entity. analyzed quality samples augmented training set. samples pairs manually checked whether pair seed triple represented valid relation sentence. among randomly sampled pairs valid relations. among pairs invalid relations failure distant supervision assumption errors seed triples entity linking errors. references alan akbik alexander l¨oser. kraken nary facts open information extraction. proceedings joint workshop automatic knowledge base construction web-scale knowledge extraction pages montr´eal canada june. association computational linguistics. s¨oren auer christian bizer georgi kobilarov jens lehmann richard cyganiak zachary ives. dbpedia nucleus open data. proceedings international semantic asian conference asian semantic conference iswc’/aswc’ pages berlin heidelberg. springer-verlag. michele banko michael cafarella stephen soderland matt broadhead oren etzioni. open information extraction web. proceedings international joint conference artiﬁcal intelligence ijcai’. razvan bunescu raymond mooney. shortest path dependency kernel relation extraction. proceedings human language technology conference conference empirical methods natural language processing pages vancouver british columbia canada october. association computational linguistics. janara christensen mausam stephen soderland oren etzioni. analysis open information extraction based semantic role labeling. proceedings sixth international conference knowledge capture k-cap pages york usa. acm. ronan collobert jason weston l´eon bottou michael karlen koray kavukcuoglu pavel kuksa. natural language processing journal machine learning rescratch. search november. joachim daiber jakob chris hokamp pablo mendes. improving efﬁciency proaccuracy multilingual entity extraction. ceedings international conference semantic systems i-semantics pages york usa. acm. anthony fader stephen soderland oren etzioni. identifying relations open information extraction. proceedings conference empirical methods natural language processing pages edinburgh scotland july. association computational linguistics. anthony fader luke zettlemoyer oren etzioni. open question answering curated proceedings extracted knowledge bases. sigkdd international conference knowledge discovery data mining pages york usa. acm. santiago fern´andez-lanza. dependency-based open information extraction. proceedings joint workshop unsupervised semi-supervised learning pages avignon france april. association computational linguistics. gers schmidhuber. recurrent nets proceedings ieeetime count. inns-enns international joint conference neural networks ijcnn volume pages vol.. mausam michael schmitz stephen soderland robert bart oren etzioni. open language learnproceedings information extraction. joint conference empirical methods natural language processing computational natural language learning pages jeju island korea july. association computational linguistics. tomas mikolov ilya sutskever chen greg corrado jeff dean. distributed representations words phrases compositionality. burges bottou welling ghahramani weinberger editors advances neural information processing systems pages curran associates inc. andrea moro roberto navigli. integrating syntactic semantic analysis open information extraction paradigm. proceedings international joint conference artiﬁcal intelligence ijcai’. stephen soderland john gilmer robert bart oren etzioni daniel weld. open information extraction relations hours. text analysis conference tac’. daniel weld. open information proceedings extraction using wikipedia. annual meeting association computational linguistics pages uppsala sweden july. association computational linguistics. lili yunchuan chen peng jin. classifying relations long short term memory networks along shortest depenproceedings conferdency paths. ence empirical methods natural language processing pages lisbon portugal september. association computational linguistics. potential problem forward lstm layer considers information past. thus fails capture information future. address problem using additional backward lstm layer receives input sequences beginning prediction score threshold during feedback negative sampling. furthermore dimword dimpos dimdep dimne moreover diml equivalent dimensions input vector dimh much like regularization method used assigned input vector dropout rate apply softmax operation ﬁnal output natural choice training objective cross-entropy. equation training samples represents network parameters mlst denotes parameters lstm units. used adam update rule maximize training objective stochastic gradient descent shufﬂed mini-batches. adam parameters. mohamed yahya steven whang rahul gupta alon halevy. renoun fact extraction nominal attributes. proceedings conference empirical methods natural language processing pages doha qatar october. association computational linguistics. four components lstm unit forget gate candidate memory content forget input gate receive current input previous output previous memory content multiplied matrices respectively. then multiplied values summed bias result non-linearly transformed sigmoid function candidate memory content receives current input previous output multiplied matrices respectively. then multiplied values summed bias result non-linearly transformed hyperbolic tangent function tanh output gate also receives current input previous output considers current memory content rather previous memory content current memory content combination candidate memory content previous memory content weighted values input gate forget gate respectively finally current output normalized current foreign affairs committee called upon prime minister david cameron boycott event. system <the foreign affairs committee; called upon; prime minister david cameron> <the foreign affairs committee; called prime minister david cameron boycott event> <the foreign affairs committee; called boycott; event> <the foreign affairs committee; called upon; prime minister david cameron> <the foreign affairs committee; boycott; event> <the foreign affairs committee; called boycott; event> <the foreign affairs committee; called upon; prime minister david cameron> article uae's provisional constitution declares islam ofﬁcial state religion. system <article uae's provisional constitution; declares; islam ofﬁcial state religion> <kiev; capital ukraine> <it; southeast capital kiev dnieper river> <it; southeast capital kiev dnieper river> <it; southeast dnieper river capital kiev> <it; southeast capital south-central part ukraine> <it; southeast capital dnieper river> <it; capital> <it; southeast capital> <south-central; part ukraine> <it; gigantic merlion statue representing mascot national personiﬁcation singapore prominently seen promenade. system <the gigantic merlion statue; seen; prominently> explicit <the gigantic merlion statue; seen above; promenade> explicit <the gigantic merlion statue; representing; mascot national personiﬁcation singapore> explicit <the gigantic merlion statue; seen; representing mascot national personiﬁcation singapore> <the gigantic merlion statue; mascot national personiﬁcation singapore> <the gigantic merlion statue; prominently seen above; promenade> <the gigantic merlion statue; prominently seen above; promenade> <the mascot; prominently seen above; promenade> <the school; founded august <the nshe board regents; approved; year budget> <the nshe board regents; approved year budget august year budget; approved august <regents; approved; year budget> <regents; approved year budget august <the school; ofﬁcially founded august <the nshe board regents; approved; year budget> <the nshe board regents; approved; year budget> <the school; ofﬁcially founded; <the school; ofﬁcially founded nshe board regents approved year budget august <the school; ofﬁcially founded nshe board regents approved year budget august <the school; ofﬁcially founded august <the nshe board regents; approved; year budget>", "year": 2016}