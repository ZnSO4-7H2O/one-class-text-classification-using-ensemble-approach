{"title": "Client-server multi-task learning from distributed datasets", "tag": ["cs.LG", "cs.AI"], "abstract": "A client-server architecture to simultaneously solve multiple learning tasks from distributed datasets is described. In such architecture, each client is associated with an individual learning task and the associated dataset of examples. The goal of the architecture is to perform information fusion from multiple datasets while preserving privacy of individual data. The role of the server is to collect data in real-time from the clients and codify the information in a common database. The information coded in this database can be used by all the clients to solve their individual learning task, so that each client can exploit the informative content of all the datasets without actually having access to private data of others. The proposed algorithmic framework, based on regularization theory and kernel methods, uses a suitable class of mixed effect kernels. The new method is illustrated through a simulated music recommendation system.", "text": "client-server architecture simultaneously solve multiple learning tasks distributed datasets described. architecture client associated individual learning task associated dataset examples. goal architecture perform information fusion multiple datasets preserving privacy individual data. role server collect data real-time clients codify information common database. information coded database used clients solve individual learning task client exploit informative content datasets without actually access private data others. proposed algorithmic framework based regularization theory kernel methods uses suitable class mixed eﬀect kernels. method illustrated simulated music recommendation system. solution learning tasks joint analysis multiple datasets receiving increasing attention diﬀerent ﬁelds various perspectives. indeed information provided data speciﬁc task serve domainspeciﬁc inductive bias others. combining datasets solve multiple learning tasks approach known machine learning literature multitask learning learning learn context analysis inductive transfer process investigation general methodologies simultaneous learning multiple tasks important topics research. many theoretical experimental results support intuition that relationships exist tasks simultaneous learning performs better separate learning theoretical results include extension multi-task setting generalization bounds notion vc-dimension methodology learning multiple tasks exploiting unlabeled data importance combining datasets especially evident biomedicine. pharmacological experiments training examples typically available speciﬁc subject technological ethical constraints makes hard formulate quantify models experimental data. obviate problem so-called population method studied applied success since seventies pharmacology population methods based knowledge subjects albeit diﬀerent belong population similar individuals data collected subject informative respect others population approaches belongs family so-called mixed-eﬀect statistical methods. methods clinical measurements diﬀerent subjects combined simultaneously learn individual features physiological responses drug administration population methods applied success also biomedical contexts medical imaging bioinformatics classical approaches postulate ﬁnite-dimensional nonlinear dynamical systems whose unknown parameters determined means optimization algorithms strategies include bayesian estimation stochastic simulation nonparametric population methods information fusion diﬀerent related datasets widespread also econometrics marketing analysis goal learn user preferences analyzing user-speciﬁc information information related users e.g. so-called conjoint analysis aims determine features product mostly inﬂuence customer’s decisions. collaborative approaches estimate user preferences become standard methodologies many commercial systems social networks name collaborative ﬁltering recommender systems e.g. pioneering collaborative ﬁltering systems include tapestry grouplens referralweb phoaks recently collaborative ﬁltering problem attacked machine learning methodologies bayesian networks mcmc algorithms mixture models dependency networks maximum margin matrix factorization coming back machine learning literature single-task context much attention given last years non-parametric techniques kernel methods gaussian processes approaches powerful theoretically sound mathematical foundations regularization theory inverse problems statistical learning theory bayesian estimation ﬂexibility kernel engineering allows estimation functions deﬁned generic sets arbitrary sources data. methodologies recently extended multi-task setting. general framework solve multi-task learning problems using kernel methods regularization proposed relying theory reproducing kernel hilbert spaces vector-valued functions many applications real-time processing examples required. on-line multitask learning schemes natural application data mining problems involving large datasets therefore required scale well number tasks examples. on-line task-wise algorithm solve multi-task regression problems proposed. learning problem formulated context on-line bayesian estimation e.g. within gaussian processes suitable covariance functions used characterize non-parametric mixed-eﬀect model. features algorithm capability exploit shared inputs tasks order reduce computational complexity. however algorithm centralized structure tasks sequentially analyzed able address neither architectural issues regarding information privacy protection. paper multi-task learning distributed datasets addressed using client-server architecture. scheme clients one-to-one correspondence tasks individual database examples. role server collect examples diﬀerent clients order summarize informative content. example associated task becomes available server executes on-line update algorithm. diﬀerent tasks sequentially analyzed architecture presented paper process examples coming order diﬀerent learning tasks. summarized information stored disclosed database whose content available download enabling client compute estimate exploiting informative content datasets. particular attention paid conﬁdentiality issues especially valuable commercial recommender systems e.g. first require speciﬁc client cannot access clients data. addition individual datasets cannot reconstructed disclosed database. kind clients considered active passive ones. active client sends data server thus contributing collaborative estimate. passive client downloads information disclosed database without sending data. regularization problem parametric bias term considered mixed-eﬀect kernel used exploit relationships tasks. albeit speciﬁc mixed-eﬀect nonparametric model quite ﬂexible usefulness demonstrated several works paper organized follows. multi-task learning regularized kernel methods presented section class mixed-eﬀect kernels also introduced. section eﬃcient centralized oﬀ-line algorithm multitask learning described solves regularization problem section section rather general client-server architecture described able eﬃciently solve online multi-task learning distributed datasets. server-side algorithm derived discussed subsection client-side algorithm active passive clients derived subsection section simulated music recommendation system employed test performances algorithm. conclusions paper. appendix contains technical lemmas proofs. notice vectors deﬁned paper necessarily elements vector space. deﬁnition vector adopted paper similar used standard object-oriented programming languages c++. function independent regarded sort average task whereas non-parametric individual shift. value related shrinking individual estimates toward average task. function learned tasks examples referred unique task hand tasks learned independently tasks related all. many applications multi-task learning input data shared tasks number diﬀerent basis functions appearing expansion considerably less explained below feature exploited derive eﬃcient incremental online algorithms multi-task learning. introduce vector unique inputs next result shows coeﬃcients obtained solving system linear equations involving small-sized matrices complexity depends number unique inputs rather total number examples. algorithm oﬀ-line procedure whose computational complexity scales following section client-server on-line version algorithm derived preserves complexity bound. complete computational scheme reported algorithm initialization deﬁned resorting empty matrices whose manipulation rules found particular initialized empty matrix. respect assumed functions bias return empty matrix output applied empty matrices. algorithm mainly based matrix factorizations matrix manipulation lemmas appendix. rest subsection extensive proof devoted show algorithm correctly updates relevant quantities triple becomes available task three cases possible applying formulas p-th data task lines algorithm obtained. check correctly updated lines algorithm observe that taking account deﬁnition applying lemma algorithm reorganized algorithm ﬁnal part algorithm coincides algorithm however case input also requires updating factors matrix assume strictly positive diagonal lower triangular. strictly positive kinds decompositions used. particular linear obtain coeﬃcients algorithm access undisclosed data required. nevertheless shown next client compute estimate without access undisclosed data. even necessary required information contained disclosed quantities client point view knowledge equivalent knowledge turn also computed using factorization deﬁnition assumption mentioned introduction kind clients considered. disclosed data vector obtained client still needs individual coeﬃcients vector order perform predictions task. active client simply receive vector server passive client must compute independently. interestingly turns computed knowing disclosed data together private data task indeed line algorithm decouples respect diﬀerent tasks section proposed algorithm applied simulated music recommendation problem order predict preferences several virtual users respect artists. artist data obtained audioscrobbler database dump last dump released audioscrobbler/lastfm creative commons license. lastfm internet radio provides individualized broadcasts based user preferences. database dump includes users playcounts artists names possible rank artists according global number playcounts. sorting artists according decreasing playcounts ranking artists selected. simulated music recommendation system relies music type classiﬁcation expressed means tags particular main tags lastfm considered. i-th artist associated vector tags whose values obtained querying lastfm i.e. outperforms separate pooled approaches. interestingly performances remain fairly stable range values figure shows distribution hitsj users correspondence values achieving optimal rmse. although selected minimize rmse remarkably good performances obtained also respect tophits score finally true estimated top- hits reported average user representative users artists true top- correctly retrieved estimated top- reported bold-face. concerning computational burden worth observing without exploiting presence repeated inputs mixed-eﬀect structure kernel complexity naive approach would order cube recent studies highlighted potentialities kernel methods applied multi-task learning eﬀective implementation involve solution architectural complexity issues. paper emphasis posed architecture reference learning distributed datasets. general class kernels mixed-eﬀect structure shown optimal solution given collaborative client-server architecture enjoys favorable computational conﬁdentiality properties. interacting server client solve estimation task taking advantage data clients without direct access them. client’s privacy preserved since active passive clients allowed architecture. former agree send data server latter exploit information server without disclosing private data. proposed architecture several potential applications ranging biomedical data analysis", "year": 2008}