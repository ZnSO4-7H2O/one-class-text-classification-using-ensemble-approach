{"title": "Recurrent-Neural-Network for Language Detection on Twitter  Code-Switching Corpus", "tag": ["cs.NE", "cs.CL"], "abstract": "Mixed language data is one of the difficult yet less explored domains of natural language processing. Most research in fields like machine translation or sentiment analysis assume monolingual input. However, people who are capable of using more than one language often communicate using multiple languages at the same time. Sociolinguists believe this \"code-switching\" phenomenon to be socially motivated. For example, to express solidarity or to establish authority. Most past work depend on external tools or resources, such as part-of-speech tagging, dictionary look-up, or named-entity recognizers to extract rich features for training machine learning models. In this paper, we train recurrent neural networks with only raw features, and use word embedding to automatically learn meaningful representations. Using the same mixed-language Twitter corpus, our system is able to outperform the best SVM-based systems reported in the EMNLP'14 Code-Switching Workshop by 1% in accuracy, or by 17% in error rate reduction.", "text": "joseph chee chang chu-cheng language technologies institute school computer science carnegie mellon university pittsburgh josephcc chuchenglincs.cmu.edu mixed language data difﬁcult less explored domains natural language processing. research ﬁelds like machine translation sentiment analysis assume monolingual input. however people capable using language often communicate using multiple languages time. sociolinguists believe code-switching phenomenon socially motivated. example express solidarity establish authority. past work depend external tools resources part-of-speech tagging dictionary look-up named-entity recognizers extract rich features training machine learning models. paper train recurrent neural networks features word embedding automatically learn meaningful representations. using mixed-language twitter corpus system able outperform best svm-based systems reported emnlp’ code-switching workshop accuracy error rate reduction. code–switching refers phenomenon speaker changes different languages single utterance conversation. sociolinguists believe people code–switch sociolinguistic motivations e.g. express solidarity familiarity establish authority distance. also shown code–switching grammatical regularities switching points almost never occur certain points. implies using sophisticated models identify structures mixed-language sentence potentially help language detection task. also computational models code–switching thus used verify linguistic theories. nonetheless also used guide downstream applications correct language models practical problem current social media processing. there’s recently workshop shared task modeling code–switching social media. participants provided code–switched tweets four language pairs. asked label tweets language token level. strongly resembles established sequential labeling problems tagging named-entity recognizer almost participants adapted either conditional random ﬁelds model support vector machine model results general. nepali–english spanish–english participants able score around however many participants efforts extensive careful feature engineering. features used shared task include dictionaries character word language models even output systems language identiﬁcation systems. despite efforts outcomes always positive. example previous work found existing systems translate improvement named entities submission comparison proposed rnn–based classiﬁer takes character n–grams pre–trained embeddings texts. system competitive best results reported shared task. paper structured follows. sec. review related work literature. sec. describe code–switched tweets annotation scheme detail. sec. describe rnn–based model. experiment results sec. section review previous methods language identiﬁcation monolingual multilingual texts computational models code switching. also review previous work neural networks processing natural language re-representing words semantic vector space. monolingual language identiﬁcation. monolingual language identiﬁcation generally treated text categorization problem often deﬁned given text assign label predeﬁned languages task. baldwin excellent review. sake completeness include relevant work here. featurizing text problem treated standard supervised learning problem. indeed many well known classiﬁers naive bayes kernel methods used literature. featurizing text important subproblem. existing featurization include per–language character frequency n–grams linguistically motivated features stop word lists. people also achieved excellent accuracy lower–level features byte sequence. multilingual langid. monolinguality document however always realistic. example researchers found people posting tweets using multiple languages time. used topic modeling model languages occur document. people also used sequential labeling algorithms identify language segments document king abney computational models code–switching. monolingual multilingual language identiﬁcation seek assign label single document sentence. computational models code– switching hand takes step further tries pinpoint switching languages happens. equivalently code–switching assigns language label token level stead sentence level document level. linguists proposed syntactic theories predicts locations code–switching time prohibits happening certain places. theories incorporated computational models recurrent-neural-network natural language processing sequential natural natural language data many problems natural language processing form tagging traditionally solved using linear hidden markov model frequently used state-of-the-art machine learning models include chained-conditional random fields maximum-entropy markov model sometimes support vector machines recently researchers also begun explore using architectures rival even outperform machine learning approaches. mesnil project -hot word vectors using task-speciﬁc supervised embedding layer experimented using elman-type jordan-type rnns. results show using exact features rnns outperform state-of-the-art crf-based model score. work extend structure include additional character ngram features pre-trained generalized embedding layer. results show proposed architecture trained simple lexical features outperform state-of-the-art svm-based systems trained rich features generated using external tools named entity recognizers dictionaries. experiments conducted twitter data provided emnlp code–switching workshop twitter’s privacy policy organizers allowed provide tweets themselves. instead participants provided unique tweet character offsets. then participants crawl data themselves. situations like deletion privacy setting changes made users initial crawl able crawl tweets total. language–pair breakdown listed table although main goal shared task predict code–switching points data annotated ﬁner–grained scheme listed table noted large variance label’s frequency. mixed ambiguous despite signiﬁcance linguistic theory bilinguallism appear happen often social media. hand named entities occurs frequently extent main deciding factor participants’ overall performance since lang lang relatively easy classify. stated sec. decide make task realistic training classiﬁer considers possible languages. therefore single corpus tweets. number tokens language table proposed network architecture based elman-type jordan-type recurrent neural network. follow implementation mesnil -word window capture immediate context forming dimention context vector supervised word embedding layer used project word onto dimension real value vector. second layer node hidden layer using sigmoid function. finally softmax function output layer. long-term dependencies captured using either elman-type jordan-type recurrent structure current output depends output previous hidden layer ﬁnal layer respectively. extending architecture incorporate character ngram features mentioned technique extract -dimension -hot vectors current word append dimension context vector embedding layer project onto real value vector space. incorporate pre-trained wordvec model current word look model feed projected real value vector directly hidden layer. another alternative approach replace supervised embedding layer completely pre-trained wordvec model think general embedding model task speciﬁc embedding model generalize method. following subsection talk pre-trained wordvec model also extract simple character ngram features. evaluation section show performance model trained different network structures using different feature combinations. employ skip–gram embeddings features. skip–gram word embeddings mikolov bilinear model encourages words similar contexts similar embeddings. implementation gensim trained large twitter corpus random samples live feed. randomly sample tweets spanning roughly days. ﬁlter speciﬁc languages. idea words different languages tend share different contexts. embeddings provide good separation languages. proved provide improvement code–switching task character n–grams prove valuable features languages often distinct character combinations. example word starts likely spanish english. conversely word ends tion likely english. features widely used existing programs ldig. approach extract number character ngram word. window characters extract character bigrams trigrams begining word resulting dimention character ngram vector. example character ngram vector word architecture test effectiveness different neural network structures ﬁrst smaller data preliminary study. besides testing elman-type jordan-type structures also tested effectiveness proposed extensions. preliminary study dataset contains total tweets training validation rest evaluation. jordan+ngram jordan model additional -hot character ngram vector. elman+ngram elman model additional -hot character ngram vector. elman+ngram+wv elman+ngram conﬁguration addtional pre-trained shown ﬁgure basic architecture jordan-type elman-type achieved similary performance adding character ngrams improves performance architectures roughly best performing system uses elman architecture character ngrams pre-trained wordvec performed score adding pre-trained wordvec features directly hidden layer provided additional improvement. suggests comparing basic structures proposed extensions improve performance roughly language identiﬁcation task. validation. acknowledge using different test evaluate systems comparing results systems form workshop. also separate tweets training validation evaluation sets using disjoint sets authors. nevertheless test data test data workshop collected using exact fashion none overlapping sets authors training set. emnlp code–switching workshop provided simple deterministic baseline categories. given word looks training corpus pick frequent language label {lang lang} label. returns other. case returns language frequent. according overview paper workshop teams shared task adapted model language identiﬁcation. best nepali–english submission barman used classiﬁcation framework. used character n–grams context capitalization word length dictionaries features. best spanish–english submission also used main classiﬁer. used character n–grams context dictionaries. additionally also take language model probabilities features. table. show performance full systems. compare results reported workshop evaluate system accuracy different categories best performing systems reported emnlp’ code-switching workshop. used tweets train networks need rest training validation testing. using signiﬁcantly smaller training comparing previous systems access full training data tweets provided workshop. figure language identiﬁcation accuracy comparing baseline best reported systems emnlp’ workshop. parenthesis indicates size training number tweets. similar preliminary experiment results elman-type jordan-type rnns performed similarly. comparing state-of-the-art systems networks able perform higher accuracy english-spanish category reduction error rate. english-nepali category jordan-type network performed higher accuracy reduction error rate. shows recurrent neural networks embedding layers able learn meaningful representations language detection using lexical features able rival svm-based systems depend sophisticated feature extraction re-represent input data. paper tackled important natural language processing task language detection code-switching twitter corpus using recurrent neural networks. novel application previous research focused using machine learning methods chained conditional random ﬁelds solve type problems. fact emnlp code-switching workshop participating team used build models best performing teams used svm-based model. previous work already compared rnns crf-based model natural language processing results show rnns produce better accuracy named entity recognition task. also tested found proposed extensions rnns also outperform state-of-the-art svm-based system accuracy error rate reduction using simpler features smaller training data. work initiated deep learning course language technologies institute carnegie mellon university offered bhiksha raj. authors would also like thank teaching assistant zhenzhong insightful discussions. references baldwin timothy marco. language identiﬁcation long short matter. human language technologies annual conference north american chapter association computational linguistics stroudsburg association computational linguistics. isbn ---. http//dl. acm.org/citation.cfm?id=.. dershowitz nachum. proceedings first workshop computational approaches code switching chapter aviv university system code-switching workshop shared task association computational linguistics http//aclweb.org/anthology/w-. barman utsab wagner joachim chrupała grzegorz foster jennifer. proceedings first workshop computational approaches code switching chapter dcu-uvt word-level language classiﬁcation code-mixed data association computational linguistics http//aclweb.org/anthology/w-. berk-seligson susan. linguistic constraints intrasentential code-switching study spanish/hebrew bilingualism. language society issn http//www.jstor.org/stable/. cantone katja francesca m¨uller natascha. nase nase¡/i¿? gender marking within switched reveals architecture bilingual language faculty. lingua chew choong mikami yoshiki nagano robin lee. language identiﬁcation pages based improved n-gram algorithm. international journal computer science issues gumperz john discourse strategies. studies interactional sociolinguistics. cambridge university press isbn http//books.google.com/books? id=aujnghwl_koc. king levi baucom eric gilmanov timur k¨ubler sandra whyatt maier wolfgang rodrigues paul. iucl+ system word-level language identiﬁcation extended markov models. proceedings first workshop computational approaches code switching doha qatar october association computational linguistics. http//emnlp.org/workshops/codeswitch/call.html. kruengkrai srichaivattana sornlertlamvanich isahara language identiﬁcation communications information technology iscit based string kernels. ieee international symposium volume ./iscit.. kudo taku matsumoto yuji. chunking support vector machines. proceedings second meeting north american chapter association computational linguistics language technologies association computational linguistics ying fung pascale. language modeling functional head constraint code switching proceedings conference empirical methods natural speech recognition. language processing association computational linguistics http//aclweb.org/anthology/d-. chu-cheng ammar waleed levin lori dyer chris. submission shared proceedings first workshop task language identiﬁcation code-switched data. computational approaches code switching doha qatar october association computational linguistics. http//www.aclweb.org/anthology/ ling wang xiang guang dyer chris black alan trancoso isabel. microblogs parallel corpora. proceedings annual meeting association computational linguistics association computational linguistics http//aclweb.org/anthology/p-. marco baldwin timothy. langid.py off-the-shelf language identiﬁcation tool. proceedings system demonstrations stroudsburg association computational linguistics. http//dl.acm.org/citation. cfm?id=.. solorio thamar blair elizabeth maharjan suraj bethard steven diab mona gohneim mahmoud hawwari abdelati alghamdi fahad hirschberg julia chang alison overview ﬁrst shared task language identiﬁcation code-switched data. emnlp", "year": 2014}