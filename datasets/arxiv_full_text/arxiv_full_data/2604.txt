{"title": "Error Asymmetry in Causal and Anticausal Regression", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "It is generally difficult to make any statements about the expected prediction error in an univariate setting without further knowledge about how the data were generated. Recent work showed that knowledge about the real underlying causal structure of a data generation process has implications for various machine learning settings. Assuming an additive noise and an independence between data generating mechanism and its input, we draw a novel connection between the intrinsic causal relationship of two variables and the expected prediction error. We formulate the theorem that the expected error of the true data generating function as prediction model is generally smaller when the effect is predicted from its cause and, on the contrary, greater when the cause is predicted from its effect. The theorem implies an asymmetry in the error depending on the prediction direction. This is further corroborated with empirical evaluations in artificial and real-world data sets.", "text": "abstract. generally difﬁcult make statements expected prediction error univariate setting without knowledge data generated. recent work showed knowledge real underlying causal structure data generation process implications various machine learning settings. assuming additive noise independence data generating mechanism input draw novel connection intrinsic causal relationship variables expected prediction error. formulate theorem expected error true data generating function prediction model generally smaller effect predicted cause contrary greater cause predicted effect. theorem implies asymmetry error depending prediction direction. corroborated empirical evaluations artiﬁcial real-world data sets. nature prediction problem quite complex. many factors lead observed data underlying distributions causal relationships variables. general statement expected error given problem therefore tough task. especially domain health care precise estimator indispensable ensure right conclusions treatment patient. order make statement expected error typical approach analyze data properties properties utilized algorithm detail. example facing noisy classiﬁcation problem chance sample belongs class chance class best guess unknown sample would always class therefore exists risk misclassiﬁcation. analog statement possible regression problems exists inﬁmum error noise affecting data. paper show inﬁmum depends prediction direction. emphasize optimal estimator w.r.t. risk minimization necessarily minimize prediction error. therefore detailed knowledge underlying problem give additional useful information area machine learning also regards general prediction tasks. independence mechanism cause. work based ideas claimed that certain assumptions underlying causal directions variables important implications various machine learning scenarios. similar address setting observable variables variable cause variable effect. analyze expected prediction error regression tasks true data generating function utilized prediction model draw attention fact factors expected error fundamentally different effect predicted cause cause predicted effect. error generally depends heavily noise actual shape underlying function generated data another crucial factor differs prediction directions. recognized past explicitly pointed paper. particular formulate fundamental theorem states asymmetry prediction error regard prediction direction variables exists. graphical models provide framework describe causal structure variables represent joint distribution variables vertices directed acyclic graph direct causal inﬂuence variable variable indicated arrow vertices. theoretical analysis consider variable model observable cause variable observable effect variable latent noise variable noise affects effect assumed independent denote probability distribution random variable corresponding density simplicity denotes either density distribution density value respect depending context. notation also used joint conditional distributions. densities assumed strictly positive causal relationship cause effect deﬁned mechanism determines effect given cause noise illustrated figure assume observed data generated following intrinsic causal relationship. following term mechanism conditional general prediction problems goal predict target variable predictor variable prediction models typically learn mapping based conditional deﬁned fig. illustration simple causal structure variables; cause effect effect inﬂuenced noise assumed independent cause effect exists mechanism determines effect. effect predicted cause cause predicted effect observed data sampled joint distribution knowledge causal relationship rarely available requires additional domain knowledge. available causal direction inferred causal inference methods determine whether cause effect respectively. knowledge causal direction particularly important domain biomedicine e.g. provides better understanding relationship symptoms diseases. analysis causal relationship assumed known rather focus implications prediction. example simple example causal problem would predict street based observation whether raining problem clear street becomes soon starts rain. therefore clear causal relationship rain cause street effect. unlikely observe street without rain occurrences happen street observed even raining. seen noise inﬂuencing effect. simplicity assumed street instantly dries soon rain stopped dependency causal conditional given table anticausal conditional table probability observing street without rain therefore conditional represents rain problem anticausal perspective task would predict raining based whether street dry. note occur table heavily depends error analysis simple binary example already reveals fundamental difference predicting causal predicting anticausal direction. street always rain matter likely rain. hand inferring rain based observation street highly depends likeliness rain comparing conditionals respect risk misclassiﬁcation error source case noise case product therefore different risks misclassiﬁcation expected depending causal prediction direction. motivated simple binary example want identify difference error source predicting causal anticausal direction regression problems. this analyze expected prediction error terms expected loss true data generating function serves prediction model. following consider continuous real valued variables squared error loss function. deterministic oracle function. cause assumed normalized without loss generality since matter scaling. cause effect variables assumed share unobserved common causes also known causal sufﬁciency assumption anms particularly utilized causal inference ﬁtted true causal direction aside exceptions linear functions gaussian noise inverse oracle function assumed deﬁned whole domain practice often assumed much smaller compared var]. assumption true causal relationships mostly carelessly ignored prediction models. nevertheless causal structure crucial implications prediction pointed paper. independence mechanism cause independence mechanism cause crucial assumption concerning work. here independence different classical statistical deﬁnition postulates cause distribution independent mechanism therefore information particular changing inﬂuence vice versa. hand effect distribution contain information assumption also related autonomous data generation process exogeneity assuming deterministic oracle function strictly monotonically increasing diffeomorphism independence assumption formulated terms positive dependency slope distribution cause important formulation statistical properties represents measure dependency might helpful rather density distribution function integrated support note postulate implies illustrated figure similar formulation also possible postulating log) independent allows several information theoretic interpretations contains information mechanism independence assumption seems plausible considering aforementioned rain problem. mechanism shown table generating street designed independently it’s input distribution therefore changing cause distribution thus probability observing rain inﬂuence mechanism. soon raining street conditional remain matter likely observe rain all. hand conditional table indicates observing street highly depends likely rain all. changes effect distribution changes cause distribution remains same. dependency cause distribution depends properties well captured conditionals therefore probabilities misclassiﬁcation causal anticausal direction show asymmetrical behavior. following analyze implications independence assumption causal anticausal regression problems terms expected prediction error. anticausal prediction problem goal learn inverse oracle function order predict cause effect since assume represents true relationship variables. according inﬂuenced noise hence predicted cause noisy too. already leads three important aspects. first prediction also affected noise effect thus ordinary regression techniques least squares regression fail accurately predict noise free cause generated data. second needs invertible accurate anticausal predictions. injective restricted domains surjective domains general information loss predicting anticausal direction. third accurate estimation anticausal direction obtained inverting causal direction. latter discussed section already mentioned section assume strictly monotonically increasing diffeomorphism convenient analysis according diffeomorphism assumption signiﬁcantly weakened constraining almost everywhere differentiable figure regarding intrinsic structure prediction problem optimal prediction possible oracle function captures structure used estimator. natural variability system stochastic noise inﬂuencing effect rarely possible infer oracle function. further even oracle function used noise represents irreducible error. given estimator input value target value prediction quality estimator measured loss function squared error lseφ) typical loss function regression problems. implicitly assumed joint distribution reﬂects underlying problem. densities seen weighting factors error. intuitively error higher weighted likely observe further likely observe given many applications prediction cause rather effect interest. instance predicting disease caused certain symptom. anticausal prediction scenarios called calibration models provide several techniques predict cause based effect idea many calibration models model invert order obtain model anticausal direction. referred inverse regression. hand could also simply regression model referred reverse regression sometimes direct regression however difference inverse reverse regression. literature regarding comparison somewhat underdeveloped understanding difference important. work provide comparison inverse fig. simple example least squares solution inverse prediction highly biased represent regression problem deﬁned true prediction model given also coincides least squares solution. corresponding reverse regression point solution reverse regression biased solution inverse regression. many data scientists naively perform reverse regression simplicity lack awareness difference inverse regression preferred anticausal prediction problems minimize risk wrong conclusions. seeing this inverse regression anticausal predictions provide thereby contribution comparison. general relation cause effect according independence assumption deﬁned oracle function thus reﬂects inverse relation. obvious deterministic case without noise however assume additive noise effect case inferring causal oracle function deﬁnes general problem regression tasks instance solved minimizing loss function. example least squares solution causal direction coincides simple example would illustrated figure least squares solution clearly different true inverse relationship deﬁned instance desired optimal inverse prediction would least squares solution gives seeing this inverse regression ﬁrst estimating causal function invert order obtain estimation inverse function suggested clearly better choice. also shows knowing causal direction provide useful information prediction reduce risk fatal decision patient treatment misinterpretation prediction results. mentioned before will thus consider inverse regression instead reverse regression anticausal predictions show next section that using true function expected error causal prediction hence different expected error anticausal prediction additive noise. difference captured expected least squared error causal anticausal predictions. subscript ·e|c indicates effect predicted cause. here used additive noise assumption terms independent joint density equal since deterministic assumption sufﬁciently small noise rest error neglected natural assumption regression problems. since strictly monotonically increasing diffeomorphism. note represents lower bound expected prediction error inverse regression necessarily coincide lower bound reverse regression. comparing already reveals fundamental difference; expected error causal prediction independent anticausal predictions heavily depends possible given case linear. also becomes strictly equal linear case. non-linear expression becomes greater alternative interpretation ﬁrst case minimizes squared error causal direction respect noise guarantee minimize expected squared error anticausal direction stated therefore additive noise higher negative impact expected error anticausal predictions compared causal predictions. note choice squared error loss function intrinsic relation captured squared error. theorem strictly monotonically increasing diffeomorphism hold expected error oracle function causal regression smaller equal expected error inverse oracle function anticausal regression. assumptions hold theorem states asymmetry prediction error respect whether cause predicted effect effect cause w.r.t. data generating function. extension implications independence postulate theorem implies positive dependency slope error anticausal prediction causal prediction. asymmetry theoretical practical implications various domains. brieﬂy discussed section corollary normalized hold expected error oracle function causal regression strictly smaller expected error inverse oracle function anticausal regression. emin ne)min emax ne)max denote domain minimum domain maximum respectively. practice approximately given normalizing observed effect data. noise range becomes note corollary particularly interesting seeing smaller error causal regression expected cause effect data scaling. ﬁrst evaluated theorem artiﬁcial data sets fulﬁll independence additive noise assumption. this performed evaluations different settings. ﬁrst setting assumed oracle function inverse known. second setting causal direction oracle function assumed unknown. further used real-world data sets robust made assumptions general theorem general root mean squared errors predicting causal anticausal direction compared. theorem holds rmse predicting causal direction smaller equal rmse predicting anticausal prediction. artiﬁcial data sets generated uniformly distributed cause data values chose functions monotonically increasing diffeomorphisms. oracle function normalized terms attaining values parameters deﬁned additive noise gaussian distributed. therefore cause data noise data effect data generated following causal direction known ﬁrst setting assume know oracle function true causal direction. here want evaluate theorem holds optimal setting. every function tested different values noise variance data samples averaged runs. according theorem rmse difference increase increase noise variance fig. overview rmse predicting causal anticausal direction artiﬁcial data sets. ﬁgure captions denote corresponding function cases rmse causal prediction smaller equal rmse anticausal prediction. difference becomes bigger higher variance noise higher degree non-linearity note different nature functions values thus axes differently scaled. degree non-linearity linear case rmse causal anticausal prediction equal direct consequence table shows results linear function support conclusion. rmse predicting causal anticausal direction perfectly represent variance noise. case exponential function inverse given results different values increasing noise variance summarized figures also results conform theorem. rmse causal direction always approximately represents noise variance rmse anticausal direction always bigger causal direction noise variance greater inverse asin· here rmse difference also increases increased noise variance less extreme e.g. case figure seeing figure equation explained higher slope exponential function sinus function. case exponential function data points fall regions small slope case sinus function slightly data points fall regions small slope regions high slope. last function power function inverse deﬁned figures show results various values again results conform theorem similar results exponential function. theorem holds data sets. prediction error causal direction always smaller equal prediction error anticausal direction. expected magnitude error difference greatly depends noise variance degree non-linearity higher non-linearity and/or noise variance higher difference. causal direction unknown setting assume true causal direction unknown. evaluations investigate robust theorem respect merely obtaining estimation might realistic scenario. evaluation generate data similar aforementioned data generation process since assume know causal direction normalized whole effect data instead functional output. regression model utilized smoothing spline ﬁtted least-squares error minimization causal directions. smoothing spline ﬂexible regression model less sensitive choice parameters e.g. gaussian process regression. noise variance varied range however want clarify theorem makes statement expected error least-squares solution anticausal direction expected error according experiment justiﬁed idea least-squares solution approximately represents anticausal direction noise variance sufﬁciently small. cases noise variance covered theorem rather serve gaining additional insights towards error asymmetry respect least-squares solutions. further experiments indicate theorem applied practical domain causal inference. figure summarize rmse functions exp. according corollary rmse causal regression smaller rmse anticausal regression experimental results capture well even though theorem provide statements highly noisy data. figures show examples least-squares solution causal anticausal direction rmse causal solution around rmse anticausal solution around difference case might evaluations show systematic asymmetry. fig. overview rmse using least-squares solutions causal anticausal direction prediction. ﬁgure captions denote corresponding function cases rmse predicting causal direction smaller equal rmse predicting anticausal direction. difference becomes bigger higher variance noise. example figures show least-squares solutions causal anticausal direction respectively. evaluations real-world data sets used causeeffect real-world benchmark data sets causal inference. data collection various cause effect pairs provide knowledge true causal relationship variables. description found real-world data true generally unknown therefore assumed power function power function provides monotonic increasing behavior offers direct interpretation parameter measure non-linearity ﬂexible e.g. exponential function. used data sets total omitted data sets multiple variables data sets extreme performance causal anticausal prediction. further data approximately monotonically decreasing sign effect data changed similar second experiment artiﬁcial data sets normalized cause effect data based normalized data parameters power functions estimated causal direction. parameters also used inverse model differ optimal least-squares solution. note since function roughly approximates true oracle function normalization inaccurate thus parameter also needs estimated. evaluations artiﬁcial data sets compared rmse predicting causal anticausal direction using estimated power function. data sets rmse causal prediction smaller anticausal prediction. overview results given figure parameter model indicates degree non-linearity. error difference cases marginally favor anticausal regression increases slightly increase explained small ﬂuctuations noise inaccurate model estimations. accurate estimation true reveal clear performance gap. overall surprisingly results substantially support theorem respect observation data always fulﬁll additive noise assumption. further cause effect probably many unobserved common causes seem irrelevant theorem. indicates robustness violation made assumptions. note evaluation regarding comparison rmse least-square solution real-world data could interesting possible application causal inference justiﬁed theorem uncontrollable experiment setting violations made assumptions. analysis regarding causal inference might interesting future work beyond scope paper. expected prediction errors various machine learning methods extensively studied past. however classical learning theories mainly focused general nature statistical algorithms data dependent properties consider connection causality generation process data corresponding implications prediction error. terms considering causal nature problem points causal direction data matters transfer learning tasks goal transfer knowledge data another similar data set. related work respect independence assumption done brought asymmetry independence assumption explicitly context semi-supervised learning. argumentation additional input data sampled increase prediction performance causal problems information mechanism contrary additional samples improve performance anticausal data seeing effect case therefore information mechanism. asymmetry also exploited terms causal inference idea infer true causal direction variables comparing variable higher correlation drawback method theoretical analysis allows deterministic relationship cause effect without noise. work particularly point asymmetric relationship expected prediction error causal prediction direction speciﬁcally allowing noise. best knowledge explored previously. inverse reverse regression already mentioned section direct least squares estimator anticausal direction higher bias inverted least squares solution causal direction. early work comparing inverse reverse regression argue reverse regression preferred since minimizes squared prediction error however conclusion criticized many researchers argued conclusions made based reverse regression reﬂect actual relationship cause effect obvious regarding statement boundary expected error inverse regression clear. therefore theorem gives direct contribution discussion since provides statement lower boundary prediction error respect further violation theorem probably indication highly biased estimator. calibration models calibration models introduced section suggest invert prediction model causal direction inverse predictions tackle problem biased predictions. theorem provides general relationship prediction errors causal model inverse domain calibration models. extent knowledge general relationship provided previous work domain without assumptions model. terms future work calibration models theorem might useful developing approaches estimating rather complex models easily obtained inverting seeing obtained least squares solucausal inference theorem also exploited causal inference principle based comparison prediction errors. direct usage could scenario data observed function known beforehand expert knowledge causal direction. problem setting would determine whether kind scenario could e.g. interesting protein interactions functional relationship might known causal direction. function inverted theorem provides direct determine cause effect based prediction error. also experiments indicate direct application theorem causal inference might already possible simple comparison rmse least-squares solution causal anticausal direction assumption small error noise. further positive dependency error slope anticausal causal direction could also exploited. particularly interesting causal inference methods test independence input prediction error comparison error slope offer general setting regarding assumptions paper addressed implications optimal prediction causal anticausal regression problems explicitly allow noise affecting effect. independence assumption additive noise model intrinsic causal structure give crucial information prediction capabilities statistical learning models also general prediction tasks. recognized past. based theoretical analysis concluded theorem states expected prediction error causal regression problems smaller equal anticausal problems respect normalized oracle function it’s inverse implies dependency error slope anticausal causal direction. empirically evaluated theorem various artiﬁcial realworld data sets give supporting results indicate made assumptions quite robust violations. result signiﬁcantly contribute theoretical practical work. future work explore error asymmetry order provide principle causal inference. this also work generalization result allows dependency input variable noise variable make applicable larger range problems. another interesting aspect could error causal anticausal classiﬁcation problems different factors regression problems. extension multivariate case also interest future work supported jsps kakenhi center innovation program japan science technology agency jst. ﬁnal publication available springer http//dx.doi.org/./s--z.", "year": 2016}