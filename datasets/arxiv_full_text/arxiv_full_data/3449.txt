{"title": "Regularizing deep networks using efficient layerwise adversarial  training", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Adversarial training has been shown to regularize deep neural networks in addition to increasing their robustness to adversarial examples. However, its impact on very deep state of the art networks has not been fully investigated. In this paper, we present an efficient approach to perform adversarial training by perturbing intermediate layer activations and study the use of such perturbations as a regularizer during training. We use these perturbations to train very deep models such as ResNets and show improvement in performance both on adversarial and original test data. Our experiments highlight the benefits of perturbing intermediate layer activations compared to perturbing only the inputs. The results on CIFAR-10 and CIFAR-100 datasets show the merits of the proposed adversarial training approach. Additional results on WideResNets show that our approach provides significant improvement in classification accuracy for a given base model, outperforming dropout and other base models of larger size.", "text": "adversarial training shown regularize deep neural networks addition increasing robustness adversarial examples. however impact deep state networks fully investigated. paper present efﬁcient approach perform adversarial training perturbing intermediate layer activations study perturbations regularizer training. perturbations train deep models resnets show improvement performance adversarial original test data. experiments highlight beneﬁts perturbing intermediate layer activations compared perturbing inputs. results cifar- cifar- datasets show merits proposed adversarial training approach. additional results wideresnets show approach provides signiﬁcant improvement classiﬁcation accuracy given base model outperforming dropout base models larger size. deep neural networks shown tremendous success several computer vision tasks recent years however seminal works adversarial examples shown dnns susceptible imperceptible perturbations input intermediate layer activations respectively. optimization perspective also showed adversarial training used regularization approach training deep networks. however effect adversarial training techniques regularizers deep networks systematically explored since computational overhead adversarial training justify marginal improvements conventional regularization techniques. work propose simple efﬁcient learning algorithm uses adversarial perturbations intermediate layer activations provide stronger regularization improving robustness deep network adversarial data. avoid expensive step explicitly generating adversarial examples different layers rather perturb activations current batch gradients accumulated activations previous batch mini-batch setting. even though gradient directions adversarial input layer show strongly adversarial applied intermediate layers. using gradients inputs belonging different class current input ensure adversarial perturbations intermediate layers directed towards adversarial class force network learn robust representations layer resulting improved discriminability. proposed pipeline signiﬁcant overhead training thus easily extended performing adversarial training deep neural networks. approach complements dropout regularizer achieves regularization beyond dropout. avoids over-ﬁtting generalizes well achieving signiﬁcant improvement performance test set. show trained network extremely robust adversarial examples even explicitly trained adversarial inputs. intent paper generate adversarial perturbations standalone images rather adversarial gradients efﬁciently regularize training. perform several ablative experiments highlight properties proposed approach adversarial training present results deep networks resnets state models wideresnets using standard datasets cifar- cifar-. related work many approaches proposed regularize training procedure deep networks. early stopping statistical techniques like weight decay commonly used prevent overﬁtting. specialized techniques dropconnect dropout successfully applied deep networks resnets. faster convergence deep architectures made possible batch normalization added beneﬁt additional regularization provided training even made dropout regularization unnecessary cases. work szegedy showed existence adversarial perturbations computer vision tasks solving box-constrained optimization approach generate perturbations. also showed training network feeding back adversarial examples regularizes training makes network resistant adversarial examples. relatively expensive layerwise training procedure analysis limited small datasets shallow networks. proposed fast gradient sign method generate adversarial examples. perform adversarial training proposed modiﬁed loss function also account loss adversarial examples. showed signiﬁcant improvements network’s response adversarial examples obtained regularization performance beyond dropout. proposes virtual adversarial training framework show regularization beneﬁts relatively deep models taking three times normal training time. proposed iterative approach generate much stronger adversarial perturbations also presented score function measure robustness classiﬁers examples. furthermore recent approaches deep contrastive smoothing distillation stability training focused solely improving robustness deep models adversarial inputs. work present efﬁcient layerwise approach adversarial training demonstrate ability strong regularizer deep models beyond specialized methods mentioned above addition improving model robustness adversarial inputs. recent theoretical works analyze effect random semi-random adversarial perturbations classiﬁer robustness. presented fundamental upper bounds robustness classiﬁer depends factors curvature decision boundary distinguishability class cluster centers. wang introduce notion strong robustness classiﬁers point differences generalization robustness characterizing topology learned classiﬁcation function. analysis perturbations section present approach highlight differences related methods perform adversarial training. addition perform small scale experiment study properties proposed adversarial training analyzing singular value spectrum jacobian. visualize impact perturbations intermediate layer activations conclude illustrating connection robust optimization-based approaches. start deﬁning notation. {xi}n denote labels. denote classiﬁer mapping maps image discrete label work modeled deep unless speciﬁed otherwise. denote loss function deep network represents network parameters input output respectively. deep network consists layers denotes backpropagated gradient loss function output layer iteration expression corresponds input layer loss layer. previous works adversarial training observed training model adversarial examples acts regularizer improves performance base network test data. deﬁne adversarial perturbations solution box-constrained optimization follows given input target label intend minimize ||r|| subject note that optimization trivial hence exact minimizer unique approximate using box-constrained l-bfgs. concretely value found using line-search minimizer following problem satisﬁes interpreted ﬁnding perturbed image closest misclassiﬁed training procedure framework involves optimizing layer using pool adversarial examples generated previous layers. training procedure rather cumbersome even applied shallow networks layers. overcome computational overhead l-bfgs optimization performed intermediate layer propose fast gradient sign method generate adversarial examples. linearizing cost function around value model parameters given iteration obtain norm constrained perturbation follows \u0001.sign). show perturbed images reliably cause deep models misclassify inputs. noted formulation adversarial perturbation understood looking ﬁrst order approximation loss function neighborhood training sample solution result maximizing second term respect norm constraint. central idea behind generating perturbations using approach training adversarial objective function acts good regularizer training model original inputs adversarially perturbed inputs objective function makes model robust adversaries provides marginal improvement performance original test data. intuitively procedure understood perturbing training sample within ball radius direction maximally increases classiﬁcation loss. work combine aspects formulations discussed follows generating adversarial perturbations intermediate layers rather using input layer sampling perturbations along directions moves training samples towards neighboring class centers hence making harder classiﬁcation task. order facilitate representation layerwise activations loss function denote collection layerwise responses {xl}l− then denotes loss function intermediate layer activations perturbed according notation used previous section special case consider following objective obtain perturbation ideally training example solution problem consists generating perturbation corresponding maximally confusing class; words choosing class maximizes divergence absence prior knowledge class cooccurences solving explicitly training sample every iteration time consuming. hence propose approximate solution problem gradients computed previous sample intermediate layer cached used perturb activations current sample. mini-batch setting amounts caching gradients previous mini-batch. ensure class constraint satisﬁed requirement successive batches little lateral overlap terms class labels. experiments observed random shufﬂe data satisﬁes requirement. given procedure accumulating gradients longer required perform extra gradient descent-ascent step method generate perturbations current batch. since gradient accumulation procedure computational cost training seamlessly integrated existing supervised training procedure including even deep networks shown experiments. algorithm efﬁcient layerwise adversarial training procedure inputs deep network loss function parameters containing convolutional blocks. batch sampled iteration size input-output pairs gradient accumulation layers {gc}c initialized zero. perturbation parameter sample batch size images training data perform regular forward pass gc’s active perform backward pass using classiﬁcation loss function. gradient accumulation layer training procedure summarized algorithm sign denotes signum function. gradient accumulation layers batch normalization layer convolutional block case layers present gradient accumulation layers convolution layer. subtle detail overlooked algorithm value constant layers rather normalized multiplying range gradients generated respective layers. test time gradient accumulation layers removed trained model. table comparison strength adversarial examples approach applied input using layerwise perturbations described section reported numbers classiﬁcation accuracies different values important question needs addressed light proposed optimization strategy gradient directions generated previous samples adversarial answer question perform empirical experiment measure performance conventionally trained deep model test data cifar-. described earlier test sample intermediate layer activations perturbed using gradients accumulated previous sample. comparison also show performance model adversarial data generated using method. metrics table observed using accumulated gradients previous batch adversarial perturbations results bigger drop performance. signiﬁes aggregated effect layerwise perturbations adversarial compared perturbing input layer done approach. performed additional experiment input layer perturbed using gradients previous sample instead perturbing intermediate layers. found resulted negligible drop baseline performance indicating gradients adversarial enough used perturb input. next compare effect layerwise adversarial perturbation described previous section random layerwise perturbations. figure shows dimensional t-sne visualization embeddings belonging ﬁnal fc-layer range values intensity adversarial perturbation. used pretrained network trained cifar- figure t-sne visualization ﬁnal fc-layer features dimension network randomly chosen classes cifar- data different values intensity shows adversarial perturbations bottom shows random perturbations intensity. clear random perturbations affect linear separability data adversarial perturbations extremely effective leading network misclassify perturbed data. dataset compute embeddings randomly chosen classes test data. bottom effect random perturbations zero mean unit standard deviation applied layerwise original data also shown. visualization accuracy values clearly observed layerwise perturbations described extremely adversarial base network. notice even higher values data perturbed layerwise random gradient directions remains clearly linearly separable adversarially perturbed data unable distinguished base model. order acquire better understanding regularizing properties mapping function learned using proposed adversarial training approach perform experiment using small neural network consisting fully connected layers sizes fully connected layer followed hyperbolic tangent activations. grayscale version cifar- dataset testbed norm weight decay applied training. data augmentation regularization methods dropout used training. train three networks baseline network network gradient accumulation layers network using training approach described cross entropy loss used train networks. terms classiﬁcation accuracy proposed method improves baseline performance original data accuracy network singular value analysis gain deeper understanding encoder mapping learned network perform analysis similar computing singular values jacobian encoder. since small architecture able explicitly compute jacobian sample test set. average singular value spectrum jacobian test data shown figure make following observations singular value spectrum computed approach fewer dominant singular values decays much faster rate compared base network training suppresses response network strongly dimensions approach achieves strong suppression trailing dimensions. implies network able better capture data variations relevant classifying original test data. hand achieves better robustness adversarial examples suppressing network’s response strongly even leading dimensions. degree adversarialness order validate implication perform empirical experiment cifar- test dataset. test image used generate perturbations using three networks trained above. three perturbed images classiﬁed baseline network. process repeated images test set. table lists performance baseline network perturbed images various values intensity perturbation. intuitively experiment characterizes relative strengths adversarial examples generated three networks mentioned above. observe robustness base network different adversarial examples occur following order base. concurs singular value spectrum analysis presented stronger suppression network’s response less sensitive becomes adversarial directions. several regularization problems machine learning ridge regression lasso robust svms shown instances general robust optimization framework point connection proposed adversarial training approach robust optimization borrow idea uncertainty sets explain brieﬂy uncertainty denoted represents epsilon ball around norm point adversarial training thought training hard examples strongly resist classiﬁcation. setting uncertainty sets adversarial training method could seen sampling perturbations input space norm. work extend idea uncertainty sets input activations layerwise activations. thought sampling perturbations feature space learned deep network. represent uncertainty activation layer then proposed adversarial training approach equivalent sampling perturbations intermediate layer uncertainty sets makes feature representation learned layers become robust training. moreover generating perturbations inputs belong class current input directions sampled uncertainty tend move perturbed feature representation towards direction adversarial class. effect observed t-sne visualization shown figure section provide experimental analysis proposed approach show layerwise adversarial training improves performance model original test data increases robustness adversarial inputs. demonstrate generality training procedure present results cifar- cifar- using resnet- resnet- networks. resnet networks publicly available torch implementation architecture publicly available implementation consists batch normalization experiments solver nesterov momentum base learning rate dropped every epochs case cifar- every epochs case cifar-. total training duration epochs. employ random ﬂipping data augmentation procedure standard mean/std preprocessing applied conforming original implementations. resnet baseline models without regularization start overﬁtting trained longer hence perform early stopping report best results. perturbed models early stopping necessary; learning continues longer duration shows good convergence behavior. refer model trained using algorithm erturbed throughout section. proposed training method summarized algorithm batch inputs perturbed intermediate layers gradients accumulated previous batch. section present empirical comparison following variants fgs-orig original joint loss based adversarial training proposed shown used value values yield signiﬁcant improvements. fgs-inter setting different gradients perturb intermediate models trained cifar- dataset. data augmentation dropout regularization applied. training parameters similar ones used previous section. generate adversarial test data cifar- test dataset using method since shown generate adversarial examples reliably. test models original adversarial test data different values adversarial strength table shows results different training strategies. corresponds original test data values indicate strength adversarial perturbation added input image. results make following observations approaches based perturbing intermediate layers improve performance original data signiﬁcantly compared perturbing input marginally decrease adversarial test performance. hand perturbing input layer yields best adversarial test performance among compared approaches performing marginally worse baseline original test data. observations indicate possibility trade-off exists adversarial robustness classiﬁcation performance. perform experiment compare regularization performance proposed adversarial training dropout. architecture used previous sections perform experiments without dropout cifar- cifar- datasets. understand full extent regularization performance perform data augmentation experiment. following observations could made table perturbed model performs better baseline model without dropout. thus proposed training improves performance even dropout based networks. complex task like cifar- proposed adversarial training approach gives better regularization performance compared provided dropout .%). since proposed adversarial perturbations intended move inputs towards directions strongly resist correct classiﬁcation able create discriminative representation tasks larger number classes. section analyze effect adversarial perturbations starting lowest convolutional layers model edges/shape information deeper layers model abstract concepts. experiment network batch normalization used previous section. experiments performed cifar- dataset. data augmentation dropout applied. clear results table improvement performance proposed layerwise perturbations become signiﬁcant applied deeper layers network line observation made performing layerwise alternate training proposed becomes infeasible even moderately deep architectures training scheme provides efﬁcient framework infuse adversarial perturbations throughout structure deep models. behavior cnns adversarial data generated intrigue computer vision since work effects deeper networks explored well. observe adversarial perturbations hidden layer activations generalize across different samples leverage observation devise efﬁcient adversarial training approach could used train deep architectures. experiments analysis make following observations contrary recent methods inconclusive role perturbing intermediate layers adversarial training shown deep networks play signiﬁcant role providing strong regularization aggregated adversarial effect perturbing intermediate layer activations much stronger perturbing input signiﬁcant improvement classiﬁcation accuracy entails capturing variations data distribution adversarial robustness improved suppressing unnecessary variations learned network providing efﬁcient adversarial training approach could used deep models hope inspire robust network designs future. wide residual networks recently proposed deep architectures generated state results cifar- cifar- datasets. experiment publicly available implementation train scratch using proposed adversarial training approach using parameter settings described original paper section speciﬁcally ours-joint approach described section used training. data augmentation applied ﬂipping random cropping done native implementation. results shown table table classiﬁcation error rates cifar- cifar- wideresnet architectures. results reported average runs. comparison provide published baseline results. denotes results obtained single run. cifar- response local perturbation depends jacobian mapping metric spaces then denote jacobian evaluated bounded local perturbation neighborhood ﬁrst order truncated expansion given ||·||f denotes frobenius norm; vectors norm denotes singular value jacobian; direct application cauchy-schwarz inequality. applying result singular value spectra plotted figure paper base network without adversarial training extremely sensitive local perturbations compared adversarially trained networks using proposed approach. setting parameter following notation section denote gradient loss function backpropagated layer. max) min). then value layer calculated exact value cross-validated using held set. practice found training approach overly sensitive tuned network cifar- used value networks resnets wideresnets cifar- cifar- datasets. note that cases ﬁxed value speciﬁed figure paper value used layers ignoring normalizing factor swami sankaranarayanan rama chellappa supported ofﬁce director national intelligence intelligence advanced research projects activity iarpa contract views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied odni iarpa u.s. government. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright annotation thereon. alhussein fawzi seyed-mohsen moosavi-dezfooli pascal frossard. robustness classiﬁers adversarial random noise. corr abs/. http //arxiv.org/abs/.. kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition pages alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems pages takeru miyato shin-ichi maeda masanori koyama shin ishii. virtual adversarial training regularization method supervised semi-supervised learning. arxiv preprint arxiv. seyed-mohsen moosavi-dezfooli alhussein fawzi pascal frossard. deepfool simple accurate method fool deep neural networks. proceedings ieee conference computer vision pattern recognition pages nicolas papernot patrick mcdaniel somesh ananthram swami. distillation defense adversarial perturbations deep neural networks. security privacy ieee symposium pages ieee salah rifai pascal vincent xavier muller xavier glorot yoshua bengio. contractive autoencoders explicit invariance feature extraction. proceedings international conference machine learning pages florian schroff dmitry kalenichenko james philbin. facenet uniﬁed embedding face recognition clustering. proceedings ieee conference computer vision pattern recognition pages shaham yutaro yamada sahand negahban. understanding adversarial training increasing local stability neural nets robust optimization. arxiv preprint arxiv. nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research christian szegedy wojciech zaremba ilya sutskever joan bruna dumitru erhan goodfellow fergus. intriguing properties neural networks. arxiv preprint arxiv. matthew zeiler sixin zhang yann fergus. regularization neural networks using dropconnect. sanjoy dasgupta david mcallester editors proceedings international conference machine learning volume pages jmlr workshop conference proceedings http//jmlr. org/proceedings/papers/v/wan.pdf. stephan zheng yang song thomas leung goodfellow. improving robustness deep neural networks stability training. proceedings ieee conference computer vision pattern recognition pages", "year": 2017}