{"title": "Audio Visual Emotion Recognition with Temporal Alignment and Perception  Attention", "tag": ["cs.CV", "cs.CL", "cs.LG"], "abstract": "This paper focuses on two key problems for audio-visual emotion recognition in the video. One is the audio and visual streams temporal alignment for feature level fusion. The other one is locating and re-weighting the perception attentions in the whole audio-visual stream for better recognition. The Long Short Term Memory Recurrent Neural Network (LSTM-RNN) is employed as the main classification architecture. Firstly, soft attention mechanism aligns the audio and visual streams. Secondly, seven emotion embedding vectors, which are corresponding to each classification emotion type, are added to locate the perception attentions. The locating and re-weighting process is also based on the soft attention mechanism. The experiment results on EmotiW2015 dataset and the qualitative analysis show the efficiency of the proposed two techniques.", "text": "national laboratory pattern recognition institute automation chinese academy sciences institute neuroscience state laboratory neuroscience center excellence brain approaches fusing modalities shown modalities fused performance robustness emotion recognition system improve measurably. although combining audio visual modalities improve recognition accuracy audio visual fusion still problem. three fusion strategies widely utilized. currently works combines modalities decision level decision-level fusion inputs coming different modalities single-modal recognition results combined end. since humans display audio visual expressions complementary redundant manner assumption conditional independence audio visual data streams decision-level fusion incorrect results loss information mutual correlation modalities feature level fusion another utilized audio visual emotion recognition. combines visual descriptors audio features using multiple kernel learning audio-video clips classified classifier. common extracting audio visual features separately pooling features single vectors feature sets concatenating vectors single feature vector classification feature level fusion methods consider temporal coupling audio visual streams. address problem model-level fusion proposed make correlation audio visual streams. particular multiple stream hidden markov models artificial neural network-based fusion proposed. however models require different modalities strong synchronization. audio visual signals always different frame rates temporal alignment audio visual features models necessary often manually operated. paper focuses problems audio-visual emotion recognition video. audio visual streams temporal alignment feature level fusion. locating re-weighting perception attentions whole audio-visual stream better recognition. long short term memory recurrent neural network employed main classification architecture. firstly soft attention mechanism aligns audio visual streams. secondly seven emotion embedding vectors corresponding classification emotion type added locate perception attentions. locating re-weighting process also based soft attention mechanism. experiment results emotiw dataset qualitative analysis show efficiency proposed techniques. emotion recognition plays important role human machine interaction. early researches mainly focus utterance level speech emotion recognition static image level facial expression recognition. however emotion temporally dynamic event better inferred audio video feature sequences. point view proved cognitive researchers argue dynamics human behaviors crucial interpretation moreover number recent studies affective computing demonstrate point view. meanwhile human emotions expressed multimodal way. psychological study highlighted importance using multiple modalities strengthen accuracy emotion analysis. authors analyzes strengths weaknesses vision-only audio-only based expression analysis systems. also outline figure soft attention aligns visual feature sequence audio representation sequence temporal aligned audio visual streams encoded lstm layer learn dynamics audio visual coupling lstm encoding added emotion embedding vectors locate re-weight perception attentions audio-visual stream soft attention. final classification results based combined weighted perception attentions classification. order locate emotional salient parts emotion embedding vectors corresponding emotion classification types added proposed model. emotion embedding vector works anchor choose increase weights salient parts specific emotion type. locating re-weighting combining emotion type unique hidden representation final classification. emotion embedding vectors jointly learned neural network parameters. input gate forget gate output gate calculated eq.. cell state hidden state represents input lstm time step affine transformation consisting trainable parameters dimensionality dimensionality proposed architecture given audio input feature sequence lstm layer paper utilize lstm-rnn model audio visual streams. particularly soft attention mechanism employed audio visual streams alignment. mechanism enables neural network learn align audio visual streams predict emotion type jointly. without manually temporal alignment believe model less information loss better recognition results. models utilized sequence classification hidden representation encodes input information start current time step. final classification result often calculated hidden reprensetation last time step previous study shows average hidden representations different time steps better results. however last-time encoding average encoding optimal choices? believe perception process special audio-video clip people’s attention focus several sub-clips emotionally salient. sub-clips provide better clues attention paid better emotion perception. studies video emotion recognition filed also prove point view. example select frames video clip make final classification also shows competitive performance. frames seen emotional salient part. last-time encoding fixed-length vector fully contain necessary information whole audio-visual stream. problem also exists based machine translation previous study proved average encoding better last time encoding. however average encoding encodes classifier fusion equal weights given sub-parts whole sequence. perception attention exists perceiving audio-visual clips given equal weights sub-parts optimal way. thus better encoding explored. find perception attentions increase weights perception attentions solution. hypothesis different emotion types different perception attentions. locate perception attentions emotion embedding vectors model. equals number emotion types needs classified. emotion embedding vector works anchor select emotional salient parts whole audio-visual stream. based soft attention mechanism attention scores emotion type calculated follows mapping matrix orignal dimension dimension represents emotion type attention distribution hidden representation specially calculated emotion type classification score audio-visual clip classified emotion type calculated first learns dynamics audio sequence encodes hidden representation visual feature sequence represented represent length audio feature sequence visual feature sequence separately. visual representation dynamics audio visual coupling encoded together another lstm layer layer soft attention mechanism utilized align audio visual streams. alignment window technique applied. time step soft attention mechanism considers sub-sequence whole sequence predefined window width median alignment. given calculate coarse alignment streams time step. adding window utilize prior knowledge also result lower complexity. accurate alignment calculated follow ways weights mapping element softmax score aligned scores normalized softmax softmax thought probability model believes corresponding frames temporal aligned calculating probabilities soft attention mechanism computes expected value every time step taking expectation every time step hidden representation audio-visual lstm calculated encodes input features start time step normally ways final classification results. first based last hidden representation. classification results last time encoding represented classification score based weighted combination perception attentions. compared average encoding last-time representations lstm-rnn utilized efficiently information loss decreased. implementation proposed model encode audio visual streams times similar architecture. first time encodes hidden utilized locate attentions representation obtain smaller network. second time encodes combines processing calculate separately better performance obtained compared compute based believe separate encoding decrease correlation among parameters. thus easier optimize. emotiw provides common benchmarks emotion recognition researchers mimics real-world conditions. sub-challenges audio-visual based emotion recognition challenge image based static facial expression recognition challenge afew sub-challenge assign single emotion label video clip universal emotions neutral. databases divided three sets challenge training validation testing. training validation sets utilized train emotion recognizer. prediction results testing utilized rank participants. sample rate audio data afew khz. video data afew frames second. video features mainly focus face part. face shape provides import clues facial expression landmarks’ location face face shape feature. feature normalization clip features also reflect head movement head pose. landmarks’ locations whitened final dimensions kept. face appearance feature utilize features extracted convolutional neural network model. previous work utilizes model trained face recognition dataset extract face representation generalized facial expression recognition problem. employ strategy train model celebrity faces wild facescurb dataset designed face recognition tasks. face images people used training labels identities. architecture three fully connected layers five convolutional layers. compared three fully connected layers convolutional layers better generation performance deeper layers extract abstract features thus extract feature pooling layer appearance feature. dimension number features pool meanwhile training data relatively small. thus employ random forest algorithm implemented scikit-learn feature selection features kept appearance feature set. random forest classifier trained sfew database seven emotion labels assigned single static face image. audio feature utilize yaafe toolbox extract audio features. features toolbox extracted. audio data resampled default parameters feature utilized. finally dimensions features extracted frame frame length audio features whitened final dimensions kept. follow challenge criterion emotiw utilize training validation testing set. utilize landmarks provided organizers shape feature. caffe implementation utilized extract face appearance features cropped face image provided organizers. verification perception alignment compare average encoding last-time encoding proposed model. thus mainly architectures comparison. first average encoding last-time encoding audio visual inputs. architecture memory cells utilized audio lstm audio-visual lstm. dimensions hidden layers lstm layers equal dimension lstm layers. audio feature sequence hidden layer first audio lstm. visual feature sequences face shape feature face appearance feature hidden layer nodes separately. hidden representations feature sets concatenated together another hidden layer. fused hidden representation face shape appearance features utilized align audio stream audio-visual lstm. second proposed model temporal alignment module perception attention module. main difference compared first architecture exists perception attention module added. architecture first dimensions utilized dimension seven emotion embedding vectors also thus second model smaller parameters verification audio visual alignment performance average encoding single face appearance feature audio visual inputs temporal alignment module compared. model appearance feature hidden layer lstm layer memory cells. models trained adadelta maximum training epoch dropout regularization technique utilized layers except lstm layer. drop rate weight decay layers parameter applied prevent fitting. early stopping technique also employed. best results testing chosen best prediction accuracy validation set. figure examples temporal alignment audio visual streams. gray values bars indicate scores alignment audio frames given window particular visual frame. normalized maximum value rows sample corresponding four locations given window columns corresponding time steps. bottom sample first location given window. fig. show alignments audio visual streams change increase time step. looking overall examples show clear shift attention focuses given window time step increases obvious attention shift time steps middle time steps show clear change mode. time step increases attentions shift start window shift figure examples perception attentions emotion type audio visual stream. gray values bars indicate attention scores. normalized maximum value rows sample corresponding seven emotion types angry disgust fear happy neutral surprise. columns corresponding time steps sample. reverse start happens middle time steps. changing mode happens almost samples matter many time steps sample first half fig. also shows different attention distribution similar even distribution. reason mainly dataset. emotiw dataset collected movies wild environment background noise audio modality sound human alignment tends become even distribution. perception attention visualization shows fig.. perception attentions various distributions different samples randomly pick several samples represent samples. figure shows perception attentions locate different parts whole audio-visual streams start close middle figure also shows perception locate multiple locations second half random distribution whole stream picking process also find relative large proportion samples distribution fig.e perception attention mainly locates first frames. reasons explain result. first neural network fails learn right distribution totally. second come dataset collection process. annotators label audio-visual recording clip begins find emotional salient part. thus figure projection emotion embedding vectors. vectors projected space dimension reduction eigenvalues kept vector. projection initialized embedding vectors model training. projection learned embedding vectors. table reports accuracy comparison experiments. results show performance slightly better combine audio visual modalities feature level. proposed model also works better average encoding last-time encoding model. table also shows performance comparisons several state-of-the-art models. first three results three performers emotiw significant gaps compared leading models. performers focus designing better features. work close work. however model utilizes decision level fusion improve performance significantly. fact model appearance feature behaves similar ours. conclude decision level model works better feature level fusion dataset better features needed model. paper utilize soft attention mechanism temporally align audio visual streams fuse streams feature level. also emotion embedding vectors soft attention mechanism output layer locate re-weight perception attentions audio visual stream. compared widely utilized average encoding last-time encoding model decrease information loss output layer utilize output efficiently. besides qualitative analysis quantitative analysis show effectiveness proposed techniques. also think proposed model especially perception attention technique utilized sequence classification tasks. future plan explore better features emotion classification task since still large space improve compared state-of-the-art models. start audio-visual clip emotional salient frame. context seven emotion embedding vectors added. seven emotion type almost focus emotional salient sub-parts relative little differences attention distribution. suggests emotional salient parts attract attention judge emotion types. fig. shows projection emotion embedding vectors random initialization model training final values model training finished relative positions vectors change totally. however clear patterns among relative positions vector jointly observing confusion matrix details best submitted result shown fig.. confusion matrix shows angry happy neutral easier classify. surprise fear easy misclassified angry. disgust easy confuse happy. reason data distribution balance. fine grained classification among angry fear surprise needs effort. work supported national high-tech research development program china national natural science foundation china major program national social science fund china strategic priority research program", "year": 2016}