{"title": "Are Saddles Good Enough for Deep Learning?", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "Recent years have seen a growing interest in understanding deep neural networks from an optimization perspective. It is understood now that converging to low-cost local minima is sufficient for such models to become effective in practice. However, in this work, we propose a new hypothesis based on recent theoretical findings and empirical studies that deep neural network models actually converge to saddle points with high degeneracy. Our findings from this work are new, and can have a significant impact on the development of gradient descent based methods for training deep networks. We validated our hypotheses using an extensive experimental evaluation on standard datasets such as MNIST and CIFAR-10, and also showed that recent efforts that attempt to escape saddles finally converge to saddles with high degeneracy, which we define as `good saddles'. We also verified the famous Wigner's Semicircle Law in our experimental results.", "text": "abstract. recent years seen growing interest understanding deep neural networks optimization perspective. understood converging low-cost local minima suﬃcient models become eﬀective practice. however work propose hypothesis based recent theoretical ﬁndings empirical studies deep neural network models actually converge saddle points high degeneracy. ﬁndings work signiﬁcant impact development gradient descent based methods training deep networks. validated hypotheses using extensive experimental evaluation standard datasets mnist cifar- also showed recent eﬀorts attempt escape saddles ﬁnally converge saddles high degeneracy deﬁne ‘good saddles’. also veriﬁed famous wigner’s semicircle experimental results. understanding deep neural networks optimization perspective emerged active area study owing great success deep learning recent years. given non-convex nature loss function deep networks difﬁcult tell traditional gradient descent algorithm converged popularly understood today deep networks converge local minima historically practitioners neural networks always considered converging local minima major concerns neural network training. however recent work last years gradually clarifying perception hypothesizing converging global minima essential converging local minima cost function values important further recent work shown local minima good global minima large parameter spaces deep neural networks critical points saddle points. time recent methods attempt escape saddle points training. recent developments understanding deep neural networks trained also emphasized fact understanding optimization methods gradient descent low-dimensional spaces necessarily translate large dimensions. issues raise important pertinent questions deep neural network models converge all. considering overwhelming presence saddle points high-dimensional spaces neural networks converge low-cost local minima? instead converge saddle points? saddle point better other? importantly saddles good enough deep learning? pursuit answering questions limited fact characterizing nature critical points high-dimensional spaces intractable. also limited work understanding saddle points training deep networks however important question answer help provide important insights model solutions obtained deep learning well novel methods converge ‘better’ solutions quickly. show empirically convergence ‘good’ saddle suﬃces achieve convergence practically useful deep neural networks best knowledge ﬁrst eﬀort characterizing convergence points models. importance contributions work lies clarifying understanding dnns converge. decades understood dnns converge local minima. work brings fresh perspective understanding claiming dnns actually converge saddle points. understanding could fundamentally inﬂuence design gradient descent algorithms used train deep networks result methods make training eﬃcient. understanding derived herein provide dimension look stochastic gradient descent variants especially respect degeneracy parameter space during training. existing theoretical work consider degeneracy analysis assume existence strict saddle methods. assumption however hold practice show work. work provide momentum theoretical work takes account degeneracy saddle points encountered training dnns. remainder paper organized follows. section discusses background literature motivation work. section proposes hypothesis models converge saddle points studies this. section characterizes nature saddle model converges section studies existing methods attempt escape saddles actually achieve. present analysis work diﬀerent network settings section conclude pointers future work section known presence multiple symmetries parameter space directly correlated proliferation saddle points error surface dnns. common symmetries noted recent work scaling symmetries permutation symmetries. presence scaling symmetries explained follows. firstly given weight matrices neighboring layers scaling leaves loss function unchanged. particularly relevant using relu activations linear networks. batch-norm layers used network random scaling weight matrices result symmetry issue. known symmetries lead presence multiple saddles error surface permutation symmetries occur order hidden units permuted respective connected weights make output neural network invariant input. symmetries also cause hessian degenerate critical points last years seen growing attention community issue saddle points training deep networks. pascanu well dauphin challenged popular claim local minima main concern training deep networks. instead highlighted local minima concern low-dimensional parameter spaces local minima high-dimensional spaces reasonably costs concern instead replaced presence saddle points slow training. choromanska corroborated claim showing low-index critical points large models concentrate band global minimum they fact showed essential look global minimum since indicate overﬁtting. observations focused eﬀorts last years escaping analyzing saddle points anandkumar kawaguchi hardt introduce deﬁnitions discussing methods. deﬁnition critical points. consider smooth function critical point critical points classiﬁed considering hessian eigenvalues positive critical point local minimum. eigenvalues negative critical point local maximum. eigenvalues positive negative critical point negative eigenvalues. note standard categorization saddle points. order study saddle points constrained setting introduced concept strict saddle deﬁned below. disparate eﬀorts address issue saddle points dnns since single solution problem. early work existence saddles single hidden layer shown baldi hornik pascanu dauphin recently identiﬁed problem saddle points training deep networks proposed ‘saddle-free’ newton method help optimization method escape saddle points training. particular proposed second-order trust region-based method uses curvature function deﬁne trust region. proposed simple approach called noisy-stochastic gradient descent escape strict saddles polynomial time. noisy-sgd small amount random noise added calculated gradient every iteration. authors claim noise helps escape strict saddles. easily understood intuitively using deﬁnition strict saddle requires strong negative curvature every direction critical point. hence adding random noise gradient helps escape saddles. although work theoretical guarantees empirical studies validate claims. besides assumption strong negative curvature directions critical points unrealistic high-dimensional parameter spaces deep neural networks recently kawaguchi showed deep neural networks depth width every local minimum global minimum. also showed every critical point global minimum saddle point. signiﬁcant result questions motive methods escape saddles asks question dnns indeed converge then? even recent work showed perturbed form gradient descent always converged second-order stationary point number iterations ‘dimension-free’. particular state saddle points non-degenerate second-order stationary points local minima proposed perturbed gradient descent method escapes saddle points converge local minima. dnns. assumptions made proving theoretical results don’t hold practice cases. recent eﬀorts show interesting theoretical results limited validation results empirical studies aforementioned recent efforts make assumption saddle points models non-degenerate assumption unrealistic models parameter space order millions. recent related eﬀorts hardt also provide insights convergence properties gradient descent non-convex error surfaces assume saddles non-degenerate. figure explains issue degenerate saddle using example. figure represents loss function plateau w-interval figure provides mesh view function. although global minimum occurs traditional gradient descent algorithm converges degenerate saddle algorithm initialized point saddles serious concern diﬃcult either ﬁrst-order second-order methods escape them. classical work machine learning watanabe states almost learning machines singular fisher information matrix zero eigenvalues. observations highlight need study problem saddle points particular context high-dimensional dnns. addition kawaguchi’s recent claim critical point global minimum saddle point seems contend methods’ claim ‘escaping saddles’ converging local minima training. hence work seek study conﬂicting narratives characterizing saddles obtained training dnns practice. speciﬁcally question dnns converge local minima? saddles good enough?. based analysis recent work studies section propose hypothesis existing deep network models including successful practice actually converge saddle points. theoretically guaranteed ﬁrst-order learning algorithm nonconvex converges critical point. always understood converged point local minimum. hypothesize dnns converge saddles. answering question easy straightforward given high-dimensional nature loss function. preliminary theoretical support hypothesis designed extensive experimental evaluation setup study hypothesis describe below. experimental study evaluate hypothesis carried diﬀerent datasets traditionally used deep learning community. mnist cifar- particular work evaluate hypothesis. datasets long studied community validation hypothesis datasets signiﬁcant impact. mnist database training consists images test images. cifar- color image dataset comprising classes training size test size dataset consist training batches test batch randomly chosen images. source code reproduce results found https//github.com/ ravisankaradepu/degenerate_saddle. train diﬀerent dnns datasets compute eigenvalues hessian every iteration. presence positive negative eigenvalues hessian convergence helps identify corresponding critical point saddle. although evaluation criteria seems rather trivial diﬃcult practice explicit computation hessian matrix daunting task impossible larger networks. hence choice architectures experiments restricted capability available computational infrastructure calculating hessian. however evident experimental results later section chosen architectures yield results comparable best results obtained datasets. particular trained multi layer perceptron single hidden layer train mnist cifar-. train mnist used input neurons hidden neurons relu activations output neurons state-of-the-art cifar-. hence instead passed images cifar model known good performance dataset used features penultimate layer input single-layer mlp. thus cifar- ﬁnally input neurons hidden neurons relu activations output function. terminated learning experiments error diﬀerence consecutive epochs less weights networks initialized randomly normal distribution. separate trials experiments work report mean standard deviation results across trials. experiments carried using diﬀerent variants gradient descent methods viz. momentum adam adagrad momentum method used nesterov momentum parameter adam results experiments mnist dataset shown table cifar- shown table evidently signiﬁcant number eigenvalues negative convergence datasets validating hypothesis deep networks indeed converge saddle point. models deeper wider networks example hidden layers neurons layer totals around million weights. intractable validate experiment hessians size. network hidden layer neurons million parameters tractable computing hessian still provides accuracy comparable accuracies convergence similar networks mnist. table also shows number hidden neurons single-layer increased accuracy increases convergence along increase model complexity. chose network architecture provides best performance given constraint imposed model complexity available computational resources memory allows hessian work). accuracy cifar- however near current state-of-the-art results although smaller network. note choice architectures work aligns focus understand behaviour convergence gradient descent algorithms dnns highly competitive models. validation hypothesis deep networks indeed converge saddle points question follows diﬀerent categories saddle points? kind saddles deep networks converge study question section. various kinds saddle functions existing literature best knowledge provides comprehensive categorization kinds saddle points occur practice. hence summarize diﬀerent types saddles discussed hitherto hypothesize addition converging saddle deep neural network models successful practice today converge degenerate saddles. carry empirical validation hypothesis section hypothesis understood intuitively too. known ﬁrst-order learning algorithms stop making progress norm gradient becomes zero critical point. critical point saddle variants basic gradient descent algorithms attempt lower cost critical point adding momentum perturbations gradient. however region around saddle degenerate even perturbations help thereby letting network converge. study hypothesis extended previous experiments section capture degeneracy saddle dnns converged deﬁne degeneracy number zero eigenvalues hessian converged point. report degeneracy converged points results show signiﬁcant number eigenvalues zero convergence datasets providing test accuracy state-of-theart methods. observed considered gradient descent methods. momentum-based gradient descent mnist highlights extent degeneracy saddle. negative eigenvalues included show critical points indeed saddles. ﬂatness converged point makes diﬃcult gradient-descent based algorithms escape saddles. table thus validates hypothesis converged point indeed degenerate saddle. analyzing hessian critical points using plots proposed bray fraction negative eigenvalues hessian critical point error obtained critical point. bray noticed plane critical points concentrate monotonically increasing curve ranges implies strong correlation fraction negative eigenvalues hessian accuracy. figure shows plot cifar- trained network simple words wigner’s states large random symmetric matrices distribution eigenvalues appears like semicircle mean zero also random matrices dimension matrix increases probability zero eigenvalues random matrix nearly dauphin also noted semicircle holds hessians obtained deep learning settings. plots number negative eigenvalues decreases lowering error hence convergence number negative eigenvalues reduced course training wigner’s expected either number zero eigenvalues increases many positive eigenvalues come closer zero directly inﬂuences degeneracy saddle. supports hypothesis deep neural network models converge degenerate saddle points. discussed section methods proposed last years escaping saddle points training dnns validation hypothesis section deep neural networks actually converge kind saddle practice raises important questions ‘good’ saddle would want deep neural network converge methods propose escape saddle points achieve? answer questions section starting second one. dauphin proposed saddle-free newton method escape saddle points training dnns. method uses krylov subspace descent loss function optimized lower-dimensional krylov subspace calculated lanczos iteration hessian. recently proposed noisy-sgd escape strict saddles followed perturbed gradient descent also escape strict saddles. recent eﬀorts summarized table saddle-free newton uses krylov subspace-based approach heuristic rescale gradient /|λi| adds small amount random uniform noise gradient help escape saddle adds noise sampled unit ball parameters performing gradient descent certain iterations order understand methods achieve light validation hypothesis section carried empirical evaluation performing similar experiments prove hypotheses analyzed nature converging point aforementioned methods table noisy-sgd saddle-free newton already noted before assumption strict saddles made hold practice nevertheless studied method. trained arcifar-. table presents results noisy-sgd convergence. evident results even method attempts escape saddles converges degenerate saddle providing performance comparable state-of-the-art. case saddle-free newton authors’ implemented method autoencoder mnist used architecture evident results even methods propose escape saddle points converge degenerate saddles thus validating hypotheses work. surmise methods seem take gradient descent method ‘poor’ saddle ‘better’ saddle process training dnn. brings ﬁrst question raised beginning section indeed good saddle? deﬁnite answer deﬁne ‘good’ saddle coin deﬁnition kind saddles good enough dnns converge evidently critical points convergence results satisfy deﬁnition good saddle. order validate claim deep networks converging deﬁnition ‘good saddle’ also studied degeneracy changes every epoch. note easy experiment since computing hessian every epoch drastically slows training compute-intensive. hence trained much smaller network pattern degeneracy. result shown figure indicates degeneracy parameter space increases every epoch training. also explained fact learning algorithm able escape saddles lesser degeneracy initially degeneracy saddle increases learning algorithm ﬁnds diﬃcult escape wide plateau around converged point. results discussed clearly validate deep models tend converge degenerate saddles. inference provide dimension researchers approach similar gradient descent algorithms training dnns. theoretical eﬀorts last years proposed methods escape saddles assumption saddles zero eigenvalues. evident work practical assumption provide motivation newer theoretical work considers degeneracy saddles. addition experiments conducted also studied impact weight initialization network depth hypotheses proposed work. many recent eﬀorts claimed good weight initialization helps converge better critical point training dnns. table summarizes weight initialization methods proposed recent past. impact weight initialization converged point experimented various types initializations degeneracy converged point. table studies eﬀect various successful weight initialization methods standard nesterov momentum. network architectures impact network depth converged point since depth plays prominent role success dnns also investigated relationship network depth nature converged critical point. motivation study also comes wigner’s semicircle wigner proved random matrices dimension matrix increases probability zero eigenvalues random matrix nearly network depth leads increase size parameters hence hessian. table studies eﬀect depth degeneracy. performed experiments till depth beyond becomes intractable explicitly compute hessian current hardware setup. table ﬁrstly veriﬁes wigner’s semicircle setup explicitly shown count positive negative eigenvalues experiments converged saddles too. degeneracy saddle gradually increases increasing depth. number parameters depth- mnist corresponding number zero eigenvalues fraction zero eigenvalues number parameters depth- depth- networks respectively corresponding fraction zero eigenvalues showing pattern increasing degeneracy. similar trend also observed training cifar-. best knowledge ﬁrst eﬀort empirically validate wigner’s semicircle deep neural networks. mentioned section architecture network cifar close state-of-the-art performance network architecture mnist relatively lower higher accuracy requires deeper networks tractable compute hessian. conducted experiment show increasing number hidden neurons mnist architecture chosen work actually takes performance network close state-of-the-art. table presents results. words results show mnist architecture chosen work provides best-inclass performance network size. also conducted experiment cifar- architecture observed architecture chose work quite good itself. work proposed hypothesis deep neural network models gradient descent methods training often converge degenerate saddle points. validated hypothesis using experimental evaluation standard datasets mnist cifar-. studied nature convergence points methods recently proposed escaping saddles found cases models converged degenerate saddles. extensive experiments work provided fresh perspective understanding training deep networks direct impact newer optimization methods proposed train deep networks. future work explore theoretical implications observations especially pertains results random matrix theory. also plan investigate methods attempt escape higher-order saddles context ﬁndings work.", "year": 2017}