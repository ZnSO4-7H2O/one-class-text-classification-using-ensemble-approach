{"title": "Deep convolutional neural networks for predominant instrument  recognition in polyphonic music", "tag": ["cs.SD", "cs.CV", "cs.LG", "cs.NE"], "abstract": "Identifying musical instruments in polyphonic music recordings is a challenging but important problem in the field of music information retrieval. It enables music search by instrument, helps recognize musical genres, or can make music transcription easier and more accurate. In this paper, we present a convolutional neural network framework for predominant instrument recognition in real-world polyphonic music. We train our network from fixed-length music excerpts with a single-labeled predominant instrument and estimate an arbitrary number of predominant instruments from an audio signal with a variable length. To obtain the audio-excerpt-wise result, we aggregate multiple outputs from sliding windows over the test audio. In doing so, we investigated two different aggregation methods: one takes the average for each instrument and the other takes the instrument-wise sum followed by normalization. In addition, we conducted extensive experiments on several important factors that affect the performance, including analysis window size, identification threshold, and activation functions for neural networks to find the optimal set of parameters. Using a dataset of 10k audio excerpts from 11 instruments for evaluation, we found that convolutional neural networks are more robust than conventional methods that exploit spectral features and source separation with support vector machines. Experimental results showed that the proposed convolutional network architecture obtained an F1 measure of 0.602 for micro and 0.503 for macro, respectively, achieving 19.6% and 16.4% in performance improvement compared with other state-of-the-art algorithms.", "text": "important useful information users included audio tags. huge demand music search owing increasing number music ﬁles digital format. unlike text search difﬁcult search music input queries usually text format. instrument information included tags allows people search music speciﬁc instrument want. addition obtained instrument information used various audio/music applications. instance instrument-speciﬁc tailored audio equalization applied music; moreover music recommendation system reﬂect preference users musical instruments. furthermore also used enhance performance tasks. example knowing number type instrument would signiﬁcantly improve performance source separation automatic music transcription; would also helpful identifying genre music. instrument recognition performed various forms. hence term instrument recognition instrument identiﬁcation might indicate several different research topics. instance many related works focus studiorecorded isolated notes. name eronen used cepstral coefﬁcients temporal features classify orchestral instruments several articulation styles achieved classiﬁcation accuracy instrument family level individual instruments diment used modiﬁed group delay feature incorporates phase information together mel-frequency cepstral coefﬁcients achieved classiﬁcation accuracy instruments applied sparse coding cepstrum temporal sum-pooling achieved fmeasure classifying instruments also reported classiﬁcation result multi-source database previous works krishna sreenivas experimented classiﬁcation solo phrases rather isolated notes. proposed line spectral frequencies gaussian mixture model achieved accuracy instrument family individual instruments. moreover essid reported classiﬁcation system mfccs along principal components analysis achieved overall recognition accuracy solo phrases instruments. abstract—identifying musical instruments polyphonic music recordings challenging important problem ﬁeld music information retrieval. enables music search instrument helps recognize musical genres make music transcription easier accurate. paper present convolutional neural network framework predominant instrument recognition real-world polyphonic music. train network ﬁxed-length music excerpts single-labeled predominant instrument estimate arbitrary number predominant instruments audio signal variable length. obtain audio-excerpt-wise result aggregate multiple outputs sliding windows test audio. investigated different aggregation methods takes average instrument takes instrument-wise followed normalization. addition conducted extensive experiments several important factors affect performance including analysis window size identiﬁcation threshold activation functions neural networks optimal parameters. using dataset audio excerpts instruments evaluation found convolutional neural networks robust conventional methods exploit spectral features source separation support vector machines. experimental results showed proposed convolutional network architecture obtained measure micro macro respectively achieving performance improvement compared state-of-the-art algorithms. various instruments. human easily identify instruments used music still difﬁcult task computer automatically recognize them. mainly music real world mostly polyphonic makes extraction information audio highly challenging. furthermore instrument sounds real world vary many ways timbre quality playing style makes identiﬁcation musical instrument even harder. used synthesized polyphonic audio studio-recorded single tones. heittola used non-negative matrix factorization -based source-ﬁlter model mfccs synthesized polyphonic sound achieved recognition rate polyphonic notes randomly generated instruments. kitahara used various spectral temporal modulation features linear discriminant analysis classiﬁcation. reported that using feature weighting musical context recognition rates trio quartet. duan proposed uniform discrete cepstrum mel-scale spectral representation radial basis function kernel support vector machine classify types western instruments. classiﬁcation accuracy randomly mixed chords polyphonic notes generated using isolated note samples musical instrument sound database around polyphony notes polyphony notes. shown above previous works focused identiﬁcation instrument sounds clean solo tones phrases. recent research studies polyphonic sounds closer real-world situation artiﬁcially produced polyphonic music still professionally produced music. real-world music many factors affect recognition performance. instance might highly different timbre depending genre style performance. addition audio might differ quality great extent depending recording production environments. paper investigate method predominant instrument recognition professionally produced western music recordings. utilize convolutional neural networks learn spectral characteristics music recordings musical instruments perform instrument identiﬁcation polyphonic music excerpts. major contributions work presented paper follows. present convnet architecture predominant musical instrument identiﬁcation training data single labeled target data multi-labeled unknown number classes existing data. introduce method aggregate outputs convnets short-time sliding windows predominant instruments music excerpt variable length conventional method majority vote often fails. remainder paper organized follows. section introduce emerging deep neural network techniques ﬁeld. next system architecture section includes audio preprocessing proposed network architecture detailed training conﬁguration explanation various activation functions used experiment. section evaluation section contains information dataset testing conﬁguration including aggregation strategy evaluation scheme. then illustrate performance proposed convnet section results section analysis effects activation function analysis window size aggregation strategy identiﬁcation threshold instrument-wise analysis. moreover present qualitative analysis based visualization convnet’s intermediate outputs understand network captured pattern input data. finally conclude paper section ability traditional machine learning approaches limited terms processing input data form. hence usually input learning system typically classiﬁer hand-crafted feature representation requires extensive domain knowledge careful engineering process. however getting common design system automatically discover higherlevel representation data stacking several layers nonlinear modules called deep learning recently deep learning techniques widely used across number domains owing superior performance. basic architecture deep learning called deep neural network feedforward network multiple hidden layers artiﬁcial neurons. dnn-based approaches outperformed previous state-of-the-art methods speech applications phone recognition largevocabulary speech recognition multi-lingual speech recognition noise-robust speech recognition many variants modiﬁed architectures deep learning depending target task. especially recurrent neural networks convnets recently shown remarkable results various multimedia information retrieval tasks. rnns highly powerful approaches sequential inputs recurrent architecture enables hidden units implicitly maintain information past elements sequence. since languages natively contain sequential information widely applied handle text characters spoken language. reported rnns shown successful result language modeling spoken language understanding hand convnet useful data local groups values highly correlated forming distinctive local characteristics might appear different parts array hence popular approaches recently image processing area handwritten digit recognition mnist dataset image tagging cifar- dataset. addition reported outperformed state-of-the-art approaches several computer vision benchmark tasks object detection semantic segmentation category-level object recognition also speech-recognition tasks fig. schematic proposed convnet containing times repeated double convolution layers followed max-pooling. last max-pooling layer performs global max-pooling fully connected layer followed sigmoid outputs. composed harmonics various musical instruments human voice. musical instrument produces unique timbre different playing styles type spectral characteristics music signal might appear different location time frequency image. convnets usually composed many convolutional layers inserting pooling layer convolutional layers allows network work different time scales introduces translation invariance robustness local distortions. hierarchical network structures convnets highly suitable representing music audio music tends present hierarchical structure time different features music might salient different time scales hence although convnets commonly used technique image processing increasing number attempts apply convnets music signal. reported convnet outperformed previous state-of-the-art approaches various tasks onset detection automatic chord recognition music structure/boundary analysis instrument identiﬁcation found recent report park although still ongoing work predominant instrument recognition method; hence instruments target instrument sounds exist. research differs deal polyphonic music work based studio recording single tones. addition research also differs single-label data training estimate multi-label data used multilabel data training phase. moreover focused end-to-end approach promising using audio signals makes system rely less domain knowledge preprocessing usually shows slightly lower performance using spectral input melspectrogram recent papers data automatically discover representations needed classiﬁcation detection however appropriate preprocessing input data still important issue improve performance system. ﬁrst preprocessing step stereo input audio converted mono taking mean left right channels downsampled original sampling frequency. allows frequencies nyquist frequency sufﬁcient cover harmonics generated musical instruments removing noises possibly included frequencies range. moreover audios normalized dividing time-domain signal maximum value. then downsampled time-domain waveform converted time-frequency representation using short-time fourier transform samples window size samples size next linear frequency scale-obtained spectrogram converted mel-scale. number melfrequency bins following representation learning papers music annotation hamel reasonable setting sufﬁciently preserves harmonic characteristics music greatly reducing dimensionality input data. finally magnitude obtained mel-frequency spectrogram compressed natural logarithm. convnets seen combination feature extractor classiﬁer. convnet architecture generally follows popular alexnet vggnet structure contains deep architecture using repeated several convolution layers followed max-pooling shown figure method using smaller receptive window size smaller stride convnet becoming highly common especially computer vision ﬁeld study zeiler fergus sermanet shown superior performance ilsvrc-. description mel-spectrogram convolution ﬁlters convolution ﬁlters max-pooling dropout convolution ﬁlters convolution ﬁlters max-pooling dropout convolution ﬁlters convolution ﬁlters max-pooling dropout convolution ﬁlters convolution ﬁlters global max-pooling ﬂattened fully connected dropout sigmoid table illustrate detailed convnet architecture input size layer parameter values except zero-padding process. input convolution layer zero-padded preserve spatial resolution regardless input window size increase number channels convolution layer factor every convolution layers starting last max-pooling layer eight convolutional layers perform global max-pooling followed fully connected layer. recently reported global average pooling without fully connected layer classiﬁer layer less prone overﬁtting shows better performance image processing datasets cifar- mnist however empirical experiment found global average pooling slightly decreases performance global max-pooling followed fully connected layer works better task. last classiﬁer layer sigmoid layer. common softmax layer target label system must able handle multiple instruments present time thus sigmoid output used. training regularized using dropout rate max-pooling layer. dropout technique prevents overﬁtting units training data randomly dropping units neural network training phase furthermore added dropout fully connected layer well rate since fully connected layer easily suffers overﬁtting. addition conducted experiment various time resolutions optimal analysis size. training data ﬁxed audio performed training dividing training audio used label divided chunk. audio divided without overlap training affects validation loss used early stopping. fifteen percent training data randomly selected used validation training stopped validation loss decrease epochs. initialization network weights another important issue lead unstable learning process especially deep network. used uniform distribution zero biases convolutional fully connected layers following glorot bengio activation function followed convolutional layer fully connected layer. section introduce several activation functions used experiment comparison. traditional model activation neuron using hyperbolic tangent sigmoid function. however non-saturating nonlinearities rectiﬁed linear unit allow much faster learning saturating nonlinearities particularly models trained large datasets moreover number works shown performance relu better sigmoid tanh activation thus modern studies convnets relu model output neurons input channel. relu simply suppresses whole negative part zero retaining positive part. recently several modiﬁed versions relu introduced improve performance further. first leaky-relu introduced mass compresses negative part rather make zero might cause initially inactive units remain inactive. deﬁned parameter give small gradient negative part. second parametric relu introduced basically similar lrelu compresses negative part. however prelu automatically learns parameter negative gradient unlike lrelu. deﬁned choice activation function considerably inﬂuences identiﬁcation performance. difﬁcult speciﬁc activation function always performs best highly depends parameter setting input data. instance empirical evaluation convnet activation functions reported performance lrelu better relu prelu sometimes worse basic relu depending dataset value moreover works regarding activation function image classiﬁcation task audio processing domain. hence empirically evaluated several activation functions explained tanh relu lrelu prelu suitable activation function task. lrelu leaky relu normal leaky relu used reported performance lrelu considerably differs depending value leaky relu works better used separate test audio data irmas dataset used training. first sliding window used analyze input test audio size analysis window training phase. size sliding window half window size. then aggregated sigmoid outputs sliding windows summing outputs class-wise obtain total amount activation instrument. summed sigmoid activations normalized range dividing maximum activation. irmas dataset includes musical audio excerpts annotations predominant instruments present intended used automatic identiﬁcation predominant instruments music. dataset used paper predominant instrument classiﬁcation bosch includes music various decades past century hence differing audio quality great extent. addition dataset covers wide variability musical instrument types articulations recording production styles performers. dataset divided training testing data audio ﬁles -bit stereo wave sampling rate. training data consisted audio ﬁles excerpts distinct recordings. subjects paid obtain data pitched instruments shown table selected music tracks hand testing data consisted audio ﬁles lengths tracks training data included. unlike training data testing data contained predominant target instruments. hence total number training labels identical number audio ﬁles number testing labels number testing audio ﬁles latter multi-label. training testing dataset musical instruments percussion bass included annotation even exist music excerpts. training phase used ﬁxed length window input data convnet speciﬁc ﬁxed shape. however testing audios variable lengths much longer training audio. developing system handle variable length input data valuable music real life varies length. performed short-time analysis using overlapping windows obtain local instrument information audio excerpts. since annotation exists audio clip observed multiple sigmoid outputs aggregated make clip-wise decision. tried different strategies aggregation average normalized referred throughout paper respectively. simply took average sigmoid outputs class-wise whole audio clip thresholded without normalization. method intended capture existence instrument mean probability might return result without detected instrument. ﬁrst summed sigmoid outputs class-wise whole audio excerpt normalized values dividing maximum value among classes values scaled placed zero followed thresholding. method based assumption humans perceive predominant since number annotations class equal computed precision recall measure micro macro averages. micro averages calculated metrics globally regardless classes thus giving weight instrument higher number appearances. hand calculated metrics label found unweighted average macro averages; hence related number instances represents overall performance classes. finally repeated experiment three times calculated mean standard deviation output. used lrelu activation function analysis window aggregation strategy identiﬁcation threshold default settings experiment possible showed best performance. experiment variables listed table iii. first compared performance proposed convnet existing algorithm irmas dataset. effect activation function analysis window aggregation strategy identiﬁcation threshold recognition performance analyzed separately following subsections. result network achieved micro measure macro measure. existing algorithm fuhrmann herrera used typical hand-made timbral audio features framewise mean variance statistics train svms bosch improved algorithm source separation called fasst preprocessing step. fig. schematic obtaining multi-label output test audio signal. input audio analyzed sliding window multiple sigmoid outputs aggregated using different strategies estimate predominant instrument testing audio excerpt. instrument relatively scaled sense strongest instrument always detected existence instruments judged relative strength compared activate instrument. majority vote common choices number classiﬁcation tasks used system. majority vote ﬁrst predicts classes analysis frame vote wins. however using method task would result disregarding accompaniment instruments piano example music signal composed various musical instruments usually sounds overlapped time domain presence accompaniments usually much weaker voice lead instruments. target identify arbitrary number predominant instruments testing data instruments aggregated value threshold considered predominant instruments. using higher value identiﬁcation threshold lead better precision obviously decrease recall. hand lower threshold increase recall lower precision. hence tried range values threshold optimal value measure explained next performance evaluation section. values used values used threshold threshold values empirically chosen wide enough range best performance schematic aggregation process illustrated figure terms precision fuhrmann herrera’s algorithm showed best performance micro macro measure. however recall around resulted measure. proposed convnet architecture outperformed existing algorithms irmas dataset micro macro measure shown figure result observed learned feature input data classiﬁed convnet works better conventional handcrafted features svms. case using rectiﬁed units activation function possible observe signiﬁcant performance improvement compared tanh baseline expected shown table unlike result presented imagenet classiﬁcation work prelu show performance improvement showed matching performance relu task. hand using lrelu showed better performance using normal relu prelu. using lrelu small gradient showed similar performance relu expected lrelu leaky alpha setting showed best identiﬁcation performance matched result empirical evaluation work convnet activation function result shows suppressing negative part activation rather making zero certainly improves performance compared normal relu making whole negative part zero might cause initially inactive units never active mentioned above. moreover result shows using leaky relu proved work well image classiﬁcation task also beneﬁts musical instrument identiﬁcation. mentioned above conducted experiment diverse analysis window sizes optimal analysis resolution. figure shows micro macro measure various analysis frame sizes according identiﬁcation threshold observed longest window clearly performed poorer shorter window sizes regardless identiﬁcation threshold. however shortening analysis frame decreased overall performance again. result seen optimal analysis window size task. using shorter analysis frame helped increase temporal resolution found short window size identifying instrument. using higher value identiﬁcation threshold leads better precision decreases recall. contrary lower threshold results better recall lower precision. fig. class-wise performance instrument identiﬁcation. analyze effect parameter instrument compared optimal setting different aggregation strategy analysis window size lrelu identiﬁcation threshold. hence used measure harmonic mean precision recall evaluate overall performance. terms measure found appropriate threshold showed best performance macro measure shown figure current system uses certain identiﬁcation threshold instruments. however think might room improvement using different thresholds instrument various types instruments included experiment. example amplitude piano sound relatively small number music excerpts usually used accompanying instrument. hand ﬂute sound music mostly louder others usually used lead instrument. conducted experiment different strategies aggregation convnet outputs explained testing conﬁguration section. performance demonstrated table threshold returned highest measure strategy. result showed better identiﬁcation performance overall. slight performance micro measure difference notable macro measure. result shows performing class-wise followed normalization better aggregation method predominant instrument identiﬁcation taking class-wise mean values. likely training testing audios differing quality great extent depending recording production time audio-excerpt-wise normalization helped minimize results demonstrated focused overall identiﬁcation performance. section analyze discuss result instrument-wise observe system performance detail. shown figure identiﬁcation performance varies great extent depending instruments. regardless parameter setting observed system recognizes voice music well showing measure hand cello clarinet showed relatively poor performance compared instruments showing measure around results highly likely affected insufﬁcient number training audio samples. deep learning number training examples critical performance compared case using hand-crafted features aims learn feature low-level input data. illustrated table number training audio samples voice largest number training audio. contrary audio excerpts used cello clarinet respectively least third least number training audio. believe increasing number training data cello clarinet would helpful increase identiﬁcation performance instruments. addition number test audio samples cello clarinet much less instruments too. dataset test audio samples cello clarinet respectively ﬁrst second least number test audio audio samples human voice. evaluating system small number test data would make result less reliable less stable identiﬁcation results. apart issue related number audio high identiﬁcation performance voice class highly likely owing spectral characteristic distinct musical instruments. instruments used experiment usually produce relatively clear harmonic patterns; fig. visualization t-sne clustering result. represents clustering result intermediate state proposed model. left right ﬁrst four plots clustering result activations convolutional block last plots clustering result activations hidden dense layer ﬁnal sigmoid output respectably. upper plots drawn sample used training lower plots validation data samples. regarding aggregation strategy using instead decreased identiﬁcation performance organ piano. result indicates showed slight advantage instruments usually used accompaniment instrument using aggregation better cases. hand using analysis window instead default window considerably decreased performance especially ﬂute acoustic guitar electric guitar violin. result shows using longer analysis window disadvantage cases. finally using identiﬁcation threshold caused considerable performance loss especially ﬂute saxophone trumpet violin showed slight improvement electric guitar organ piano. result understood mean using lower threshold identiﬁcation performance helps detect instruments usually background using higher threshold suitable instruments frequently used lead instrument wind instruments usually show relatively strong presence music. mentioned results section result indicates potential performance improvement using different identiﬁcation threshold instrument. understand internal mechanism proposed model conducted visual analysis various visualization methods. first tried clustering layer’s intermediate hidden states given input data sample verify encoding behavior layer contributes clustering input samples. selected t-distributed stochastic neighbor embedding algorithm technique dimensionality reduction high-dimensional data. second exploited deconvolution method identify functionality unit proposed convnet model visual analysis. system basically repeats convolutional layers followed pooling layer grouped three components call convolutional block throughout section simplicity. t-sne algorithm based stochastic neighbor embedding algorithm converts similarities given data points joint probability embeds high-dimensional data points lower-dimensional space minimizing kuller-leibler divergence joint probability low-dimensional embedding high-dimensional data points. method highly effective especially dataset dimension high advantage algorithm accorded well condition target observations necessarily high dimension since reshaped layer’s ﬁlter activations single vector respectively. visualization exploiting t-sne could observe layer contributed classiﬁcation dataset. reﬂecting gradually changing inter-distance data points stage proposed model four intermediate activations extracted convolutional block hidden fully connected layer another ﬁnal output layer. compression dimensionality computational efﬁciency pooled maximum values activation matrices unit. process dimensionality layer’s output could diminished layer’s unit size. visualized randomly selected training validation data samples entire dataset verify model exactly works generalizes classiﬁcation capability. figure clearly shown data samples class instrument well grouped group separated farther level encoding higher particularly training set. clustering clearer former case tendency clustering validation also found similar training condition. another visualization method deconvolution recently introduced useful analysis tool qualitatively evaluate node convnet. main principle method inverse every stage operations reaching target unit generate visually inspectable image been consequence ﬁltered trained subfunctionality target unit method possible reveal intuitively internal sub-function works within entire deep convolutional network tends thought black box. functionality sub-part proposed model explored. generated deconvoluted images like figure arbitrary input melspectrogram unit entire model. visual analysis resulting images could several aspects sub-functionalities proposed model units ﬁrst layer tend extract vertical horizontal diagonal edges input spectrogram like lower layers convnets usual image object recognition task. second layer fourth layer deconvoluted image indicates unit mid-layers functionality searches particular combinations edges extracted ﬁrst layer. found difﬁcult strongly declare sub-part proposed model detects speciﬁc musical articulation expression. however inductive manner could units indicate understood sub-function musical expression detector. conducted visual analysis deconvoluted image independent music signals kind sound sources differently labeled. cases activated units ﬁrst layer strongly suggested primary functionality detect harmonic component input mel-spectrogram ﬁnding horizontal edges shown ﬁgures figure however second layer higher layers highly activated units’ behavior appeared quite different respective input signal. instance activated unit signal second layer showed functionality similar onset detection detecting combination vertical horizontal edges. compared unit activated units third layer showed different functionality seems activate unstable components vibrato articulation slur singing voice part detecting particular combination diagonal horizontal edges. hand model’s behavior signal different. clearly shown second third layers’ output figure highly activated sub-functions trying detect dense ﬁeld stable horizontal edges paper described apply convnet identify predominant instrument real-world music. trained network using ﬁxed-length single-labeled data identify arbitrary number predominant instrument music clip variable length. results showed deep convnet capable achieving good performance learning appropriate feature automatically input data. proposed convnet architecture outperformed previous state-of-the-art approaches predominant instrument identiﬁcation task irmas dataset. mel-spectrogram used input convnet source separation preprocessing unlike existing works. conducted several experiments various activation functions convnet. tanh relu used baseline recently introduced lrelu prelu also evaluated. results conﬁrmed relu worked reasonably well facto standard recent convnet studies. furthermore obtained better results lrelu normal relu especially leaky setting performance tanh worse rectiﬁer functions expected prelu showed matching performance relu task. paper also investigated different aggregation methods convnet outputs applied music excerpts various lengths. experimented different aggregation methods class-wise mean probability class-wise followed normalization experimental results showed better aggregation method effectively deals quality difference audios audio-excerpt-wise normalization process. addition conducted extensive experiment various analysis window sizes identiﬁcation thresholds. analysis window size using shorter window improved performance increasing temporal resolution. however short obtain accurate identiﬁcation performance found optimal window size. trade-off precision recall depending identiﬁcation threshold; hence used measure harmonic mean precision recall. result threshold value showed best performance. intermediate outputs using t-sne showed feature representation became clearer time input data passed convolutional blocks. moreover visualization using deconvolution showed lower layer tended capture horizontal vertical edges higher layer tended seek combination edges describe spectral characteristics instruments. fig. mel-spectrogram input signals respective deconvoluted results. left columns right columns image denoted respectively calculated independent music signals. signals polyphonic music segment randomly cropped original music. moreover signals consist mainly voice acoustic guitar sound. however dominant instrument labeled voice labeled acoustic guitar. images represents deconvoluted signal overlaid original signal. extracted results four intermediate stages proposed model. deconvolution outputs extracted convolutional block. target signals highest activated units point chosen deconvoluted visualized. left right images arranged order decreasing absolute unit activation. region green region deconvoluted image indicate positive value negative value result respectably. remaining area magnitude activation relatively lower regions. range activation result normalized purpose clear visualization. audio processing domain. however audio signal processing especially music signal processing many different aspects compared image processing area convnets extensively used. example spectral characteristics usually overlapped time frequency unlike objects image makes detection difﬁcult. moreover music signals much repetitive continuous compared natural images present various lengths. believe applying musical knowledge aggregation part adaptive thresholding instrument improve performance further warrants deeper investigation. eronen klapuri musical instrument recognition using cepstral coefﬁcients temporal features acoustics speech signal processing icassp’. proceedings. ieee international conference vol. diment rajan heittola virtanen modiﬁed group delay feature musical instrument recognition international symposium computer music multidisciplinary research marseille france l.-f. y.-h. yang sparse cepstral codes power scale instrument identiﬁcation acoustics speech signal processing ieee international conference ieee krishna sreenivas music instrument recognition isolated notes solo phrases acoustics speech signal processing proceedings.. ieee international conference vol. kitahara goto komatani ogata okuno instrument identiﬁcation polyphonic music feature weighting minimize inﬂuence sound overlaps eurasip journal applied signal processing vol. duan pardo daudet novel cepstral representation timbre modeling sound sources polyphonic mixtures acoustics speech signal processing ieee international conference lecun jackel bottou cortes denker drucker guyon muller sackinger simard learning algorithms classiﬁcation comparison handwritten digit recognition neural networks statistical mechanics perspective vol. hinton deng dahl a.-r. mohamed jaitly senior vanhoucke nguyen sainath deep neural networks acoustic modeling speech recognition shared views four research groups signal processing magazine ieee vol. schluter bock improved musical onset detection convolutional neural networks acoustics speech signal processing ieee international conference ieee humphrey bello rethinking automatic chord recognition convolutional neural networks machine learning applications international conference vol. ieee grill schl¨uter music boundary detection using neural networks combined features two-level annotations proceedings international society music information retrieval conference malaga spain palaz collobert doss estimating phoneme class conditional probabilities speech signal using convolutional neural networks arxiv preprint arxiv. simonyan zisserman very deep convolutional networks large-scale image recognition arxiv preprint arxiv. zeiler fergus visualizing understanding convolutional networks computer vision–eccv springer sermanet eigen zhang mathieu fergus lecun overfeat integrated recognition localization detection using convolutional networks arxiv preprint arxiv. kyogu associate professor seoul national university leads music audio research group. research focuses signal processing machine learning techniques applied music audio. received computerbased music theory acoustics stanford university. zhang delving deep rectiﬁers surpassing human-level performance imagenet classiﬁcation proceedings ieee international conference computer vision miyamoto kameoka roux uchiyama tsunoo nishimoto sagayama harmonic percussive sound separation application mir-related tasks advances music information retrieval. springer yoonchang born seoul republic korea studied electronic engineering systems king’s college london moved queen mary university london received meng degree digital audio music system engineering first class honours currently candidate digital contents information studies music audio research group seoul national university republic korea. main research interest lies within developing deep learning techniques automatic musical instrument recognition. jaehun born seoul republic korea currently researcher music audio research group. research interests include signal processing machine learning techniques applied music audio. received english literature linguistics seoul national university received degree digital contents information studies music audio research group seoul national university republic korea.", "year": 2016}