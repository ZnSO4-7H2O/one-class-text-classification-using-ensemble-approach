{"title": "Learning Infinite RBMs with Frank-Wolfe", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In this work, we propose an infinite restricted Boltzmann machine~(RBM), whose maximum likelihood estimation~(MLE) corresponds to a constrained convex optimization. We consider the Frank-Wolfe algorithm to solve the program, which provides a sparse solution that can be interpreted as inserting a hidden unit at each iteration, so that the optimization process takes the form of a sequence of finite models of increasing complexity. As a side benefit, this can be used to easily and efficiently identify an appropriate number of hidden units during the optimization. The resulting model can also be used as an initialization for typical state-of-the-art RBM training algorithms such as contrastive divergence, leading to models with consistently higher test likelihood than random initialization.", "text": "work propose inﬁnite restricted boltzmann machine whose maximum likelihood estimation corresponds constrained convex optimization. consider frank-wolfe algorithm solve program provides sparse solution interpreted inserting hidden unit iteration optimization process takes form sequence ﬁnite models increasing complexity. side beneﬁt used easily efﬁciently identify appropriate number hidden units optimization. resulting model also used initialization typical state-of-the-art training algorithms contrastive divergence leading models consistently higher test likelihood random initialization. restricted boltzmann machines two-layer latent variable models layer hidden units model distribution visible units rbms widely used capture complex distributions numerous application domains including image modeling human motion capture collaborative ﬁltering also widely used building blocks deep generative models deep belief networks deep boltzmann machines intractability likelihood function rbms usually learned using contrastive divergence algorithm approximates gradient likelihood using gibbs sampler. practical problem using need decide size hidden layer performing learning challenging decide optimal size. simple heuristic search ‘best number hidden units using cross validation testing likelihood within pre-deﬁned candidate set. unfortunately extremely time consuming; involves running full training algorithm possible size thus search relatively small sizes using approach. addition log-likelihood highly non-convex performance sensitive initialization learning algorithm. although random initializations routinely used practice algorithms like would valuable explore robust algorithms less sensitive initialization well smarter initialization strategies obtain better results. work propose fast greedy algorithm training rbms inserting hidden unit iteration. algorithm provides efﬁcient determine size hidden layer adaptive fashion also used initialization full cd-like learning algorithm. method based constructing convex relaxation parameterized distribution weights hidden units training problem framed convex functional optimization solved using efﬁcient frank-wolfe algorithm effectively adds hidden unit iteration solving relatively fast inner loop optimization. related work contributions connect number different themes existing work within machine learning optimization. give brief discussion prior related work. number works convex relaxations latent variable models functional space related gradient boosting method supervised learning bengio propose convex neural network number hidden units unbounded learned bach analyzes appealing theoretical properties model. clustering problems several works convex functional relaxation also proposed forms convex relaxation also developed layer latent variable models also considerable work extending directed/hierarchical models inﬁnite models dimensionality latent space automatically inferred learning. methods bayesian nonparametric models brief overview found orbanz directions explored undirected models particularly rbms. welling propose boosting algorithm feature space model; feature added boosting iteration instead hidden unit. nair hinton conceptually weights inﬁnite number binary hidden units connect sigmoid units noisy rectiﬁed linear units recently côté larochelle extend ordered model inﬁnite number hidden units nalisnick ravi technique word embedding. ordered sensitive ordering hidden units viewed mixture rbms. contrast model incorporates regular rbms special case enables model selection standard rbms. frank-wolfe method classical algorithm solve constrained convex optimization. recently received much attention uniﬁes large variety sparse greedy methods including boosting algorithms learning dual structured marginal inference using graphical models verbeek proposed greedy learning algorithm gaussian mixture models inserts component step resembles algorithm procedure. beneﬁt provides better initialization random initialization. likas investigate greedy initialization k-means clustering. dimensions respectively model parameters including pairwise interaction term r|v|×|h| bias term r|v|× visible units. drop bias term hidden units since achieved introducing dummy visible unit whose value always one. partition function serves normalize probability typically intractable calculate exactly. rbms bipartite structure conditional distributions fully factorized calculated closed form i-th column corresponds weights connected i-th hidden unit. assume hidden unit takes values softplus function marginalize form shows free softplus functions different weights energy linear term provides foundation development. given dataset {vn}n σvn) logistic function applied element-wise manner. positive part gradient calculated exactly since conditional distribution fully factorized. negative part arises derivatives log-partition function intractable. stochastic optimization algorithms persistent popular methods approximate intractable expectation using gibbs sampling. section ﬁrst generalize model deﬁned model inﬁnite number hidden units also viewed convex relaxation functional space. then describe learning algorithm. general model motivated ﬁrst term treated empirical average softplus function logv)) empirical distribution weights {wi}. extend this deﬁne general distribution weight replace empirical averaging expectation gives following generalization inﬁnite number hidden units temperature parameter controls effective number note assumed intuitively deﬁnes semi-parametric model whose probability linear bias term parameterized nonlinear term parameterized weight distribution controls magnitude nonlinear term. model regarded convex relaxation regular shown following result. proposition model includes standard special case constraining |h|. moreover log-likelihood model concave w.r.t. function respectively jointly concave point parameter plays special role model reduce standard equals number particles would otherwise fractional rbm. fractional leads challenging inference problem standard since standard gibbs sampler longer directly applicable. discuss point section given dataset {vn}n estimator involves convex functional optimization valid distributions introduce functional norm regularization penalize likelihood large values alternatively could equivalently ||w||≤c restricts challenging directly solve optimization standard gradient descent methods involves optimizing function inﬁnite dimensions. instead propose solve using frank-wolfe algorithm projection-free provides sparse solution. assume already iteration frank-wolfe ﬁnds maximizing linearization objective function step size parameter convex combination step guarantees remains distribution update. typical step-size case equals average earlier solutions obtained linear program. apply frank-wolfe solve problem need calculate functional gradient e.q. show expectation intractable calculate stochastic optimization draw samples using mcmc. note second terms gradient enforce intuitive moment matching condition optimal introduces importance weights adjust empirical data previous model moments match other. suppose optimum iteration item added shown step size taken therefore frank-wolfe update naturally interpreted greedily inserting hidden unit current model particular update temperature parameter according proposition directly transform model regular frank-wolfe step enables convenient blocked gibbs sampling inference. compared standard optimization following nice properties current model depend means draw enough samples iteration reuse optimization objective function evaluated explicitly given samples hence efﬁcient off-the-shelf optimization tools l-bfgs used solve optimization efﬁciently. iteration method involves much fewer parameters hence deﬁnes series easier problems less sensitive initialization. note similar greedy learning strategy successfully applied learning mixture models greedily inserts component step approach provide better initialization optimization using multiple random initializations. obtain update bias parameter gradient descent optimize gradient descent simply updating efﬁcient works well practice. summarize frank-wolfe learning algorithm algorithm adding hidden units rbm. besides initializing delta function random learning model scratch also adapt algorithm incrementally hidden units existing according proposition continue frank-wolfe simply initialize iterations removing hidden units. since hidden units added greedy manner want remove hidden unit frank-wolfe learning provided respect objective hidden units added. variant frank-wolfe away-steps requirement directly applied. shown improve sparsity ﬁnal solution point section need take equal number particles order model reduce standard rbm. takes general real number obtain general fractional model inference challenging standard block gibbs sampler directly applicable. practice setting correspond regular seems give best performance completeness discuss fractional detail section propose metropolis-hastings algorithm draw samples fractional rbm. believe fractional framework provides avenue improvements future work. approximation justiﬁed considering special case magnitude weights large softplus function essentially reduces relu function case become equivalent max. concretely guarantee following bound proposition therefore fractional well approximated standard leveraged design inference algorithm example gibbs update proposal metropolis-hastings update speciﬁc given min)) section test performance frank-wolfe learning algorithm datasets mnist caltech silhouettes mnist handwritten digits database contains images training test images image includes pixels associated digit label binarize grayscale images thresholding pixels randomly select images training validation set. caltech silhouettes dataset images binary pixels image represents objects silhouette class label dataset divided three subsets examples training validation testing. figure average test log-likelihood datasets increase number hidden units. correctly identify appropriate hidden layer size high test log-likelihood addition initialized gives higher test likelihood random initialization number hidden units. best viewed color. training algorithms train rbms algorithm. ﬁxed learning rate selected using validation mini-batch size selected epochs training minist epochs caltech. early stopping applied monitoring difference average log-likelihood training validation data intractable log-partition function cancelled train rbms hidden units. incrementally train model frank-wolfe algorithm ﬁxed step size selected using validation data regularization strength selected algorithm early stopping criterion randomly initialize algorithm times select best validation set; meanwhile also initialize model learned frank-wolfe. test likelihood evaluate test likelihood learned models estimate partition function using annealed importance sampling temperature parameter selected following standard guidance ﬁrst temperatures spaced uniformly spaced uniformly spaced uniformly gives total intermediate distributions. summarize averaged test log-likelihood mnist caltech silhouettes figure report result averaged runs experiments error bars indicating standard deviations estimations. evaluate test likelihood model adding every hidden units. perform early stopping average log-likelihood training validation data largely increases. shown figure procedure selects hidden units mnist hidden units caltech; purely illustration purposes continue experiment reaching hidden units. identiﬁed number hidden units roughly corresponds maximum test log-likelihood three algorithms suggesting identify appropriate number hidden units optimization. also model learned initialization consistently performs better best result random initializations. implementation running time procedure twice number hidden units. therefore initialized provides practical strategy learning rbms requires approximately three times computation time single simultaneously identifying proper number hidden units obtaining better test likelihood. classiﬁcation performance method evaluated using discriminant image classiﬁcation tasks. take hidden units’ activation vectors generated three algorithms figure feature multi-class logistic regression class labels mnist caltech. figure basic tends worse fully trained small numbers hidden units added outperforms hidden units added. meanwhile initialized outperforms using best random initializations. work propose convex relaxation restricted boltzmann machine inﬁnite number hidden units whose corresponds constrained convex program function space. solve program using frank-wolfe provides sparse greedy solution interpreted inserting single hidden unit iteration. method allows easily identify appropriate number hidden units progress learning provide advanced initialization strategy state-of-the-art training methods achieve higher test likelihood random initialization. references aslan cheng zhang schuurmans. convex two-layer modeling. nips bach. breaking curse dimensionality convex neural networks. arxiv. belanger sheldon mccallum. marginal inference mrfs using frank-wolfe. nips workshop beygelzimer hazan kale luo. online gradient boosting. nips bradley bagnell. convex coding. clarkson. coresets sparse greedy approximation frank-wolfe algorithm. transactions m.-a. côté larochelle. inﬁnite restricted boltzmann machine. neural computation frank wolfe. algorithm quadratic programming. naval research logistics quarterly friedman. greedy function approximation gradient boosting machine. annals statistics guélat marcotte. comments wolfe’s ‘away step’. mathematical programming hinton. practical guide training restricted boltzmann machines. utml hinton osindero y.-w. teh. fast learning algorithm deep belief nets. neural computation hinton. training products experts minimizing contrastive divergence. neural computation jaggi. revisiting frank-wolfe projection-free sparse convex optimization. icml krishnan lacoste-julien sontag. barrier frank-wolfe marginal inference. nips krizhevsky hinton factored -way restricted boltzmann machines modeling natural images. likas vlassis verbeek. global k-means clustering algorithm. pattern recognition marlin swersky chen freitas. inductive principles restricted boltzmann machine nalisnick ravi. inﬁnite dimensional word embeddings. arxiv. nowozin bakir. decoupled approach exemplar-based unsupervised learning. icml orbanz teh. bayesian nonparametric models. encyclopedia machine learning. salakhutdinov hinton. deep boltzmann machines. aistats salakhutdinov murray. quantitative analysis deep belief networks. icml salakhutdinov mnih hinton. restricted boltzmann machines collaborative ﬁltering. icml", "year": 2017}