{"title": "Understanding Dropout: Training Multi-Layer Perceptrons with Auxiliary  Independent Stochastic Neurons", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "In this paper, a simple, general method of adding auxiliary stochastic neurons to a multi-layer perceptron is proposed. It is shown that the proposed method is a generalization of recently successful methods of dropout (Hinton et al., 2012), explicit noise injection (Vincent et al., 2010; Bishop, 1995) and semantic hashing (Salakhutdinov & Hinton, 2009). Under the proposed framework, an extension of dropout which allows using separate dropping probabilities for different hidden neurons, or layers, is found to be available. The use of different dropping probabilities for hidden layers separately is empirically investigated.", "text": "abstract. paper simple general method adding auxiliary stochastic neurons multi-layer perceptron proposed. shown proposed method generalization recently successful methods dropout explicit noise injection semantic hashing proposed framework extension dropout allows using separate dropping probabilities different hidden neurons layers found available. different dropping probabilities hidden layers separately empirically investigated. paper describe simple extension multi-layer perceptron uniﬁes recently proposed training tricks training mlp. example proposed extension generalization using dropout training proposed method extends conventional deterministic augmenting hidden neuron auxiliary stochastic neuron activation needs sampled. activation added stochastic neurons independent variables weight edge connecting auxiliary neuron existing hidden neuron ﬁxed learned. consequently learning parameters extended require special learning algorithm standard backpropagation paper starts brieﬂy describing proposed method adding auxiliary stochastic neurons mlp. then described dropout explicit noise injection well semantic hashing special cases proposed framework. understanding method dropout proposed framework reveals possible separate dropping probabilities hidden neurons single empirical investigation provided using different dropping probabilities separate hidden layers. follows predeﬁned probability distribution value sampled evaluation since incoming edge auxiliary neuron neuron independent variable mlp. case activation j-th hidden neuron l-th layer nonlinear function. i-th hidden neuron hidden layer edge weight respectively. hyperbolic tangent function tanh rectiﬁed linear function common choice. fig. illustration. straightforward learn parameters mlp. since attempt learn usual backpropagation used. difference ordinary auxiliary stochastic neurons activations auxiliary neurons need sampled forward computation. however ﬁxed parameters either learned predetermined user trivial make prediction given sample. stochastic activation auxiliary neurons forward computation output neurons differ. obvious approach compute output activation several times take average pick frequent one. however preferred increased computational cost well potentially high variance. another preferred compute expected activation output neurons distribution deﬁned auxiliary stochastic neurons. often difﬁcult well nonlinear activation functions. however possible linearize computational path approximating nonlinear function linearly push expectation operator auxiliary neuron. then compute approximate expected activation output neurons ﬁnal prediction. author acknowledges similar method hidden neuron independent noise source called semi-hard stochastic neuron recently proposed independently work. dropout regularization technique forces activations randomly selected half hidden neurons layer zero training mlp. like proposed method adding auxiliary stochastic neurons training dropout changes forward computation leaves error backpropagation consider using rectiﬁed linear hidden neurons. then learning dropout equivalent training auxiliary stochastic neurons follows bernoulli distribution mean weight edge connecting auxiliary stochastic neuron hidden neuron negative inﬁnity. then activation exact procedure applies hidden neuron activation function converges zero limit negative inﬁnity. logistic sigmoid activation function example. cases types activation functions ways ﬁxing connection strengths hidden neuron corresponding auxiliary neuron needed. trained using procedure dropout proposed outgoing weights halved compensate loss approximately half hidden neurons training phase. mild approximation show procedure halving outgoing weights corresponds computing expected activation output neurons auxiliary stochastic neurons. linearly approximate expectation output neurons push expectation operator evaluation activation hidden neuron activation dropped zero probability expected activation becomes instance case rectiﬁed linear hidden neuron linear approximation unnecessary computing expectation becomes exact output neurons linear single layer hidden neurons. agrees well original formulation dropout formulated procedure halving outgoing weights taking geometric average exponentially many neural networks share parameters. however procedure perspectives becomes approximate number nonlinear hidden layers increases. proposed framework that albeit informally procedure halving outgoing weights well approximated activation function hidden neuron approximated well linearly. potential consequences this. applying dropout hidden neurons activation function well approximated linearly max-pooling work well noticed already previous work secondly piece-wise linear activation function rectiﬁed linear function well-suited using dropout. agree well recent ﬁnding another piece-wise linear activation function called maxout works well dropout formulation extend original dropout dropping hidden neuron probability instead case testing time outgoing weight multiplied furthermore allows different dropping probabilities hidden neurons. denote dropping probability hidden neuron j-th hidden neuron l-th hidden layer section describe popular training schemes realized special cases proposed procedure adding auxiliary stochastic neurons. training schemes discuss denoising semantic hashing denoising autoencoder aims reconstruct clean sample given explicitly corrupted input. obvious special case proposed general framework. section consider adding additive white gaussian noise input component. constructed ordinary autoencoder adding additional hidden layer input ﬁrst hidden layer. additional layer many hidden neurons number input variables. hidden neuron connected i-th input component weight auxiliary stochastic neuron follows standard normal distribution. connection strength time computed activation sampled standard normal distribution. equivalent explicitly adding additive white gaussian noise variance training over compute hidden activation original ﬁrst computing expected activation since activation simply copy input words learned parameters parameters ordinary autoencoder trained without explicitly adding noise. adding intermediate hidden layers auxiliary stochastic neurons emulate adding multiple types noise sequentially input. instance common practice adding white gaussian noise dropping small portion input components achieved adding another intermediate hidden layer drops components randomly like dropout described section method explicitly injecting noise input obviously applicable standard performs classiﬁcation furthermore proposed framework method naturally allows noise even hidden neurons work regularization similarly using dropout. idea adding noise hidden neurons well input variables recently applied deep generative stochastic network semantic hashing proposed extract binary code document using deep autoencoder small sigmoid bottleneck layer. important details training procedure white gaussian noise input signal bottleneck layer encourage activations hidden neurons bottleneck layer close possible. procedure exactly equivalent adding auxiliary stochastic neuron bottleneck hidden neuron. activation auxiliary stochastic neuron sampled standard normal distribution multiplied connection strength corresponds variance added noise. since connection strength ﬁxed auxiliary stochastic neuron independent input neuron ordinary backpropagation used without complication resulting stochastic auxiliary neurons. although paper focuses interpreting various recently proposed training schemes proposed framework adding auxiliary stochastic neurons. able potentially useful extensions existing schemes understanding perspective. extend usage dropout using different dropping probabilities hidden neurons another inject white gaussian noise hidden neurons. section present preliminary experiment result showing effect using separate dropping probability hidden layer injecting white gaussian noise input hidden neuron. fig. contour plots interpolated classiﬁcation errors. figure obtained mlps trained using separate dropping probabilities hidden layers. figure obtained mlps trained injecting white gaussian noise inputs hidden neurons using separate standard deviations hidden layers. ﬁgures best viewed color. trained mlps hidden layers rectiﬁed linear neurons handwritten digit dataset using either dropout separate dropping probabilities hidden layers injecting white gaussian noise. effect choosing separate dropping probabilities hidden layers trained mlps dropping probability l-th hidden layer randomly selected interval similarly mlps trained separate noise variances hidden layers exponent noise variance l-th hidden layer randomly chosen training training samples randomly split training validation sets ratio learning early stopped checking prediction error validation maximum number epochs limited used recently proposed method called adadelta adapt learning rates automatically. since ﬁxed size effectively means hyperparameters tune. result ﬁrst experiment tested using separate dropping probabilities hidden layers shown fig. interestingly observed dropping probability near resulted relative good accuracy. however extreme dropping probability close either used ﬁrst hidden layer performance dropped signiﬁcantly regardless dropping probability second hidden layer using small dropping probability second hidden layer also turned hurt generalization performance signiﬁcantly. suggests fig. result second experiment shown. general shows generalization performance highly affected level noise injected ﬁrst hidden layer accordance previous research showing adding noise input improves classiﬁcation accuracy test samples however closer look ﬁgure shows adding noise upper hidden layer helps achieving better generalization performance important lesson preliminary experiments possible achieve better generalization performance carefully tuning auxiliary stochastic neurons. amounts instance choosing different dropping probabilities case dropout injecting different levels gaussian noise. deeper investigation using various architectures datasets however required. paper described general method adding auxiliary stochastic neurons multi-layer perceptron procedure effectively makes hidden neurons stochastic require change standard backpropagation algorithm commonly used train mlp. proposed method turned generalization recently introduced training schemes. instance dropout found special case binary auxiliary neurons connection strengths dependent input signal. method explicitly injecting noise input neurons used instance denoising autoencoder found obvious application proposed auxiliary stochastic neurons following standard normal distribution. furthermore found trick making activations hidden neurons bottleneck layer autoencoder used semantic hashing equivalent simply adding white gaussian auxiliary stochastic neuron hidden neuron bottleneck layer. paper however attempt explain instance dropout helps achieving better generalization performance. training dropout proposed framework differ greatly ordinary training. difference randomness explicitly deﬁned injected auxiliary stochastic neurons. left future investigate whether simple injection randomness causes favorable performance trained dropout exist behind-the-scene explanations. argument applies denoising autoencoders semantic hashing well. important thing note proposed method equivalent building stochastic activation functions. possible equivalent model auxiliary stochastic neurons guaranteed expected every stochastic emulated ordinary augmented auxiliary stochastic neurons. however advantage using proposed method adding auxiliary neurons compared true stochastic need modunderstanding method dropout proposed framework another extension found allows using separate dropping probability hidden layer. similarly observed also possible proposed framework inject white gaussian noise hidden layer instead injecting input. experiments provided empirical evidence showing better generalization performance achieved using separate dropping probabilities different hidden layers case dropout well injecting white gaussian noise hidden layers. experiments quite limited however extensive evaluation required future.", "year": 2013}