{"title": "Why PairDiff works? -- A Mathematical Analysis of Bilinear Relational  Compositional Operators for Analogy Detection", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "abstract": "Representing the semantic relations that exist between two given words (or entities) is an important first step in a wide-range of NLP applications such as analogical reasoning, knowledge base completion and relational information retrieval. A simple, yet surprisingly accurate method for representing a relation between two words is to compute the vector offset (\\PairDiff) between their corresponding word embeddings. Despite the empirical success, it remains unclear as to whether \\PairDiff is the best operator for obtaining a relational representation from word embeddings. We conduct a theoretical analysis of generalised bilinear operators that can be used to measure the $\\ell_{2}$ relational distance between two word-pairs. We show that, if the word embeddings are standardised and uncorrelated, such an operator will be independent of bilinear terms, and can be simplified to a linear form, where \\PairDiff is a special case. For numerous word embedding types, we empirically verify the uncorrelation assumption, demonstrating the general applicability of our theoretical result. Moreover, we experimentally discover \\PairDiff from the bilinear relation composition operator on several benchmark analogy datasets.", "text": "representing semantic relations exist given words important ﬁrst step wide-range applications analogical reasoning knowledge base completion relational information retrieval. simple surprisingly accurate method representing relation between words compute vector offset corresponding word embeddings. despite empirical success remains unclear whether pairdiff best operator obtaining relational representation word embeddings. conduct theoretical analysis generalised bilinear operators used measure relational distance word-pairs. show that word embeddings standardised uncorrelated operator independent bilinear terms simpliﬁed linear form pairdiff special case. numerous word embedding types empirically verify uncorrelation assumption demonstrating general applicability theoretical result. moreover experimentally discover pairdiff bilinear relation composition operator several benchmark analogy datasets. different types semantic relations exist between words hypernymy ostrich bird antonymy consider entities obcold. serve even richer diversity relations founder-of bill gates microsoft capital-of tokyo japan. identifying relations words entities important various natural language processing tasks automatic knowledge base completion analogical reasoning relational information retrieval example solve word analogy problem form relationship words pair must correctly identiﬁed order candidates similar relations example given query bill gates microsoft steve jobs relational search engine must retrieve apple inc. founder-of relation exists ﬁrst second entity pairs. main approaches creating relation embeddings identiﬁed literature. ﬁrst approach given corpora knowledge bases word relation embeddings jointly learnt objective optimised approach word relation embeddings considered independent parameters must learnt embedding method. example transe learns word relation embeddings accurately predict relations given knowledge base using learnt word relation embeddings. relations learnt independently words refer methods based approach independent relational embedding methods. second approach creating relational embeddings apply operator word embeddings compose embedding relation exits words any. contrast ﬁrst approach learn relational embeddings hence considered unsupervised setting compositional operator predeﬁned. popular operator composing relational embedding word embeddings pairdiff vector difference word embeddings speciﬁcally given words represented word embeddings respectively relation given pairdiff operator. mikolov showed pairdiff accurately solve analogy equations queen woman used arrows denote embeddings corresponding words. bollegala showed pairdiff used proxy learning better word embeddings vylomova conducted extensive empirical comparison pairdiff using dataset containing different relation types. besides pairdiff concatenation circular correlation convolution used prior work representing relations words. relation embedding composed using word embeddings instead learning separate parameter refer methods based approach compositional relational embedding methods. note approach implicitly assumed exist single relation words. paper focus operators used compositional relational embedding methods. assume words relations represented vectors embedded common space operator seeking must able produce vector representing relation words given word embeddings input. although different proposals computing relational embeddings word embeddings remains unclear best operator task. space operators used compose relational embeddings open vast. space particular interest computational point-of-view bilinear operators parametrised using tensors matrices. specifically consider operators consider pairwise interactions word embeddings contributions individual word embeddings towards relational embedding optimality relational compositional operator evaluated example using expected relational distance/similarity analogous nonanalogous word-pairs. assume word embeddings standardised uncorrelated word-pairs i.i.d prove bilinear relational compositional operators independent bilinear pairwise interactions input word embeddings. moreover regularised settings bilinear operator simpliﬁes linear combination input embeddings expected loss positive negative instances becomes zero. empirically validate uncorrelation assumption different pre-trained word embeddings continuous bag-of-words model skip-gram negative sampling global vectors word embeddings created using latent semantic analysis sparse coding latent dirichlet allocation empirical evidence implies theoretical analysis applicable relational representations composed wide-range word embedding learning methods. moreover experimental results show bilinear operator reaches optimal performance different word-analogy benchmark datasets satisﬁes requirements pairdiff operator. hope theoretical analysis expand understanding relational embedding methods inspire future research accurate relational embedding methods using word embeddings input. already mentioned methods representing relation words broadly categorised groups depending whether relational embeddings learnt independently word embeddings composed word embeddings case relational embeddings fully depend input word embeddings. next brieﬂy overview different methods fall category. detailed survey relation embedgiven knowledge base entity linked entity relation transe model scores tuple norm vector proposed rescal uses hmrt scoring function matrix embedding relation similar rescal neural tensor network also models relation matrix. however compared vector embeddings relations matrix embeddings increase number parameters estimated resulting increase computational time/space likely overﬁt. overcome limitations distmult models relations vectors elementwise multilinear product unfortunately distmult cannot capture directionality relation. complex embeddings overcome limitation distmult using complex embeddings deﬁning score real part denotes complex conjugate observation made mikolov relation words represented difference word embeddings sparked renewed interest methods compose relational embeddings using word embeddings. word analogy datasets google dataset semeval task dataset bats etc. established benchmarks evaluating word embedding learning methods. different methods proposed measure similarity relations exist given word pairs cosmult cosadd pairdiff vylomova studied extent vectors generated using simple pairdiff encode different relation types. supervised classiﬁcation settings conclude pairdiff cover wide range semantic relation types. holographic embeddings proposed nickel circular convolution embeddings words create embedding relation exist between words. showed circular correlation indeed elementwise product fourier space mathematically equivalent complex embeddings although pairdiff operator widely used prior work computing relation embeddings word embeddings best knowledge theoretical analysis conducted explaining conditions pairdiff optimal focus paper. consider problem representing semantic relation given words assume already represented d-dimensional space respectively word embeddings relation words represented using different linear algebraic structures. popular alternatives vectors matrices vector representations preferred matrix representations smaller number parameters learnt words relations represented dimensional space useful performing linear algebraic operations using representations space. example transe strength relation exists words computed norm vector using word relation embeddings. direct comparisons word relation embeddings would possible words relations embedded vector space. ﬁrst project word embeddings lower δ-dimensional space using dimensionality reduction method whereas learn higher δ-dimensional overcomplete word representations original d-dimensional word embeddings. therefore limit theoretical analysis case ease description. different functions used satisfy domain range requirements speciﬁed limit bilinear functions general functional form given here rd×d×d -way tensor slice real matrix. denote k-th slice element ﬁrst term corresponds pairwise interactions rd×d nonsingular projection matrices involving ﬁrst-order contributions respectively towards consider problem learning simplest bilinear functional form according given dataset analogous word-pairs t))}. speciﬁcally would like learn parameters distance analogous wordpairs minimised. concrete example distance function consider popularly used euclidean distance loss) word pairs given provided analogous word-pairs task could trivially achieved setting parameters zero. however trivial solution would generalise unseen test data. therefore addition would require nonanalogous word-pairs negative examples. negative examples often generated prior work randomly corrupting positive relational tuples training adversarial generator analyse properties relational embeddings. uncorrelation correlation distinct dimensions word embedding zero. might think uncorrelation word embedding dimensions strong assumption later show validity empirically wide range word embeddings. standerdised zero mean unit variance. linear transformation word embedding space affect topology embedding space. particular translating word embeddings zero mean shown improve performance similarity tasks holds theorem consider bilinear relational embedding deﬁned computed using uncorrelated word embeddings. word embeddings standerdised expected loss given relationally independent word pairs independent likewise uncorrelation assumption relational independence follows expectations zero. similar argument used show terms involve disappear therefore play part expected loss positive examples. similarly show independent expected loss negative examples. therefore expected loss entire training dataset independent regularised loss special case attempt minimise expected loss regularisation frobenius norm regularisation achieved sending zero tensor according theorem independent four expectations zero uncorrelation assumption. term equal standeridisation assumption cancel out. similar argument used show third expectation term vanishes. independence relational ep+ep+ moreover word embeddings assumed standerdised unit variance therefore evaluates zero none terms arising purely remain expected loss positive examples. word embeddings. here empirically verify uncorrelation assumption different input word embeddings. purpose create cbow glove embeddings ukwac corpus. context window tokens select words occur least times corpus. publicly available implementations methods original authors parameters recommended values create dimensional word embeddings. representative counting-based word embeddings create word co-occurrence matrix weighted positive pointwise mutual information apply singular value decomposition obtain -dimensional embeddings refer latent semantic analysis embeddings. latent dirichlet allocation create topic model represent word distribution topics. ideally topic capture semantic category topic distribution provides semantic representation word. gensim extract topics january dump english wikipedia. contrast abovementioned word embeddings dense structured used hierarchical sparse coding produce sparse hierarchical word embeddings. given word embedding matrix rm×d correspond d-dimensional embedding word vocabulary containing words compute correlation matrix rd×d element denotes pearson correlation coefﬁcient j-th dimensions word embeddings words. construction histograms cross-dimensional correlations shown figure dimensional word embeddings obtained methods described above. mean absolute pairwise correlations embedding type standard deviation indicated ﬁgure. similarly fourth expectation term q)ii independent positive instances equal expected loss negative instances gives relational embedding given interesting note pairdiff special case general case word embeddings nonstanderdised unit variance diagonal matrix variance i-th dimension word embedding space enforce standerdisation. considering parameters relational embedding analogous batch normalisation appropriate parameters normalisation learnt training. bedding learning method used crossdimensional correlations distributed narrow range almost zero mean. result empirically validates uncorrelation assumption used theoretical analysis. moreover result indicates theorem applied wide-range existing word embeddings. learning relation representations theoretical analysis claims performance bilinear relational embedding independent tensor operator empirically verify claim conduct following experiment. purpose bats dataset contains semantic syntactic relation types generate positive examples pairing word-pairs relation types. approximately relation type word-pairs enables generate total positive training instances form t)). pair related relation randomly select pairs different relation type according distance pairs create negative instances. collectively refer positive negative training instances training dataset. methods created learn relational embeddings according minimising loss avoid overﬁtting perform regularisation regularised diagonal matrices initialise parameters uniformly sampling adagrad initial learning rate figure shows frobenius norm tensor values word embeddings. cases training progresses goes zero predicted theorem regularisation. moreover approximately reached cases implies pairdiff operator. among input word embeddings compared figure highest mean correlation implies dimensions correlated word embeddings. expected design because hierarchical structure imposed dimensions word embedding training. however embeddings also satisfy requirements expected pairdiff. result shows claim theorem empirically true even uncorrelation assumption mildly violated. corresponding relational embeddings candidate word-pair highest relational similarity stem word-pair selected correct answer word analogy question. reported accuracy ratio correctly answered questions total number questions. hand semeval dataset semantic relations relation word-pairs four prototypical examples. task assign score word pair average relational similarity given word-pair prototypical word-pairs relation. maximum difference scaling used evaluation measure task. relational dimensional cbow embeddings. level performance reported pairdiff semeval datasets respectively shown horizontal dashed lines. figure training loss gradually decreases number training epochs performance relational embeddings semeval datasets reach pairdiff operator. result indicates relational embeddings learnt converge pairdiff operator training data also generalise unseen relation types semeval test datasets. showed that word embeddings standardised uncorrelated expected distance analogous non-analogous word-pairs independent bilinear terms relation embedding simpliﬁes popular pairdiff operator regularised settings. moreover provided empirical evidence showing uncorrelation word embedding dimensions cross-dimensional correlations narrowly distributed around mean close zero. interesting future research direction work extend theoretical analysis nonlinear relation composition operators nonlinear neural networks. seen bilinear relational representation given indeed converge form predicted theoretical analysis different types word embeddings. however remains unclear whether parameters learnt training instances generated bats dataset accurately generalise benchmark datasets analogy detection. emphasise focus outperform relational representation methods proposed previous works rather empirically show learnt operator converges popular pairdiff analogy detection task. measure generalisation capability learnt relational embeddings bats measure performance benchmark datasets semeval -task. note retrain semeval simply values learnt bats purpose evaluate generalisation learnt operator. analogical questions given stem wordpair candidate word-pairs task select word-pair relationally similar stem word-pair. relational similarity word-pairs computed cosine similarity danushka bollegala yutaka matsuo mitsuru ishizuka. relational model semantic similarity words using automatically extracted proc. lexical pattern clusters web. emnlp. pages antoine bordes nicolas usunier alberto garciadurán jason weston oksana yakhenko. translating embeddings modeling multirelational data. proc. nips. pages scott deerwester susan dumais george furnas thomas landauer richard harshman. indexing latent semantic analysis. american society information science anna gladkova aleksandr drozd satoshi matsuoka. analogy-based detection morphological semantic relations word embedproc. dings works doesn’t. hlt-naacl. pages sergey ioffe christian szegedy. batch normalization accelerating deep network training proc. mareducing internal covariate shift. chine learning research. pages omer levy yoav goldberg dagan. improving distributional similarity lessons learned word embeddings. transactions association computational linguistics maximilian nickel kevin murphy volker tresp evgeniy gabrilovich. review relational machine learning knowledge graphs. proc. ieee. volume pages ekaterina vylomova laura rimell trevor cohn timothy baldwin. take took gaggle goose book read evaluating utility vector differences lexical relational learning. proc. acl. pages", "year": 2017}