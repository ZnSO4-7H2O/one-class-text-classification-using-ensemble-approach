{"title": "Be Precise or Fuzzy: Learning the Meaning of Cardinals and Quantifiers  from Vision", "tag": ["cs.CL", "cs.AI", "cs.CV"], "abstract": "People can refer to quantities in a visual scene by using either exact cardinals (e.g. one, two, three) or natural language quantifiers (e.g. few, most, all). In humans, these two processes underlie fairly different cognitive and neural mechanisms. Inspired by this evidence, the present study proposes two models for learning the objective meaning of cardinals and quantifiers from visual scenes containing multiple objects. We show that a model capitalizing on a 'fuzzy' measure of similarity is effective for learning quantifiers, whereas the learning of exact cardinals is better accomplished when information about number is provided.", "text": "proportions sets consequence speakers reliably answer questions involving quantiﬁers even contexts preclude counting well children lacking exact cardinality concepts understand appropriately quantiﬁers grounded contexts knowledge precise numbers neither necessary sufﬁcient learning meaning quantiﬁers. inspired evidence present study proposes computational models learning meaning cardinals quantiﬁers visual scenes. hypothesis learning cardinals requires taking account number instances target object scene learning quantiﬁers instead would better accomplished model capitalizing measure evaluating ‘fuzzy’ amount target objects scene particular focus cases quantiﬁcation strategies might used namely scenes containing target distractor objects approach thus different salient objects detection distinction targets/distractors missing respect cardinals approach similar propose model counting people natural people refer quantities visual scene using either exact cardinals natural language quantiﬁers humans processes underlie fairly different cognitive neural mechanisms. inspired evidence present study proposes models learning objective meaning cardinals quantiﬁers visual scenes containing multiple objects. show model capitalizing ‘fuzzy’ measure similarity effective learning quantiﬁers whereas learning exact cardinals better accomplished information number provided. everyday life people refer quantities using either cardinals natural language quantiﬁers although share number syntactic semantic pragmatic properties learned fairly stable order acquisition across languages quantity expressions underlie fairly different cognitive neural mechanisms. first handled differently language acquisition system children recognizing disparate characteristics since early development even becoming ‘full-counters’ second neural processing cardinals relies brain region devoted representation quantities quantiﬁers rather elicit regions general semantic processing intuitively cardinals quantiﬁers refer quantities different former representing mapping word exact cardinality latter expressing ‘fuzzy’ numerical concept denoting relations scenes recent work aimed counting either everyday objects natural images geometrical objects attributes synthetic scenes respect quantiﬁers approach similar quantiﬁers some quantify sets colored dots. differently ours however works tackle issue either classiﬁcation problem visual question answering task less focus learning meaning representation cardinal/quantiﬁer. knowledge ﬁrst attempt jointly investigate mechanisms obtain meaning representaton cardinal/quantiﬁer resulting language-to-vision mapping. based geometric intepretation propose cosine product similarity between target object scene measures quantiﬁers cardinals respectively. former ranging evaluates similarity vectors respect orientation irrespectively magnitudes. vectors overall similar closer are. ideally cosine similarity image depicting scene containing either dogs without distractors equal therefore would indicate proportion ‘dogness’ scene highest. product hand deﬁned product cosine vectors euclidean magnitudes. taking account magnitudes measure ideally encodes information regarding number times target object repeated scene. above-mentioned example indeed product would respectively. simpliﬁed setting thus would equal number dogs. furthermore propose ‘objective’ meaning cardinal/quantiﬁer learned means cross-modal mapping linguistic representation target object quantity visual scene. test hypotheses carry proof-of-concept synthetic datasets describe section first explore visual data means proposed similarity measures second learn meaning representations cardinals quantiﬁers evaluate task retrieving unseen combinations order test hypothesis need dataset visual scenes crucially include multiple objects. moreover objects scene repeated might instance objects ‘three’/‘most’ dogs. although large number image datasets currently available among many others) fully satisﬁes requirements. typically images depict salient object even multiple salient objects present handful cases contain targets distractors bypass issues present work experiment synthetic visual scenes made images representing object. choice using ‘patchwork’ object-depicting images motivated need representing reasonably large variability avoid matching quantiﬁer always number target objects allow cardinals represented scenes different numbers distractors. time issues related object localization. experiment quantiﬁers most deﬁned priori ratios respectively. consistently goals arguably simpliﬁed setting neither take account pragmatic uses reﬂect possible overlappings. reasons avoid using quantiﬁers whose meaning overlaps meaning many others. cardinals concerned experiment scenarios cardinality targets ranges acquired children incrementally subsequent stages development higher numbers learned upon knowledge ability counting also ranging widely known exhibit peculiar properties images imagenet starting full list concepts corresponding images extracted cassani discarded concepts whose corresponding word low/null frequency large corpus used issues related concept identiﬁcation used single representation selected concepts. technically computed centroid vector averaging -dimension visual features corresponding images extracted used vgg- model pretrained imagenet ilsvrc data implemented matconvnet toolbox centroid vectors reduced normalized length used build scenarios. building scenarios constraint distractors different other. moreover distractors whose visual cosine similarity respect target lower average selected. scenario target distractor vectors summed together. result scenario represented vector. also experimented scenarios vectors concatenated obtain vector further reduced pca. since pattern results only-vision evaluation turned similar results obtained ‘summed’ setting space limitations focus ‘summed’ setting. datasets built dataset containing scenarios. split -datapoint training dataset training validation datapoint testing dataset testing. datasets split according ‘combinations’ mixture targets distractors scenario. reported table kept different combinations train test. note numerator refers number targets denominator total number objects. number distractors thus given difference values. illustrate train-q ‘few’ represented scenarios whereas test-q ‘few’ represented scenarios initial scenarios obtained building total different scenarios objects. particular effort paid making datasets balanced possible. designing combinations ‘few’ ‘most’ example controlled proportion targets scene order avoid making easier learn. also combinations thought avoid biasing cardinals toward ﬁxed proportions targets/distractors. only-vision evaluation ﬁrst step carry preliminary evaluation aimed exploring visual data. intuition information encoded similarity measures correct observe cosine effective product distinguishing different latter better cosine moreover qs/cs ordered scale. test hypothesis compute cosine distances product similarity target-scenario pair train test figure reports distribution respect cosine respect product train. seen boxplots ordered scale. particular cosine distance highest scenarios lowest scenarios. product highest four scenarios lowest scenarios. intuition conﬁrmed results radial-kernel classiﬁer either cosine product similarities predictors. better predicted cosine product whereas product better predictor cosine shown figure ordered scale indeed represented much lesser extent plotted product cosine similar pattern results similar plots emerged experimenting test. cross-modal mapping core proposal meaning learned means cross-modal mapping linguistic representation target object number scenarios representing target object given setting approach word represented figure reports single learning event few) proposed model. illustrate meaning learned mapping word visual scenes amount ‘targetness’ less whereas learned mapping word scenarios number targets mapping conjecture would mimic multimodal mechanism children acquire meaning learned function representing evaluated scenarios containing unseen mixture target objects distractors. encoded correct meaning quantiﬁed expression function retrieve unseen scenarios containing correct quantity target objects. experiment three different models linear cosine neural network dotproduct neural network ﬁrst model simple linear mapping. second singlelayer neural network maximizes cosine similarity input output vector third similar neural network approximates product input output. evaluate mapping functions means retrieval task aimed picking correct scenarios test among scenarios built upon target object. recall test combinations classes concept. results reported table nn-cos overbest model whereas nn-dot best model particular mean average precision higher nn-cos reaching slightly better nn-dot high number cases confounded nn-cos model conversely precision positions always higher nn-dot compared models. qualitative analysis results emerges best-predictive models make ‘plausible’ errors i.e. confound cs/qs close ordered scale. table reports confusion matrices best performing models. besides retrieving cases instead most nn-cos model often confounds similarly nn-dot model often confounds three four three overall models pick responses opposite ‘scale’ thus suggesting meaning representation learn encodes certain extent information ordered position quantiﬁed expressions. propose meaning learned means language-to-vision mapping show models capitalizing product cosine better account respectively. future research plan further investigate issue using real-scene images avoid constraining visual data. moreover plan experiment broader quantiﬁers higher cardinals. latter investigation particular would allow verify whether approach suitable ‘cardinal functions’ beyond subitizing range. might observe models keep making cognitively plausible errors picking items close target ordered scale. evidence believe would motivate ‘one quantifed expression function’ approach partially inspired evidence that human brain so-called number neurons tuned preferred numbers simplifying somewhat number would activate speciﬁc neurons. finally believe taking account speakers’ uses would constitute natural next step toward complete modelling meaning quantiﬁed expressions. grateful germán kruszewski invaluable contribution developing discussing intuitions behind work. also grateful marco baroni aurélie herbelot gemma boleda ravi shekhar advice feedback. gratefully acknowledge support nvidia corporation donation gpus used research iv&l funding second author’s research visit aimed working project. references david barner amanda libenson pierina cheung mayu takasaki. crosslinguistic relations quantiﬁers numerals language acquisition evidence japanese. journal experimental child psychology marco baroni georgiana dinu germán kruszewski. don’t count predict systematic comparison context-counting context-predicting semantic vectors. pages prithvijit chattopadhyay ramakrishna vedantam ramprasaath dhruv batra devi parikh. counting everyarxiv preprint objects everyday scenes. arxiv.. deng dong richard socher li-jia fei-fei. imagenet large-scale hierarchical image database. computer vision pattern recognition cvpr ieee conference pages ieee. justin halberda taing jeffrey lidz. development ‘most’ comprehension potential dependence counting ability preschoolers. language learning development justin johnson bharath hariharan laurens maaten fei-fei lawrence zitnick ross girshick. clevr diagnostic dataset compositional lanarxiv guage elementary visual reasoning. preprint arxiv.. napoleon katsos chris cummins maria-josé ezeizabarrena anna gavarró jelena kuvaˇc kraljevi´c gordana hrzica kleanthes grohmann athina skordi kristine jensen lópez lone sundahl crosslinguistic patterns acquisition quantiﬁers. proceedings national academy sciences tsung-yi michael maire serge belongie james hays pietro perona deva ramanan piotr dollár lawrence zitnick. microsoft coco common objects context. computer vision–eccv pages springer. henry railo veli-matti karhu jeremy mast henri pesonen mika koivisto. rapid accurate processing multiple journal objects brieﬂy presented scenes. vision olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein imagenet internalarge scale visual recognition challenge. tional journal computer vision santi seguí oriol pujol jordi vitria. learning count deep object proceedings ieee conference features. computer vision pattern recognition workshops pages ionut sorodoc angeliki lazaridou gemma boleda aurélie herbelot sandro pezzelle raffaella bernardi. ‘look green circles’ learning quantify images. proceedings workshop vision language acl. chuansheng chen yang zhang xinlin zhou. dissociated neural correlates quantity processing quantiﬁers numbers numerosities. human brain mapping shugao mehrnoosh sameki stan sclaroff margrit betke xiaohui shen brian price radomir mech. salient object subitizing. proceedings ieee conference computer vision pattern recognition pages", "year": 2017}