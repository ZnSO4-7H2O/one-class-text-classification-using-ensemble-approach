{"title": "Convolutional neural networks with low-rank regularization", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Large CNNs have delivered impressive performance in various computer vision applications. But the storage and computation requirements make it problematic for deploying these models on mobile devices. Recently, tensor decompositions have been used for speeding up CNNs. In this paper, we further develop the tensor decomposition technique. We propose a new algorithm for computing the low-rank tensor decomposition for removing the redundancy in the convolution kernels. The algorithm finds the exact global optimizer of the decomposition and is more effective than iterative methods. Based on the decomposition, we further propose a new method for training low-rank constrained CNNs from scratch. Interestingly, while achieving a significant speedup, sometimes the low-rank constrained CNNs delivers significantly better performance than their non-constrained counterparts. On the CIFAR-10 dataset, the proposed low-rank NIN model achieves $91.31\\%$ accuracy (without data augmentation), which also improves upon state-of-the-art result. We evaluated the proposed method on CIFAR-10 and ILSVRC12 datasets for a variety of modern CNNs, including AlexNet, NIN, VGG and GoogleNet with success. For example, the forward time of VGG-16 is reduced by half while the performance is still comparable. Empirical success suggests that low-rank tensor decompositions can be a very useful tool for speeding up large CNNs.", "text": "cheng tong xiao zhang xiaogang wang weinan program applied computational mathematics princeton university department electronic engineering chinese university hong kong department electrical engineering computer science university michigan arbor {chengtweinan}math.princeton.edu; yeezhangumich.edu {xiaotongxgwang}ee.cuhk.edu.hk large cnns delivered impressive performance various computer vision applications. storage computation requirements make problematic deploying models mobile devices. recently tensor decompositions used speeding cnns. paper develop tensor decomposition technique. propose algorithm computing low-rank tensor decomposition removing redundancy convolution kernels. algorithm ﬁnds exact global optimizer decomposition effective iterative methods. based decomposition propose method training low-rank constrained cnns scratch. interestingly achieving signiﬁcant speedup sometimes lowrank constrained cnns delivers signiﬁcantly better performance nonconstrained counterparts. cifar- dataset proposed low-rank model achieves accuracy also improves upon state-of-the-art result. evaluated proposed method cifar ilsvrc datasets variety modern cnns including alexnet googlenet success. example forward time reduced half performance still comparable. empirical success suggests low-rank tensor decompositions useful tool speeding large cnns. course three years cnns revolutionized computer vision setting performance standards many important applications e.g. krizhevsky farabet long breakthrough made possible abundance training data deployment computational hardware large models. models typically require huge number parameters achieve stateof-the-art performance take weeks train even high-end gpus. hand growing interest deploying cnns low-end mobile devices. processors computational cost applying model becomes problematic alone training especially real-time operation needed. storage millions parameters also complicates deployment. modern cnns would many applications computational cost storage requirement could signiﬁcantly reduced. recent works speeding cnns. denton proposed low-rank approximation clustering schemes convolutional kernels. achieved speedup single convolutional layer drop classiﬁcation accuracy. jaderberg suggested using different tensor decomposition schemes reporting speedup drop accuracy text recognition application. lebedev explored decomposition approximate convolutional kernels. vanhoucke showed using -bit quantization parameters result signiﬁcant speedup minimal loss accuracy. method used conjunction low-rank approximations achieve speedup. convolution operations constitute bulk computations cnns simplifying convolution layer would direct impact overall speedup. convolution kernels typical tensor. observation might signiﬁcant amount redundancy tensor. ideas based tensor decomposition seem particularly promising remove redundancy suggested previous works. paper develop tensor decomposition idea. method based jaderberg several signiﬁcant improvements. contributions summarized follows algorithm computing low-rank tensor decomposition. low-rank tensor decompositions non-convex problems difﬁcult compute general jaderberg iterative schemes approximate local solution. particular form low-rank decomposition exact closed form solution global optimum. hence obtain best data-independent approximation. furthermore computing exact solution much effective iterative schemes. tensor decomposition important step approximating cnns able obtain exact solution efﬁciently thus provides great advantages. method training low-rank constrained cnns scratch. previous works focus improving testing time computation cost. achieved approximating ﬁne-tuning pre-trained network. based low-rank tensor decomposition convolutional kernels parameterized naturally enforces lowrank constraint. networks parameterized low-rank constrained manner layers non-constrained counterparts. widely observed deeper networks harder train able train deep low-rank constrained cnns layers help recent training technique called batch normalization ioffe szegedy evaluation large networks. previous experiments jaderberg denton give promises effectiveness low-rank approximations. methods tested extensively large models generic datasets. moreover iterative methods used approximation local minima hurt performance. paper test proposed method various state-of-the-art models including alexnet googlenet datasets used include cifar ilsvrc. achieved signiﬁcant speedups models comparable even better performance. success variety models give strong evidence low-rank tensor decomposition useful tool simplifying improving deep cnns. numerical experiments show signiﬁcant speedup achieved minimal loss performance consistent previously reported results. surprisingly previous efforts report slight decrease change performance found signiﬁcant increase classiﬁcation accuracy cases. particular cifar- dataset achieve classiﬁcation accuracy low-rank model improves upon original also upon state-of-the-art results dataset. aware signiﬁcant improvements low-rank approximations reported previous literature. rest paper organized follows. discuss related work section introduce decomposition scheme section results typical networks including alexnet googlenet cifar ilsvrc datasets reported section conclude summary discussion section using low-rank ﬁlters accelerate convolution long history. classic examples include high dimensional wavelet systems constructed wavelets using tensor products. context dictionary learning learning separable ﬁlters suggested rigamonti speciﬁc cnns works related ours jaderberg lebedev jaderberg addition improvements summarized previous section another difference approximation stage. jaderberg network approximated layer layer. layer approximated lowrank ﬁlters parameters layer ﬁxed layers ﬁne-tuned based reconstruction error criterion. scheme ﬁne-tunes entire network simultaneously using discriminative criterion. jaderberg reported discriminative ﬁne-tuning inefﬁcient scheme found works well case. lebedev decomposition kernel tensors proposed. lebedev used non-linear least squares compute decomposition. also based tensor decomposition idea decomposition based different scheme numerical advantages. decomposition ﬁnding best low-rank approximation ill-posed problem best rank-k approximation exist general case regardless choice norm proposed scheme decomposition always exists exact closed form solution decomposition. principle decomposition scheme proposed scheme used train cnns scratch. decomposition convolutional layer replaced four convolutional layers. although effective depth network remains same makes optimization much harder gradients inserted layers prone explosion. this application scheme larger deeper models still problematic numerical issues. lastly different both consider much larger models challenging. thus results provide strong evidence low-rank approximations applicable variety state-of-the-art models. line method jaderberg proposed tensor decomposition scheme based conceptually simple idea replace convolutional kernel consecutive kernels lower rank. following introduce details decomposition algorithms using decomposition approximate pre-trained network train one. approximation pre-trained formally convolutional kernel tensor rn×d×d×c numbers output input feature maps respectively spatial kernel size. also view ﬁlter array notation rd×d×c represent n-th ﬁlter. rx×y input feature map. output feature deﬁned superscript index channels. goal approximation facilitates efﬁcient computation maintaining classiﬁcation accuracy cnn. propose following scheme hyper-parameter controlling rank rn××d×k horizontal ﬁlter rk×d××c vertical ﬁlter filters ﬁrst layer alexnet. low-rank approximation using proposed schemes corresponding speedup layer. note low-rank approximation captures information including directionality original ﬁlters. low-rank ﬁlters trained scratch intuition behind approximation scheme exploit redundancy exist spatial dimensions across channels. note convolutions equation dimensional space. estimate reduction computation scheme. direct convolution deﬁnition requires operations. scheme computational cost associated vertical ﬁlters horizontal ﬁlters giving total computational cost acceleration achieved choose principle typical ﬁrst layer acceleration times. learn approximating parameters two-step strategy. ﬁrst step approximate convolution kernel layer minimizing note step done parallel inter-layer dependence. ﬁne-tune whole based discriminative criterion restoring classiﬁcation accuracy. minimization problem closed form solution. summarized following theorem proof found appendix. theorem gives efﬁcient algorithm computing exact decomposition. theorem deﬁne following bijection maps tensor matrix rc×d×d×n rcd×dn tensor element maps figure proposed parametrization low-rank regularization. left original convolutional layer. right low-rank constraint convolutional layer rank-k. algorithm provided theorem extremely fast. experiments completes less second modern cnns small convolutional kernels. iterative algorithms jaderberg take much longer especially data-dependent criterion. addition iterative algorithms often lead local minimum leads inferior performance even ﬁne-tuning. proposed algorithm solves issue directly provides global minimum best dataindependent approximation. numerical demonstrations given section using scheme train scratch conceptually straightforward. simply parametrize convolutional form rest different training non-constrained cnn. trainable parameters. convolutional layer parametrized composition convolutional layers resulting layers original one. although effective depth increased additional layers make numerical optimization much challenging exploding vanishing gradients especially large networks. handle problem recent technique called batch normalization transform normalizes activations internal hidden units hence effective deal exploding vanishing gradients. reported ioffe szegedy deeper networks trained successfully larger learning rates used. empirically effective learning low-rank constrained networks. illustration transformation original convolutional layer low-rank constraint figure details found numerical experiments section. cifar- dataset small today’s standard good testbed ideas. deploy models baseline models; customized model. compare performance corresponding low-rank constrained versions. models dataset learned scratch. conﬁgurations baseline models low-rank counterparts outlined table substitute every single convolutional layer baseline models convolutional layers parameter introduced previous section. speciﬁcations network pairs same. rectiﬁed linear unit applied every layer except last one. implementation model slightly different introduced replace convolutional layer layer constitutes small fraction total execution time. hence efﬁciency gain factorizing layer small. networks trained back propagation optimize multinomial logistic regression objective. batch size learning learning rate initially decreases factor every time validation error stops decreasing. models dropout units probability inserted every relu. exact speciﬁcations parameters reader check https//github.com/chengtaipu/lowrankcnn. evaluated performance models without data augmentation. data augmentation images ﬂipped horizontally probability translated directions pixel. otherwise subtract mean images normalize channel. results listed table performance low-rank constrained versions networks better baseline networks without data augmentation. notably low-rank model outperforms baseline model know also better previously published results. study empirical performance speedup change vary rank choose cnn+dropout baseline model data augmentation described above. results listed table number parameters network reduced large factor especially second third layers. speedup speciﬁc layer speedup whole network achieved. practice difﬁcult speedup match theoretical gains based number operations roughly proportional reduction parameters. actual gain also depends software hardware optimization strategies convolutions. results table based nvidia titan gpus torch cudnn backend. applying low-rank constraints convolutional layers total number parameters convolutional layers reduced large factor without degrading much performance. example parameters convolutional kernels reduced relative performance +.%. nevertheless parameters fully connected layers still occupy large fraction. limits overall compression ability low-rank constraint. recent works focusing reducing parameters fully connected layers combining techniques proposed scheme explored future research. ilsvrc well-known large-scale benchmark dataset image classiﬁcation. adopt three famous models alexnet variant) vgg- googlenet variant) baselines. caffenet vgg- directly downloaded caffe’s model ﬁne-tuned training convergence bn-inception model trained scratch ourselves. introduced low-rank decomposition applied convolutional layer kernel size greater input images ﬁrst warped cropped different models. single center crop testing stage evaluate performance top- accuracy validation set. detailed training parameters available https//github.com/chengtaipu/lowrankcnn. before hyper-parameter controls trade-off speedup factor classiﬁcation performance low-rank models. therefore ﬁrst study effect layer information conﬁgure whole low-rank model better overall performance. decompose speciﬁc layer different time keeping parameters layers ﬁxed. performance ﬁne-tuning respect theoretical layer speedup demonstrated figure general choose layer value accelerates forward computation hurt performance signiﬁcantly automatic choosing based eigengap ﬁrst eigenvectors account variations. similar choosing number principal components pca. detailed low-rank model structures listed table table low-rank models ilsvrc. vgg- convolution module contains three sub-convolutional layers. googlenet inception module contains consecutive convolutional layers. corresponding shown cell brevity. baselines beginning performance restored ﬁne-tuning. claimed denton data-dependent criterion leads better performance found true upon approximation ﬁne-tuning difference criteria negligible last compare low-rank models baselines perspective classiﬁcation performance well time space consumption. results summarized table low-rank models achieve comparable performances. initialized closed form weights approximation slightly inferior baselines. low-rank alexnet trained scratch could achieve even better performance. observation reveals low-rank structure could better discriminative power generalization ability. hand running time number parameters consistently reduced. note large gaps theoretical actual speedup mainly implementations current operations signiﬁcantly slow forward computation. suggests room accelerating low-rank models designing speciﬁc numerical algorithms. paper explored using tensor decomposition techniques speedup convolutional neural networks. introduced algorithm computing low-rank tensor decomposition method training low-rank constrained cnns scratch. proposed method evaluated variety modern cnns including alexnet googlenet success. gives strong evidence low-rank tensor decomposition generic tool speeding large cnns. table comparisons low-rank models baselines. theoretical speedup weights reduction computed concerning convolutional layers decomposed. actual speedup based forward computation time whole net. hand interesting fact low-rank constrained cnns sometimes outperform non-constrained counterparts points things. local minima issue. although expressive power low-rank constrained cnns strictly smaller non-constrained observed cases former smaller training error. seems suggest low-rank form helps cnns begin better initialization settles better local minimum. issue over-ﬁtting. shown observation many cases constrained model higher training error generalizes better. overall suggests room improvement numerical algorithms regularizations models. yangqing shelhamer evan donahue jeff karayev sergey long jonathan girshick ross guadarrama sergio darrell trevor. caffe convolutional architecture fast feature embedding. arxiv preprint arxiv. lebedev vadim ganin yaroslav rakhuba maksim oseledets ivan lempitsky victor. speeding-up convolutional neural networks using ﬁne-tuned cp-decomposition. arxiv preprint arxiv. russakovsky olga deng krause jonathan satheesh sanjeev sean huang zhiheng karpathy andrej khosla aditya bernstein michael berg alexander fei-fei imagenet large scale visual recognition challenge. ijcv april szegedy christian yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru vanhoucke vincent rabinovich andrew. going deeper convolutions. arxiv preprint arxiv.", "year": 2015}