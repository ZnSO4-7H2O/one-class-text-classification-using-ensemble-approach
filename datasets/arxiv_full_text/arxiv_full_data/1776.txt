{"title": "Neural Semantic Encoders", "tag": ["cs.LG", "cs.CL", "stat.ML"], "abstract": "We present a memory augmented neural network for natural language understanding: Neural Semantic Encoders. NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves over time and maintains the understanding of input sequences through read}, compose and write operations. NSE can also access multiple and shared memories. In this paper, we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks: natural language inference, question answering, sentence classification, document sentiment analysis and machine translation where NSE achieved state-of-the-art performance when evaluated on publically available benchmarks. For example, our shared-memory model showed an encouraging result on neural machine translation, improving an attention-based baseline by approximately 1.0 BLEU.", "text": "present memory augmented neural network natural language understanding neural semantic encoders. equipped novel memory update rule variable sized encoding memory evolves time maintains understanding input sequences read compose write operations. also access multiple shared memories. paper demonstrated effectiveness ﬂexibility different natural language tasks natural language inference question answering sentence classiﬁcation document sentiment analysis machine translation achieved state-of-the-art performance evaluated publically available benchmarks. example shared-memory model showed encouraging result neural machine translation improving attention-based baseline approximately bleu. recurrent neural networks successful modeling sequences particularly rnns equipped internal short memories long short-term memories achieved notable success sequential tasks lstm powerful learns control short term memories. however short term memories lstm part training parameters. imposes practical difﬁculties training modeling long sequences lstm. recently several studies explored ways extending neural networks external memory unlike lstm short term memories training parameters neural network longer coupled adapted. paper propose novel class memory augmented neural networks called neural semantic encoders natural language understanding. offers several desirable properties. variable sized encoding memory allows model access entire input sequence reading process; therefore efﬁciently delivering long-term dependencies time. encoding memory evolves time maintains memory input sequence read compose write operations. sequentially processes input supports word compositionality inheriting temporal hierarchical nature human language. read write relevant encoding memories simultaneously multiple nses access shared encoding memory effectively supporting knowledge representation sharing. ﬂexible robust suitable practical tasks trained easily gradient descent optimizer. evaluate different real tasks. four them models state-of-theart results. results suggest model shared memory encoder decoder promising approach sequence transduction problems machine translation abstractive summarization. particular observe attention-based neural machine translation improved shared-memory models. also analyze memory access pattern compositionality show model captures semantic syntactic structures input sentence. figure high-level architectures neural semantic encoders. reads writes encoding memory time step mma-nse accesses multiple relevant memories simultaneously pioneering work attempts extend deep neural networks external memory neural turing machines implements centralized controller ﬁxed-sized random access memory. memory addressable content location based access mechanisms. authors evaluated algorithmic tasks copying sorting sequences. comparison neural turing machines addresses certain drawbacks ntm. single centralized controller usually takes modular approach. main controller decomposed three separate modules performs read compose write operation. compose module introduced addition standard memory update operations order process memory entries input information. main advantage memory update. despite sophisticated addressing mechanism controller mechanism avoid information collision memory. particularly controller emits separate access weights explicitly encode knowledge information read written moreover ﬁxed-size memory memory allocation de-allocation protocol. therefore unless controller intelligent enough track previous read/write information hard processing long sequences memory content overlapped information overwritten throughout different time scales. think potential reason makes hard train makes training stable. also note effectiveness location based addressing introduced unclear. introduce novel systematic memory update approach based soft attention mechanism. writes information recently read memory locations. accomplished sharing memory vector read write modules. memory update scalable potentially robust train. provided variable sized memory thus unlike size memory relaxed. novel memory update mechanism variable sized memory together prevent information collision issue avoid need memory allocation de-allocation protocols. memory location memory stores token representation input sequence encoding. provides anytime-access entire input sequence including tokens future time scales permitted attention-based encoders. lastly addresses small algorithmic problems focuses large-scale language understanding tasks. rnnsearch model proposed seen variation memory augmented networks ability read historic output states rnns soft attention. work combines soft attention memory networks similar rnnsearch memnns designed non-writable memories. constructs layered memory representations showed promising results artiﬁcial real question answering tasks. note rnnsearch memnns avoid memory update management overhead simply using non-writable memory storage. another variation memnns dynamic memory network equipped episodic memory seems ﬂexible different settings. although differs memory-augumented models many aspects soft attention mechanism type similarity measures retrieve relevant information external memory. example implements cosine similarity memnns vector product. uses vector product similarity measure faster compute. related work includes neural program-interpreters learns sub-programs compose high-level programs. uses execution traces provide full supervision. researchers also explored ways unbounded memory lstm using particular data structure. although type architecture provides ﬂexible capacity store information memory access constrained data structure used memory bank stack queue. overall expensive train scale previously proposed memory-based models. models required clever engineering tricks work successfully. aforementioned memory augmented neural networks tested synthetic tasks whereas paper evaluated wide range real large-scale natural language applications. proposed approach training consists examples transform input token word embedding neural semantic encoders model four main components read compose write modules encoding memory rk×l variable number slots embedding dimension length input sequence. memory slot vector corresponds vector representation information word memory. particular memory initialized embedding vectors {xt}l evolved time read compose write operations. figure illustrates architecture nse. performs three main operations every time step. initializing memory slots corresponding input representations processes embedding vector retrieves memory slot expected associatively coherent current input word slot location deﬁned vector read module emits attending memory slots. compose module implements composition operation combines memory slot current input. write module transforms composition output encoding memory space writes resulting representation slot location memory. instead composing embedding vector hidden state produced read module time concretely vectors ones given read function composition state encoding memory time step matrix ones denotes outer product duplicates left vector times form matrix. read function sequentially maps word embeddings internal space memory mt−. equation looks slots related input computing association degree memory slot hidden state calculate association degree product transform scores fuzzy vector normalizing tmax function. since vector fuzzy slot composed retrieved taking weighted slots equation process also seen soft attention mechanism equation compose process retrieved slot current hidden state resulting vector encoder output space. finally write representation memory location pointed vector equation vector emitted read module reused inform write module recently read slots. first slot information retrieved erased representation located. performs iterative process words input sequence read. encoding memories {m}t although reads single word time anytime-access entire sequence stored encoding memory. encoding memory maintains mental image input sequence. memory initialized embedding vector time term freshly initialized memory baby memory. reads input content time baby memory evolves reﬁnes encoded mental image. read functions neural networks training parameters nse. name suggests lstm multi-layer perceptron paper. since fully differentiable trained gradient descent optimizer. sequence sequence transduction tasks like question answering natural language inference machine translation beneﬁcial access relevant memories addition one. shared multiple memory access allows nses exchange knowledge representations communicate accomplish particular task throughout encoding memory. extended easily able read write multiple memories simultaneously multiple nses able access shared memory. figure depicts high-level architectural diagram multiple memory access-nse ﬁrst memory shared memory accessed nses. given shared memory rk×n encoded processing relevant sequence length mma-nse access relevant memory deﬁned almost standard nse. read module emits additional vector shared memory composition function mma-nse different memory slots retrieved shared memories depending encoded semantic representations. composed together current input written back corresponding slots. note mma-nse capable accessing variable number relevant shared memories composition function takes dynamic inputs chosen. classiﬁer handcrafted features lstm encoders dependency tree encoders spinn-pi encoders mma-nse lstm attention lstm word-by-word attention mma-nse attention mlstm word-by-word attention lstmn deep attention fusion decomposable attention model full tree matching nti-slstm-lstm global attention describe section experiments different tasks order show effective ﬂexible different settings. report results natural language inference question answering sentence classiﬁcation document sentiment analysis machine translation. tasks challenge model terms language understanding semantic reasoning. models trained using adam hyperparameters selected development set. chose one-layer lstm read/write modules tasks used two-layer lstm. pre-trained glove vectors glove vectors obtained word embeddings. word embeddings ﬁxed training. embeddings out-of-vocabulary words zero vector. crop input sequence ﬁxed length. padding vector inserted padding. models regularized using dropouts weight decay. natural language inference main tasks language understanding. task tests ability model reason semantic relationship sentences. order perform well task able capture sentence semantics able reason relation sentence pair i.e. whether premise-hypothesis pair entailing contradictory neutral. conducted experiments stanford natural language inference dataset consists premise-hypothesis pairs train/dev/test sets target label indicating relation. following setting output sentence input input layer computes concatenation mma-nse attention model similar lstm attention model. particularly attends premise encoder outputs {hp}t constructs attentively blended vector premise. model obtained accuracy score. best performing model task performs tree matching attention mechanism lstm. answer sentence selection integral part open-domain question answering. task model trained identify correct sentences answer factual question candidate sentences. experiment wikiqa dataset constructed wikipedia dataset contains pairs train/dev/test sets. setup used language inference task kept same except replace tmax layer sigmoid layer model following conditional probability distribution. question answer encoded vectors denotes output hidden layer mlp. trained mma-nse attention model minimize sigmoid cross entropy loss. mma-nse ﬁrst encodes answers questions accessing answer encoding memories. preliminary experiment found multiple memory access attention answer encoder outputs {ha}t crucial problem. following previous work adopt evaluation metrics task. batch size initial learning rate train model epochs. used dropouts word embeddings weight decay. word embeddings pre-trained glove vectors. task linear mapping layer transforms word embeddings lstm inputs. table presents results model previous models task. classiﬁer handcrafted features model trained features. bigram-cnn model simple convolutional neural net. lstm lstm attention models outperform previous best result nearly implementing deep lstm three hidden layers nasm improves sets strong baseline combining variational auto-encoder soft attention. mma-nse attention model exceeds nasm approximately task. evaluated stanford sentiment treebank dataset comes standard train/dev/test sets subtasks binary sentence classiﬁcation ﬁne-grained classiﬁcation classes. trained model text spans corresponding labeled phrases training evaluated model full sentences. sentence representations passed two-layer classiﬁcation. ﬁrst layer relu activation units binary ﬁne-grained setting. second layer tmax layer. read/write modules one-layer lstm hidden units word embeddings pre-trained glove vectors. batch size initial learning rate regularizer strength train model epochs. write/read neural nets last linear layer regularized dropouts. table compares result model state-of-the-art methods subtasks. best performing methods exploited parse tree provided treebank task exception dmn. dynamic memory network model memory-augmented network. model outperformed state-of-the-art results subtasks. evaluated models document-level sentiment analysis publically available largescale datasets imdb consisting movie reviews different classes yelp consisting restaurant reviews different classes. document datasets associated human ratings used ratings gold labels sentiment classiﬁcation. particularly used pre-split datasets stack lstm another document modeling. ﬁrst encodes sentences second lstm takes sentence encoded outputs constructs document representations. document representation given output tmax layer. whole network trained jointly backpropagating cross entropy loss. used one-layer lstm hidden units read/write modules pre-trained glove vectors task. batch size initial learning rate regularizer strength trained model epochs. write/read neural nets document-level nse/lstm regularized dropouts softmax layer dropouts. order speedup training created document buckets considering number sentences document i.e. documents number sentences together bucket. buckets shufﬂed updated epoch. curriculum scheduling although observed help sequence training. table shows results. report performance metrics accuracy mse. best results task previously obtained conv-grnn lstm-grnn also stacked models. models ﬁrst learn sentence representations lstm combine document representation using gated recurrent neural network models outperformed previous state-of-the-art models terms accuracy approximately hand systems tend show poor results imdb dataset. imdb dataset contains longer documents yelp classes yelp dataset classes distinguish. stacked nses performed slightly better nse-lstm imdb dataset. possibly encoding memory document level preserves long dependency documents large number sentences. lastly conducted experiment neural machine translation problem mostly deﬁned within encoder-decoder framework encoder provides semantic syntactic information source sentences decoder decoder generates target sentences conditioning information partially produced translation. efﬁcient encoding attention-based introduced implemented three different models. ﬁrst model baseline model similar proposed model lstm encoder/decoder soft attention neural attends source sentence constructs focused encoding vector target word. second model nse-lstm encoder-decoder encodes source sentence generates targets lstm network using output states attention network. last model nse-nse setup encoding part nse-lstm decoder uses output state access encoder memory i.e. encoder decoder nses access shared memory. memory encoded ﬁrst nses read/written decoder nses. used english-german translation corpus iwslt evaluation campaign corpus consists sentence-aligned translation talks. data pre-processed lowercased moses toolkit. merged sets development sets test data. sentence pairs length longer words ﬁltered out. resulted pairs train/dev/test sets. kept frequent words german dictionary. english dictionary words. glove vectors used embedding words source sentence whereas lookup embedding layer used target german words. note word embeddings usually optimized along models. however evaluation purpose experiment optimize english word embeddings. besides beam search generate target sentences. lstm encoder/decoders layers units. read/write modules one-layer lstm number units lstm encoder/decoders. ensures average number sentences words document imdb yelp https//github.com/moses-smt/mosesdecoder modiﬁed preparedata.sh script https//github.com/facebookresearch/mixer figure word association composition graphs produced memory access. directed arcs connect words composed compose module. source nodes input words destination nodes correspond accessed memory slots. denotes beginning sequence. number parameters models roughly equal. models trained minimize word-level cross entropy loss regularized input dropouts output dropouts. batch size initial learning rate lstm-lstm models regularizer strength train model epochs. report bleu score models. table reports results. baseline lstm-lstm encoder-decoder obtained bleu test set. nse-lstm improved baseline slightly. given small improvement nse-lstm unclear whether encoder helpful nmt. however replace lstm decoder another introduce shared memory access encoder-decoder model improve baseline result almost bleu. nse-nse model also yields increasing bleu score set. result demonstrates attention-based systems improved shared-memory encoder-decoder model. addition memory-based systems perform well translation long sequences preserving long term dependencies. capabable performing multiscale composition retrieving associative slots particular input time step. analyzed memory access order compositionality memory slot input word model trained snli data. figure shows word association graphs sentence picked snli test set. association graph constructed inspecting vector input word connect active slot pointed note graph components clustered around semantically rich words \"sits\" \"wall\" \"autumn\" \"three\" \"puppies\" \"tub\" \"vet\" memory slots corresponding words semantically rich current context frequently accessed. graph able capture certain syntactic structures including phrases modiﬁer relations another interesting property model tends perform sensible compositions processing input sentence. example retrieved memory slot corresponding \"wall\" \"three\" reading input \"rock\" \"are\". appendix show step-by-step visualization memory states ﬁrst sentence. note encoding memory evolved time. time step four memory slot \"quietly\" encodes information \"quiet little child\". model forms another composition involving \"quietly\" \"quietly sits\". last time step able least frequently accessed slots memory. least accessed slots correspond function words frequently accessed slots content words tend carry rich semantics proposed memory augmented neural networks achieved state-of-the-art results evaluated representative tasks. capable building efﬁcient architecture single shared multiple memory accesses speciﬁc task. example task accesses premise encoded memory processing hypothesis. task accesses answer encoded memory reading question machine translation shares single encoded memory encoder decoder. ﬂexibility architectural choice memory access allows robust models better performance. initial state memory stores information word input sequence. paper used word embeddings represent words memory. different variations word representations character-based models left evaluated memory initialization future. plan extend learns select access relevant subset memory set. could also explore unsupervised variations example train produce encoding memory representation vector entire sentences documents using either existing models skip-gram model would like thank abhyuday jagannatha anonymous reviewers insightful comments suggestions. work supported part grant national institutes health opinions ﬁndings conclusions recommendations expressed material authors necessarily reﬂect sponsor. references jeffrey elman. finding structure time. cognitive science sepp hochreiter jürgen schmidhuber. long short-term memory. neural computation kyunghyun bart merriënboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. arxiv preprint arxiv. jason weston sumit chopra antoine bordes. memory networks. icml edward grefenstette karl moritz hermann mustafa suleyman phil blunsom. learning transduce ankit kumar ozan irsoy jonathan james bradbury robert english brian pierce peter ondruska ishaan gulrajani richard socher. anything dynamic memory networks natural language processing. corr abs/. scott reed nando freitas. neural programmer-interpreters. iclr samuel bowman gabor angeli christopher potts christopher manning. large annotated samuel bowman gauthier abhinav rastogi raghav gupta christopher manning christopher potts. fast uniﬁed model parsing sentence understanding. corr abs/. diederik kingma jimmy adam method stochastic optimization. iclr jeffrey pennington richard socher christopher manning. glove global vectors word diederik kingma welling. auto-encoding variational bayes. iclr richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts. recursive deep models semantic compositionality sentiment treebank. emnlp yoon kim. convolutional neural networks sentence classiﬁcation. emnlp ozan irsoy claire cardie. modeling compositionality multiplicative recurrent neural networks. proceedings annual international conference machine learning pages kalchbrenner phil blunsom. recurrent continuous translation models. emnlp volume page small table represents memory state single time step. current time step input token listed table. memory slots pointed query vector highlighted color. brackets represent word composition order slot.", "year": 2016}