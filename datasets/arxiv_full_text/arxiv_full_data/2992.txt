{"title": "SE3-Nets: Learning Rigid Body Motion using Deep Neural Networks", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "abstract": "We introduce SE3-Nets, which are deep neural networks designed to model and learn rigid body motion from raw point cloud data. Based only on sequences of depth images along with action vectors and point wise data associations, SE3-Nets learn to segment effected object parts and predict their motion resulting from the applied force. Rather than learning point wise flow vectors, SE3-Nets predict SE3 transformations for different parts of the scene. Using simulated depth data of a table top scene and a robot manipulator, we show that the structure underlying SE3-Nets enables them to generate a far more consistent prediction of object motion than traditional flow based networks. Additional experiments with a depth camera observing a Baxter robot pushing objects on a table show that SE3-Nets also work well on real data.", "text": "abstract— introduce se-nets deep neural networks designed model learn rigid body motion point cloud data. based sequences depth images along action vectors point wise data associations se-nets learn segment effected object parts predict motion resulting applied force. rather learning point wise vectors se-nets predict transformations different parts scene. using simulated depth data table scene robot manipulator show structure underlying se-nets enables generate consistent prediction object motion traditional based networks. additional experiments depth camera observing baxter robot pushing objects table show se-nets also work well real data. ability predict environment changes based forces applied fundamental robot achieve speciﬁc goals. instance order arrange objects table desired conﬁguration robot able reason push individual objects requires understanding physical quantities object boundaries mass surface friction relationship forces. standard approach robot control physical model environment perform optimal control policy leads goal state. instance extensive work utilizing mujoco physics engine shown strong physics models enable solutions control problems complex contact-rich environments shortcoming models however rely accurate estimates state system unfortunately estimating values mass distribution surface friction object using visual information force feedback extremely difﬁcult. main reasons humans still better robots performing even simple tasks pushing arbitrary object along desired trajectory. humans achieve even though control policies informed approximate intuitive notions physics research shown humans learn mental models young potentially observing effect actions physical world work explore deep learning model concept \"physical intuition\" learning model predicts changes environment based speciﬁc actions. here focus modeling motion systems rigid bodies predicting object table moves pushed robot manipulator. importantly want learn predictive models sequences point clouds observed depth camera along continuous action vectors supervision learning provided point-wise associations consecutive point clouds higher level information object segmentation provided learner. standard deep network architecture trained data predict motion individual observed points vanilla network architecture able learn represent explicit notion objects motion useful control tasks higher-level reasoning. overcome limitation introduce se-nets learn segment scene \"salient\" objects predict motion objects effect applied actions. se-nets represent motion environment transforms widely used robotics computer vision graphics model rigid body motion. se-nets notion disentangling motion objects location environment se-nets explicitly predicting transforms encode motion dense pointwise masks specify contribution towards point. finally network combines masks input differentiable transform layer blends motions produce predicted output point cloud. absence constraints se-nets represent arbitrary per-point motion. bias network model rigid motion adapt weight sharpening technique used show results segmentation environment distinct objects along rigid motion. show results three simulated scenarios system predicts motion varying number rigid objects effect applied forces robot four actuated joints. present experiments testing different parts network show results highlighting robustness se-nets different types noise similar found real world data. also show capability se-nets operate real world data collected using baxter robot pushing objects table. main contributions paper follows. introduce se-nets deep neural network architecture models scene dynamics segmenting scene distinct objects jointly predicting rigid body motion. show se-nets learn solely based sequences control depth camera data pointwise associations without need explicit segmentation information training data. also provide substantial experimental evidence se-nets outperform standard deep learning baselines applied real robot data. paper organized follows. discussing related work introduce se-nets section followed experimental evaluation discussion. robotics mentioned before many optimal control techniques require \"dynamics\" model predicts effect actions state system early work learning dynamics models data focused lowdimensional state control representations recent work looked learning forward rigid-body dynamics models assuming tracking information available contrast methods boots learn model using predictive state representations predict depth images given history prior images control. also large body work investigating robots interact objects manipulation object exploration tasks summarized recent survey focus area however learning predictive motion models data present paper. recently deep models used learning dynamics models robotics reinforcement learning mapping pixel images low-dimensional encodings standard optimal control methods applied similar ﬂavor se-nets explicitly model dynamics scene rigid body motion salient objects jointly learning object segmentation motion prediction. physics prediction recent work deep learning looked problem predicting stability stacked blocks ball motion object dynamics images particular work agrawal finn closely related focusing modeling effect robot actions scene albeit using images. unlike explicitly predict dense point cloud forward model encoding motion using transforms masks. similar composite motion using predicted masks main differences transforms capture complex plane motions masks sharpened enforce rigid-body assumption improving prediction accuracy. predicting rotation related work computer vision literature looked problem predicting motion primarily rotation pairs images differing work operate data incorporate continuous actions explicitly predict rigid body motion object masks. independently parallel work handa proposed deep models using rigid body transforms depth image registration alignment model object motion effect actions. anisms focus parts environment related task performance concept disentangling representations separate variations environment. model differentiable dense pointwise attender learns focus parts environment motion occurs using cues segment objects. also central model idea disentangling motion object location. finally model related spatial transformer network though model effect actions dense attention restrict transformations. given point cloud depth sensor ndimensional continuous action input se-nets model scene dynamics rigid body motions constituent objects generate transformed point cloud essence se-nets decompose scene rigid objects predicting object mask attends parts scene containing object rigid body transform quantiﬁes object’s motion. note setting pre-speciﬁed network parameter limits number distinctly moving objects parts fig. shows general architecture se-nets three major components encoder generates joint latent state given input point cloud control decoder predicts object masks corresponding transforms ﬁnal transformation layer generates transformed point cloud encoder parts convolutional encoder generates latent state point cloud fully connected network encodes control vector adopt late fusion architecture concatenate outputs parts produce ﬁnal encoding used decoder predictions. decoder decomposes motion prediction problem sub-problems identifying grouping together points move together subsequently predicting transformation parameters quantify object’s motion. predicting motion masks mask decoder attends parts scene exhibit motion grouping points move together form objects. example points belonging rigid object grouped together move presupposing scene distinctly moving objects formulate k-class labeling problem input point belong motion-classes. unfortunately formulation nondifferentiable discreteness labeling. instead fig. se-net architecture. input point cloud n-dimensional action vector encoded concatenated joint feature vector decoder uses encoding predict object masks transforms used transform input cloud \"transform layer\" generate output. mask weights sharpened normalized prediction. conv convolution fully connected deconv deconvolution concatenation de-convolutional architecture compute dense object masks generating masks input resolution. following recent work skip-add architecture wherein convolutional layer outputs de-convolutional layer inputs. gives sharper reconstructions object shapes contours improving overall performance. predicting rigid transforms mentioned before represent motion environment using rigid body transforms. rigid body transform speciﬁed rotation translation point affected transformation moves space rather space transforms. hand ﬂexibility represent rigid non-rigid motions combination transforms oject masks. additionally avoid potential singularities arise blending space. spite advantages using current framework model rigid motion without explicit regularization lead over-ﬁtting blurry predictions. avoid this encourage network predict rigid motions form regularization object mask. enforcing rigidity simple restrict network predict rigid motions force per-point mask probability vector make binary decision predicted transforms instead blending. mentioned before naive formulation lead non-differentiability. instead smoothly encourage mask weights towards binary decision using form weight sharpening noise sampled gaussian proportional training epoch. practice combination noise growing exponent forces network push decisions apart resulting nearly binary distributions training. finally test time network segments input point cloud distinct objects predicts motion applies rigid transform input point generate transformed output point cloud evaluate se-nets well predict motion multiple simulated tasks using gazebo physics simulator real world data collected using baxter robot poking objects table. ﬁrst present results simulated data followed tests robustness network different types noise hyper-parameter choices ﬁnally discuss results real world data. output point corresponding input point eqn. computes convex combination transformed input points transformed transforms weights given object mask. consequence relaxation mask effective transform given point generally eqn. blends four simulated tasks using gazebo physics simulator consisting scenes ﬁxed camera looks rigid bodies moving effect applied forces. settings network takes point cloud dimensional continuous control vector input predicts resulting point cloud seconds future. assume control held ﬁxed duration. detail data collection next. single dataset random scenes ball collides placed random position table. scene place ball random location front continuously apply randomly chosen constant force ball directing collide box. given scene simulation second record data discard frames falls table. across scenes vary start pose objects applied force keeping objects’ size mass constant. also vary table size introduce background variations. across scenes total examples control vector -dimensional consisting ball pose applied force ball’s orientation control model cases ball undergoes spin. multiple boxes test generalization system different object sizes masses number objects generated second dataset varies three random. scene anywhere objects varied sizes proportional mass ball collides randomly chosen box. consider examples single ball collision discard involve multiple collisions hard system model motion without temporal information. dataset different scenes total examples. controls represented single dataset baxter third dataset consists sequences depth images looking baxter robot controlled move right randomly. scene apply constant randomly chosen velocity randomly chosen joints robot’s right arm. scene lasts second bring rest. randomly reset pose every scenes. total dataset scenes examples. controls task commanded joint velocities -dimensional vector values non-zero household objects ﬁnal simulated dataset tests generalization system irregular object shapes. household objects linemod dataset scene objects randomly placed table ball colliding randomly chosen object. total random scenes examples. controls similar datasets dataset particularly challenging signiﬁcant amounts toppling fast rotations objects’ irregular shapes. implemented system using deep learning package torch trained networks using adam optimization method along google’s batch normalization technique speed training. start training initialize layer predicting transforms predict identity transform found improve convergence. initially weight sharpening penalty zero slowly ramp noise parameter exponent till reach preset maximum. number objects chosen apriori baxter dataset datasets comparison train three variants se-nets compare baseline networks predict scene well baseline always predicts zero motion se-net fig. million flow network trained predict dense optical directly using conv/deconv architecture similar original se-net without prediction module transform layer. motion baseline always predicts zero motion. small networks strided convolution pooling large networks pooling instead striding. networks parametric-relu non-linearity. trained networks iterations single baxter datasets iterations datasets train/test split. depending network size training takes anywhere hours half nvidia titan gpu. training targets given input point cloud control se-nets predict output point cloud transforming input points would like predictions match ground truth targets specify true position input points future time. order compute targets though need track associate given input point across multiple depth images. work assume associations given training time either physics simulator object tracker discuss ways overcome assumption later sec. evaluation metric report mean squared error predicted scene ground truth averaged across points non-zero ground truth ﬂow. metric takes account errors mask predictions also loss function used train networks. fig. prediction results three simulated datasets. images rendered projecting predicted point cloud using camera parameters rounded nearest pixel without interpolation. input point cloud ball highlighted applied force shown green; ground truth; predictions generated different networks. point clouds networks computed adding predicted input. black regions images correspond parts occluded input later became visible objects’ motion image best viewed high resolution. better understanding results please refer supplementary video. table average per-point across tasks networks. network achieves best error compared baselines even though directly trained predict ﬂow. motion\" results quantify average magnitude motion datasets fig. results \"household objects\" dataset showing topping sliding motion. network segments objects correctly predicts consistent motion leading sharp images baseline smears object across image. ball highlighted red. table average per-point networks trained noise added depth data associations networks’ performance degrades gracefully increasing noise compared large errors baseline. table reports test results ﬁrst three simulated datasets. networks signiﬁcantly outperform counterpart small large networks tasks. also -fold cross validation large networks found improvements baselines statistically signiﬁcant. networks also achieve large reduction prediction error compared zero motion baseline even large motions practice we’ve seen similar performance shorter horizons smaller subtle motions. fig. shows representative predictions made networks three simulated datasets. results highlight another advantage approach compared naive baseline consistency sharpness predictions. segmenting scene distinct objects predicting individual transforms network ensures points belong object rigidly move together interpretable manner. results sharp prediction little noise compared networks constraints. increasing layer depth networks somewhat compensate this still signiﬁcant amount noise predictions resulting smearing across canvas surprisingly noticed networks perform quite poorly examples points move ball moves scene networks able predict ball’s motion quite accurately. also present mask predictions made networks fig. colors indicate masks predicted networks datasets near binary network successfully segments ball distinct objects without explicit supervision. practice found crucial give network examples ball moves independently provides implicit knowledge ball distinct objects. cases training examples always ball contact network hard time separating objects often masking together. baxter dataset depending motion network usually segments distinct parts often split elbow. comparison penalty\" se-net rarely predicts binary masks often blending across different leads overﬁt datasets large motion single dataset still performs quite well datasets. interestingly penalty\" networks signiﬁcantly outperform smaller fig. shows representative results testing household objects dataset network deal complex shaped objects holes. clear network model dynamics objects well resulting predictions signiﬁcantly sharper compared large network poorly. also seen network able gracefully handle cases objects topple undergo large motions. finally test consistency network modeling sequential data allowing network predict scene dynamics multiple steps future. fig. shows results baxter sequence feed network’s predictions back input control vector steps future. compare ground truth predictions remain consistent across time without signiﬁcant noise addition. comparison predictions large network degrade time noise cascades. better understanding results encourage readers look supplementary video show prediction results many sequences. robustness networks hyper-parameter choices noisy data. sensitivity number objects prior experiments chosen number predicted apriori knowledge datasets. test sensitivity algorithm parameter trained networks setting large number cases found network automatically segments scene correct number objects remaining mask channels assigned identity. also little performance drop experiments. robustness depth noise test whether network capable handling types noise seen real depth sensors trained networks types depth noise first added gaussian noise standard deviation scaled noise depth common commodity depth sensors. second increased noise without scaling depth. table shows performance large networks types noise performance degrades signiﬁcantly outperform baseline network. additionally network still able segment objects properly tests. robustness noise data association test well networks respond uncertainty data association fig. object masks predicted networks. masks rendered directly images baxter masks colored based arg-max operation across k-mask channels. se-net predictions near-binary seen distinct coloring distinctly moving object penalty masks mixed coloring across scene indicating masks binary. image best viewed color. allowing spurious ground-truth associations computing training targets. allow point randomly associated point window around long depth differences larger threshold. train increasingly noisy settings associating window threshold window threshold table shows results tests. network strongly outperforms baseline errors almost half baseline. test simulate systematic association bias still shows network robust uncertain associations. believe strong structural constraints inherent network allow average noise leading robust predictions even highly noisy settings. results real data fig. shows representative sequence testing held pokes cascade predictions forward second. errors increase time network able segment predict consistent motion object arm. comparison network performs poorly again suggest reader look supplementary video results. overall believe strong proof concept showing easily generate training data needed networks se-nets able learn scene dynamics limited real world data. future plan collect data train larger networks handle complicated dynamics toppling falling. finally present results preliminary evaluation se-nets real world data obtained baxter robot interacting three objects tabletop scene poking stick attached it’s end-effector. start poking action randomly place single object table. similar choose random direction poke keeping end-effector level constant orientation. poking action lasts around seconds record point clouds depth camera mounted torso robot along joint encoder data. preliminary effort collected poke actions mostly sliding rotational object motion total examples. dart motion tracker generate ground truth data associations training. trained small se-net networks predict frame seconds future allow large motions. commanded joint angles velocities control limited quantity data make modiﬁcation speed training provide se-net ground truth mask robot arm. makes problem easier network still segment object jointly predict object’s motion. learning intuitive models physical world data promising alternative explicitly designed physics-based models. fact models learned data tightly coupled perception thus well suited closed-loop control. toward long term goal introduced se-nets deep learning model predicts changes environment based applied actions parameterized series rigid transforms applied points environment. se-nets selectively learn focus parts scene motion occurs segmenting scene objects predicting motions distinct object. showed separation works well practice results strong performance four simulated real robot task multiple rigid bodies motion. se-nets able generalize across different scenes produce results consistent observed rigid motion compared traditional networks. several promising directions future work. first experiments indicate se-nets learn predictive models real data conﬁdent larger data collection effort would enable train networks generalize across many types objects scenes. fig. multi-step prediction results real world data collected poking objects baxter robot. network predictions sharper baselines highlighting consistency learned segmentation transforms. area improvement learning data associations currently provide part training setup. ﬁrst step towards would formulate loss function based iterative closest point matching able align close-by depth data. another exciting direction penalty version se-nets model nonrigid motion exploring strong regularization locality priors masks improve efﬁciency generalization. areas future work include learning multi-step recurrent se-nets sequential prediction using se-net models closed-loop control. bohg hausman sankaran brock kragic schaal sukhatme interactive perception leveraging action perception perception action arxiv preprint arxiv. kulkarni whitney kohli tenenbaum deep convolutional inverse graphics network nips yang reed m.-h. yang weakly-supervised disentangling recurrent transformations view synthesis nips handa bloesch p˘atr˘aucean stent mccormac davison gvnn neural network library geometric computer vision computer vision–eccv workshops gregor danihelka graves wierstra draw recurrent hinterstoisser lepetit ilic holzer bradski konolige navab model based training detection pose estimation texture-less objects heavily cluttered scenes accv collobert kavukcuoglu farabet torch matlab-like environment machine learning biglearn nips workshop kingma adam method stochastic optimization", "year": 2016}