{"title": "Reinforcement-based Simultaneous Algorithm and its Hyperparameters  Selection", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Many algorithms for data analysis exist, especially for classification problems. To solve a data analysis problem, a proper algorithm should be chosen, and also its hyperparameters should be selected. In this paper, we present a new method for the simultaneous selection of an algorithm and its hyperparameters. In order to do so, we reduced this problem to the multi-armed bandit problem. We consider an algorithm as an arm and algorithm hyperparameters search during a fixed time as the corresponding arm play. We also suggest a problem-specific reward function. We performed the experiments on 10 real datasets and compare the suggested method with the existing one implemented in Auto-WEKA. The results show that our method is significantly better in most of the cases and never worse than the Auto-WEKA.", "text": "many algorithms data analysis exist especially classiﬁcation problems. solve data analysis problem proper algorithm chosen also hyperparameters selected. paper present method simultaneous selection algorithm hyperparameters. order reduced problem multi-armed bandit problem. consider algorithm algorithm hyperparameters search ﬁxed time corresponding play. also suggest problem-speciﬁc reward function. performed experiments real datasets compare suggested method existing implemented auto-weka. results show method signiﬁcantly better cases never worse auto-weka. keywords algorithm selection hyperparameter optimization multi-armed bandit reinforcement learning goal supervised learning data model given dataset allows make accurate predictions. build model lots learning algorithms exist especially classiﬁcation. algorithms show various performances diﬀerent tasks. prevents usage single universal algorithm build data model existing datasets. performance algorithms depends hyperparameters selection dramatically aﬀects performance algorithms. automated simultaneous selection learning algorithm hyperparameters sophisticated problem. usually problem divided subproblems solved independently algorithm selection hyperparameter optimization. ﬁrst select algorithm algorithms second best hyperparameters preselected algorithm. ﬁrst subproblem typically solved testing algorithms prechosen hyperparameters portfolio many practitioners. methods also selecting algorithms randomly heuristics using k-fold cross-validation last method requires running comparing algorithms. methods universally applicable. however subproblem scope research interest decades. decision rules used several decades papers nowadays eﬀective approaches exists meta learning approach reduce algorithm selection problem supervised learning problem. requires training datasets meta-feature vector evaluated. meta-features useful characteristics datasets number categorical numerical features object size many others that algorithms datasets thus class labels formed based empirical risk evaluation. meta-classiﬁer learnt prepared data datasets objects best algorithms labels. worth note better solve problem learning rank problem second subproblem hyperparameter optimization hyperparameter vector learning algorithm leads best performance algorithm given dataset. example hyperparameters support vector machine include kernel function hyperparameters; neural include number hidden layers number neurons them. practice algorithms hyperparameters usually chosen manually moreover sometimes selection problem reduced simple optimization problem instance however method universally applicable. since hyperparameter optimization classiﬁcation algorithms often applied manually requires time lead acceptable performance. several algorithms solve second subproblem automatically grid search random search stochastic gradient descent tree-structured parzen estimator bayesian optimization including sequential model-based optimization sequential model-based algorithm conﬁguration introduced. based smbo algorithm. another idea implemented predicting best hyperparameter vector meta-learning approach reinforcement-based approach used operate several optimization threads diﬀerent settings. solution simultaneous selection algorithm hyperparameters important machine learning applications papers devoted search. moreover papers consider special case. possible solutions build huge algorithms prechosen hyperparameters select solution implemented algorithms chosen hyperparameters used. however pure algorithm selection approach cannot provide insurance algorithms quality problem. simply include hyperparameter vector presented learning algorithms best performance. another possible solution sequential optimization hyperparameters every learning algorithm portfolio selection best them. solution implemented auto-weka library allows choose base learning algorithms meta-algorithms ensemble algorithms optimize hyperparameters smac method simultaneously automatically. method described goal work suggest method simultaneous learning algorithm parameters selection faster exhaustive search without aﬀecting found solution quality. order multi-armed bandit-based approach. remainder paper organized follows. section describe details learning algorithm hyperparameter selection problem subproblems. suggested method based multi-armed bandit problem presented section section experiment results presented discussed. section concludes. formal description algorithm selection problem. given algorithms chosen hyperparameters learning dataset pair consisting object label. choose parametrized algorithm eﬀective respect quality measure algorithm eﬃciency appraised dataset partition learning test sets empirical risk estimation test set. paper consider simultaneous algorithm selection hyperparameters optimization. given learning algorithm ak}. learning algorithm associated hyperparameter space goal algorithm minimizing empirical risk assume hyperparameter optimization performed sequential hyperparameter optimization process. give formal description. sequential hyperparameter optimization process learning algorithm hyperparameter optimization method learning algorithm time budget also stores best found hyperparameter vectors within previous iterations {λj}k hyperparameter optimization methods listed introduction described sequential hyperparameter optimization process instance grid search smbo algorithm family including smac method used paper. suppose sequential hyperparameter optimization process associated learning algorithm previous problem solved running processes. however problem arises best algorithm search time minimization problem. practice similar problem interesting practical terms. problem ﬁnding best algorithm ﬁxed time. describe formally. problem source hyperparameter optimization time limit split equal small intervals call time budgets. solve time budgets assignment problem. lets look problem diﬀerent way. time interval choose process interval interval starts. quality reached algorithm given dataset priori unknown. hand time spent searching hyperparameters best learning algorithms subtracted time spent improve hyperparameters best learning algorithm. hand time spent tuning single algorithm miss better algorithms. thus since marginal solution problem seems tradeoﬀ exploration exploitation tradeoﬀ detection classical problem reinforcement learning special case multi-armed bandit problem cannot assume hidden process state transformation aﬀects performance algorithms thus assume environment static. multi-armed bandit problem problem bandit’s arms. playing arms grants certain reward. reward chosen according unknown probability distribution speciﬁc arm. iteration agent chooses reward agent’s goal minimize total loss time paper following algorithms solving problem ε-greedy iteration average reward ¯rat estimated agent plays maximal average reward probability random probability play inﬁnite number times average reward converges real reward probability paper associate arms sequential hyperparameters optimization processes presented listing there mabsolver implementing multi-armed bandit problem solution getconfig function returns best found conﬁguration iterations algorithm question need answer deﬁne reward function. ﬁrst deﬁne reward diﬀerence current empirical risk optimal empirical risk found previous iterations. however meet several disadvantages. optimization process ﬁnds hyperparameters leads almost optimal algorithm performance reward extremely small. also selection reward function seem good option mabs since probability distribution depend number iterations. order reward function corresponding probability distribution change algorithm performance apply little trick. instead deﬁning reward function itself deﬁne average reward function. order smac algorithm features. describe smac algorithm. iteration current optimal hyperparameter vectors known algorithm. local search applied hyperparameter vectors distinction position optimal vector improve algorithm quality. hyperparameter vectors added set. moreover random hyperparameter vectors added set. selected conﬁgurations sorted expected improvement best conﬁgurations that. data given dataset number iterations time budget iteration {πi}i=...n sequential hyperparameter optimization processes. result algorithm chosen hyperparameters support vector machine logistic regression random forest perceptron decision tree previously stated given time solution main problem. suggested method requires splitting small equal intervals give small interval selected process iteration. compare method performance diﬀerent time budget values optimal value. consider time budgets seconds second step. suggested method datasets german credits krvskp described above. solutions multi-armed bandit problem .-greedy .-greedy softmax. conﬁguration times. results show regularity assume time budget seconds. quality comparison consider suggested method diﬀerent multi-armed bandit problem solutions .-greedy .-greedy softmax na¨ıve reward function solutions tmaxe suggested reward function. time budget iteration seconds general time limitation hours seconds. conﬁguration times random seeds smac algorithm. auto-weka also limited hours selects algorithms speciﬁed above. experiment results shown table results show suggested method signiﬁcantly better cases auto-weka datasets variations reach smallest empirical risk. fundamental diﬀerence results suggested method variations. nevertheless algorithms tmaxe suggested reward function achieved smallest empirical risk cases. experiment results show suggested approach improves existing solution simultaneous learning algorithm hyperparameters selection problem. moreover suggested approach impose restrictions hyperparameter optimization process search performed entire hyperparameters space learning algorithm. signiﬁcant suggested method allows select learning algorithm hyperparameters whose quality worse auto-weka outcome quality. table comparison auto-weka suggested methods selecting classiﬁcation algorithm hyperparameters given dataset. performed independent runs conﬁguration report smallest empirical risk achieved auto-weka suggested method variations. highlight bold entries minimal given dataset. leads appropriate number pairs. moreover wilcoxon test assumptions carried. therefore test checks comparison auto-weka variation suggested method. since number samples meaningful results untypical results consider minimization problem test best runs dataset. finally ε-greedy algorithms others. proves statistical signiﬁcance obtained results. paper suggest examine solution actual problem algorithm hyperparameters simultaneous selection. proposed approach based multi-armed bandit problem solution. suggest reward function exploiting hyperparameter optimization method properties. suggested function better na¨ıve function applying multi-armed bandit problem solutions solve main problem. experiment result shows suggested method outperforms existing method implemented auto-weka. suggested method improved applying meta-learning order evaluate algorithm quality preprocess given dataset running algorithm. evaluation used prior knowledge algorithm reward. moreover context vector hyperparameters optimization process solutions contextual multi-armed bandit problem. select datasets meta-learning empirical risk estimate context. authors would like thank vadim strijov unknown reviewers useful comments. research supported government russian federation russian foundation basic research", "year": 2016}