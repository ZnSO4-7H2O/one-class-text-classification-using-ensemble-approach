{"title": "Tikhonov Regularization for Long Short-Term Memory Networks", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "It is a well-known fact that adding noise to the input data often improves network performance. While the dropout technique may be a cause of memory loss, when it is applied to recurrent connections, Tikhonov regularization, which can be regarded as the training with additive noise, avoids this issue naturally, though it implies regularizer derivation for different architectures. In case of feedforward neural networks this is straightforward, while for networks with recurrent connections and complicated layers it leads to some difficulties. In this paper, a Tikhonov regularizer is derived for Long-Short Term Memory (LSTM) networks. Although it is independent of time for simplicity, it considers interaction between weights of the LSTM unit, which in theory makes it possible to regularize the unit with complicated dependences by using only one parameter that measures the input data perturbation. The regularizer that is proposed in this paper has three parameters: one to control the regularization process, and other two to maintain computation stability while the network is being trained. The theory developed in this paper can be applied to get such regularizers for different recurrent neural networks with Hadamard products and Lipschitz continuous functions.", "text": "well-known fact adding noise input data often improves network performance. dropout technique cause memory loss applied recurrent connections tikhonov regularization regarded training additive noise avoids issue naturally though implies regularizer derivation different architectures. case feedforward neural networks straightforward networks recurrent connections complicated layers leads difﬁculties. paper tikhonov regularizer derived long-short term memory networks. although independent time simplicity considers interaction weights lstm unit theory makes possible regularize unit complicated dependences using parameter measures input data perturbation. regularizer proposed paper three parameters control regularization process maintain computation stability network trained. theory developed paper applied regularizers different recurrent neural networks hadamard products lipschitz continuous functions. construct mapping lstm units. concept introduced remedy vanishing gradient problem reﬁned later papers lstm unit three gates input output forget ones used control data unit. input gates parameters must trained regularization often improves network performance prevents overﬁtting. despite tremendous performance gain many applications abundance techniques regularize networks including dropout weight decay standard regularization approach recurrent neural networks general lstms particular suffer overﬁtting. usage techniques feedforward neural networks straightforward though application rnns leads difﬁculties. first dropout applied recurrent connections cause memory loss problem authors tried avoid. second though regularization used obvious must applied whether regularization parameter used several ones regularize differently parts unit. note latter case computationally intense former leads slower training since necessary optimal values. feasible address problems derivation regularizer lstm unit based tikhonov regularization technique. shown adding noise initial data equivalent tikhonov regularization. almost time authors showed possibility apply concept recurrent neural networks regularizer obtained calculating paper upper bound calculated regularizer lstm networks case solving regression task sum-of-squares objective though regularizer derived loss. paper organized follows. section describes network architecture regularizer derived assessing upper bound output perturbation. section provides theoretical justiﬁcation form lstm regularizer. section describes learning procedure regularizer derived previously relaxed optimization problem. section concludes paper. order minimize output gate perturbation necessary take account memory perturbation perturbation result applying following functions input data unit forget gate function input gate function input unit function thus possible write following equation memory perturbation paper tikhonov regularizer derived lstm unit ﬁnding upper bound output perturbation difference actual output network observed noise added inputs network. regularizer three parameters ﬁrst measures degree input perturbation thus controls regularization process used maintain computation stability regularization. regularizer used approach overﬁtting problem lstm networks taking account weights gates independently also interaction parts lstm complex structure. mathematical justiﬁcation proposed regularization derivation provided enables regularizers different architectures.", "year": 2017}