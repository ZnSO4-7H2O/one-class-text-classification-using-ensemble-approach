{"title": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era", "tag": ["cs.CV", "cs.AI"], "abstract": "The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.", "text": "figure curious case vision datasets computation power model sizes continued increase last years size largest training dataset surprisingly remained constant. that? would happened used resources increase dataset size well? paper provides sneak-peek could dataset sizes increased dramatically. ously gpus model capacity continued grow datasets train models remained stagnant. even -layer resnet signiﬁcantly capacity depth still trained images imagenet circa that? belittled importance data front deeper models computational power? happen scale amount training data performance double? paper takes ﬁrst steps towards clearing clouds mystery surrounding relationship ‘enormous data’ deep learning. exploit alsuccess deep learning vision attributed models high capacity; increased computational power; availability large-scale labeled data. since signiﬁcant advances representation capabilities models computational capabilities gpus. size biggest dataset surprisingly remained constant. happen increase dataset size paper takes step towards clearing clouds mystery surrounding relationship ‘enormous data’ visual deep learning. exploiting jft-m dataset noisy labels images investigate performance current vision tasks would change data used representation learning. paper delivers surprising ﬁndings. first performance vision tasks increases logarithmically based volume training data size. second show representation learning still holds promise. improve performance many vision tasks training better base model. finally expected present state-of-theart results different vision tasks including image classiﬁcation object detection semantic segmentation human pose estimation. sincere hope inspires vision community undervalue data develop collective efforts building larger datasets. unanimous agreement current convnet revolution product labeled datasets large computational power every year increase computational power datasets fortunate. imagenet dataset labeled images based categories used train alexnet years ago. curiready existing jft-image dataset ﬁrst introduced hinton expanded dataset images labeled categories. annotations automatically obtained therefore noisy exhaustive. annotations cleaned using complex algorithms increase precision labels; however still approximately error precision. data investigate nature relationship amount data performance vision tasks. speciﬁcally look power data visual representation learning evaluate learned representation variety vision tasks image classiﬁcation object detection semantic segmentation human pose estimation. experiments yield surprising ﬁndings suggests collection larger-scale dataset study visual pretraining greatly beneﬁt ﬁeld. ﬁndings also suggest bright future unsupervised self-supervised representation learning approaches. seems scale data overpower noise label space. performance increases logarithmically based volume training data. logarithmic relationship performance vision tasks amount training data used representation learning. note previous papers large-scale learning shown diminishing returns even log-scale. presents state-of-the-art results several benchmarks using models learned jft-m. example single model achieve compared coco detection benchmark. ever since seminal work krizhevsky showcased power convolutional neural networks large-scale image recognition task work done make accurate. common approach increase complexity networks increasing width depth networks. example simonyan zisserman proposed vgg- model uses smaller convolutional ﬁlters depth layers. since representational power depth models continued grow every year. googlenet -layer network. paper perform experiments resnet models proposed core idea residual connections layers helps optimization very-deep models. results stateof-the-art performances number recognition tasks. convolutional neural networks learn hierarchy visual representations. visual representations shown effective wide range computer vision tasks learning visual representations require large-scale training data. however biggest detection segmentation datasets still order hundreds thousands images. therefore approaches employ pre-training. original model learning using million labeled images imagenet trained target tasks yield better performance huang thoroughly evaluated inﬂuence multiple convnet architectures object detection performance found closely correlated models’ capacity classiﬁcation performances imagenet. signiﬁcant work increasing representational capacity convnets amount training data pre-training remain kind ﬁxed years. prime reason behind lack human veriﬁed image datasets larger imagenet. order overcome bottleneck recent efforts visual representation learning using web-supervision unsupervised paradigms. however efforts still still exploratory nature lower performance compared fully-supervised learning. paper shift discussion models data. paper inspired several papers time paid closer look impact properties data rather models. pereira presented survey paper look impact data ﬁelds natural language processing computer vision. argued unlike physics areas likely impact using data-driven approaches. anrelated work empirical study torralba efros highlighted dataset biases current computer vision approaches impacts future research. speciﬁcally focus understanding relationship data visual deep learning. efforts understand relationship. example oquab showed expanding training data cover labels imagenet-m improves object detection performance. similarly showed using smaller subset images training imagenet hurts performance. studies also show selection categories training important random addition categories tends hurt performance. happens number categories increased still need manual selection categories? similarly neither efforts demonstrated data effects signiﬁcantly larger scale. recent work looked training convnets signiﬁcantly larger data. looked geo-localization utilized yfcc-m dataset representation learning. however unlike ours showed plateauing detection performance trained images. that? believe could possible reasons yfcc-m images come flickr. includes images better visual diversity. usage user feedback signals reduces label noise. yfcc-m much bigger vocabulary size noisier annotations. importantly real effect data smaller alexnet models. experiments gain larger model sizes. introduce jft-m dataset used throughpaper. jft-m follow version dataset introduced jft-m dataset closely related derived data powers image search. version dataset images labels average image labels. images labeled categories e.g. type animals types vehicles labeled dataset. categories form rich hierarchy maximum depth hierarchy maximum number child parent node images labeled using algorithm uses complex mixture signals connections webpages user feedback. algorithm starts billion image label pairs ends labels images select labeled images high precision. however still noise labels approximately labels dataset noisy. since exhaustive annotation estimate recall labels. figure shows kind noise exists dataset. labels generated automatically problem ‘tortoise’ figure jft-m dataset noisy terms label confusion incorrect labels. labels generated complex mixture signals annotated cleaned humans. x-axis corresponds quantized distances k-means centroids computed based visual features. finally important discuss data distribution jft-m. distribution heavily long-tailed e.g. ‘ﬂowers’ ‘subarau’ images ‘train conductors’. fact tail heavy categories less images approximately categories less images category. although several novel convnet architectures recently proposed decide standard residual network architecture layers state-of-the-art performance ease comparison previous work. train resnet- model jftm fully-connected layer outputs network classiﬁcation. image labels mutually exclusive compute per-label logistic loss treat non-present labels negatives. alleviate issue missing labels hand-designed label hierarchy missing labels accordingly. example image label ‘apple’ also considered correct example ‘fruit’. training input images resized pixels randomly cropped image pixels normalized range independently channel random reﬂection data augmentation. weight decay batch normalization convolutional layers. rmsprop optimizer used momentum batch size learning rate initially decay every steps. asynchronous gradient descent training nvidia gpus. model implemented tensorflow. please note full training schedule takes iterations around epochs. however time constraints train models iterations epochs takes approximately months. study impact training iterations section approaches evaluate quality visual representations learned training data. ﬁrst approach freeze model weights models pure feature extractors. second approach model weights initialization ﬁne-tune weights tasks. evaluating visual representations select three representative computer vision tasks object detection semantic segmentation human pose estimation. perform rigorous ablative analysis observe effect dataset size vocabulary size etc. object detection task. tasks show jft-m provides signiﬁcant improvement compared baseline imagenet resnet. de-duplication concern using large-scale sets jft-m possible overlap training test sets. duplication exist current frameworks well e.g. validation images imagenet near-duplicate images training set. however ensure duplication affect results performed experiments removing near-duplicate images test sets. found difference performance insigniﬁcant experiments. therefore report de-duplicated test-set results appendix object detection. faster rcnn framework state-of-the-art performance. faster rcnn two-stage model. ﬁrst stage called region proposal network aims generating classagnostic object proposals. second stage classiﬁer takes boxes predicted crops feature maps generate classiﬁcation predictions reﬁned bounding predictions. stages share common feature generated convnet classiﬁer additional convolutional layers ﬁnal classiﬁcation regression layers. resnet- model pre-trained jft-m data split model ﬁrst part starts conv block ends parts conv block used feature extraction shared classiﬁer; second part consists conv block used classiﬁer. semantic segmentation. deeplab framework resnet- base architecture task semantic segmentation. particular variant adds four branches conv block resnet architecture. branch atrous convolutional figure comparison training progress random initialization imagenet initialization jft-m data. x-axis number training steps y-axis shows metric computed fastevalk. weights. ﬁnal classiﬁcation fully-connected layer input units output units parameters. handle parameter servers split vertically equal sized sub-fc layers distribute around different parameter servers. imagenet baseline observed hyperparameters selected train jft-m data yield sub-optimal performance training imagenet therefore imagenet momentum optimizer momentum initial learning duced factor every epochs train model total steps. similar jftm training asynchronous gradient descent training nvidia gpus parameter servers. monitoring training progress jft-m validation chollet ‘fastevalk’. fastevalk consists images labels classes unlike labels jft-m images fastevalk densely annotated around labels image average. metric computed mean average precision top- predictions. note class weighted common class among social media images. tried strategies initialize model weights training random initialization initializing imagenet checkpoint. settings used training schedule found fastevalk benchmark model trained imagenet initialization performs better ﬁrst iterations becomes random initialization. figure shows training progress settings. fastevalk benchmark model trained imagenet initialization performs better ﬁrst iterations becomes layer predicts sub-sampled pixel-wise class probabilities. predictions branches fused together produce ﬁnal segmentation output. please refer deeplab-aspp-l model details. pose estimation. follow framework proposed papandreou uses person bounding boxes detected faster rcnn applies resnet fully convolutionally produce heatmaps offsets keypoints. novel scoring non-maximum suppression scheme used suppress duplicate detections improve performance. simply replace base models used framework trained resnet- models. present results ﬁne-tuning jft-m resnet- checkpoints four tasks image classiﬁcation object detection semantic segmentation human pose estimation. ﬁne-tune jft-m pre-trained resnet using imagenet classiﬁcation data compare resnet model trained scratch. experiment standard ilsvrc ‘train’ ‘val’ sets training evaluation. training images validation images classes. imagenet training setup described section imagenet baseline lowered initial learning rate initialize model weights jft-m checkpoint trained iterations ﬁne-tune imagenet iterations. table compares ﬁne-tuning results models trained scratch. reference show random initialization performance open-sourced checkpoint authors report top- top- accuracies single crop evaluated. ﬁne-tuning jft-m gives considerable performance boost top- top- accuracies. performance trained representations performance learned visual representations saturate certain amount data? plateauing effect data? important representational capacity? number classes factor learning visual could clean data help improve coco held-out images standard ‘val’ validation refer ‘minival∗’ images used combination standard training remaining validation images training. unless otherwise speciﬁed coco results reported minival∗ set. particular interested mean average precision threshold average thresholds best resnet models also evaluate coco ‘test-dev’ split pascal ‘trainval’ images pascal training report performance pascal test images using map. metric. tensorflow faster rcnn implementation adopt default training hyperparameters except learning rate schedules. asynchronous training workers parameter servers momentum optimizer used momentum worker takes single input image step batch size classiﬁer training respectively. input images resized minimum pixels maximum pixels maintaining aspect ratio. data augmentation used random ﬂipping. learning rate steps model trained steps. training schedules selected held-out validation images using open-source resnet- model found training schedules work well checkpoints keep ﬁxed throughout fairer comparison. durtable object detection performance comparisons baseline methods coco test-dev split. ﬁrst four faster rcnn detectors based resnet- architecture last based inceptionresnet-v architecture. inference single image scale crop single detection model used experiments. vanilla faster rcnn implementations used systems except also includes reﬁnement context. ﬁrst present performance comparison imagenet checkpoints. table shows detection performance coco ‘test-dev’ split. show faster rcnn baseline competitive also report results faster rcnn paper uses reﬁnement context information. imagenet baseline performs competitively. evaluate jft-m trained scratch initialization imagenet models outperforms imagenet baseline large margins boost map. respectively. reference also show performance imagenet trained inceptionresnetv table would like point gain even signiﬁcant recently achieved doubling number layers inception resnet clearly indicates indications plateauing effect model representation capacity; terms data still easily gained. figure object detection performance initial checkpoints pre-trained different subsets jft-m scratch. x-axis data size log-scale y-axis detection performance coco minival∗ map. pascal test mance also improves. comparison table show imagenet counterpart trained epochs performance imagenet checkpoints improves faster jft-m respect number epochs. would also like point learning schedules developed using experience smaller datasets. envision better learning schedules provide improvement epochs used. experiment randomly sample subset images jft-m training data. training schedule jft-m model training. pick checkpoints corresponding epoch subset. study impact learned visual representations also conduct experiments freeze model weights layers conv block. experiments change learning rate decay happen steps total number training steps tend converge earlier. figure show checkpoints trained different jft-m subsets blue curve corresponds regular faster rcnn training curve corresponds freezing feature extractors. surprisingly ﬁne-tuning offers signiﬁcantly better performance data sizes. interestingly performance grows logarithmically pretraining data expands particularly true feature extraction layers frozen. study number training epochs affects object detection performance. experiment report results coco minival∗ set. table shows performance comparison jft-m model trained epochs respectively. number training steps increases perforjft-m labels total. understand large number classes brings select subset labels direct correspondence imagenet labels sample jft-m images contain table object detection performance mean coco minival∗ set. compare checkpoints pre-trained images labels limited imagenet classes images covering classes. table shows performance comparison coco minival∗ set. models perform other. indicates performance beneﬁt comes training images instead labels. finally study impact model capacity images available training. conduct experiments -layer -layer -layer resnet models. model trained scratch jftm data hyper parameters used resnet- experiments. comparison also train models imagenet data till convergence using hyper parameters resnet-. figure shows performance ﬁne-tuning different pre-trained models coco minival∗set. observe higher capacity models better utilizing data. example case resnet- gain smaller compared using resnet-. pascal semantic segmentation benchmark pixel-wise labels foreground classes background class. standard practice models trained augmented pascal ‘trainaug’ images report quantitative results pascal ‘val’ using standard mean intersection-over-union metric. deeplab-aspp-l model four parallel branches conv block resnet architecture. branch convolutional layer different atrous rate different atrous rates enable model capture objects context different scales. output branch pixel-wise scores classes resolution output scores added together normalized ﬁnal pixel-wise class probabilities. training mini-batch momentum. model trained iterations using minibatch images momentum initialize learning rate polynomial learning rate policy layers trained l-regularization data-augmentation multiscale training/testing post-processing using crfs task. initialize deeplab-aspp-l model using imagenet jft-m trained checkpoints ﬁnal classiﬁcation layer checkpoints replaced four convolutional branches intable human pose estimation performance coco ‘test-dev’ split. follow implementation g-rmi pose change resnet- initial checkpoints imagenet pretrained jft-m pre-trained. images resized results conv block resnet network well predictions entire model. comparison imagenet models. present quantitative comparison jft-m checkpoints imagenet checkpoints figure jft-m checkpoint outperforms imagenet points. further observe jft-m model trained imagenet checkpoint provides points boost vanilla imagenet checkpoint. impact data size. figure present analysis impact training data size randomly sampling subset images jft-m training base checkpoints observe performance increases logarithmically pre-training dataset increases. train fully-convolutional pose detector initializing base resnet model checkpoints ﬁne-tuning. model trained sgd+momentum steps. learning rate dropped factor steps starting base learning rate. best hyper parameter combination model selected independently used experimentation. table present pose estimation results evaluated coco ‘test-dev’ set. g-rmi pose uses imagenet pre-trained checkpoint ﬁne-tuning models jft-m initialization perform much better. note fair comparison g-rmi pose show performance coco images used training ensembling performed. person detection results provided authors apply trained pose detectors person boxes. expected performance computer vision algorithms would always improve data? personal correspondences several researchers general consensus seems everyone expects gain performance numbers dataset size increased dramatically decreasing marginal performance dataset grows. tremendous amount time spent engineering parameter sweeps; little time spent collectively data. paper attempt focus back data. models seem plateauing comes performance respect data modest performance improvements still possible exponential increases data. another major ﬁnding paper having better models leading substantial gains imagenet sufﬁcient parameters representational power. representation learning underlying debates spend time collecting data individual tasks detection segmentation. ﬁndings show still gained representation learning. improved base models base features lead signiﬁcant gains performance. disclaimer large scale learning would like highlight training regime learning schedules parameters used paper based understanding training convnets images. searching right hyper-parameters requires signiﬁcant effort even training model epochs needed months gpus. therefore sense quantitative performance reported paper underestimates impact data reported image volumes. acknowledgements work would possible without heroic efforts image understanding expander teams google built massive dataset. would speciﬁcally like thank duerig neil alldrin howard zhou chen david chechik zheyun feng xiangxin rahul sukthankar help. also thanks vale team apis speciﬁcally jonathan huang george papandreou liang-chieh chen kevin murphy helpful discussions.", "year": 2017}