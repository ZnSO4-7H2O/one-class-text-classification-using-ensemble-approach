{"title": "Hashing with binary autoencoders", "tag": ["cs.LG", "cs.CV", "math.OC", "stat.ML"], "abstract": "An attractive approach for fast search in image databases is binary hashing, where each high-dimensional, real-valued image is mapped onto a low-dimensional, binary vector and the search is done in this binary space. Finding the optimal hash function is difficult because it involves binary constraints, and most approaches approximate the optimization by relaxing the constraints and then binarizing the result. Here, we focus on the binary autoencoder model, which seeks to reconstruct an image from the binary code produced by the hash function. We show that the optimization can be simplified with the method of auxiliary coordinates. This reformulates the optimization as alternating two easier steps: one that learns the encoder and decoder separately, and one that optimizes the code for each image. Image retrieval experiments, using precision/recall and a measure of code utilization, show the resulting hash function outperforms or is competitive with state-of-the-art methods for binary hashing.", "text": "attractive approach fast search image databases binary hashing highdimensional real-valued image mapped onto low-dimensional binary vector search done binary space. finding optimal hash function diﬃcult involves binary constraints approaches approximate optimization relaxing constraints binarizing result. here focus binary autoencoder model seeks reconstruct image binary code produced hash function. show optimization simpliﬁed method auxiliary coordinates. reformulates optimization alternating easier steps learns encoder decoder separately optimizes code image. image retrieval experiments using precision/recall measure code utilization show resulting hash function outperforms competitive state-of-the-art methods binary hashing. consider problem binary hashing given high-dimensional vector want l-bit vector using hash function preserving neighbors binary space. binary hashing emerged recent years eﬀective technique fast search image databases. search original space would cost time space using ﬂoating point operations search binary space costs constant factor much smaller. hardware compute binary operations eﬃciently entire dataset main memory workstation. search binary space produce false positives negatives retrieve larger neighbors verify ground-truth distance still eﬃcient. many diﬀerent hashing approaches proposed last years. formulate objective function hash function binary codes tries capture notion neighborhood preservation. approaches things common typically performs dimensionality reduction noted outputs binary codes latter implies step function binarization applied real-valued function input optimizing diﬃcult. practice approaches follow two-step procedure ﬁrst learn real hash function ignoring binary constraints output resulting hash function binarized example continuous dimensionality reduction algorithm apply step function. procedure seen ﬁlter approach suboptimal example thresholded projection necessarily best thresholded linear projection obtain latter must optimize objective jointly linear mappings thresholds respecting binary constraints learning wrapper approach words optimizing real codes projecting onto binary space optimizing codes binary space. paper show joint optimization respecting binary constraints training actually carried reasonably eﬃciently. idea recently proposed method auxiliary coordinates general strategy transform original problem involving nested function separate problems without nesting solved easily. case allows reduce drastically complexity binary constraints. focus binary autoencoders i.e. code layer binary. believe ﬁrst apply model construct eﬃcient optimization algorithm. section describes binary autoencoder model objective function. section derives training algorithm using explains carefully implemented steps optimization binary space carried eﬃciently parallelizes well. hypothesis constraining optimization binary space results better hash functions test experiments using several performance measures traditional precision/recall well reconstruction error entropy-based measure code utilization show linear hash functions resulting optimizing binary autoencoder using consistently competitive state-of-the-art even latter uses nonlinear hash functions sophisticated objective functions hashing. basic hashing approaches data-independent locality-sensitive hashing based random projections thresholding kernelized generally outperformed data-dependent methods learn speciﬁc hash function given dataset unsupervised supervised way. focus unsupervised data-dependent approaches. typically based deﬁning objective function either hash function binary codes optimizing however usually achieved relaxing binary codes continuous space thresholding resulting continuous solution. example spectral hashing essentially version laplacian eigenmaps binary constraints relaxed approximate eigenfunctions computed thresholded provide binary codes. variations include using anchorgraphs deﬁne eigenfunctions obtaining hash function directly binary classiﬁer using codes spectral hashing labels approaches optimize instead nonlinear embedding objective depends continuous parametric hash function thresholded deﬁne binary hash function objective depends thresholded hash function threshold relaxed optimization recent work tried respect binary nature codes thresholds using alternating optimization directly objective function entry weight matrix hash function subset binary codes since objective function involves large number terms binary codes weights coupled optimization slow. also learn hash function codes ﬁxed suboptimal. closest model binary autoencoder iterative quantization fast competitive hashing method. ﬁrst obtains continuous low-dimensional codes applying data seeks rotation makes codes close possible binary. latter based optimal discretization algorithm ﬁnds rotation continuous eigenvectors graph laplacian makes close possible discrete solution postprocessing spectral clustering. objective function continuous codes obtained pca. np-complete problem local minimum found using alternating optimization solution elementwise procrustes alignment problem closed-form solution based svd. ﬁnal hash function form thresholded linear projection. hence postprocessing codes seen suboptimal approach optimizing binary autoencoder binary constraints relaxed optimization projects continuous codes back binary space. semantic hashing also uses autoencoder objective deep encoder optimization uses heuristics guaranteed converge local optimum either training continuous problem backpropagation applying threshold encoder rounding encoder output backpropagation forward pass ignoring rounding backward pass consider well-known model continuous dimensionality reduction autoencoder deﬁned broad sense composition encoder maps real vector onto real code vector decoder maps back eﬀort reconstruct although ideas apply generally encoders decoders objective functions paper mostly focus least-squares error linear encoder decoder. well known optimal solution pca. hashing encoder maps continuous inputs onto binary code vectors bits write step function applied elementwise i.e. otherwise desired hash function minimize following problem given dataset high-dimensional patterns usual least-squares error code layer binary. optimizing nonsmooth function diﬃcult np-complete. gradients exist zero nearly everywhere. call binary autoencoder linear optimize decoder binary codes input pattern. without binary constraint i.e. rl×n model dates back sometimes called least-squares factor analysis solution pca. binary constraints problem np-complete includes particular case solving linear system integer feasibility problem. call model binary factor analysis believe model studied before least hashing. hash function obtained ﬁtting binary classiﬁer inputs code bits. ﬁlter approach optimal approach since optimizes jointly recently proposed method auxiliary coordinates idea break nested functional relationships judiciously introducing variables equality constraints. solved optimizing penalized function using alternating optimization original parameters coordinates results coordination-minimization algorithm. recall nested problem model introduce auxiliary coordinates outputs i.e. codes input patterns obtain following equality-constrained problem note codes binary. apply quadratic-penalty method minimize following objective function progressively increasing constraints eventually satisﬁed advantage auxiliary coordinates individual steps easy solve besides exhibit signiﬁcant parallelism. describe steps detail below. resulting algorithm alternates steps encoder decoder codes iterations allow encoder decoder mismatched since encoder output equal decoder input coordinated increases mismatch reduced. overall algorithm optimize although algorithm shown produce convergent algorithms diﬀerentiable objective function cannot apply theorem carreira-perpi˜n´an wang binary nature problem. instead show algorithm converges local minimum ﬁnite local minimum understood k-means point globally minimum given vice versa. following theorem valid choice linear. proof. appears step change there change either since steps exact. step minimizes µkzn theorem global minimizer statement follows fact bounded prove fact. clearly holds ﬁxed take values ﬁnite namely even functions inﬁnite number diﬀerent functions possible ﬁnite results exact ﬁxed possible ﬁnite figure continuous path induced quadratic-penalty objective function function objective functions quadratic-penalty objective related shown minimizers quadratic-penalty objective trace path continuously function space. objective functions correspond limits respectively. optimized independently must optimally resulting figure shows graphically connection objective functions ends continuous path space quadratic-penalty function deﬁned. practice learn model small value keep constant running algorithm. itself increase iterate steps value. usually algorithm stops iterations changes parameters occur. data bias parameter necessary example able instead equivalently values. solution regression computed note constant factor o-notation small binary e.g. involves sums multiplications. since binary hamming distance objective function number misclassiﬁed patterns separates bit. classiﬁcation problem using labels auxiliary coordinates linear classiﬁer however rather minimizing this solve easier closely related problem linear high penalty misclassiﬁed patterns optimize margin plus slack. besides easier surrogate loss advantage making solution unique generalizing better test data also although used quadratic penalty spirit penalty methods penalize constraint violations increasingly. since limit constraints satisﬁed exactly classiﬁcation error using zero hence linear optimum nested problem anyway. liblinear warm start note svms decoder function trained parallel. thus although problem binary np-complete good even exact solution obtained practical values small further intensive computation large number independent problems step take much advantage parallel processing. spent signiﬁcant eﬀort making step eﬃcient yielding good exact solutions. proceeding show reduce problem stated uses matrix equivalent problem using matrix enumeration small solved exactly enumeration worst-case runtime cost small constant factors practice perfectly practical workstation without parallel processing datasets experiments. alternating optimization larger alternating optimization groups bits converges local minimum step although experiments ﬁnds near-global optima using good initialization. intuitively makes sense warm-start this i.e. initialize code found previous iteration’s step since close optimum converge. however empirically codes change ﬁrst iterations following initialization works better early iterations solve relaxed problem s.t. rather strongly convex bound-constrained quadratic program variables unique minimizer found eﬃciently. this result also derived expanding norms matrix computing cholesky decomposition. however product squares singular values loses precision roundoﬀ error speed solution noting common special structure. objective term matrix term separable developed admm algorithm simple parallelizes vectorizes well reuses matrix factorizations qps. faster matlab’s quadprog. warm-start continuous solution previous step. order binarize continuous minimizer could simply round elements instead apply greedy procedure eﬃcient better optimally binarize evaluating objective function remaining elements ﬁxed picking best. essentially pass alternating optimization continuous values bits. finally pick best binarized relaxed solution warm-start value alternating optimization. ensures quadratic-penalty function decreases monotonically iteration. accelerations naively enumeration involves evaluating vectors evaluating costs average roughly multiplications sums. enumeration sped pruned still ﬁnding global minimum using upper bounds incremental computation necessary suﬃcient conditions solution. essentially need evaluate every code vector every every code vectors; know solution near recognize solution increases improves bound becomes eﬀective patterns pruned. upon convergence step costs search given keep running bound r¯zk µk¯z scan codes increasing hamming distance distance ¯e/µ. thus ﬁrst codes likely optimal keep reﬁning bound better codes. finally exist global optimality necessary suﬃcient conditions binary quadratic problems easy evaluate allows recognize solution soon reach stop search conditions also determine whether continuous solution relaxed global minimizer binary user parameters method initialization binary codes schedule penalty parameter since penalty augmented lagrangian method. general methods setting schedule requires tuning practice. fortunately simpliﬁed case reasons. need drive termination occurs ﬁnite easily detected whenever step changes parameters occur. gives practical stopping criterion. order generalize well unseen data stop iterating optimize precision validation decreases. form early stopping guarantees improve initial besides faster. initialization details schedule appear section propose evaluation measure binary hash functions used know. binary hash function maps population high-dimensional real vectors onto population l-bit vectors intuitively good hash function make best available codes equally example bits distinct real vectors good hash function would ideally assign diﬀerent -bit code vector order avoid collisions. given l-bit hash function dataset real vectors obtain l-bit binary codes measure code utilization hash function dataset entropy code distribution deﬁned follows. number vectors binary code code distribution discrete probability distribution deﬁned l-bit integers probability code since ·+pl− i.e. code probabilities normalized counts computed data points. works whether smaller larger dataset sample distribution high-dimensional vectors estimate code usage induced hash function distribution based sample size entropy measured bits. entropy real number satisﬁes codes coincident codes distinct. hence entropy large available codes used uniform since entropy measured bits cannot said measure eﬀective number bits code distribution induced hash function dataset good hash function good code utilization making available codes avoid collisions preserve neighbors. however crucial realize optimal code utilization necessarily result optimal hash function fully reliable proxy precision/recall. indeed code utilization directly related distances data vectors easy construct hash functions produce optimal code utilization necessarily good preserving neighbors. example pick ﬁrst hash function hyperplane splits space half spaces half data points then within half second hash function etc. generates binary decision tree oblique cuts impractical hyperplanes internal node. real vectors follow gaussian distribution using hash functions thresholded principal components give maximum entropy hash functions hyperplane splits space half spaces containing half vectors hyperplanes orthogonal. gives thresholded method mentioned earlier generally competitive methods seen experiments. generally expect tpca random projections mean achieve high code utilization high-dimensional datasets low-dimensional projections approximately gaussian caveat mind code utilization measured eﬀective number bits still useful evaluation measure hash functions. also important advantage depend user parameters. particular depend ground truth size retrieved size allows compare binary hashing methods single number report values experiments section show indeed correlates well precision terms ranking methods particularly methods model objective function used three datasets experiments commonly used benchmarks image retrieval. cifar contains color images classes. ignore labels paper images training images test set. extract gist features image. nus-wide contains high-resolution color images training test. extract wavelet features image. experiments also nuswide-lite subset dataset containing images training images test. sift-m contains training high-resolution color images test images represented sift features. report precision recall test using true neighbors nearest images euclidean distance original space retrieved neighbors binary space either nearest images hamming distance images within hamming distance also compare algorithms using entropy-based measure code utilization described section experiments evaluate eﬀectiveness algorithm minimize objective whether translates better hash functions runtime parallel speedup; precision recall code utilization compared representative state-of-the-art algorithms. focus purely objective function study gain obtained optimization respects binary constraints suboptimal ﬁlter approach relaxing constraints binarizing result thresholding optimal rotation compute reconstruction error tpca optimal mapping given binary codes. nus-wide-lite subset nus-wide dataset. initialize tpca alternating optimization steps. search true neighbors report results range bits dominates methods reconstruction error expected also precision might expect. tpca consistently worst method signiﬁcant margin intermediate. hence respect binary constraints optimization better hash function. experiments consistently show precision signiﬁcantly increases initialization leading competitive methods. figure iterations runtime optimization objective function step). step alternating optimization g-bit groups warm-start relaxed initialization surprisingly warm-start initialization leads worse objective function values binarized relaxed one. fig. shows dashed lines solid lines reason that early optimization codes undergo drastic changes iteration next warm-start initialization farther good optimum relaxed one. late optimization codes change slowly warm-start perform well. relaxed initialization resulting optima almost using exact binary optimization. also surprisingly diﬀerent group sizes eventually converge almost result using exact binary optimization using relaxed initialization. likewise using alternating optimization step rather enumeration curves barely vary. course runtime iteration grows exponentially fig. shows training time speedup achieved parallel processing cifar bits. matlab parallel processing toolbox processors simply replace parfor loops iteration diﬀerent processor. observe nearly perfect scaling particular problem. note that larger number bits larger parallelization ability step. rough indication runtimes training cifar images nus-wide images using bits alternating optimization step takes respectively codes produced algorithm. observed initializing tends produce best results overall experiments. using produces also good results simpler faster option desired. initialize tpca since seems work best. order able ﬁxed schedule make data zero-mean rescale largest feature range alter euclidean distances normalizes scale. start double iteration step). noted section algorithm skip values improve precision validation stop ﬁnite value compare following algorithms thresholded iterative quantization spectral hashing kernelized locality-sensitive hashing anchorgraph hashing spherical hashing note several learn nonlinear hash functions sophisticated error functions uses linear hash function simply minimizes reconstruction error. experiments output tpca initialize respectively. known retrieval performance given algorithm depends strongly size neighbor used report experiments small large number points ground truth set. nus-wide considered ground truth neighbors query point retrieved neighbors retrieve either nearest neighbors neighbors within hamming distance fig. shows results. annsift-m considered ground truth neighbors retrieved neighbors fig. shows results. curves average test set. also show precision recall results cifar dataset well retrieved images sample query images test nus-wide-lite dataset diﬀerent ground truth sizes. although relative performance diﬀerent methods varies depending reported size trends clear. generally beats methods sometime signiﬁcant margin. become close cifar nus-wide dataset respectively. also quite competitive consistently worse situation precision appears decrease large small. reason many test images neighbors hamming distance less report zero precision them. suggests hash function ﬁnds avoid collisions bits available. practice would fig. shows results using diﬀerent initializations codes respectively. although initialization aﬀect local optimum algorithm ﬁnds optimum always better initialization tends give competitive results algorithms. fig. shows eﬀective number bits methods datasets nus-wide annsiftm compared ﬁgure nus-wide ﬁgure annsift-m. method show training test also show diagonal-horizontal line indicate upper bound methods line closer better code utilization. mentioned section tpca consistently gives largest consistently reliable indicator good precision/recall. however reasonably good correlation precision methods display comparable order measures. especially true comparing precision plots corresponding retrieving neighbors within hamming distance particularly clear annsift-m dataset. methods precision drops high also show stagnating values. small number points limits entropy achievable. methods making good available codes collisions need larger hamming distance neighbors. particular explains drop precision small also noted earlier. training test larger would expect precision continue increase values correlation performance precision also seen comparing methods using model optimizing objective binary autoencoder reconstruction error mac. consistent improvement precision seen too. suggests better optimization eﬀected improving hash function number points dataset small compared number available codes stagnates increases obvious test annsift-m since small. together precision/recall curves used determine number bits given database. leftmost plot shows actual code distribution histogram. plot number high-dimensional vectors code binary code used code distribution section unnormalized without plotting zero-count codes. entropy distribution gives value middle plot. high entropy corresponds large number used codes uniformity counts. contribution paper reveal connection binary autoencoders. seen fast approximate optimization objective function using ﬁlter approach algorithm corrected version itq. admittedly objective functions suited information retrieval autoencoder explicitly encouraging distances original hamming space match order preserve nearest neighbors however autoencoders result good hash functions evidenced good performance method using neural nets). reason that continuous codes autoencoders capture data manifold smooth indirectly preserve distances encouraging similar images similar codes—even worsened note that although similar respects binary autoencoder graphical model particular stacked restricted boltzmann machine binary autoencoder composes deterministic mappings binary outputs another binary inputs. hence objective function binary autoencoders discontinuous. objective function rbms diﬀerentiable involves normalization factor computationally intractable. results diﬀerent optimization nonsmooth optimization binary autoencoders smooth optimization using sampling approximate gradients rbms. vincent many hashing approaches essentially ignored binary nature problem approximated relaxation truncation possibly disregarding hash function learning binary codes. inspiration work capitalize decoupling introduced method auxiliary coordinates able break combinatorial complexity optimizing binary constraints introduce parallelism problem. armed algorithm shown respecting binary nature problem optimization possible eﬃcient leads better hash functions competitive state-of-the-art. particularly encouraging given autoencoder objective best retrieval focused linear hash functions. algorithm intuitive form reuse existing well-developed code. extension nonlinear hash reconstruction mappings straightforward interesting much improve linear case. paper step towards constructing better hash functions using framework. believe apply widely objective functions. symmetric matrix general np-hard problem beck teboulle gave global optimality conditions problem simply expressed terms problem’s data involving primal variables dual variables here λmin smallest eigenvalue vector ones diag diagonal matrix diagonal entries vector diag diagonal matrix diagonal entries qii. intuitively smaller disregard quadratic term trivially solve separable linear term. furthermore relation solution binary problem close enough relaxed one. optimizer relaxed problem satisﬁes λmine global optimizer binary problem case particularly interested suﬃcient conditions combine computationally conditions comparable cost evaluating objective function l-bit vector besides arithmetic operations common evaluating necessary suﬃcient conditions global optimality binary problem latter fast test determine whether found global minimizer stop search likewise necessary suﬃcient conditions determine whether solution relaxed problem global minimizer discrete problem since function quadratic function convex. also note computing diag λmin done data points training since problems share matrix", "year": 2015}