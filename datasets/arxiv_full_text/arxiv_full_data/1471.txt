{"title": "A Survey of Current Datasets for Vision and Language Research", "tag": ["cs.CL", "cs.AI", "cs.CV", "cs.GL"], "abstract": "Integrating vision and language has long been a dream in work on artificial intelligence (AI). In the past two years, we have witnessed an explosion of work that brings together vision and language from images to videos and beyond. The available corpora have played a crucial role in advancing this area of research. In this paper, we propose a set of quality metrics for evaluating and analyzing the vision & language datasets and categorize them accordingly. Our analyses show that the most recent datasets have been using more complex language and more abstract concepts, however, there are different strengths and weaknesses in each.", "text": "integrating vision language long dream work artiﬁcial intelligence past years witnessed explosion work brings together vision language images videos beyond. available corpora played crucial role advancing area research. paper propose quality metrics evaluating analyzing vision language datasets categorize accordingly. analyses show recent datasets using complex language abstract concepts however different strengths weaknesses each. bringing together language vision intelligent system long ambition research beginning shrdlu ﬁrst vision-language integration systems continuing recent attempts conversational robots grounded visual world past years inﬂux large vision language corpora alongside dramatic advances vision research sparked renewed interest connecting vision language. vision language corpora provide alignments visual content recognized computer vision algorithms language understood generated using natural language processing techniques. past year recent work proposed methods image video captioning summarization reference question answering name few. newly crafted large-scale vision language datasets played crucial role deﬁning research serving foundation training/testing helping benchmarks measuring system performance. crowdsourcing large image collections provided flickr made possible researchers propose methods vision language tasks alongside accompanying dataset. however datasets emerged space become unclear different methods generalize beyond datasets evaluated data useful moving ﬁeld beyond single task towards solving larger problems. paper take step back document moment time making record major available corpora driving ﬁeld. provide quantitative analysis corpora order understand characteristics each compare another. quality dataset must measured compared related datasets quality data distort entire subﬁeld. propose criteria analyzing evaluating comparing quality vision language datasets other. knowing details dataset compared similar datasets allows researchers deﬁne precisely task trying solve select dataset best suited goals aware implications biases datasets could impose task. teria. datasets present chosen available community cover data created support recent focus image captioning work. importantly provide evolving website containing pointers references many vision-to-language datasets believe valuable unifying quickly expanding research tasks language vision. quality dataset highly dependent sampling scraping techniques used early data collection process. however content datasets play major role narrowing focus ﬁeld. datasets affected reporting bias frequency people write actions events states directly reﬂect real-world frequencies phenomena; also affected photographer’s bias photographs somepredictable within given domain. suggests datasets useful towards larger goal provided alongside quantitative metrics show compare similar corpora well general background corpora. metrics used indicators dataset bias language richness. higher level argue clearly deﬁned metrics necessary provide quantitative measurements dataset compares previous work. helps clarify benchmark research progressing towards broader goal data comes play. section propose metrics characterize vision language datasets. focus methods measure language quality used across several corpora. also brieﬂy examine metrics vision quality. evaluate several recent datasets based proposed metrics section results reported tables figure syntactic complexity measures amount embedding/branching sentence’s syntax. report mean yngve frazier measurements provides different counting number nodes phrase markers syntactic trees. part speech distribution measures distribution nouns verbs adjectives parts speech. abstractconcrete ratio indicates range visual non-visual concepts dataset covers. abstract terms ideas concepts ‘love’ ‘think’ concrete terms objects events mainly available senses. purpose list common abstract terms english deﬁne concrete terms words except small function words. average sentence length shows rich descriptive sentences are. perplexity provides measure data skew measuring expected sentences corpus according model trained ancorpus. analyze perplexity dataset -gram language model learned generic words english dataset. analyze pair-wise perplexity datasets section vision quality focus survey mainly language however characteristics images videos corresponding annotations important vision language research. quality vision dataset characterized part variety visual subjects scenes provided well richness annotations visual dependencies boxes). moreover vision corpus abstract real images datasets either original photo title descriptions provided online users captions generated crowd workers existing images. former datasets tend larger size contain contextual descriptions. user-generated captions captioned photo dataset contains million images original user generated captions collected wild systematic querying flickr. dataset collected querying flickr speciﬁc terms objects actions ﬁltered images descriptions longer certain mean length. d´ej`a images dataset consists unique user-generated captions associated flickr images caption aligned multiple images. dataset collected querying flickr high frequency nouns ﬁltered least verb judged good captions workers amazon’s mechanical turk crowd-sourced captions uiuc pascal dataset probably ﬁrst datasets aligning images captions. pascal dataset contains images sentences image. flickr images extends previous flickr datasets includes crowd-sourced captions describe images people involved everyday activities events. microsoft coco dataset includes complex everyday scenes common objects naturally occurring contexts. objects scene labeled using per-instance segmentations. total dataset contains photos basic object types million labeled instances images paired captions. dataset gave rise cvpr image captioning challenge continuing benchmark comparing various aspects vision language research. abstract scenes dataset created goal representing real-world scenes clipart study scene semantics isolated object recognition segmentation issues image processing. removes burden low-level vision tasks. dataset contains images children playing captions densely labeled images existing caption datasets provide images paired captions brief image descriptions capture subset content image. measuring magnitude reporting bias inherent descriptions helps understand discrepancy learn speciﬁc task image captioning versus learn generally photographs people take. dataset useful provides image annotation content selection microsoft research dense visual annotation corpus provides images flickr dataset densely labeled textual labels bounding boxes facets annotated object. approximates gold standard visual recognition. rough estimate reporting bias image captioning determined percentage top-level objects mentioned captions dataset objects annotated. average available top-level objects image captions reports average objects. detailed analysis reporting bias beyond scope paper found many biases found abstract scenes also present photos. video datasets aligned descriptions generally represent limited domains small lexicons fact video processing understanding compute-intensive task. available datasets include short videos described sentences includes video clips external synonym paraphrasing resource perform matching labels captions dataset provides paraphrases object object labeled multiple turkers labeled relations table summary statistics quality metrics sample major datasets. brown report frazier yngve scores automatically acquired parses also compute sentences gold parses setting mean frazier score mean yngve score outdoor environments) showing multiple simultaneous events subset four objects person backpack chair trash-can. video manually annotated several sentences describing occurs video. microsoft research video description corpus contains parallel descriptions short video snippets descriptions sentence summaries actions events video described amazon turkers. dataset paraphrase bilingual alternatives captured hence dataset useful translation paraphrasing video description purposes. recent work demonstrated n-gram language modeling paired scene-level understanding image trained large enough datasets result reasonable automatically generated captions works proposed step beyond description generation towards deeper tasks question answering present attempts below visual madlibs dataset subset images coco dataset aims beyond describing objects image. given image three amazon turkers prompted complete ﬁll-in-the-blank template questions ‘when look picture feel selected automatically based image content. dataset contains total madlib question answers. visual question answering dataset created task openended system presented image free-form natural-language question able answer question. dataset contains real images abstract scenes paired questions answers. real images include images coco dataset clip-art abstract scenes made ‘paperdoll’ human models adjustable limbs objects animals. amazon turkers prompted create ‘interesting’ questions resulting questions answers. toronto coco-qa dataset also visual question answering dataset questions automatically generated image captions coco dataset. dataset total images questions one-word answer objects numbers colors locations. analyze datasets introduced section according metrics deﬁned section using stanford corenlp suite acquire parses part-of-speech tags also include brown corpus reference point. evidence dataset captures abstract concepts datasets almost words found abstract concept resource. deja corpus least number abstract concepts followed coco vdc. reﬂects differences coltable perplexities across corpora rows represent test sets columns training sets make perplexities comparable used vocabulary frequency cutoff models -grams. table illustrates similarities datasets ﬁne-grained perplexity measure usefulness given training predicting words given test set. datasets coco flickrk clipart generally useful out-domain data compared datasets. test sets quite idiosyncratic yield poor perplexity unless trained in-domain data. shown figure coco dataset balanced across tags similarly balanced brown corpus clipart dataset provides highest proportion verbs often correspond actions/poses vision research flickrk corpus provides nouns often correspond object/stuff categories vision research. emphasize distinction qualitatively good dataset task dependent. therefore metrics obtained results provide researchers objective criteria make decision whether dataset suitable particular task. detail recent growth vision language corpora compare contrast several recently released large datasets. argue newly introduced corpora measure compare similar datasets measuring perplexity syntactic complexity abstractconcrete word ratios among metrics. leveraging metrics comparing across corpora research sensitive datasets biased different directions deﬁne corpora accordingly. figure simpliﬁed part-of-speech distributions eight datasets. include tags balanced brown corpus contextualize shallow syntactic biases. mapped nouns verbs adjectives tags lecting various corpora example deja corpus collected speciﬁcally visual phrases used describe multiple images. corpus also syntactically simple phrases measured frazier yngve; likely caused phrases needing general enough capture multiple images. syntactically complex sentences found flickrk coco datasets. however dataset suffers high perplexity background corpus relative datasets odds relatively short sentence lengths. suggests automatic caption-to-question conversion creating unexpectedly complex sentences less reﬂective general language usage. contrast coco flickrk dataset’s relatively high syntactic complexity line relatively references stanislaw antol aishwarya agrawal jiasen margaret mitchell dhruv batra lawrence zitnick devi parikh. visual question answering. arxiv preprint arxiv.. rehj cantrell matthias scheutz paul schermerhorn xuan robust spoken instruction understanding hri. pamela hinds hiroshi ishiguro takayuki kanda peter kahn editors pages acm. david chen william dolan. collecting highly parallel data paraphrase evaluation. proceedings annual meeting association computational linguistics human language technologies volume pages stroudsburg usa. association computational linguistics. jianfu chen polina kuznetsova david warren yejin choi. d´ej`a image-captions corpus expressive descriptions repetition. proceedings conference north american chapter association computational linguistics human language technologies pages denver colorado may–june. association computational linguistics. jeff donahue lisa anne hendricks sergio guadarrama marcus rohrbach subhashini venugopalan kate saenko trevor darrell. long-term recurrent convolutional networks visual recognition description. corr abs/.. fang saurabh gupta forrest iandola rupesh srivastava deng piotr doll´ar jianfeng xiaodong margaret mitchell john platt lawrence zitnick geoffrey zweig. captions visual concepts back. corr abs/.. farhadi mohsen hejrati mohammad amin sadeghi peter young cyrus rashtchian julia hockenmaier david forsyth. every picture tells story generating sentences images. proceedings european conference computer vision part eccv’ pages berlin heidelberg. springer-verlag. nelson francis henry kucera. brown corpus manual manual information accompany standard corpus present-day edited american english digital computers. brown university providence rhode island usa. dowty karttunen zwicky editors natural language parsing psychological computational theoretical perspectives pages cambridge university press cambridge. jonathan gordon benjamin durme. reporting bias knowledge extraction. automated knowledge base construction workshop knowledge extraction cikm akbc’. sahar kazemzadeh vicente ordonez mark matten tamara berg. referitgame referring objects photographs natural scenes. proceedings conference empirical methods natural language processing pages doha qatar october. association computational linguistics. gunhee seungwhan moon leonid sigal. joint photo stream blog post summarization exploration. ieee conference computer vision pattern recognition geert-jan kruijff hendrik zender patric jensfelt henrik christensen. situated dialogue spatial organization what where. why? international journal advanced robotic systems special issue human robot interactive communication march. tsung-yi michael maire serge belongie james hays pietro perona deva ramanan piotr dollar lawrence zitnick. microsoft coco common objects context. corr abs/.. mateusz malinowski mario fritz. multiworld approach question answering realworld scenes based uncertain input. advances neural information processing systems pages jonathan malmaud jonathan huang vivek rathod nicholas johnston andrew rabinovich kevin murphy. whats cookin? interpreting cooking videos using text speech vision. north american chapter association computational linguistics human language technologies june denver colorado usa. christopher manning mihai surdeanu john bauer jenny finkel steven bethard david mcclosky. stanford corenlp natural lanproceedings guage processing toolkit. annual meeting association computational linguistics system demonstrations pages cynthia matuszek nicholas fitzgerald luke zettlemoyer liefeng dieter fox. joint model language perception grounded proc. internaattribute learning. tional conference machine learning edinburgh scotland june. iftekhar naim young song qiguang liang huang henry kautz jiebo daniel gildea. discriminative unsupervised alignment natural language instructions corresponding video segments. north american chapter association computational linguistics human language technologies june denver colorado usa. vicente ordonez girish kulkarni tamara berg. imtext describing images using million captioned photographs. neural information processing systems cyrus rashtchian peter young micah hodosh julia hockenmaier. collecting image annotations using amazon’s mechanical turk. proceedings naacl workshop creating speech language data amazon’s mechanical turk csldamt pages stroudsburg usa. association computational linguistics. michaela regneri marcus rohrbach dominikus wetzel stefan thater bernt schiele manfred pinkal. grounding action descriptions videos. transactions association computational linguistics marcus rohrbach sikandar amin mykhaylo andriluka bernt schiele. database grained activity detection cooking activities. ieee conference computer vision pattern recognition ieee ieee june. kai-yuh hsiao nikolaos mavridis. conversational robots building blocks proceedings grounding word meaning. hlt-naacl workshop learning word meaning non-linguistic data volume hltnaacl-lwm pages stroudsburg usa. association computational linguistics. bart thomee david shamma gerald friedland benjamin elizalde karl douglas poland damian borth li-jia data challenges multimedia research. arxiv preprint arxiv.. torralba efros. unbiased look proceedings ieee dataset bias. conference computer vision pattern recognition cvpr pages washington usa. ieee computer society. subhashini venugopalan huijuan jeff donahue marcus rohrbach raymond mooney kate saenko. translating videos natural language using deep recurrent neural networks. north proceedings conference american chapter association computational linguistics human language technologies pages denver colorado june. mark yatskar michel galley lucy vanderwende luke zettlemoyer. evil evil description generation densely labeled images. proceedings third joint conference lexical computational semantics pages dublin ireland august. association computational linguistics dublin city university. peter young alice micah hodosh julia hockenmaier. image descriptions visual denotations similarity metrics semantic inference event descriptions. transactions association computational linguistics haonan jeffrey mark siskind. grounded language learning video described senproceedings annual meettences. association computational linguistics volume pages soﬁa bulgaria. association computational linguistics. best paper award. lawrence zitnick devi parikh lucy vanderwende. learning visual interpretation ieee international conference sentences. computer vision iccv sydney australia december pages", "year": 2015}