{"title": "Multi-task Learning of Pairwise Sequence Classification Tasks Over  Disparate Label Spaces", "tag": ["cs.CL", "cs.NE", "stat.ML"], "abstract": "We combine multi-task learning and semi-supervised learning by inducing a joint embedding space between disparate label spaces and learning transfer functions between label embeddings, enabling us to jointly leverage unlabelled data and auxiliary, annotated datasets. We evaluate our approach on a variety of sequence classification tasks with disparate label spaces. We outperform strong single and multi-task baselines and achieve a new state-of-the-art for aspect- and topic-based sentiment analysis.", "text": "combine multi-task learning semisupervised learning inducing joint embedding space disparate label spaces learning transfer functions label embeddings enabling jointly leverage unlabelled data auxiliary annotated datasets. evaluate approach variety sequence classiﬁcation tasks disparate label spaces. outperform strong single multi-task baselines achieve stateof-the-art aspecttopic-based sentiment analysis. multi-task learning semi-supervised learning successful paradigms learning scenarios limited labelled data recent years applied almost areas nlp. applications example include partial parsing text normalisation neural machine translation keyphrase boundary classiﬁcation contemporary work typically focuses learning representations useful across tasks often hard parameter sharing hidden layers neural networks tasks share optimal hypothesis classes level representations leads improvements however sharing hidden layers neural networks effective regulariser potentially loose synergies classiﬁcation functions trained associate representations class labels. paper sets build architecture synergies exploited many tasks disparate label sets weakly correlated e.g. part-of-speech tags correlate dependencies sentiment correlates emotion etc. thus propose induce joint label embedding space using label embedding layer allows model relationships show helps learning. addition tasks labels closely related able model relationship also directly estimate corresponding label target task based auxiliary predictions. propose train label transfer network jointly model produce pseudo-labels across tasks. used label unlabelled auxiliary task data utilising ‘dark knowledge’ contained auxiliary model predictions. pseudo-labelled data incorporated model semisupervised learning leading natural combination multi-task learning semi-supervised learning. additionally augment data-speciﬁc diversity features learning. contributions contributions model relationships labels inducing joint label space multi-task learning. propose label transfer network learns transfer labels tasks propose semi-supervised learning leverage training. evaluate approaches variety classiﬁcation tasks shed light settings multi-task learning works. learning task similarities existing approaches learning similarities tasks enforce clustering tasks induce shared prior learn grouping approaches focus homogeneous tasks employ linear bayesian models. thus directly applied setting tasks using disparate label sets. multi-task learning neural networks recent work multi-task learning goes beyond hard parameter sharing considers different sharing structures e.g. sharing lower layers induces private shared subspaces approaches however able take account relationships labels learning. another related direction train disparate annotations task contrast different nature tasks requires modelling label spaces. semi-supervised learning exists wide range semi-supervised learning algorithms e.g. self-training co-training tri-training combinations thereof several also used nlp. approach probably closely related algorithm called coforest co-forest like here learner improved unlabeled instances labeled ensemble consisting learners. note also several researchers proposed using auxiliary tasks unsupervised also leads form semi-supervised models. label transformations idea manually mapping label sets learning mapping facilitate transfer new. zhang distributional information language-speciﬁc tagset tagset used languages order facilitate crosslingual transfer. related work canonical correlation analysis transfer tasks disparate label spaces. problem deﬁnition multi-task learning scenario access labelled datasets tasks training time target task particularly care about. training dataset task consists examples base model labels deep neural network performs classic hard parameter sharing shares parameters across tasks task-speciﬁc softmax output layers output probability distribution task according following equation label embedding layer order learn relationships labels propose label embedding layer embeds labels tasks joint space. instead training separate softmax output layers above introduce label compatibility function measures similar label embedding hidden representation figure multi-task learning hard parameter sharing tasks labels task. shared representation used input task-speciﬁc softmax layers optimise crossentropy losses label embedding layer embeds task labels joint embedding space uses prediction label compatibility function. semi-supervised label transfer network addition optimises unsupervised loss lpseudo pseudo-labels auxiliary/unlabelled data. pseudo-labels produced main task using concatenation auxiliary task label output embeddings input. models product objective function rely negative sampling hinge loss negative instances known. efﬁciency purposes matrix multiplication instead single product softmax instead sigmoid activations li)×l label embedding matrix tasks dimensionality label embeddings. practice hidden dimensionality padding apply task-speciﬁc mask order obtain task-speciﬁc probability distribution pti. shared across tasks allows learn relationships labels joint embedding space. show figure label transfer network allows learn relationships between labels. order make relationships would like leverage predictions auxiliary tasks estimate label target task. introduce label transfer network network takes auxiliary task outputs input. particular deﬁne output label embedding task label embeddings encode general relationship labels model’s probability distribution predictions encodes ﬁnegrained information useful learning trained labelled target task data. example corresponding label output embeddings auxiliary tasks multi-layer perceptron trained negative log-likelihood objective lltn produce pseudo-label target task designates concatenation. mapping tasks yields another signal useful optimisation regulariser. also seen mixtureof-experts layer experts auxiliary task models. label embeddings learned jointly main model sensitive relationships labels separately learned mixture-of-experts model relies semi-supervised downside requires additional parameters relies predictions auxiliary models impacts runtime testing. instead using prediction directly provide pseudolabels unlabelled auxiliary task data utilising auxiliary predictions semi-supervised learning. loss term loss equation learned together model pseudo-labels produced early training likely helpful based unreliable auxiliary predictions. reason ﬁrst train base model convergence augment ltn. show full semi-supervised learning procedure figure data-speciﬁc features domain shift datasets different tasks common instance learning models different label sets output label embeddings might contain sufﬁcient information bridge domain gap. mitigate discrepancy augment ltn’s input features found useful transfer learning particular number word types type-token ratio entropy simpson’s index rényi entropy diversity features. calculate feature example. features concatenated input ltn. propose several additional improvements seek alleviate burden skip-connections shown useful multitask learning recent work furthermore task-speciﬁc layer output layer useful learning taskspeciﬁc transformations shared representations experiments evaluate wide range particular text classiﬁcation tasks. choose pairwise classiﬁcation tasks—i.e. condition reading sequence ansequence—as interested understanding knowledge transferred even complex interactions. ﬁrst work best knowledge transfer learning pairwise sequence classiﬁcation tasks. implement models tensorﬂow release code https//github.com/ coastalcph/mtl-disparate. topic-based sentiment analysis topic-based sentiment analysis aims estimate sentiment tweet known given topic. data semeval- task subtask predicting twopoint scale positive negative ﬁve-point scale ranging highly negative highly positive respectively. example dataset would classify tweet power home dark listening ac/dc hope it’ll make electricity come back again known topic ac/dc labelled positive sentiment. evaluation metrics topic- topic- macro-averaged recall macro-averaged mean absolute error respectively averaged across topics. target-dependent sentiment analysis targetdependent sentiment analysis seeks classify sentiment text’s author towards entity occurs text positive negative neutral. data dong example instance expression like settlers catan wii? labelled neutral towards target wii’.’ evaluation metric macroaveraged data semeval- task subtask slot laptops restaurants domains. example sentence price cannot well manhattan labelled positive towards aspects restaurant prices food quality. evaluation metric domains accuracy stance detection stance detection requires model given text target entity might appear text predict whether author text favour target whether neither inference likely data semeval- task subtask example dataset would predict stance tweet prepared continue policies liberal left greece towards topic donald trump labelled favor. evaluation metric macro-averaged score favour against classes example dataset document dino ferrari hooked whopper wels catﬁsh could biggest world. headline fisherman lands stone catﬁsh could biggest world hooked labelled agree. evaluation metric accuracy natural language inference natural language inference task predicting whether sentences entails contradicts neutral towards another one. multi-genre corpus repeval shared task example instance would sentence pair children adults children base model base model bidirectional encoding model state-of-theart model stance detection conditions bidirectional lstm encoding text bilstm encoding target. unlike augenstein pre-train word embeddings larger unlabelled indomain text task mainly interested exploring beneﬁt multi-task learning generalisation. training settings bilstms hidden layer dimensions -dimensional randomly initialised word embeddings label embedding size train models rmsprop learning rate batch size early stopping validation main task patience main results shown table comparison state art. present results multi-task learning network label embeddings multi-task learning label transfer semi-supervised extension model. tasks least architectures better single-task learning; architectures much better single-task learning. compare taskdependent architectures. architectures contrast optimised compare favourably state main objective develop novel approach multi-task learning leveraging synergies label sets knowledge marginal distributions unlabeled data. example pre-trained word embeddings class weighting deal label imbalance domainspeciﬁc sentiment lexicons nevertheless approach outperforms state-of-the-art two-way topic-based sentiment analysis poor performance compared stateof-the-art multinli expected; alternate among tasks training model sees comparatively small number examples corpora orders magnitude larger datasets. reason achieve good performance tasks main tasks still useful auxiliary tasks seen table label embeddings results show that indeed modelling similarity tasks using label embeddings sometimes leads much better performance. figure shows why. figure visualise label embeddings mtl+lel model trained tasks using pca. similar labels clustered together across tasks e.g. positive clusters negative clusters neutral clusters visualisation also provides picture auxilary tasks beneﬁcial extent expect synergies multitask learning. instance notion positive sentiment appears similar across topic-based aspect-based tasks conceptions negative neutral sentiment differ. addition model failed learn relationship multinli labels tasks possibly accounting poor performance inference task. evaluate correlation label embeddings task performance bjerva recently suggested mutual information target auxiliary task label sets good predictor gains multi-task learning. auxilary tasks task show auxiliary tasks achieved best performance table contrast existing work restrict performing multi-task learning auxiliary task indeed fnc- multinli target fnc- multinli absa-l target fnc- multinli topic- fnc- multinli target topic- topic- absa-l target stance multinli topic- absa-r target perform detailed ablation analysis model results shown table ablate whether whether whether output main model output prediction whether regulariser semi-supervised learning test whether diversity features whether main model predictions often combination auxiliary tasks achieves best performance. in-domain tasks less used assumed; target consistently used twitter main tasks. addition tasks higher number labels e.g. topic- used often. tasks provide ﬁne-grained reward signal help learning representations generalise better. finally tasks large amounts training data fnc- multinli also used often. even directly related larger amount training data indirectly leveraged multi-task learning help model focus relevant parts representation space observations shed additional light multi-task learning useful beyond existing studies table ablation results task-speciﬁc evaluation metrics test early stopping set. means output relabelling function shown task predictions predictions tasks. main preds feats means main model predictions used features relabelling function. main model means main model predictions model trains relabelling function used. note multinli down-sample training data. lower better. bold best. underlined second-best. table error analysis withsemi-supervised learning tasks. metric shown percentage correct predictions made either relabelling function main model respectively relative number correct predictions. understand performance analyse learning curves relabelling function main model. examples tasks without semi-supervised learning shown figure observe relabelling model take long converge fewer parameters main model. relabelling model learned alongside main model main model performance ﬁrst stagnates starts increase again. tasks main model ends higher task score relabelling model. hypothesise means softmax predictions other even highly related tasks less helpful predicting main labels output layer main task model. best learning relabelling model alongside main model might regulariser main model thus improve main model’s performance baseline model case topic- analyse performance look degree predictions main model relabelling model individual instances complementary another. said differently measure percentage correct predictions made relabelling model made main model relative number correct predictions overall. results task shown table without semi-supervised learning. observe that even though relabelling function overall contributes score lesser degree main model substantial number correct predictions made relabelling function missed main model. prominently pronounced absa-r proportion presented multi-task learning architecture leverages potential synergies between classiﬁer functions relating shared representations disparate label spaces enables learning mixtures labeled unlabeled data. presented experiments combinations eight pairwise sequence classiﬁcation tasks. results show leveraging synergies label spaces sometimes leads improvements presented state aspect-based topic-based sentiment analysis. analysis showed learned label embeddings indicative gains multi-task learning auxiliary tasks often beneﬁcial across domains label embeddings almost always better performance. also investigated dynamics label transfer network exploiting synergies disparate label spaces. sebastian ruder supported irish research council grant number ebppg// science foundation ireland grant number sfi//rc/. anders søgaard supported starting grant number gratefully acknowledge support nvidia corporation donation titan used research. references martín abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin tensorﬂow large-scale machine learning heterogeneous distributed systems. arxiv preprint arxiv. isabelle augenstein rocktäschel andreas vlachos kalina bontcheva. twitter stance detection bidirectional conditional encoding. proceedings emnlp. caroline brun julien perez claude roux. xrce semeval- task feedbacked ensemble modelling syntactico-semantic knowledge aspect based sentiment analysis. proceedings semeval qian chen xiaodan zhen-hua ling jiang diana inkpen. recurrent neural network-based sentence encoder gated attention natural language inference. arxiv preprint arxiv. ronan collobert jason weston léon bottou michael karlen koray kavukcuoglu pavel kuksa. natural language processing scratch. journal machine learning research dong furu chuanqi duyu tang ming zhou adaptive recursive neural network target-dependent twitter sentiment classiﬁcation. proceedings acl. pages eisner rocktäschel isabelle augenstein sebastian riedel. matko bosnjak emojivec learning emoji representations description. proceedings socialnlp. bjarke felbo alan mislove anders søgaard iyad rahwan sune lehmann. using millions emoji occurrences learn any-domain representations detecting sentiment emotion sarcasm. proceedings emnlp. kazuma hashimoto caiming xiong yoshimasa tsuruoka richard socher. joint manytask model growing neural network multiple tasks. proceedings emnlp. ayush kumar sarah kohail amit kumar asif ekbal chris biemann. iit-tuda semeval task beyond sentiment lexicon combining domain dependency distributional semantics features aspect based sentiment analysis. proceedings semeval ming zhi-hua zhou. improve computeraided diagnosis machine learning techniques using undiagnosed samples. ieee transactions systems cybernetics preslav nakov alan ritter sara rosenthal veselin stoyanov fabrizio sebastiani. semeval task sentiment analysis twitter. proceedings semeval. diego california. nikita nangia adina williams angeliki lazaridou samuel bowman. repeval shared task multi-genre natural language inference sentence representations. proceedings repeval. elisavet palogiannidi athanasia kolovou fenia christopoulou filippos kokkinos elias iosif nikolaos malandrakis haris papageorgiou shrikanth narayanan alexandros potamianos. tweester semeval- task sentiment analysis twitter using semantic-affective model adaptation. proceedings semeval. pages barbara plank anders søgaard yoav goldberg. multilingual part-of-speech tagging bidirectional long short-term memory models auxiliary loss. proceedings acl. maria pontiki dimitris galanis haris papageorgiou androutsopoulos suresh manandhar mohammed al-smadi mahmoud al-ayyoub yanyan zhao bing orphée clercq veronique hoste marianna apidianaki xavier tannier natalia loukachevitch evgeniy kotelnikov núria salud maria jiménez-zafra gül¸sen eryi˘git. semeval- task aspect based sentiment analysis. proceedings semeval. benjamin riedel isabelle augenstein georgios spithourakis sebastian riedel. simple tough-to-beat baseline fake news arxiv preprint challenge stance detection task. arxiv.. sebastian riedel limin andrew mccallum benjamin marlin. relation extraction matrix factorization universal schemas. proceedings naacl-hlt pages xuejun liao lawrence carin balaji krishnapuram. multi-task learning classiﬁcation dirichlet process priors. journal machine learning research", "year": 2018}