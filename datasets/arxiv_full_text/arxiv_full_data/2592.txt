{"title": "On Context-Dependent Clustering of Bandits", "tag": ["cs.LG", "cs.AI", "cs.IR", "stat.ML"], "abstract": "We investigate a novel cluster-of-bandit algorithm CAB for collaborative recommendation tasks that implements the underlying feedback sharing mechanism by estimating the neighborhood of users in a context-dependent manner. CAB makes sharp departures from the state of the art by incorporating collaborative effects into inference as well as learning processes in a manner that seamlessly interleaving explore-exploit tradeoffs and collaborative steps. We prove regret bounds under various assumptions on the data, which exhibit a crisp dependence on the expected number of clusters over the users, a natural measure of the statistical difficulty of the learning task. Experiments on production and real-world datasets show that CAB offers significantly increased prediction performance against a representative pool of state-of-the-art methods.", "text": "investigate novel cluster-of-bandit algorithm collaborative recommendation tasks implements underlying feedback sharing mechanism estimating neighborhood users context-dependent manner. makes sharp departures state incorporating collaborative effects inference well learning processes manner seamlessly interleaving explore-exploit tradeoffs collaborative steps. prove regret bounds various assumptions data exhibit crisp dependence expected number clusters users natural measure statistical difﬁculty learning task. experiments production real-world datasets show offers significantly increased prediction performance representative pool state-of-the-art methods. many prominent applications bandit algorithms computational advertising web-page content optimization recommendation systems main sources information embedded preference relationships users items served. preference patterns emerging clicks views purchase items typically exploited collaborative ﬁltering techniques. fact common knowledge recommendation systems practice collaborative effects carry information user preferences than demographic metadata content recommendation functionalities incorporated diverse online services requirements often differ vastly. instance movie recommendation system catalog relatively static ratings items accumulate easily deploy collaborative ﬁltering methods dista university insubria italy university cambridge united kingdom kanpur india telefonica research spain amazon berlin germany. correspondence claudio gentile <claudio.gentileuninsubria.it>. matrix factorization restricted boltzmann machines. methods become practically impossible dynamic environments news youtube video recommendation deal continuous stream items recommended along users served. dynamic environments pose dual challenge recommendation methods present items users order optimally gather preference information content available user-item preference information gathered ideally would like exploit content information also importantly collaborative effects observed across users items. users serve many content universe changes rapidly time recommendation services show strong adaptation matching user preferences high algorithmic scalability/responsiveness allow effective on-line deployment. typical scenarios like social networks users engaged technology-mediated interactions inﬂuencing other’s behavior often possible single groups communities made users sharing similar interests and/or behavior. communities static time often clustered around speciﬁc content types given users fact host multiplex interdependent communities depending speciﬁc content items changing dramatically call multiplex interdependent clusterings users induced content universe context-dependent clustering. addition above users change time users targeted service others sign unregister. thus recommendation method readily adapt changing users items. paper introduce analyze algorithm simple ﬂexible algorithm rooted linear contextual bandit framework incorporating collaborative effects traditional approaches contextual bandits ignore adapts match user preferences face constantly evolving content universe targeted users. implements context-dependent clustering intuition computing clusterings bandits allows content item cluster users groups within group users tend react similarly item gets recommended. distinguishes allowing distinct items induce distinct clusterings frequently observed practice clusterings turn suggestive natural context-dependent feedback sharing mechanism across users. thus able exploit collaborative effects contextual bandit settings manner similar neighborhood techniques used batch collaborative ﬁltering. analyze algorithm theoretical experimental standpoint. theoretical side provide regret analysis number users engaged essentially enters regret bound expected number context-dependent clusters users natural measure predictive hardness learning users. also extend result provide sharper bound sparsity assumptions user model vectors. experimental side present comparative evidence production real-world datasets algorithm signiﬁcantly outperforms terms prediction performance state-of-the-art contextual bandit algorithms either leverage clustering context-independent fashion. literature contextual bandit algorithms large surveyed here. sequel brieﬂy mention believe works closely related ours. technique sequentially clustering users bandit setting introduced also inspired earlier references e.g. transfer learning stochastic bandits low-rank bandits. developments relies k-means clustering proposes distributed clustering conﬁdence ball algorithms solving linear bandit problems peer peer networks. related papers implement feedback sharing mechanisms leveraging social information among users include cases users grouped context-dependent. even related work recent paper proposes simultaneously cluster users well items item clusters dictating user clusters. however severe limitation approach content universe ﬁnite known advance addition resulting algorithm somewhat involved. compared previous works approach distinguishes simple ﬂexible well performing feedback propagation among users context-dependent manner. demostrated section offers signiﬁcant performance boosts real-world recommendation settings. consider bandit clustering model standard literature crucial difference allow user behavior similarity represented family clusterings depend speciﬁc feature vector consideration. particular represent users. item represented feature vector seen inducing partition user small number clusters users belonging cluster share similar behavior w.r.t. users lying different clusters signiﬁcantly different behavior. much ﬂexible model allows users agree opinion certain items disagree others something often holds pracimportant note mapping tice. specifying actual partitioning clusters determined common user behavior within cluster unknown learner inferred based user feedback. make things simple assume contextdependent clustering determined linear functions parameterized unknown vector hosted user ||ui|| users cluster w.r.t. different clusters w.r.t. parameter henceforth call assumption γ-gap assumption. note assumptions standard literature user vectors corresponding users context user index denote true neighborhood w.r.t. i.e. hence simply cluster belongs w.r.t. notice henceforth assume instance vectors satisfy ||x|| standard linear bandit settings references therein) unknown user vector determines behavior user precisely upon receiving context vector user reacts delivering payoff value conditionally zero-mean sub-gaussian error variable variance parameter hence conditioned past quantity indeed expected payoff observed user context vector fact sake concreteness assume throughout standard online learning settings learning broken discrete sequence time steps time learner receives user index representing user serve content notice user serve change round round user recur several times. together learner receives context vectors xtct} ||xtk|| encoding content currently available recommendation user learner compelled pick xtkt recommend observes it’s feedback form payoff whose expectation generated exogenous process sense represents data hand. shall section performance algorithm depend properties data. practical goal learner maximize tot= time steps. theoretical standpoint instead interested bounding cumulative regret achieved algorithms. precisely regret learner time extent average payoff best choice hindsight user exceeds average payoff algorithm’s choice i.e. case model items possess informative features always resort noncontextual bandit setting implement approach simply take items apply one-hot encoding assigning i-th item i-th canonical basis vector i-th position zero everywhere else context vector. easy expected payoff given user item simply j-th component vector would obtain regret bound gracefully improves context-dependent clustering structure users becomes stronger. speciﬁcally values taken number clusters would particular interest since expect reap strongest collaborative effects small whereas much done collaborative analysis consequently desirable regret bound would diminishes recall function context vector means expect regret bound also depend properties actual data ct}t section that suitable stochastic assumptions ct}t generated regret analysis essentially replaces dependence total number users smaller quantity expected number clusters users expectation draw context vectors present context-aware bandits upper-conﬁdence boundbased algorithm performing recommendations context-sensitive bandit clustering model. similar previous works maintains vector estimate serve proxy unknown user vector time also maintains standard correlation matrices mit. standard conﬁdence bound function user item time deit suitable funcrived cbit tion however makes sharp departures previous works items recommended well estimates updated. item recommendation time required serve user presenting item items xtct} available time ﬁrst computes item users likely give item similar payoff nitt estimated neighborhood user steps allowing user inherit updates item served another user users indeed agree opinion item sufﬁciently high degree conﬁdence. feedback received user algorithm updates proxies wjt. conﬁdent regarding opinion along direction formally cbitt− proxy user updated however conﬁdent i.e. cbitt− proxy updates performed users it’s estimated neighborhood respect whose opinions conﬁdent too. notice users undergo update motiworth noting extremely ﬂexible handling ﬂuid users context-sensitive user aggregation step repeated every round allows users added dropped seamless manner. strike contrast past approaches bandit aggregation goblin club cofiba involved feedback sharing mechanisms across users implemented based either static network laplacians time-evolving connected components graphs given users. regret analysis regret analysis depends speciﬁc measure hardness data hand observed sequence users {it}t corresponding sequence item sets {ct}t xtct} hardness pairing ct}t level deﬁned words roughly measures number rounds need wait worst case possible users possible ways building matrices rank-one adjustments based data found ct}t correlation matrices eigenvalues lower bounded based hardness deﬁnition following result summarizes main efforts section. full proof given appendix along ancillary results. incorporates collaborative effects lifting notions user proxy conﬁdence bounds users uses simple averagj∈n cbjt lift cbnt wjt. next uses aggre|n| gated conﬁdence bounds cbnitt aggregated proxy vectors wnittt− select item xtkt based upper conﬁdence estimation step. proxy updates classical approaches update user proxies solving regularized least squares problem involving items served previously user payoffs received. however remains fully committed collaborative approach sub-gaussian random vector variance parameter ||x|| full rank smallest eigenvalue also finally sequence {it}t generated uniformly random independent variables. probability least comments order. theorem delivers deterministic regret bound cumulative regret composed terms. ﬁrst term measure hardness data sequence ct}t hand whereas second term usual -style term linear bandit regret analyses however note dependence second term total number users served gets replaced much |nit depends actual size smaller quantity context-dependent clusters served users. shortly pairings ct}t generated favorable manner sampling vectors i.i.d. according unknown distribution instance space hardness measure upper bounded high probability term form similarly second term simple case second term roughly effort learning -many clusters bandits clustering known. thus example ratio quantiﬁes hardness problem insofar clustering concerned. again favorable circumstances relate |nit expected number contextdependent clusters users expectation w.r.t. random draw context vectors. hand making assumptions whatsoever ct}t generated makes hard exploit cluster structure. instance ct}t generated adaptive adversary might cause thereby making bound theorem vacuous. however naive algorithm disregards cluster structure making attempts incorporate collaborative effects running n-many independent linucb-like algorithms coincide algorithms’ recommendations. resulting number retained records around loosely depending different algorithms runs. technique delivers reliable estimates logged policy makes random choices actually simulated random logged policy follows. round retained served current user payoff value also included extra items drawn uniformly random that item occurs item served system times. notice random selection independent available payoff cup. dataset released online advertising competition instances derived session logs search engine soso.com. search session included user query information divided multiple instances described using impressed time certain depth position. instances aggregated user query. took chronological order among instances seeded algorithm ﬁrst instances payoffs binary. resulting dataset distinct users distinct ads. similar tuenti dataset generated random recommendation lists random logged policy. employed one-hot encoding well dataset. number retained records around avazu. dataset released avazu clickrate prediction challenge kaggle. click-through data ordered chronologically nonclicks clicks subsampled according different strategies. before simulated random logged policy recommendation lists size payoffs binary. ﬁnal dataset users items number retained records around again took one-hot encoding items. lastfm delicious. datasets extracted music streaming service last.fm social bookmarking service delicious. lastfm dataset includes users items decorollary cbjt deﬁned lemma γ-gap assumption hold. assume context vectors generated lemma sub-gaussian assumption therein holds finally sequence {it}t generated described lemma then probability least regret satisﬁes sparse user models. conclude pointer additional result sparse linear models contained supplemental line past analyses sparse linear bandits single user s-sparse sense holds replacing least-squares solution step figure solution computed two-stage fully corrective method allows obtain improved regret bound. speciﬁcally replace factor factor multiplying tested production real-world datasets compared standard baselines well state-ofthe-art bandit clustering bandit algorithms. features used items one-hot encoding adopted. tried follow much possible previous experimental settings like described tuenti. tuenti spanish social network website serves site data contains impressions viewed users along variable registers click dataset contains users records/timesteps. adopted encoding scheme items hence items described unit-norm vectors since available payoffs associated licious refers users items preprocessing data followed previous experimental settings datasets used e.g. speciﬁcally after tf-idf representation available items context vectors generated retaining ﬁrst principal components. binary payoffs created follows. lastfm user listened artist least payoff otherwise delicious payoff user bookmarked otherwise. processed datasets make suitable multi-armed bandit algorithms. recommendation lists size generated random ﬁrst selecting index random users padding vectors chosen random available items time step least items payoff current user repeated times datasets. used ﬁrst dataset tune algorithms’ parameters grid search report results remaining results averaged runs. compared number state-of-the bandit clustering-of-bandit methods club sequentially reﬁnes user clusters based conﬁdence ellipsoid balls; seeded graph users initial random erdos-renyi graphs sparsity parameter randomized algorithm repeated times averaged results tunable parameter across values parameter chosen within number clusters dynucb increased according exponential progression starting ending finally parameter simply fact value happen signiﬁcant inﬂuence performance version tested. delivers almost double compared baselines. cab’s performance advantage moderate avazu datasets. expected since exploiting collaborative effects important dataset like tuenti users exposed compared dataset avazu dataset much broader base provides strong indication effectively exploits collaborative effects. general ﬁrst three datasets found offer beneﬁts cold-start region also continues maintain lead throughout. lastfm delicious datasets results report consistent lastfm methods outperformed cab. overall performance bandit methods seems though relatively poor; attributed lastfm dataset generated. users typically little interaction music serving system songs played generated recommender. hence collaborative effects relatively weak compared datasets tuenti. hand delicious dataset best performing strategy seems linucb-multiple deliberately avoids feedback sharing mechanism among users. dataset reﬂects user web-browsing patterns evinced bookmarks. line past experimental evidence dataset seem contain collaborative information hence hardly expect take advantage clustering efforts. shed light figure plotted average distance linear model user corresponding linear models users function datasets user linear models computed taking whole test treating pairing training sample least-squares estimator user conclusion draw visually comparing left right plots figure delicious estimated user models tend signiﬁcantly separated lastfm easily explains effectiveness linucb-multiple. moreover delicious studies shown tags used item features generally chosen users reﬂect interests personal hence expect features diverge even similar websites. hand lastfm tags typically reﬂecting genre song. figure average euclidean distance served user users function datasets lastfm delicious distance computed associating user model vector obtained regularized least-squares solution based available data user results experiments summarized figures results come remaining datasets using data tuning. online advertising datasets tuenti avazu measured performance using click-through rate hence higher curves better. lastfm delicious datasets instead report ratio cumulative regret tested algorithm cumulative regret hence lower better. experimental setting line past work area results reproduce here. moreover data prepared ﬁndings give reliable estimates actual performance actual regret performance tested algorithms. paper proposed novel contextual bandit algorithm personalized recommendation systems. algorithm able effectively incorporate collaborative effects implementing simple context-dependent feedback sharing mechanism. approach greatly relaxes restrictions requirements imposed earlier works offers much higher ﬂexibility handling practical situations like on-the-ﬂy inclusion exclusion users. additional assumptions data generated provided crisp regret analysis depending expected number clusters users natural context-dependent notion difﬁculty learning task. theoretical ﬁndings strengthened sparse model scenario users improved bounds shown. carried extensive experimental comparison number production real-world datasets encouraging results compared available approaches. started test thompson sampling versions competitors observed signiﬁcant statistical difference compared section theoretical standpoint would nice complement upper bound corollary lower bound helping characterize regret complexity problem. experimental standpoint planning sparse bandit version algorithm undergo similar experimental validation presented here. audibert jean yves munos remi szepesvari csaba. exploration-exploitation tradeoff using variance estimates multi-armed bandits. theoretical computer science volume setting nitt small enough along selected direction number weight updates performed round exactly equal size true neighborhood nit. proof theorem consider bound lemma write proof continues lemma setting freedman-style matrix tail bound consequence above following high-conﬁdence estimate holding probability least uniformly martingale difference sequence apply standard concen|nit larger tration inequalities. particular light fact conditional variance conditional mean e.g. conclude that probability least section give details modiﬁed work user models s-sparse i.e. denote supp support vector user assume sake simplicity |si| also make standard assumption non-zero coordinates vectors take vanishing values. formally assume either else |ui| note different users different supports must s∗-sparse. sparse user models arise user item vectors extremely high dimensional features useful encoding preference patterns every user. rather every user chooses features best encode preferences. sparse models also extremely popular resource constrained settings dense models expensive store slow predict with. cases performing least squares regression obtain proxy vectors expected give poor results also requires much larger number trials user effectively estimate prohibitive since users typically interact sparsely recommendation systems. ={it denote number times user served till time also rti×d denote matrix item context vectors served user till time denote vector sub-gaussian error values introduced payoffs user offered past. also denote submatrix contains columns present rest zeroed out. vector notation denote vector coordinates retained rest zeroed out. however abuse notation using context correlation matrix formed using item context vectors correlation matrix. denote using restricted coordinates ameliorate challenges high dimensional settings sparse user models present spcab algorithm adapts extremely high dimensional features. spcab algorithm identical algorithm subspace pursuit would convenient. offer linear rate convergence whenever requisite properties mentioned below satisﬁed. algorithm gives general outline methods general sparse recovery objective function required consider case ridge-regression function induced linear bandit problem mentioned properties would crucial analyzing sparse recovery methods restricted strong convexity restricted strong smoothness outlined below. deﬁnition differentiable function said satisfy restricted strong convexity sparsity level strong convexity constraint following holds s.t. sketch regret bound proof spcab proving counterparts lemmata sparse user model case. lemma require modiﬁcations. first invoke sparse recovery guarantees standard martingale arguments show following result two-stage fully corrective methods applied user proxy estimation problem. theorem suppose objective function satisﬁes parameters given αs+s∗ respectively. suppose algorithm invoked satisﬁes probability least then τ-th iterate algorithm bounding equivalent establishing upper bound equivalent lower bound eigenvalues mit. case need show easy equivalent demonstrating rsc/rss properties objective function fit. corresponding restricted eigenvalue requirement propose counterpart hardness coefﬁcient sphd wherein wish upper bound time possible correlation matrices users s∗-restricted eigenvalues bounded results would require bound sphd fortunately technique gentile using freedman-style inequalities used prove bound lemma still harnessed give using above show following regret bound spcab sparse user model setting. theorem spcab executed bandit clustering setting s∗-sparse user models satisfying requisite properties mentioned above probability least regret spcab satisﬁes notice drastic reduction dependence regret bound. spcab enjoys regret bound depends weakly ambient dimension problem setting dependence also notice bound improved item contexts dimensional well. suppose item contexts sampled distribution support r-dimensional space. then easily shown spcab without modiﬁcations offers following regret bound.", "year": 2016}