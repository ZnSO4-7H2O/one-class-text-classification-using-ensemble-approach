{"title": "Scalable and Sustainable Deep Learning via Randomized Hashing", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "Current deep learning architectures are growing larger in order to learn from complex datasets. These architectures require giant matrix multiplication operations to train millions of parameters. Conversely, there is another growing trend to bring deep learning to low-power, embedded devices. The matrix operations, associated with both training and testing of deep networks, are very expensive from a computational and energy standpoint. We present a novel hashing based technique to drastically reduce the amount of computation needed to train and test deep networks. Our approach combines recent ideas from adaptive dropouts and randomized hashing for maximum inner product search to select the nodes with the highest activation efficiently. Our new algorithm for deep learning reduces the overall computational cost of forward and back-propagation by operating on significantly fewer (sparse) nodes. As a consequence, our algorithm uses only 5% of the total multiplications, while keeping on average within 1% of the accuracy of the original model. A unique property of the proposed hashing based back-propagation is that the updates are always sparse. Due to the sparse gradient updates, our algorithm is ideally suited for asynchronous and parallel training leading to near linear speedup with increasing number of cores. We demonstrate the scalability and sustainability (energy efficiency) of our proposed algorithm via rigorous experimental evaluations on several real datasets.", "text": "deep residual network ilsvrc competition error rate layers billion flops. handle large neural networks researchers usually train high-performance graphics cards large computer clusters. basic building block neural network neuron. neuron activates speciﬁc feature appears input. magnitude neuron’s activation measure neuron’s conﬁdence presence absence feature. ﬁrst layer deep network neurons detect simple edges image. successive layer learns complex features. example neurons parts face ﬁrst eyes ears mouth before distinguishing diﬀerent faces. classiﬁcation tasks ﬁnal layer classic linear classiﬁer softmax svm. multiple layers combine form complex non-linear function capable representing arbitrary function adding neurons network layers increases expressive power. enhanced expressive power made massive-sized deep networks common practice large scale machine learning systems shown impressive boost benchmarks. growing size complexity networks efﬁcient algorithms training massive deep networks distributed parallel environment currently sought problem academia commercial industry. example google used -billion parameter neural network took three days train -node cluster totaling cores. instantiation network spanned servers. distributed computing environments parameters giant deep networks required split across multiple nodes. however setup requires costly communication synchronization parameter server processing nodes order transfer gradient parameter updates. sequential dense nature gradient updates prohibits eﬃcient splitting neural network parameters across computer nodes. clear avoid costly synchronization without resorting ad-hoc breaking network. ad-hoc breaking deep networks well understood likely hurt performance. synchronization major hurdles scalability. asynchronous training ideal solution sensitive conﬂicting overlapping parameter updates leads poor convergence. abstract current deep learning architectures growing larger order learn complex datasets. architectures require giant matrix multiplication operations train millions parameters. conversely another growing trend bring deep learning low-power embedded devices. matrix operations associated training testing deep networks expensive computational energy standpoint. present novel hashing based technique drastically reduce amount computation needed train test deep networks. approach combines recent ideas adaptive dropouts randomized hashing maximum inner product search select nodes highest activation eﬃciently. algorithm deep learning reduces overall computational cost forward back-propagation operating signiﬁcantly fewer nodes. consequence algorithm uses total multiplications keeping average within accuracy original model. unique property proposed hashing based back-propagation updates always sparse. sparse gradient updates algorithm ideally suited asynchronous parallel training leading near linear speedup increasing number cores. demonstrate scalability sustainability proposed algorithm rigorous experimental evaluations several real datasets. deep learning revolutionizing big-data applications after responsible groundbreaking improvements image classiﬁcation speech recognition recent upsurge data much faster rate computing capabilities neural networks growing larger process information eﬀectively. state-ofthe-art convolutional neural networks contained layers. afterward successive year brought deeper architectures greater accuracy. last year microsoft’s mobile phones low-power devices. example recent work aimed leveraging vast data mobile devices. work users train neural networks local data periodically transmit models central server. approach preserves privacy user’s personal data still allows central server’s model learn eﬀectively. work dependent training neural networks locally. back-propagation popular algorithm training deep networks. iteration back-propagation algorithm composed giant matrix multiplications. matrices large especially massive networks millions nodes hidden layer common industrial practice. large matrix multiplications parallelizable gpus energy-eﬃcient. users require phones tablets long battery life. reducing computational costs neural networks directly translates longer battery life critical issue mobile industry. current challenges deep learning illustrate great demand algorithms reduce amount computation energy usage. reduce bottleneck matrix multiplications ﬂurry works around reducing amount computations associated them. revolve around exploiting low-rank matrices precision updates. review techniques details section however updates techniques hard parallelize making unsuitable distributed large scale applications. contrary proposal capitalizes sparsity activations reduce computations. best knowledge ﬁrst proposal exploits sparsity reduce amount computation associated deep networks. show approach admits asynchronous parallel updates leading perfect scaling increasing parallelism. recent machine learning research focused techniques dealing famous problem over-ﬁtting deep networks. notable line work improved accuracy neural networks updating neurons highest activations. adaptive dropout sampled neurons proportion aﬃne transformation neuron’s activation. winner-take-all approach kept top-k% neurons calculating hard threshold mini-batch statistics. found selective choice nodes sparse updates provide natural regularization however approaches rely ineﬃcient brute-force techniques best neurons. thus techniques equally expensive standard back-propagation method leading computational savings. propose hashing-based indexing approach train test neural networks capitalizes rich theory randomized sub-linear algorithms database literature perform adaptive dropouts eﬃciently requiring signiﬁcantly less computation memory overhead. furthermore hashing algorithms embarrassingly parallel easily distributed small communications perfect large-scale distributed deep networks. idea index neurons hash table using locality sensitive hashing. hash tables neurons allow select neurons highest activations without computing activations sub-linear time leading signiﬁcant computational savings. moreover show since approach results sparse active neurons randomly gradient updates unlikely overwrite. updates ideal asynchronous parallel gradient updates. known asynchronous stochastic gradient descent converge number simultaneous parameter updates small. heavily leverage sparsity unique proposal. several deep learning benchmarks show approach outperforms standard algorithms including vanilla dropout high sparsity levels matches performance adaptive dropout winner-take-all needing less computation contributions present scalable sustainable algorithm training testing fully-connected neural networks. idea capitalized recent successful technique adaptive dropouts combined smart data structure based recently found locality sensitive hashing maximum inner product search show signiﬁcant reductions computational requirement training deep networks without signiﬁcant loss accuracy particular method achieves performance state-of-the-art regularization methods dropout adaptive dropout winner-take-all using neurons standard neural network. scheme naturally leads sparse gradient updates. sparse updates ideally suited massively parallelizable asynchronous training demonstrate sparsity opens room truly asynchronous training without compromise accuracy. result obtain near-linear speedup increasing number processors. several recent advances aimed improving performance neural networks. reduced number ﬂoating point multiplications mapping network’s weights stochastically forward propagation performing quantized back-propagation replaces ﬂoating-point multiplications simple bit-shifts. reducing precision orthogonal approach easily integrated approaches. uses structured matrix transformations low-rank matrices reduce number parameters fullyconnected layers neural network. low-rank constraint leads smaller memory footprint. however approximation well suited asynchronous parallel training limiting scalability. instead random sparse activations leveraging database advances approximate query processing easily parallelized. brieﬂy review dropouts variants popular sparsity promoting techniques relying sparse activations. although randomized sparse activations found favorable better generalization deep networks best knowledge sparsity adequately exploited make deep networks computationally cheap parallelizable. provide ﬁrst evidence. dropout primarily regularization technique addresses issue over-ﬁtting randomly dropping half nodes hidden layer training network. nodes independently sampled every stochastic gradient descent epoch reinterpret dropout technique reducing number multiplications forward backward propagation phases ignoring nodes randomly network computing feed-forward pass. known network’s performance becomes worse many nodes dropped network. usually nodes network dropped training network. test time network takes average thinned networks form prediction input data uses full computations. adaptive dropout evolution dropout technique adaptively chooses nodes based activations. methodology samples nodes network sampling done probability proportional node activations dependent current input. adaptive dropouts demonstrate better performance vanilla dropout notable feature adaptive dropouts possible drop signiﬁcantly nodes compared dropouts still retaining superior performance. winner-take-all extreme form adaptive dropouts uses mini-batch statistics enforce sparsity constraint. technique largest non-zero activations used forward backward phases training. approach requires computing forward propagation step selecting nodes hard threshold. unfortunately techniques require full computation activations selectively sample nodes. therefore intended better generalization reducing computations. approach uses insight selecting sparse hidden nodes highest activations reformulated dynamic approximate query processing problem solve eﬃciently using locality sensitive hashing. diﬀerentiating factor adaptive dropout winner-take-all approach sub-linear time randomized hashing determine active nodes instead computing inner product node individually. also another orthogonal line work uses hashing reduce memory. introduced type deep learning architecture called hashed nets. objective decrease number parameters given neural network using universal random hash function node weights. network connections hash value restricted values. architecture virtual appearance regular network maintaining small subset real weights. point hashed nets complementary low-rank assumption convenient reducing complexity general matrix operations. however low-rank dense updates promote sparsity friendly distributed computing. principle holds deep networks. illustrate simple example. consider layer ﬁrst network shown figure insight weight matrix rmxn hidden layer low-rank structure rank representation rmxr rrxn. low-rank structure improves storage requirements matrix-multiplication time shown figure equivalent representation network using intermediate hidden layer contains nodes uses identity activation function. weight matrices hidden layers second network matrix decomposition uses structured matrix transformations low-rank matrices reduce number parameters fully-connected layers neural network. network structure equivalent structure matrices require dense gradient update ideally suited data parallelism example need sequential gradient descent updates. work move away low-rank assumption instead make randomness sparsity reduce computations. later show sparsity approach well suited asynchronous stochastic gradient descent leading near-linear scaling. noted methodology diﬀerent notions making networks sparse thresholding node weights permanently approach makes every node weight picks selectively diﬀerent inputs. contrary permanent sparsiﬁcation static weights used every input rest discarded. neural networks built layers neurons. combination multiple hidden layers creates complex non-linear function capable representing arbitrary function forward propagation equation output previous layer weight matrix layer bias term layer activation function. common non-linear activation functions sigmoid tanh relu. neural networks trained using stochastic gradient descent η∇j. gradient neural network calculated layer-by-layer using back-propagation algorithm. momentum technique calculates parameter update weighted previous momentum term current gradient η∇j. reduces training time neural networks mitigating neighbor search. high-level idea similar items bucket high probability. hash function maps input data vector integer collision occurs hash values data vectors equal probability collision hash function proportional similarity distance data vectors sim. essentially similar items likely collide hash ﬁngerprint vice versa. sub-linear search algorithm able answer approximate nearest-neighbor queries sub-linear time idea create hash tables given collection interested querying nearest-neighbor items hash tables generated using locality sensitive hash family. assume access appropriate locality sensitive hash family similarity interest. classical parameterized algorithm generate diﬀerent hash functions given ...; hkj]. diﬀerent evaluations appropriate locality sensitive hash function. hash functions formed concatenating sampled hash values pre-processing phase construct hash tables data storing elements location hash-table store pointers vector hash tables storing data vectors memory ineﬃcient. query phase given query search nearest-neighbors. report union points buckets union hash tables. note scan elements probe diﬀerent buckets hash table. shows family given similarity measure appropriate choice algorithm provably sub-linear. practice nearconstant time query requires bucket probes. hashing inner products recently shown allowing asymmetric hash functions algorithm converted sublinear time algorithm maximum inner product search locality sensitive hashing self-similarity closest nearest-neighbor query however self-similarity necessarily correspond highest inner product ||x|| therefore asymmetric transformation necessary maximum inner product search standard nearest-neighbor search work heavily leverages algorithms. purpose paper assume create eﬃcient data structure queried large inner products. details. figure illustration low-rank assumption neural networks naturally leads fewer parameters network contains layers neurons respectively. weighted connections layers characterized weight matrix rmxn constrained rank rmxr rrxn. equivalent network contains three layers neurons. layers represented matrix rmxr bottom layers characterized matrix rrxn. intermediate layer uses identity activation function output network equals network layer’s output non-linear activation function. stochastic nature sgd. adaptive gradient descent variant adapts learning rate parameter. global learning rate normalized squared gradients. favors larger learning rates infrequent small parameter updates. adaptive dropout adaptive dropout recently proposed dropout technique selectively samples nodes probability proportional monotonic function activations. achieve this authors chose bernoulli distribution. idea given input layer generate bernoulli random variable every node layer bernoulli probability monotonic function activation. then binary variable sampled distribution used determine associated neuron’s activation kept zero. adaptive selectivity allows fewer neurons activated hidden layer improve neural network performance standard dropout winner-take-all approach extreme case adaptive dropout takes activations hidden layer also performs well. locality-sensitive hashing multi-probe common complaint classical algorithm requires signiﬁcant number hash tables. large increases hashing time memory cost. simple solution probe multiple \"close-by\" buckets every hash table rather probing bucket thus given query addition probing hash table also generate several addresses probe slightly perturbing values simple idea signiﬁcantly reduces number tables needed work hash tables. details. winner-take-all technique shows consider nodes large activations given input ignore rest computing feedforward pass. furthermore back-propagation updates performed chosen weights. denote nodes signiﬁcant activations. denote total number nodes neural network gradient update winner-take-all needs perform ﬁrst work compute followed updating weights. seems quite wasteful. particular given input ﬁnding active search problem solved eﬃciently using smart data structures. furthermore data structure dynamic eﬃcient gradient updates also eﬃcient. node weight input activation monotonic function inner product thus given selecting nodes large activations equivalent searching collection weight vectors large inner products equivalently query processing perspective treat input query search problem selecting nodes done eﬃciently sub-linear time number nodes using recent advances maximum inner product search proposal create hash tables indexes generated asymmetric locality sensitive hash functions tailored inner products. hash tables given query input eﬃciently approximate active implementation challenge. also update nodes gradient update. perform updates instead save signiﬁcant computation. thus need data structure updates eﬃcient well. years research data structures many choices make update eﬃcient. describe system details section equivalence adaptive dropouts turns using randomized asymmetric locality sensitive hashing ﬁnding nodes large inner products formally statistical sense equivalent adaptive dropouts non-trivial sampling distribution. proportion nodes activation suﬃcient. argue bernoulli sub-optimal choice. another non-intuitive eﬃcient choice. choice comes theory locality-sensitive hashing primarily considered black technique fast sub-linear search. core idea comes observation that given search query parametrized algorithm inherently samples sub-linear time points datasets probability proportional collision probability monotonic function similarity query retrieved point. details. expression holds controls sparsity buckets. reasonable choice always make buckets arbitrarily sparse. even buckets heavy always simple random sampling buckets adding constant factor probability keeping monotonicity intact. theorem hash-based sampling given input layer parametrized hashing algorithm selects node associated weight vector probability proportional collision probability associated locality sensitive hash function. function monotonic proof. probability ﬁnding node hash table probability missing node independent hash tables choose sample buckets randomly another constant factor eﬀective collision probability becomes recent advances hashing inner products make monotonic function inner products translating deep network notation given activation vector previous layer query layer’s weights search space sample probability proportional monotonic function node activation. thus naturally integrate adaptive dropouts hashing. overall simply choosing speciﬁc form adaptive dropouts leads eﬃcient sampling time sub-linear number neurons exists eﬃcient family sampling distributions adaptive dropouts sampling updating cost sublinear number nodes. particular construct eﬃcient sampling scheme nodes given layer probability choosing node greater probability choosing node current activation node higher proof. since asymmetric functions collision probability monotonic function inner product also montonic function neuron’s activation. activation functions including sigmoid tanh relu monotonic functions inner products. composing monotonic functions retains monotonicity. therefore corollary follows monotonic respect addition monotonicity goes direction inverse function exists also monotonic. layer hash function layer hash tables layer active randomly initialize parameters layer constructhashfunction constructhashtable stopping criteria training epoch detail hash function hidden layer composed randomized hash functions. sign asymmetrically transformed random projection details) generate bits data vector. bits stored together eﬃciently integer forming ﬁngerprint data vector. create hash table buckets keep nonempty buckets minimize memory footprint bucket stores pointers nodes whose ﬁngerprints match bucket’s instead node itself. ﬁgure showed hash table likely miss valuable nodes practice. implementation generate hash tables hidden layer hash table independent random projections. ﬁnal active hash tables union buckets selected hash table. layer hash tables. eﬀectively tunable parameters bits tables control size quality active sets. bits increase precision ﬁngerprint meaning nodes bucket likely generate higher activation values given input. tables increase probability ﬁnding useful nodes missed randomness hash functions. efﬁcient query updates algorithm critically depends eﬃciency query update procedure. hash table eﬃcient data structures diﬃcult challenge. querying single hash table constant time operation bucket size small. bucket size controlled sub-sampling bucket. always possibility crowded buckets randomness many near-duplicates data. crowded buckets informative safely ignored sub-sampled. figure visual representation neural network using randomized hashing build hash tables hashing weights hidden layer hash layer’s input using layer’s randomized hash function query layer’s hash table active perform forward back-propagation neurons active set. solid-colored neurons hidden layer active neurons. update weights hash tables rehashing updated weights hash locations. argued approach simple. randomized hash functions build hash tables nodes hidden layer. sample nodes hash table probability proportional node’s activation sub-linear time. perform forward back propagation active nodes retrieved hash tables. later update hash tables reorganize modiﬁed weights. figure illustrates example neural network hidden layers input nodes output nodes. hash tables built hidden layer weighted connections node hashed place node corresponding bucket. creating hash tables store initial parameters one-time operation requires cost linear number parameters. forward propagation pass input hidden layer hashed hash function used build hidden layer’s hash table. input’s ﬁngerprint used collect active nodes hash table. hash table contains pointers nodes hidden layer. then forward propagation performed nodes active rest hidden layer’s nodes part active ignored automatically switched without even touching them. back propagation pass active reused determine gradient update parameters. rehash nodes hidden layer account changes network training. never need store weights hash tables. instead store references weight vectors makes hash tables light entity. further reduce number diﬀerent hash tables required using multi-probe updating weight vector associated node require changing location hash table updates lead change hash value. hash table data structure update issue buckets sparse. updating weight requires insertion deletion respective buckets. plenty choices eﬃcient insert delete data structure buckets. theory even buckets sparse red-black-tree ensure insertion deletion cost logarithmic size bucket. however using simple arrays buckets sparse preferable easy parallelize. arrays insertion deletion size buckets. controlling size easily achieved sub-sampling bucket. since create independent hash tables even reasonably large process quite robust cheap approximations. reduce size using multi-probing multi-probe binary hash function quite straightforward. randomly bits k-bit hash generate addresses. plenty choices make hashing signiﬁcantly faster cheap re-ranking authors show around reduction computations image search incorporating diﬀerent algorithmic systems choices. overall cost every layer every stochastic gradient descent update compute hashes input probe around buckets take union. experiments i.e. hash computations only. many techniques reduce hashing cost probe around buckets hash tables obtaining active nodes leading union buckets total. process gives active nodes usually signiﬁcantly smaller compared number nodes update weights along hash table. overall cost order number nodes experiments show node layer update around nodes only. bottleneck cost calculations activations nodes every node around ﬂoating point multiplications each. beneﬁts even signiﬁcant larger networks. bonus sparse updates parallelized mentioned need update weights associated nodes active sparse unlikely multiple updates overwrite weights. intuitively assuming enough randomness data vector small active chosen randomly among nodes. unlikely multiple active sets randomly selected data vectors signiﬁcant overlaps. small overlaps imply fewer conﬂicts updating. fewer conﬂicts updating ideal ground updates parallelized without overhead. fact theoretically experimentally random sparse updates parallelized without compromising convergence parallel updates pressing challenges large-scale deep learning systems vanilla deep networks sequential parallel updates lead poor convergence signiﬁcant overwrites. experimental results section support known phenomena. exploiting unique property show near linear scaling algorithm increasing processors without hurting convergence. approximate hashing compare expensive exact approaches adaptive dropouts winner-takes-all terms accuracy? particular hashing working intended? test neural network implementation used four publicly available datasets mnistm norb convex rectangles statistics datasets summarized table mnistm convex rectangles datasets contain images forming -dimensional feature vectors. mnistm task classify handwritten digit image correctly. derived applying random deformations translations mnist dataset. convex dataset objective identify single convex region exists image. goal rectangles dataset discriminate tall wide rectangles overlaid black white background image. norb dataset contains images toys belonging categories various lighting conditions camera angles. data point stereo image pair. resize image concatenate image pairs together form -dimensional feature vector. sustainability experiments approach techniques -core intel machine memory. approach uses stochastic gradient descent momentum adagrad since approach uniquely selects active nodes hidden layer focused cpu-based approach simplify combining randomized hashing neural networks. relu activation function used methods. learning rate approach using standard grid search ranged parameters randomized hash tables bits tables multi-probe creating series probes hash tables. stop early samples enough nodes even exhausting buckets. since evaluate levels selection order increase percentage nodes retrieved increase number probes buckets. experiments ﬁxed threshold number active nodes selected hash tables guarantee amount computation within certain level. figures show accuracy method neural networks hidden layers percentage active nodes ranging standard neural network baseline experiments marked dashed black line. hidden layer contains nodes. x-axis represents average percentage active nodes epoch selected technique. approach performs forward back propagation steps nodes selected hidden layer. baseline techniques except dropout perform forward propagation step node ﬁrst compute activations setting node activations zero based corresponding algorithm. thus proposal requires lesser number multiplications compared standard neural network training procedure. method gives best overall accuracy fewest number active nodes. fact approximate method even slightly better adaptive dropouts surprising long known small amount random noise leads better generalization. examples number multiplications constant multiple percentage active nodes hidden layer. parameters alpha beta determine many nodes kept active. used alpha beta experiments. model diverged number active nodes dropped below data adaptive dropout lowering computational cost running neural networks running fewer operations reduces energy consumption heat produced processor. however large neural networks provide better accuracy arbitrarily reducing amount computation hurts performance. experiments show method performs well computation levels providing best worlds high performance processor computation. approach ideal mobile phones thermal power design watts reducing processor’s load directly translates longer battery life. scalability show experiments demonstrate scalability approach large-scale distributed computing environments. speciﬁcally testing approach maintains accuracy improves training time increase number processors. asynchronous stochastic gradient descent momentum adagrad implementation runs model similar initializations multiple training examples concurrently. gradient applied without synchronization locks maximize performance. experiments intel xeon machine cores memory. relu activation used models learning rate ranged figure classiﬁcation accuracy diﬀerent levels active nodes networks mnist norb convex rectangles datasets. standard neural network baseline accuracy. clearly adaptive sampling hashing signiﬁcantly eﬀective random sampling panels hidden layers. bottom panels hidden layers figure shows method performs asynchronous stochastic gradient descent using neurons full-sized neural network. neural network three hidden layers hidden layer contains neurons. x-axis represents number epochs completed y-axis shows test accuracy given epoch. compare model converges multiple processors working concurrently. since asgd implementation locks depends sparsity gradient ensure model converges performs well experiments method converges similar rate obtains accuracy regardless number processors running asgd. figure illustrates method scales multiple processors. inherent sparsity randomized hashing approach reduces number simultaneous updates allows asynchronous models without performance penalty. show corresponding drop wall-clock computation time epoch adding processors. achieve roughly speed running asgd threads. figure compares performance approach standard neural network running asgd -cores. used standard network mini-batch size clearly out-perform standard network experimental datasets. however since large number processors applying gradients parameters gradients constantly overridden preventing asgd converging optimal local minimum. approach produces spare gradient reduces conﬂicts diﬀerent processors gradient updates quite sparse running multiple asgd updates parallel aﬀect convergence rate hashing based approach. even running cores parallel convergence indistinguishable sequential update four datasets. instead vanilla parallel convergence eﬀected. convergence general slower compared sparse lsh. slow convergence dense updates leads overwrites. parallelizing dense updates aﬀects four datasets diﬀerently. convex dataset convergence poor. expected obtain near-perfect decrease wall clock times increasing number processors lsh-%. note many overwrites atomic overwrites necessary create additional overhead hurt parallelism. thus near-perfect scalability also indicates fewer gradient overwrites. largest dataset mnistm running time epoch -core implementation seconds. -core implementation runs seconds. since convergence aﬀected amounts speedup training process processors. figure classiﬁcation accuracy diﬀerent levels active nodes networks mnist norb convex rectangles datasets. standard neural network baseline accuracy. perform amount computation standard neural network. techniques select nodes high activations full computations achieve better accuracy. compare approach determine whether randomized algorithm achieves comparable performance reducing total amount computation. data adaptive dropout computation levels models diverged number active nodes dropped panels hidden layers. bottom panels hidden layers datasets fewer training examples less parallel work large number processor especially working cores. behaviors mnistm around million training examples. demonstrated concrete evidence showing deep networks scaled-up. believe integration sparsity approximate query processing already eﬃcient diﬀerent systems future large-scale machine learning. machine learning huge parameter space becoming common phenomenon. remains promising algorithm optimization eﬀectiveness simplicity. updates single data point unlikely change entire parameter space signiﬁcantly. iteration expected change small parameters depending current sample. sparse update occurs enough information data point. however identifying small active parameters search problem typically requires computations order parameters. exploit rich literature approximate query processing active parameters eﬃciently. course approximate active contains small amount random noise often good generalization. sparsity randomness enhance data parallelism sparse random updates unlikely overwrite other. approximate query processing already sits decades research systems database community makes algorithm scalable variety distributed systems. thus forget systems challenges reformulating machine learning problem approximate query processing problem levering ideas implementation rich literature. randomized hashing approach designed minimize amount computation necessary train neural network eﬀectively. training neural networks requires signiﬁcant amount computational resources time limiting scalability neural networks sustainability embedded low-powered devices. results show approach performs better methods using nodes executed standard neural network. implication neural networks using approach mobile devices longer battery life maintaining performance. also show inherent sparsity approach scales near-linearly asynchronous stochastic gradient descent running multiple processors. future work optimize approach diﬀerent mobile processors. many choices choose including nvidia tegra qualcomm snapdragon platforms. recently movidius released plans specialized deep learning processor called myriad processor combines best aspects gpus dsps cpus produce gflops using power. plan leverage diﬀerent platform architectures achieve greater eﬃciency randomized hashing approach. figure convergence randomized hashing approach several training epochs using asynchronous stochastic gradient threads. used network mnist norb convex rectangles datasets. standard network’s computation performed experiment. figure performance comparison randomized hashing approach standard network using asynchronous stochastic gradient descent intel xeon machine -cores. used networks mnist norb convex rectangles networks initialized settings experiment. figure wall-clock epoch approach gained using asynchronous stochastic gradient descent. used network mnist norb convex rectangles smaller performance gains convex rectangles datasets enough training examples processors eﬀectively. standard network’s computation performed experiment. mohamed jaitly senior vanhoucke nguyen sainath deep neural networks acoustic modeling speech recognition shared views four research groups. signal processing magazine ieee methods generic object recognition invariance pose lighting. computer vision pattern recognition cvpr proceedings ieee computer society conference volume pages ii–. ieee loosli canu bottou. training invariant support vector machines using selective sampling. bottou chapelle decoste weston editors large scale kernel machines pages josephson wang charikar multi-probe eﬃcient indexing high-dimensional similarity search. proceedings international conference large data bases pages vldb endowment search locality sensitive hashing using ternary content addressable memories. proceedings sigmod international conference management data pages shrivastava asymmetric minwise hashing indexing binary inner products containment. proceedings international conference world wide pages international world wide conferences steering committee", "year": 2016}