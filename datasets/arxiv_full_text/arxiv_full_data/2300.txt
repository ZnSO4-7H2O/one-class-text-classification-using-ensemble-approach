{"title": "Pooling homogeneous ensembles to build heterogeneous ensembles", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In ensemble methods, the outputs of a collection of diverse classifiers are combined in the expectation that the global prediction be more accurate than the individual ones. Heterogeneous ensembles consist of predictors of different types, which are likely to have different biases. If these biases are complementary, the combination of their decisions is beneficial. In this work, a family of heterogeneous ensembles is built by pooling classifiers from M homogeneous ensembles of different types of size T. Depending on the fraction of base classifiers of each type, a particular heterogeneous combination in this family is represented by a point in a regular simplex in M dimensions. The M vertices of this simplex represent the different homogeneous ensembles. A displacement away from one of these vertices effects a smooth transformation of the corresponding homogeneous ensemble into a heterogeneous one. The optimal composition of such heterogeneous ensemble can be determined using cross-validation or, if bootstrap samples are used to build the individual classifiers, out-of-bag data. An empirical analysis of such combinations of bootstraped ensembles composed of neural networks, SVMs, and random trees (i.e. from a standard random forest) illustrates the gains that can be achieved by this heterogeneous ensemble creation method.", "text": "ensemble methods outputs collection diverse classiﬁers combined expectation global prediction accurate individual ones. heterogeneous ensembles consist predictors different types likely different biases. biases complementary combination decisions beneﬁcial. work family heterogeneous ensembles built pooling classiﬁers homogeneous ensembles different types size depending fraction base classiﬁers type particular heterogeneous combination family represented point regular simplex dimensions. vertices simplex represent different homogeneous ensembles. displacement away vertices effects smooth transformation corresponding homogeneous ensemble heterogeneous one. optimal composition heterogeneous ensemble determined using cross-validation bootstrap samples used build individual classiﬁers out-of-bag data. empirical analysis combinations bootstraped ensembles composed neural networks svms random trees illustrates gains achieved heterogeneous ensemble creation method. introduction building effective classiﬁer speciﬁc problem difﬁcult task. successful variety aspects need taken account structure data information used prediction number labeled examples available induction noise level among others. another crucial choice type predictor used. strategies implemented different classiﬁers diverse. instance decision trees adopt divideand-conquer approach original prediction task recursively divided partitioning attribute space disjoint regions. within regions prediction problem simpler original. neural network provides global sub-symbolic representation decision problem terms synaptic weights. anillustration strategy adopted kernel methods suppor vector machines svms original problem embedded extended feature space. extended space discrimination problem solved ﬁnding minimal margin hyperplane separates classes except possibly instances. practice often ﬁnds combining outputs individual classiﬁers often leads accurate predictions. whence popularity ensemble methods necessary condition obtain improvements ensemble members diverse. additions individual predictors complementary sense tends make errors different test instances. homogeneous ensembles composed classiﬁers type. ensembles composed classiﬁers different types called heterogeneous. strategies generate diversity among base classiﬁers different homogeneous heterogeneous ensembles. homogeneous ensembles main difﬁculty generate diversity even learning algorithm used. bootstrap techniques randomized steps base learning algorithm noise injection class labels adaptive emphasis protocols techniques exploited mainly context homogeneous ensembles also used achieve diversity heterogeneous ensembles however since different learning algorithms used generate base learners heterogeneous ensembles intrinsically diverse. case main difﬁculty resides determining optimal combine predictions different models ensemble. broadly speaking methods build heterogeneous ensemble grouped categories. ﬁrst family methods ﬁxed number different models combined. second strategy build collection models different parametrizations select best static heterogeneous ensemble proposed. study different base classiﬁers combined support vector machine multilayer perceptron logistic regression nearest neighbors decision tree. parameters architecture individual classiﬁers determined using -fold cross-validation. proposed approach shows good results speciﬁc application lithofacies classiﬁcation. combination several carefully optimized strong learners deep neural networks adaboosts gaussian processes proposed. study shows good performance proposed combination several image classiﬁcation tasks respect constituents. however problem determining number classiﬁers type need used solved fully satisfactory manner. furthermore optimal composition ensemble problem-dependent. possible overcome difﬁculty create library classiﬁers select subset ﬁnal ensemble instance library different methods trained wide range different parametrizations build. models included library individual classiﬁers ensembles. ensemble methods used include boosted trees using different decision tree algorithms ensemble size bagged trees using different base decision trees. addition individual trees bagged ensembles also added library. individual classiﬁers included svms trained different parameters multilayer perceptrons etc. library models iterative greedy selection algorithm applied build ﬁnal ensemble. procedure starts empty ensemble. then iteration model maximizes performance measure included ensemble models library aggregated. finally ensemble best performance validation selected ﬁnal combination. tsoumakas made several interesting contribution line research instance authors propose greedy selection method library composed classiﬁers neural networks nearest neighbor classiﬁers svms decision trees. type classiﬁer parameter grid deﬁned single model trained node grid. proposal ensemble grown incrementally selecting library classiﬁer time. step selection made terms individual accuracy complementarity rest classiﬁers ensemble. problems investigated heterogeneous ensembles found accurate constituents. genetic algorithm proposed select optimum structure heterogeneous ensemble different base models. selection techniques also known ensemble pruning also extensively applied homogeneous ensembles work propose analyze heterogeneous ensembles individual classiﬁers selected homogeneous ensembles. goal build family heterogeneous ensembles smoothly transformed other. family heterogeneous ensembles size built pooling different fractions base classiﬁers homogeneous ensembles different types. depending proportion classiﬁers type particular heterogeneous combination created. family heterogeneous ensembles represented regular simplex dimensions. vertices simplex represent different homogeneous ensembles. optimal fraction type classiﬁers ﬁnal ensemble found performing search simplex. paper organized follows section design process build optimal heterogeneous ensembles pooling homogeneous ensembles described; section presents comprehensive empirical evaluation proposed methodology comparison corresponding homogeneous ensembles individual classiﬁers. finally conclusions present work summarized. study analyze heterogeneous ensembles pooling individual classiﬁers different homogeneous ensembles. this ﬁrst train ensembles size composed different types base classiﬁers. heterogeneous ensemble size created pooling classiﬁers ensembles number base classiﬁers pooled homogeneous ensemble optimum percentage type base classiﬁer obtained cross-validation out-of-bag error grid search space given neous ensembles built manner number rather large even small values instance different heterogeneous ensembles built. order reduce search space ensembles evaluated using intervals base classiﬁers type. instance followings conﬁgurations generated ensembles could tested etc. reduces search nally ensemble composition minimum validation error determined optimal ensemble. case ensemble conﬁguration minimum validation error average ensemble compositions minima validation error selected optimal heterogeneous ensemble. study used three homogeneous ensembles random forests ensembles support vector machines multilayer perceptrons. base classiﬁers ensembles created using random samples training allow fast validation optimum heterogeneous ensemble means out-of-bag validation order generate ensembles svms following randomized procedure used. first sets partially optimized parameters svms obtained. details sets partially optimized parameters obtained given below. then ensemble built batches svms. batch uses different parameters individual svms trained different random bootstrap sample without replacement size original training set. variability among svms increased. using subbagging advantage respect using standard bootstrap samples base models trained faster. speedup approximately times considering near quadratic training times svms. addition performance sampling strategies bootstrapping subbagging demonstrated equivalent obtain sets partially optimized parameters ﬁrst deﬁne parameter grid. next subbagging sample generated. svms trained combination parameters validated left-out set. finally parameter lower error kept building ensemble. process repeated times obtain sets parameters. procedure used generate ensembles mlps. training time complexity ensemble depends size parameter grid sampling rate complexity base classiﬁer. notwithstanding spite creating ensemble svms procedure faster train training single grid search cross-validation common training next section show validity procedure generate homogeneous ensembles svms mlps also procedure obtain heterogeneous ensembles them. experimental results section present empirical analysis heterogeneous ensembles combination homogeneous base classiﬁers. furthermore validate procedure obtain ensembles partial optimization training parameters. carried analysis datasets repository tested datasets except synthetic problems training test sets generated using random stratiﬁed sampling sizes original sets respectively. synthetic classiﬁcation problems ringnorm threenorm twonorm examples sampled random training testing using independent realizations. results reported averages executions except breast chess german tic-tac-toe ozone spambase averages executions computational limitations. three homogeneous ensembles size trained. speciﬁcally ensembles used standard random forest partially optimized ensemble support vector machines multi layer perceptrons. used rsnns randomforest packages creating svms mlps respectively. setting possible conﬁgurations heterogeneous endataset e-svm .±.* australian boston breast bupa chess colic german .±.* heart hepatitis .±.* ionosphere ozone .±.* parkinsons .±.* pima .±.* ringnorm spambase .±.* sonar .±.* threenorm .±.* tic-tac-toe .±.* twonorm semble reduce computational burden identiﬁcation optimum combination base classiﬁers evaluated heterogeneous ensembles intervals base learners reduces optimization evaluations. addition since base classiﬁers three analyzed ensembles generated using random subsamples training optimum heterogeneous conﬁguration obtained out-of-bag validation reduce computational cost. values hyperparameters kernel selected grid number neurons hidden layer optimized values building partially optimized ensemble sets hyperparameter obtained using out-of-bag. random forest default parameters used. optimized single base learner carried out. purpose single single trained using within-train -fold cross-validation grid search sets parameters given above. average errors experiments shown table single homogeneous ensembles composed svms mlps dataset best method highlighted boldface second best method underlined. addition overall comparison methods shown figure mean procedure proposed demˇsar diagram average ranks method shown. methods connected horizontal solid line indicate differences average rank statistically signiﬁcant according nemenyi test table observed ensemble mlps clearly outperforms single mlp. differences favourable ensemble mlps except ionosphere parkinsons. differences single ensemble counterpart pronounced ones observed mlps. ensemble obtains better result single datasets. result observed figure average rank e-svm slightly better single svm. however difference statistically signiﬁcant. even thought differences statistically signiﬁcant analysis shows procedure build ensembles svms detrimental. using base classiﬁers observe differences statistically signiﬁcant respect single mlp. addition setting observed training time e-svm times faster training single using grid search -fold cross-validation. hand ensemble mlps times slower single linear complexity respect number training instances. section performance proposed procedure built heterogeneous ensembles pooling homogeneous ensembles analyzed. objective optimum proportion possible base classiﬁers include ﬁnal heterogeneous ensemble. possible selected proportions correspond different heterogeneous ensemble represented point regular simplex dimensions. shown figure three representative datasets heart colic tic-tactoe. plot figure shows dimensional simplex average test error different combinations base classiﬁers intervals classiﬁers using color scale scheme. darker colors indicate higher average error indicated color legend top-right plot. three vertices plots correspond three tested homogeneous ensembles. vertices upper left right bottom left plot correspond e-svm e-mlp random forest respectively. displacement away vertices smoothly transforms corresponding homogeneous ensembles heterogeneous one. horizontal axis shows number selected mlps heterogeneous ensemble vertical axis indicates number svms minus number random trees. addition plots show average selected positions using outof-bag validation average position best test errors plots figure different behaviours combination base classiﬁers observed. heart best position observed quite centered showing heterogeneous ensemble composed base classiﬁers different types beneﬁcial improve generalization performance ensemble. however general trend observed center plot case best result clearly located vertices simplex correspond homogeneous ensemble random forest case. finally important note optimum location need close best homogedataset e-mlp e-svm .±.* australian boston breast bupa chess .±.* colic .±.* german heart hepatitis ionosphere ozone parkinsons .±.* pima .±.* ringnorm spambase sonar threenorm .±.* .±.* tic-tac-toe .±.* twonorm shown table proposed method best second best method datasets. e-svm also achieves rather good results somehow less consistent. e-svm method obtains highest number best performances performance worst datasets. finally random forest e-mlp obtain best results respectively. results summarized using demˇsar plot figure diagram observed proposed procedure signiﬁcantly better random forest e-mlp proposed methodology average rank better e-svm difference statistically signiﬁcant. conclusions study continuous family heterogeneous ensembles size varying proportions base classiﬁers different types analyzed. ﬁrst generate different homogeneous ensembles. diversiﬁcation ensembles obtained using subsampling randomization techniques. heterogenous ensemble built pooling classiﬁers homogeneous ensembles. proportions classiﬁers different types heterogeneous combination represented point simplex dimensions. vertices simples corresponds homogeneous ensembles. optimal proportion base classiﬁers ﬁnal ensemble strongly problem-dependent estimated using out-of-bag data. empirical evaluation carried proposed strategy consistently exhibits excellent performance. problems investigated either ﬁrst second accurate method. results show proposed combination better homogeneous ensembles; i.e. random forest ensembles mlps ensembles svms. addition differences average ranks statistically signeous ensemble. instance tic-tac-toe location minimum error close random forest vertex spite fact homogeneous ensemble presents worst average performance. finally observe average location minima identiﬁed using out-of-bag quite close location test. also observed however smaller datasets identiﬁcation optimum point less accurate. table average test errors homogeneous ensembles svms mlps random forest proposed strategy investigated problems reported. best second best results dataset highlighted boldface underlined respectively. addition table shows average percentage classiﬁers type selected out-of-bag validation heterogeneous ensembles. percentages shown order ensembles shown svms random trees. last column table indicates entropy selected percentages classiﬁers divided maximum entropy mohammad nazmul haque nasimul noman regina berretta pablo moscato. heterogeneous ensemble combination search using genetic algorithm class imbalanced data classiﬁcation. plos chih-wei chih-chung chang chih-jen lin. practical guide support vector classiﬁcation. technical report department computer science national taiwan university loris nanni sheryl brahnam stefano ghidoni alessandra lumini. toward generalpurpose heterogeneous ensemble pattern classiﬁcation. computational intelligence neuroscience ioannis effective voting katakis ioannis vlahavas. heterogeneous classiﬁers. jean-franc¸ois boulicaut floriana esposito fosca giannotti dino pedreschi editors machine learning ecml pages berlin heidelberg springer berlin heidelberg. banﬁeld hall bowyer kegelmeyer. comparison decision tree ensemble creation techniques. ieee transactions pattern analysis machine intelligence rich caruana alexandru niculescumizil geoff crew alex ksikes. ensemble selecproceedings tion libraries models. twenty-ﬁrst international conference machine learning icml pages york acm. joacir marques oliveira eulanda miranda santos jos´e reginaldo hughes carvalho leyne abuim vasconcelos marques. ensemble heterogeneous classiﬁers applied lithofacies classiﬁcation using logs different wells. international joint conference neural networks ijcnn pages dallas thomas dietterich ghulum bakiri. error-correcting output codes general method improving multiclass inductive learning programs. proceeding aaai- pages aaai press", "year": 2018}