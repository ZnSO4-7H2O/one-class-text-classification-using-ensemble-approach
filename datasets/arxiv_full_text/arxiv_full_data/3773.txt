{"title": "Scalable Multi-Output Label Prediction: From Classifier Chains to  Classifier Trellises", "tag": ["stat.ML", "cs.CV", "cs.DS", "cs.LG", "stat.CO"], "abstract": "Multi-output inference tasks, such as multi-label classification, have become increasingly important in recent years. A popular method for multi-label classification is classifier chains, in which the predictions of individual classifiers are cascaded along a chain, thus taking into account inter-label dependencies and improving the overall performance. Several varieties of classifier chain methods have been introduced, and many of them perform very competitively across a wide range of benchmark datasets. However, scalability limitations become apparent on larger datasets when modeling a fully-cascaded chain. In particular, the methods' strategies for discovering and modeling a good chain structure constitutes a mayor computational bottleneck. In this paper, we present the classifier trellis (CT) method for scalable multi-label classification. We compare CT with several recently proposed classifier chain methods to show that it occupies an important niche: it is highly competitive on standard multi-label problems, yet it can also scale up to thousands or even tens of thousands of labels.", "text": "multi-output inference tasks multi-label classiﬁcation become increasingly important recent years. popular method multi-label classiﬁcation classiﬁer chains predictions individual classiﬁers cascaded along chain thus taking account inter-label dependencies improving overall performance. several varieties classiﬁer chain methods introduced many perform competitively across wide range benchmark datasets. however scalability limitations become apparent larger datasets modeling fully-cascaded chain. particular methods’ strategies discovering modeling good chain structure constitutes mayor computational bottleneck. paper present classiﬁer trellis method scalable multi-label classiﬁcation. compare several recently proposed classiﬁer chain methods show occupies important niche highly competitive standard multi-label problems also scale thousands even tens thousands labels. multi-output classiﬁcation supervised learning problem instance associated qualitative discrete variables rather single variable. since label variables often strongly correlated modeling dependencies allows methods improve performance expense increased computational cost. multi-label classiﬁcation special case labels binary; already attracted great deal interest development machine learning literature last years. authors give recent review many references number recent popular methods mlc. figure shows relationship different classiﬁcation paradigms according number labels type vast range active applications including tagging images categorizing documents labelling video media learning relationship among genes biological functions. labels either relevant not. example image labelled beach urban; news article sectioned europe economy. relevance usually indicated irrelevance general scheme information month gender. note month therefore simply irrelevant not. task received relatively less attention however mlc-transformation methods equally applicable moc. indeed paper deal family methods based approach. note also that integer represented binary form task ‘decode’ task vice versa. paper focus scalable methods able effectively deal large datasets feasible complexity. many recent methods particularly based classiﬁer chains tend engineered investing evermore computational power model label dependencies presenting poor scalability properties. ﬁrst part paper review state-of-the-art methods literature show powerful solutions well-suited deal large-size label sets. instance classiﬁer chains consider full cascade labels along chain model joint probability distribution either explore possible label orders chain incurring exponential complexity number labels compare small subset chosen random ineffective large dimensional problems. main contribution paper novel highly-scalable method classiﬁer trellis rather imposing long-range ultimately computationally complex dependency model classiﬁer chains captures essential dependencies among labels efﬁciently. achieved considering predeﬁned trellis structure underlying graphical model dependent nodes sequentially placed structure according easily-computable probabilistic measures. experimental results across wide datasets show able scale large sets remaining competitive standard problems. fact experiments close running time naive baseline method neglects statistical dependency labels. also ensemble version method multiple times different random seeds classiﬁcation done majority voting signiﬁcantly outperform single-shot demonstrates method quite robust initialization. paper organized follows. first section formalize notation describe problem’s setting. section review state-of-the-art methods literature well various strategies modeling label dependence. review augmented empirical results. section make studies theory earlier sections present classiﬁer trellis method. section carry sets experiments ﬁrstly compare state-of-the-art multi-label methods task; secondly show also provide competitive performance typical structured output prediction task finally section discuss results take conclusions. appendixes included help readability paper support presented results. compare low-complexity methods infer label dependencies training data. review monte carlo methods required paper perform probabilistic approximate inference label associated test input. fig. example multi-label classiﬁcation possible values label labels implicitly features. circles squares triangles elements active label hexagons show vectors coincides true test label high probability. furthermore conditional distribution usually unknown estimated classiﬁer construction stage. standard setting classiﬁcation supervised task infer model labelled examples y)}n apply predict labels novel unlabelled examples prediction phase usually straightforward single-output case since values needs selected. more usually unknown estimated training data y)}n order construct model therein precisely lies main challenge behind since must select possible values; clearly much difﬁcult task furthermore ﬁnding given quite challenging computational point view large values labels binary labels namely possible label values typically notated figure shows example three labels strong co-occurrence interpret ﬁrst label implies second label high probability around. learning model goal capturing kind dependence among labels order improve classiﬁcation performance; efﬁciently enough scale size data application domains interest. typically means connecting labels appropriate structure. table summarizes main notation used paper. section step relevant methods mlc/moc recently developed well several works speciﬁcally related novel method presented later sections. methods discussed here also method presented section build model ﬁrst selecting suitable model label joint posterior distribution using model provide prediction test input ﬁrst step state-of-the-art methods present complexity bottleneck deal large sets labels offers signiﬁcantly better complexity-performance trade-off. description instance input vector; label output possible values l-dimensional label/output vector training data binary multi-class classiﬁcation multi-label multi-output classiﬁcation naive solution multi-output learning training k-class models i.e. independent classiﬁers using classify times test instance hl]. represented directed graphical model shown figure note approach implicitly assumes independence among target variables greedy inference comes concern error propagation along chain since incorrect estimate negatively affect following labels. however problem always serious easily overcome ensemble therefore although exist number approaches avoiding error propagation exhaustive iteration various search options ensemble approach. parents label proposed known bayesian classiﬁer chains since remind bayesian networks. using structure makes training individual classiﬁers faster since fewer inputs them also speeds kind inference. figure shows example many possible network structures. unfortunately ﬁnding optimal structure hard impossibly large search space. consequently recent point interest ﬁnding good suboptimal structure used. literature focused around idea label dependence excellent discussion). least complex approach measure marginal label dependence i.e. relative co-occurrence frequencies labels. approach considered latter authors exploited frequent sets approach measures co-occurrence several labels incorporate edges bayesian network. however noted problems attributes negative co-occurrence resulting algorithm deal moderately large datasets ﬁnal network construction approach ends rather involved. finding graph based conditional label dependence inherently demanding input feature space must taken account i.e. classiﬁers must trained. course training time strongly limiting factor here. however particularly interesting approach modelling conditional dependence so-called lead method presented scheme tries remove ﬁrst dependency labels feature common parent labels facilitate learning label dependencies. order lead trains ﬁrst independent classiﬁer label uses dependency relations residual errors classiﬁers learn bayesian network following standard approach lead thus fast method ﬁnding conditional label dependencies shown good performance small-sized datasets. neither lead methods assume particular constraint underlying graph well suited high dimensional regime complexity. however underlying directed graph sparse algorithm modiﬁcations state-of-the-art solution directed structured learning. pc-algorithm runs worst case exponential time true underlying graph sparse reduces polynomial runtime. however typically case mlc/moc problems. sake comparison different mlc/moc approaches paper consider lead methods infer direct dependencies labels. order delve deeper issue structure learning using lead methods generated synthetic dataset underlying structure known compare solutions degree similarity respect true graphical model. experiments illustrate main problems behind learning graphical model structure scratch typically dense networks cannot control complexity associated training evaluating probabilistic classiﬁers corresponding resulting factorization issue solved classiﬁer trellis method proposed section conditional dependency networks conditional dependency networks represent alternative approach conditional distribution factorizes according undirected graphical model i.e. normalizing constant positive function potential subset labels notion directionality dropped thus simplifying task learning graph structure. undirected graphical models natural domains spatial relational data. therefore well suited tasks image segmentation regular problems unlike classiﬁer chain methods construct approximation based product probabilistic yne) variables connected undirected graph. finally p|yne) approximated probabilistic classiﬁer order classify test input approximate inference using gibbs sampling viable option. present formulation monte carlo approaches specially tailored perform approximate inference mlc/moc methods based bayesian networks undirected graphical models. ﬁnal note related work many ‘families’ methods designed multi-label multi-output structured output prediction classiﬁcation including many ‘algorithm adapted’ methods. fairly complete recent overview seen example. however methods suffer similar challenges classiﬁer chains family; similarly attempt model dependence remain tractable using approximations form randomness cite recent example uses ‘random graphs’ resembles since uses undirected graphical models sense randomness graphs considered. view many methods employed multi-label chain classiﬁers properly compared literature particularly regard method ﬁnding structure. conclusive evidence modelling marginal dependencies enough whether advisable model conditional dependencies also much return gets heavy investment searching ‘good’ graph structure random structures. labels independent structure. ensemble random order ensemble bccs based marginal dependence above conditional dependence optimal possible chain orders figure shows example structure found bcc-fs bcc-lead real dataset namely music base classiﬁer support vector machines ﬁtted logistic models order obtain probabilistic output default hyper-parameters provided implementation weka framework table conﬁrms ic’s assumption independence harms performance bulk literature justiﬁed trying overcome this. however also suggests investing factorial exponential time best-ﬁtting chain order label combination guarantee best results. fact comparing results ebcc-lead ebcc-fs even relatively higher investment conditional label dependence marginal dependence necessarily off. finally ecc’s performance quite close mcc. surprisingly method tends provide excellent performance even though learns randomly ordered chains. however discussed section complexity prohibitively large high dimensional problems. fig. graphs derived music dataset links based marginal dependence conditional dependence based links mutual information therefore links represent co-occurrences mutual exclusiveness generally graph makes intuitive sense; amazed happy neither strongly similar opposite emotions thus much beneﬁt modelling together; angry sad. goal highly scalable cc-based method brings common question structure use. hand ignoring important dependency relations harm performance typical problems. hand assumptions must made scale large scale problems. even though certain types problems clear notion local structure underlying labels assumption valid general problems labels might highly correlated example. therefore cannot escape need discover structure must efﬁciently. furthermore structure used allow fast inference. proposed solution classiﬁer trellis relieve burden speciﬁcation ‘from scratch’ maintain ﬁxed structure namely lattice trellis escapes high complexity complete structure learning time avoids complexity involved discovering structure instead impose structure a-priori seek improvement order labels within structure. figure gives three simple example trellises vertices trellis corresponds labels dataset. note relationship simply pattern pa). namely parents label labels laying vertices left trellis structure linked structure model dependence among labels. hence instead trying solve hard structure discovery problem simple heuristic place labels ﬁxed structure sensible order tries maximize label dependence parents children. ensures good structure captures main label dependencies maintaining scalability large number labels data. namely employ efﬁcient hill climbing method insert nodes trellis according marginal dependence information manner similar method essentially nodes progressively added graph based mutual information. since algorithm starts placing random label upper left corner vertex different trellis discovered different random seeds. label directly connected ﬁxed number parent labels directed graph except border cases parents possible. computational cost algorithm simple calculations involved practice easily able scale tens thousands labels. indeed show later section method fast effective. furthermore possible limit complexity searching number labels given proper user-deﬁned parent pattern ensured trellis obtained algorithm directed acyclic graph. hence need check cycles construction time consuming stage many algorithms employ probabilistic classiﬁers construct approximation according directed graph. approach simply referred classiﬁer trellis afterwards either inference greedily monte carlo sampling alternatively note interpret trellis structure provided algorithm terms undirected graph following approach classiﬁer dependency networks described section example figure would compares refer approach classiﬁer dependency trellis outlined algorithm algorithm respectively. argue undirected version powerful since learning undirected graph typically easier learning directed graph encodes causal relations. however cdtconstructs undirected graphical model greedy inference cannot implemented rely monte carlo sampling methods test stage. effect clearly noticed table finally consider simple ensemble method similar proposed improve classiﬁer chain methods classiﬁers built different random seed ﬁnal label decision made majority voting. makes training time inference times larger. denote method ect. similar approach could followed given higher computational cost test stage concerns regarding scalability approach excluded simulations. firstly section compare e/ct high-performance methods discussed section show imposed trellis structure compete fully-cascaded chains discovered structures like provided bcc. approach based trellis structures achieves similar performance presenting improved scalable properties consequently signiﬁcantly lower running times. methods considered listed table table summarize complexity. represents input dimensions ensemble methods gibbs iterations cdt. complexity intuitive measure experimental results reported section conﬁrm running times indeed close independent classiﬁers ensemble random best random ensemble bccs classiﬁer trellis figure left ensemble classiﬁer dependency trellis figure middle table complexity algorithm roughly sorted training complexity. represents input dimensions train complexity indicates roughly many values looked classiﬁer. test complexity indicates many individual models addressed inference. first conﬁrm proposed hill climbing strategy actually beneﬁcial effect cross validation smaller datasets. results displayed table signiﬁcant increase performance seen decrease standard deviation conﬁrms proposed hill climbing strategy help optimizing performance decreasing sensitivity initialization. then compare methods listed table datasets table results predictive performance displayed table running times seen table small datasets support vector machines base classiﬁers ﬁtted logistic models alternative authors used logistic regression directly probabilistic output. experience obtain better faster all-round performance svms. note that best accuracy highly recommended tune base classiﬁer. however wish avoid dimension instead focus multi-label methods. larger datasets instead stochastic gradient descent maximum epochs deal scale presented large problems. methods implemented made available within meka framework; open-source multi-output learning framework based weka machine learning framework implementations pertain weka. results conﬁrm competitive terms performance running time. using hamming score ﬁgure merit given running times reported clearly superior rest methods. note table shows ct’s running times close namely method neglects statistical dependency labels. respect exact match accuracy performance results measures oriented recovery whole labels achieve competitive results respect high-complexity methods model full-chain labels. table reports training test average times computed different methods. also include explicitly number labels dataset. note also even though considers possible random initializations signiﬁcantly improve performance cases suggest hill climbing strategy makes algorithm quite robust respect initialization. regarding scalability note computed train/running times scale roughly linearly number labels instance mediamill dataset delicious approximately order magnitude higher observe running times multiplied approximately factor datasets. conclusions drawn also largest datasets compare instance running times local localk. finally scalable modiﬁcation shows worse performance requires larger test running times. order illustrate signiﬁcant statistical differences methods figure include results nemenyi test based table table however note excluded rows dnfs. nemenyi test rejects null hypothesis average rank difference greater critical distance na/nd algorithms datasets according table value method rank greater another method least critical distance considered statistically better. figure method place spanning average rank method point plus critical distance. thus pair bars overlap correspond methods statistically different terms performance. note that regarding training test running times overlaps considerably whereas methods need signiﬁcantly training time. statistically stronger exact match. performs particularly well hamming score indicating error propagation limited compared methods. following section present framework behind localization datasets local localk tables investigate application type structured output prediction problem segmentation localization. section consider localization application using light sensors based real-world scenario described number light sensors arranged around room purpose detecting location person. take ‘segmentation’ view problem synthetic models generate observations thus creating semi-synthetic dataset allows easily control scale complexity. figure shows scenario. top-down view room light sensors arranged around edges light source bottom edge four targets. note targets detected come light sensor light source target lower right corner undetectable. sensor model consider simplicity speciﬁc instance {yij}w moreover denote position d-th sensor triangle vertices triangle detection zone d-th sensor. deﬁne indicator variable fig. results nemenyi test based table table methods’ bars overlap considered statistically indifferent. graphs based time interpreted higher rank corresponds slower times. fig. tile localization scenario. light sensors arranged around edges scenario coordinates light source points horizontal axis. example three observations positive note object bottom-right tile cannot detected. dynamic noise ﬂipping pixels uniformly random. method applied problem infer binary vector encodes presence blockinglight elements room given vector measurements light sensors. finally also consider sensor provides observations {xdk}m interested studying compute posterior distribution variables depend making inference directly using posterior distribution straightforward. address problem steps. first measurements received sensor considered bernoulli trials probability results already given table table results table illustrate robustness algorithm address multi-output classiﬁcation several scenarios. beyond training knowledge underlying model needed achieve remarkable classiﬁcation performance. emphasize property compare estimator presented above exploits perfect knowledge sensor model. table shows results using algorithm sensors different values corresponding results obtained provided table detailed discussion results provided next section. however remark increasing number tiles given number sensors makes problem harder ﬁner resolution sought. explains decrease performance seen tables increases. multi-label literature found independent classiﬁers consistently under-perform thus justifying development complex methods model label dependence. however contrary much multi-label literature suggests greater investments modelling label dependence always correspond greater returns. fact appears many methods literature over-engineered. small experiment table suggests none approaches investigated particularly dominant ability uncover structure respect predictive performance. indeed results indicate none techniques signiﬁcantly better another. using ‘safe bet’ terms high accuracy since models long term dependencies fully cascaded chain; also noted previously terms ebcc clear advantage methods surprisingly also clear difference searching structure based marginal dependence versus conditional label dependence. makes difﬁcult justify computationally complex expenditures modelling dependence basis improved accuracy; particularly large datasets scalability crucial. presented classiﬁer trellis alternative methods model full chain methods unravel label graphical model structure scratch bcc. approach systematic consider ﬁxed structure place labels ordered procedure according easily computable mutual information measures algorithm ensemble version performs particularly well exact match surprisingly perform much stronger expected beginning. perform strong overall much scalable indicated table algorithm emerges powerful algorithm able excellent performance near running times. nemenyi test shown statistical similitude classiﬁcation outputs mcc/ecc proving approach based classiﬁer trellis captures necessary inter-label dependencies achieve high performance classiﬁcation. moreover analyzed impact trellis structure chosen performance. future work intend experiment trellis structures different degrees connectedness. weiwei cheng krzysztof dembczy´nski eyke h¨ullermeier. bayes optimal multilabel classiﬁcation probabilistic classiﬁer chains. icml international conference machine learning haifa israel june omnipress. andrea pohoreckyj danyluk l´eon bottou michael littman editors. proceedings annual international conference machine learning icml montreal quebec canada june volume international conference proceeding series. anna goldenberg andrew moore. tractable learning large bayes structures sparse data. proceedings twenty-ﬁrst international conference machine learning icml pages york acm. abhishek kumar shankar vembu aditya krishna menon charles elkan. learning inference probabilistic classiﬁer chains beam search. peter flach tijl nello cristianini editors machine learning knowledge discovery databases volume pages springer order delve deeper issue structure learning generated synthetic dataset underlying structure known. synthetic generative model follows. feature vector consider d-dimensional independent gaussian vector binary d-dimensional vector containing exactly ones assume directed acyclic graph labels label parent. vectors dependency label graph generated uniformly random. given value parent label ypa) following probabilistic model used generate label cumulative distribution function normal gaussian distribution. consequently control likelihood equal parent ypa) thus modulating complexity inferring dependencies using lead methods. figure show three examples synthetically generated datasets terms ground truth structure structure discovered using lead methods three different scenarios ‘easy’ ‘medium’ ‘hard’ datasets. recall mutual information matrix methods difference lead matrix based error frequencies rather label frequencies. visually appears lead able discover original structure relative difﬁculty dataset. appears small improvement lead. conﬁrmed batch analysis using f-measure random datasets random difﬁculty ranging ‘easy’ ‘hard’ gets lead gets depth comparison taking account varying numbers labels features left future work. better understanding mlc/moc approaches described section novel scheme introduced work achieved describing monte carlo procedures used perform approximate inference graphical models constructed approximate given probabilistic model conditional distribution test input goal scheme generating samples used estimate mode marginal distribution label |x∗) relevant statistical function data. directed acyclic graphical model probabilistic dependencies variables ordered. instance scheme factorizes according possible draw samples directly conditional density p|y− exact sampling performed simple manner. repeat undirected graphical model exact sampling generally unfeasible. however markov chain monte carlo technique able generate samples target density implemented. within class gibbs sampling often adequate approach. assume conditional distribution factorizes according undirected graphical model then initial conﬁguration repeat considered sample certain burn-in period i.e. thus samples discarded whereas samples used perform desired inference task. problems associated mcmc schemes difﬁculty determining exactly chain converged correlation among generated samples", "year": 2015}