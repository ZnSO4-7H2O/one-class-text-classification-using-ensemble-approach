{"title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "tag": ["cs.NE", "cs.CL", "cs.LG"], "abstract": "We propose zoneout, a novel method for regularizing RNNs. At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization. But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks. We perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST.", "text": "david krueger tegan maharaj jános kramár mohammad pezeshki nicolas ballas rosemary anirudh goyal yoshua bengio† aaron courville‡ christopher mila université montréal firstname.lastnameumontreal.ca. école polytechnique montréal firstname.lastnamepolymtl.ca. equal contributions. †cifar senior fellow. ‡cifar fellow. propose zoneout novel method regularizing rnns. timestep zoneout stochastically forces hidden units maintain previous values. like dropout zoneout uses random noise train pseudo-ensemble improving generalization. preserving instead dropping hidden units gradient information state information readily propagated time feedforward stochastic depth networks. perform empirical investigation various regularizers zoneout gives signiﬁcant performance improvements across tasks. achieve competitive results relatively simple models characterword-level language modelling penn treebank text datasets combining recurrent batch normalization yields state-of-the-art results permuted sequential mnist. regularizing neural nets signiﬁcantly improve performance indicated widespread early stopping success regularization methods dropout recurrent variants paper address issue regularization recurrent neural networks novel method called zoneout. rnns sequentially construct ﬁxed-length representations arbitrary-length sequences folding observations hidden state using input-dependent transition operator. repeated application transition operator different time steps sequence however make dynamics sensitive minor perturbations hidden state; transition dynamics magnify components perturbations exponentially. zoneout aims improve rnns’ robustness perturbations hidden state order regularize transition dynamics. like dropout zoneout injects noise training. instead setting units’ activations dropout zoneout randomly replaces units’ activations activations previous timestep. dropout expectation random noise test time. results simple regularization approach applied time architecture conceptually extended model whose state varies time. compared dropout zoneout appealing preserves information forwards backwards network. helps combat vanishing gradient problem observe experimentally. also empirically evaluate zoneout classiﬁcation using permuted sequential mnist dataset language modelling using penn treebank text datasets demonstrating competitive state performance across tasks. particular show zoneout performs competitively proposed regularization methods rnns including recently-proposed dropout variants. code replicating experiments found http//github.com/teganmaharaj/zoneout zoneout seen selective application dropout nodes modiﬁed computational graph shown figure zoneout instead dropping units zone previous value zoneout like dropout viewed train pseudo-ensemble injecting noise using stochastic identity-mask rather zero-mask. conjecture identity-masking appropriate rnns since makes easier network preserve information previous timesteps going forward facilitates rather hinders gradient information going backward demonstrate experimentally. figure zoneout special case dropout; unit hidden activation next time step zoneout seen applying dropout hidden state delta ht−. update dropped becomes ht−. initially successful applications dropout rnns applied dropout feed-forward connections recurrent connections several recent works propose methods limited way. bayer successfully apply fast dropout deterministic approximation dropout rnns. semeniuta apply recurrent dropout updates lstm memory cells i.e. drop input/update gate lstm/gru. like zoneout approach prevents loss long-term memories built states/cells grus/lstms zoneout preserving units’ activations exactly. difference salient zoning hidden states lstm analogue recurrent dropout. whereas saturated output gates output nonlinearities would cause recurrent dropout suffer vanishing gradients zoned-out units still propagate gradients effectively situation. furthermore recurrent dropout method speciﬁc lstms grus zoneout generalizes model sequentially builds distributed representations input including vanilla rnns. also motivated preventing memory loss moon propose rnndrop. technique amounts using dropout mask every timestep authors show results improved performance speech recognition experiments. semeniuta show however past states’ inﬂuence vanishes exponentially function dropout probability taking expectation test time rnndrop; problematic tasks involving longer-term dependencies. propose another technique uses mask timestep. motivated variational inference drop rows weight matrices input output embeddings lstm gates instead dropping units’ activations. proposed variational technique achieves single-model state-of-the-art test perplexity word-level language modelling penn treebank. equivalent zoning units layer time. typical input timestep causing issues naive implementation stochastic depth. zoning entire layer means input corresponding timestep completely ignored whereas zoning individual units allows take element input sequence account. also found using residual connections recurrent nets instability presumably parameter sharing rnns. concurrent work singh propose zoneout resnets calling skipforward. experiments zoneout outperformed stochastic depth dropout proposed swapout technique randomly drops either identity residual connections. unlike singh apply zoneout rnns outperforms stochastic depth recurrent dropout. like zoneout clockwork rnns hierarchical rnns update units’ activations every timestep updates periodic whereas zoneout’s stochastic. inspired clockwork rnns experimented zoneout variants target different update rates schedules different units performance beneﬁt. hierarchical multiscale lstms learn update probabilities different units using straight-through estimator combined recently-proposed layer normalization achieve competitive results variety tasks. authors note method interpreted input-dependent form adaptive zoneout. recent work hypernetwork dynamically rescale row-weights primary lstm network achieving state-of-the-art character-level penn treebank combined layer normalization two-layer network. scaling viewed adaptive differentiable version variational lstm could similarly used create adaptive differentiable version zoneout. recent work conditions zoneout probabilities suprisal sets state enwik recurrent neural networks process data sequentially constructing corresponding sequence representations hidden state trained remember emphasize task-relevant aspects preceding inputs incorporate inputs transition operator converts present hidden state input hidden state zoneout modiﬁes dynamics mixing original transition operator identity operator according vector bernoulli masks long short-term memory rnns hidden state divided memory cell intended internal long-term storage hidden state used transient representation state timestep widely used formulation lstm computed four gates including forget gate directly connects memories previous timestep element-wise multiplication. large values forget gate cause cell remember previous value. gates control information cell. gate weight matrix bias vector; example forget gate brevity write naive application dropout lstms would zero-mask either memory cells hidden states without changing computation gates dropping memory cells example changes computation follows figure zoneout recurrent dropout strategy lstm. dashed lines zero-masked; zoneout corresponding dotted lines masked corresponding opposite zero-mask. rectangular nodes embedding layers. zoneout values hidden state memory cell randomly either maintain previous value updated usual. introduces stochastic identity connections subsequent time steps usually different zoneout masks cells hiddens. also experiment variant recurrent dropout reuses input dropout mask zoneout corresponding output gates evaluate zoneout’s performance following tasks character-level language modelling penn treebank corpus word-level language modelling penn treebank corpus character-level language modelling text corpus classiﬁcation hand-written digits permuted sequential mnist also investigate gradient past hidden states using pmnist. character-level task train networks layer hidden units. train lstms learning rate overlapping sequences batches optimize using adam clip gradients threshold settings match used cooijmans also train grus tanh-rnns parameters above except sequences nonoverlapping learning rates grus tanh-rnns respectively. small values zoneout signiﬁcantly improve generalization performance three models. intriguingly zoneout increases training time tanh-rnn decreases training time lstms. focus investigation lstm units dynamics zoning states cells provide interesting insight zoneout’s behaviour. figure shows exploration zoneout lstms various zoneout probabilities cells and/or hiddens. zoneout cells probability zoneout states probability outperform best-performing recurrent dropout combining leads best-performing model achieves competitive recent state-of-the-art compare zoneout recurrent dropout weight noise norm stabilizer explore stochastic depth recurrent setting also tried shared-mask variant zoneout used pmnist experiments mask used cells hiddens. neither stochastic depth shared-mask zoneout performed well separate masks sampled unit. figure shows best performance achieved regularizer well unregularized lstm baseline. results reported table learning curves shown figure zoneout probabilities also improve baseline grus tanh-rnns reducing tanh-rnn. similarly zoneout probabilities work best hidden states lstms. memory cells lstms however higher probabilities work well perhaps large forget-gate values approximate effect cells zoning out. conjecture best performance achieved zoneout lstms stability state cell. probability zoned zoned carries information previous timestep forward react ’normally’ information. word-level task replicate settings zaremba best single-model performance. network layers units weights initialized uniformly model trained epochs learning rate learning rate reduced factor epoch. gradient norms clipped dropout non-recurrent connections achieve competitive results. perform search models conjecture large model size requires regularization feed-forward connections. adding zoneout recurrent connections model optimized dropout non-recurrent connections however able improve test perplexity report best performance achieved given technique table figure validation character-level penn treebank different probabilities zoneout cells hidden states comparison unregularized lstm zoneout stochastic depth zoneout recurrent dropout norm stabilizer weight noise enwik corpus made ﬁrst bytes wikipedia dumped mar. text \"clean text\" version corpus; html tags removed numbers spelled symbols converted spaces lower-cased. datasets created hosted mahoney single-layer network units initialized orthogonally batch size learning rate sequence length optimize adam clip gradients maximum norm early stopping matching settings cooijmans results reported table figure shows training validation learning curves zoneout compared unregularized lstm recurrent dropout. sequential mnist pixels image representing number presented time left right bottom. task classify number shown image. pmnist pixels presented random order. compare recurrent dropout zoneout unregularized lstm baseline. models single layer units trained epochs using rmsprop decay rate moving average gradient norms. learning rate gradients clipped maximum norm shown figure table zoneout gives signiﬁcant performance boost compared lstm baseline outperforms recurrent dropout although recurrent batch normalization outperforms three. however adding zoneout recurrent batch normalized lstm achieve state performance. setting zoneout mask shared cells states recurrent dropout probability zoneout probabilities table validation test results different models three language modelling tasks. results reported best-performing settings. performance char-ptb text measured bitsper-character word-ptb measured perplexity. char-ptb text models -layer unless otherwise noted; word-ptb models -layer. results line implementation experiments. models line nr-dropout v-dropout h-lstm+ln -hm-lstm+ln investigate hypothesis identity connections introduced zoneout facilitate gradient earlier timesteps. vanishing gradients perennial issue rnns. effective many techniques mitigating vanishing gradients always imagine longer sequence train longer-term dependence want capture. compare gradient unregularized lstm zoning dropping recurrent connections epoch training loss respect cell activations pmnist. compute average gradient norms timestep method normalize average gradient norms average gradient norms timesteps. figure shows zoneout propagates gradient information early timesteps much effectively dropout recurrent connections even effectively unregularized lstm. effect observed hidden states introduced zoneout novel simple regularizer rnns stochastically preserves hidden units’ activations. zoneout improves performance across tasks outperforming many alternative regularizers achieve results competitive state penn treebank text datasets state results pmnist. searching zoneout probabilites allows tune zoneout task zoneout probabilities states reliably improve performance existing models. perform hyperparameter search achieve results simply using settings previous state art. results pmnist word-level penn treebank suggest zoneout works well combination regularizers recurrent batch normalization dropout feedforward/embedding layers. conjecture beneﬁts zoneout arise main factors introducing stochasticity makes network robust changes hidden state; identity connections improve information forward backward network. grateful hugo larochelle chorowski students mila especially ça˘glar gülçehre marcin moczulski chiheb trabelsi christopher beckham helpful feedback discussions. thank developers theano fuel blocks acknowledge computing resources provided computecanada calculquebec. also thank samsung support. would also like acknowledge work pranav shyam learning hierarchies. research developed funding defense advanced research projects agency force research laboratory views opinions and/or ﬁndings expressed authors interpreted representing ofﬁcial views policies department defense u.s. government. yoshua bengio nicholas léonard aaron courville. estimating propagating gradients stochastic neurons conditional computation. corr abs/. http//arxiv.org/abs/.. geoffrey hinton nitish srivastava alex krizhevsky ilya sutskever ruslan salakhutdinov. improving neural networks preventing co-adaptation feature detectors. arxiv preprint arxiv. nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research tijmen tieleman geoffrey hinton. lecture .-rmsprop divide gradient running average recent magnitude. coursera neural networks machine learning bart merriënboer dzmitry bahdanau vincent dumoulin dmitriy serdyuk david warde-farley chorowski yoshua bengio. blocks fuel frameworks deep learning. corr abs/. experiment suggested anonreviewer iclr review process goal disentangling effects zoneout noise injection training process identity connections. based results observe noise injection essential obtaining regularization beneﬁts zoneout. experiment zoneout mask sampled beginning training used examples. means identity connections introduced static across training examples using static identity connections resulted slightly lower training error zoneout worse performance unregularized lstm train validation sets shown figure figure training validation curves lstm static identity connections compared zoneout compared vanilla lstm showing static identity connections fail capture beneﬁts zoneout.", "year": 2016}