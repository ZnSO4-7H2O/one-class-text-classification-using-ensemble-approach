{"title": "Regularized Dynamic Boltzmann Machine with Delay Pruning for  Unsupervised Learning of Temporal Sequences", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "We introduce Delay Pruning, a simple yet powerful technique to regularize dynamic Boltzmann machines (DyBM). The recently introduced DyBM provides a particularly structured Boltzmann machine, as a generative model of a multi-dimensional time-series. This Boltzmann machine can have infinitely many layers of units but allows exact inference and learning based on its biologically motivated structure. DyBM uses the idea of conduction delays in the form of fixed length first-in first-out (FIFO) queues, with a neuron connected to another via this FIFO queue, and spikes from a pre-synaptic neuron travel along the queue to the post-synaptic neuron with a constant period of delay. Here, we present Delay Pruning as a mechanism to prune the lengths of the FIFO queues (making them zero) by setting some delay lengths to one with a fixed probability, and finally selecting the best performing model with fixed delays. The uniqueness of structure and a non-sampling based learning rule in DyBM, make the application of previously proposed regularization techniques like Dropout or DropConnect difficult, leading to poor generalization. First, we evaluate the performance of Delay Pruning to let DyBM learn a multidimensional temporal sequence generated by a Markov chain. Finally, we show the effectiveness of delay pruning in learning high dimensional sequences using the moving MNIST dataset, and compare it with Dropout and DropConnect methods.", "text": "work propose novel regularization technique called delay pruning designed recently introduced generative model called dynamic boltzmann machine unlike conventional boltzmann machine trained collection static patterns dybm designed unsupervised learning temporal pattern sequences. dybm motivated postulates observations biological neural networks allowing exact inference learning weights based timing spikes unlike restricted boltzmann machine dybm speciﬁc hidden units network unfolded time allowing inﬁnitely many layers furthermore dybm viewed fully-connected recurrent neural network memory units conduction delays units implemented form ﬁxed length ﬁrst-in ﬁrst-out queues. spike originating pre-synaptic neuron travels along fifo queue reaches post-synaptic neuron ﬁxed delay. length fifo queues equal minus maximum delay value. completely novel architecture dybm applying existing regularization methods difﬁcult lead better generalization performance. such proposed delay pruning technique allows method regularized training fifo queues. speciﬁcally training truncates lengths zero randomly selected fifo queues. evaluate performance delay pruning stochastic multi-dimensional time series compare dropout dropconnect unsupervised learning high-dimensional moving mnist dataset. next sections ﬁrst give brief overview dybm learning rule followed delay pruning algorithm experimental results conclusion. abstract—we introduce delay pruning simple powerful technique regularize dynamic boltzmann machines recently introduced dybm provides particularly structured boltzmann machine generative model multi-dimensional time-series. boltzmann machine inﬁnitely many layers units allows exact inference learning based biologically motivated structure. dybm uses idea conduction delays form ﬁxed length ﬁrst-in ﬁrst-out queues neuron connected another fifo queue spikes pre-synaptic neuron travel along queue post-synaptic neuron constant period delay. here present delay pruning mechanism prune lengths fifo queues setting delay lengths ﬁxed probability ﬁnally selecting best performing model ﬁxed delays. uniqueness structure non-sampling based learning rule dybm make application previously proposed regularization techniques like dropout dropconnect difﬁcult leading poor generalization. first evaluate performance delay pruning dybm learn multidimensional temporal sequence generated markov chain. finally show effectiveness delay pruning learning high dimensional sequences using moving mnist dataset compare dropout dropconnect methods. deep neural networks successfully applied learning large number image recognition machine learning tasks. however neural network based models typically well suited scenarios large amounts available labelled datasets. increasing network complexity achieve impressive levels performance. caveat lead gross over-ﬁtting generalization issues trained presence limited amount training samples. result wide range techniques like adding penalty term bayesian methods adding noise training data etc. regularizing developed. recently focus deep architecture dropout dropconnect techniques proposed ways prevent over-ﬁtting randomly omitting feature detectors training sample. speciﬁcally dropout involves randomly deleting activations layer forward pass back-propagating error remaining units. dropconnect generalizes randomly omitting weights boltzmann machines dybm trained timeseries patterns. speciﬁcally dybm gives conditional probability next values time-series given historical values. conditional probability depend whole history time-series dybm thus used iteratively generative model time-series. dybm deﬁned multiple layers units layer represents recent values time-series remaining layers represent historical values time-series. recent values conditionally independent given historical values. dybm equivalent inﬁnite number layers recent values depend whole history time series. train dybm likelihood given time-series maximized respect conditional distribution next values given historical values. similar dybm consists network artiﬁcial neurons. neuron takes binary value following probability distribution depends parameters dybm. unlike values dybm change time depends previous values. dybm stochastically generates multi-dimensional series binary values. learning conventional based hebbian formulation often approximated sampling based strategy like contrastive divergence. formulation concept time largely missing. dybm like biological networks learning dependent timing spikes. called spike-timing dependent plasticity stdp states synapse strengthened spike presynaptic neuron precedes spike post-synaptic neuron synapse weakened temporal order reversed dybm uses exact online learning rule properties ltd. delays memory units illustrated figure neuron connected another spike pre-synaptic neuron travels along axon reaches post-synaptic neuron synapse delay consisting constant period dij. dybm fifo queue causes conduction delay. fifo queue stores values pre-synaptic neuron last units time. stored value pushed position toward head queue time incremented unit. value pre-synaptic neuron thus given postsynaptic neuron conduction delay. moreover dybm aggregates information spikes past neural eligibility traces synaptic eligibility traces stored memory units. neuron associated learnable parameter called bias. strength synapse pre-synaptic neuron post-synaptic neuron represented learnable parameters called weights. divided components. dybm shown figure shown equivalent inﬁnitely many layers units similar dybm weight units right-most layer figure unlike layer dybm common number units bias weight dybm shared among different units particular manner. formally dybm-t layers positive integer inﬁnity. values units t-th layer consider values time units layer associated bias term gives matrix whose element denotes weight i-th unit time j-th unit time weight turn divided components. introduced previous section neuron stores ﬁxed number neural eligibility traces. neural eligibility trace j-th neuron immediately time calculated weighted past values neuron recent values weighing more fig. dybm consists network neurons memory units. presynaptic neuron connected post-synaptic neuron fifo queue. spike pre-synaptic neuron reaches post-synaptic neuron constant conduction delay. neuron memory unit storing neural eligibility traces. synaptic eligibility trace associated synapse pre-synaptic neuron post-synaptic neuron summarizes spikes arrived synapse fifo queue. where decay rate neural eligibility trace. neuron also stores synaptic eligibility traces weighted values reached neuron pre-synaptic neuron conduction delay recent values weighing more. namely postsynaptic neuron stores ﬁxed number synaptic eligibility traces. k-th synaptic fig. pictorial representation delay pruning method dynamic boltzmann machines. original dybm showing single axonal connection fifo queue units. setup delay pruning shown single axon. typically computation gradient intractable large however dybm using speciﬁc form weight sharing exact efﬁcient gradient calculation possible. speciﬁcally limit using formulation neural synaptic eligibility traces parameters dybm computed exactly using online stochastic gradient rule maximizes log-liklihood given delay pruning provides method training dybm general neural networks fifo queues regularization choosing best performing model improved prediction test dataset. speciﬁcally refers truncating fifo queue lengths zero setting respective delay values unit length randomly selected axons probability figure displays difference architecture connected neurons fifo queue original dybm delay pruned version. procedure carried follows initialize dybm parameters delay length fifo queues connecting neurons selected randomly within certain range. neuron connected another neuron fifo queues lengths initialized dij−. calculate negative log-likelihood respect true distribution temporal-pattern training sample. training sample current training cycle every ﬁxed number epochs validate previously learned dybm predicting temporal sequence pattern. calculate negative log-likelihood neurons connected dybm-t pθ|x) conditional probability given interval denote )t∈i. units layer weight other conditional probability property conditional independence analogous rbms. dybm seen model time-series following sense. speciﬁcally given history time-series dybm-t gives probability next values time-series pθ|x). dybm-∞ next values depend whole history time-series. principle dybm-∞ thus model time-series possibly long-term dependency long values time-series moment conditionally independent given values preceding moment. using conditional probability given dybm probability sequence length given arbitrarily deﬁne namely values zero corresponding history. stdp based learning rule dybm-t derived log-likelihood given time-series maximised maximising log-likelihood log-likelihood tnl) training data respect distribution deﬁned trained dybm. update performance evaluation measure calculating difference update best model pointer point towards learnt network minimum far. select random variable bernoulli distribution probability keep original maximum delay otherwise current maximum delay dmax thus truncating current fifo queue length zero. similar dropout dropconnect applying delay pruning algorithm amounts sampling best performing thinned network. case thinned network consists fifo queues survived pruning procedure. presentation training cycle thinned network sampled trained. result procedure train across ensemble models thus effectively regularize dybm prevent over-ﬁtting. finally instead averaging across ensemble select best performing model. analogous bagging based ensemble learning method machine learning areas designed different experiments increasing complexity order evaluate effect delay pruning dybm. given dybm neural network suitable learning generative model temporal sequences tasks chosen show effect regularization modelling predicting high-dimensional temporal patterns. experiments conducted using purely based java implementation dybm macbook intel core ghz. dybm trained using mini-batches samples training cases. sample trained maximum ﬁfty thousand time steps. delay pruning carried continuously. every epoch dybm tested sample validation currently best performing model updated. every mini-batch eligibility traces dybm reinitialized learned weights previous mini-batch transferred next batch. training stopped maximum time reached estimated negative log-likelihood trained dybm matched true negative log-likelihood validation entire epoch. learning rates initially ﬁxed small value adjusted training using optimisation technique adaptive moment estimation bias weight parameters initialized randomly normal distribution mean standard deviation dybm uses fully-connected network neuron self-connection fifo queue. neuron held three neural three synaptic eligibility traces respectively. task involved learning model predict next sequence -dimensional stochastic time-series here length time-series. time-series synthetically generated using discrete-time one-markov process depicted fig. probability generate state ﬁxed transition probability different state vice versa ﬁxed number training testing data respectively. small training deliberately chosen order check generalisation ability original dybm compared dybm regularised delay pruning training. fifo queue connection delay initialized randomly number neurons neuron encoding input dimensions. probability pruning ﬁxed fig. shows example plot section training dataset. ﬁrst trained tested original dybm without regularization. fig. plot negative loglikelihood respect true data distribution estimated negative log-likelihood test data respect distribution deﬁned trained dybm. observed achieved poor generalization correlation coefﬁcient keeping parameters same re-training dybm delay pruning regularization method resulted signiﬁcantly better generalization prediction test data timeseries. clearly observed high correlation coefﬁcient estimated negative loglikelihood true negative log-likelihood data distribution. unsupervised learning image sequences difﬁcult problem. avoiding over-ﬁtting order predict future pattern sequences considerably challenging. such task designed test ability delay pruned dybm temporal sequence image frames learn underlying representation. test generating original input sequences also predicting future image frames correct temporal order. fig. predicting multidimensional discrete-time markov process state diagram one-markov process training sample example showing -dimensional stochastic binary time-series. white regions represent black regions represent correlation negative log-likelihood predicted series true negative log-likelihood test data without delay pruning delay pruning. fig. temporal sequence reconstruction prediction bits reduced resolution moving mnist dataset. dybm trained ﬁrst frames. depicted input sequence. testing dybm reconstruct original input sequence well predict next frames images. compared actual sequence order calculate percentage test accuracy. last shows effect randomizing learned weights. chosen randomly training placed initially random locations inside patch. depicted digit assigned velocity whose direction chosen uniformly randomly unit circle whose magnitude also chosen uniformly random ﬁxed range. digits bounced-off edges patch overlapped location. order reduce learning time complexity task preserve spatial complexity reduced resolution original image patches bits makes considerably difﬁcult recognize original digits patterns move frames temporal order. binarized image patches using threshold value dybm trained sample videos selected randomly original dataset tested using another randomly selected samples. training video sample reshaped matrix consisting ﬁrst frames input sequence dybm. column matrix represents demonstrated novel regularization technique called delay pruning dynamic boltzmann machine specially suitable learning generative model multidimensional temporal pattern sequences. even presence relatively small training test dataset delay pruning prevents over-ﬁtting give good generalized performance. uniqueness structure dybm delay pruning form randomly truncating length fifo queues leads change spiking dynamics network shortening memory spikes pre-synaptic post-synaptic neuron. experimental results show delay pruning signiﬁcantly outperforms state methods enabling unit dybm network give prediction accuracy reduced moving mnist dataset. srivastava hinton krizhevsky sutskever salakhutdinov dropout simple prevent neural networks overﬁtting journal machine learning research vol. zeiler zhang fergus regularization neural networks using dropconnect proceedings international conference machine learning osogami otsuka seven neurons memorizing sequences alphabetical images spike-timing dependent plasticity scientiﬁc reports vol. salakhutdinov mnih hinton restricted boltzmann machines collaborative ﬁltering proceedings international conference machine learning. boltzmann machines spike-timing dependent plasticity arxiv preprint arxiv. fig. performance comparison dybm regularized delay pruning regularization dropout dropconnect respectively moving mnist image sequence prediction task. unlike used single considerably smaller input dybm network learn reconstruct sequences well predict future. reconstruction tested letting dybm trained input sequences forward time ﬁrst frames. observed fig. dybm delay pruning signiﬁcantly good reconstructing original input sequences also able predict next frames. despite relatively small training dataset best case test prediction accuracy compared ground truth signiﬁcantly high dybm delay pruning baseline performance standard dybm observed bottom panels fig. randomizing learned weights completely destroyed ability network either reconstruct predict sequences. prediction beyond frames future considerably worse error starting accumulate predicted third frame. order compare performance delay pruning state regularization techniques trained dybm dropout dropconnect task. noted that peculiarity structure dybm straightforward application dropout dropconnect difﬁcult. case apply techniques considering time unfolded dybm-t regularisation applied units connections layers except units layer. layer acts analogous visible layer standard rbms. fig. probability deletion pruning effects test prediction accuracy cases. however dybm delay pruning signiﬁcantly outperformed dropconnect dropout regularization", "year": 2016}