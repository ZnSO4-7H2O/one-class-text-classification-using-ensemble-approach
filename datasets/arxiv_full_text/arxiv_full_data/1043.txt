{"title": "Efficient Convolutional Auto-Encoding via Random Convexification and  Frequency-Domain Minimization", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "The omnipresence of deep learning architectures such as deep convolutional neural networks (CNN)s is fueled by the synergistic combination of ever-increasing labeled datasets and specialized hardware. Despite the indisputable success, the reliance on huge amounts of labeled data and specialized hardware can be a limiting factor when approaching new applications. To help alleviating these limitations, we propose an efficient learning strategy for layer-wise unsupervised training of deep CNNs on conventional hardware in acceptable time. Our proposed strategy consists of randomly convexifying the reconstruction contractive auto-encoding (RCAE) learning objective and solving the resulting large-scale convex minimization problem in the frequency domain via coordinate descent (CD). The main advantages of our proposed learning strategy are: (1) single tunable optimization parameter; (2) fast and guaranteed convergence; (3) possibilities for full parallelization. Numerical experiments show that our proposed learning strategy scales (in the worst case) linearly with image size, number of filters and filter size.", "text": "omnipresence deep learning architectures deep convolutional neural networks fueled synergistic combination ever-increasing labeled datasets specialized hardware. despite indisputable success reliance huge amounts labeled data specialized hardware limiting factor approaching applications. help alleviating limitations propose efﬁcient learning strategy layer-wise unsupervised training deep cnns conventional hardware acceptable time. proposed strategy consists randomly convexifying reconstruction contractive auto-encoding learning objective solving resulting large-scale convex minimization problem frequency domain coordinate descent main advantages proposed learning strategy single tunable optimization parameter; fast guaranteed convergence; possibilities full parallelization. numerical experiments show proposed learning strategy scales linearly image size number ﬁlters ﬁlter size. heart recent success deep convolutional neural networks several application domains computer vision speech recognition natural language processing synergistic combination ever-increasing labeled datasets specialized hardware form graphical processing units ﬁeld programmable gate arrays computer clusters despite indisputable success relying huge amounts labeled data specialized hardware limiting factor approaching applications. furthermore applying deep learning techniques locally platforms limited resources cannot rely huge amounts labeled data specialized hardware. therefore argue growing need resource-efﬁcient unsupervised learning strategies capable training deep cnns conventional hardware acceptable time. main purpose unsupervised learning leverage wealth unlabeled data disentangling causal factors context supervised pattern classiﬁcation empirical evidence shows that unsupervised pre-training helps disentangling class manifolds lower layers deep cnns upper layers better disentangled subject supervised training combination boosts overall performance ratio unlabeled labeled samples high work propose train deep cnns greedy layer-wise manner using reconstruction contractive auto-encoding learning objective rcae learning objective proven capture local shape data-generating distribution hence capturing manifold structure data. meanwhile minimizing rcae learning objective involves solving non-convex minimization problem often addressed using stochastic gradient descent methods despite empirical success applied deep cnns disadvantage computationally expensive manual tuning optimization parameters learning rates convergence criteria. moreover inherently sequential nature makes difﬁcult parallelize using gpus distribute using computer clusters overcome above-mentioned difﬁculties propose convexify rcae learning objective. inspired recent work propose adopt random convexiﬁcation strategy ﬁxing encoding parameters learning decoding parameters. transforming randomly convexiﬁed rcae objective frequency domain using discrete fourier transform obtain learning objective propose minimize using coordinate descent main advantages proposed learning strategy single tunable optimization parameter; fast guaranteed convergence; possibilities full parallelization. main motivation work efﬁcient unsupervised training deep cnns. adopt greedy layer-wise learning strategy consider deep cnns stack single-layer cnns ﬁlters input space rd×d×c i.e. space c-channel images tion contractive auto-encoding learning objective rcae learning objective proven capture high-density regions data-generating distribution twice-differential reconstruction function regularization parameter approaching asymptotically. consider following parametric reconstruction function denoting entry-wise application twice-differentiable activation function model parameters consist encoding ﬁlters {a}k decoding ﬁlters {w}k yielding dimension full convolution decoding function. core idea behind proposed layer-wise learning strategy sample entry encoding parameters {a}k i.i.d. pre-determined density functions respectively keeping ﬁxed learning decoding parameters {w}k frequency domain. deﬁne complex-valued decoding parameters associated {w}k f{w} cd×d discrete fourier transform given training images {xn}n sampled i.i.d. datac= used parseval’s theorem conjunction convolution theorem transforming randomly convexiﬁed rcae objective spatial frequency domain. direct consequence minimizing transformed rcae objective reduces solving independent k-dimensional regularized linear least-squares problems offers full parallelization possibilities. propose solve independent k-dimensional problems using coordinate descent recently witnessed resurgence interest largescale optimization problems simplicity fast convergence context rcae method consists iteratively minimizing along k-th coordinate keeping remaining coordinates ﬁxed yielding following strategy updating ﬁlters frequency domain main advantages derived ﬁlter update strategy single tunable optimization parameter fast guaranteed convergence possibilities parallelization offered frequency-domain transformation lrcae. important note incremental data acquisition involved also computed parallel. decoding ﬁlters learned frequency domain transformed back spatial domain using inverse sanity check overall computational efﬁciency proposed learning strategy we’ve implemented cd-based frequency-domain rcae minimization matlab we’ve used built-in fast fourier transform transforming rcae objective frequency domain. hyperbolic tangent used activation function. encoding ﬁlters biases randomly ﬁxed sampling entries independently zero-mean normal distribution standard deviations respectively. learning decoding ﬁlters we’ve implemented fairly naive version frequency-domain initializing complex-valued ﬁlter performing single cycle trough ﬁlter coordinates experiments used caltech- object category dataset whitened images. figure time-complexity cd-based rcae minimization caltech- dataset. left cpu-time function image size middle cpu-time function number ﬁlters. right cpu-time function ﬁlter size figure reconstruction error convergence rate cd-based rcae minimization caltech dataset. left reconstruction error function regularization parameter right convergence rate learned decoding ﬁlters function number training images convergence rate computed using average squared difference consecutive ﬁlter updates. original curve noisy average involved computation convergence rate. smoothed curve therefore plotted order highlight general trend. figure visualization learned decoding ﬁlters reconstruction result caltech- dataset left previously unseen testing image. middle learned decoding ﬁlters. right reconstruction result previously unseen testing image using learned decoding ﬁlters ﬁrst experiment we’ve measured computational time-complexity terms cpu-time intel® core™ machine. figure shows proposed method linear time-complexity w.r.t. image size number ﬁlters ﬁlter size. knowing naive implementation involve particular form parallelism linear time best possible complexity achieve situations algorithm read entire input sequentially. second experiment study inﬂuence regularization parameter reconstruction error analyze convergence rate learned decoding ﬁlters function number training samples robust estimation reconstruction error averaged batch images used training. figure left illustrates reconstruction error reaches minimum regularization parameter approaches figure right illustrates roughly training images learned decoding ﬁlters ˆw}k already settle. clearly highlights advantages random convexiﬁcation frequency-domain minimization using figure depicts decoding ﬁlters reconstruction result previously unseen testing image obtained cd-based minimization rcae objective using training images. we’ve proposed efﬁcient learning strategy layer-wise unsupervised training deep cnns. main contributions proposed learning strategy random convexiﬁcation frequencydomain transformation reconstruction contractive auto-encoding objective yields relatively easy-to-solve large-scale regularized linear least-squares problem. we’ve proposed solve problem using coordinate descent main advantages single tunable optimization parameter; fast guaranteed convergence; possibilities full parallelization. numerical experiments show that fairly naive implementation proposed learning strategy scales linearly image size number ﬁlters ﬁlter size. also observe that using relatively training images learned ﬁlters already settle yield decent reconstruction results unseen testing images. believe inherently parallel nature proposed learning strategy offers interesting possibilities increasing computational efﬁciency using sophisticated implementation strategies. work supported agency innovation science technology flanders grant interdisciplinary research program emo-app project national natural science foundation china", "year": 2016}