{"title": "Similarity Preserving Representation Learning for Time Series Analysis", "tag": ["cs.AI", "cs.LG"], "abstract": "A considerable amount of machine learning algorithms take instance-feature matrices as their inputs. As such, they cannot directly analyze time series data due to its temporal nature, usually unequal lengths, and complex properties. This is a great pity since many of these algorithms are effective, robust, efficient, and easy to use. In this paper, we bridge this gap by proposing an efficient representation learning framework that is able to convert a set of time series with equal or unequal lengths to a matrix format. In particular, we guarantee that the pairwise similarities between time series are well preserved after the transformation. The learned feature representation is particularly suitable to the class of learning problems that are sensitive to data similarities. Given a set of $n$ time series, we first construct an $n\\times n$ partially observed similarity matrix by randomly sampling $O(n \\log n)$ pairs of time series and computing their pairwise similarities. We then propose an extremely efficient algorithm that solves a highly non-convex and NP-hard problem to learn new features based on the partially observed similarity matrix. We use the learned features to conduct experiments on both data classification and clustering tasks. Our extensive experimental results demonstrate that the proposed framework is both effective and efficient.", "text": "considerable amount machine learning algorithms take instance-feature matrices inputs. such cannot directly analyze time series data temporal nature usually unequal lengths complex properties. great pity since many algorithms effective robust efﬁcient easy use. paper bridge proposing efﬁcient representation learning framework able convert time series equal unequal lengths matrix format. particular guarantee pairwise similarities time series well preserved transformation. learned feature representation particularly suitable class learning problems sensitive data similarities. given time series ﬁrst construct partially observed similarity matrix randomly sampling pairs time series computing pairwise similarities. propose extremely efﬁcient algorithm solves highly non-convex np-hard problem learn features based partially observed similarity matrix. learned features conduct experiments data classiﬁcation clustering tasks. extensive experimental results demonstrate proposed framework effective efﬁcient. introduction modeling time series data important challenging task. considered challenging problems data mining. although time series analysis attracted increasing attention recent years models analyze time series data still much fewer models developed static data. latter category models usually take instance-feature matrices inputs cannot directly analyze time series data temporal nature usually unequal lengths complex properties great pity since many static models effective robust efﬁcient easy use. introducing time series analysis greatly enhance development domain. work bridge proposing efﬁcient unsupervised representation learning framework able convert time series data equal unequal lengths matrix format. particular pairwise similarities time series data well preserved transformation. therefore learned feature representation particularly suitable similarity-based models variety learning problems data clustering classiﬁcation learning rank. notably proposed framework ﬂexible time series distance similarity measures mikowski distance cross-correlation kullback-leibler divergence dynamic time warping similarity short time series distance. work similarity default since known best measure time series problems wide variety domains given total time series ﬁrst step generate similarity matrix equaling similarity time series however computing pairwise similarities requires call algorithm times timeconsuming large. concrete example generating full similarity matrix takes hours intel xeon processor main memory. order signiﬁcantly reduce running time follow setting matrix completion assuming similarity matrix low-rank. natural assumption since algorithm captures co-movements time series shown driven small number latent factors according theory matrix completion randomly sampled entries needed perfectly recover low-rank matrix. allows sample pairs time series generate partially observed similarity matrix time spent generating similarity matrix signiﬁcantly reduced factor number scales allinearly respect takes minutes construct partially observed similarity matrix observed entries given generated partially observed similarity matrix second step learns feature representation time series pairwise similarities well approximated inner products features. solve symmetric matrix factorization problem factorize i.e. learning feature representation rn×d matrix projection operator deﬁned observed entry despite relatively simple formulation optimization problem hard solve since highly non-convex np-hard. address challenge propose efﬁcient exact cyclic coordinate descent algorithm. wisely updating variables taking advantage sparse observed entries proposed algorithm incurs computational cost thus learn feature representations extremely efﬁcient way. evaluate performance learned feature representation real-world data sets conduct experiments data classiﬁcation clustering tasks. results show classical static models learned features outperform state-of-the-art time series classiﬁcation clustering algorithms accuracy computational efﬁciency. summary main contributions work two-fold bridge time series data great amount static models learning feature representation time series. learned feature representation preserves pairwise similarities time series data general enough applied variety learning problems. propose ﬁrst best knowledge optimization parameter free algorithm solve symmetric matrix factorization problem partially observed matrix. proposed algorithm highly efﬁcient converges fast rate. related work section review existing work learning feature representations time series data. among them family methods derived features represent time series. instance proposed mean standard deviation kurtosis skewness time series represent control chart patterns. authors introduced features trend seasonality serial correlation chaos nonlinearity self-similarity partition different types time series. used easy compute features mean standard deviation slope temporal importance curves guide time series classiﬁcation. order automate selection features time series classiﬁcation authors proposed greedy forward method automatically select features thousands choices. besides several techniques proposed represent time series certain types transformation discrete fourier transformation discrete wavelet transformation piecewise aggregate approximation symbolic aggregate approximation addition deep learning models elman recurrent neural network long short-term memory capable modeling complex structures time series data learn layer feature representations. outstanding performance number applications become increasingly popular recent years. feature representations learned algorithms usually problemspeciﬁc general enough applications multiple domains. besides learned features cannot preserve similarities time series data thus suitable problems sensitive data similarity. limitations inspire propose problemindependent similarity preserving representation learning framework time series data. section ﬁrst present general approach similarity preserving time series representation learning framework. propose extremely efﬁcient algorithm signiﬁcantly faster naive implementation. problem deﬁnition general framework given time series {t··· equal unequal lengths goal convert matrix rn×d time series similarities well preserved transformation. speciﬁcally learn mapping function satisﬁes stands inner product commonly used similarity measure analyzing static data. denotes pairwise time series similarity computed number functions. work dynamic time warping algorithm measure similarity default. warping sequences non-linearly time dimension algorithm calculate optimal match given temporal sequences equal unequal lengths. superior performance successfully applied variety applications including computer animation surveillance gesture recognition signature matching protein sequence alignment speech recognition normally algorithm outputs pairwise distance between temporal sequences thus need convert similarity score. since inner product space induced normed space using generate similarity dtw+dtw−dtw signiﬁcantly improve efﬁciency ﬁrst step make observation similarity matrix low-rank. fact algorithm measures level co-movement between time series shown dictated small number latent factors indeed verify lowrankness another way. since matrix special case wigner random matrix gaps consecutive eigenvalues small implies low-rank property since energy concentrated eigenvalues based theory matrix completion randomly sampled entries needed perfectly recover low-rank matrix. thus don’t need compute pairwise similarities. instead randomly sample pairs time series compute similarities within selected pairs. words generate partially observed similarity matrix observed entries }n×n binary matrix indicating indices sampled pairs. running time ﬁrst step signiﬁcantly reduced factor since factor scales almost linearly greatly save running time large. instance takes seconds construct partially observed similarity matrix observed entries times faster generating full similarity matrix. given partially observed similarity matrix second step aims learn feature representation matrix instead ﬁrst completing full similarity matrix factorize propose efﬁcient symmetric factorization algorithm able directly factorize partially observed similarity matrix i.e. rn×d minimizes following optimization problem objective function regularization term since already bounds frobenius norm despite relatively simple formulation solving problem non-trivial since objective function highly non-convex problem np-hard. address issue propose efﬁcient optimization algorithm solves problem based exact cyclic coordinate descent although shows symmetric matrix factorization problem solved gradient descent algorithm well proposed coordinate descent algorithm following advantages iteration algorithm directly updates coordinate robust choice similarity measures reciprocal distance. time series almost identical distance close thus reciprocal tends inﬁnity. order learn matrix intuitive idea factorize similarity matrix rn×n detail idea consists steps i.e. similarity matrix construction step symmetric matrix factorization step. ﬁrst step constructs similarity matrix follows notation represents sub-matrix includes ﬁrst rows ﬁrst columns. although inner products learned data points x··· well approximate similarities time series idea described practical since steps painfully slow large. order generate similarity matrix need call algorithm times. addition naive implementation eigen-decomposition takes time. although reduce cost computing largest eigenvalues corresponding eigenvectors still computational intensive large concrete example length time series takes hours generate similarity matrix days compute largest eigenvalues intel xeon processor main memory. parameter-free scalable algorithm subsection propose extremely efﬁcient approach signiﬁcantly reduces computational costs steps learns feature representation high precision. addition efﬁciency another nice property proposed approach optimization parameter free user need decide optimization parameter step length learning rate. optimum. thus need decide optimization parameter; directly updating coordinates optimums using up-to-date information algorithm highly efﬁcient converges fast rate. iteration exact cyclic method variables ﬁxed variable updated optimal value. main strengths algorithm capacity update variables extremely efﬁcient way. besides proposed algorithm takes advantage sparse observed entries reduce computational cost. precise algorithm consists loops iterate entries update values. outer loop algorithm traverses column assuming columns known ﬁxed. i-th iteration optimizes i-th column minimizing following subproblem xnjxt inner loop proposed algorithm iterates coordinate selected column updates value. speciﬁcally updating j-th entry solve following optimization problem contains indices observed entries i-th matrix constant independent xji. since fourth-degree polynomial function shown later updated efﬁciently way. algorithm describes detailed steps proposed exact cyclic algorithm. proposed algorithm incurs computational cost iteration. lines algorithm computed operations. costs computing proportional cardinality besides derivative third-degree polynomial thus roots computed closed form. using cardano’s method optimal solution calculated constant time given computed likewise lines algorithm also take time since matrix updated considering observed entries. proposed algorithm per-iteration cost signiﬁcantly faster recent matrix factorization algorithms take addition per-iteration cost also expect proposed algorithm yields fast convergence. because algorithm always uses newest information update variables variable updated optimum single step. hypothesis veriﬁed convergence test conducted non-invasive fetal thorax testbed testbed contains total time series length test generate full similarity matrix computing similarities time series pairs randomly sample entries generate partially observed matrix call proposed algorithm factorize matrix setting measure performance proposed method compute error rates i.e. observed error /pωf underlying true error xxtf iteration. figure shows converge function time. ﬁgure clearly demonstrates proposed exact cyclic algorithm converges fast takes second iterations converge. besides construction accuracy also encouraging. observed error underlying true error rates close result indicates inner products learned features well approximate pairwise similarities time series also veriﬁes learn accurate enough features computing small portion pairwise similarities. addition test validates low-rank assumption. shows similarity matrix accurately approximated rank matrix. experiments section evaluate proposed framework i.e. similarity preserving representation learning classiﬁcation clustering tasks. tasks learn feature representations setting number features number iterations sample size learned features static models compare state-of-the-art time series classiﬁcation clustering algorithms. experiments window size min) dtw-related algorithms evaluated here. although improve performance tuning problemdependent warping window size skip step experiments ensure superior performance achieved proposed representation learning framework instead tuning algorithm. results obtained linux server intel xeon main memory. classiﬁcation task real-world time series data sets used classiﬁcation analysis. research institute partnered world’s largest online brokers challenge problem predicting clients’ propensity trading options. provided historical records data sampled clients lengths time series range month months. data contains dynamic attributes none related option trading. given data task predict whether clients trade options next months. besides also conduct experiments benchmark data sets i.e. forda phalangesoutlinescorrect italypowerdemand handoutlines time series repository data sets used experiments widely given feature representations learned proposed spiral framework algorithms xgboost -regularized logistic regression implemented liblinear approaches denoted spiral-xgb spiral-lr respectively compared state-of-theart time series classiﬁcation algorithms nn-dtw long short-term memory xgboost algorithm default parameters speciﬁed source code. parameter logistic regression determined -fold cross validation. measure classiﬁcation performance. experiments study repeated times aucs averaged trials reported table table ﬁrst observe xgboost logistic regression algorithms learned features outperform baseline algorithms data sets. particular method spiralxgb yields best performance data sets method spiral-lr performs best data set. encouragingly spiral-lr achieve better classiﬁcation performance nn-dtw lstm almost data sets. veriﬁes feature representation learned proposed method powerful even classical models logistic regression achieve satisfactory performance hand although lstm successfully applied number sequence prediction tasks fails deliver strong performance empirical study. conjecture data sets large enough lstm model complex structures. besides nn-dtw usually strong baseline considered hardto-beat literature however also yields overworse performance spiral-lr spiral-xgb. possible reason nn-dtw uses -nearest neighbor classiﬁer thus sensitive noises outliers. observation shows another advantage proposed method learning feature representation instead directly developing time series model method ﬂexible exploit strengths different learning algorithms. addition superior performance spiral-lr spiral-xgb efﬁcient well. signiﬁcantly lower running time baseline algorithms. example takes nn-dtw days classify forda data running time spiral-lr spiral-xgb minutes. clustering task similar time series classiﬁcation time series clustering also important task found numerous applications. test learned features clustering task conduct experiments another time series data sets. since data clustering unsupervised learning task merge training testing sets together. statistics data sets summarized table features learned proposed framework kmeans algorithm clustering method compare state-of-the-art time series clustering algorithm k-shape shown outperform many state-of-the-art partitional hierarchical spectral clustering approaches. besides also compare method clustering algorithms kmeansdtw clds since ideas similar respects. kmeans-dtw popular time series clustering algorithm uses algorithm measure pairwise distances data points. although looks similar idea spiral-kmeans also utilizes kmeans algorithms less desirable spiral-kmeans mainly because kmeans-dtw suffers high computational cost since needs compute pairwise distances time series cluster centers iteration; distance satisfy triangle inequality thus make cluster centers computed averaging multiple time series drift cluster designing efﬁcient algorithm needs call function times embedding time series data euclidean space preserving original similarities proposed method spiral successfully addresses issues. similar idea representation learning presented paper clds also learns feature representation time series data partition learned representation em-like algorithm. study normalized mutual information measure coherence inferred clustering ground truth categorization. scales higher value implies better partition. experiment repeated times performance averaged trials reported table compared baseline algorithms clustering method spiral-kmeans yields best performance seven data sets indicating delivers state-of-theart performance. addition spiral-kmeans signiﬁcantly lower running time baseline clustering algorithms evaluated here. instance clustering data takes k-shape clds kmeans-dtw minutes respectively. comparison clustering algorithm spiral-kmeans spends minute half partition data set. ﬁnally note kmeans choice clustering algorithms take learned features input. replacing advanced clustering algorithms expected achieve even better clustering performance. conclusions paper propose similarity preserving representation learning framework time series analysis. given time series idea ﬁrst generate partially observed similarity matrix observed similarities factorize matrix learn feature representation. propose ﬁrst optimization parameter free algorithm factorizing partially-observed symmetric matrix. proposed algorithm updates variables exact cyclic coordinate descent incurs computational cost iteration. feature representation learned proposed framework preserves similarities time series data general enough applied variety learning problems. empirical studies classiﬁcation clustering tasks verify deng runger vladimir. time series forest classiﬁcation feature extraction. information sciences efrat venkatasubramanian. curve matching time warping light ﬁelds algorithms computing similarity curves. journal mathematical imaging vision keogh chakrabarti pazzani mehrotra. dimensionality reduction fast similarity search large time series databases. knowledge information systems marˇcenko pastur. distribution eigenvalues sets random matrices. mathematics ussr-sbornik muda begam elamvazuthi. voice recognition algorithms using frequency cepstral coefﬁcient dynamic time warping techniques. arxiv preprint arxiv. niennattrakul ratanamahatana. inaccuracies shape averaging method using dynamic time warping time series international conference computational scidata. ence pages springer rakthanmanon campana mueen batista westover zakaria keogh. addressing data time series mining trillions time series subsequences dynamic time warping. tkdd j´erˆome vial hicham noc¸airi patrick sassiat sreedhar mallipatu guillaume cognon didier thi´ebaut b´eatrice teillet douglas rutledge. combination dynamic time warping multivariate analysis comparison comprehensive two-dimensional chromatograms application plant extracts. journal chromatography wang mueen ding trajcevski scheuermann keogh. experimental comparison representation methods distance measures time series data. data mining knowledge discovery keogh shelton ratanamahatana. fast time series classiﬁcation using numerosity reduction. icml pages qiang yang xindong challenging problems data mining research. international journal information technology decision making", "year": 2017}