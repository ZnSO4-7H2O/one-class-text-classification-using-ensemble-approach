{"title": "Compression of Fully-Connected Layer in Neural Network by Kronecker  Product", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "In this paper we propose and study a technique to reduce the number of parameters and computation time in fully-connected layers of neural networks using Kronecker product, at a mild cost of the prediction quality. The technique proceeds by replacing Fully-Connected layers with so-called Kronecker Fully-Connected layers, where the weight matrices of the FC layers are approximated by linear combinations of multiple Kronecker products of smaller matrices. In particular, given a model trained on SVHN dataset, we are able to construct a new KFC model with 73\\% reduction in total number of parameters, while the error only rises mildly. In contrast, using low-rank method can only achieve 35\\% reduction in total number of parameters given similar quality degradation allowance. If we only compare the KFC layer with its counterpart fully-connected layer, the reduction in the number of parameters exceeds 99\\%. The amount of computation is also reduced as we replace matrix product of the large matrices in FC layers with matrix products of a few smaller matrices in KFC layers. Further experiments on MNIST, SVHN and some Chinese Character recognition models also demonstrate effectiveness of our technique.", "text": "paper propose study technique reduce number parameters computation time fully-connected layers neural networks using kronecker product mild cost prediction quality. technique proceeds replacing fully-connected layers so-called kronecker fully-connected layers weight matrices layers approximated linear combinations multiple kronecker products smaller matrices. particular given model trained svhn dataset able construct model reduction total number parameters error rises mildly. contrast using low-rank method achieve reduction total number parameters given similar quality degradation allowance. compare layer counterpart fully-connected layer reduction number parameters exceeds amount computation also reduced replace matrix product large matrices layers matrix products smaller matrices layers. experiments mnist svhn chinese character recognition models also demonstrate eﬀectiveness technique. model approximation aims reducing number parameters amount computation neural network models keeping quality prediction results mostly same. model approximation important real world application neural network satisfy time storage constraints applications. within pre-speciﬁed resource constraint minimize diﬀerences outputs functions possible inputs. example setup directly minimize diﬀerences output functions formulation give constraints structure meaning model used approximate another model. practice structural similar model often used approximate another model. case model approximation approached modular fashion w.r.t. layer. rank approximation linear regression dates back anderson sainath liao zhang denton rank approximation fully-connected layer used; jaderberg rigamonti lebedev considered rank approximation convolution layer. zhang considered approximation multiple layers nonlinear activations. coeﬃcients weight term fully-connected layers organized matrices possible perform low-rank approximation matrices achieve approximation layer consequently whole model. given singular value decomposition matrix udv∗ unitary matrices diagonal matrix diagonal made singular values rank-k approximation fm×n however post-processing approach ensures getting optimal approximation rank constraint still guarantee approximation optimal w.r.t. input data. i.e. optimum following well diﬀerent rank-r approximation w.r.t. given input regularization term. weight term layers conceptually matrix rank function regularization term. however rank function well-deﬁned inﬁnite-precision numbers nuclear norm used convex proxy jaderberg recht next propose kronecker product matrices particular shapes model approximation section also outline relationship kronecker product approximation low-rank approximation section bergstra collobert used algorithms lower computation complexity typical inputs neural networks. analysis mostly immune hidden constant problem computation complexity analysis underlying computations transformed model also carried matrix products. factors selected formulation. however convolutional neural network input layer tensor order natural shape constraints leverage otherwise input matrix natural choices explore heuristics pick convolutional layer processing images input data tensor order tnchw runs diﬀerent instances data runs channels given images runs rows images runs columns images. often reshaped matrix fully connected layer runs diﬀerent instances data runs combined dimension channel height width images. weights fully-connected layer would matrix runs output number channels. i.e. formulation require fc×k fh×k kkk. number parameters reduced underlying assumption model transformation invariant across rows columns images. note four formulations linearly combined produce possible kinds formulations. would design choice respect trade number parameters amount computation particular formulation select. fully-connected layer whose input matrices exist natural dimensions adopt shape smaller weight matrices kfc. experiments possible arbitrarily pick decomposition input matrix dimensions enforce kronecker product structural constraint. refer formulation kfcm. similarly though iterative algorithms rank- approximation tensor friedland lathauwer optimality approximation lost input data distribution taken consideration. low-rank approximation extended beyond rank- arbitrary number ranks could extend kronecker product approximation kronecker product approximation. concretely following decomposition make fair comparison dataset train covolutional neural network fully-connected layer baseline. replace fully-connected layer diﬀerent layers according diﬀerent methods train network quality metrics stabilizes. compare method low-rank method baseline model terms number parameters prediction quality. experiments based implementation layers theanobergstra bastien framework. running time depend particular implementation details theano work report running time below. however noticeable slow experiments complexity analysis suggests signiﬁcant reduction amount computation. test results listed table number layer parameters means number parameters fully-connected layer counterpart layer. number model parameters number parameters whole model. test error min-validation model’s test error. cut- method output neurons instead fully-connected layer. lowrank- method replace fully-connected layer fully-connected layer ﬁrst layer output size second layer output size kfc-ii method replace fully-connected layer layer using formulation kfc-combined method replace fully-connected layer layer linear combined formulation svhn datasetnetzer real-world digit recognition dataset consisting photos house numbers google street view images. dataset comes formats consider second format -by- colored images centered around single character. digits training digits testing less diﬃcult samples neurons. implementation details changed. test results listed table cut-n method output neurons instead fully-connected layer. lowrank-n method replace fully-connected layer fully-connected layer ﬁrst layer output size second layer output size kfc-ii method replace fully-connected layer layer using formulation kfc-combined method replace fully-connected layer layer linear combined formulation kfc-rankn method formulation extend rank described above. also evaluate application chinese character recognition model. experiments done private dataset moment extend established chinese character recognition datasets like hcl) casia-hwdb). task also convolutional neural network. distinguishing feature neural network following convolution pooling layers layers hidden size hidden size. seen signiﬁcantly reduce number parameters. however case kfcm also leads serious degradation prediction quality. however increasing rank able recover lost prediction quality. nevertheless rank- model still small compared baseline model. paper propose study methods approximating weight matrices fullyconnected layers sums kronecker product smaller matrices resulting type layer call kronecker fully-connected layer. consider cases input fully-connected layer tensor order input matrix. found using layer signiﬁcantly reduce number parameters amount computation experiments mnist svhn chinese character recognition. formulation apply techniques outlined paper. also noted kronecker product technique also applied neural network architectures like recurrent neural network example approximating transition matrices linear combination kronecker products.", "year": 2015}