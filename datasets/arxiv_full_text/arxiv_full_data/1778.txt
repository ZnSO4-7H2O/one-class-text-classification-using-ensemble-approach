{"title": "Imitation Learning with Recurrent Neural Networks", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "We present a novel view that unifies two frameworks that aim to solve sequential prediction problems: learning to search (L2S) and recurrent neural networks (RNN). We point out equivalences between elements of the two frameworks. By complementing what is missing from one framework comparing to the other, we introduce a more advanced imitation learning framework that, on one hand, augments L2S s notion of search space and, on the other hand, enhances RNNs training procedure to be more robust to compounding errors arising from training on highly correlated examples.", "text": "present novel view uniﬁes frameworks solve sequential prediction problems learning search recurrent neural networks point equivalences elements frameworks. complementing missing framework comparing other introduce advanced imitation learning framework that hand augments ls’s notion search space hand enhances rnns’ training procedure robust compounding errors arising training highly correlated examples. tackling sequential prediction problems formidable challenge machine learning. number possible labels sequential prediction problem increases exponentially length sequence making considering label conﬁgurations intractable. hand naive supervised learning approach makes independent prediction time step suffers seriously compounding errors since input observations correlate thus violate identically independent distributed assumption required supervised approach. learning search employs similar approach immune shortcomings naive supervised approach. algorithms reduce sequential prediction problem learning policy traverse search space minimum cost. mild assumptions algorithms searn dagger generally guarantee compounding errors grow linearly trajectory lengths theoretically good applying supervised approach identically independent distributed data. like machine learning algorithms algorithms also suffer problem data sparsity. testing time algorithms encounter previously unseen state likely predict poorly. argue solution problem able capture similarities states unseen states still approximated seen states. previous works deﬁne search space representation specifying hand-crafted feature extraction function usually discrete static thus capture similiarities poorly. introduce deep learning approach learning continuous representation search space. search space unseen state approximated seen neighbors. approach search space’s representation learned jointly optimal policy optimized facilitate prediction-making process. speciﬁcally show formulate standard problem elements recurrent neural network. demonstrate that learning search space representation search policy using algorithm dagger equivalent training recurrent neural network model scheduled sampling hence method considered augmentation algorithms novel training algorithm recurrent neural networks. learning search frames sequential predicting process learning agent traversing states search space special subsets states space start states sstart ﬁnal states inal. learning agent commences states sstart. time step current state chooses action actions transitions next state following transition function complete trajectory length sequence states actions terminates state inal determine action taken next state learning agent execute policy maps state action. form imitation learning thus goal learn best policy mimics reference policy reference policy provided expert training time necessarily optimal. formalize learning goal deﬁne loss function whose value state cost taking complete trajectory starting following instead folowing best policy miminizes expected loss induced state distribution state distribution induced policy practice representation search state helpful learning good policy. example natural language processing problems search state instance text; search states usually sparsely distributed capture meaningful relationships. cases reduce sparsity feature extraction function project representation search state onto feature space simplicity re-deﬁne policy directly take input feature space i.e. recurrent neural networks variant neural networks targets sequential prediction problems. widely used tackle supervised problems involving learning mapping input sequence {xt} output sequence {yt}. feed-forward pass recurrent neural network maintains hidden representations input observations seen. speciﬁcally beginning initial input observation network. time step current input observation previous hidden vector composed non-linear function parametrized produce hidden vector surprisingly framework naturally framework without requiring substantial modiﬁcation. section show convert supervised learning problem standard problem. concretely specify fundamental components problem using fundamental components main idea view hidden representations states continuous search space non-linear transformation means transitioning states states space. formally denote dimension hidden vectors search space forward pass considered traversing states space. beginning time learning agent visits start state varies depending input sequence. state computed feature extraction function output sequence deemed series actions learning agent takes transition next state current state. construct transition function modify rnn’s non-linear function take account previous prediction. algorithm summarizes traversing procedure. drop parameter consistent deﬁnition transition function framework. however practice allow take i.e. form function widely used sequence-to-sequence models interpreted information coming environment arriving state. hence plays important role computation next states fact modeled explicitly framework still practically implemented. feature extraction function function encodes input sequence part example sequence-to-sequence models encoding rnn. given model hyperparameters ﬁxed policy equivalent conﬁguration model parameters. hence abuse notation denote parameter vector policy function. regular training procedure rnns treats true labels actions making forward passes. hence learning agent follows trajectories generated reference policy rather learned policy. words learns testing time longer access true labels predicted labels. words replace reference policy learned policy. learned policy mimic reference policy perfectly often case limited training examples discrepancy policies causes model suffer severely compounding errors. learning agent state never seen take wrong action leads completely unrecoverable trajectory. exactly problem encountered using naive supervised approach non-identically independently distributed data. dependencies training examples even explicit hidden state directly computed hidden state previous time step. order effectively learn policy performs well testing time apply algorithms training rnns. algorithm presents online dagger approach training rnns proposed previously name scheduled sampling method shown improve performances sequence-to-sequence models various tasks image captioning syntactic parsing speech recognition. training procedure differ much regular mini-batch training procedure rnns. difference that time step model stochastically determines whether true label predicted label compute next hidden state. probability using true labels decay time that training model trained almost entirely policy’s state distribution. important note predicted labels serve compute search states. examples used updating model parameters still carry true labels. words learning agent traverses search space using learned policy learning expert would visited states. formulation offers perspicuous interpretation notion search space framework. implied equation search state encodes action sequence taken reach therefore rather unknown given universe described previous works search space formulation explicitly represents composite encodings possible action sequences. besides that embedding search space vector space also allows reason geometrically. instance explicitly measure similarity states computing distance representation vectors respect geometric metric cosine distance. importantly learning continous search effective solution state sparsity problem thus potentially improve performance algorithms. adaptive dense continuous representation superior static sparse hand-crafted feature representation ways. first capable disentangling hidden non-linear dimensions sparse representation providing rigorous sense similiarities states. capturing similarity crucial dealing sparsity observed many machine learning problems neural networks yield superior performances. context imitation learning suppose learning agent encounters previously unknown state know ability relate would similar known states still offer approximated view current situation. second back-propagation training search space’s representation optimized simultaneously learned policy. hence beside control learned policy learning agent also gains control distribution states search space position states beneﬁcial decision-making process. allows learning agent improve predicting capability learning analyze situations better also learning realize situations better.", "year": 2016}