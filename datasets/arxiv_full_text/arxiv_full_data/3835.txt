{"title": "Compositional Sequence Labeling Models for Error Detection in Learner  Writing", "tag": ["cs.CL", "cs.NE", "I.5.1; I.2.6; I.2.7"], "abstract": "In this paper, we present the first experiments using neural network models for the task of error detection in learner writing. We perform a systematic comparison of alternative compositional architectures and propose a framework for error detection based on bidirectional LSTMs. Experiments on the CoNLL-14 shared task dataset show the model is able to outperform other participants on detecting errors in learner writing. Finally, the model is integrated with a publicly deployed self-assessment system, leading to performance comparable to human annotators.", "text": "paper present ﬁrst experiments using neural network models task error detection learner writing. perform systematic comparison alternative compositional architectures propose framework error detection based bidirectional lstms. experiments conll- shared task dataset show model able outperform participants detecting errors learner writing. finally model integrated publicly deployed self-assessment system leading performance comparable human annotators. automated systems detecting errors learner writing valuable tools second language learning assessment. work recent years focussed error correction error detection performance measured byproduct correction output however assumes systems able propose correction every detected error accurate systems correction might optimal detection. closed-class errors incorrect prepositions determiners modeled supervised classiﬁcation approach content-content word errors frequent error type pose serious challenge error correction frameworks evaluation error correction also highly subjective human annotators rather agreement gold-standard corrections therefore treat error detection learner writing independent task propose system labeling token correct common approaches similar sequence labeling tasks involve learning weights probabilities context n-grams varying sizes relying previously extracted high-conﬁdence context patterns. methods suffer data sparsity treat words independent units miss potentially related patterns. addition need specify ﬁxed context size therefore often limited using small window near target. neural network models address weaknesses achieved success various tasks language modeling speech recognition recent developments machine translation also shown text varying length represented ﬁxed-size vector using convolutional networks recurrent neural networks paper present ﬁrst experiments using neural network models task error detection learner writing. perform systematic comparison alternative compositional structures constructing informative context representations. based ﬁndings propose novel framework performing error detection learner writing achieves state-of-the-art results datasets errorannotated learner essays. sequence labeling model creates single variable-size network whole sentence conditions label words predicts labels together. effects different datasets overall performance investigated incrementally providing additional training data model. finally integrate error detection framework publicly deployed self-assessment system leading ﬁeld automatically detecting errors learner text long rich history. work focussed tackling speciﬁc types errors usage incorrect prepositions articles verb forms adjective-noun pairs however limited work general error detection systems could handle types errors learner text. chodorow leacock proposed method based mutual information chi-square statistic detect sequences part-of-speech tags function words likely ungrammatical english. gamon used maximum entropy markov models range features tags string features outputs constituency parser. pilot helping shared task also evaluated grammatical error detection number different error types though systems error-type speciﬁc best approach heavily skewed towards article preposition errors extend line research working towards general error detection systems investigate neural compositional models task. related area grammatical error correction also gained considerable momentum past years four recent shared tasks highlighting several emerging directions current state-of-the-art approaches broadly separated categories tagging classiﬁcation models deployed error detection. addition automated writing feedback systems indicate presence location errors better pedagogic point view rather providing panacea correcting errors learner text. section evaluate neural sequence tagging model latest shared task test data compare participating systems task error detection. construct neural network sequence labeling framework task error detection learner writing. model receives series tokens input outputs probability token sentence correct incorrect given context. architectures start vector representations individual words length sentence. different composition functions used calculate hidden vector representation token context representations passed softmax layer producing probability distribution possible labels every token context investigate alternative neural network architectures task error detection convolutional bidirectional recurrent bidirectional lstm multi-layer variants them. convolutional neural network token labeling hidden vector calculated based ﬁxed-size context window. convolution acts feedforward network using surrounding context words input therefore learn detect presence different types n-grams. assumption behind convolutional architecture memorising erroneous token sequences training data sufﬁcient performing error detection. figure alternative neural composition architectures error detection. convolutional network deep convolutional network recurrent bidirectional network deep recurrent bidirectional network. bottom layers embeddings individual tokens. middle layers context-dependent representations built using different composition functions. layers softmax output layers predicting label distribution every input token. deep convolutional network adds extra convolutional layer architecture using ﬁrst layer input. creates convolutions convolutions thereby capturing complex higher-order features dataset. nonlinear function sigmoid function. instead ﬁxed context window information passed sentence using recursive function network able learn patterns disregard pass forward. recurrent network structure referred elman-type network elman bidirectional consists recurrent components moving opposite directions sentence. unidirectional version takes account context left target token bidirectional version recursively builds separate context representations either side target token. left right context concatenated used hidden representation recurrent networks shown perform well task language modeling learn incremental composition function predicting next token sequence. however language models estimate probability token unable differentiate infrequent incorrect token sequences. error detection composition function needs learn identify semantic anomalies ungrammatical combinations independent frequency. bidirectional model provides extra information allows network context sides target token. irsoy cardie created extension architecture connecting together multiple layers bidirectional elman-type recurrent network modules. deep bidirectional calculates context-dependent representation token using bidirectional uses input another bidirectional rnn. multi-layer structure allows model learn complex higher-level features effectively perform multiple recurrent passes sentence. separate hidden vectors pass information different time steps includes gating mechanisms modulating output. lstms successfully applied various tasks speech recognition machine translation natural language generation comparison non-neural models also report results using crfs popular choice sequence labeling tasks. trained crf++ implementation dataset using features unigrams bigrams trigrams -word window surrouding target word predicted label also conditioned previous label sequence. current input previous hidden state biases previous internal state logistic function. internal state calculated based current input previous hidden state interpolated previous internal state using weights element-wise multiplication. finally hidden state calculated passing internal state tanh nonlinearity weighting values conditioned internal state opposed previous linear combination equation lstm less susceptible vanishing gradients time thereby able make longer context making predictions. addition network learns modulate itself effectively using gates predict operation required time step thereby incorporating higher-level features. order architecture error detection create bidirectional lstm making advanced features lstm incorporating context sides target token. addition experiment deep bidirectional lstm includes consecutive layers bidirectional lstms modeling even evaluate alternative network structures publicly released first certiﬁcate english dataset dataset contains short texts written learners english additional language response exam prompts eliciting freetext answers assessing mastery upperintermediate proﬁciency level. texts manually error-annotated using taxonomy error types. released test evaluation containing sentences leaving sentences training. separate sentences training development hyper-parameter tuning. dataset contains manually annotated error spans various types errors together suggested corrections. convert tokenlevel error detection task labeling token inside error span incorrect. order capture errors involving missing words error label assigned token immediately incorrect motivated intuition token correct considered isolation incorrect current context another token preceeded main evaluation measure error detection also measure adopted conll- shared task error correction combines precision recall assigning twice much weight precision since accurate feedback often important coverage error detection applications following chodorow also report counts predicted correct tokens. related evaluation measures -scorer i-measure convolutional networks window size either side target token produce -dimensional context-dependent vector. recurrent networks hidden layers size either direction. also added extra hidden layer size composition functions output layer allows network learn separate non-linear transformation reduces dimensionality compositional vectors. parameters optimised using gradient descent initial learning rate adam algorithm dynamically adapting learning rate batch size sentences. development evaluated epoch best model used ﬁnal evaluations. table contains results experiments comparing different composition architectures task error detection. lowest score compared neural models. memorises frequent error sequences high precision generalise sufﬁciently resulting recall. ability condition previous label also provide much help task possible labels errors relatively sparse. architecture using convolutional networks performs well achieves second-highest result test set. designed detect error patterns ﬁxed window words large enough require advanced composition functions. contrast performance bidirectional recurrent network somewhat lower especially test set. elman-type recurrent networks context signal distant words decreases fairly rapidly sigmoid activation function diminishing gradients. likely birnn achieves highest precision systems predicted label mostly inﬂuenced target token immediate neighbours allowing network detect short highconﬁdence error patterns. convolutional network uses context words equal attention able outperform bi-rnn despite ﬁxed-size context window. best overall result highest achieved bidirectional lstm composition model architecture makes full sentence building context vectors sides target token improves birnn utilising advanced composition function. application linear update internal cell representation lstm able capture dependencies longer distances. addition gating functions allow adaptively decide information include hidden representations output error detection. found using multiple layers compositional functions deeper network gave comparable slightly lower results composition architectures. contrast irsoy cardie experimented elman-type networks found improvements using multiple layers bi-rnns. differences explained task beneﬁting alternative features evaluation performed opinion mining target sequences longer phrases need identiﬁed based semantics whereas many errors learner writing short identiﬁed contextual mismatch. addition networks contain extra hidden layer output allows models learn higherlevel representations without adding complexity extra compositional layer. essentially inﬁnitely many ways committing errors text introducing additional training data alleviate problems data sparsity. experimented incrementally adding different error-tagged corpora training measured resulting performance. allows provide context results obtained using datasets gives estimate much annotated data required optimal performance error detection. datasets consider follows table contains results obtained incrementally adding training data bi-lstm model. found incorporating nucle dataset improve performance using fce-public dataset likely corpora containing texts different domains writing styles. texts written young intermediate students response prompts eliciting letters emails reviews whereas nucle contains mostly argumentative essays written advanced adult learners. differences datasets offset beneﬁts additional training data performance remains roughly same. contrast substantial improvements obtained introducing ielts datasets increasing score roughly ielts dataset contains essays proﬁciency levels mid-level english learners provides model distribution ‘average’ errors learn from. adding even training data figure also shows fce-public test function total number tokens training data. optimal trade-off performance data size obtained around million tokens introducing dataset. conll- shared task focussed automatically correcting errors learner writing. nucle dataset provided main training dataset participants allowed include annotated corpora external resources. evaluation students recruited write essays annotated experts. used methods section converting shared task annotation tokenlevel labeling task order evaluate models error detection. addition correction outputs participating systems made available online therefore able report performance task. order convert output error detection labels corrected sentences aligned original input using levenshtein distance changes proposed system resulted corresponding source words labeled errors. results annotations shared task test data seen table ﬁrst evaluated human annotators respect other order estimate upper bound task. average roughly shows task difﬁcult even human experts rather agreement. shown table provide error detection results participants shared task camb cuui preserve relative ranking also error detection evaluation. camb system lower precision highest recall also resulting highest cuui close performance slightly higher precision. ofﬁcial shared task susanto published system combines several alternative models outperforms shared task participants evaluated error correction. however error detection receives lower results ranking evaluated system detected small number errors high precision reach highest finally present results bi-lstm sequence labeling system error detection. using fce-public training overall performance rather training small contains texts different domain. however results show model behaves expected since seen similar language training labels large portion tokens errors. indicates network trying learn correct language constructions limited data classiﬁes unseen structures errors opposed simply memorising error sequences training data. trained datasets section model achieves highest systems conll- shared task test annotations absolute improvement previous best result. worth noting full bi-lstm trained data conll contestants. however shared task systems restricted nucle training submissions also used differing amounts training data various sources. addition conll systems mostly combinations many alternative models camb system hybrid machine translation rule-based system language model re-ranker; cuui consists different classiﬁers individual error type; p+p+s+s combination four different error correction systems. contrast bi-lstm single model detecting error types therefore represents scalable data-driven approach. section perform extrinsic evaluation efﬁcacy error detection system examine extent generalises higher levels granularity task automated essay scoring. speciﬁcally replicate experiments using text-level model described andersen currently deployed self-assessment tutoring system online automated writing feedback tool actively used language learners. system predicts overall score given text provides holistic assessment linguistic competence language proﬁciency. authors trained supervised ranking perceptron model fce-public dataset using features error-rate estimates language model various lexical grammatical properties text replicate experiment average probability token essay correct according error detection model additional feature scoring framework. system retrained fce-public evaluated correctly predicting assigned essay score. table presents experimental results. culated average inter-annotator correlation data existing system demonstrated levels performance close human assessors. nevertheless bi-lstm model trained fce-public complements existing features combined model achieves absolute improvement around percent corresponding relative error reduction respect human performance. even though bi-lstm trained dataset system already includes various linguistic features capturing errors error detection model manages further improve performance. bi-lstm trained available data section combination achieves substantial improvements. relative error reduction pearson’s correlation system actually outperforms human annotators spearman’s correlation. paper presented ﬁrst experiments using neural network models task error detection learner writing. alternative compositional network architectures modeling context evaluated. based ﬁndings propose novel error detection framework using token-level embeddings bidirectional lstms context representation multi-layer architecture learning complex features. structure allows model classify token correct incorrect using full sentence context. self-modulation architecture lstms also shown beneﬁcial allows network learn advanced composition rules remember dependencies longer distances. datasets. found largest beneﬁt obtained training million tokens text learners varying levels language proﬁciency. contrast including even data higher-proﬁciency learners gave marginal improvements. part future work would beneﬁcial investigate effect automatically generated training data error detection evaluated performance existing error correction systems conll- task error detection. experiments showed success error correction necessarily mean success error detection current best correction system best shared task detection system addition neural sequence tagging model specialised error detection able outperform participating systems. finally performed extrinsic evaluation incorporating probabilities error detection system features essay scoring model. even without additional data combination improved performance already close results human annotators. addition error detection model trained larger training essay scorer able exceed human-level performance.", "year": 2016}