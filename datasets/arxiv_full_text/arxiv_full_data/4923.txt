{"title": "Criticality & Deep Learning I: Generally Weighted Nets", "tag": ["cs.AI", "cs.LG"], "abstract": "Motivated by the idea that criticality and universality of phase transitions might play a crucial role in achieving and sustaining learning and intelligent behaviour in biological and artificial networks, we analyse a theoretical and a pragmatic experimental set up for critical phenomena in deep learning. On the theoretical side, we use results from statistical physics to carry out critical point calculations in feed-forward/fully connected networks, while on the experimental side we set out to find traces of criticality in deep neural networks. This is our first step in a series of upcoming investigations to map out the relationship between criticality and learning in deep networks.", "text": "motivated idea criticality universality phase transitions might play crucial role achieving sustaining learning intelligent behaviour biological artiﬁcial networks analyse theoretical pragmatic experimental critical phenomena deep learning. theoretical side results statistical physics carry critical point calculations feed-forward/fully connected networks experimental side traces criticality deep neural networks. ﬁrst step series upcoming investigations relationship criticality learning deep networks. various systems nature display patterns forms attractors recurrent behavior caused ubiquity systems similar statistical properties exhibit order lead term universality since phenomena show cosmology animals chemical physical systems landscapes biological preypredator systems endless many others furthermore universality turns simplistic mathematical models exhibit statistical properties parameters tuned correctly. suﬃces study n-particle systems simple atomistic components interactions since already exhibit many non-trivial emergent properties large limit. certain order parameters change behavior non-classical fashion speciﬁc noise levels. using rich deep knowledge gained statistical physics systems mathematical properties learn novel behaviors deep learning ups. speciﬁcally look collection units lattice various pair interactions; units curie-weiss model. physical point view basic analytically solvable models still possesses rich emergent properties critical phenomena. however given general mathematical structure model already used explain population dynamics biology opinion formation society machine learning many others systems rich diverse origination posses almost identical behavior criticality. latter case machine learning curie-weiss model encodes information fully connected feed-forward architectures ﬁrst order. similar work done insights ising models fully connected layers drawn applied architectures; natural link energy function autoencoder established. address generalisation fully connected system understand properties moving deep learning network applying techniques intuition. article organised follows section gives short introduction critical systems appropriate examples physics; section concrete non-linear feed forward physical counterpart discuss architectures well; turn investigating practical question whether spot traces criticality current deep learning nets finally summarise ﬁndings hint future directions rich statistical systems deep learning. critical phenomena ﬁrst thoroughly explained analysed ﬁeld statistical mechanics although observed various systems lacking theoretical understanding. study criticality belongs statistical physics incredibly rich wide ﬁeld hence brieﬂy summarise results interest present article; deﬁnitely much comprehensive coverage found e.g.. nutshell subject concerned behavior various macroscopic values interest inferred magnetisation system ﬁrst derivative wrt. called order parameter carries various denominations polarisation density opinion imbalance etc. depending system hand. basically measures arranged homogeneous system inﬂuence outside ﬁeld couples spins full treatment derivation model including critical behavior found equation state magnetisation recognise typical power laws ubiquitous critical systems. quantity interested though second derivative free energy wrt. basically -point correlation function spins again expanding second derivative free energy small couplings looking neighbourhood critical temperature yields systems neighbourhood critical points thus looks systems composed many identical particles trying derive properties macroscopic parameters density polarisation microscopic properties interactions particles; statistical mechanics hence understood bridge macroscopic phenomenology microscopic dynamics nutshell criticality achieved macroscopic parameters show anomalous divergent behavior phase transition. depending system hand parameters might magnetisation polarisation correlation density etc. speciﬁcally correlation function components displays divergent behavior signals strong coordinated group behavior wide range magnitudes. usually noise certain values induce phase transition accompanied critical anomalous behavior. given relevance physics also mathematical analogy deep learning networks brieﬂy review curieweiss model non-constant coupling examine behavior criticality. simplistic fully solvable model magnet curie-weiss model possesses many interesting features exhibits critical behavior correctly predicts experimental ﬁndings. mathematics later used deep learning setup brieﬂy present main properties solutions sake self-consistency. pairwise other long distances; inclusion factor multiplying quadratic spin term makes long-range interaction tractable large limit. furthermore directed external magnetic ﬁeld couples every spin since coupling spins constant since every spin interacts every spin hamiltonian rewritten hamiltonian describes layer containing rectiﬁed linear units b-layer common bias term weight matrix sums binary inputs coming depending whether relu threshold reached activated hence binary values allowed both inputs b-layer. displaying power behavior power coeﬃcient innocent looking equation actually tremendous consequences implies correlation simply restrict nearest neighbours goes long distances slowly decaying; further power behavior self-similar fractal patterns system islands equal magnetisation form within islands scales. also correlation diverges criticality point carry explicit calculations case interest non-constant matrix couplings later section main motivations look criticality exploit artiﬁcial networks universal arising phenomenon well various hints occurrence biological neural systems systems sizable enough gaining complexity critical behavior emerges also applies man-made nets various measures formulated detect criticality show power distribution behavior. world wide e.g. number links source number links away source exhibit power distribution uncovered various networks sizable enough citation behavior scientiﬁc articles social networks etc. simple generic metric detect criticality networks degree distribution deﬁned number links connecting node. further also correlation nodes nontrivial nodes similar degree higher probability connected nodes diﬀerent degree chapter vii. follow similar path proposed grow experimental network nodes simplest preferential directed attachment towards existing nodes function degree here denotes probability node grow link another node degree hence every node prefer nodes higher degrees leading overall power distribution observed real world systems. additional metrics look single neuron activity well layer activity pattern behavior; details section obtain quantities interest derivatives case respect partition function still contains product double integrals solved saddle point approximation; recall one-dimensional case order solve model analytically perform spins hindered quadratic term sisj. standard overcome problem gaussian linearisation trick replaces quadratic term square root linear additional continuous variable mean ﬁeld integrated entire unfortunately coupling scalar hence linearise term term keep track weight matrix entries. first insert identities dirac delta function hamiltonian used already stated could perform binary units since show linearly exponential change variables delta identity; eﬀectively converted binary values integrals leading general we’re interested numerical multiplicative constants later logging partition computing free energy terms simple additive constants without contribution diﬀerentiating expression combined deliver self consistency mean ﬁeld equation fully connected layer further denoting hamiltonian satisfying stationarity conditions reads used original equation taking derivatives. worth contemplating ﬁrst equations capture essence criticality system including it’s power behavior. weight matrix reduces scalar coupling equations reduce classical system display behavior shown furthermore encodes information needed ﬁnding critical point matrix system hand; recall already implicitly solved terms stationarity equation hence place holders functions we’re thus left non-linear system ﬁrst order diﬀerential equations variables produce poles speciﬁc values couplings temperature criticality. equation already displays manifestly consistency equation mean ﬁeld taking ﬁrst derivative wrt. leaves exactly consistency equation construction; need address large limit; obviously second term coming determinant clearly vanishes large-n limit logarithm slowly increasing divide ﬁrst term double order hence well deﬁned average limit; last term cosh expanded linear hence well deﬁned average dividing hence we’re left free energy investigating criticality partition function theoretical setup turn practical question current deep learning networks exhibit critical behaviour diﬀerently spot traces critical phenomena them? instead directly attacking partition function real world deep neural nets start practical observation systems around criticality show power distributions certain internal attributes. networks experiments cifar dataset training models epochs using relu activations adam optimizer without gradient clipping inferences epochs. feed forward network layers nodes convolutional layers followed fully connected layers autoencoder layer nodes. weight distributions looked sums absolute values outgoing weights node weighted order node. log-log plot counts versus node order deﬁned above detect linear behavior. layer activation patterns counted frequency layer activations inference epochs. figures log-log plots layer activation frequencies versus respective counts feedforward layer autoencoder. hidden layer activation pattern frequencies autoencoder resembles truncated straight line indicating learning hidden features unsupervised manner give rise scale free power phenomena accordance ﬁndings architectures show traces power law. concretely networks look traces power laws weight distributions layer activation pattern frequencies single node activation frequencies average layer activations. following present experimental results multilayer feedspontaneous detectable criticality arise classical architectures next step create experiment systems induced criticality learning rules take account criticality. ﬁrst approach grow fully connected using preferential attachment algorithm induce least power node weights fully connected hidden hidden module. experimented diﬀerent solutions regarding input read activations hidden hidden module without changing power distribution. ﬁndings show learning systems unstable without advancement learning inference. fundamental missing part naturally induce critical state network equipped learning rules inherently take account critical state. need architectures learning rules derived critical point equations summary article make ﬁrst steps investigating relationship criticality deep learning networks. short introduction criticality statistical physics real world networks started theoretical setup fully connected layer. used continuous mean ﬁeld approximation techniques tackle partition function system ending system diﬀerential equations determine critical behaviour system. equations starting point possible network architecture induced criticality learning rules exploiting criticality. after presented results experiments aiming traces power distributions current deep learning networks multilayer feed-forward nets convolutional networks autoencoders. results except autoencoder aﬃrmative negative sense setting next necessity create networks induced criticality learning rules exploit critical state. outlook obviously fully connected layer solved analytically theoretical side limited importance translates rather simplistic architecture; realistic widely used setups e.g. convolutional recurrent nets well contain feed-forward mechanism strongly deviating hence partially mapped theoretical treatment; would deﬁnitely essential address theoretically convolution mechanism deep understood collection terms corresponding unique combination vector length representing speciﬁc state spins; however conveniently written product binary summands contains exactly possible states spin product comes upper formula. expanding lowest order obtain notes place regarding matrix symmetric construction hence mediates equally sized layers; further higher terms cosh function even hence contributions higher order symmetric interactions layer itself. nets establish link theoretical experimental side; also inducing criticality could prove beneﬁcial might well aﬀect learning behavior surface loss function. right hand side original hamiltonian shifted coupling additional constant factors completely hence taking logarithm second derivative won’t change outcome. also note second derivative elad schneidman michael berry ronen segev william bialek weak pairwise correlations imply strongly correlated network states neural population arxivq-bio/", "year": 2017}