{"title": "Approximation errors of online sparsification criteria", "tag": ["stat.ML", "cs.CV", "cs.IT", "cs.LG", "cs.NE", "math.IT"], "abstract": "Many machine learning frameworks, such as resource-allocating networks, kernel-based methods, Gaussian processes, and radial-basis-function networks, require a sparsification scheme in order to address the online learning paradigm. For this purpose, several online sparsification criteria have been proposed to restrict the model definition on a subset of samples. The most known criterion is the (linear) approximation criterion, which discards any sample that can be well represented by the already contributing samples, an operation with excessive computational complexity. Several computationally efficient sparsification criteria have been introduced in the literature, such as the distance, the coherence and the Babel criteria. In this paper, we provide a framework that connects these sparsification criteria to the issue of approximating samples, by deriving theoretical bounds on the approximation errors. Moreover, we investigate the error of approximating any feature, by proposing upper-bounds on the approximation error for each of the aforementioned sparsification criteria. Two classes of features are described in detail, the empirical mean and the principal axes in the kernel principal component analysis.", "text": "resource-allocating networks kernel-based methods gaussian processes radial-basis-function networks require sparsiﬁcation scheme order address online learning paradigm. purpose several online sparsiﬁcation criteria proposed restrict model deﬁnition subset samples. known criterion approximation criterion discards sample well represented already contributing samples operation excessive computational complexity. several computationally efﬁcient sparsiﬁcation criteria introduced literature distance coherence babel criteria. paper provide framework connects sparsiﬁcation criteria issue approximating samples deriving theoretical bounds approximation errors. moreover investigate error approximating feature proposing upper-bounds approximation error aforementioned sparsiﬁcation criteria. classes features described detail empirical mean principal axes kernel principal component analysis. index terms—sparse approximation adaptive ﬁltering kernelbased methods resource-allocating networks gaussian processes gram matrix machine learning pattern recognition online learning sparsiﬁcation criteria. challenges area machine learning signal processing demanding online learning paradigm cannot addressed directly conventional learning machines resourceallocating networks kernel-based methods classiﬁcation regression gaussian processes radialbasis-function networks kernel principal component analysis name few. indeed machines share essentially underlying model many parameters estimated training samples deﬁned representer theorem model inappropriate online learning sample available instant. stay computationally tractable needs restrict incrementation model complexity selecting subset samples contributes reduced-order model approximation full-order feature estimated. order overcome bottleneck online learning sparsiﬁcation schemes proposed aforementioned machines deﬁned follows instant determines sample safely discarded contributing order growth model; otherwise sample needs take part order incrementation. known online sparsiﬁcation criteria approximation criterion also called approximate linear dependency. widely investigated literature gaussian processes kernel recursive least squares algorithm kernel least mean square algorithm kernel principal component analysis criterion determines relevance discarding accepting current sample comparing predeﬁned threshold residual error approximating representation samples nonlinearly mapped samples kernel methods already contributing model. crucial issue approximation criterion computational complexity scales cubically model order. several computationally efﬁcient sparsiﬁcation criteria introduced literature essentially computational complexity scales linearly model order. sparsiﬁcation criteria rely topology samples order select relevant samples. widely investigated criteria distance coherence criteria well babel criterion. distance criterion introduced platt control complexity resource-allocating networks radial-basis-function networks retains mutually distant samples also recent advances. coherence criterion introduced honeine richard bermudez recent advances compressed sensing retains samples mutually least coherent. extension coherence criterion babel criterion uses cumulative coherence measure diversity sparsiﬁcation criteria separately investigated literature. best knowledge work studies sparsiﬁcation criteria together. conducted analyses often based computational complexity advocated criticizing computational cost approximation criterion favor sparsiﬁcation criteria. developed colleagues several theoretical results allows compare coherence approximation criterion. results extended sparsiﬁcation criteria demonstrated particular case unitnorm data. paper presents framework study online sparsiﬁcation criteria cross-fertilizing previously derived results obtaining often tighter bounds extending results sparsiﬁcation criteria distance babel criteria. hand bridge approximation criterion online sparsiﬁcation criteria ﬁrstly providing upper bounds error approximating samples already retained sample discarded sparsiﬁcation criterion secondly providing lower bounds error approximating accepted samples. hand examine relevance approximating feature sparse model obtained feature space candidate solutions parameter controls tradeoff ﬁtness error regularity solution monotonically increasing function. examples loss functions quadratic loss |ψ−yi| hinge loss yi)+ logistic regression logyi)) well unsupervised loss function −|ψ| related pca. positive deﬁnite kernel induced reproducing kernel hilbert space inner product. reproducing property states function evaluated sample using κih. property shows sample represented space moreover reproducing property leads so-called kernel trick pair samples particular kκkh used kernels expressions follows among kernels gaussian kernel unit-norm kκkh sample kernels unit-norm restricted linear kernel dealing unit-norm samples. paper restrict particular kernel space denote representer theorem provides principal result essential kernel-based machines classiﬁcation regression well unsupervised learning. states solution optimization problem takes form proof theorem derived sketch proof given footnote. theorem shows optimal solution many parameters estimated number available samples result constitutes principal bottleneck online learning. indeed online setting solution adapted based sample available instant namely instant thus including pair training prove representer theorem decompose function hand evaluation independent since κih. hand monotonically increasing function guarantees pythagorean theorem used. therefore null minimizes regularization term without affecting ﬁtness term. reference known work reference recent work approximation sample error discarded samples error atom approximation feature error mean error principal axes aforementioned sparsiﬁcation criteria including approximation criterion. provide upper bounds error approximating feature general case. furthermore explore detail particular features empirical mean principal axes kernel principal component analysis picture cross-fertilization extensions given paper illustrated table remainder paper organized follows. next section introduces kernel-based machines online learning presents issues studied work. section presents aforementioned computationally efﬁcient sparsiﬁcation criteria. section investigates bounds error approximation samples either discarded accepted sparsiﬁcation criterion. results extended section problem approximating feature. section concludes document discussions. kernel-based machines online learning section introduce kernel-based machines online learning presenting approximation criterion issues studied paper. machine learning seeks feature connecting input space output space using training samples denoted considering loss function deﬁned measures error desired optimization output estimated problem consists minimizing regularized empirical risk form could also include removal process spirit ﬁxed-budget concept discarding atom well approximated atoms investigated instance nonetheless dictionary still δapproximate. removal process affect results given paper. approximation criterion requires inversion gram matrix associated dictionary computational expensive process. computational complexity scales cubically size dictionary i.e. operations. moreover evaluation condition expressed requires matrix multiplications instant. computation cost counteract beneﬁts several online learning techniques gradient-based least-mean-square algorithms reduce computational burden approximation criterion several computationally efﬁcient sparsiﬁcation criteria proposed literature sharing essentially computational complexity scales linearly size dictionary i.e. operations instant. known criteria distance coherence babel criteria; section description. approximation criterion relies establishing dictionary error approximating atoms linear combination atoms cannot smaller given threshold moreover decision discarding sample dictionary deﬁned process namely approximation error linear combination atoms smaller threshold approximation criterion possesses duality accepting discarding samples value thresholding approximation error case sparsiﬁcation criteria. predeﬁned dependent expression `xm} subset namely denote dictionary atoms elements. throughout paper quantities associated dictionary accent case instance m-by-m gram matrix whose entry eigenvalues matrix denoted given non-increasing order. instant corresponding parameters challenges arise online learning setting. determining optimal dictionary instant combinatorial optimization problem optimality measured comparing reduced-order solution feature full-order form elegant overcome computationally intractable problem recursive update determining kernel function needs included dictionary discarded since efﬁciently approximated atoms already belonging dictionary. essence approximation criterion. approximation criterion initially proposed classiﬁcation regression gaussian processes. online learning kernels studied system identiﬁcation recently kernel principal component analysis operates follows current sample discarded sufﬁciently represented linear combination atoms already belonging dictionary; otherwise included dictionary. formally kernel function included dictionary positive threshold parameter controls level sparseness. norm residual error obtained projecting onto space spanned dictionary. optimal value coefﬁcient obtained nullifying derivative cost function respect leads coherence measure extensively studied literature compressed sensing particular case linear kernel unit-norm samples general case kernel formalism coherence dictionary deﬁned measure coherence criterion introduced constructs dictionary atoms mutually least coherent restricting measure predeﬁned value null value yields orthogonal basis. criterion includes current kernel function dictionary coherence measure examines largest correlation pairs atoms dictionary thorough analysis provided babel measure considering maximum cumulative correlation atom atoms dictionary babel criterion online sparsiﬁcation deﬁned follows current kernel function included dictionary section bridge approximation criterion online sparsiﬁcation criteria. purpose hand section iv-a derive upper bounds error approximating discarded samples atoms dictionary obtained distance coherence babel criterion. hand section iv-b derive lower bounds error approximating atom atoms sparse dictionary scrutiny. aforementioned sparsiﬁcation criteria operate pre-processing scheme selecting samples independently resulting sparse representation feature. words selection relevant subset `xm} based topology samples; independent power dictionary approximate accurately feature form reduced-order model section study relevance approximating feature sparse dictionary obtained sparsiﬁcation criterion including approximation criterion. derive upper bounds approximation error feature examining detail particular class features empirical mean studied section relevant principal axes kernel-pca investigated section v-b. novel sample available instant sparsiﬁcation rule determines included dictionary incrementing model order setting sparsiﬁcation criteria measure relevance complexity-incrementation comparing current kernel function atoms dictionary. deﬁned either dissimilarity measure i.e. constructing dictionary mutually distant atoms similarity measure i.e. constructing dictionary least coherent correlated atoms. threshold used control level sparsity dictionary. investigated criteria outlined following. natural propose sparsiﬁcation criterion constructs dictionary large distances entries thus discarding samples close atoms already belonging dictionary. current kernel function included dictionary predeﬁned positive threshold otherwise efﬁciently approximated multiplicative constant atom dictionary. easy optimal value scaling factor since left-hand-side expression residual error projection onto given threshold quadratic error approximating linear combination atoms resulting dictionary upper-bounded secondly second expression upper bound trickier. approximation error given norm residual projection onto subspace spanned dictionary atoms namely given since satisfy condition section study elementary issue approximating sample span sparse dictionary kernel-based framework. issue considered folds hand error approximating discarded sample hand error approximating accepted sample namely approximating atom dictionary atoms. provide upper bounds former lower bounds latter sparsiﬁcation criteria studied previous section. worth noting approximation criterion relies duality discarding accepting samples value thresholding approximation error case criteria examined following. projection operator onto subspace spanned atoms dictionary resulting sparsiﬁcation criterion. thus sample projection kernel function onto subspace given `pκ. quadratic norm latter corresponds maximum inner product unit-norm functions subspace. writing also consider normalized version babel measure substituting κ/pκ deﬁnitions equivalent dealing unit-norm atoms. best knowledge formulation used literature. moreover looses matrix-norm notion. `k\\{i} -by- submatrix gram matrix obtained removing i-th i-th column i.e. entries associated `λ\\{i}m− smallest eigenvalue. combining inequalities expression investigated following distance coherence babel criteria. sparsiﬁcation criterion lower bound written using corresponding summation expression appropriate lower bound eigenvalues derived nutshell appendix. amely condition form satisﬁed namely theorem sample satisfying coherence condition given threshold quadratic error approximating linear combination atoms resulting dictionary upper-bounded theorem sample satisfying babel condition given threshold quadratic error approximating linear combination atoms resulting dictionary upper-bounded turns upper bound equal aforementioned particular choice hand eigenvalues gram matrix associated γ-babel dictionary upper-bounded given appendix; details. combination results concludes proof. summation term upper-bounded thanks babel deﬁnition moreover eigenvalue lower-bounded derived lemma appendix. concludes proof. theorem compared work authors propose lower bound quadratic approximation error unit-norm atoms easy lower bound given theorem tighter previously proposed bound extends result atoms unit-norm. section study relevance approximating feature projection onto subspace spanned atoms dictionary. upper bound approximation error derived following theorem sparsiﬁcation criterion speciﬁc bounds term threshold criterion given following theorem moreover results explored particular kernel-based learning algorithms empirical mean principal axes features estimated. theorem consider approximation feature sparse solution given projecting onto subspace spanned atoms given dictionary. quadratic error approximation upper-bounded upper bound approximation linear combination atoms dictionary. proof projection operator onto subspace spanned atoms dictionary scrutiny approxij= `αjκ. second inequality follows coherence condition. hand lower bound eigenvalues associated γ-coherent dictionary atoms derived lemma appendix. complete proof combine results theorem sparsiﬁcation criterion seen dimensionality reduction technique identiﬁes subspace selecting relevant samples available ones. since unsupervised approach natural connect kernel principal component analysis. sake clarity assumed data centered feature space; connections uncentered case. principal component analysis seeks principal axes capture data variance. principal axes correspond eigenvectors associated largest eigenvalues covariance matrix. kernel-based counterpart i.e. kernel-pca k-th principal axis takes coefﬁcients entries k-th eigenvector gram matrix moreover unit-norm principal axes coefﬁcients /nλk. expression k-th eigenvalue gram matrix also called principal value. following highlight connections kernel-pca online sparsiﬁcation criteria. theorem k-th principal kernel functions associated eigenvalue corresponding gram matrix. approximation dictionary kernel functions quadratic error upper-bounded upper bound approximation linear combination atoms dictionary. proof theorem straightforward substituting /nλk theorem theorem shows that condition used dictionary upper bound error approximating kernel function principal axes associated largest principal values smallest approximation errors. therefore relevant principal axes small error span sparse dictionary. discarded thanks used sparsiﬁcation criterion. former contribute error latter take part summation namely discarded samples size dictionary. upper bound quadratic error discarding samples given section iv-a. then revisiting upper bounds given section iv-a sparsiﬁcation criterion easily show following results. expressions non-unit-norm atoms derived without difﬁculty theorems empirical mean fundamental feature sample essential many statistical methods. instance investigated visualization clustering nonnegative data one-class classiﬁcation kernel-based methods. following study relevance approximating empirical mean projection onto subspace spanned atoms dictionary. empirical mean namely theorem indeed provide sharper bound relaxing cauchy-schwarz inequality thanks fact coefﬁcients constant i.e. independent consequence revisiting expression followed decomposition proof theorem discarded samples contribute summation term. therefore quadratic approximation error upper-bounded follows results generalize previous work approximation coherence criteria provide tighter bounds ones previously known literature. indeed upper bound δ/λk derived approximation criterion coherence criterion studied upper bound /λk. paper studied approximation errors sample dealing distance coherence babel criterion revealing criteria roughly based approximation process. deriving upper bound error approximating sample discarded dictionary explored atoms sufﬁcient represent sample. dual condition namely showing atom dictionary necessary also exhibited providing lower bound approximation atom dictionary atoms. moreover beyond analysis single sample extended results estimation feature describing detail classes features empirical mean principal axes kernel-pca. work devise particular sparsiﬁcation criterion. provided framework study online sparsiﬁcation criteria. argued criteria behave essentially identical mechanism share many interesting desirable properties. without loss generality considered framework kernel-based learning algorithms. worth noting machines intimately connected gaussian processes approximation criterion initially proposed appendix provides bounds eigenvalues gram matrix associated sparse dictionary sparsity measures investigated paper. completeness bounds nutshell details. cornerstone results wellknown gerˇsgorin discs theorem revisited gram matrix associated sparse dictionary states eigenvalues lies union discs centered diagonal entry radius given absolute values noumir honeine richard simple one-class classiﬁcation methods proc. ieee international symposium information theory usa) july jenssen mean vector component analysis visualization clustering nonnegative data neural networks learning systems ieee transactions vol. gilbert muthukrishnan strauss tropp improved sparse approximation quasi-incoherent dictionaries international conference image processing vol. sept. paul honeine born beirut lebanon october received dipl.-ing. degree mechanical engineering m.sc. degree industrial control faculty engineering lebanese university lebanon. received ph.d. degree systems optimisation security university technology troyes france postdoctoral research associate systems modeling dependability laboratory since september assistant professor university technology troyes france. research interests include nonstationary signal analysis classiﬁcation nonlinear statistical signal processing sparse representations machine learning. particular interest applications sensor networks biomedical signal processing hyperspectral imagery nonlinear adaptive system identiﬁcation. co-author best paper award ieee workshop machine learning signal processing. past years published peerreviewed papers. sch¨olkopf herbrich smola generalized representer theorem proc. annual conference computational learning theory european conference computational learning theory colt/eurocolt springer-verlag", "year": 2014}