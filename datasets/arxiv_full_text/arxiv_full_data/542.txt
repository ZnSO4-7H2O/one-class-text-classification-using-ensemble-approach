{"title": "Detecting Off-topic Responses to Visual Prompts", "tag": ["cs.CL", "cs.LG", "cs.NE", "I.2.7; I.2.6; I.5.1"], "abstract": "Automated methods for essay scoring have made great progress in recent years, achieving accuracies very close to human annotators. However, a known weakness of such automated scorers is not taking into account the semantic relevance of the submitted text. While there is existing work on detecting answer relevance given a textual prompt, very little previous research has been done to incorporate visual writing prompts. We propose a neural architecture and several extensions for detecting off-topic responses to visual prompts and evaluate it on a dataset of texts written by language learners.", "text": "automated methods essay scoring made great progress recent years achieving accuracies close human annotators. however known weakness automated scorers taking account semantic relevance submitted text. existing work detecting answer relevance given textual prompt little previous research done incorporate visual writing prompts. propose neural architecture several extensions detecting off-topic responses visual prompts evaluate dataset texts written language learners. evaluating relevance learner essays respect assigned prompt important part automated writing assessment existing systems able assign high-quality assessments based grammaticality known vulnerable memorised off-topic answers critical weakness high-stakes testing. addition students limited relevant vocabulary shift topic answer familiar direction automated assessment systems able capture. solutions detecting topical relevance help prevent weaknesses provide informative feedback students. setting students asked write short description image order assess language skills would like automatically evaluate semantic relevance answers. intuitive method comparing multiple modalities shared distributed space semantically similar entities mapped similar vector representations regardless information source. frome used principle improve image recognition ﬁrst training separate visual textual components mapping images space word embeddings. performed information retrieval tasks related model based convolutional networks. klein learned associate word embeddings images using fisher vectors. paper start similar architecture based approach used kiros image caption generation propose modiﬁcations make model suitable discriminating relevant irrelevant answers. framework uses lstm text composition pre-trained image recognition model extracting visual features. representations mapped space prediction made relevance text given image. propose novel gating component decides parts image considered current similarity calculation based ﬁrst reading input sentence. application dropout word embeddings visual features helps increase robustness otherwise noisy dataset assisted regularising model. finally standard loss function replaced version crossentropy encouraging model jointly optimise batches. evaluate dataset short answers language learners written response visual prompts experiments show perforautomated methods scoring essays short answers made great progress recent years achieving accuracies close human annotators. however known weakness automated scorers taking account topical relevance submitted text. students limited language skills attempt shift topic response familiar direction automated systems would able detect. high-stakes examination framework weakness could exploited memorising grammatically correct answer presenting response prompt. able detect topical relevance help prevent weaknesses provide useful feedback students also step towards evaluating creative aspects learner writing. existing work detecting answer relevance given textual prompt limited previous research done extend visual prompts. recent work investigated answer relevance visual prompts part automated scoring systems reduced problem textual similarity task relying hand-written reference descriptions image without directly incorporating visual information. proposed relevance detection model takes image sentence input assigns score indicating relevant image text. formulating scoring problem instead binary classiﬁcation allows treat model output conﬁdence score classiﬁcation threshold selected later stage based speciﬁc application. kiros describe supervised method mapping image sentence space allows generate similar vector representations images semantically similar descriptions. base approach multimodal relevance scoring architecture introduce several modiﬁcations order adapt task discriminating outline framework seen figure input sentence ﬁrst passed long short-term memory component mapping vector representation visual features input image extracted using model trained image recognition. visual representation conditioned input sentence mapped vector representation given input function predicts conﬁdence score answer relevant image. next sections describe components detail. next apply dropout word embeddings sentence. dropout method regularising neural networks shown provide performance imrovements. neuron activations layer zero probability preventing model excessively relying presence speciﬁc features. process also thought training randomly constructed smaller network training iteration resulting full combination model. test time values retained scaled compensate difference. dropout commonly applied weights inside network also recent work deploy dropout directly word embeddings relevance scoring model needs handle texts different domains including error-prone sentences language learners dropout embeddings allows introduce robustness training process. lstm component processing word embeddings building sentence representation. similar traditional recurrent neural network specialised gating functions allow dynamically decide information carry forward forget. lstm calcufigure outline relevance detection model. input sentence image mapped vector representations using modality-speciﬁc functions. vectors given relevance function assigns real-valued score based similarity. last hidden representation calculated based words sequence thereby allowing model iteratively construct semantic representation whole sentence. vector represent given input sentence relevance scoring model. since wordlevel processing ideal handling spelling errors learner texts future work could also investigate character-based extensions text composition described wieting order images feature vectors pretrained image recognition model combined supervised transformation component. make bvlc googlenet image recognition model based architecture described szegedy provided caffe toolkit googlenet layer deep convolutional network trained imagenet data detect different image classes. input image passed network probability distribution possible classes produced. instead using output layer extract neuron activations second-to-last layer network takes advantage visual feature processing various levels network retains general distributed representation image compared using output layer. similarly word embeddings textual composition apply dropout probability directly image vectors introduces variance otherprevious process maps image dimensional vector contains useful visual information optimised relevance scoring task. introduce gating component modulates image vector based textual vector representation input sentence. vector gating weights calculated nonlinear weighted transformation sentence vector indicates element-wise multiplication. architecture allows model ﬁrst read input sentence determine look corresponding image block irrelevant information image vector. also disconnect backpropagation vector gating weights forces model optimise score prediction leaving specialise handling gating. finally pass image representation fully connected non-linear layer allows model transform pre-trained googlenet space representation specialised relevance scoring transition cosine dot-product required order facilitate scoring function. setting scoreexp acts softmax layer requiring input values unbounded functioning correctly whereas cosine would restrict values range scoring function based softmax encourages model distinguish relevant irrelevant images. hinge loss function also optimised minibatches independently optimises relevance score training pair whereas softmax connects scores pairs probability distribution. distribution optimised using cross-entropy speciﬁcally focuses instances incorrectly relatively high scores compared pairs dataset. addition optimising towards larger score known correct example also reduces scores pairs batch. given image text written response image goal system assign score return decision relevance text. evaluate framework experimental dataset collected english proﬁle containing answers written language learners response visual prompts form photographs. part instructions students able select image wanted write about free choose write. length collected answers ranges sentences. scoring optimisation based vector representations input sentence image want assign score indicates related are. kiros used cosine measure similarity function measures angle vectors returning value range commonly used similarity calculations language processing model optimised predict high score image-sentence pairs image sentence related score randomly constructed pairs. loss function hinge loss margin score difference positive negative example greater training required otherwise error backpropagated weights updated accordingly related image-text pairs training randomly constructed pairs entry generating negative examples make sure resulting contain examples image otherwise model would accidentally optimise related examples towards score. dataset contains real-world examples task visual relevance detection therefore also proposes range challenges. answers provided students various stages learning english means texts contain numerous writing errors. spelling mistakes prevent model making full word embeddings previously unseen grammatical mistakes cause trouble lstm composition function. students also interpreted open writing task various different ways answered describing content image others instead talked personal memories triggered image even created short ﬁctional story inspired photo. answers vary quite writing style vocabulary size sentence length. ideally would like train model examples pairs images sentences speciﬁcally annotated semantic relevance. however since collected dataset large enough training neural networks make flickrk dataset contains implicitly relevant pairs images corresponding descriptions. flickrk image captioning dataset containing images hand-written sentences describing image. splits karpathy training development; dataset sizes shown table training model presented sentences corresponding images batch making sure images within batch unique. loss function section minimised maximise predicted scores relevant pairs minimise scores random combinations. theano used implement neural network model. texts tokenised lowercased sentences padded special markers start positions. vocabulary includes words appeared training least twice plus extra token unseen words. words represented -dimensional embeddings initialised publicly available vectors trained cbow parameters initialised random values normal distribution mean standard deviation trained epochs measuring performance development every full pass data used best model evaluating test set. parameters optimised using gradient descent initial learning rate adam algorithm dynamically adapting learning rate training. dropout applied word embeddings image vectors order avoid outlier results randomness model affects random initialisation sampling negative image examples trained conﬁguration different random seeds present averaged results. evaluate visual relevance detection model training flickrk testing dataset learner responses visual prompts. order handle multiple sentences written responses every sentence ﬁrst scored individually scores averaged sentences. every textual answer dataset create negative datapoint pairing random image. task accurately detect whether pair truly relevant randomly created assigning high relevance scores. order convert model output binary classiﬁcation employ leave-one-out optimisation example time used testing others used calculate optimal threshold accuracy. also report average precision precision detecting irrelevant answers returned instances measure quality ranking require ﬁxed threshold. results different system architectures seen table baseline lstmcos system based framework kiros uses lstm composing sentence vector calculates relevance score ﬁnding cosine similarity sentence vector image vector optimises model using hinge loss function. model already performs relatively well able distinguish relevant random image-text pairs accuracy. picture people different attitude. foreground people waiting green light order cross street. child talking adult something side road instead women lots left hand chatting mobile telephone. generally speaking picture full bright colours conveys idea crowded city. looking pictures reminds time went scuba diving sea. it’s fascinating surrounded water ﬁshes everything seems coulorful adventurous. another good part diving coming swim surface sunlight coming nearer nearer breathe real again. table predicted scores best relevance scoring model given example sentences learner dataset included photo prompt. ﬁrst sentences written response image whereas last written different photo. gating architecture described section vector representation text used calculate dynamic mask applied image vector. allows model ﬁrst read sentence decide parts image important similarity calculation. inclusion gating component improves accuracy average precision next change scoring optimisation functions described section cosine similarity measure substituted product between vectors removing useful bounds score allowing ﬂexibility model. addition hinge loss function exchanged calculating negative cross-entropy softmax. hinge loss performs pairwise comparisons applies sharp cut-off softmax ties examples probability distribution provides gradual prioritisatable results different system conﬁgurations flickrk development test sets. report accuracy average predicted scores positive negative examples. finally apply dropout probability -dimensional word embeddings input sentence -dimensional image representation produced bvlc googlenet. randomly setting half values training additional variance introduced available data model becomes robust handling noisy learnergenerated text. integrating dropout improves performance average precision table contains examples predicted scores ﬁnal model given example sentences written language learners. sentences model successfully distinguishes relevant irrelevant topics assigning lower scores last sentences describe different image. however model also makes mistake incorrectly assigns score third sentence likely happens sentence much longer convoluted examples training data leading lstm lose important information sentence representation. comparison also evaluate system architectures flickrk dataset table setting present model sentence images flickrk test known relevant others selected randomly. accuracy measured proportion test cases model chooses correct image relevant one. random baseline chance ﬁnding correct image input sentence negative examples every positive example. also report average scores assigned models positive negative pairs images sentences. seen averaged predicted scores table ﬁnal system free push positive negative examples apart larger margin increasing average score difference order magnitude. figure contains predicted scores different images given example sentences input. seen system returns high scores sentences paired relevant images also offers intuitive grading relevance. ﬁrst sentence describing orange shirt bicycle model assigned reasonably high scores images containing bikes orange objects. similarly second sentence system found alternative images containing dogs wooden ﬂoors. order analyse possible weaknesses model manually examined cases difﬁcult system. figure contains examples flickrk development valid image-description pair received negative score relevance model. negative score necessarily mean error depends chosen threshold indicates model conﬁdence correct pairing. rare terms source confusion model word used training data sufﬁciently make relevance calculation difﬁcult. example unicycle batons relatively rare terms cause confusion example addition description mentions photo depicts crowd building. alternative source confusion comes visual component googlenet trouble certain images. image-sentence pairs development data best model assigned negative scores those unique image indicating visual component trouble detecting content certain unusual images examples regardless textual composition. issues represent cases model faced input substantially different training examples therefore fails perform well possible. remedied either creating models able generalise better unseen examples expanding sources available training data. also analysed gating component conditioned text vector applied image vector. calculation gating weights includes bias term logistic function means could easily adapt always predicting vector effectively leaving image vector unmodiﬁed. instead found model actively makes additional architecture choosing switch many features image vector. figure shows visualisation gating weights example sentences used figure values close represented white values close shown blue. seen quite features receive weights close zero therefore effectively turned off. addition sentences fairly different gating signatures demonstrating weights calculated dynamically based input sentence. presented system mapping images sentences shared distributed vector space evaluating semantic similarity. task motivated applications automated language assessment scoring systems focusing grammaticality otherwise vulnerable model starts learning embeddings words input sentence composing vector representation using lstm. parallel image ﬁrst passed pre-trained image detection model extract visual features supervised layer transform representation suitable space. found applying dropout word embeddings visual features allowed model generalise better providing consistent improvements accuracy. next introduced novel gating component ﬁrst reads input sentence decides visual features image pipeline important speciﬁc sentence. found model actively makes component predicting different gating patterns depending input sentence substantially improves overall performance evaluations. finally moved pairwise hinge loss optimising probability distribution possible candidates found improved relevance accuracy. experiments performed different datasets collection short answers written language learners response visual prompts image captioning dataset pairs single sentences photos. relevance assessment model able distinguish unsuitable image-sentence pairs datasets model modiﬁcations showed consistent improvements tasks. conclude automated relevance detection short textual answers visual prompts performed mapping images sentences distributed vector space potentially useful addition preventing off-topic responses automated assessment systems.", "year": 2017}