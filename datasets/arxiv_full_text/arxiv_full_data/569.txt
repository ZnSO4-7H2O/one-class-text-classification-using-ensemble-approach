{"title": "Lexical Translation Model Using a Deep Neural Network Architecture", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "In this paper we combine the advantages of a model using global source sentence contexts, the Discriminative Word Lexicon, and neural networks. By using deep neural networks instead of the linear maximum entropy model in the Discriminative Word Lexicon models, we are able to leverage dependencies between different source words due to the non-linearity. Furthermore, the models for different target words can share parameters and therefore data sparsity problems are effectively reduced.  By using this approach in a state-of-the-art translation system, we can improve the performance by up to 0.5 BLEU points for three different language pairs on the TED translation task.", "text": "model improve translation quality different conditions maxent models linear classiﬁers. hand hierarchical non-linear classiﬁers model dependencies different source words better since perform abstraction input. hence introducing non-linearity modeling lexical translation could improve quality. moreover since many pairs source target words co-occur rarely sharing information different classiﬁers could improve modeling well. order address issues developed discriminative lexical model based deep neural networks. since train neural network target words multivariate binary classiﬁer model share information different target words. furthermore probability longer linear combination weights depending surface source words. thanks non-linearity able exploit semantic dependencies among source words. paper organized follows. section review previous works related lexical translation methods well translation modeling using neural networks. describe approach including network architecture training procedures section section provides experimental results translation systems different language pairs using proposed lexical translation model. finally conclusions drawn section since beginnings several approaches increase context used lexical decisions presented. moving word-based phrase-based step employing wider contexts translation systems made. pbmt lexical joint models allow local source target contexts form phrases. lately advanced joint models proposed either enhance joint probability model source target sides engage suitable contexts. paper combine advantages model using global source sentence contexts discriminative word lexicon neural networks. using deep neural networks instead linear maximum entropy model discriminative word lexicon models able leverage dependencies different source words non-linearity. furthermore models different target words share parameters therefore data sparsity problems effectively reduced. since ﬁrst attempt statisical machine translation approach drawn much interest research community huge improvements translation quality achieved. still plenty problems addressed. translation decision depends quite small context. standard phrase-based statistical machine translation main components translation language models. translation model modeled counting phrase pairs sequences words extracted bilingual corpora. using phrase segments instead words pbmt exploit local source target contexts within segments. context information outside phrase pairs used. n-gram language model context target words considered. several directions proposed leverage information wider contexts phrase-based framework. example discriminative word lexicon exploits occurence words whole source sentence predict presence words target sentence. wider context information encoded features employed discriminative framework. hence train maximum entropy model target word. standard phrase-based n-gram translation models basically built upon statistical principles maximum entropy smoothing techniques. recently joint models learned using neural networks non-linear translation relationships semantic generalization words performed follow n-gram translation direction model conditional probability target word given history bilingual phrase pairs using neural network architecture. model k-best rescorer instead n-gram decoder. devlin longer source contexts renew joint formula included decoder rather k-best rescoring module. schwenk calculate conditional probability target phrase instead target word given source phrase. although aforementioned works essentially augment joint translation model inherent limitation exploit local contexts. estimate joint model using sequences words basic unit. hand several approaches utilizing global contexts. motivated bangalore hasan calculate probability target word given source words necessarily belong phrase. mauser suggest another lexical translation approach named discriminative word lexicon concentrating predicting presence target words given source words. niehues extend model employ source target contexts used maxent classiﬁer task. carpuat similar work direction terms using whole source sentence perform lexical choices target words. treat selection process word sense disambiguation task target words phrases senses. extract rich feature source sentences including source words input classiﬁer. still problem persists since shallow classiﬁers task. considering advantages non-linear models mentioned before using deep neural network architectures learn dwl. take advantages directions. side model uses non-linear classiﬁcation method leverage dependencies different source sentences well semantic generalization ability. side employing global contexts model complement joint translation models local contexts. approach modeled using maximum entropy model determine probability using target word translation. therefore individual models every target word trained. model trained return probability word given input sentence. input model source sentence thus need represent input sentence. done representing sentence words thereby ignoring order words. maxent model indicator feature every input word. formally given source sentence represented features source vocabulary model approximates probability target word given source sentence discuss alternative method using neural network estimate probabilities next section. source context considered sentence longer represented words ngrams. using representation could integrate order information words dimension input space increased. also adapt extension model encoding bigrams trigrams ordinary words source vocabulary. input output neural network-based source target sentences would like learn lexical translation relationship. original approach represent source sentence binary column vector {|}|vs| considered vocabulary source corpus. source word neural network training instance comprised sentence pair maximize similarity conditional probability either depending appearance corresponding word target sentence neural network operates multivariate classiﬁer gives probabilistic score binary decision independent variables appearances target words. minimize cross entropy error function binary target sentence vector appears sentence value corresponding index otherwise. hence source sentence representation sparse vector depending considered vocabulary representation scheme applied target sentence sparse binary column vector considered target vocabulary figure depicts main neural network-based architecture learning lexical translation feedforward neural network three hidden layers. matrix rvs×|h| connects input layer ﬁrst hidden layer. matrices r|h|×|h| r|h|×|h| encodes learned translation mapping compact global feature spaces source target contexts. matrix r|h|×|vt| computes lexical translation output. number units ﬁrst second third hidden layers respectively. lexical translation distribution words target sentence given source sentence computed forward pass investigate impact network conﬁguration built simpler architecture hidden layer featuring translation relationship source target sentences. refer simnndwl comparison section later. in-house phrase-based decoder used search best solutions among translation hypotheses optimization features depending settings performed using minimum error rate training weights optimized tested separate sets talks. development consists sentences containing words. test consists sentences containing words. investigate impact approach employing different conﬁgurations neural networks described details following section. evaluate conﬁgurations english→french also english→chinese german→english similar translation system setups. nndwl models trained small subset mentioned training corpora mainly data. although corpus quite small compared overall training data important since matches best test data. order speed process testing different conﬁgurations therefore train nndwl corpus except comparison reported section statistics training validation data nndwl shown table main neural network architecture proposed sizes hidden layers respectively. original source target vocabularies english→french direction trained preprocessed data includes words includes words. non-linearity calculations large network training extremely time-consuming. order boost efﬁciency limit source target vocabularies frequent ones. words outside lists treated unknown words. vary size considered vocabularies values keeping sizes hidden layers preliminary experiments layout lead best performance. used layout remaining paper. calculation problem occurs source contexts even seriously curse dimentionality. hence applied cut-off scheme source-side bigrams trigrams most-frequent bigram trigram numbers multiply probability word even word occurs several times sentence. models translation system however restrict overusing particular word. furthermore keep track words whose probabilities calculated already additional book keeping would required. order avoid difﬁculties come following approximation given length target sentence order speed calculation target word probabilities pre-calculate probabilities given source sentence prior translations. naive approach would need pre-calculate probabilities possible target words given source sentence. would lead slow calculations. therefore ﬁrst deﬁne target vocabulary source sentence vocabulary comprised respective words phrase pairs matching source sentence. using deﬁnition need precalculate probabilities words target side phrase table target words whole corpus. calculate score every phrase pair even starting translation. system baseline state-of-the-art translation system english french without dwl. baseline system several components trained different corpora independent features log-linear framework utilized in-house phrase-based decoder. system trained epps common crawl giga corpora talks. monolingual data used train language models includes corresponding monolingual parts parallel corpora plus news shufﬂe gigaword. data preprocessed phrase table built using scripts moses package adapt general corpora in-domain data using backoff approach described adaptation also conducted monolingual data. train -gram language model using srilm toolkit addition several non-word language models included capture dependencies source target words reduce impact data sparsity. bilingual language model described well cluster language model based word classes generated mkcls algorithm short-range reordering performed preprocessing step described training proposed architecture gradient descent batch size learning rate used. gradients calculated averaging across minibatch training instances process performed epochs. epoch current neural network model evaluated separate validation model best performance utilized calculating lexical translation scores afterwards. regularize models regularizer. alternative also experiment dropout technique neurons last hidden layer randomly dropped probability however help indicated performance system later. training done gpus using theano toolkit. report results using different nndwl conﬁgurations mainly english→french translation system. also report results using best conﬁgurations language pairs. varying vocabulary sizes source target sentences helps dramatically reduce neural network training time also affects translation quality. experiments neural networks -mostfrequent-word vocabularies show biggest improvements around bleu points translating english french. perform better using maximum entropy approach nndwl whole source target vocabularies. nndwl models achieve notable bleu gains compared strong baseline worse original maxent model. might fact original maxent model uses source contexts whereas nndwl models uses source words. nndwl model -most-frequent-word vocabularies including source contexts helps cases harm translation performance cases. most-frequent bigrams mostfrequent trigrams achieve best improvements bleu points baseline. gains adding source contexts vocabulary-size nndwl model clearly observed case -vocabulary-size model. might indicate numbers source contexts proportional somehow size vocabularies. compare main architecture simpler architecture simnndwl consisting -unit hidden layer. simnndwl trains faster translation time performance signiﬁcantly affected. since decreases bleu score using simnalso train nndwl models bigger corpus concatinating epps ted. results table shows using bigger corpus improve translation quality. models trained in-domain data only i.e. perform similar better models trained data broader domains. observation also holds true original maxent models reported conducted experiments nndwl models mainly english-to-french translation system order investigate impact method strong baseline. however would like inspect effect language pairs long-range dependencies differences word order. purpose built similar nndwl models integrate translation systems language pairs. tables show results english→chinese german→english respectively. case english→chinese direction nndwl signiﬁcantly improves translation quality increment bleu points baseline. best bleu gain comes nndwl most-frequent-word vocabularies source contexts containing bigrams trigrams. case german→english direction nndwl also helps gain bleu points baseline best model however improvements notably different compared original maxent dwl. paper described deep neural network approach modeling integration standard phrase-based translation system. using neural networks non-linear classiﬁer enables ability learning abstract representation global contexts dependencies. investigated various network conﬁgurations different language pairs. deployed best nndwl model feature decoder helps improve bleu points compared strong baseline. nndwl require linguistic resources feature engineering. thus easily ported languages. furthermore probability calculation done preprocessing step. therefore model would signiﬁcantly slow translation process. although feature linguistic resources nndwl useful modeling translation probability languages avalaible. future work integrate linguistic features model. moreover context vector words might helpful reducing data sparseness problem. mauser hasan extending statistical machine translation discriminative trigger-based lexicon models proceedings conference empirical methods natural language processing volume volume ser. emnlp singapore niehues waibel error-driven discriminative word lexicon using sentence structure features proceedings eighth workshop statistical machine translation niehues herrmann vogel waibel wider context using bilingual language models machine translation proceedings sixth workshop statistical machine translation edinburgh scotland devlin zbib huang lamar schwartz makhoul fast robust neural network joint models statistical machine translation proceedings association computational linguistics baltimore bangalore haffner kanthak statistical machine translation global lexical selection sentence reconstruction proceedings acl) prague czech republic hasan ganitkevitch andr´es-ferrer triplet lexicon models statistical machine translation proceedings conference empirical methods natural language processing. association computational linguistics cettolo girardi federico inventory transcribed translated talks year proceedings conference european association machine translation trento italy koehn hoang birch callison-burch federico bertoldi cowan shen moran zens dyer bojar constantin herbst moses open source toolkit statistical machine translation demonstration session prague czech republic june niehues waibel detailed analysis different strategies phrase table adaptation proceedings tenth conference association machine translation america hinton srivastava krizhevsky sutskever salakhutdinov improving neural networks preventing co-adaptation feature detectors arxiv preprint arxiv. bergstra breuleux bastien lamblin pascanu desjardins turian warde-farley bengio theano math expression compiler proceedings python scientiﬁc computing conference june", "year": 2015}