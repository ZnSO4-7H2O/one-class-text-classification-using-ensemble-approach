{"title": "Universal Learning of Repeated Matrix Games", "tag": ["cs.LG", "cs.AI"], "abstract": "We study and compare the learning dynamics of two universal learning algorithms, one based on Bayesian learning and the other on prediction with expert advice. Both approaches have strong asymptotic performance guarantees. When confronted with the task of finding good long-term strategies in repeated 2x2 matrix games, they behave quite differently.", "text": "study compare learning dynamics universal learning algorithms based bayesian learning prediction expert advice. approaches strong asymptotic performance guarantees. confronted task ﬁnding good long-term strategies repeated matrix games behave quite diﬀerently. today data mining machine learning typically treated problem-speciﬁc people propose algorithms solve particular problem prove properties performance guarantees algorithms evaluate algorithms real data afterwards real-world applications. contrast seems universal learning i.e. single algorithm applied problems neither feasible terms computational costs competitive performance. nevertheless understanding universal learning important hand practical success would lead artiﬁcial intelligence. hand principles ideas universal learning immediate course machine learning research aims exploring establishing general concepts algorithms. reinforcement learning split trees among others. thorough discussion e.g. paper concentrate approaches strong theoretical guarantees limit agent based bayesian learning based prediction expert advice models work setup sequential decision problem agent interacts environment discrete time time step agent action receives feedback environment. feedback consists loss plus maybe information. addition instantaneous loss also consider cumulative loss instantaneous losses current time step average round loss cumulative loss divided total number time steps far. learning theory known concentrates passive problems actions inﬂuence instantaneous loss future behavior environment. regression classiﬁcation time-series prediction tasks common bayesian learning prediction expert advice many others fall category. contrast deal active problems. environment reactive i.e. react actions standard situation considered reinforcement learning. cases harder theory often impossible obtain relevant performance bounds general. approaches consider compare based ﬁnite countably inﬁnite base classes. bayesian decision approach base class consists hypotheses models environment. model complete description behavior environment. order prove guarantees usually assumed true environment contained model class. experts algorithms contrast work class decision-makers experts. performance guarantees proven without assumptions worst case relative best expert class. approaches model class endowed prior. model class ﬁnite contains elements common choose uniform prior universal learning turns universal base classes approaches constructed programs ﬁxed universal turing machine. program naturally corresponds element base class prior weight deﬁned −length prior probability distribution class i.e. contents. paper better understand actual learning dynamics properties universal approaches universally optimal sense speciﬁed later. clearly universal base class computationally expensive infeasible use. restrict simpler base classes universal much weaker sense employ complete markov base classes element sees previous time step. although classes truly universal general enough expect outcome good indication dynamics true universal learning. problems study paper matrix games. matrix games simple enough universal algorithm restricted base class learn something provide interesting nontrivial cases reactive environments really active learning necessary. moreover direct competition universal learners. paper structured follows next sections present universal learning approaches together theoretical guarantees. section contains simulations followed discussion section passive problems. every inductive inference problem brought following form given string xx...xt− guess continuation following assume symbols ﬁnite alphabet concreteness reader think strings sampled probability distribution predicting according probability conditioned history optimal. unknown predictions based approximation happens bayesian sequential prediction model class ...} ﬁnite countable distributions strings additionally conditionalized past actions y<t. actions necessary dealing sequential decision problems introduced above. agree convention learner issues show ξ-predictions rapidly converge µ-predictions almost surely assume contains true distribution serious constraint include computable probability distributions universal model class corresponds programs ﬁxed universal turing machine passive prediction problem behavior environments depend actions interpret action prediction assume function deﬁning instantaneous loss. average round regret tends least rate precisely active problems. environment reactive i.e. depends action easy construct examples greedy bayes optimal loss minimization optimal. instead far-sighted aiξ-agent chooses action depth expectimintree agent computes means refer horizon. knew ﬁnal time advance enough computational resources could choose according ﬁxed horizon taking ﬁxed small computationally feasible moving horizon variant. however cause consistency problems sequence actions started time step seem favorable next time step thus disrupted. therefore also almost consistent horizon variant takes ﬁrst step start theoretically appealing alternative consider future discounted loss inﬁnite depth solution bellman equations. found together discussion proof following optimality theorem aiξ. matrix games straightforward setup. consider opponent i.e. environment know action deciding reaction matrix games implemented recursively shown figure additionally assume environments markov players internal states corresponding reaction playing. since step don’t know must evaluate aixirec aixirec compute weighted mixture possible actions long know loss matrix completely additionally compute expectation assignments losses consistent history. pre-deﬁne ﬁnite contains possible losses. simulations below actual losses always large negative value encourages agent explore long doesn’t know losses completely. obtaining interesting results moderate tree depth otherwise loss observed relatively would explore large depth. phenomenon explained detail section markov decision processes probably intensively studied possible environments. environmental behavior depends last action observation precisely case matrix game. game markov player modelled xt−→xt counts often history state transformed state action yt−. laplace’ succession note posterior estimate changes along expectimin tree disregarding important fact done temporal diﬀerence learning variants would result greedy policies rescue exploration ad-hoc methods show exist self-optimizing policies class ergodic mdps although class transition matrices contains non-ergodic environments variant theorem applies hence bayes optimal policy self-optimizing ergodic markov players intuitive reason class compact non-ergodic environments measure zero. instead predicting acting optimally respect model class construct agent class base agents. show accomplished fully active problems. resulting algorithm radically diﬀer agent. prediction expert advice popular last decades. base predictors called experts. goal design master algorithm time step selects expert follows advice thereby want keep master’s regret ℓmaster small cumulative loss best expert hindsight time usually known advance. state-of-the-art experts algorithms achieve this loss bounds similar proven replaced prior weight best expert hindsight bounds hold worst case i.e. without assumption data generating process. particular environment provides feedback adaptive adversary. since bounds imply bounds expectation bayesian setting expert advice sense stronger prediction strategy. order protect adaptive adversaries need randomize. work build follow perturbed leader algorithm introduced don’t even need told true outcome master’s decision. need analysis learning losses experts bounded wlog. master’s actual decision based past cumulative loss experts. concept must prevent master learning fast achieved introducing learning rate decreases zero appropriate rate growing literature assumes experts classes ﬁnite size uniform prior particular learning rate non-stationary. case arbitrary non-uniform prior countable expert classes treated active problems. passive full observation game discussed notion regret problematic even adaptive adversary. however situation changes reaction environment depends past actions. consider simple case experts always suggesting action action environment reactive unfair expert incurs loss long stay initial action soon perform diﬀerent action subsequent rounds experts incur loss sensible strategy soon explore actions compared pure experts incur large loss. consequently need consider diﬀerent notion regret performance compared expert could achieve actually situation. example action sequence perform badly experts. another problem reactive environments necessarily valid feedback experts round. previous example chose ﬁrst action learned expert loss time legitimate make assumption loss expert even environment tells pure expert loss interested loss expert situation i.e. ﬁrst action loss know. precisely know loss expert correct action history last time step past acted suggested. instead trying track action history therefore feedback currently selected expert discard information. commonly referred bandit setup. fortunately issue successfully addressed forcing exploration i.e. sampling according prior certain probability exploration rate decreased zero appropriately growing thus time step decide either follow perturbed leader explore. accordingly call algorithm bounds bandit setup typically similar given large amount recent literature diﬃcult obtain similar assertions algorithms. however know result proven requires rapidly decaying weights therefore appropriate universal expert classes. increasing horizon. environment reactive suﬃcient consider short-term performance selected expert ﬁrst recognized considered repeated game prisoner’s dilemma opponent motivating example case good long term strategy cooperating however defecting dominant i.e. instantaneous loss defecting always smaller cooperating. order notice expert performs well evaluate least time steps. general evaluate chosen expert increasing number time steps hope perform well arbitrary reactive environments. means master works diﬀerent time scale time step gives control selected expert time steps consequence instantaneous losses master observes longer uniformly bounded fortunately turns analysis remains valid grows unboundedly slowly enough. convergence rate average master’s loss optimum aﬀected obtain ﬁnal rate t−/. resulting algorithm speciﬁed figure together subroutine fpl. instantaneous cumulative losses time scales always clear notation chose simplest guarantee this. simulations concentrate following faster learning variant approximate probability selected expert monte-carlo simulation. always learn unbiased estimate analysis works modiﬁcation however resulting better bounds. hand modiﬁed learns faster. case non-uniform prior possibly inﬁnitely many experts exploration must according prior weights. causes another problem loss estimates need bounded forbids exploring experts small prior weights. hence deﬁne expert entering time modiﬁed uses active experts guarantees additionally ﬁnite active step algorithm remains computationally feasible. already indicated goal explore compare performance universal learning approaches presented particular problems solved passive greedy learners. repeated matrix games well suited begin describing experimental setup universal learners. that discuss matrix games presenting experimental results highlighting interesting aspects. terms losses rather rewards transformed simply inverting sign. discussion results keep rewards standard game theory.) single game proceeds following ﬁrst player chooses action simultaneously second column action players without knowing opponent’s move. reward payed player revealed players. repeated game consists single games. chose least opponent fast learning least randomized player participates repeated times usually average shown. consider symmetric games player position player symmetric strategy meet precisely three types symmetry matching pennies inverting action player battle sexes game inverting players’ actions games latter games call action defect cooperate. games consider rewards agents used speciﬁed previous sections classes two-state markov environments deterministic four-state markov experts respectively. concentrate presentation almost consistent horizon variant since performs always better moving horizon variant. concentrate faster learning variant. prisoner’s dilemma. dilemma classical. reward matrices following interpretation players accused crime committed together. interrogated separately. player either cooperate player defects punishments according players’ joint decision none gives evidence minor sentence. gives evidence keeps quiet traitor clear giving evidence i.e. defecting instantaneously dominant action regardless opponent does immediate reward always larger defecting. however players would agree cooperate social optimum guarantees better long-term reward repeated game. wellknown instance case playing strategy strategy cooperates ﬁrst move subsequently performs action previous move. similar harder learn three defect ﬁrst move cooperate cooperated respectively three times row. note although three aiξ’s model class probabilistic versions strategies probability adversary defected cooperate adversary cooperate next round chosen correctly expected number rounds cooperate adversary respectively figure shows cases learns quickly best actions. opponent memoryless example uniform random player constantly defects short time. cooperates short time. ﬁgure shows average round rate cooperation exploratory moves converges optimal action however learn cooperate three tat. reason general problem order increase exploration needs exponential depth expectimin tree. assume certain action sequence length favorable true environment however high current weight. instance cooperating three times favorable -tit tat. order recognize worth exploring build branch depth expectimin tree small probability however. needs exponentially large subtree branch accumulate enough reward order encourage exploration. problem arises plays another aiξ. here perfectly symmetric setting results playing actions move hence correctly learning. might remedy varying tree depth second however turns case aiξ’s learn cooperate possible reason). stag hunt. game also known assurance. reward matrices players hunting together. cooperate catch stag. however player might trust other case chases rabbits instead. case won’t anything tries cooperate. defect conﬂict player gets less rabbits. although optimum players cooperate need trust suﬃciently. player plays uniformly random better rabbits. also defecting lower variance. maybe surprising observe learn cooperate -tit reason defecting relatively good payoﬀ therefore exploration encouraged discussed previous subsection. depth tree increased learns cooperation -tit also moving horizon variant even problems exploration learn cooperating -tit even depth explanation even decides explore time step next step exploration might correctly continued tree explored diﬀerent level. observation also made prisoner‘s dilemma. fact consistent horizon variant performs always better moving horizon. before learns much slower explores robustly neither -tit problem. unlike prisoner’s dilemma competing learn mutual cooperation almost half cases average lucky instances given ﬁgure. valid symmetry problem already observed prisoner’s dilemma. original slower learning variant reaches average level performance time steps instead steps moreover variance twice high. chicken game interpreted follows coauthors write paper tries spend little eﬀort possible. succeeds whole work high reward. hand anything paper thus reward. finally decide cooperate reward. here repeated game socially optimal take turns cooperating defecting. still best situation player emerges dominant defector defecting games cooperates. opponent steadily alternates cooperating defecting quickly learns adapt. observed figure performance given terms average round reward instead cooperation rate. however obstinate enough perform well stubborn adversary would cooperate opponent defected three successive time steps. here learns cooperate leaving opponent favorable role dominant defector. aiξs play other symmetry problem. interestingly break symmetry giving second depth turn dominant defector behaves diﬀerently game learns deal steadily alternating adversary emerges dominant defector stubborn would give precedence cases. hard explain since beginning plays essentially random. thus learns quickly defect remains nothing learning cooperate. however always happen minority cases defects enough decides cooperate dominant defector. assume authors good cooperating costs cooperating compensate synergy. could assign reward instead mutual cooperation. less interesting situation easy chicken cooperating optimal long-term strategy like previous games. figure universal learners show similar performance like chicken game. learn deal alternating partner. also learns dominate stubborn adversary plays less favorite action opponent insists three times that. dominated stubborn player. however always dominates finally contrast chicken game symmetry problem learn alternate. matching pennies. player conceals palm coin either heads tails revealed simultaneously. match ﬁrst player wins otherwise second. zero-sum game figure shows results last game present. learn exploit predictable adversary namely player alternating games balanced long beginning succeeds exploit little. aiξs compete important break symmetry altogether universal learners perform well repeated matrix games. usually learn prefer optimal long-term action greedy behavior possible able exploit predictable adversary learn good strategies necessary foresee opponent’s action approaches presented compared learns much faster explores thoroughly. course trade-oﬀ exploration fast learning. interestingly depend adversary fast learning exploration better long-term strategy chicken battle sexes proﬁts learning fast dictating preferred action looses stubborn opponent exploring enough.", "year": 2005}