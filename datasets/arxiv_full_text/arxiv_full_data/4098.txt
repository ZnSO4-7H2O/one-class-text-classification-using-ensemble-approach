{"title": "Efficient Convolutional Neural Networks for Pixelwise Classification on  Heterogeneous Hardware Systems", "tag": ["cs.CV", "cs.AI", "I.2.6; I.5.1"], "abstract": "This work presents and analyzes three convolutional neural network (CNN) models for efficient pixelwise classification of images. When using convolutional neural networks to classify single pixels in patches of a whole image, a lot of redundant computations are carried out when using sliding window networks. This set of new architectures solve this issue by either removing redundant computations or using fully convolutional architectures that inherently predict many pixels at once.  The implementations of the three models are accessible through a new utility on top of the Caffe library. The utility provides support for a wide range of image input and output formats, pre-processing parameters and methods to equalize the label histogram during training. The Caffe library has been extended by new layers and a new backend for availability on a wider range of hardware such as CPUs and GPUs through OpenCL.  On AMD GPUs, speedups of $54\\times$ (SK-Net), $437\\times$ (U-Net) and $320\\times$ (USK-Net) have been observed, taking the SK equivalent SW (sliding window) network as the baseline. The label throughput is up to one megapixel per second.  The analyzed neural networks have distinctive characteristics that apply during training or processing, and not every data set is suitable to every architecture. The quality of the predictions is assessed on two neural tissue data sets, of which one is the ISBI 2012 challenge data set. Two different loss functions, Malis loss and Softmax loss, were used during training.  The whole pipeline, consisting of models, interface and modified Caffe library, is available as Open Source software under the working title Project Greentea.", "text": "work presents analyzes three convolutional neural network models efﬁcient pixelwise classiﬁcation images. using convolutional neural networks classify single pixels patches whole image redundant computations carried using sliding window networks. architectures solve issue either removing redundant computations using fully convolutional architectures inherently predict many pixels once. implementations three models accessible utility caffe library. utility provides support wide range image input output formats pre-processing parameters methods equalize label histogram training. caffe library extended layers backend availability wider range hardware cpus gpus opencl. gpus speedups observed taking equivalent network baseline. label throughput megapixel second. analyzed neural networks distinctive characteristics apply during training processing every data suitable every architecture. quality predictions assessed neural tissue data sets isbi challenge data set. different loss functions malis loss softmax loss used training. firstly would like express gratitude supervisor funke guidance motivation opportunity visit hhmi janelia ashburn virginia usa. also thank supervisor prof. angelika steger collaborating institute neuroinformatics made research project possible. thank stephan gerhard julien martel interesting discussions neural networks technical report. besides advisors would like thank srinivas turaga stephan saalfeld collaboration hhmi janelia inspired extend scope research gave insight applications neural networks image segmentation connectomics. besides this janelia nicest campus research institutes seen far. would like thank especially taylor greg stoner bruno stefanizzi generous hardware sponsoring empowered development caffe library enabled neural network models beyond possible regular hardware. also thank timmy assistance development clblas ing. herv´e chevanne providing drivers support gpus. hardware using devices years pleasure work together engineers using newest hardware software technology research. research time line difﬁculties encountered reproducibility results outlook device abstracted backend improving training data parameter grid search testing volumetric architectures convolutional neural networks forward-backward neural networks mostly based convolutions machine learnable kernels pooling operations element-wise non-linear activation functions. networks employed various image classiﬁcation object recognition tasks. prominent example imagenet alexnet object recognition. recent networks many case layers millions learnable parameters. work focused classifying biomedical data particular neural tissue electron microscopy images challenge kind data sets training data scarce data sets generated everyday pictures handwritten letters online collections images. annotating ground truth neural tissue images manual work every single pixel labeled. consequently improving training speeds primary objective optimize for. data processed trained model afterwards however easily reach terabyte-scale. therefore crucial develop networks efﬁcient possible forwarding step. work presents three efﬁcient pixel classiﬁcation networks. training classical mislabeling objectives softmax cross-entropy loss might useful pixelwise classiﬁcations biological images using spatial context information generate error signal train neural networks perform better. therefore research also considers different training methods including malis criterion. efﬁcient convolutional neural networks means network models analyzed designed efﬁcient possible getting task done case pixelwise classiﬁcation electron microscopy neural tissue images. heterogeneous hardware systems means network models used also efﬁcient possible variety compute devices. objective makes efﬁcient neural networks accessible users allows existing hardware clusters segmentation tasks neural networks done. caffe stands convolutional architecture fast feature embedding state-of-the-art neural network library heavily optimized nvidias cuda technology. many cases library therefore already efﬁcient using certain gpus. missing fast support support gpus accelerator devices intel. library still development large community pixelwise classiﬁcation means labeling pixel image based local context around pixel. figure shows works sliding window networks outputs single pixel input tile size minibatch processing output many pixels once still inefﬁcient work hongsheng allows make existing networks efﬁcient giving identical prediction results alternatively fully convolutional models directly output bigger patch depicted figure method training processing called patch-based opposed minibatch-based combination possible useful images data tiled large minibatches still advantages training every element minibatch picked independently. processing networks output large patches always performing better. malis criterion ﬁrst introduced srinivas turaga criterion supports alternative training neural networks afﬁnity graphs speciﬁc useful biomedical data areas separated background borders. overview contributions caffe landscape terms models utilities library changes given figure work tackles given problem levels using efﬁcient blas libraries backend development frontends easy network models. caffe library extended layers adaptions compability backend. layers also affect functionality cuda backend. opencl backend development mostly focused versatility completeness cpus kinds compute devices used network models. includes compability three different blas libraries clblas viennacl-blas cblas. report also providing introduction segmentation pixelwise classiﬁcation neural networks. contains details necessary understand existing models readers able easily design neural networks based ﬁndings research project. combination contributions caffe library infrastructure summarized working title project greentea. however greentea also stands opencl backend architecture optimized high ﬂexibility greentea name choice simply prefer greentea coffee greentea used abbreviation like case caffe greentea supposed good brain using neural network machine learning toolset seems appropriate. data section data section sliding window networks pixelwise classiﬁcation section strided kernel networks pixelwise classiﬁcation section ronneberger network architecture section network architecture combining aspects section number feature maps network layer. size feature network layer. size total network input padding network layer padding. network layer stride. network layer kernel size. network layer kernel stride. layers layers memory blobs blobs network weights denotes number weights. device host memory usage. number queues caffe opencl backend. network minibatch size. afﬁnity graph data denotes afﬁnity graph diff. pixel image data denotes image diff. data shows neural tissue drosophila larva ventral nerve cord acquired using serial section transmission electron microscopy hhmi janelia research campus training data consists images pixels sstem corresponding segmentation. segmented nine different labels consolidated foreground background label training evaluation horizontal cell membranes background vertical cell membranes background vertical cell membranes background vertical cell membranes background cell membrane junctions background glia cells background mitochondria foreground synapses background cell interior foreground idea behind label consolidation network training learn separation borders neural cells. especially important malis criterion loss segment foreground background. softmax loss also possible network learn labels separately combine accordingly afterwards. network learn features separately either membrane example depends different orientations convolution ﬁlters. means network trained labels afterwards nine labels extracted applying short ﬁne-tuning training phase network. slices data similar cross validation splitting data different ways applied. total amount pixels training therefore mpixel. data corresponding test train scripts available caffe neural models repository dataset data isbi challenge images corresponding segmentation images training pixels. neural tissue features similar scale data images less contrast fuzzy. training images gives total mpixel half much test data used separate stack images size pixels. images segmentation ground truth available public download thus evaluation reported section solely based reports ofﬁcial isbi evaluation still open results. chapter describes network architectures training processing data sets conﬁgured label classiﬁcation however also nine labels tested even could learned softmax loss. malis loss foreground background separation implemented. networks architectures special striding padding used. therefore striding parameter except downsampling sliding window pooling operations padding always networks square size ﬁlters feature maps simpliﬁes descriptions value parameter. models generalized arbitrary dimensions different sizes dimension. time project networks dimensional conﬁgured modiﬁed caffe library provided mini batch size used processing network choice arbitrary limited memory. networks minibatch size sufﬁcient reach utilization often memory sufﬁcient bigger mini batch sizes rather useful increase network output size using minibatches data allows enough input pictures. efﬁcient networks presented almost size output prediction maps. constraints even divisibility pooling operations well size constraints given convolutions strided kernels met. networks therefore different input image sizes depending memory requirements data image size without re-design re-training networks. results numerically identical case. networks data sets features different scales necessary adapt kernel sizes layers good predictions training. here networks conﬁgured mitochondrion would context pixel prediction centered mitochondrion. sliding window networks classify image taking pixel border padding size around input classify center pixel running patch neural network. next pixel labeled shifting window patch pixel classifying neighboring pixel ﬁrst one. pixels also processed minibatch increase utilization amortize direct memory access transfer times still inefﬁcient context neighboring pixels overlaps ﬁlters applied whole context. redundant computations reduced patch input pixels using networks. sliding window network described developed julien martel baseline calculating speedups obtained networks. structure network also used designing network core network. reason used basis already trained data good results. learning rate weight decay training parameters available. however numerical evaluation publication network architecture exists. sliding window network evaluated in-depth terms benchmarking quality assessment. caffe neural tool used detailed benchmarking processing work minibatches current form. segmentation performance however numerically equal network derived network. hongsheng provide pseudo code convert sliding window network strided kernel network. however incomplete consistency checking kernel sizes feature output sizes. also theory converting inner product layers described. therefore provide complete version although without considering padding striding. enforces layers network. data dimension processed separately kernel size kernel stride output dimension algorithm able convert networks consistency issues fully automatized. caffe prototxt network conﬁgurations kernel size kernel stride provided. output dimensions computed given network input size padding striding parameters left away default correct values. algorithm assumes |lsw| |lsk| taking account input data layer layer type handled special -case handled using exact conﬁguration original network. fails however layer anything element-wise operation because kernel stride conversion initial input size equal network used. afterwards arbitrary input size used training processing output size using also helps prove results stay numerically same. case strides introduced pooling operations implicitly ignored. taken account ﬁrst inner product layer span whole feature size external kernel size originally inner product layers sw-net normal convolutions computational savings compared network anymore. clear observation layer isolates context pixel overlappings feature maps exist layer. number input output feature maps remains exactly networks layers. line implies downsamlpling pooling layers converted. pooling instead would need handled like convolutions terms kernel output size would change kernel stride assessed further. line caffe handle downsampling poolings overlap border networks causes implicit zero padding. allowed networks therefore line checks continuing without check would cause strided pooling layer overlap data would normally separated stride also output feature maps wrong sizes continue. decrease feature sizes formula. convolution inner product layers implicitly inherit behavior. pooling separates output pixels stride equal downsampling kernel size line kernel stride last layer implies pixels output feature maps independent other. case network converted correctly possibly lacks least inner product layer. layer data conv relu conv. relu pool conv relu conv. relu pool conv relu conv. relu pool. pool relu conv. relu conv. relu relu conv. prob softmax ﬁnal three inner product layers network actually become convolutions special properties rules stay converted convolution layers. afterwards arbitrary kernel stride kernel size spans pixel feature map. cause confusion conﬁgured layers. patch original network context pixels. converting classifying patch pixels given table padding beginning actually becomes pixels. padding split border around patch classify. simplify issue border pixels side assumed cropped pixel bottom right side. results behavior running pixel sliding window network across input also padded pixels corrected version pixels original version. estimate number free parameters trained model following formula used u-net presented network conﬁguration described ronneberger paper table describes network style table network order compare them. layer names chosen style networks. u-net contracting expanding sections source code running u-net well prototxt conﬁguration ﬁles available download time project thus network presented here interpretation might differ original design. paper give details mergecrop layer upconvolution work. conﬁgurations u-net included caffe neural models therefore incompatible original work segmentation results comparable. convolution relu conv relu pooling pool convolution relu conv relu convolution relu conv relu pooling pool convolution relu conv relu convolution relu conv relu pooling pool conv relu convolution relu conv relu convolution relu upconv conv mergecrop conv relu convolution relu conv relu convolution relu upconv conv mergecrop conv relu convolution relu conv relu convolution relu upconv conv mergecrop conv relu convolution relu conv relu convolution relu upconv conv mergecrop conv relu convolution relu conv relu convolution relu prob u-net architecture parameters using equation corresponding values table thus parameters sk-net. number weights rises towards middle network convolution kernel size throughout network feature maps every contracting step. network architecture combines ideas models. majority convolutions therefore free parameters trained downsampled feature maps using contracting path steps u-net. contracting step sequence layers network network outputs pixel labels. context considered pixel classiﬁcation pixels. result network input pixels. network part required accept pixels input outputs pixels. therefore sees context pixels. factor downsampled feature maps. prior experiments contracting expanding steps successful many features datasets vanished times downsampling network also became much harder train gets deep. network mainly relied ﬁlter results contracting expanding step pairs classify given input. signals coming subnetwork largely ignored setting weights close zero convolution following ﬁrst mergecrop layer. using downsampling step ﬁxed issue subnetwork contributed properly training. subnetwork contains main computational costs also carries parameters features meaningful enough beginning subnetwork ﬁrst layers subnetwork. type layer memorydata data conv relu conv. relu conv relu conv. relu pool conv relu conv. relu pool conv relu conv. relu pool conv relu conv. relu pool. pool conv. relu relu relu conv. relu deconv. upconv conv. conv mergecrop mergecrop conv relu conv. relu conv relu conv. relu prob usk-net architecture parameters using equation corresponding values table fraction sku-net weights. savings mainly come reducing layer weights. layer still expensive network balanced sk-net. inner product layers less important subnetwork merges convolves feature maps beginning network together upsampled signals layer. figure displays balanced feature maps processing paths. furthermore usk-net inherits advantage bigger kernel sizes going conv layer. means usk-net learn features looking bigger context inside feature maps still almost reaching speed u-net forward processing. standard caffe binary support training patches interface written caffe library. option pycaffe interface custom python code load images pre-process patch training processing depicted figure caffe neural tool conﬁgured training processing benchmarking prototxt similar conﬁguration ﬁles used networks solvers caffe. important functionalities histogram equalization training image preprocessor prepare label images various ways ﬁlling neural network. caffe network prototxt conﬁguration processing. caffemodel containing trained network weights. folder images segmentation output options optionally memory blobs network stored processing. feature called ﬁlter output useful check network learns right convolution ﬁlters. benchmarking tool re-uses existing training processing conﬁgurations ﬁlls network random data. report memory usage layer wise forward backward times total processing time network. convolution layers also estimate store computational complexity. used generate results chapter label consolidation allowing combine multiple labels one. technique used data consolidation applied histogram equalization allowing balance difﬁcult rare labels consolidating background foreground. also allows mark important small difﬁcult features training data. rotation training patches random multiple random mirroring training patches. blurring training patches gaussian kernel size. blurring zero mean variance picked random normal distribution. mean variance distribution selected. arbitrary rotations make training patch smaller corner cutting. means less available total patches computation histogram equalization methods gets much complicated. assumptions patch prior label posterior distributions hold anymore. questionable useful additional training data would elastic deformations would potential generating unique training data biological images neural tissue images histogram equalization technique balance frequency labels network sees training. training patches leads dependent pixels close proximity image training results worse minibatches. minibatches possible create database single pixel labels minibatch draw independent pixels train stochastic gradient descent step. ﬁrst equalization approach patch prior prefer patches rare labels. done comparing label distribution within patch total label frequency training images. patches labels equation calculates weight patch based total label distribution frequency within patch equation calculates label posterior distribution based patch weights label frequency within patch. normalization factor method help patches rare labels patches drawn random higher probability others. example synapse label cannot balance labels similar distribution every patch example cell membranes versus cell interior. patch size gets bigger approaches size training images label distribution patch prior approaches original label distribution. thus patch prior works small training patches. also completely calculating label frequencies taken account pixel labels closer border image covered less patches center image. patch start offset inside image. corner pixels example covered patch starts corner. masked membrane cell interior labels common masked high probability. result completely balanced label histogram least frequent label consequently never gets masked. best solution train exact label distribution desired remains minibatch training. theoretically also possible usku-net inefﬁcient terms training speed. minibatch training standard networks also sw-net. network weights updated every time patch minibatch forwarding backwarding. gradients accumulated error pixels softmax loss layer normalized stable training. balanced label distribution mostly important softmax multi label classiﬁcation. background-foreground separation malis patch prior little effect masking used all. malis already focus error problematic zones globally whole patch runs network therefore label distribution training matter. adapting caffe library efﬁcient pixelwise classiﬁcation heterogeneous hardware contains programming work scope project changing lines code compared bvlc master branch modiﬁed existing layers pixelwise inputs outputs. additional layers architectures. additional layers malis loss. redesign n-dimensional layers support convolutions changes implemented break backward compability original library. existing network models trained networks still used. source code remains highly maintainable ahead caffe bvlc branch whole scope project. layers layers kernel size inner stride kernel result kernel looks feature context pixels also called external kernel size. motivation kernels able convert single pixel prediction networks networks) patch prediction networks works visible figures well strided representation figure convolutions matrix multiplication stays normal convolutions. imcol colim memory copy kernels adapted. this used existing kernel codes provided hongsheng changing existing imcol colim kernels support trivial kernels read data iterating input feature maps dimensions. within dimension iteration goes kernel size copying data convolution buffer. strided kernels iterating kernel reading pointer increased dimension stride multiplied kernel stride instead adding dimension stride. dimension stride starting ﬁrst dimension denoting input feature size. iteration scheme applies pooling operation kernels. layers adapted work together layers long kernel size caffe library able specify arbitrary amount dimensions blob memory infrastructure used pass data layers. however layers automatically work higher dimensions. element-wise kernels nothing changed. convolutions pooling operations require slight redesign. convolutions matrix multiplication stays normal strided kernel convolutions existing kernels normal n-dimensional convolutions jeff donahue however kernels support default kernel stride scope research project hhmi janelia combined existing code strided kernel dimensional convolutions generalized form convolutions supporting dimensions kernel strides. dimension limit exists allocating local arrays dynamic sizes allowed opencl cuda. arrays required store temporary variables iterators dimension. report analyze networks dimension higher possible replace convolutionsk layers convolutionnd layers sknet arbitrary dimensional network. required reduce network output size feature count order meet memory constraints. layer outputs blob containing feature maps different amount feature maps. input cropped size output feature maps size backpropagation error maps propagated copying inverse direction. uusk-net backwarding enabled input input gets differential data different path neural network. copying back would overwrite gradients interfere intended training. malis stands maximum afﬁnity learning image segmentation. implementation caffe provide based existing code compute malis criterion matlab torch srinivas turaga additional layer forward backward functions memory management additional layers connectedcomponent afﬁnity contributions. layers implemented code opencl cuda. labels rectiﬁed linear unit activation could also used. split layer required feed ground truth label blob following components afﬁnity layers. layer structure separable malis loss used feeding external connected component label afﬁnity maps instead computing indirectly. likewise neural network directly learn afﬁnity graphs instead pixelwise labels. malis criterion calculating minimum spanning trees pixel pairs afﬁnity graphs selecting minimum edge tree error edge position indeed error prediction actual malis function called twice afﬁnity maps then contain errors background prediction errors foreground. result malis criterion able isolate errors background prediction errors cell interior resulting error backpropagation ∆a+. equations afﬁnity maps denote combination vertical horizontal afﬁnity maps. afﬁnity graphs contain value edge neighboring pixels. value close stands high afﬁnity means unconnected. details malis criterion works internally found original paper malis srinivas turaga implementation quite efﬁcient contribute massively training time. error derived using afﬁnity layer backpropagation depicted figure ﬁrst look seems like cell interior errors light gray areas exist picture. enhancing contrast marking errors membrane spots becomes clear happens malis criterion calculates minimum spanning trees pixels cell interior merged cells separated. edge corrected almost always cells pixel pairs within cells shared edges spanning tree. edge therefore accumulate high error value spots every training patch. cell interior errors edges corrected occur border cell interior areas mislabeled cell membrane. leads denser distribution error less error intensity edge. figure error generated malis loss. image corresponds diff component memory blob prob figure gray background color zero error black spots negative errors malis loss u/usk-net models match perfectly networks output large patches pixels without reaching memory limits current gpus connected components malis loss large context work soon patch size small barely covers cell malis becomes useless. sliding window networks example would necessary compute many forwarding iterations ﬁrst able malis generate error backpropagate. would also necessary retain blobs forwarding iterations. whole process would efﬁcient terms memory consumption training times. equations horizontal oriented afﬁnity vertical one. afﬁnity maps size input image additionally minimum index maps stored loss distribute accordingly backpropagation. softmax layer figure ground truth labels actually store foreground background image used produce afﬁnity foreground prediction considered stores foreground background resulting afﬁnity graph correctly higher values connected pixels disconnected ones. backpropagation afﬁnity loss attributed single pixels again malis criterion attribute error afﬁnity graph. foregound background error balance softmax function. loss attributed symmetrically −∆i−. also ways compute estimation afﬁnity graph averaging neighboring pixels. choice minimum selection worked particularly well loss resolution aliasing computing way. malis criterion selects minimum edge afﬁnity graph creating loss maps thus using minimum valued pixel attributing pixel loss makes sense. objectives minimize error either increasing decreasing afﬁnity neighboring pixels. connected components layer small layer based opencv ﬂoodﬁlling algorithm outputs separated connected components foregroundbackground labeled ground truth based malis loss knows areas separated connected. here cell membrane considered background assigned component. version caffe library additional versatile backend various compute devices based opencl viennacl available. backend called greentea part project greentea consisting frontend models modiﬁed caffe library section overview interesting aspects backend works caffe library changed feature opencl backend feature equivalent cuda backend. layers used backends. opencl backend also unit test veriﬁed passes test cases original caffe library. tests invoked executing make runtest source code folder. remains possible compile library support backends once. compute kernel blas calls dispatched dynamically runtime depending kind device selected. every device available registered devicecontext object stores device backend type. network layers syncedmem blob objects carry pointer devicecontext allows express neural network object device relationships. feature allowing future multi-device networks keeping track memory blob device essential. opencl backend dispatch blas calls viennacl-blas clblas gpus. viennacl-blas header-only therefore easier clblas optimized certain gpus compiled separately. opencl hybrid implementation describes opencl backend used selecting device instead device. fundamental differences memory allocation blas library calls. syncedmem object instantiated memory allocated host memory rather device memory. differently backend memory allocated opencl. allows access underlying memory pointer opencl memory objects also able memory opencl kernels. involved memory objects underlying host pointer recovered mapped pointer. point opencl backend ensures compute kernels accessing memory concurrently done executing safe access memory pointers. blas call dispatched cblas library here optimized blas device selected. optimally blas fully parallelized uses cores. numa issues might occur. using opencl backend cpus design decision. alternative would parallelize existing backend openmp pragmas. however computational complexity resides blas calls opencl kernels using local memory extensively well also cpus. blas device speciﬁc needs optimized memory architecture needs different version opencl backend. caffe implements gemm convolutions. advantage highly efﬁcient blas libraries available speciﬁcally various devices clblas cublas openblas. hard implement convolutions efﬁciently using direct convolution. performance implementations portable different kernel sizes hardware types. preliminary experiments even efﬁciency could reached layer sk-net compared using gemm convolution including time input reshaping particularly difﬁcult good local global memory access patterns programming kernels direct convolution. blas libraries already optimized local memory caches optimally. gemm convolutions also simplify implementation higher dimension strided kernel convolutions code reshaping input adapted. caffe functions called imcol colim. huge disadvantage gemm convolution memory requirement convolution buffer discussed section assuming square sized kernels output images equation gives buffer size ﬂoat elements. kernel output dimension denoted network architectures report matrix multiplication consists three matrices rm×k rk×n rm×n. caffe weight matrix column data imcol layer output. dimensions fout fin. resulting computational complexity blas libraries row-major nn-sgemm used. row-major means leading dimension memory matrix row. means matrices transposed. sgemm stands single precision general matrix matrix multiplication. scope project convolutions considered. suitable libraries available devices needed supported. normal convolution also uses additional memory kernels stretched size input fourier space. device memory typically insufﬁcient store kernels reuse recomputing every kernel time intensive. cudnn library also implements modiﬁed form gemm convolutions also evaluated direct convolution options recent paper using fast convolutions exists managing device memory remains difﬁcult. chapter assesses performance different models across variety hardware devices. essential speeding neural networks ﬁnding eliminating computation memory bottlenecks. theoretical computations chapter assume dimensional square sized compute kernels feature maps. accordance network architectures graphics cards kindly sponsored noted acknowledgments. card special features high precision performance error correcting memory. used caffe opencl driver large amount memory importance. workstation card consumer cards nvidia also used without restrictions long enough devices personal workstation specially acquired test date hardware extended testing processors issues possible device workstation could access brieﬂy time hhmi janelia. benchmarks chapter gpus used available gpus capable running models forwardbackward-mode wide range output sizes memory requirements. exceptions hardware comparison benchmarks indicated explicitly. modiﬁed version caffe project greentea supports variety conﬁgurations perform differently depending compute device. table represents setup used benchmarking. best performing setup possible combination backend device models chapter openblas compiled cores openmp supports vector extensions available cpus used. alternatively cblas header interface also supports intel atlas replacements openblas. could also used clblas. advisable clblas optimized gpus different memory architecture cpus. cache hierarchy needs fast local memory buffer blocks matrix temporarily. local memory exist therefore would result copying data needlessly host memory. results efﬁciency cpus already slow memory interface compared gpus viennacl used part opencl backend viennacl-blas also available alternative clblas. slower clblas networks presented almost size output networks memory computational restrictions given device. networks re-trained case results numerically identical. second objective processing output sizes matching dataset non-square outputs pixels also possible. then sizes image sizes process divisible output size network. like that computations wasted. also important note easy scaling rule memory requirements change different output sizes depends number feature maps sizes layers well maximum convolution buffer size upper bound estimation memory usage increases proportional output size total padding size table follows network gets efﬁcient bigger ratio removing overlapping computations. using network example results efﬁciency minibatch sliding window network times slower corresponding network output. networks memory allocation would scale order limiting output patch-cube size quickly. smallest memory available test hardware thus networks table forward processing. conﬁgurations training described chapter except half size used resulting memory requirements visible figure table training model pixels improves training malis loss requires device memory close scaling proportional predicts. memory usage training always higher processing requirements. error difference stored back propagation. data blobs therefore stored twice layers back propagated making difference training processing roughly size data blobs seen figure models data difference weights smallest contributors memory usage. biggest consumption comes temporary buffer needed compute convolutions matrix multiplications. effect much smaller u-net architectures. assess this size buffer estimated maximum buffer allocation worked opencl cuda single memory block. upper limit mbuffer. expensive convolution layer sk-net possible compute limits network output size according equation maximum u-net buffer dominant factor limits network. usk-net design layer less importance less input feature maps smaller kernel size. reduces mbuffer limitation allows bigger output patches making network efﬁcient. opencl backend memory overhead mbuffer number parallel work queues minibatch size. helps speed many small convolutions occur networks starting convolutions parallel. feature used cuda backend. cuda solution case cudnn streams convolutions batches efﬁcient parallel queues cudnn importance here architectures work efﬁciently reusing convolution buffer also feature improved caffe library original caffe library memory overhead would much higher allocating buffer convolution layer buffers cases persistent freeing re-allocating would cause much time overhead. re-using buffer would also decrease peak allocation further. alternative ways implement convolutions discussed section memory consumption decreased training data blobs persist forwardbackward-computation calculate difference update weights training. however processing lower bound estimate given mtotal mbuffer assumes blobs network persistent input output memory consuming layer stored. network blobs seen directed acyclic graph therefore estimate higher layers blobs exist parallel other. currently caffe implementation re-uses blobs way. would necessary analyze instantiating network ﬁrst. then blobs would allocated assigned implication lower devices gpus less memory even mobile devices would capable classifying larger networks presented report. chose implement enough memory available figure indicates reduction would large memory consumed matrix-matrix multiplication buffer labeling throughput overall performance measure neural networks. also shows different devices backends perform. even using fastest network gives speedup compared achievable caffe prior project greentea using network gives identical results original network speedup still factor gpus speedups compared sw-net possible. nvidia scales similarly opencl cuda backend. networks executed directly legacy backend layers strided kernel layers merge crop native kernels implemented. available cuda opencl. fact opencl backend cpus better parallelized native execution caffe speedups networks similar cpus gpus justiﬁes implementing kernels. looking figure convolution layers especially layer responsible overall performance. many input output feature maps therefore high computational complexity layer. convolution layers consist fast memory copy operation arrange data matrix-matrix multiplications optimized blas become possible sgemm call itself efﬁciency values table figure direct proxies efﬁcient blas works devices. layers operate complexity includes layers used networks contribute small fraction total forwarding time important note clblas optimized cublas type matrix-matrix multiplications gpus currently perform worse expected opencl backend. clblas project greentea still quite recent projects performance expected increase optimizations future. convolution layers times less expensive half efﬁcient. small convolutions memory copy kernel launch overhead lower efﬁciency. small convolutions conv also fully utilize many threads available gpus inefﬁcient layers short computation times here important optimization. network mostly consists inefﬁcient layers number feature maps convolution sizes increased improves segmentation. using minibatches increase utilization opencl using multiple queues efﬁciency cuda using cudnn u-net convolutions order magnitude complexity result balanced network terms forward timings balancing comes trading feature size feature count towards middle network. upconvolution layers contribute network forwarding time. less efﬁcient optimum -nearest neighbor interpolation constant weights computed. would effort memory copy operation forwarding accumulating four nearest neighbor values backward computation. currently implemented using caffe deconvolution reversed convolution layer. layer already existed caffe direct upsampling feature maps implemented. convolution kernels grouped meaning upsampling kernel considers single feature map. advantage deconvolution layer also allows adaptive weights interpolations bilinear upsampling. network layer expensive efﬁcient network balanced contributes forwarding time compared original sk-net. efﬁciency forwarding times expected mixture sku-net. except conv layers relatively high efﬁciency. inefﬁcient layers fout fout respectively. directly result matrix multiplications matrices strongly nonsquare shapes therefore less efﬁcient issue came testing opencl hybrid backend performance scale expected systems cpu. systems non-uniﬁed memory access cpus share address space memory every processor cache memory interface. accessing data across comes large performance penalty. compute kernels matrix-matrix multiplication blas library custom opencl kernels cause threads work adjacent data. means write operation likely invalidate cache lines across cpus. point synchronization overhead seems become larger speedup additional cores working algorithms. cores gives cores whole system. number threads frontend gave best performance keeping least blas library temporarily tied processor operating system’s scheduler. opencl backend also allocates memory used threads allocated memory interfaces. table shows impact numa issues taking network example. layers sufﬁciently parallelized evident looking figure differences architectures also possible explanation processors based instruction sets generation. memory interface also fast enough keep computations correcting processor frequency speedup factor using cores using processor effective speedups measured much slower speedups convolution layers complexity effects openblas running cores optimal memory allocations dominant. proxy matrix-matrix multiplications used convolutions perform. expected speedup processors need presented caffe library separate devices. library used individual instances. opencl hybrid backend uses separate parallelization mechanisms solutions would need applied opencl backend needs split processor setup sub-devices using device ﬁssion. splitting rule needs cores belonging processor tied sub-device. used caffe instance. device ﬁssion extension opencl already available cores used frontend selected sub-device need same. permanent access system processors opencl installed time test solutions. implementing solutions remains open issue time project. comparison backends devices perform widely used network image classiﬁcation uses minibatches multiple opencl queues alexnet included caffe library also evaluated. cuda backend advantage opencl backend terms speed less versatile. seems less efﬁcient minibatches smaller matrix-matrix multiplications nvidia performs worse nvidia backend. usku-net performs better using backend gpus. especially backward computation much slower using opencl instead cuda. algorithms used same therefore difference difﬁcult explain. possibly different optimizations need applied backward step convolution layers sequential bottleneck adding gradients minibatch. using opencl hybrid backend intel outperforms backend almost factor two. speedup comes parallelization greentea’s opencl compute kernels single-threaded caffe backend. blas library used convolutions multithreaded cases. data sets three models training loss functions processing loss function three training conﬁgurations training iterations conﬁguration respectively combined amount training iterations chosen training combinations feasible week gpus providing total tflop/s training data large cases thus loss always converged iterations training method. possible trainings overﬁt early stopping applied. technically networks amount examples training even though iterations same chosen output sizes network differently. however gradient accumulation different learning rates weight initialization hard estimate effect amount labels seen. networks converged stable loss negligible. malis loss without using patch prior preferring training patches based label histogram. softmax loss hand used patch prior enabled. justiﬁed different characters loss functions softmax computes per-pixel error malis gives errors problematic pixels only concentrated pixels patch pre-processing step images enhanced clahe normalized range training data images blurred randomly chosen gaussian kernel. training patches also rotated multiples mirrored randomly interestingly possible start training network malis loss directly. loss converge output feature maps drifted classiﬁed foreground background. therefore weights network malis only-training initialized using iterations softmax training ﬁrst. lowest number iterations training converged small loss afterwards. network architectures show behavior trained well starting malis directly. related weight initialization well fact malis works better bigger training patch sizes. training limited pixels output much less context malis work pixels respectively. rank internal ranking indicated rand error. loss function softmax indicates iterations softmax loss function. malis indicates iterations malis loss function. indicated training executed softmax malis training iterations. ranking data expected taking average rank usk-net performs better sk-net precise u-net. goes training methods using softmax malis minimizes rand error better using malis only softmax outperforms malis pixel precision. hand malis criterion improves rand error penalizing merge split errors. hand training malis also decrease pixel accuracy seen inspecting result visually. softmax training initializes networks better malis best training method start softmax transit malis tuning. obviously network architecture performed best pixel accuracy rand error. warping error much slower sk-net performs best. visual analysis based image number stack includes glia cell considered background. thickness makes harder label correctly especially malis loss. diffuse membrane light texture hard separate cell interior. separation correct malis conﬁgurations lead connected cells although trainings malis show uncertainty could sufﬁcient segment correctly. diffuse parts always edge pushed foreground background makes segmentation fragile. membrane close proximity mitochondrion. except networks trained softmax gets labeled correctly. downsampling u-net sub-optimal pixel error mitochondrion long thin structure. consequently leads wrong classiﬁcation membrane shape. issue isolated misclassiﬁcations within cell rejected higher level reconstructing connectome. mitochondrion close image border. training classiﬁcation uses border mirroring make missing context lead errors near border independent network architecture. expected membranes labeled thinner malis error criterion stopping provide loss membranes soon cells sufﬁciently separated even though separation border thin training malis separates cells well scores best visual results match numerical evaluation. performs best pixel error softmax best rand error softmax malis. however using softmax huge difference pixel error visual results networks. malis work well networks leaving uncertainty prediction. still good segmentation thresholding gray scale values applied. numeric evaluation test different thresholds score rather good despite uncertainty. training similar training main difference patch prior used softmax malis. instead error masking enabled using softmax gives thicker borders. motivated input images slightly blurred thus cell membranes less sharp harder distinguish cell interior. interestingly usk-net softmax malis loss training performs unexpectedly worse dataset performed best. common network combined softmax training performs best pixel error. overall data ranking much harder explain visual inspection reveals usk-net often overconﬁdent labeling cell interior connected cells separated. data happen. finally results u-net obtained ronneberger could reached probably used less transformations extend training data. inherently clear network would perform better given bigger training data set. mitochondrion close proximity parallel cell membrane. network removes membrane trainings u-net performs little better still merges cells. network correctly uncertainty mitochondrion instead oriented structures even faint partially labeled membrane sharp enough similar thickness membrane. issue isolated within cell cutting cell connected. convolutional networks hard impossible label correctly. would require high level knowledge object predicted membrane enclosing cell not. diffuse cell interior similar membrane texture. networks membrane connection area. training data examples diffuse membranes networks slightly overﬁtted training data case. collaboration request ini. discussing ideas funke. hongsheng paper released research proposal ﬁnished accepted. research beginning. getting sliding window network work opencl backend development discussing project arrival amd’s gpus pull request modiﬁed caffe bvlc public release caffe neural models public release caffe neural tool ronneberger paper released testing u-net design usk-net. collaboration hhmi janelia virginia critical source code development ﬁnished. writing report ﬁnal evaluation experiments. ﬁrst idea research project implement strided kernels. however release hongsheng paper problem already solved. source code able translate existing sliding window networks strided kernel networks. events lead shift focus implement opencl backend support variety hardware. important existing clusters gpus could used instead nvidia gpus. nice side effect completely re-writing whole caffe library opencl gaining complete understanding library bottlenecks layers work important objectives optimizations network design are. turned running networks across devices give advantage case architectures perfect scaling possible running independent instances network device. requires devices enough memory hold networks. assumption amd’s gpus became available networks scale desired pixel classiﬁcations second desired speed. original ideas speed layers network using methods multi device execution fourier transform convolutions direct convolutions work. release ronneberger paper research focus shifted analyzing u-net approach able classify megapixel second. training u-net difﬁcult sk-net thus tried implement network architecture based ﬁndings sk-net u-net resulted experimental usk-net. usk-net performs similarly u-net produced better results small training data sets looking isbi results test metrics well fact authors malis criterion srinivas turaga hhmi janelia collaboration lead development additional loss layer caffe finally last feature implemented freezing source code dimensional strided kernel support pooling convolution layers feature requested stephan saalfeld srinivas turaga hhmi janelia. used modiﬁed network architectures blocks volumetric-isotropic data sets even time obvious difﬁculty keep general research pixelwise classiﬁcation images important papers released project research. shift focus original plans required times. includes taking account results dropping planned approaches. also work keep changes caffe library changed many core aspects network format shape speciﬁcations memory blobs. broke compability existing code honghsheng al.’s approach well existing sliding window network constantly pulling changes bvlc master branch adapting branch changes necessary. beneﬁt gained backwards compatibility ofﬁcial version always guaranteed branch ahead whole scope project. programming caffe neural tool diversity formats labels input data complex handle. especially loading storing pictures variety pixel formats support stacking multiple images single tricky. last always obvious network learn expected features not. training parameter tests require hours training acceptable production rather cumbersome debugging. evaluation training networks numeric results possible freezing critical parts source code results differ ﬁxing bugs changes library. resulted weeks left stage. outlook give brief introduction future plans improved caffe version extended cases models missing features caffe neural tool ideas time scope project. currently opencl cuda backend implemented sideby-side quite code duplication caffe library. support future multi-device training methods remove redundancy backend uniﬁed remaining code duplication resides actual compute kernels used inside layers. minimize bugs occur backend make software veriﬁcation much easier handle. also shorten time required newly developed layers become available devices. time project improved caffe library drops full support legacy backend favor opencl hybrid solution cpus. tuning work correctly numa processors still required. backend remains fallback layers work opencl cuda malis loss criterion ﬁrst step improve results network architectures evaluated using training data acquired artiﬁcially ground truth. advisable models given data clear winner among networks. results vary strongly numerical analysis revealed network architectures presented examples whole class possible networks. many parameters kernel sizes networks combined type networks evaluated. especially deep multi-path networks useful merging feature maps different architectures making ouput label predictions. architecture example. depending data networks conﬁgured many ways example depth direction likely less physical resolution width height dimension data acquired slicing electron microscopy. considered choosing kernel size kernel stride span depth dimension. example isbi dataset spans microns resolution nm/pixel visual inspection error metrics used report give information accurate label predictions compared ground truth. connectomics important objective. tools process segmentations able correct certain errors predictions testing likely ﬁnal result merge split errors lead uncorrectable errors. objective could useful selecting network architecture loss function training method. research project combines many disciplines high performance computing machine learning visual computing connectomics. able implement originally planned features found replacements ideas turned work badly. finally results research include useful versatile stack open source software extended future. working project already possible establish growing user base models tools introduced project used efﬁciently large variety data sets hardware making ﬂexible. collaboration hhmi janelia also great experience. hardware sponsoring shows programming project greentea efﬁcient machine learning libraries general high interest also hardware manufacturers.", "year": 2015}