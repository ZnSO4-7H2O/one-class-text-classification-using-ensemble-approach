{"title": "Structure Learning of Probabilistic Logic Programs by Searching the  Clause Space", "tag": ["cs.LG", "cs.AI"], "abstract": "Learning probabilistic logic programming languages is receiving an increasing attention and systems are available for learning the parameters (PRISM, LeProbLog, LFI-ProbLog and EMBLEM) or both the structure and the parameters (SEM-CP-logic and SLIPCASE) of these languages. In this paper we present the algorithm SLIPCOVER for \"Structure LearnIng of Probabilistic logic programs by searChing OVER the clause space\". It performs a beam search in the space of probabilistic clauses and a greedy search in the space of theories, using the log likelihood of the data as the guiding heuristics. To estimate the log likelihood SLIPCOVER performs Expectation Maximization with EMBLEM. The algorithm has been tested on five real world datasets and compared with SLIPCASE, SEM-CP-logic, Aleph and two algorithms for learning Markov Logic Networks (Learning using Structural Motifs (LSM) and ALEPH++ExactL1). SLIPCOVER achieves higher areas under the precision-recall and ROC curves in most cases.", "text": "learning probabilistic logic programming languages receiving increasing attention systems available learning parameters structure parameters languages. paper present algorithm slipcover structure learning probabilistic logic programs searching clause space. performs beam search space probabilistic clauses greedy search space theories using likelihood data guiding heuristics. estimate likelihood slipcover performs expectation maximization emblem. algorithm tested real world datasets compared slipcase sem-cp-logic aleph algorithms learning markov logic networks aleph++exactl). slipcover achieves higher areas precision-recall curves cases. keywords probabilistic inductive logic programming statistical relational learning structure learning distribution semantics logic programs annotated disjunction cp-logic recently much work machine learning concentrated representation languages able combine aspects logic probability leading birth whole ﬁeld called statistical relational learning ability model complex uncertain relationships among entities important learning accurate models many domains. standard frameworks handling features ﬁrst-order logic probability theory respectively. thus would like able learn perform inference languages integrate unlike traditional inductive logic programming methods address complexity issue. probabilistic logic programming recently received increasing attention ability incorporate probability logic programming. among various proposals based distribution semantics gaining popularity basis languages probabilistic logic programs probabilistic horn abduction prism independent choice logic logic programs annotated disjunctions problog cp-logic various works started appear problem learning parameters languages distribution semantics leproblog uses gradient descent lfi-problog emblem expectation maximization approach expectations computed directly bdds. problem learning structure languages also becoming interest works theory compression algorithm problog presented ground lpads learned using bayesian networks techniques. slipcase also learns structure lpads performing beam search space probabilistic theories using likelihood data guiding heuristics. estimate performs limited number expectation maximization iterations emblem. structure learning task addressed discriminative generative approach. discriminative learning problem characterized speciﬁc target predicate must predicted. search clauses directly guided goal maximizing predictive accuracy resulting theory target predicates. generative learner attempts contrary learn theory equally capable predicting truth value predicates. paper propose evolution slipcase called slipcover structure learning probabilistic logic programs searching clause space. slipcase based simple search strategy iteratively performs theory revision. diﬀerently slipcover ﬁrst searches space clauses storing promising ones dividing clauses target predicates clauses background predicates discriminative approach. search starts bottom clauses generated progol looks good reﬁnements terms performs greedy search space theories trying clause target predicate current theory. finally performs parameter learning emblem best target theory plus clauses background predicates. slipcover learn general lpads including non-ground programs. generalizes ﬁrst order logic markov networks several parameter structure learning algorithms proposed. paper demonstrate system based competitive superior existing purely methods. moreover paper shows improved search strategy implemented slipcover produces superior results respect simpler slipcase. paper organized follows. section presents probabilistic logic programming concentrating lpads. section describes emblem details. section illustrates slipcover section discusses related work. section present results experiments. section concludes paper proposes directions future work. distribution semantics interesting approaches integration logic programming probability. introduced prism language shared many languages. program languages deﬁnes probability distribution normal logic programs called instances. normal program assumed total well-founded model thus program associated herbrand interpretation model distribution instances directly translates distribution herbrand interpretations. then distribution extended queries probability query obtained marginalizing joint distribution query programs. languages following distribution semantics diﬀer deﬁne distribution logic programs expressive power transformations linear complexity convert others paper lpads general syntax. review semantics case function symbols sake simplicity. lpads alternatives encoded head clauses form disjunction atom annotated probability. formally logic program annotated disjunctions consists ﬁnite annotated disjunctive clauses annotated disjunctive clause form clause hini logical atoms bimi logical literals πini real numbers interval indicated body. note that bimi clause corresponds non-disjunctive clause. head annotated disjunctive clause implicitly contains extra atom null πik. denote stitution grounds identiﬁes head atoms. practice ciθj corresponds random variable atomic choice assignment atomic choices consistent head selected ground clause; assume independence diﬀerent choices. composite choice consistent atomic choices probability composite choice product probabilities independent atomic choices i.e. πik. selection composite choice that clause ciθj ground contains atomic choice indicate selections. selection identiﬁes normal logic program deﬁned {)θj| called instance since selections composite choices assign probability instances πik. particularly loose suﬃcient condition soundness lpads bounded term-size property deﬁned based characterization well-founded semantics terms iterated ﬁxpoint bounded term-size program iteration ﬁxpoint size true atoms grow indeﬁnitely. lpad without negation clauses’ bodies sound well-founded model coincides least herbrand model. southwest-northeast direction east-west direction contains three volcanoes active italy. program models possibility eruption earthquake occurs stromboli. sudden energy release island fault rupture eruption volcano island probability earthquake area probability event probability energy release occurs probability sure ruptures occur faults. clause groundings {x/southwest northeast} {x/east west} random variables clause grounding instead random variable take three values since three head atoms; similarly take values since head atoms. instances query eruption true probability .·.·.+.·.·.+.·.·.+.·.·.+.·.·. instance second term corresponds following instance example shows multiple-head atoms particularly useful clauses causal interpretation body represents event that happening random consequence among head. addition note lpad allows events eruption earthquake occurr time virtue multiple groundings clause instance shows. semantics associates random variable every grounding clause. domains result many random variables. order contain number variables thus simplify inference introduce approximation level instantiations grounding variables clauses expenses accuracy modeling domain. typical compromise accuracy complexity consider grounding variables head only ground atom entailed separate ground instances clause assigned probability things equal ground atom entailed single ground clause standard semantics ﬁrst would larger probability evidence available entailment. approximate semantics interpreted stating ground atom entailed clause probability given annotation least substitution variables appearing body body true. adopted semantics experiments slipcover slipcase section approximate semantics associated single random variable case instances query eruption true probability eruption assigned lower probability respect standard semantics independent groundings clause diﬀering fault name considered separately. practice inference algorithms explanations query composite choice explanation query entailed every instance particular algorithms covering explanations query composite choices covering respect every program entailed problem computing probability query thus reduced computing probability boolean function resulting boolean function feruption takes value values variables correspond explanation goal. equations single explanation conjoined conjunctions diﬀerent explanations disjoined. explanations thus encoded function explanations however diﬀerently instances necessarily mutually exclusive respect other probability query computed summation fact computing probability formula disjunctive normal form shown p-hard various techniques proposed solving inference problem exact approximate using multivalued decision diagrams modifying resolution exploiting speciﬁc conditions using monte carlo approach represents function taking boolean values multivalued variables means rooted graph level variable. node associated variable level child possible value variable. leaves store either given values variables compute value traversing graph starting root returning value associated leaf reached. mdds built combining simpler mdds using boolean operators. building mdds simpliﬁcation operations applied merge delete nodes. merging performed diagram contains identical sub-diagrams deletion performed arcs node point node. reduced obtained often much smaller number nodes respect original mdd. used represent since mdds split paths basis values variable branches mutually disjoint dynamic programming algorithm applied computing probability example reduced corresponding query eruption example shown figure labels edges represent values variable associated source node. packages manipulation decision diagrams however restricted work binary decision diagrams i.e. decision diagrams variables boolean. packages oﬀer boolean operators among bdds apply simpliﬁcation rules result operations order reduce much possible size. node children corresponding value variable associated indicated child corresponding value variable indicated child. drawing bdds rather using edge labels -branch going child distinguished -branch drawing dashed line. work mdds package must represent multi-valued variables means binary variables. various options possible found following provides good performance multi-valued variable corresponding ground clause ciθj values boolean variables xijni− represent equation means conjunction xijk− xijk equation means conjunction xijni−. according transformation -valued variables converted boolean variables -valued variable converted boolean variable explanations encoded equivalent function value boolean variables means that ground clauses head atom eruption chosen -branch nodes must followed regardless variables fact omitted diagram. bdds obtained used well computing probability queries associating every boolean variable xijk parameter represents parameters obtained multi-valued variables fig. decision diagrams query eruption example representing explanations encoded function built multivalued variables; corresponding representing explanations encoded function built binary variables. bdds probabilistic logic programming inference related performing inference bayesian networks. minato presented method compiling exponentially-sized multi-linear functions using compact zero-suppressed representation. ishihata compile multiple evidence sets single shared shares common subgraphs multiple bdds. darwiche described algorithm compiling propositional formulas conjunctive normal form deterministic decomposable negation normal form tractable logical form model counting polynomial time techniques ordered literature. emblem learns lpad parameters using expectation maximization algorithm expectations computed directly bdds. based algorithms proposed using similar approach respect lfi-problog emblem targeted discriminative learning chose parameter learning slipcover. particular emblem lfi-problog diﬀer construction bdds lfi-problog builds whole partial interpretation emblem single ground atoms speciﬁed target predicate interested good predictions. moreover lfi-problog treats missing nodes updates counts accordingly compute contributions deleted paths table. typical input emblem target predicates megaexamples theory. mega-examples sets ground facts describing portion domain interest must contain also negative facts target predicates expressed neg. among predicates describing domain user indicate target facts predicates mega-examples form queries bdds built encoding disjunction explanations input theory lpad. emblem applies expectation maximization algorithm expectations e-step computed directly bdds built target facts. predicates treated closed-world open-world. ﬁrst case body clauses resolved facts mega-example. second case body clauses resolved facts clauses theory. latter option theory cyclic emblem uses depth bound sld-derivations avoid going inﬁnite loops proposed value bound. building bdds emblem starts cycle steps expectation maximization repeated examples reaches local maximum maximum number steps executed. emblem shown algorithm consists cycle functions expectation maximization repeatedly called; function expectation returns data used stopping criterion. emblem stops diﬀerence current previous iteration drops threshold diﬀerence fraction current suppose merge rule applied building fusing together identical sub-diagrams. result call complete binary decision diagram every path contains node every level. πikx pρn∈rn qd∈ρn forward probability probability mass paths root pρn∈rn qd∈ρn backward probability probability mass paths leaf. paths leaf. root root tree query needed compute expression b)πikx represents probabilities paths passing x-edge node indicating expression computing forward probability backward probability bdds’ nodes requires traversals graph cost linear number nodes. counts stored variables contains i.e. node every variable appears path. case contribution deleted paths must taken account. done algorithm keeping array entry every level stores algebraic suppose program example single example eruption. figure built passed expectation form pointer root node resulting forward backward probabilities shown figure slipcover learns lpad ﬁrst identifying good candidate clauses searching theory guided data. emblem takes input mega-examples indication predicates target i.e. want optimize predictions ﬁnal theory. megaexamples must contain positive negative examples predicates appear head clauses either target non-target search space clauses identify candidate ones performed according language bias expressed means mode declarations. following mode declaration either head declaration modeh body declaration modeb schema ground literal integer called recall. schema template literals head body clause contain special placemarker terms form \\type +type -type stand respectively ground terms input variables output variables type. input variable body literal clause must either input variable head output variable preceding body literal clause. mode declarations language i.e. clauses head atoms obtained head declaration replacing placemarkers ground terms placemarkers input variables. extend type mode declarations placemarker terms form treated deﬁning diﬀer creation bottom clauses subsection mode declarations used also slipcase. extended mode declarations respect slipcase allowing head declarations form modeh used generate clauses head atoms. schemas atoms obtained replacing placemarkers variables pi/ari predicates admitted body. used indicate variables shared atoms head. ﬁrst phase aims searching space clauses order promising ones employed subsequent greedy search phase. starting promising clauses greedy search able generate good ﬁnal theories. search space clauses split turn steps construction beams containing bottom clauses beam search beams reﬁne bottom clauses overall output search phase represented lists reﬁned promising clauses target predicates background predicates. clauses inserted target predicate appears head otherwise lists sorted decreasing second phase greedy search space theories starting empty theory lowest value target clause time added list addition emblem extended theory {cl} likelihood data computed score resulting theory better current best clause kept theory otherwise discarded done clause algorithm shows initial beams predicate appearing modeh declaration generated slipcover building bottom clauses progol means predeﬁned language bias algorithm outputs initial clauses subsequently reﬁned function clauserefinements. order generate bottom clause mode declaration modeh speciﬁed language bias input mega-example selected answer goal schema selected schema denotes literal obtained replacing placemarkers distinct variables mega-example atom randomly sampled replacement former available training mega-examples latter answers found goal schema. saturated body literals using progol’s saturation method encoded function saturation shown algorithm method deductive procedure used atoms related terms used initialize growing input terms erms terms corresponding placemarkers body declaration considered turn. terms erms substituted placemarkers generate goals. goal executed database successful ground instances added body clause; positive examples considered solve goal. term corresponding placemarker inserted erms already present. cycle repeated user-deﬁned number times. resulting ground clause processed obtain program clause replacing term placemarker variable using variable identical terms. terms corresponding placemarkers instead kept clause. initial beam beam associated predicate p/ar contain clause empty body bottom clause process repeated number input mega-examples number answers thus obtaining bottom clauses. except fact goal call composed atom. order build head goal called answers ground kept these input terms erms built body literals found function saturation above. resulting bottom clauses form built initial bottom clauses gathered beams cycle every predicate either target background performed iteration slipcover runs beam search space clauses predicate clauserefinements shown algorithm computes reﬁnements adding literal literals body deleting atom head case multiple-head bottom clauses number disjuncts greater furthermore reﬁnements must respect input-output modes bias declarations must connected number variables must exceed user-deﬁned number couple indicates reﬁned clause together literals′ literals allowed body cl′; tuple indicates specialized clause disjunct head removed. encoding explanations deriving single-clause theory together facts mega-examples; derivations exceeding depth limit cut. parameters data computed algorithm; used score updated clause clause inserted list promising clauses target predicate appears head otherwise insertion order decreasing clause range restricted i.e. variables head appear positive literal body inserted lists maximum size insertion increases size maximum last element removed. algorithm function insert used insert order clause score score list elements. beam search repeated beam becomes empty maximum number iterations reached. separate search clauses similarity covering loop systems aleph progol. diﬀerently case however test example requires computation explanations search stops ﬁrst matching clause. interaction among clauses probabilistic logic programming happens clauses recursive. adding clauses theory adds explanations example increasing probability clauses added individually theory. clauses recursive examples head predicates used resolve literals body thus test examples individual clauses approximates case show example execution uw-cse dataset used experiments discussed section uw-cse describes computer science department university washington diﬀerent predicates advisedby/ yearsinprogram/ taughtby/. predict predicate advisedby/ namely fact person advised another person. language bias contains modeh declarations two-head clauses work makes extensive well-known techniques inverse entailment algorithm ﬁnding speciﬁc clauses allowed language bias strategy identiﬁcation good candidate clauses. thus slipcover closely related systems progol aleph perform structure learning logical theory building clauses iteratively. compare slipcover aleph section learns parameters lpads using information bottleneck method em-like algorithm found avoid local maxima trapped. good performance diﬀerent mega-examples share herbrand base. condition emblem performs better slipcover evolution slipcase terms search strategy. slipcase based simple search strategy reﬁnes lpad theories trying possible theory revisions. slipcover instead uses bottom clauses guide reﬁnement process thus reducing number revisions exploring eﬀectively search space. moreover slipcover separates search promising clauses theory. means modiﬁcations able better ﬁnal theories terms respect slipcase shown section following highlight detail diﬀerences algorithms. slipcase performs beam search space theories starting trivial lpad using data guiding heuristics. starting theory beam search user-deﬁned good starting point theory composed probabilistic clause empty body form target predicate target predicate tuple variables. step search theory highest removed beam reﬁnements generated evaluated means inserted order decreasing beam. reﬁnements selected theory constructed according language bias based modeh modeb declarations progol style. following admitted reﬁnements adding removing literal clause adding clause empty body removing clause. beam search ends following occurs maximum number steps reached beam empty diﬀerence current theory best previous drops threshold slipcover search strategy diﬀers since composed phases beam search space clauses order promising clauses greedy search space theories. beam searches performed algorithms diﬀer slipcover generates reﬁnements single clause time evaluated search space theories slipcover starts empty theory iteratively extended clause time generated previous beam search. moreover background clauses ones non-target predicate head treated separately adding bloc best theory target predicates. parameter optimization step executed clauses never involved target predicate goal derivation removed. slipcover search strategy allows eﬀective exploration search space resulting time savings higher quality ﬁnal theories shown experiments section previous works learning structure probabilistic logic programs include proposed scheme learning probabilities structure bayesian logic programs combining techniques learning interpretations setting score-based techniques learning bayesian networks. share approach scoring function data given candidate structure greedy search space structures. raedt presented algorithm performing theory compression problog programs. theory compression means removing many clauses possible theory order maximize likelihood w.r.t. positive negative examples. clause added theory. raedt thon introduced probabilistic rule learner probfoil combines rule learner foil problog logical rules learned probabilistic data sense examples classiﬁcations probabilistic. rules allow predict probability examples description. setting parameters ﬁxed structure learned. llpad allpad learn ground lpads ﬁrst generating candidate clauses satisfying certain constraints solving integer linear programming model select subset clauses assigns given probabilities examples. llpad looks perfect match allpad looks solution minimizes diﬀerence learned given probabilities examples. cases learned clauses restricted mutually exclusive bodies. sem-cp-logic learns parameters structure ground cplogic programs. performs learning considering bayesian networks equivalent cp-logic programs applying techniques learning bayesian networks. particular applies structural expectation maximization algorithm iteratively generates reﬁnements equivalent bayesian network greedily chooses maximizes score slipcover used score experiments giving inferior results. moreover slipcover diﬀers sem-cplogic also searches clause space reﬁnes clauses standard reﬁnement operators allow learn ground theories. getoor described comprehensive framework learning statistical models called probabilistic relational models extend bayesian networks concepts objects properties relations them specify template probability distribution database. template includes relational component describes relational schema domain probabilistic component describes probabilistic dependencies hold method automatic construction existing database shown together parameter estimation structure scoring criteria deﬁnition model search space. santos costa presented extension logic programs makes possible specify joint probability distribution missing values database logic program analogy prms. extension based constraint logic programming called clp. existing systems like aleph used learn programs simple modiﬁcations. paes described ﬁrst theory revision system pforte probabilistic first-order revision theories examples starts approximate initial theory applies modiﬁcations places performed badly classiﬁcation. pforte uses two-step approach. completeness component uses generalization operators address failed proofs classiﬁcation component addresses classiﬁcation problems using generalization specialization operators. presented alternative algorithms learn scratch. authors proposed approaches. ﬁrst beam search adds clause time theory using weighted pseudo-likelihood scoring function. second called shortest-ﬁrst search adds best clauses length considering clauses length huynh mooney introduced two-step method inducing structure mlns learning large number promising clauses speciﬁc conﬁguration aleph followed application discriminative parameter learning algorithm. algorithm diﬀers standard weight learning exact probabilistic inference method l-regularization parameters order encourage assigning weights clauses. complete method called aleph++exactl; compare slipcover section structure markov logic theories learned applying generalization relational pathﬁnding. database viewed hypergraph constants nodes true ground atoms hyperedges. hyperedge labeled predicate symbol. first hypergraph clusters constants found pathﬁnding applied lifted hypergraph. resulting algorithm called lhl. domingos presented algorithm learning markov logic networks using structural motifs based observation relational data frequently contain recurring patterns densely connected objects called structural motifs. limits search patterns. like views database hypergraph groups nodes densely connected many paths hyperedges connecting nodes motif. evaluates whether motif appears frequently enough data ﬁnally applies relational pathﬁnding rules. process called createrules step followed weight learning alchemy system. experimented various datasets found superior methods thus representing state markov logic networks’ structure learning general. compare slipcover section diﬀerent approach taken algorithm presented performs discriminative structure learning repeatedly adding clause theory iterated local search performs walk space local optima. share approach discriminative nature algorithm scoring function. dataset records mutations hiv’s reverse transcriptase gene patients treated drug zidovudine. contains examples speciﬁes presence classical zidovudine mutations denoted atoms atoms indicate location mutation occurred amino acid position mutated goal discover causal relationships occurrences mutations virus predicates target. uw-cse uw-cse dataset contains information computer science department university washington split mega-examples containing facts particular research area. goal predict advisedby predicate namely fact person advised another person represents target predicate. webkb webkb dataset describes pages computer science departments four universities. used version dataset contains pages links along words pages. page labeled subset categories student faculty research project course. goal predict categories pages’ words link structures. mutagenesis mutagenesis dataset contains information number aromatic heteroaromatic nitro drugs including chemical structures terms atoms bonds number molecular substructures ﬁvesix-membered rings benzenes phenantrenes others. fundamental prolog facts bond stating compound bond type bondtype found atoms atom atom stating compound atom element element type atomtype partial charge charge. facts many elementary molecular substructures deﬁned used tabulation these available dataset rather clause deﬁnitions based bond/ atm/. greatly sped learning. problem predict mutagenicity drugs. prediction mutagenesis important relevant understanding prediction carcinogenesis. subset compounds positive levels mutagenicity labeled active constitute positive examples remaining ones inactive constitute negative examples. hepatitis hepatitis dataset derived pkdd discovery challenge database contains information laboratory examinations hepatitis infected patients. seven tables used store information. goal predict type hepatitis patient target predicate type type type type generated negative examples type/ adding fact type fact neg) fact type fact neg). statistics domains reported table number negative testing examples sometimes diﬀerent negative training examples because training explicitly provide negative examples testing consider ground instantiations target predicates positive negative. slipcover implemented prolog compared aleph slipcase sem-cp-logic probabilistic logic programs aleph++exactl markov logic networks. experiments performed linux machines intel core processor ram. table characteristics datasets experiments target predicates number constants predicates tuples positive negative training testing examples target predicate folds. number tuples includes target positive examples. slipcover slipcase slipcover oﬀers following parameters number mega-examples build bottom clauses number bottom clauses built mega-example number saturation steps maximum number clause search iterations size beam maximum number variables rule maximum numbers target background clauses respectively semantics additional parameters emblem. slipcase oﬀers following parameters number theory revision iterations maximum number rules learned theory respectively minimum diﬀerence relative diﬀerence theory reﬁnement iterations ﬁnally emblem’s parameters. parameters semantics shared slipcover. slipcover always limit size bottom clauses. parameters slipcover slipcase chosen avoid lack memory errors keep computation time within hours. true also depth bound used domains language bias allowed aleph modiﬁed standard settings follows maximum number literals clause uw-cse mutagenesis since clause bodies generally long. minimum number positive examples covered acceptable clause suggested system manual search strategy forced continue remaining elements search space deﬁnitely worse current best element weight learning step generative discriminative according whether accurately predict speciﬁc predicate respectively; discriminative case used preconditioned scaled conjugate gradient technique found state testing used ﬁve-fold cross-validation. computed probability mutation example given value remaining mutations. presence mutation example considered positive example absence negative example. drew precision-recall curve receiver operating characteristics curve computed area curve using methods reported recently boyd showed that skew larger aucpr adequate evaluate performance learning algorithms skew ratio number positive examples total number examples. since mutagenesis hepatitis skew close datasets computed normalized area curve proposed case aleph tests annotated head learned clause probability testing order turn sharp logical classiﬁer probabilistic assign higher probability examples successful derivations. tables show p-value paired two-tailed t-test signiﬁcance level diﬀerence aucpr aucroc slipcover slipcase/sem-cp-logic/aleph/lsm/aleph++exactl datasets figures show curves datasets figures show curves. curves obtained collecting testing examples together probabilities assigned testing single building curves methods language bias slipcover slipcase allowed atom appear head body slipcover relevant since predicates target. input theory slipcase composed probabilistic clause form <mutation>. mutations. clauses ﬁnal theories characterized head atom one/two head atoms medical literature states tend occur together tend occur together well. slipcase connections simple learned explain aucs. slipcover instead learns many clauses connections found probabilities larger clauses. longer learning time respect systems mainly depends theory search phase since list contain clauses ﬁnal theories average many theory reﬁnement steps executed. uw-cse language bias slipcover allowed predicates appear head body clauses; except advisedby/ background predicates. moreover nine modeh facts declare disjunctive heads three shown section modeh declarations deﬁned looking hand crafted theory used parameter learning disjunctive clause theory modeh fact derived. dataset head reﬁnement applied. approximate semantics used limit learning time. language bias slipcase allowed advisedby/ appear head predicates body only; reason depth bound. input theory composed clauses form advisedby.. used discriminative training algorithm learning weights specifying advisedby/ non-evidence predicate mc-sat algorithm inference test folds specifying advisedby/ query predicate. dataset slipcover achieves higher aucpr aucroc systems. diﬃcult dataset testiﬁed values areas achieved systems represents challenge structure learning algorithms. slipcase learns simple programs composed single clause fold; also webkb language bias slipcover slipcase allowed predicates representing four categories head body clauses. moreover body could contain atom linkto atom word constant representing word appearing pages. input theory slipcase composed clauses form <category>page.. category. target predicates treated closed world case corresponding literals clauses’ body resolved examples mega-examples clauses theory limit execution time therefore relevant. approximate semantics used slipcase limit learning time. aleph aleph++exactl overcame limit target predicate executing aleph four times fold target predicate. removed target predicate modeb declarations prevent aleph testing cyclic theories going loop. coursepage coursepage coursepage coursepage coursepage coursepage coursepage coursepage coursepage coursepage coursepage coursepage coursepage coursepage coursepage domain slipcase learns fewer simpler clauses fold slipcover. moreover slipcase search strategy generates thousands reﬁnements theory extracted beam slipcover beam search generates less hundred reﬁnements bottom clause thus achieving lower learning time. dataset slipcover achieves higher aucpr aucroc systems except aleph++exactl achieves aucpr slipcover statistically signiﬁcant higher aucroc. diﬀerences slipcover aleph instead statistically signiﬁcant. hepatitis language bias slipcover slipcase allowed type/ head predicates body clauses hence depth relevant. slipcover relevant type/ appear clause heads approximate semantics necessary limit learning time. initial theory slipcase contained facts type.. type.. used discriminative training algorithm learning weights specifying type/ non-evidence predicate mc-sat algorithm inference test fold specifying type/ query predicate. long execution time mainly aﬀected createrules phase counts true groundings possible unit binary clauses always true data took hours folds; moreover phase produces short clause every fold. overall remarks results tables show slipcover achieves larger areas systems aucpr aucroc datasets except mutagenesis aleph++exactl behaves slightly better. slipcover always outperforms slipcase advanced language bias search strategy. experimented various slipcase parameters order obtain execution time similar slipcover’s best match could shown. increasing number slipcase iterations often gave memory error building bdds could closer match. slipcover’s advantage lies smaller memory footprint allows applied larger domains eﬀectiveness bottom clauses guiding search comparison complex clause construction process lsm. presented slipcover algorithm learning structure parameters logic programs annotated disjunctions performing beam search space clauses greedy search space theories. applied languages based distribution semantics. tested algorithm real datasets uw-cse webkb mutagenesis hepatitis evaluated performance comparison systems slipcase sem-cp-logic aleph aleph++exactl aucpr aucroc aucnpr mutagenesis hepatitis. slipcover achieves largest values metrics cases. shows application well-known techniques ﬁeld gives results competitive superior state art. future plan experiment search strategies local search space reﬁnements. moreover plan investigate whether techniques help improving performance.", "year": 2013}