{"title": "NeST: A Neural Network Synthesis Tool Based on a Grow-and-Prune Paradigm", "tag": ["cs.NE", "cs.AI", "cs.CV"], "abstract": "Neural networks (NNs) have begun to have a pervasive impact on various applications of machine learning. However, the problem of finding an optimal NN architecture for large applications has remained open for several decades. Conventional approaches search for the optimal NN architecture through extensive trial-and-error. Such a procedure is quite inefficient. In addition, the generated NN architectures incur substantial redundancy. To address these problems, we propose an NN synthesis tool (NeST) that automatically generates very compact architectures for a given dataset. NeST starts with a seed NN architecture. It iteratively tunes the architecture with gradient-based growth and magnitude-based pruning of neurons and connections. Our experimental results show that NeST yields accurate yet very compact NNs with a wide range of seed architecture selection. For example, for the LeNet-300-100 (LeNet-5) NN architecture derived from the MNIST dataset, we reduce network parameters by 34.1x (74.3x) and floating-point operations (FLOPs) by 35.8x (43.7x). For the AlexNet NN architecture derived from the ImageNet dataset, we reduce network parameters by 15.7x and FLOPs by 4.6x. All these results are the current state-of-the-art for these architectures.", "text": "neural networks begun pervasive impact various applications machine learning. however problem ﬁnding optimal architecture large applications remained open several decades. conventional approaches search optimal architecture extensive trial-and-error. procedure quite inefﬁcient. addition generated architectures incur substantial redundancy. address problems propose synthesis tool automatically generates compact architectures given dataset. nest starts seed architecture. iteratively tunes architecture gradient-based growth magnitudebased pruning neurons connections. experimental results show nest yields accurate compact wide range seed architecture selection. example lenet-- architecture derived mnist dataset reduce network parameters ﬂoating-point operations alexnet architecture derived imagenet dataset reduce network parameters flops results current state-of-the-art architectures. last decade neural networks begun revolutionize myriad research domains computer vision speech recognition robotic control ability distill intelligence dataset multi-level abstraction even leads super-human performance thus emerging cornerstone modern artiﬁcial intelligence architecture derived given dataset huge impact ﬁnal performance. illustrate this table compare several well-known imagenet large scale visual recognition challenge describe architecture terms network depth number network parameters number connections performance terms top- error rate imagenet dataset. contest-winning obtained using back-propagation algorithm training weights. thus network architecture becomes distinguishing characteristic. architectural diversity leads substantially different ﬁnal performance seen table though critically important efﬁciently derive appropriate architecture given dataset remained open problem especially large datasets. researchers traditionally derived architecture sweeping architectural parameters training corresponding architecture point diminishing returns classiﬁcation performace. suffers three major problems fixed architecture bp-based methods train weights architectures. utilize gradient information weight space keep architecture ﬁxed throughout entire training process. thus approach lead better architecture. lengthy upgrade searching appropriate architecture trial-and-error extremely inefﬁcient. problem exacerbated deeper contain tens millions parameters. trial deep easily consume tens hours even fastest graphical processing units note gpus currently main workhorse training. even available computational power expended researcher efforts still takes years unveil superior architectures given application image classiﬁcation vast redundancy signiﬁcantly over-parameterized. even best-known humandeﬁned architectures image classiﬁcation alexnet suffer substantial storage computation redundancy example showed number parameters ﬂoating point operations alexnet reduced respectively loss accuracy address problems propose novel synthesis tool trains weights architectures. inspired learning mechanism human brain nest starts synthesis seed architecture allows grow connections neurons based gradient information easily adapt problem hand. then prunes away insigniﬁcant connections neurons based magnitude information avoid redundancy. enables nest generate compact accurate nns. used nest synthesize various compact mnist imagenet datasets. show later nest leads drastic reductions number parameters flops relative corresponding state-of-the-art baselines maintaining slightly improving classiﬁcation accuracy hence dramatically cutting memory cost inference run-time energy consumption. various attempts made past automate process architecture selection. strategies fall major categories evolutionary algorithm structure adaptation discuss approaches next. architecture selection formulated search problem discrete complex architecture space provide promising unwieldy solution. earliest ‘neuro-evolution’ methods proposed miller uses nature-inspired genetic algorithm ﬁrst encode architectures genes improve architectures progressive natural selection. numerous extensions improvements made ‘neuro-evolution’ method last years. examples include different encoding methods simultaneous weight architecture evolution algorithmic modiﬁcation redesign despite progress made three decades remain quite inefﬁcient. search mechanism involves cumbersome iterations reproduction mutation recombination selection. limits scalability. example cannot tackle imagenet dataset million training images. stateof-the-art support cifar- cifar- datesets contains training images approaches mature. constructive approach inefﬁcient deep large-scale nns. moreover typically yields signiﬁcant redundancy. destructive approach relies heavily fully-trained accurate starting point. however starting points obtained time-consuming trial-and-error. section propose nest leverages constructive destructive approaches grow-and-prune paradigm. ﬁrst give high-level overview nest zoom speciﬁc growth pruning algorithms. unless otherwise stated adopt notations given table represent various variables. ﬁrst illustrate nest approach fig. synthesis begins initial seed architecture typically sparse partially connected shown fig. then utilizes sequential phases synthesize gradient-based growth phase magnitude-based pruning phase. growth phase gradient information architecture space used gradually grow connections neurons feature maps achieve desired accuracy. pruning phase inherits synthesized architecture weights growth phase iteratively removes redundant connections neurons based magnitudes. finally nest comes rest lightweight model incurs accuracy degradation relative fully connected model. algorithm shows details grow-and-prune synthesis algorithm. sizeof extracts total number parameters test checks accuracy validation set. prior synthesis constraints maximum size desired accuracy show major components algorithm fig. provide details components next. initial sparse seed contains small fraction active connections propagate gradient information. locate ‘dormant’ connections reduce effectively evaluate ∂l/∂w ‘dormant’ connections activate connections large gradient magnitudes |∂l/∂w|. policy activates ‘dormant’ connections efﬁcient reducing also assist avoiding local minimum illustrate policy plot connections grown input ﬁrst layer lenet-- fig. image center much higher grown connection density image margins. consistent fact almost mnist digits centered. neuroscience perspective connection growth algorithm coincides well famous hebbian theory neurons together wire together deﬁne stimulation magnitude algorithm neuron growth layer input birth strength growth ratio denote number neurons layer number neurons layer rm×n bridging gradient matrix operation extract mean value non-zero elements begin neuron layer initialize wout presynaptic neuron layer postsynaptic neuron layer respectively. note connections activated based hebbian theory would strong correlation also magnitude presynaptic postsynaptic cells thus large value shown gradient respect presynaptic neuron correlations layer layer). initialize weights based batch gradients reduce value policy also greedily reduces since newly added neuron connects neurons previous subsequent layers target neuron pairs layers strong presynaptic postsynaptic correlations. algorithm incorporates policy illustrates neuron growth iterations detail. adding neuron layer evaluate bridging gradient neurons previous subsequent layers. connect correlated neuron pairs neuron layer. initialize weights based bridging gradient neuron addition enables gradient descent thus decreasing value implement square root rule weight initialization order imitate update bridging connection connects layer]. update leads change gmn| linearly proportional effect update. thus weight initialization mathematically imitates update. though illustrated algorithm tanh activation function weight initialization rule works equally well activation functions rectiﬁed linear unit leaky rectiﬁed linear unit birth strength factor strengthen connections newly grown neuron. mechanism prevents connections becoming weak survive pruning phase. speciﬁcally square root rule based weight initialization scale newly added weights operation extracts mean value non-zero elements. strengthens weights. practice appropriate range. gradient-based weight initialization method easily outperforms naive approach assigns random values weights. fig. shows percentage reduction value loss function gradient-based growth versus naive approach applied lenet-. note value decreases becomes difﬁcult reduce stochastic approach. thus neither method continues show improvements. convolutional layers share connection growth methodology policy however unique feature growth algorithm convolutional layers differs neuron growth algorithm convolutional layer convolve input images kernels generate feature maps. thus feature need initialize corresponding kernels. summarize feature growth policy follows policy connection/neuron removal minor adverse impact recover retraining. policy variants pruning insigniﬁcant weights partial-area convolution. pruning insigniﬁcant weights targeted reducing memory computation power requirements. partial-area convolution reduces run-time flops explain variants detail next. magnitude-based weight pruning ﬁrst proposed gloger veriﬁed large-scale extend approach incorporate batch normalization technique. technique reduce internal covariate shift normalizing layer inputs. signiﬁcantly improves training speed behavior hence widely applied large consider batch normalization layer treat connections small effective weights insigniﬁcant. pruning insigniﬁcant weights iterative process. iteration prune insigniﬁcant weights layer retrain whole recover performance. partial-area convolution common convolutional neural networks convolutional layers typically consume parameters contribute total flops inference time convolutional layer kernels shift convolve entire input image generate feature maps. process incurs signiﬁcant redundancy since whole input image interest particular kernel. anwar presented method prune connections not-of-interest input image particular kernel however pruning method coarse-grained. incurs substantial performance degradation. instead discarding entire image proposed partial-area convolution algorithm allows kernels convolve image areas interest. refer area area-of-interest. prune away connections image areas avoid incurring unnecessary flops. illustrate process fig. green area depicts area-of-interest image whereas area depicts parts interest. thus green connections kept whereas ones pruned away. partial-area convolution pruning also iterative process. present iteration algorithm ﬁrst convolve input images convolution kernels generate feature maps input input images kernel matrix feature mask pruning ratio output feature maps denote rm×n×p×q depthwise feature hadamard multiplication convolve stored form four-dimensional feature matrix pruning threshold thres percentile elements pruning ratio mark elements whose values threshold insigniﬁcant prune away input connections. retrain whole pruning iteration. current implementation utilize mask disregard pruned convolution area. partial-area convolution enables substantial flops reduction without performance degradation. example reduce flops lenet- applied mnist. compared conventional cnns intrinsically force ﬁxed square-shaped area-of-interest kernels allow kernel self-explore preferred shape area-of-interest. exploration cuts redundancy unveils interesting shapes correspond different kernels. example fig. shows area-of-interest found layer- kernels lenet- applied mnist. observe signiﬁcant overlaps image central area kernels interested section present experimental results obtained nest training performed using tensorﬂow nvidia tesla gpus. nest synthesize compact mnist imagenet datasets. select seed architectures based clues existing lenet alexnet architectures respectively. nest exhibits major advantages wide seed range nest yields high-performance wide range seed architectures. greedy nature enables gradient descent architecture space. ability start wide range seed architectures alleviates reliance human-deﬁned architectures offers freedom designers. drastic redundancy removal nest-generated compact state-of-the-art nns. compared architectures generated pruning-only methods generated grow-and-prune paradigm much fewer parameters require much fewer flops. derive seed architectures original lenet-- lenet- networks lenet-- multi-layer perceptron hidden layers. lenet- convolutional layers three fully connected layers. afﬁne-distorted mnist dataset lenet-- achieve lowest error rate separately discuss results growth pruning phases combination next. first derive nine seed architectures term lenet-- ‘birth points’. seeds contain fewer neurons connections layer original lenets. number neurons layer product ratio corresponding number original lenets randomly initialize possible connections seed architecture. also ensure neurons network connected. ﬁrst sweep lenet-- step-size grow architectures seeds. study impact seeds time growth post-growth sizes target accuracy summarize results lenet-- lenet- fig. fig. respectively. models ﬁgure share target accuracy. make major observations growth phase follows observation trade-off growth time post-growth size. smaller seed architectures often lead smaller post-growth sizes expense higher growth time. later show smaller seeds thus smaller post-growth sizes better since also lead smaller ﬁnal sizes. observation post-growth size saturates full exploitation synthesis freedom target accuracy smaller seed longer beneﬁcial. hence minimum post-growth size associated given target accuracy constraint evident left dashed curves fig. fig. beyond point using smaller seed architecture increases growth time reduce post-growth size. next prune post-growth lenet remove redundant neurons/connections. show post-pruning sizes compression ratios lenet-- different seeds fig. compression ratio quotient pre-pruning size divided post-pruning size. again models ﬁgure target accuracy. major observations pruning phase follows observation larger pre-pruning larger post-pruning cases larger number weights still larger pruning. fundamental limitation pruning-only approach. thus synthesize compact choose smaller seed architecture within appropriate range table table show smallest models could synthesize lenet-- lenet respectively. tables conv% refers percentage area-of-interest full image partial-area convolution act% refers percentage non-zero activations results better previous state-of-the-art. compare results related results literature table without loss accuracy able reduce number connections flops lenet-- respectively relative baseline caffe model results also substantially outperform reference models various model linear classiﬁer network polynomial classiﬁer k-nearest neighbors svms caffe model layer-wise pruning network pruning lenet-- lenet-- caffe model layer-wise pruning network pruning lenet- estimated value design perspectives. note lenet- caffe model variant original lenet- proposed contains less layer parameters connections. implementation incorporate activation function shift mechanism improve accuracy reduce flops. growth phase leaky relu activation function improve accuracy. leaky relu alleviates ‘dying relu’ problem leads better performance then keep network weights change activation functions relu retrain finally prune network relu activation function take advantage relu’s zero outputs reduce flops. next nest synthesize ilsvrc image classiﬁcation dataset initialize much simpler seed architecture base alexnet proposed original alexnet contains feature maps convolutional layers neurons three fully connected layers. seed architecture contains feature maps convolutional layers neurons fully connected layers. randomly activate possible connections seed architecture. ensure neurons connected. batch normalization instead dropout implementation since batch normalization regularizer eliminate need dropout table illustrates evolution alexnet seed synthesis ﬂow. seed contains parameters. number increases growth phase decreases pruning phase. finally nest synthesizes alexnet-based model contains parameters flops top- error rate table compares model synthesized nest various alexnet-based inference models. choose alexnet caffe model baseline baseline chosen grow-and-prune synthesis paradigm outperforms pruning-only methods listed table fundamental limitation pruning methods larger pre-pruning yields larger post-pruning thus pruning methods inherit certain amount suboptimality associated original large nns. section discuss inspirations human brain behind synthesis methodology. human brain continuously provided serendipitous inspirations modern examples include fundamental idea neuron layered structure even convolution kernels. synthesis methodology incorporates three inspirations human brain. first number synaptic connections brain varies different human ages rapidly increases upon baby’s birth peaks months decreases steadily thereafter. experiences similar learning process nest. initial seed simple sparse akin baby’s brain birth point. growth phase rapidly grows connections neurons based outside information thus reacting information manner similar baby brain reacts. pruning phase reduces number synaptic connections vast redundancy akin baby brain matures adult brain. clearer view trend show plot number connections lenet-- along synthesis lifespan fig. seen curve shares similar pattern number synapses human brain evolves second learning processes brain result rewiring synapses neurons. brain grows prunes away large amount synaptic connections every nest wakes connections thus effectively rewiring neurons pairs learning process. thus mimics ‘learning rewiring’ mechanism human brains. third small fraction neurons active given time human brains. known sparse neuron response phenomena mechanism enables human brain operate ultra-low power however fully connected contain substantial amount insigniﬁcant neuron responses inference. address problem include magnitude-based neuron/connection pruning algorithm nest remove redundancy thus achieving sparsity compactness. leads huge storage computation reductions. paper proposed synthesis tool nest synthesize compact accurate nns. nest starts sparse seed architecture adaptively adjusts architecture gradient-based growth magnitudebased pruning ﬁnally arrives compact high accuracy. lenet-- targets mnist dataset reduced number network parameters flops alexnet reduced number network parameters flops synthesis results lenets alexnet constitute current state-of-the-art.", "year": 2017}