{"title": "Quantum Clustering and Gaussian Mixtures", "tag": ["stat.ML", "cs.CV"], "abstract": "The mixture of Gaussian distributions, a soft version of k-means , is considered a state-of-the-art clustering algorithm. It is widely used in computer vision for selecting classes, e.g., color, texture, and shapes. In this algorithm, each class is described by a Gaussian distribution, defined by its mean and covariance. The data is described by a weighted sum of these Gaussian distributions. We propose a new method, inspired by quantum interference in physics. Instead of modeling each class distribution directly, we model a class wave function such that its magnitude square is the class Gaussian distribution. We then mix the class wave functions to create the mixture wave function. The final mixture distribution is then the magnitude square of the mixture wave function. As a result, we observe the quantum class interference phenomena, not present in the Gaussian mixture model. We show that the quantum method outperforms the Gaussian mixture method in every aspect of the estimations. It provides more accurate estimations of all distribution parameters, with much less fluctuations, and it is also more robust to data deformations from the Gaussian assumptions. We illustrate our method for color segmentation as an example application.", "text": "mixture gaussian distributions soft version k-means considered stateof-the-art clustering algorithm. widely used computer vision selecting classes e.g. colortexture shapes algorithm class described gaussian distribution deﬁned mean covariance. data described weighted gaussian distributions. propose method inspired quantum interference physics instead modeling class distribution directly model class wave function magnitude square class gaussian distribution. class wave functions create mixture wave function. ﬁnal mixture distribution magnitude square mixture wave function. result observe quantum class interference phenomena present gaussian mixture model. show quantum method outperforms gaussian mixture method every aspect estimations. provides accurate estimations distribution parameters much less ﬂuctuations also robust data deformations gaussian assumptions. illustrate method color segmentation example application. given large labeled data able separate clusters great interest computer vision clustering also interpreted bayesian method data likelihood model. combined prior model data forms posterior probability. paper solely focused clustering model particular focus data generated multiple guassian distributions characterized mean covariances. gaussian mixture model standard model describe data combined expectation-maximization algorithm standard approach recover gaussian parameters well number points class inspired quantum methods e.g. reformulate classical model produce quantum model. using theory expectation-maximization recover gaussian parameters well number points class. inspiration proposed method comes wave interference phenomena previous work classical model mixes distributions probabilities canceled. formulating distributions terms wave functions magnitude square describe probability distribution allow mixtures interfere. eﬀect cancellation probabilities allows identify clusters data accurately. introduce wave function describe system points. probability associated system points magnitude square wave function. wave function associated point class given restrict analysis class cases. main reason compare performance classical case. ﬁrst devise experiments randomly controlled data sets fully analyze statistical properties quantum inspired method classical mixture gaussians devise color segmentation experiments data illustrate technique applied outperform state techniques computer vision application. standard deviation along class tests considered points class. refer parameters ground truth experiments samples come single gaussians parameters many samples. sample mixture gaussians model. goal experiment compare performance quantum model classical estimating number points class center class eigenvalues covariance estimations allow understand model best identiﬁes original gaussian distribution parameters number points arbitrarily ﬁxed generate data sets sizes class diﬀerent large enough statistical measurement. gaussian parameters generate data points ﬁrst class center second gaussian distribution varied starting create greater overlap. brought second center towards ﬁrst center repeated experiments figure represents center estimation trial. purple dots represent estimation classical method cyan dots represents estimation quantum method. black dots image represent ground truth centers. images shows vary noticeable trials quantum method almost always provide good answers classical method less accuracy much variation. quantify assertion ﬁgure figure y-axis column estimated error column standard deviation rows centers coordinates. bottom rows variances coordinates. x-axis graphs varying variances quantum method proves accurate robust accurate robust method. data separated diﬀerence methods reduced. estimated error values identical since known/constant. argue quantum method utilizes phase parameter help capture overlapping distributions. quantum method phase diﬀerences classes yields interference phenomena stronger data overlaps. want demonstrate interference better captures overlapped data. this experiments described above measure overlap data shown ﬁgure show phase diﬀerence still overlap measured approximately sheds light results figure classical model still performs compared quantum model. classical method highly sensitive overlaps quantum model chooses phase diﬀerence classes accurately recover gaussian parameters number points class. shown table calculates experimentally prevent variables converging correct values. second also asses ﬁnal solution’s objective functions method oﬀers better cost function third generating landscapes also test whether initial values centers aﬀect estimated variables. sample values range minimum maximum values iterations increments classical method same maximum minimum values sample increments values. however quantum method fully specify need phase values well thus quantum method sample values obtained iteration procedure. number iterations predetermined rather depends convergence. increase close true objective function. classical method true objective function obtained ﬁxing parameters quantum method parameters except ﬁxed. order obtain variables performed exhaustive search increments obtain retrieve lowest possible objective function deﬁned figure landscapes quantum method show objective function changing ﬁxed parameters cases ﬁxed cases ﬁxed table speciﬁes original/true objective function lower ones classical case interference phenomena. figure landscapes classical method show objective function changing ﬁxed parameters cases ﬁxed cases ﬁxed table speciﬁes original/true objective function higher ones quantum case interference phenomena. objective function ﬁnal values lower original/true objective function means method performing adequately. order remove biases would cause quantum method outperform classical counter part start distributions methods performed equally well. case distributions separated overlap measurement classes equal covariance number points. parameters classes ﬁxed trials values quantum method outperforms classical method proving robust performance distributions ideal gaussian distributions. figure y-axis estimated error standard deviation centers x-axis deformation increased. throughout experiments avoid advantage quantum method drew data variances without deformations methods perform similarly evidenced results figure y-axis estimated error standard deviation variances quantum method outperforms accuracy robustness since standard deviation x-axis deformation increased. notice classical system completely failed identify parameters accurately standard deviation decreases error identifying suggests bias towards even estimations wrong. color images pixel represented space i.e. pixel three numbers assigned representing amount green blue color segmentation problem consists assigning pixel class foreground background clustering point view pixel color data point task discover classes color space assign pixel corresponding class. figure image magritte’s empire light. pixel assigned either creating threshold channel. green blue levels original image pixel assigned otherwise. course computer vision employ notion neighborhood nearest pixels likely belong class. here focus clustering problem i.e. pixel treated independent. build complete color segmentation problem could ﬁnal clustering probabilities neighborhood prior work posterior probability. would lead graph partitioning problem known solved algorithm however goal focus data probability problem i.e. clustering probabilities. demonstrate quantum inspired method works signiﬁcantly better state-of-the-art gaussian mixture model. l*a*b* color space color space color-opponent space dimensions lightness color-opponent dimensions based non-linearly compressed coordinates lightness represents darkest black brightest white color channels represent true neutral gray values red/green opponent colors represented along axis green negative values positive values. yellow/blue opponent colors represented along axis blue negative values yellow positive values. used early computer vision color work precisely good perceptual metric properties setting clustering problem l*a*b* space ﬁrst colored binary images according corresponding class gaussian distributions generated l*a*b* color space. class assignment according clustering method ﬁnal expectation values determine class indicates class assign color black indicates class assign color white. distributions used compute number points class methods. thus order produce ﬁnal segmentation given pixel assigned black color otherwise assigned white color. results artiﬁcially colored images clustering method classical quantum inspired trials image starting time random initialization. account variability classical case used image estimated parameters closest average cohort. quantify comparison methods calculated estimated error variables figure every pixel binary image seen ﬁgure assigned random point l*a*b* space generated corresponding class gaussian distributions. ﬁxed parameters case deﬁned class gaussian distributions table summarizes average quantum classical estimated errors using relative error ratio quantum error classical one. note although error number point estimation classical case reasonable solely image number points classes actual assignment classes poor. quantum method poorly black region manipulated image accurately marked orange region classical performed poorly everywhere. figure original image available http//alexgoldblum.com/new-york/nyc-skyline/ binarized. colored artiﬁcially randomly sampling pixel l*a*b* following gaussian parameters table summarizes average quantum classical estimated errors using relative error ratio quantum error classical one. paper solely investigated clustering problem. state clustering method gives close-form solutions gaussian mixture model inspired quantum methods interference phenomena causes probability cancellations reformulated classical model quantum model applied method parameter estimation. showed quantum method outperforms classical method every aspect estimations cases clusters uneven number data points diﬀerent covariances. quantum method able recover accurate estimations distribution parameters much less ﬂuctuations. regimes distributions separated observe classical model sensitive slight overlaps. also show quantum method robust data deformations gaussian assumptions able segment colors accurately color experiments. plotted landscapes model showed smooth leading often global minima solutions. cases data separated initial condition desired solution quantum method reached local minima global minima. noted quantum method extra variable computed phase class. biggest diﬀerence models cost function quantum model returns lower value overlapping case classical solution suggesting interference phenomena drives quantum system. allowed quantum method produce much accurate robust solutions compared classical method. working towards segmentation clustering also interpreted bayesian method data likelihood model combined prior model data form posterior probability. case future clustering model produced techniques graph cuts could employed yield ﬁnal better results color segmentation.", "year": 2016}