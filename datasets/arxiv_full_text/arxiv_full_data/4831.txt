{"title": "Learning Shared Representations in Multi-task Reinforcement Learning", "tag": ["cs.AI", "cs.LG"], "abstract": "We investigate a paradigm in multi-task reinforcement learning (MT-RL) in which an agent is placed in an environment and needs to learn to perform a series of tasks, within this space. Since the environment does not change, there is potentially a lot of common ground amongst tasks and learning to solve them individually seems extremely wasteful. In this paper, we explicitly model and learn this shared structure as it arises in the state-action value space. We will show how one can jointly learn optimal value-functions by modifying the popular Value-Iteration and Policy-Iteration procedures to accommodate this shared representation assumption and leverage the power of multi-task supervised learning. Finally, we demonstrate that the proposed model and training procedures, are able to infer good value functions, even under low samples regimes. In addition to data efficiency, we will show in our analysis, that learning abstractions of the state space jointly across tasks leads to more robust, transferable representations with the potential for better generalization. this shared representation assumption and leverage the power of multi-task supervised learning. Finally, we demonstrate that the proposed model and training procedures, are able to infer good value functions, even under low samples regimes. In addition to data efficiency, we will show in our analysis, that learning abstractions of the state space jointly across tasks leads to more robust, transferable representations with the potential for better generalization.", "text": "enabled scale complex environments scenarios previously prohibited required great amount feature engineering shown thus learning good abstraction given environment agent’s role seems component developing complex optimal control mechanisms. progress made improving learning individual single tasks seems less work trying re-use efﬁciently transfer information task another nevertheless natural assume different tasks agent needs learn life share structure in-build redundancy. potentially could leverage speed-up learning. work propose address aspect learning robust transferable abstractions environment generalize tasks. value functions central ideas reinforcement learning successfully used conjunction functional approximators generalize large state-action spaces. concise readily assess goodness state learnt efﬁciently even off-policy fashion. enables decouple data gathering learning process importantly allows re-use past experiences collected arbitrary exploratory policies recently value functions shown exhibit nice compositional structure respect state space goal states consistent earlier studies suggest value functions capture represent knowledge beyond current goal leveraged re-used. similar structures identiﬁed hierarchical reinforcement learning literature motivated choice explicitly modelling presence shared structure state-action value space. investigate paradigm multi-task reinforcement learning agent placed environment needs learn perform series tasks within space. since environment change potentially common ground amongst tasks learning solve individually seems extremely wasteful. paper explicitly model learn shared structure arises state-action value space. show jointly learn optimal value-functions modifying popular valueiteration policy-iteration procedures accommodate shared representation assumption leverage power multi-task supervised learning. finally demonstrate proposed model training procedures able infer good value functions even samples regimes. addition data efﬁciency show analysis learning abstractions state space jointly across tasks leads robust transferable representations potential better generalization. reinforcement learning gained popularity seen remarkable successes last years exploiting beneﬁting greatly recent developments general functional approximators neural networks least part success seems linked ability universal functional approximators distill meaningfully representations high-dimensional input states. using multi-task formulation following recent work done ﬁrstly outline general ways learning tasks jointly sharing knowledge across extending popular procedures learning value function fitted q-iteration fitted policy iteration accommodate shared structure assumption. furthermore taking advantage multitask methods developed supervised settings extend work account taskspeciﬁc components. also show empirically lead overimprovement policies inferred well decrease number samples task needed achieve good performance. explore nature representation learnt potential transferability related tasks. show learning able infer compressed structure nevertheless captures transferable knowledge similar option-like transition models without ever specifying partition desirable states subgoals. finally argue learning leads robust reﬁned representations deemed crucial learning planning complex environments. deﬁne markov decision process tuple states actions transition dynamics provides probability next state reward signal assumed bounded rmax∀s discount factor. given policy deﬁne value function discounted cumulative reward agent expected collect starting state taking action accordingly policy denote optimal value function note ﬁnding automatically gives optimal policy acting greedily respect values. following denote greedy operation gq∗. consider scenario agent resides environment needs perform series tasks. overall goal learn succeed tasks. environment described state-action space transition kernel tasks speciﬁed different rewards signals task formally gives rise mdps share structure. thus leverage structure expect learning process lead better generalization. propose model shared structured found deﬁned mdp-s shared embedding state-action space build individt}t=t considered optimal value functions tasks potentially ones. thus paper interested learning shared embedding well ultimately optimal behaviour tasks considered. following present extend popular paradigms learning value functions fitted q-iteration fitted policy iteration incorporate shared structure assumption. come employing multi-task learning procedure target-ﬁtting step q-iteration policy evaluation step policy-iteration. section outline general framework using approximate value iteration infer optimal q-values tasks given environment following mt-rl setup previously introduced. proposed algorithm extension fitted q-iteration allows joint learning transfer across tasks. following recipe step iteration loop sample experience {)|s compute onestep target based current estimate value function. then treating estimates ground truth obtain regression problem state-action space onto targets case mt-rl obtain regression problem task could principle solve regression problems independently task would amount applying individually task. assumption shared structure tasks would like make common ground learning process arrive robust abstractions input space. thus propose solving regression problems jointly accounting building upon common representation. detailed description proposed procedure outlined algorithm algorithm extensive literature deal multi-task inference exploit shared structure tasks purely supervised settings take look instantiations step throughout work. similar argument presented last section mt-fqi extend framework general policy iteration mt-rl scenario. policy iteration algorithms rely alternating procedure policy evaluation step policy improvement step. extend framework multi-task case deﬁning policies {πt}t task evolve policies jointly iteration please outline proposed procedure algorithm implement policy improvement step acting greedily respect current estimates value function. step done individually task. hand allow joint learning sharing knowledge policy evaluation step. gives rise general procedure call multi-task policy evaluation algorithm mt-pe given policies {πt}t task collection experiences t=dt. algorithm approximate corresponding value functions associated acting policy task note that general step requires on-policy data policy task could quite demanding inefﬁcient data-wise numbers tasks grow mention inner loop another iterative algorithm work implementation policy evaluation step circumvents problem. making bellman expectation equation compute regression targets approximating {qπt using experience form previously collected case fitted-q iteration. πt)) therefore reduced original problem regression problems solved jointly shared input space representation. similar multi-task learning step employed mt-fqi shared structure learnt model input policies {πt}t rather optimal ones. nevertheless constantly improving policies presented mt-pe step eventually able convergence optimal policies thus point policy evaluation step able recover shared structure amongst optimal value functions. section look couple methods plug algorithms step. assume linear parametrization state-action value space -i.e. assume s.t. value function interest well approximated linear combination features. case ﬁtted value iteration want features well intermediate targets ultimately case policy iteration evaluation step interested feature space well approximates value function corresponding current poli≈ cies thus looking s.t. policies improve ﬁtting optimal near-optimal value functions. certainly regression step done perfectly policy iteration continue improve policies limit converge optimal value functions. thus representation come learning procedure similar ones learned value-iteration procedures. consequently ultimately want terms representation features space spans optimal value functions interest. joint problem tryt= minw regularizer weight vectors encourages feature sharing. time wish learn compact abstraction state-action space shared among tasks. make formal assess performance behaviour proposed model learning procedures -room navigation task state space described valid positions agent might take position grid wall agent access four actions {→←↑↓}. consider deterministic dynamics directions walls considered elastic bumping walls effect state. tasks speciﬁed target locations environment agent need navigate sampled random valid states environment. specify starting state agents need learn navigate selected goal position part environment. agent transitions goal states collects positive reward. reward signal provided. since proposed methods off-policy thus decoupling experience gathering learning sample modest amount experience front considered tasks done principle behaviour policy data-gathering employ uniformly random exploration. data gathered provided proceed learning. experiments conducted restrictive sample budget |dt| firstly would like compare proposed joint-representation learning single tasks counterparts effects enforcing learning shared representation would have. assess quality inferred greedy policies amount reward able produce random starts proxy real value function ˆqt. depending selection starting states difﬁculty tasks thus amount reward achievable vary. ease interpretation report normalized value estimate respect optimal value function starting states. example results ﬁrst training tasks displayed figure obtained training randomly sampled tasks samples experience task. procedure used construct informative shared features shown experimental section. however scenarios tasks beneﬁt small sparse features represent particularities individual task low-dimensional shared subspace. deﬁnitely case many practical applications observed purely supervised settings well simply restricted constrain tasks using single shared structure. thus researchers come various ways incorporating task-speciﬁc components reference therein showed modelling explicitly improve learning interpretability resulting representations. work choose formulations introduced learn low-dimensional shared representation before well task speciﬁc vector place strong sparsity constraint encourage common features still identiﬁed shared. note zero matrix treating task completely independent hand zero tasks recover previous formulation. furthermore place orthogonality condition shared features inferred enforcing figure quality inferred greedy policies trained individually jointly tasks sample budget samples/task. show average cumulative reward achieved agent random initial positions values normalized emp. cases respect optimal cumulative reward achievable starting positions single-task learning struggles sample regime whereas joint-learning methods able discover much better policies even recover optimal ones. joint-learning procedures manage learn good policies quite close optimal ones substantially outperform single-task learning. please note proposed extension allow task-speciﬁc features cases improves performance even considering small common features also gives much faster convergence shared subspace. indeed behaviour seems consistent lower samples sizes although worth mentioning divergence occur often extrem conditions regularization parameters might ensure convergence provide solution often worse even single task. outside extreme cases policy value iteration methods perform similarly figure tend converge solution. better idea average task performance obtain changes training look average distance estimate value functions iteration optimal ones small environment computed analytically. results samples budgets displayed figure observe quite difference single-task multi-task procedures terms recovering true optimal value functions. convergence better happens much faster even asymptotic superior solution. nevertheless closeness optimal value functions euclidean space necessarily imply relation policy space. plot quality policies function value/policy iterations available figure here report normalized average regret policies general converge much faster value functions comparing q-value convergence figure please also note multi-task fitted policy-iteration procedures inherit speedy convergence present single-task counterpart. probably interesting phenomenon encountered learning shared representations nature dimensional representations inferred. visualize inferred shared features respective weights value-function produced mt-fqi constraint shared subspace -dimensions. even seem permissive actually obtain strong activations features inferred presented figure thus learnt representation dimensional time expressive enough effectively approximate optimal value functions. learnt representations resemble option-like features essentially inform agent across tasks navigate efﬁciently rooms negotiate narrow hallways. indeed easily transferable ’skills’ learning task. test hypothesis augmenting represenfigure convergence optimal value function assessed euclidean norm ||q∗ different sample complexities samples/task samples/task samples/task different methods proposed. report average tasks shaded area corresponds variances mean. note sample/task joint-representation learning algorithms obtain convergence true optimal value functions tasks. also note join-learning methods second method allowing task-speciﬁcity yields better approximations figure evaluate quality policy learnt produce different empirical estimate sample complexities samples/task samples/task samples/task different methods proposed. seen convergence plot above samples multi-task methods reliably recover optimal value functions implicitly optimal policies single-task methods tasks. time half budget multi-task learning already able recover optimal policies single-task methods converge suboptimal value function figure ﬁrst three relevant shared features corresponding three eigenvalues learnt afpi-mtfl tasks randomly sampled four rooms. please note already enable navigation pair rooms. figure weighting coefﬁcients three prominent shared features. values ﬁrst feature clearly dominated tasks. bottom rescaled version activation prominent features. blue corresponds negative activation positive ones. given nature features readily read looking sign weight room task’s goal state instance look second task negative activation north-side environment west-side environment. goal indeed located position top-left room. figure average performance tasks without transfer shared features assessed normalized average cumulative reward collected random starts environment. value functions tasks produced original feature respectively augmented features space policy trained original representation versus augmented representation seeing varying amount samples augmented representation able produce good performance smaller sample sizes. general learning based transferred representation able produce policy equivalent ones could learn without transfer twice much data. behaviour consistent convergence. previously learnt shared representation seems account general topology dynamics environment value functions. nicely partition environment relevant regions facilitate global navigation local neighbourhood goal. features characteristic options skills macro-actions literature potential drastically improve efﬁciency scalability methods tion corresponding newly deﬁned semi-mdp const semi-mdp indeed able construct value function based solely learnt -dim feature space successfully completes speciﬁed task. results navigation options available figure following formulation option generalization primitive actions temporally extended course action. initiation option available policy going follow options triggered probability termination. case value function take form termination state options denote k)γk probability options terminate state exactly steps. note term accounts transition dynamics policy option termination criteria which task-invariant. moreover note generally unless option happens goal. thus equation simpliﬁes linear combination option transition φµo) termination independent task depends weighted value function termination states tasks incorporates dependency task individual policy employed option terminated. similar parametrization assumed suggest learnt representation able capture represent efﬁciently option-like transition models without specifying subgoals policies initial states. hypothesize learnt shared space actually compressed basis option-transition models. order test hypothesis consider intuitive options test option learnt basis span successfully represent option-policy. deﬁne option navigating speciﬁc room room initialization states outside room termination state desired room. also deﬁne maintains transition dynamics state action space reward signal zero outside target room constant positive reward desired termination states room note value funcfigure learned greedy policies value functions maxa) enabling navigation four rooms based share feature subspace discovered multi-task value function learning goals randomly sampled environment. value functions learnt using features ψaso required samples recover option-like policies enable agent reach desired room. please note deﬁned options quite extended ones. simpler ones would include making outside particular room along lines options deﬁned easily recovered well. actually simpler options require samples obtain desired behaviour although might optimal please consult supplementary material details. fact able express whole variety intuitively deﬁned options much dimensionality common subspace building clear indication expressiveness shared representation potential transferability aiding learning tasks within good collection methods tackle various aspect multi-task reinforcement learning approach methods learn jointly either value functions policies tasks different structure environment assumptions. recent study also employs idea shared feature space learning procedure proposed transferring tuned knowledge different ours. main novel idea work introduces modelling explicitly shared abstraction state-action space reﬁned throughout learning process optimizing value functions. ability change representation throughout learning process model improving policies crucial. option-like features could emerges already incorporate transition model good policies generalize tasks shown previous section. methods investigated study sparsity multi-task closely related learning procedure work seen generalization method although focus model assumptions quite different. perhaps relevant prior work shares vision modelling assumption approach model shared staterepresentation goals assume linear factorization state embbeding task/goal embbeding. work investigated problem representation learning multi-task/multi-goal reinforcement learning. introduced multi-task paradigm showed popular classes planning algorithms ﬁtted q-iteration approximate policy iteration extended learn multiple tasks jointly. focusing linear parametrization q-function showed least ways harness power well-established multi-task learning transfer algorithms developed supervised settings apply inferring joint structure optimal value functions implicitly policies. argued shown preliminary experiments beneﬁt integrating joint treatment goals exploiting commonality tasks. ought lead efﬁcient learning better generalization. although encouraging results paradigm need investigation assess convergence behaviour scalability complex tasks employing multi-task learning representation learning procedures hope work serve staring point. antos andr´as szepesv´ari csaba munos r´emi. learning value-iteration based ﬁtted policy iteration single trajectory. approximate dynamic programming reinforcement learning adprl ieee international symposium ieee calandriello daniele lazaric alessandro restelli marcello. sparse multi-task reinforcement learning. advances neural information processing systems learning knowledge unsupervised sensorimotor international conference interaction. autonomous agents multiagent systems-volume international foundation autonomous agents multiagent systems zhou jiayu chen jianhui jieping. clustered multi-task learning alternating structure optimization. advances neural information processing systems mnih volodymyr kavukcuoglu koray silver david rusu andrei veness joel bellemare marc graves alex riedmiller martin fidjeland andreas ostrovski georg human-level control deep reinforcement learning. nature schaul horgan daniel gregor karol silver david. universal value function approximators. proceedings international conference machine learning silver david huang maddison chris guez arthur sifre laurent driessche george schrittwieser julian antonoglou ioannis panneershelvam veda lanctot marc mastering game deep neural networks tree search. nature sutton richard precup doina singh satinder. between mdps semi-mdps framework temporal abstraction reinforcement learning. artiﬁcial intelligence figure learned greedy policies value functions maxa) enabling navigation four rooms based share feature subspace discovered multi-task value function learning goals randomly sampled environment. value functions learnt using features ψaso", "year": 2016}