{"title": "What Happened to My Dog in That Network: Unraveling Top-down Generators  in Convolutional Neural Networks", "tag": ["cs.NE", "cs.CV", "cs.LG", "stat.ML"], "abstract": "Top-down information plays a central role in human perception, but plays relatively little role in many current state-of-the-art deep networks, such as Convolutional Neural Networks (CNNs). This work seeks to explore a path by which top-down information can have a direct impact within current deep networks. We explore this path by learning and using \"generators\" corresponding to the network internal effects of three types of transformation (each a restriction of a general affine transformation): rotation, scaling, and translation. We demonstrate how these learned generators can be used to transfer top-down information to novel settings, as mediated by the \"feature flows\" that the transformations (and the associated generators) correspond to inside the network. Specifically, we explore three aspects: 1) using generators as part of a method for synthesizing transformed images --- given a previously unseen image, produce versions of that image corresponding to one or more specified transformations, 2) \"zero-shot learning\" --- when provided with a feature flow corresponding to the effect of a transformation of unknown amount, leverage learned generators as part of a method by which to perform an accurate categorization of the amount of transformation, even for amounts never observed during training, and 3) (inside-CNN) \"data augmentation\" --- improve the classification performance of an existing network by using the learned generators to directly provide additional training \"inside the CNN\".", "text": "top-down information plays central role human perception plays relatively little role many current state-of-the-art deep networks convolutional neural networks work seeks explore path top-down information direct impact within current deep networks. explore path learning using generators corresponding network internal effects three types transformation rotation scaling translation. demonstrate learned generators used transfer top-down information novel settings mediated feature ﬂows transformations correspond inside network. speciﬁcally explore three aspects using generators part method synthesizing transformed images given previously unseen image produce versions image corresponding speciﬁed transformations; zero-shot learning provided feature corresponding effect transformation unknown amount leverage learned generators part method perform accurate categorization amount transformation even amounts never observed training; data augmentation improve classiﬁcation performance existing network using learned generators directly provide additional training inside cnn. deep learning many recent successes; example deep learning approaches made strides automatic speech recognition visual object recognition machine translation successes demonstrate wide-ranging effectiveness deep learning approaches remains useful information current deep learning less able bring bear. take speciﬁc example consider much current deep learning practice dominated approaches proceed input output fundamentally bottom-up fashion. current performance extremely impressive strongly bottom-up characteristics leave room whether providing deep learning ability also incorporate top-down information might open path even better performance. demonstrated role top-down information human perception provides suggestive indication role top-down information could play deep learning. visual illusions provide clearest examples strong effect top-down/prior information human perception; beneﬁts top-down information human perception widespread subtler notice prominent examples include color constancy interpretation visual scenes would otherwise relatively meaningless another particularly common experience human ability focus speciﬁc conversation noisy room distinguishing relevant audio component among potentially overwhelming interference. motivated importance top-down information human perception well successful incorporation top-down information non-deep approaches computer vision pursue approach bringing topinformation current deep network practice. potential beneﬁts incorporating topinformation deep networks include improved prediction accuracy settings bottominformation misleading insufﬁciently distinctive well generally improved agreement multiple classiﬁcation predictions made single image particularly appealing direction future work top-down information improve resistance adversarial examples related work incorporation top-down information visual tasks stands intersection three ﬁelds cognitive science computer vision deep learning. succinctly inspiration cognitive science prior examples computer vision actual instantiation deep learning. consider turn. cognitive science even stroop’s work noted human perception world simple direct path from e.g. photons reaching retina interpretation world around researchers established pervasive important role top-down information human perception striking demonstrations role top-down information human perception come form visual illusions incorrectly perceiving concave side plastic chaplin mask convex beneﬁts top-down information easy overlook simply top-down information often playing role smooth functioning perception. sense beneﬁts consider absence top-down information human perception would trouble useful abilities establishment color constancy across widely varying illumination conditions interpretation images might otherwise resemble unstructured jumble dots non-deep computer vision observations role top-down information human perception inspired many researchers computer vision. widely-cited work topic considers human perception machine perception chain research stretches back even early days computer vision research recent works demonstrating performance beneﬁts top-down information tasks object perception include deep computer vision recent related works computer vision distinct differences goal approach however. whereas spatial transformer networks pursue architectural addition form might describe learned standardizing preprocessing inside network primary focus exploring effects types transformations consider. also investigate method using explored effects improve vanilla alexnet performance imagenet. hand state goal directly impose good transformation properties representation space pursue group theoretic approach; contrast approach centered effects representations existing namely alexnet. also point approach suitable dealing images much larger able pursue application involving entire imagenet dataset. another recent work modeling random ﬁelds convolutional layers; however perform image synthesis study explicit top-down transformations. image generation cnns part exploration make recent work generating images corresponding internal activations cnn. special purpose method presented method generally applicable speciﬁc formulation inversion problem leads generated images signiﬁcantly differ images network trained with. technique suited purposes subsequent visualizations. feature ﬂows intermediate steps process computation feature ﬂows vector ﬁelds computed using siftflow approach features used place sift features. existing work touched usefulness vector ﬁelds derived feature ﬂows. related much theoretical diffeomorphism-based perspective another early reference touching ﬂows however ﬂows computed image pixels rather features. uses feature ﬁelds means visualizing spatio-temporal features learned convolutional gated also tasked image analogy problem. image analogy problem also present work focusing gated boltzmann machines; image analogy performed ﬁeld gated experts ﬂow-ﬁelds used visualizations. rather pursue special purpose re-architecting enable performance zero-shot learning-type image analogy tasks pursue approach works existing trained object classiﬁcation speciﬁcally alexnet focus experiments subset afﬁne image operations rotation scaling translation. order avoid edge effects might arise performing operations images object near boundary image provided meta information select suitable images. pipeline generator learning select within images ilsvrc cls-loc task experiments rotate/scale/translate central object; wish central object remain entirely image transformations. bounding information ensure case select images bounding centered square rectangular occupies pixels image. images satisfy requirements; subsequently refer images original images. generating transformed image pairs rotation running example. ease reference notation denote transformed version original image central object rotated orientation degrees; original image using notation consider image pairs difference image amount rotation central object. example pair central object orientation θinit ﬁrst image orientation θinit second. begin consider regularly-spaced values initial orientation angle θinit value rotation amount means original images pairs form total pairs focus subsequent processing rotation experiments. computing alexnet features next transformed image pair caffe library’s pretrained reference alexnet model compute alexnet features associated image pair. using notation denote collection alexnet feature values resulting image input means collected alexnet features pairs form alexnet layers parameters ﬁrst convolutional layers ﬁnal fully connected layers attention focused convolutional layers rather fully connected layers since convolutional layers retain spatial layout corresponds original image fully connected layers lack spatial layout. computing per-layer feature flows ease reference introduce notation refer alexnet features layer input image entire network image features pair focus attention layer time; layer relevant pair particular convolutional layer pair compute feature vector ﬁeld best describes values values compute feature vector ﬁelds using siftflow method however instead computing sift features compute alexnet features. figure illustration computed feature vector ﬁelds. layer feature pair refer corresponding feature recalling compute feature ﬂows convolutional layers collecting feature vector ﬁelds {conv conv} results total values; collectively refer collected-across-conv-layers feature vector ﬁelds ﬂatten/vectorize feature vector ﬁeld collections pair row-stack vectorized ﬁelds obtain matrix rows columns feature flow order characterize primary variations collected feature vector ﬁelds perform matrix rows/examples columns/feature component values. retain ﬁrst eigenvectors/principal component directions contains feature vector ﬁeld component values. figure components feature ﬁelds associated rotation. ﬁrst second third rows show respectively mean ﬁrst second principal components. ﬁrst second third forth columns show respectively results conv conv conv conv. best viewed screen. denote eigen feature ﬁelds upper case letter intended recall that reshaping plot ﬁelds spatial layout corresponding associated alexnet convolutional layers. also recall eigen feature ﬁelds computed based feature pairs rotation. together mean feature ﬁeld subsequently denoted stacked ﬂattened/vectorized ‘eigen’ feature ﬁelds feature component values provide ability re-represent per-pair stacked ﬂattened/vectorized feature ﬁelds terms coefﬁcients. re-represent example stacked ﬂattened/vectorized ’pair’ feature ﬁelds coefﬁcient associated mean always equal however shortly consider setting allow coefﬁcient associated mean take values using feature flow obtain bases expressing generators taken together mean feature ﬁeld ‘eigen’ feature ﬁelds provide ability produce rerepresentations example stacked ﬂattened/vectorized rotation’ feature ﬁelds. vectors determined case feature ﬁelds associated rotations. next seek vectors bases terms seek determine alternative representations ﬁelds associated amounts rotation. particular seek regression coefﬁcients representation ﬁelds associated feature pairs derived rotation varying amounts central object. speciﬁcally follow steps feature computation process detailed earlier section using together previous θinit regression equation associated feature example form matrix containing groups columns form group basis vectors provided different different rotation conditions enabling better ﬁts. vector similarly regarded containing groups coefﬁcient values finally right hand side instance collected-across-conv-layers feature vector ﬁelds described earlier. regression expressions transform image pair since current setting original images initial orientation angles θinit rotation amounts total pairs. ease reference refer vertically stacked basis matrices value used computing associated transform image pair example regression described eqn. r.e×. similarly refer vertically stacked feature vector ﬁeld vectors form modiﬁed basis regression problem thus succinctly expressed refer minimizing argument wlsq transformations learned generators least squares coefﬁcient values wlsq predict feature ﬁelds associated speciﬁed number degrees rotation. particularly rotation degree amounts degree amounts used generated transform image training pairs used least-squares regression calibration eqn. obtain desired generator decide speciﬁc number degrees rotation speciﬁcity describe dimension total rows come vertically stacked one-per-image-pair matrices rows. thus total number rows original images initial orientations rotation amounts entries feature ﬁeld collection image pair. desired; using speciﬁed degree amount basis vectors generate corresponding column effective basis matrix sought-for generator ·wlsq element speciﬁcity refer generator arising speciﬁed rotation angle ·wlsq. could describe generators predicted speciﬁed feature ﬂows; however since generate novel feature values layers network refer generator ﬁelds simply generators. figure illustration applying learned generators features novel input image. input image. inverted image conv features show respectively inverted images conv features obtained applying learned generator ﬁelds associated rotation rotation scaling factor scaling factor translation pixels left translation pixels describe learned generators follows given collection features apply learned generator obtain feature values would arisen applying associated transformation input image. seek investigate generator ﬁelds generically order produce approximation exact features would observed e.g. rotate original image compute resulting alexnet features. speciﬁc example consider transform image pair notation corresponding alexnet feature response pair seek generator provide estimate given visualizations features often difﬁcult interpret. provide interpretable evaluation quality learned generators alexnet inversion technique applying learned generators results fig. indicate resulting features closely correspond would produced usual bottom-up process. quantitative evaluation also check error mean absolute deviation network internal layer features generated using learned generators corresponding feature values would arisen exact bottom-up processing. example looking channels alexnet conv difference generator-produced features bottom-up features associated translate left mean absolute deviation difference generator-produced bottom-up associated scale mean absolute deviation zero-shot learning seen learned generators used produce features corresponding various speciﬁed transformations provided initial features. next seek explore learned generators support zero-shot learning task. running example rotation. typical example zero-shot learning walks like duck... ﬁrst describe typical example zero-shot learning. consider task classifying central objects e.g. imagenet images animals. standard zero-shot learning approach task involves steps. ﬁrst step learn mapping input data intermediate representation second step assume access mapping intermediate semantic property representation class label. example expect wikipedia article text provide information zebra hoofed mammal stripes. training data produce accurate semantic scores hoofs present stripes present present potentially wikipedia-derived association zebra semantic properties bridge semantic properties predicted input image class label associated zebra; signiﬁcantly long predicted semantic properties accurate second part system output zebra whether training data ever contained zebra. quote well-known aphorism walks like duck swims like duck quacks like duck call thing duck. zero-shot learning context typical example zero-shot learning described above task input data vector predicted class label probabilities. task broken steps ﬁrst input data intermediate representation intermediate representation vector predicted class label probabilities. mapping input data intermediate representation learned training; mapping intermediate representation class label assumed provided background information otherwise accessible outside source setting pairs. overall goal determine mapping characterization speciﬁc transformation applied. speciﬁc instance overall goal might presented with e.g. input pair return output analogous animal example discussed above break overall mapping steps ﬁrst mapping step takes input pairs collected per-layer feature vector ﬁelds. second mapping step takes input collected per-layer feature vector ﬁelds output characterization speciﬁc transformation applied. note that contrast animal example context uses learning second mapping step rather ﬁrst. speciﬁc example description two-part process take previously never-seen image central object. obtain produce another image central object rotated amount. push original image alexnet collect resulting alexnet features layers. push rotated-central-object image alexnet collect resulting alexnet features layers. layer compute feature vector ﬁeld; ﬁrst mapping step. second mapping step takes collection computed per-layer feature vector ﬁelds predicts angle rotation applied pair images process started with. context learned generator second mapping step. discuss details approach zero-shot learning. details zero-shot learning task speciﬁc exploratory task evaluate feasibility zero-shot learning described follows generated pairs. computed feature ﬂows pairs. performed feature ﬂows determine effective basis matrix associated rotation angle calibration regression resulting wlsq vector least-squares coefﬁcients make feature predictions terms effective basis matrix. initial zero-shot learning prediction task categorize rotation angle used image pair greater less compare performance approach standard approach train siamese conﬁguration take input pairs form produce output prediction angle rotation images pair. branch siamese network receives initial image input; branch receives input central object rotated. branch structured match alexnet layers conv pool including stack channels respective pool layers branch. resulting stack provided input fully-connected layer units; takes units input produces units output; ﬁnally produces single scalar output probability rotation angle used image pair greater less test previously unseen image pairs orientation angles ranging initial zero-shot learning approach yields correct categorization time. structure comparison question many image pairs required train ‘siamese’ network level prediction performance comparable zero-shot approach? observe training pairs siamese network attains correct categorization; pairs performance improves pairs ﬁnally pairs previously illustrated ability learned generators produce variety predicted feature response maps corresponds exact feature response would arisen standard bottom-up approach; describe learned generators perform data augmentation. ground discussion consider initial image perform standard data augmentation might apply variety rotations initial image possible collection rotation angle amounts chosen notation emphasize index possible rotation angle values. data augmentation process would involve corresponding images network training proceeds usual fashion whichever transformed input image corresponding alexnet feature collection would computed used produce loss values backpropagate updates network parameters. observation learned generators produce network-internal fashion alexnet internal features akin listed n]}. speciﬁcally running rotation example learned produce predictions ﬁeld associated speciﬁed rotation angle. mentioned previously refer learned generator layer associated rotation angle regard process applying learned generator example layer alexnet features method producing feature values akin emphasize notion denote values obtained applying learned generator ﬁeld layer alexnet features using newly-established notation express proposed data augmentation follows initial image compute alexnet features desired rotation determine associated learned generator ﬁeld apply generator ﬁeld example obtain predicted feature values standard feedforward computation proceed layer produce prediction receive loss begin backpropagation process update network parameters according generated feature example. backpropagation process involves subtlety. generator means forward path network experiences warp features. correctly propagate gradients backpropagation process path gradient values follow experience inverse forward warp. describe additive inverse gridded vector ﬁeld fairly simply every vector initial ﬁeld corresponding vector inverse ﬁeld; component values negated root inverse vector placed head forward vector. inverse ﬁeld thus cancels forward ﬁeld. unfortunately exact inverse vector root locations grid used forward vector ﬁeld. obtain approximate inverse vector ﬁeld negating forward vector ﬁeld components. tests approximation often quite good; fig. using approximate inverse warp learned generator warp used context network internal data augmentation training. fig. illustration. figure illustration network internal data augmentation succinctly described generator training. left show schematic modiﬁed alexnet architecture use. primary difference incorporation conv module applying randomly selected learned generator ﬁeld. right provide comparison selected conv channels applying scale learned generator ﬁeld; applying generator ﬁeld. task. batch images training randomly select learned generator ﬁelds apply initial features conv. speciﬁcally randomly select rotation rotation scaling scaling translation pixels left translation pixels apply inverse warp backpropagating conv. evaluate performance images ilsvrc cls-loc validation set; table acknowledgments work supported iis- award northrop grumman contextual robotics grant. gratefully acknowledge support nvidia corporation donation tesla used research. thank chen-yu jameson merkow assistance saining huang helpful discussion. hinton geoffrey deng dong dahl george mohamed abdel-rahman jaitly navdeep senior andrew vanhoucke vincent nguyen patrick sainath tara kingsbury brian. deep neural networks acoustic modeling speech recognition shared views four research groups. ieee signal processing magazine nguyen yosinski jason clune jeff. deep neural networks easily fooled high conﬁdence predictions unrecognizable images. proceedings ieee conference computer vision pattern recognition russakovsky olga deng krause jonathan satheesh sanjeev sean huang zhiheng karpathy andrej khosla aditya bernstein michael berg alexander fei-fei imagenet large scale visual recognition challenge. ijcv simard patrice lecun yann denker john victorri bernard. transformation invariance pattern recognition—tangent distance tangent propagation. neural networks tricks trade springer szegedy christian zaremba wojciech sutskever ilya bruna joan erhan dumitru goodfellow fergus rob. intriguing properties neural networks. arxiv preprint arxiv. figure visualizations mean feature computed across original images whose selection described section leftmost column contains visualizations computed features arrived input image pairs differ rotation central object; center column input image pairs differ central object scaled rightmost column input image pairs differ central object scaled moving bottom within column feature ﬁelds shown respectively conv pool conv. figure conﬁrm negation generator ﬁeld good approximation additive inverse generator ﬁeld. since inverted images conv detail perform qualitative evaluation conv. since actual training uses conv perform quantitative evaluation conv. entry above begin alexnet conv features original taco image. inverted image corresponding untouched features found left. entry apply different learned generator followed negation. close correspondence images inverted resulting features image inverted untouched features conﬁrms quality approximation. moving quantitative evaluation feature values arising applying generator ﬁeld followed negation differs original alexnet conv feature values across channels conv follows approximate inverse rotate yields approximate inverse scale yields translation left approximation incurs error boundaries region yielding", "year": 2015}