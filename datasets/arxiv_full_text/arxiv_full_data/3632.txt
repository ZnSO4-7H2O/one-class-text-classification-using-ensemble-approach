{"title": "Fast Subspace Clustering Based on the Kronecker Product", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Subspace clustering is a useful technique for many computer vision applications in which the intrinsic dimension of high-dimensional data is often smaller than the ambient dimension. Spectral clustering, as one of the main approaches to subspace clustering, often takes on a sparse representation or a low-rank representation to learn a block diagonal self-representation matrix for subspace generation. However, existing methods require solving a large scale convex optimization problem with a large set of data, with computational complexity reaches O(N^3) for N data points. Therefore, the efficiency and scalability of traditional spectral clustering methods can not be guaranteed for large scale datasets. In this paper, we propose a subspace clustering model based on the Kronecker product. Due to the property that the Kronecker product of a block diagonal matrix with any other matrix is still a block diagonal matrix, we can efficiently learn the representation matrix which is formed by the Kronecker product of k smaller matrices. By doing so, our model significantly reduces the computational complexity to O(kN^{3/k}). Furthermore, our model is general in nature, and can be adapted to different regularization based subspace clustering methods. Experimental results on two public datasets show that our model significantly improves the efficiency compared with several state-of-the-art methods. Moreover, we have conducted experiments on synthetic data to verify the scalability of our model for large scale datasets.", "text": "abstract. subspace clustering useful technique many computer vision applications intrinsic dimension high-dimensional data often smaller ambient dimension. spectral clustering main approaches subspace clustering often takes sparse representation low-rank representation learn block diagonal self-representation matrix subspace generation. however existing methods require solving large scale convex optimization problem data points. therefore eﬃciency scalability traditional spectral clustering methods guaranteed large scale datasets. paper propose subspace clustering model based kronecker product. property kronecker product block diagonal matrix matrix still block diagonal matrix eﬃciently learn representation matrix formed kronecker product smaller matrices. model thermore model general nature adapted diﬀerent regularization based subspace clustering methods. experimental results public datasets show model signiﬁcantly improves eﬃciency compared several state-of-the-art methods. moreover conducted experiments synthetic data verify scalability model large scale datasets. many computer vision applications face recognition texture recognition motion segmentation visual data well characterized subspaces. moreover intrinsic dimension high-dimensional data often much smaller ambient dimension motivated development subspace clustering techniques simultaneously cluster data multiple subspaces also locate low-dimensional subspace class data. clustering methods among approaches spectral clustering methods intensively studied simplicity theoretical soundness empirical success. methods based self-expressiveness property data lying union subspaces. states point subspace written linear combination remaining data points subspace. typical methods falling category sparse subspace clustering low-rank representation uses norm encourage sparsity self-representation coeﬃcient matrix. uses nuclear norm minimization make coeﬃcient matrix low-rank. motivated self-representation based methods developed diﬀerent regularization terms coeﬃcient matrix. example least squares regression uses regularization coeﬃcient matrix. correlation adaptive subspace segmentation uses mixture regularization. low-rank sparse subspace clustering non-negative low-rank sparse construct regularization term blend nuclear norms. nuclear norm achieve accuracy estimating rank real world data subspace clustering log-determinant approximation replaces nuclear norm used non-convex rank approximations. feature selection embedded subspace clustering reveals features equally important recovery low-dimensional subspaces. feature selection nuclear norm non-convex rank approximations give enhanced performance. latent space sparse subspace clustering seeks linear projection data learns sparse representation projected latent low-dimensional space. despite fact variants achieved encouraging results practice critical limitations. approaches idea learn coeﬃcient matrix denotes correlation data points. size coeﬃcient matrix data points decomposition operation solving coeﬃcient matrix computational complexity time consuming size data large eﬃciency approaches guaranteed. experiments also paper show existing methods need several hours normal computer number test data reaches constrains feasibility methods. overcome limitation propose eﬃcient subspace clustering model based kronecker product achieves signiﬁcant reduction computational complexity quadratic using fact data point subspace written linear combination points subspace obtain points lying subspace learning sparsest combination. hence model ﬁrst learn self-representation coeﬃcient matrix formed kronecker product series small sparse matrices. constract similarity matrix based coeﬃcient matrix. propose eﬃcient subspace clustering model based kronecker product. model uses kronecker product small matrices build self-representation coeﬃcient matrix leads signiﬁcant reduction space computational complexity. model adaptive diﬀerent regularization based subspace clustering methods theoretically prove kronecker product approximation model good adaptivity. experimental results large scale synthetic data real world public datasets show method leads signiﬁcant improvement clustering eﬃciency compared state-of-the-art methods also achieving competitive accuracy. contains data points drawn aims sparse representation matrix showing mutual similarity points i.e. since point expressed terms points sparse representation matrix always exists. algorithm ﬁnds solving following optimization problem pointed ﬁnds sparsest representation data vector individually. global constraint solution method inaccurate capturing global structures data. proposed rank appropriate criterion. similar aims low-rank representation solving following optimizassc methods solve robust subspace clustering problem removing errors original data space obtaining good aﬃnity matrix based clean dataset. thus need prior knowledge structure errors usually unknown practice. peng proposed robust subspace clustering method overcomes limitation eliminating eﬀect errors projection space model based thresholding ridge regression large scale datasets. eﬀectiveness kronecker product reducing computational complexity matrix operations present kronecker product based subspace clustering model signiﬁcantly improve eﬃciency existing methods. diﬀerent subspaces. goal subspace clustering segmentation points according subspaces. based self-expressiveness property data lying union subspaces i.e. point subspace written linear combination remaining points subspace obtain points lying subspace learning sparsest combination. fig. left three subspaces normalized data points. right solutions conventional sparse subspace clustering method kronecker product based model shown space computational complexity model achieve signiﬁcant reduction compared conventional method. model aims reduce computational complexity data size rewrite denotes matrix transpose i-th dimension data points. without loss generality smaller matrices rp×q rp×q important property kronecker product block diagonal matrix matrix still block diagonal matrix follow minimize loss self-representation. optimization problem written obtained optimal solution self-representation coeﬃcient matrix i=ci i-th j-th data points diﬀerent subspaces. hence aﬃnity matrix deﬁned |c|+|c|t denotes absolute value matrix segmentation data diﬀerent subspaces obtained applying spectral clustering algorithm aﬃnity matrix whole kronecker product based subspace clustering model summarized algorithm section give theoretical analysis kronecker product based model including adaptivity diﬀerent regularizations theoretical convergence analysis complexity analysis. since many self-representation based methods diﬀerent regularizations coeﬃcient matrix show model applied variety diﬀerent regularizations. refer subspace clustering method described section krtrr utilizes frobenius norm regularize coeﬃcient matrix. simplify sparsity proof. assume decompositions respectively. nonzero entries diagonal matrix nonzero entries diagonal matrix diagonal matrix decomposition nonzero entries diagonal matrix c∗c∗. refer methods krssc krlrr. following preprocess data dpca retain spatial information data. krtrr method learn coeﬃcient matrix done refer method krnvr. optimization variants kronecker product based method essentially krtrr. summary leverage kronecker product reduce computational complexity learning coeﬃcient matrix diﬀerent regularization options e.g. frobenius norm norm nuclear norm. present four methods krssc krlrr krtrr krnvr based diﬀerent regularizations compare baseline methods section eigenvalue problem terms vector minimizing must proportional eigenvector corresponding largest eigenvalue words arbitrary matrix dimension partition based dimensions small matrices needed approximate large matrix kronecker product. moreover small matrices always convergent solution largest eigenvector partitioned large matrix. means technique used approximate large self-representation matrix kronecker product small matrices model reliable. leverage kronecker product small matrices approximate self-representation coeﬃcient matrix number small matrices size small matrices thus space complexity methods learning process self-representation coeﬃcient matrix size existing methods decomposition operation whose computational complexity methods update small matrix time size small matrix achieve computational complexity. since signiﬁcant reduction memory conducted three sets experiments real synthetic datasets verify eﬀectiveness proposed methods. several state-of-the-art classical spectral subspace clustering methods taken baseline algorithms. included sparse subspace clustering low-rank representation thresholding ridge regression nonlinear variance regularized ridge regression experiments used codes provided respective authors computing self-representation matrix parameters tuned give best clustering accuracy. applied normalized spectral clustering aﬃnity matrix evaluation criteria used clustering accuracy running time whole clustering process evaluate performance subspace clustering methods clustering accuracy calculated subspaces commonly used capture appearance faces varying illuminations test performance method face clustering database database contains images people diﬀerent poses diﬀerent illumination conditions diﬀerent expressions. experiment used face images near table average running time clustering accuracy database diﬀerent number objects. object consists face images diﬀerent illuminations expressions. denotes computational cost unacceptable memory time limit. proposed method. models number small matrices diﬀerent number objects randomly chose people trials took images subsets clustered. conducted experiments subsets report average running time clustering accuracy diﬀerent number objects table corresponding objects face. shown table eﬃciency alternative methods degrades drastically increases. space computational complexity methods unacceptable contrast computational time kronecker product based methods signiﬁcantly lower compared corresponding approaches. methods easily handle data points acceptable computing time. further table kronecker product based methods also obtain competitive clustering accuracy suggests model potentially suitable previous methods large scale dataset real world applications. database handwritten digits also widely used subspace learning clustering. test proposed methods handwritten digit clustering mnist dataset dataset contains clusters including handwrit handwritten digit images subspace clustering. diﬀerent experimental settings face clustering ﬁxed number clusters chose diﬀerent number data points cluster trials. cluster contains data points randomly chosen corresponding images number points applied methods dataset seen eﬃciency krssc krlrr krtrr krnvr signiﬁcantly outperform corresponding baseline methods indicates eﬀectiveness kronecker product method proposed paper. table also shows method variants obtain competitive clustering accuracy compared corresponding baseline methods. verify scalability method large scale datasets also experiments synthetic data. following randomly generated subspaces dimension ambient space dimension subspace contains data points randomly generated unit sphere number points memory time limit models number small matrices diﬀerent number sample points conducted experishown table advantage method variants baseline methods marked large scale datasets. dataset size reaches computational running time alternate methods comparison hours each kronecker product based methods need thousand seconds even data points. table running time decreases signiﬁcantly. clustering accuracy also guaranteed compared existing methods. limitations memory space computational complexity alternative methods applied dataset larger points. suggests methods potentially suitable large real world applications. here report experimental results synthetic dataset illustrate sensitivity kronecker product based methods parameter variations. parameters model related dataset size table shows average running time clustering accuracy running time signiﬁcantly decreases sacriﬁce clustering accuracy. implies number small matrices determined size dataset compromise eﬃciency accuracy. figure shows clustering accuracy methods diﬀerent balance parameter presented fast subspace clustering model based kronecker product. property kronecker product block diagonal matrix matrix still block diagonal matrix learn representation matrix spectral clustering using kronecker product smaller matrices. thanks superiority kronecker product reducing computational complexity matrix operations memory space computational complexity methods achieve signiﬁcant eﬃciency gain compared several baseline approaches presented four variants kronecker product based method namely krssc krlrr krtrr krnvr. experimental results face clustering handwriting digit clustering show methods achieve signiﬁcantly improvement eﬃciency compared state-of-the-art methods. moreover presented results synthetic data veriﬁed scalability methods large scale datasets.", "year": 2018}