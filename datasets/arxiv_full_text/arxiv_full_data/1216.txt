{"title": "Why M Heads are Better than One: Training a Diverse Ensemble of Deep  Networks", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Convolutional Neural Networks have achieved state-of-the-art performance on a wide range of tasks. Most benchmarks are led by ensembles of these powerful learners, but ensembling is typically treated as a post-hoc procedure implemented by averaging independently trained models with model variation induced by bagging or random initialization. In this paper, we rigorously treat ensembling as a first-class problem to explicitly address the question: what are the best strategies to create an ensemble? We first compare a large number of ensembling strategies, and then propose and evaluate novel strategies, such as parameter sharing (through a new family of models we call TreeNets) as well as training under ensemble-aware and diversity-encouraging losses. We demonstrate that TreeNets can improve ensemble performance and that diverse ensembles can be trained end-to-end under a unified loss, achieving significantly higher \"oracle\" accuracies than classical ensembles.", "text": "iii) reducing estimation optimization errors ensemble averaging reduces variance base models averaging variations objective function nonconvexity initialization stochastic learning heart arguments idea diversity train multiple learners decorrelated errors predictions averaged improve performance work rigorously treat ensembling problem right examining multiple ensembling strategies ranging standard bagging parameter sharing ensemble-aware losses. compare methods across multiple datasets architectures demonstrating standard techniques suitable deep ensembles novel approaches improve performance. ensemble-aware losses. typically ensemble members trained independently unifying loss despite fact outputs combined test time. common classical literature view ensemble members experts specialists typical practice effort made encourage diversity specialization. seems natural question whether ensemble-aware loss might result better performance. study ensemble-aware losses directly training ensemble minimize loss ensemble mean generalization multiple choice learning explicitly encourage diversity. parameter sharing. number papers demonstrated initial layers cnns tend learn simple generic features vary little models deeper layers learn features speciﬁc particular task input distribution propose family tree-structured deep networks exploit generality lower layers sharing across ensemble members reduce parameters. investigate depth sharing happen along spectrum single models independent ensembles coupling lower layers naturally forces diconvolutional neural networks achieved state-ofthe-art performance wide range tasks. benchmarks ensembles powerful learners ensembling typically treated post-hoc procedure implemented averaging independently trained models model variation induced bagging random initialization. paper rigorously treat ensembling ﬁrstclass problem explicitly address question best strategies create ensemble? ﬁrst compare large number ensembling strategies propose evaluate novel strategies parameter sharing well training ensemble-aware diversity-encouraging losses. demonstrate treenets improve ensemble performance diverse ensembles trained endto-end uniﬁed loss achieving signiﬁcantly higher oracle accuracies classical ensembles. convolutional neural networks shown impressive performance wide range computer vision tasks. important fact state-of-the-art models generally ensembles cnns including nearly performers imagenet large scale visual recognition challenge example googlenet best-performing models submitted ilsvrc challenge ensemble achieving percentage point increase accuracy single base model architecture. ensembles multiple classiﬁers trained perform task predictions averaged generate typically accurate prediction. number related justiﬁcations given success ensembles including versity ensemble members concentrated deeper unshared layers. perhaps somewhat surprisingly optimal setting classical ensemble instead treenet shares initial layers. thus tree-structured networks simple improve performance reducing parameters. model-distributed training coupled ensembles. unfortunately approaches coupling ensemble members either architecture ensemble-aware losses operate outputs ensemble members bottom parameter sharing treenets create signiﬁcant computational difﬁculties. since networks independent longer possible train separately parallel sequential training require months time even relatively small ensembles. moreover even training time concern larger models often take available possible ensemble gpu. overcome hurdles present release novel mpi-based model-parallel distributed modiﬁcation popular caffe deep learning framework implements cross-process communication layers cnn. thoroughly evaluate methodology across multiple datasets network architectures. experiments cast light ensembles deep networks demonstrating effects randomization parameter data space parameter sharing uniﬁed losses modern scale vision problems. concretely neural networks ensembles techniques improve robustness diversity grouped learners decades work machine learning research recently ensembles cnns studied. related work broadly divided categories ensemble learning general networks recent application cnns. ensemble learning theory. neural networks applied wide variety settings many diverse modiﬁcations. much theoretical foundation ensemble learning neural networks laid krogh hansen salamon provided theoretical empirical evidence diversity error distributions across member models boost ensemble performance. ensemble methods averaged predictions models trained different initializations models trained different bootstrapped training sets methods take indirect approach introducing diversity ensembles. work explicitly trained decorrelated ensembles neural networks penalizing positive correlation error distributions effective shallow networks methods applied deeper architectures. although initially proposed structured svms work guzman-rivera multiple choice learning provides attractive alternative require computing correlation error. related ideas studied context submodular list prediction. generalize apply cnns incorporate stochastic gradient descentbased training. ensembles. ensembles cnns used extensively little work focused improving ensembling process. ensembles multiple random initializations training data subsets inject diversity. example popular ensembles alexnet simply retrain different initializations average predictions. googlenet induces diversity straightforward bagging training model sampled dataset. networks like sequence sequence rnns approaches simultaneously. parameter sharing novel development cnns effect ensembles studied. recent related work bachman proposed general framework called pseudo-ensembles training robust models. deﬁne pseudo-ensemble group child models instances parent model perturbed noise process. explicitly encourage correlation model parameters parent pseudo-ensemble agreement regularizer. although outwardly related parameter sharing pseudo-ensembles fundamentally different techniques presented here parameter sharing train single robust model rather produce ensemble fewer parameters. recent work sercu uses parameter sharing context multi-task learning build common representation multilingual translation. finally dropout interpreted procedure trains exponential number highly related networks cheaply combines network similar relevant recent work brieﬂy focuses ensembles. members types ensemble specialists trained subsets possible labels subset manually designed include easily confused labels. models ﬁne-tuned shared generalist combined make ﬁnal prediction. contrast diversity-encouraging loss require human hand-designing class specialization loss naturally image classiﬁcation benchmarks cifar cifar imagenet large scale visual recognition challenge since goal present designs architectures tasks rather study effect different ensembling techniques unless otherwise noted standard models training routines. models trained stochastic gradient descent momentum weight decay. cifar. dataset caffe cifar quick network base model. reference model trained using batch size iterations momentum weight decay initial learning rate drops after iterations. refer network training procedure cifar-quick. cifar. network network model well reference training procedure runs iterations batch size momentum weight decay initial learning rate learning rate decays factor whenever training loss fails drop least iterations; occurs twice course typical training run. reference model’s accuracy lower reported perform dataset normalization procedure. refer network training procedure cifar-nin. ilsvrc. dataset network network model caffenet networks trained iterations initial learning rate momentum weight decay batch size learning rate reduced factor every iterations. caffenet batch size learning rate schedule accelerated reducing every iterations. refer models ilsvrc-nin ilsvrc-alex respectively. accuracy standard test-time procedure ensembles averaging beliefs members predicting conﬁdent class. strong performance metric indicates ensemble members generally agree correct response errors reduced smoothing across contrast oracle accuracy accuracy members. ensemble oracle selects prediction accurate ensemble member example. oracle accuracy demonstrates ensemble knows collection specialists used prior work measure ensemble performance present analysis different approaches training ensembles. section focuses standard approaches sections present novel ideas parameter sharing ensemble-aware losses. randomly initializing network weights randomly resampling dataset subsets perhaps commonly-used methods create model variation members ensembles. table presents results using three different ensembling techniques random initialization member models training data initialized using different random seeds bagging member uses initial weights trains subset data sampled original combined uses techniques. numbers table accuracies standard deviations across three trials. cifar ensembles built four members ilsvrc ensembles ﬁve. expected ensembles improve performance single base model. somewhat surprisingly bagging reduces ensemble-mean accuracy compared random initialization alone oracle accuracy remains nearly constant. result suggests bagged networks poorly calibrated conﬁdent incorrect responses negatively impacting results. individual member networks also perform worse trained original dataset. attribute table comparison standard ensembling techniques. ensembles outperform base models bagging shows smaller gains resulting reduced training data. results reduction unique training exemplars bagging introduces. given initial dataset examples draw points replacement make bagged probability example sam. expected fraction examples drawn least thus large i.e. bagging costs third unique data points losing data also introducing many duplicated data points. examine whether duplicates affect performance reran cifar experiments dataset unique examples found similar reductions accuracy indicating loss unique data primary negative effect bagging. note convex shallow models loss unique exemplars bagging typically acceptable random parameter initialization simply insufﬁcient produce diversity. best knowledge ﬁrst ﬁnding establish random initialization sufﬁcient preferred bagging deep networks given large parameter space necessity large training data. ensembles single models seen endpoints spectrum approaches single models require careful allocation parameters perform well ensembles extract much performance possible multiple instances base model. ensemble approaches likely introduce wasteful duplication parameters generic lower layers increasing training time model size. hierarchical nature cnns makes well-suited alternative ensembling approaches member models beneﬁt shared information lower layers retaining advantages classical ensembling methods. motivated observation section present evaluate family tree-structured ensembles called treenets shown figure treenet ensemble consisting zero shared initial layers followed branching point zero independent layers. training shared layers branch receive gradient information child network accumulated according back-propagation. test time path root leaf considered independent network except redundant computations shared layers need performed. evaluated novel treenet models larger architectures trained imagenet ilsvrc-alex ilsvrc-nin table presents results. table shows ensemble-mean accuracy achieved treenets splits different depths. example splitting conv means layers including conv shared branches independent afterwards. since layers contain parameters unaffected parameter sharing show results splitting parameterized layers. shared parameter networks retain performance full ensembles outperform them. best ilsvrc-nin treenet improve accuracy standard ensembles reducing parameter count lower layer representations though simple generic still room improvement. sharing level weights weight updated multiple sources supervision branch. indicates treenets could provide regularization favors slightly better level representations. evidence claim looking individual branches treenet compared independently trained networks ensemble. regardless split point treenet branch shared ensemble achieved around percentage points higher accuracy independent ensemble members. unlike classical ensembles member model performs well base architecture treenets seem boost performance ensemble individual networks well. also experimented multiple splits leading complicated balanced binary tree structures ilsvrcnin found similar improvements. reduced performance probability-averaged causing greater degradation. counter-intuitive explicitly optimizing performance ensemble-mean worse averaging independently trained models. attribute problems discuss lack diversity numerical instability. averaging outputs reduces diversity. unfortunately averaging scores probabilities training unintended consequence eliminating diversity gradients back-propagated ensemble. consider generic averaging layer expression depend gradients backpropagated ensemble members identical averaging layer responsibility mistakes shared eliminates gradient diversity. different behavior ensemble independently trained networks member receives different gradient depending individual performance. averaging probabilities unstable. attribute further loss accuracy averaging probabilities increased numerical instability. softmax function’s derivative respect input unstable outputs near however paired crossentropy loss derivative loss respect softmax input reduces simple subtraction. unfortunately similar simpliﬁcation cross-entropy average softmax outputs optimization conditions difﬁcult causing loss convergence probability-averaged networks nearly twice score-averaged networks single network. discussed role ensemble diversity context model averaging; however many settings generating multiple plausible hypotheses preferred producing single answer. ensembles naturally space produce multiple answers design. object detection pascal dataset. used fast r-cnn architecture ﬁne-tuned treenet models. test-time bounding-box regression average results member model ensemble. found statistically signiﬁcant increase mean average precision across multiple runs compared starting standard ensemble. take initial experiments imply treenet models least generalizable tasks standard ensembles. details provided supplementary materials. summarize results section found treenets initial layers outperform classical ensembles also fewer parameters reduce test-time computation time memory requirements. previous sections ensemble member trained objective independent cross-entropy ensemble member. happens objective aware ensemble? begin showing surprising result ﬁrst natural idea simply optimizing performance average-beliefs ensemble work provide intuitions case negative result shows careful design ensemble-aware loss functions crucial. propose diversity-encouraging loss function shows signiﬁcantly improved oracle performance. standard ensemble test-time classiﬁcation typically performed averaging output member networks natural explicitly optimize performance corresponding ensemble-mean loss training. four ensemble architectures settings score-averaged average last layer outputs probability-averaged average softmax probabilities ensemble members. intuitively difference settings former assumes ensemble members calibrated produce scores similar relative magnitudes latter not. assumed oracle select however easily generalized select predictors lowest loss varying number predictors trades diversity number training examples predictor sees affects generalization convergence. experimental results. begin experiments cifar-quick network. table shows individual network accuracies oracle accuracy trained ensembles different values increased member network exposed data decreased oracle accuracy exchange increased individual member performance. oracle-set loss reduces independent cross-entropy member producing standard ensemble. case showcases degree model specialization. individual network performs poorly however taken ensemble oracle accuracy clearly shows networks specialized diversiﬁed taking responsibility subset examples. best knowledge ﬁrst work demonstrate behavior. characterize member networks learning tracked test examples assigned ensemble member oracle accuracy metric figure show distribution classes assigned ensemble member results striking almost complete division label space increases increased uniformity distributions. note divisions emerge loss hand-designed pre-initialized way. figure visualize ensemble members respond input images using guided backprop similar deconv visualizations zeiler fergus images interpreted gradient indicated class score respect input image. features clear images largest inﬂuence network’s output given input image. shows visualizations single input image standard network members ensemble. however independently trained models typically converge similar solutions prompting need optimize diversity directly. section develop experiment diversity encouraging losses demonstrate effectiveness specializing ensembles. build multiple choice learning brieﬂy recap here. consider predictors probability distribution labels dataset feature vector ground truth label point view oracle listens correct loss example call oracle set-loss. intuitively given oracle select correct predictor loss example minimum loss predictors. alternatively oracle loss interpreted allowing system guess times scoring example correct guess correct. thus ensemble predictors directly comparable commonly used top-m metric used many benchmarks notice like cross-entropy upper-bound training error expression upper-bound oracle training error guzman-rivera presented coordinate descent algorithm optimizing objective. approach alternates stages ﬁrst data point assigned accurate predictors models trained convergence using assigned examples. even done parallel training multiple cnns convergence iteration intractable. thus interleave assignment step batch updates stochastic gradient descent. batch pass examples network producing probability distributions label space ensemble member. backward pass gradient loss example computed respect predictor lowest error figure percentage test examples class assigned ensemble member oracle degree specialization sharp softens almost uniform guided-backprop images standard trained ensemble members. networks specialized given class agnostic image content. networks specialized given class agnostic image content. supplementary material examples. label-space clustering. shown trained ensembles tend converge labelspace clustering member focuses subset labels. possible label-space clusterings vast results perspective train hand-designed specialist ensembles randomized label assignments. cifar randomly split labels evenly four ensemble members train respect labels. course trials found oracle-accuracy ranged mean shows generally optimization selects high quality label space clusterings respect oracle accuracy. alternative strategy presented diversify members dividing labels clusters hard distinguish classes; brieﬂy described assignments generated clustering covariance matrix label scores computed across input generalist cnn. trained ensemble using clustering method signiﬁcantly decreased oracle performance versus cifar-quick ilsvrc-alex. surprising since optimize oracle accuracy. overcoming data fragmentation. despite training member networks convergence iteration coordinate descent method results improved oracle accuracy standard ensembles. however interleaving assignment step stochastic gradient descent results data fragmentation network seeing fraction batch reduced effective batch size results noisy gradeep networks especially sensitive effects data fragmentation early training errors typically larger. guzman-rivera initial assignments ﬁrst iteration training decided clustering data clusters. contrast assignments approach based network performance initially result random initialization. investigate effect initial phase learning applied loss ﬁne-tune previously trained cifar-quick ensemble. shown table beneﬁts pretraining pronounced lower values data fragmentation severe. pretraining stabilize learning data fragmentation cifar relatively minor problem whereas training scratch larger networks using standard batch sizes consistently failed outperform standard ensembles. attribute combination data fragmentation difﬁculty initial learning. test hypothesis experimented ﬁne-tuning gradient accumulation across batches ilsvrc-alex architecture. accumulated gradients batches updating parameters ﬁne-tuned fully-trained ensemble. table shows result iterations ﬁne-tuning experiment different values setup overcame data fragmentation problem experiments demonstrate mcl’s ability quickly diversify ensemble. push further reran ﬁne-tuning experiment time initializing ensemble members network. despite starting ensemble identical networks oracle accuracy ensemble reached oracle accuracy iterations demonstrated loss effective inducing diversity however member networks specialize much ensemble-mean accuracy suffers. tried linearly combining loss standard cross-entropy balance diversity general performance. training loss improves cifar ensemble-mean accuracy standard ensemble. section developed novel framework shown produces ensembles substantially improved oracle accuracies training scratch even ﬁne-tuning single network. training ensemble single prohibitively expensive standard practice large ensembles train multiple networks either sequentially parallel. however form model coupling requires communication learners. make enable experiments scale developed release modiﬁcation caffe call mpi-caffe uses message passing interface standard enable crossgpu/machine communication. communication operations provided caffe model layers allowing network designers quickly experiment distributed networks different parts model reside different gpus machines. figure shows ensemble cifar-quick networks parameter sharing model averaging deﬁned single speciﬁcation distributed across multiple process. mpi-caffe process assigned identiﬁer setting ranks network layer belongs easily design distributed ensembles. figure mpi-caffe models deﬁned single network speciﬁcation distributed across multiple gpus. dashed lines indicate cross-process communication input/output. pass accumulates gradients backpropagation. forward pass mpigather collects inputs multiple processes outputs single network backward pass simply routes gradients back corresponding input. tested mpi-caffe framework large-scale cluster telsa node maximum node interconnect bandwidth gb/sec. characterize communication overhead ensemble measure time spent sharing various layers ilsvrc-alex× architecture. largest layer shared pool amounts broadcasting nearly million ﬂoats batch. despite layer’s size forward-backward pass time used communication. details available supplement. running theme behind ideas presented paper diversity. experiments bagging demonstrate diversity induced ensemble members random parameter initializations useful introduced bags duplicated examples. experiment explicitly training ensemble-mean performance show averaging beliefs ensemble members computing losses unintended effect removing diversity gradients. novel diversity-inducing loss shows encouraging diversity ensemble members signiﬁcantly improve performance. finally novel treenet architecture shows diversity important high-level representations low-level ﬁlters might better without training large-scale architectures made practical mpi-caffe framework. future work would like adapt framework structured predictions problems. structured context space good solutions quite large feel diverse models even greater beneﬁt.", "year": 2015}