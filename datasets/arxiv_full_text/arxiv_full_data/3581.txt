{"title": "Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Gradient-based meta-learning has been shown to be expressive enough to approximate any learning algorithm. While previous such methods have been successful in meta-learning tasks, they resort to simple gradient descent during meta-testing. Our primary contribution is the {\\em MT-net}, which enables the meta-learner to learn on each layer's activation space a subspace that the task-specific learner performs gradient descent on. Additionally, a task-specific learner of an {\\em MT-net} performs gradient descent with respect to a meta-learned distance metric, which warps the activation space to be more sensitive to task identity. We demonstrate that the dimension of this learned subspace reflects the complexity of the task-specific learner's adaptation task, and also that our model is less sensitive to the choice of initial learning rates than previous gradient-based meta-learning methods. Our method achieves state-of-the-art or comparable performance on few-shot classification and regression tasks.", "text": "gradient-based meta-learning shown expressive enough approximate learning algorithm. previous methods successful meta-learning tasks resort simple gradient descent meta-testing. primary contribution mt-net enables meta-learner learn layer’s activation space subspace task-speciﬁc learner performs gradient descent additionally task-speciﬁc learner mt-net performs gradient descent respect meta-learned distance metric warps activation space sensitive task identity. demonstrate dimension learned subspace reﬂects complexity task-speciﬁc learner’s adaptation task also model less sensitive choice initial learning rates previous gradient-based meta-learning methods. method achieves state-of-the-art comparable performance few-shot classiﬁcation regression tasks. recent deep learning methods achieve superhuman performance various tasks including image classiﬁcation playing games using copious amounts data computational resources. many problems interest learners luxuries. meta-learning methods potential solution problem; methods leverage information gathered prior learning experience learn effectively novel tasks. line research typically casts learning two-level process different scope. meta-learner operates level tasks gathering information several instances task-speciﬁc learners. task-speciﬁc learner hand operates level datapoints incorporates meta-learner’s knowledge learning process. model-agnostic meta-learning meta-learning method directly optimizes gradient descent procedure task-speciﬁc learners. task-speciﬁc learners maml share initial parameters meta-learner optimizes initial parameters gradient descent starting initial parameters quickly yields good performance. implicit assumption meta-learner operate space task-speciﬁc learners different scopes learning require equal degrees freedom. primary contribution mt-net neural network architecture task-speciﬁc learning procedure. mt-net differs previous gradient-based meta-learning methods meta-learner determines subspace corresponding metric task-speciﬁc learners learn thus setting degrees freedom task-speciﬁc learners appropriate amount. activation space cell shown figure task-speciﬁc learning mt-net. cell consists layers. addition initial weights meta-learner speciﬁes weights changed task-speciﬁc learners activation cell dimensions activation task-speciﬁc learners change within subspace value affects task-speciﬁc learning gradients sensitive task identity. best seen color. figure -dimensional task-speciﬁc learners change weights affect three intermediate activations task-speciﬁc learning happens subspace degrees freedom. additionally meta-learned parameters alter geometry activation space task-speciﬁc parameters task-speciﬁc learners sensitive change task. brieﬂy explain meta-learning problem setup applied few-shot tasks. problems k-shot regression classiﬁcation follows. training phase meta-learner given tasks {ttt task provides training test {dtitraindtitest}. assume training dtitrain examples class hence name k-shot learning. particular task {ttt assumed drawn distribution tasks given task task-speciﬁc model parameterized trained using dataset train corresponding loss using feedback collection losses loss task evaluated using test data test. given task tnew meta-learner helps model fθtnew quickly adapt task tnew warm-starting gradient updates. brieﬂy review model-agnostic meta-learning emphasizing commonalities differences maml method. maml meta-learning method applies model learns using gradient descent. method loosely inspired ﬁne-tuning learns initial parameters network network’s loss gradient steps minimized. consider model parameterized maml alternates updates determine initial parameters task-speciﬁc learners warm-start gradient descent updates tasks solved using small number examples. task-speciﬁc learner updates parameters gradient update using loss evaluated data train}. meta-optimization across tasks performed parameters updated using loss evaluated test} given recent work shown gradient-based optimization universal learning algorithm sense learning algorithm approximated arbitrary accuracy using parameterized model gradient descent. thus generality lost considering gradient-based learners method alters subset weights. furthermore whereas maml learns standard gradient descent subset method’s parameters effectively ’warp’ parameter space parameters learned meta-testing enable faster learning. present models section transformation networks mask transformation networks trained gradient-based meta-learning. t-net learns metric activation space; metric informs task-speciﬁc learner’s update direction step size. mt-net additionally learns subset weights update task-speciﬁc learning. therefore mt-net learns automatically assign roles weights. input nonlinear activation function. t-nets name transformation matrices linear transformation deﬁned plays crucial role meta-learning. note ∗for convolutional cells convolutional layer size stride convolution doesn’t change parameters shared across task-speciﬁc models determined meta-learner. taskspeciﬁc learners initial update different values since uses corresponding similar denotes tasks whereas denotes transformation matrix. similarly deﬁned magnitude determined interaction since task-speciﬁc learner performs gradient descent change resulting guided meta-learned value provide analysis behavior section hadamard product matrices dimension. binary gradient mask sampled time task-speciﬁc learner encounters task. either all-ones vector all-zeros vector parameterize probability scalar variable emphasize binary mask used task-speciﬁc learning depends meta-learned parameter weights since meta-learner optimizes loss task gradient step matrix gets assigned high probability value weights encode task-speciﬁc information. furthermore since update along model parameters meta-learner incentivized learn conﬁgurations exists clear divide task-speciﬁc task-mutual neurons. throughout section focus space instead layer parameterized thinking gradients respect loss function equivalent. note inﬂuence loss function bottlenecked chain rule shows ∇alt assuming ﬁxed space possible ∇alt loss functions isomorphic ∇ylt turn isomorphic take advantage fact learning full-rank metric space space would require many parameters even small architecture. input cell. omit superscripts throughout section. standard feedforward network resorts gradient loss function respect parameter matrix update model parameters. case single gradient step yields determined meta-learner. thus t-net incremental change proportional negative ∇alt task-speciﬁc learning t-net guided full rank metric cell’s activation space determined cell’s transformation matrix metric warps activation space model warped space single gradient step respect loss task yields parameters well suited task. proposition states sufﬁcient expressive power restrict updates subspace. note construction possible transformation binary masks would able restrict gradients axis-aligned subspaces. matrix columns let’s denote tm∇alt note update task-speciﬁc learner mt-net performs update matrix nonzero elements rows columns setting appropriate view proposition loss function cell mt-net corresponding mask parameters. d-dimensional subspace metric tensor exist conﬁgurations vector ynew steepest direction descent respect metric proof. appendix therefore mt-nets project gradients task-speciﬁc learners onto subspace preactivation space also learn metric subspace thereby learning low-dimensional linear embedding activation space. mt-net update gradient descent low-dimensional embedding meta-objective shown minimized gradient descent embedding requires steps converge sensitive task identity. successful line research few-shot learning uses feedforward neural networks learners. approaches learn update rules directly generate weights related research direction learn initial parameters ﬁxing learning rule gradient descent additionally learning learning rates weight interprets gradient-based meta-learning hierarchical bayesian inference states methods expressive enough approximate learning algorithm. work closely related line research. unlike previous work line research mt-nets learn many degrees freedom task-speciﬁc learner meta-test time. additionally mt-nets learn update rules update rules directly embedded network instead stored separate model. distance metric learning methods learn distance function datapoints. like methods mt-nets learn full metric matrix. whereas methods required constrained optimization techniques enforce learned matrix represents metric parameterization allows directly learn metric using gradient descent. recently neural networks used learn metric images achieving state-of-the-art performance few-shot classiﬁcation benchmarks. work similar recent methods learn metric feature space instead input space. method applies larger class problems including regression reinforcement learning since mt-nets require differentiable loss function. another line research few-shot learning recurrent neural network learner here meta-learning algorithm gradient descent learning algorithm update hidden cells. weights specify learning strategy processes training data uses resulting hidden state vector make decisions test data. recent work uses temporal convolutions meta-learning also closely related line research. novel components affect meta-learning performance? degree alleviate need careful tuning step size mt-nets learned subspace dimension reﬂect difﬁculty tasks? t-nets mt-nets scale large-scale meta-learning problems? experiments performed modifying code accompanying follow table loss sine wave regression. networks meta-trained using -shot regression tasks. reported losses calculated adaptation using various numbers examples. task sampled uniformly respectively. task consists training examples testing examples. sample uniformly train test sets. regressor architecture hidden cells activation size every relu nonlinearity. loss function mean squared error regressor’s prediction true value used adam meta-optimizer learning rate task-specifc learners used step size initialize identity matrices truncated normal matrices standard deviation trained meta-learner examples tested using various numbers examples show results table novel components increased meta-learning performance also performed experiments variations mt-nets. m-net uses mask like mt-net cell consists single matrix instead model -full name learns separate mask weight instead sharing mask among weights contribute activation. example size corresponding mt-net would dimension mt-net-full dimension would note mt-nets outperform maml meta-sgd variations mt-nets. discuss detail appendix performed experiments robust method variations perform sinusoid experiment section various step sizes evaluate training examples settings identical experiments section show losses adaptation maml mt-nets table mt-nets robust change step size. indicates shown section matrix capable warping parameter space recover suboptimal step size table loss -shot sine wave regression. t-nets mt-nets bost robust change step size meta-learned matrix inside cell alters effective step size. consider -shot regression tasks target function polynomial. constructed sine wave regression meta-task section varying amplitude phase frequency construct polynomial regression meta-task varying coefﬁcients polynomials order. generate cixi) uniformly sampled used network architecture hyperparameters section performed -shot regression polynomial orders since number free parameters proportional order polynomial expect higher-order polynomials require parameters adapt fraction parameters task-speciﬁc learners change calculated expected value show results figure additional results appendix number weights meta-learner mt-net sets altered increases task gets complex. interpret meta-learner mt-nets effect akin occam’s razor selects task-speciﬁc model enough complexity learn tasks. behavior emerges even though introduce additional loss terms encourage behavior. think caused noise inherent stochastic gradient descent. since meta-learner mt-net choose whether perform gradient descent particular direction incentivized directions model-speciﬁc would introduce noise network parameters thus increase loss. models matching networks prototypical networks map-ssvm fine-tune baseline nearest neighbor baseline meta-learner lstm maml l-maml meta-sgd t-net mt-net compare performance mt-nets prior work meta-learning evaluate method few-shot classiﬁcation omniglot miniimagenet datasets. used miniimagenet splits proposed experiments. model uses architecture model modules convolutions ﬁlters followed batch normalization used ﬁlters layer miniimagenet. convolutions stride omniglot max-pooling used batch normalization instead strided convolutions miniimagenet. evaluate gradient steps omniglot -way omniglot -way miniimagenet -way respectively. results shown table mt-nets achieve state-of-the-art comparable performance problems. several works reported improved performance miniimagenet using signiﬁcantly expressive architecture. report methods equal comparable expressiveness model ﬁrst described controlling network expressivity highest reported accuracy -way -shot miniimagenet classiﬁcation introduced t-nets mt-nets. transform feedforward neural network mt-net future architectural advances take advantage method. experiments showed method alleviates need careful tuning learning rate few-shot learning problems mask reﬂects complexity tasks learning adapt mt-nets also showed state-of-the-art performance challenging few-shot classiﬁcation benchmark interesting line future research theoretically analyze optimization generalization properties mt-net cells explored work. biggest weaknesses deep networks data intensive. learning learn task encountered train networks high capacity using small amount data. believe designing effective gradient-based meta-learners beneﬁcial few-shot learning setting also machine learning problems general.", "year": 2018}