{"title": "Adaptive Convolutional Filter Generation for Natural Language  Understanding", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Convolutional neural networks (CNNs) have recently emerged as a popular building block for natural language processing (NLP). Despite their success, most existing CNN models employed in NLP are not expressive enough, in the sense that all input sentences share the same learned (and static) set of filters. Motivated by this problem, we propose an adaptive convolutional filter generation framework for natural language understanding, by leveraging a meta network to generate input-aware filters. We further generalize our framework to model question-answer sentence pairs and propose an adaptive question answering (AdaQA) model; a novel two-way feature abstraction mechanism is introduced to encapsulate co-dependent sentence representations. We investigate the effectiveness of our framework on document categorization and answer sentence-selection tasks, achieving state-of-the-art performance on several benchmark datasets.", "text": "used within model exploiting meaningful semantic features n-gram fragments. however learned weights ﬁlters cases assumed ﬁxed regardless input text. although setting able capture common patterns inherent natural language sentences vital features sentencespeciﬁc neglected especially cases conditional information available paired sentence question answering problems. observation consistent following intuition reading different types documents e.g. academic papers newspaper articles people tend adopt distinct strategies better effective understanding leveraging fact words phrases different meaning imply different things depending context. example suppose asked select correct answer question candidates; natural read possible answer within context meaning known question. several research efforts sought improve adaptability cnns feature extractor text. common strategy attention mechanism employed guide extraction semantic features. embedding single sentence proposed self-attentive model attends different parts sentence combines multiple vector representations. however model needs considerably parameters achieve performance gains traditional cnns. johnson zhang utilized region embeddings input improve text classiﬁcation accuracy considered augmenting phrase-speciﬁc features. however region embeddings still ﬁxed samples model relies additional information unlabeled data. match sentence pairs introduced attention-based model reweights convolution inputs outputs extract interdependent sentence representations. wang ittycheriah explores compare aggregate framework directly capture similarity information paired sentences. however approaches suffer problem high matching complexity since similarity matrix pairwise words needs computed thus computationally inefﬁcient even prohibitive applied long sentences convolutional neural networks recently emerged popular building block natural language processing despite success existing models employed expressive enough sense input sentences share learned ﬁlters. motivated problem propose adaptive convolutional ﬁlter generation framework natural language understanding leveraging meta network generate inputaware ﬁlters. generalize framework model question-answer sentence pairs propose adaptive question answering model; novel two-way feature abstraction mechanism introduced encapsulate co-dependent sentence representations. investigate effectiveness framework document categorization answer sentence-selection tasks achieving state-of-the-art performance several benchmark datasets. last years convolutional neural networks demonstrated remarkable progress various natural language processing applications including sentence/document classiﬁcation text sequence matching language modeling machine translation abstractive sentence summarization etc. popularity cnns mainly stems from ability extract salient abstract features n-gram text fragments; ﬂexibility controlling effective context size stacking layers hierarchical manner; convolutions different parts sequence highly parallelizable thus computationally efﬁcient recurrent neural networks. cnns typically applied tasks feature extraction corresponding supervised task approached jointly encoder network text cnns typically convolve ﬁlters window size input-sentence embedding matrix glove different ﬁlter sizes propose generic adaptive convolutional ﬁlter generation mechanism natural language understanding. contrast traditional cnns convolution operation framework ﬁxed ﬁlters thus provides network stronger modeling ﬂexibility capacity. speciﬁcally introduce meta network generate input-aware ﬁlters conditioned speciﬁc input sentences; ﬁlters adaptively applied different text sequences. manner learned ﬁlters vary sentence sentence allow ﬁne-grained feature abstraction. proposed meta networks learned end-to-end together network modules training process. moreover since generated ﬁlters framework adapt different conditional information available naturally generalized tackle natural language reasoning problems. investigate effectiveness adaptive framework typical text understanding tasks document classiﬁcation question answer-selection. show architecture generates highly effective ﬁlters individual input sentence also serve bridge allow interactions sentence pairs leveraging novel two-way co-dependent feature extraction mechanism. contributions paper follows generic adaptive convolutional neural network framework proposed text representation learning introducing meta network generate ﬁlter parameters; acnn framework generalized model sentence pairs two-way co-dependent feature abstraction mechanism proposed allow interactions constructing sentence representations; architectures collobert typically utilized extracting sentence representations composition convolutional layer max-pooling operation resulting feature maps. words sentence length sentence represented matrix rd×t column represents d-dimensional embedding corresponding word. convolutional layer ﬁlters weights rk×h×d convolved every window words i.e. xt−h+t} within sentence number output feature maps manner feature maps h-gram text fragoutput feature maps convolutional layer i.e. passed pooling layer takes maximum value every forming k-dimensional vector operation attempts keep salient feature detected every ﬁlter discard information less fundamental text fragments. moreover max-over-time nature pooling operation guarantees size obtained representation independent sentence length. note basic sentence encoders ﬁlter weights different inputs suboptimal feature extraction especially case conditional information available. adaptive convolution framework adaptive convolutional neural network architecture composed principal modules ﬁltergeneration module produces ﬁlters conditioned input sentence; adaptive-convolution module applies generated ﬁlters input sentence modules jointly differentiable overall architecture trained end-to-end manner. since generated ﬁlters sample-speciﬁc rather ﬁxed different inputs acnn feature extractor text tends stronger predictive power basic encoder. general acnn framework shown schematically figure filter generation module instead utilizing ﬁxed ﬁlter weights different inputs model generates ﬁlters conditioned input sentence given input ﬁlter-generation module implemented principle deep architecture. however order handle input sentences first input encapsulated ﬁxed-length vector basic model convolutional layer employed along pooling operation described section hidden representation deconvolutional layer performs transposed operations convolutions applied produce unique ﬁlters since dimension hidden representation independent input-sentence length framework guarantees generated ﬁlters shape size every sentence. intuitively encoding part ﬁlter generation module abstracts information sentence based representation deconvolutional upsampling layer determines ﬁxed-size ﬁne-grained ﬁlters speciﬁc input i.e. ﬁlter-generation module maps described equation adaptive convolution module adaptive convolution module takes inputs generated ﬁlters input sentence sentence input ﬁltergeneration module identical different sample-speciﬁc ﬁlters input sentence adaptively encoded again basic architecture section i.e. convolutional pooling layer. acnn framework seen generalization basic represented acnn setting outputs ﬁlter-generation module constant regardless input sentence. learning-to-learn nature acnn framework approach tends greater representational power basic cnn. application text categorization direct application acnn framework text categorization aims predict label given sentence/document label topic sentiment orientation etc. illustrated figure ﬁrst feed input sentence ﬁlter-generation module producing adaptive ﬁlters allow strong modeling capacity generating networks deﬁned two-step process. have learned parameters layer ﬁlter-generating module. encoder implemented basic unit up-sampling layer one-layer deconvolutional network widely leveraged up-sample ﬁxed-length representations images single-layer up-sampling network also regarded fully-connected layer since operation equivalent matrix multiplication operation transforms vector -dimensional tensor ﬁlter weights. type up-sampling networks previously employed radford metz chintala zhang framework shape number ﬁlters ﬂexibly speciﬁed parameter deconvolutional layer. also tried implement ﬁlter generation module layer inferior performance compared basic cnn. ﬁnding similar case image recognition producing ﬁlters passed adaptive convolution module input output corresponding sentence representation denotes parameters learned adaptive convolution module. note contains bias term since ﬁlter weights already generated. subsequently vector predictor probabilities class denoted number classes adaptive question answering model considering ability acnn framework generate ﬁlters aware speciﬁc input naturally generalized question-answering applications. work consider acnn framework answer sentence selection crucial subtask open-domain question answering goal model identify correct answers candidates response factual question. given question associated list candidate answers corresponding labels ym}. correctly answers otherwise therefore task cast classiﬁcation problem where given unlabeled question-answer pair seek predict judgement conventionally question answer independently encoded basic cnns ﬁxed-length vector representations denoted respectively. directly employed predict judgement strategy could suboptimal since communication occurs questionanswer pair prediction layer. intuitively model inferring representation question meaning answer taken account features relevant ﬁnal prediction likely extracted. motivated propose adaptive cnnbased question-answer model problem. adaqa question answer representations abstracted conditioned information sequence. communication process achieved denote element-wise subtraction element-wise product respectively. indicates stacked column vectors. resulting matching vector sent layer learned) model desired conditional distribution notably share weights ﬁlter generating networks question answer model adaptivity answer selection improved without excessive increase number parameters. three modules adaqa model jointly trained end-to-end. document classiﬁcation datasets ﬁrst investigate effectiveness acnn framework document-categorization task. consider large-scale document classiﬁcation datasets yelp reviews polarity dbpedia ontology dataset yelp reviews seek predict binary label regarding review restaurant. dbpedia extracted wikipedia crowd-sourcing categorized non-overlapping ontology classes including company athlete natural place etc. sample training data validation select hyperparameters models perform early stopping. summary datasets presented table model setup randomly initialize word embeddings uniformly within update training. generated ﬁlters window size feature maps ﬁlter shape/size settings basic implementation direct comparison. utilize one-layer architecture baseline acnn model since observe signiﬁcant performance gains multilayer architecture. adam used update model parameters learning rate minibatch size dropout rate utilized embedding layer. observed larger dropout rate hurt performance make training signiﬁcantly slower. baselines comparison consider several baseline models ngrams bag-of-means method based tfidf representations built choosing frequent n-grams training correspondﬁlter-generation mechanism question generates ﬁlters convolved answer vice versa. consequently resulting question/answer embeddings counterpart-aware thus potentially informative judgement prediction. adaqa model divided three modules ﬁlter generation adaptive convolution matching modules depicted schematically figure assume question-answer pair matched represented wordembedding matrices i.e. rtq×d rta×d embedding dimension respective sentence lengths. first passed ﬁltergeneration modules produce sets ﬁlters encapsulate features corresponding input sentences. similar setup section also employ two-step process produce ﬁlters. question generating process dcnn denote basic unit deconvolution layer respectively discussed section parameters learned. process utilized produce encodings ﬁlters answer input parameters question embedding convolved ﬁlters produced answer vise versa idea abstract information answer pertinent corresponding question compared siamese architecture model selectively encapsulates important features judgement prediction removing less vital information. employ question answer representations inputs matching module following table test error rates document classiﬁcation tasks s-model indicates model single convolutional ﬁlter m-model indicates model multiple convolutional ﬁlters. results marked reported zhang zhao lecun reported conneau reported counts features; small/large word layer word-based convolutional networks features layer denoted small/large respectively; deep deep convolutional neural networks layers; sa-lstm lstm sentence classiﬁer pretrained parameters sequence autoencoder. results directly investigate whether acnn model leverage input-aware ﬁlter weights better sentence representation experiment basic acnn models single ﬁlter denoted scnn s-acnn respectively illustrated table s-acnn signiﬁcantly outperforms s-cnn datasets demonstrates advantage ﬁlter-generation module acnn framework generate ﬁne-grained ﬁlters different input sentences robust feature extraction. result convolutional ﬁlter thus limited modeling capacity s-acnn model tends much expressive basic model ﬂexibility applying different ﬁlters different sentences. gain better insight modeling ability acnn framework experiment acnn models multiple ﬁlters. corresponding document categorization accuracies presented table although convolution layer acnn model already outperforms baseline methods much deeper architectures. moreover method demonstrates higher accuracy n-grams strong baseline shown zhang zhao lecun attribute superior performance acnn framework stronger feature-extraction ability. acnn model even outperforms sa-lstm baseline sequence autoencoder pretrained better initialization classiﬁer. contrast model trained end-to-end without pre-training thus efﬁcient easier implement. notice significant performance gains stacking several acnn layers hierarchical structure; hypothesize caused fact acnn layer already expressive enough encapsulate discriminative features document assigning corresponding label. datasets explore whether ﬁlter-generation mechanism generalized model sentence pairs evaluate acnn framework datasets opendomain question answering wikiqa selqa given question task rank corresponding candidate answers sentences extracted summary section related wikipedia article. sense mean average precision mean reciprocal rank employed evaluation metrics. direct comparison existing results truncate answers maximum length tokens experiments wikiqa dataset. model setup elucidate role different parts adaqa model implement several model variants comparison vanilla model independently encodes sentence representations matching; self-adaptive acnn-based model question/answer sentence generates adaptive ﬁlters convolve input itself; one-way acnn model answer sentence representation extracted adaptive ﬁlters generated conditioned question; two-way adaqa model described section sentences adaptively encoded ﬁlters generated conditioned sequence. model variants initialize word embeddings -dimensional glove word vectors pretrained wikipedia gigaword ﬁlters window size feature maps. described section vector output matching module prediction layer implemented one-layer followed sigmoid function. adam train models batch size dropout rate employed word embedding layer. select hyperparameters choosing best model validation set. baselines also compare acnn models twelve strong baseline methods categorized follows word word lclr methods leverage lexical semantic features prediction; paragraph vector unsupervised algorithm encodes sentences ﬁxedlength representations; vanilla model indicates lexical overlap features included corresponding models. choi santos wadhawan zhou attentive pooling networks abcnn cnn-based models attention mechanism employed sentence representations; nasm attention-based lstm lstm-based models former uses latent variable stochastic attention latter leverages tree-structure sentences; l.d.c. model compares composes word vectors sentences using matching network; key-value memory networks neural semantic encoders methods based memory networks. results tables show experimental results models wikiqa selqa datasets along state-of-the-art methods. illustrated two-way adaqa model performs best compared strong baseline models suggesting proposed model effective learning counterpart-aware sentence representations natural language reasoning. moreover trend observed datasets compared selfadaptive acnn model generates ﬁlters input slightly outperforms vanilla siamese model. combined results document categorization experiments believe acnn framework simplest form utilized powerful feature extractor transforming natural language sentences ﬁxed-length vectors. wikiqa dataset two-way adaqa model shows better results one-way baseline well vanilla siamese architecture. superiority two-way adaqa model also observed experiments selqa dataset shown table much larger dataset wikiqa. indicates co-dependent feature abstraction mechanism associated performance gains. intuitively humans trying determine whether candidate answer matches certain question reading question answer would natural strategy keep meaning sentence mind. mechanism made possible ﬁlter-generation process acnn framework. notably model yields signiﬁcantly better results attentive pooling network abcnn baselines state-of-the-art encoder-based models wikiqa dataset. attribute improvement potential advantages adaqa model previous baseline methods interaction question answer takes place either before convolution. however adaqa model communication sentences inherent convolution operation thus provide abstracted features ﬂexibility; two-way feature extraction mechanism adaqa model generates codependent representations question candidate answer enabling model recover initial local maxima corresponding incorrect predictions discussion shown experiments adaptive convolutional ﬁlter-generation mechanism propose brings consistent improvements classifying document well modeling sentence pairs. basic intuition behind results that learning distinct ﬁlters different inputs advantage extracting ﬁne-grained semantic features sample-speciﬁc taskspeciﬁc ﬁnal prediction. demonstrate performance gains document categorization experiments originates improved adaptivity acnn framework implement basic model different numbers ﬁlter sizes ranguse meta network generate parameters annetwork directly inspired recent success hypernetworks text-generation tasks dynamic parameter-prediction video-frame generation contrast works focus generation problems model based adaptive ﬁlters aimed abstracting informative predictive sentence features. similar work huang designed meta network generate compositional functions treestructured neural networks encapsulating sentence features. however model suitable encoding individual sentences framework readily generalized capture interactions sentence pairs. moreover framework based models advantageous fewer parameters highly parallelizable computations relative sequential-based models. much previous work exploring interaction between sentence-pair representations question answering. miao blunsom employed stochastic vector question representation attending different parts answer preprocessed lstm encoding layer. presented attention-based models reweight question answer representations either before convolution pooling layer computing attention feature matrix. wang ittycheriah proposed two-channel model composes similar dissimilar components embedding matrices question-answering pair. common step methods compare representation sentence every word give rise high matching complexity either mentioned computation complexity burdens scalability methods long sentences large datasets. contrast adaqa model learns counterpart-aware question-answer representations incorporating interaction process convolution operation. moreover since matching complexity model independent adaqa model computationally efﬁcient generalized scenarios question answer longer sentences. proposed convolutional ﬁlter-generation mechanism introducing meta network adaptively produce input-aware ﬁlters. manner ﬁlter weights vary sample sample thus provide encoder network modeling ﬂexibility capacity. generalize framework model questionanswer sentence pairs leveraging two-way feature abstraction process. evaluate models several documentcategorization answer sentence-selection benchmarks achieving state-of-the-art results. figure yelp reviews classiﬁcation accuracy basic different number ﬁlters compared acnn model score comparison different question types vanilla adaqa model. scores type questions different models nasm results illustrated figure test accuracy model show noticeable improvement ﬁlters ﬁlter size larger importantly even ﬁlter size classiﬁcation accuracy worse acnn model restricted ﬁlters. given observations believe boosted categorization accuracy come improved ﬂexibility thus better feature extraction acnn framework associate improved answer sentence selection results reasoning capabilities adaqa model categorize questions wikiqa test types containing ‘what’ ‘where’ ‘how’ ‘when’ ‘who’. calculate scores basic adaqa model different question types. similar ﬁndings miao blunsom observe ‘how’ question hardest answer lowest scores. however adaqa model improves basic ‘how’ type question figure comparing results nasm miao blunsom adaqa model outperforms reported ‘how’ question scores large margin seen figure indicating adaptive convolutional ﬁlter-generation mechanism improves model’s ability read reason natural language sentences. work line previous efforts improving adaptivity ﬂexibility convolutional neural networks jeon proposed enhance transformation modeling capacity cnns adaptively learning ﬁlter shapes backpropagation. brabandere introduced architecture generate future frames conditioned given image adapting ﬁlter weights motion within previous video frames. although cnns widely adopted large number applications improving adaptivity vanilla modules considerably less studied. best knowl-", "year": 2017}