{"title": "Exact Tensor Completion from Sparsely Corrupted Observations via Convex  Optimization", "tag": ["cs.LG", "cs.CV", "cs.IT", "math.IT", "stat.ML"], "abstract": "This paper conducts a rigorous analysis for provable estimation of multidimensional arrays, in particular third-order tensors, from a random subset of its corrupted entries. Our study rests heavily on a recently proposed tensor algebraic framework in which we can obtain tensor singular value decomposition (t-SVD) that is similar to the SVD for matrices, and define a new notion of tensor rank referred to as the tubal rank. We prove that by simply solving a convex program, which minimizes a weighted combination of tubal nuclear norm, a convex surrogate for the tubal rank, and the $\\ell_1$-norm, one can recover an incoherent tensor exactly with overwhelming probability, provided that its tubal rank is not too large and that the corruptions are reasonably sparse. Interestingly, our result includes the recovery guarantees for the problems of tensor completion (TC) and tensor principal component analysis (TRPCA) under the same algebraic setup as special cases. An alternating direction method of multipliers (ADMM) algorithm is presented to solve this optimization problem. Numerical experiments verify our theory and real-world applications demonstrate the effectiveness of our algorithm.", "text": "paper conducts rigorous analysis provable estimation multidimensional arrays particular third-order tensors random subset corrupted entries. study rests heavily recently proposed tensor algebraic framework obtain tensor singular value decomposition similar matrices deﬁne notion tensor rank referred tubal rank. prove simply solving convex program minimizes weighted combination tubal nuclear norm convex surrogate tubal rank -norm recover incoherent tensor exactly overwhelming probability provided tubal rank large corruptions reasonably sparse. interestingly result includes recovery guarantees problems tensor completion tensor principal component analysis algebraic setup special cases. alternating direction method multipliers algorithm presented solve optimization problem. numerical experiments verify theory real-world applications demonstrate eﬀectiveness algorithm. last decade witnessed explosion academic interest robust recovery low-rank matrices severely compressive incomplete even corrupted measurements. interest mainly aroused striking fact data science engineering society images videos texts microarrays near low-dimensional subspaces discovery says stack data points column vectors matrix matrix low-rank approximately surprisingly shown mild assumptions eﬃcient techniques based convex programming minimize nuclear norm approximation matrix rank accurately recover low-rank matrices long left right singular vectors incoherent matrix standard basis modern information technology keeps developing rapidly multidimensional data becoming prevalent many application domains ranging image processing computer vision neuroscience bioinformatics conventional methods rearrange multidimensional data matrices speciﬁc unfolding ﬂattening strategies cause problem curse dimensionality also damage inherent structure like spatial correlation within original data. tensor-based modeling take full advantage multilinear structures provide better understanding higher precision natural choice situations. mimicking low-dimensional predecessors tensor-based completion robust principal component analysis formulations applied real applications promising empirical performance. recovery theory low-rank tensor estimation problems however well-established. mainly attributed tensor rank diﬀerent deﬁnitions literature drawback. candecomp/parafac decomposition approximates tensor rank-one outer products minimal number decomposition deﬁned rank. however computing rank speciﬁc tensor np-hard general kinds decompositions tucker tensor train reveal algebraic structure data notion rank extended multi-rank expressed vector ranks factors. clearly decompositions oﬀer best rank-k approximation tensor. unlike existing models t-product associated algebraic constructs introduced tensors order three higher provide framework obtain svd-like factorization named tensor-svd derive notion tensor rank referred tubal rank compared tensor decompositions t-svd shown superior capturing spatial-shifting correlation ubiquitous realworld data using algebraic framework recent papers gives suﬃcient conditions convex programming succeed exact recovery low-rank tensors incomplete grossly corrupted observations respectively. paper considers challenging problem learning low-rank tensor undersampled possibly arbitrarily corrupted measurements. problem arises wide range important applications data contain missing values gross errors simultaneously various factors information loss sensor failures software malfunctions. reader might jump section practical examples. actually problem tensor-based generalization robust matrix completion therefore call robust tensor completion hereafter. leveraging t-svd algebraic framework show obtain exact recovery target tensor high probability simply solving convex program whose object weighted combination tubal nuclear norm serving convex surrogate tubal rank -norm. conditions result holds similar regular matrix incoherence conditions coincide well much weaker couterparts given respectively. aware problem rigorously examined proposes strongly convex program proved guarantee exact recovery certain conditions well. despite considering problem study departs several fronts. first t-svd algebraic framework third-order tensors treated linear operators matrices oriented laterally quite diﬀerent classic multilinear algebraic setup tucker decomposition used work. besides tubal rank tubal nuclear norm deﬁned fourier domain diﬀer seriously multi-rank convex relaxation sum-of-nuclear-norms hence recovery theory established directly comparable result. analysis additional advantage signiﬁcant practical importance. identiﬁes simple non-adaptive choice regularization parameter model. contrast heuristic rule parameter setting suggested usually suﬀers failure real-world applications shown experiments. rest paper organized follows. section begin brief review related work. notation preliminaries tensors introduced section outline t-svd algebraic framework third-order tensors. section describes main results discusses similarities diﬀerences theory prior works. provide full proof theorem section introduce admm algorithm solve optimization problem section finally report numerical empirical results section draw conclusions section section related work low-rank tensor recovery based diﬀerent tensor factorizations associated algebraic frameworks coarsely spit branches tensor completion tensor robust principal component analysis conditions tensor factors. highly scalable algorithms proposed tasks ﬁlling missing entries multidimensional data integration decomposition block coordinate descent methods. optimization problem non-convex hence local minimum arrived know often computationally intractable determine rank best convex approximation tensor makes diﬃcult recover tensors rank particularly convex programming. inspired relation matrix rank nuclear norm propose convex surrogate multi-rank referred snn. soon after tractable measure tensor rank successfully applied various practical problems reference therein). besides empirical studies progress recovery theory achieved time. tomioka conduct statistical analysis tensor decomposition provide ﬁrst theoretical guarantee minimization. result signiﬁcantly enhanced later study proves complexity bound obtained tight employing convex surrogate also proposes simple improvement works much better high-order tensors. unfortunately researches assume gaussian measurements practice problem settings often similar matrix completion problems zhang aeron derive theoretical performance bounds algorithm proposed third-order tensor recovery limited sampling using t-svd algebraic framework. prove solving convex optimization problem minimizes tubal nuclear norm convex approximation tubal goal trpca problem learn target tensor superposition low-rank component sparse corruption component observations. problem ﬁrst proposed extensively investigated theoretically algorithmically shah consider robust decomposition based randomized convex relaxation formulation. random sparsity model proposed algorithm provides guaranteed convex relaxation multi-rank provide perfect recovery components restricted eigenvalue conditions. conditions opaque clear regarding level sparsity handled. rank sparsity tensor decomposition algorithm applies variable-splitting components utilizes classic algorithm solve unconstrained problem obtained relaxing constraints quadratic penalty terms. method many parameters tune iteration complexity guarantee. multi-linear augmented lagrange multiplier method divides original trpca problem independent robust principal component analysis problems reformulation makes ﬁnal solution hard optimal since consistency among auxiliary variables considered. convex non-convex approaches derived admm algorithm introduced guarantees recovery performance. propose convex optimization indeed simple elegant tensor extension rpca. show certain incoherence conditions solution convex optimization perfectly recovers low-rank sparse components provided tubal rank target tensor large corruption term reasonably sparse. throughout paper tensors denoted boldface euler letters matrices boldface capital letters. vectors represented boldface lowercase letters scalars lowercase letters. ﬁeld real number complex number denoted respectively. third-order tensor rnˆnˆn denote kq-th entry aijk matlab notation denote i-th horizontal lateral frontal slice respectively. speciﬁcally front slice denoted compactly apiq. denotes moreover tensor tube size denoted tensor column size denoted inner product cnˆn given trpah denotes conjugate transpose trp¨q denotes matrix trace. inner product cnˆnˆn deﬁned norms vector matrix tensor used. vector -norm |vi|. spectral norm matrix cnˆn denoted maxi σipaq introducing tensor notation terminology give basic deﬁnitions t-svd outline associated algebraic framework serve foundation analysis next section. tube lies third dimension. hence t-product tensors analogous matrixmatrix multiplication expect multiplication operation scalars replaced circular convolution tubes. perspective endowed multidimensional data arrays advantageous representation real-world applications rnˆnˆn vector i-th entry rank i-th frontal slice i.e. piqq. tensor tubal rank denoted ranktpaq deﬁned number nonzero rankp singular tubes comes t-svd deﬁnition column basis denoted tensor size entry equaling rest equaling nonzero entry appear ﬁrst front slice naturally conjugate transpose tube basis denoted tensor size kqth entry equaling consider problem formally. suppose given third-order tensor tubal rank corrupted sparse term here arbitrary magnitude. know tubal rank furthermore idea locations nonzero entries even many are. recover accurately eﬃciently observed subset noisy data given remaining entries missing. optimization problem generally np-hard discrete nature tubal-rank function pseudo-norm counts number nonzero entries replacing terms convex surrogates namely tubal nuclear norm -norm respectively leads following convex optimization problem low-rank sparse e.g. able identify low-rank tensor cases. make problem meaningful need incoherence conditions ensure sparse. deﬁnition assume ranktplq skinny t-svd said satisfy tensor incoherence conditions parameter supposed suﬃciently spread i.e. uncorrelated tensor basis means entry carries approximately amount information. matrix incoherence conditions according name respectively. factor zhang aeron indicate joint incoherence condition unnecessary tensor completion conclusion alternative however unavoidable obtaining exact solution trpca problems shown analysis. present main results. convenience denote maxpn minpn theorem suppose rnˆnˆn obeys observation uniformly distributed among sets cardinality ρnnn. also suppose observed entry independently corrupted probability then exist universal constants probability least cpnpqnq´c recovery {?ρnpqn exact probability nearly subset entries even arbitrarily corrupted. require tubal rank tensor large exact order npq{pµplogpnpqnqqq corruption term suﬃciently sparse. would like emphasize unique minimizer probability east cpnpqnq´c. corollary suppose rnˆnˆn obeys support uniformly distributed probability then exist universal constants probability least cpnpqnq´c unique minimizer {?npqn provided mention earlier related works similar study. also deﬁne notion tensor incoherence using t-svd algebraic framework propose suﬃcient conditions convex programming achieve guaranteed recovery. however simply focus problem special cases model recovery sampling complexity recovery guarantees derived diﬀerent sampling models consistent slight change constant factor expect tensor case well. think results good agreement other seeing bernoulli sampling exploited random sampling without replacement adopted paper. although corollary almost identical theorem proposed result tensor incoherence conditions much weaker. proof follows closely proofs presented main tools noncommutative bernstein inequality golﬁng scheme also helpful derivation results. subtle diﬀerences proof fairly straightforward adaption. problem corrupted observations randomly sampled original domain tubal nuclear norm deﬁned fourier domain. therefore proofs lemma theorem need additionally consider properties fourier transformation block circulant matrix. sampling strategy used theorem uniform sampling without replacement. widely used sampling models e.g. bernoulli sampling adaptive sampling random sampling replacement. facilitate proof consider i.i.d. bernoulli-rademacher model. precisely assume kq|δijk δijk’s i.i.d. bernoulli variables taking value probability zero probability bernoulli sampling denoted berpρq short. proxy uniform sampling probability failure bernoulli sampling signs nonzero entries deterministic. shown much easier work stronger assumption signs nonzero entries independent symmetric random variables introduce convenient think pγpeq ﬁxed tensor consider random sign tensor i.i.d. entries index ppmijk ppmijk components symmetric random signs. introducing noise tensor pγp|e| using standard derandomization theory assert work alternating direction method multiplier method solve convex problem admm decomposes large global problem series smaller subproblems coordinates solutions subproblems compute globally optimal solution. received renewed interest recent years fact eﬃcient tackle largescale problems solve optimization problems multiple non-smooth terms objective function. also refer exploited applications admm similar problems. updates pωpek`q closed-form solutions easy computing svds matrices. hence complexity optpnnn logpnq npqnpqnqq number iterations. resort conjugate symmetry table exact recovery random data diﬀerent sizes. scenarios synthetic tensors diﬀerent tubal ranks percentage missing entries proportion grossly corrupted observations. plicity consider tensors size varying dimension generate clean tensor tubal rank ranktplq entries rnˆrˆn rrˆnˆn independently sampled standard gaussian distribution addition fraction entries uniformly corrupted additive i.i.d. noise standard gaussian distribution random. finally randomly choose percentage test cases summarize results table ﬁrst scenario choose challenging setting second scenario. clear method gives correct rank estimation negligible relative error l}f{}l}f cases. results verify exact also consider situations entries tensors sampled diﬀerent distributions uniform distribution bernoulli distribution. similar results obtained report page limit. figure exact recovery varying rank gross corruptions diﬀerent proportions observed entries. fraction perfect recoveries across trials function tubal rank data generated above-mentioned experiments data size success. pair simulate test instances declare trial successful recovered tensor satisﬁes l}f{}l}f figure reports fraction perfect recovery pair clearly exists region recovery correct cases. moreover larger percentage missing values smaller region correct recovery becomes. considering connections among problems compare algorithm similar approaches namely tubal nuclear norm minimization trpca three diﬀerent settings. again generate random data prior experiments. ﬁrst case vanish vary compare recovery behaviors three methods tensor completion. shown ﬁrst figure performs much better methods. test three methods varying entire entries observed. since problem reduces problem settings algorithm trpca obtain identical results. contrast tnnm fails recover synthetic tensors cases owing fragile gross corruptions. figure comparison method similar approaches three diﬀerent problems low-rank tensor recovery. tensor completion. middle tensor robust principal component analysis. bottom robust tensor completion. color image. actually channel color image low-rank singular values dominate main information hence image approximately reconstructed low-tubal-rank tensor. experiment focus noisy image completion. problem unlike traditional problems image inpainting image denoising aims simultaneously missing pixels remove noise image. typical example restoration archived photographs ﬁlms archived materials prone degraded physical processes chemical decompositions lead various kinds contaminations well. necessary deal corruptions missing values jointly. also test image usually referred representative state-of-the-art algorithms image restoration. considering originally proposed image denoising enhance scheme completion denoising means ﬁrst missing pixels without considering noise apply larger. able achieve acceptable results especially fraction missing pixels relatively high since considers removing noise images purely. additional completion step bmd+ bmd++ exhibit remarkably improved performance. three tensorbased methods trpca perform much better matrix-based approaches rpca rmc. reason rpca conduct matrix recovery channel independently capable exploiting information across channels tensor-based methods take advantage multi-channel structure. also quantitative results much better obtained veriﬁes t-svd suitable capturing spatial-shifting characteristics natural images compared tucker decomposition. another possible application algorithm background modeling problem crucial task video surveillance estimate good model background variations scene. correlation frames reasonably believe background variations approximately low-rank. foreground objects generally occupy small fraction image pixels hence naturally treated sparse errors. randomly extract frames three popular color videos bootstrap hall shoppingmall. then input data generated masking randomly selected pixels frame. illustrated figure methods separate background foreground eﬀectively. separation results obtained method slightly better approaches visually. particular method extracts foreground objects fewer ghosting eﬀects. make comparison algorithm methods perform additional experiments three video sequences randomly dropping pixels frame. extremely small proportion pixels available makes sense detect foreground objects. instead interested background reconstruction situation. rpca trpca suﬀer failure capable recovering background pretty well. figure seen algorithm outperforms again. please second example figure fact rather diﬃcult separate woman intelligence transportation systems traﬃc data traﬃc volumes occupancy rates speeds usually contaminated missing values outliers hardware software malfunctions. experiment apply method estimation traﬃc volume incomplete noisy measurements. data used collected detector located sr-n sacramento county california march downloaded caltrans performance measurement system since data recorded every minutes mapped third-order tensor size low-rank randomly sample percentage traﬃc data compare recovery behavior approach several typical tensor completion methods wtucker halrtc tnnm wtucker weighted variants classic tucker decomposition respectively originally proposed tensor decomposition work conduct rigourous study problem aims learn low-tubalrank tensor partial observations arbitrarily corrupted. study rests heavily recently proposed t-svd associated algebraic framework deﬁne tubal rank tubal nuclear norm tensors. propose group tensor incoherence conditions natural elegant extensions corresponding matrix incoherence conditions respectively much weaker given conditions show exactly recover third-order tensor tubal-rank high probability establish theoretical bound exact recovery using convex optimization algorithm. numerical experiments verify theoretical analysis real-world applications demonstrate superiority method existing approaches. results conﬁrm t-svd algebraic framework outline section appropriate capture low-rank structure multidimensional data. suggests interesting apply model algorithm possible applications face recognition data mining bioinformatics. considering real data routinely thousands even billions dimensions computational cost method become expensive. require develop fast algorithms low-tubal-rank tensor recovery explore important direction future work.", "year": 2017}