{"title": "The Relative Performance of Ensemble Methods with Deep Convolutional  Neural Networks for Image Classification", "tag": ["stat.ML", "cs.CV", "cs.LG", "stat.ME"], "abstract": "Artificial neural networks have been successfully applied to a variety of machine learning tasks, including image recognition, semantic segmentation, and machine translation. However, few studies fully investigated ensembles of artificial neural networks. In this work, we investigated multiple widely used ensemble methods, including unweighted averaging, majority voting, the Bayes Optimal Classifier, and the (discrete) Super Learner, for image recognition tasks, with deep neural networks as candidate algorithms. We designed several experiments, with the candidate algorithms being the same network structure with different model checkpoints within a single training process, networks with same structure but trained multiple times stochastically, and networks with different structure. In addition, we further studied the over-confidence phenomenon of the neural networks, as well as its impact on the ensemble methods. Across all of our experiments, the Super Learner achieved best performance among all the ensemble methods in this study.", "text": "artiﬁcial neural networks successfully applied variety machine learning tasks including image recognition semantic segmentation machine translation. however studies fully investigated ensembles artiﬁcial neural networks. work investigated multiple widely used ensemble methods including unweighted averaging majority voting bayes optimal classiﬁer super learner image recognition tasks deep neural networks candidate algorithms. designed several experiments candidate algorithms network structure different model checkpoints within single training process networks structure trained multiple times stochastically networks different structure. addition studied overconﬁdence phenomenon neural networks well impact ensemble methods. across experiments super learner achieved best performance among ensemble methods study. ensemble learning methods train several baseline models rules combine together make predictions. ensemble learning methods gained popularity superior prediction performance practice. consider prediction task ﬁxed data generating mechanism. performance particular learner depends effective searching strategy approximating optimal predictor deﬁned true data generating distribution theory relative performance various learners depend model assumptions true data-generating distribution. practice performance learners depend sample size dimensionality bias-variance trade-off model. thus generally impossible know priori learner would perform best given ﬁnite sample data prediction problem widely used method cross-validation give objective honest assessment learners select single algorithm achieves best validation-performance. known discrete super learner selector asymptotically performs well best base learner library even number candidates grows polynomial sample size. instead selecting algorithm another approach guarantee predictive performance compute optimal convex combination base learners. idea ensemble learning combines predictors instead selecting single predictor well studied literature summarized referred several related studies theoretical properties ensemble learning. widely used ensemble techniques bagging boosting bagging uses bootstrap aggregation reduce variance strong learners boosting algorithms boost capacity weak learners. proposed linear combination strategy called stacking ensemble models. extended stacked generalization cross-validation based optimization framework called super learner ﬁnds optimal combination collection prediction algorithms minimizing cross-validated risk. recently super learner showed great success variety areas including precision medicine mortality prediction online learning spatial prediction. recent years deep artiﬁcial neural networks series breakthroughs variety tasks. anns shown great success almost machine learning related challenges across different areas like computer vision machine translation social network analysis high capacity/ﬂexibility deep neural networks usually high variance bias. practice model averaging multiple stochastically trained networks commonly used improve predictive performance. ﬁrst place image classiﬁcation challenge ilsvrc averaging cnns structure. ﬁrst place classiﬁcation localization challenge ilsvrc averaging multiple deep cnns. ﬁrst place using models residual network different depth form ensemble ilsvrc addition also imagenet detection task ilsvrc ensemble residual network models. however behavior ensemble learning deep networks still well studied understood. first neural networks literature focuses mainly design network structure applies naive averaging ensemble enhance performance. best knowledge detailed work investigates compares discusses ensemble methods deep neural networks. naive unweighted averaging largely used data-adaptive thus vulnerable library base learners works well networks similar structure comparable performance sensitive presence excessively biased base learners. issue could easily addressed cross-validation based data-adaptive ensemble like bayes optimal classiﬁer super learner. later sections investigate compare performance four commonly used ensemble methods image classiﬁcation task deep convolutional neural networks base learners. study mainly focuses comparison ensemble methods cnns image recognition. readers familiar deep learning could treated black-box estimator image input outputs probability vector possible class. refer interested reader details deep learning. paper algorithm candidate hypothesis base learner refer individual learner used ensemble. term ’library’ refers base learners ensemble methods. unweighted averaging common ensemble approach neural networks. takes unweighted average output score/probability base learners reports predicted score/probability. high capacity deep neural networks simple unweighted averaging improves performance substantively. taking average multiple networks reduces variance deep anns high variance bias. models uncorrelated enough variance models could dramatically reduced averaging. idea inspires random forest builds less correlated trees bootstrapping observations sampling features. score vector output last layer neural network i-th unit score corresponding k-th class/label predicted probability unit class reasonable average softmax transformation scores might varying scales magnitude across base learners score output different network might different magnitude. indeed adding constant scores classes leaves predicted probability unchanged. study compared naive averaging scores averaging softmax transformed counterparts unweighted averaging might reasonable ensemble similar base learners comparable performance deep learning literature suggests however library contains heterogeneous networks naive unweighted averaging smart choice. vulnerable weaker learners library sensitive over-conﬁdent candidate good meta-learner intelligent enough combine strength base learners data-adaptively. heuristically networks might weak overall prediction strength good discriminating certain subclasses hope meta-learner could combine strengths base learners thus yielding better strategy. majority voting similar unweighted averaging. instead averaging output probability counts votes predicted labels base learners makes ﬁnal prediction using label votes. equivalently takes unweighted average using label base learners chooses label largest value. compared naive averaging majority voting less sensitive output single network. however would still dominated library contains multiple similar dependent base learners. another weakness majority voting loss information uses predicted label. showed pairwise dependence plays important role majority voting. image classiﬁcation shallow networks usually give diverse prediction compared deeper networks. thus hypothesize majority voting would yield greater improvement base learners library shallow networks library deep networks. bayes optimal classiﬁer classiﬁcation problem shown function predictors minimizes misclassiﬁcation rate so-called bayes classiﬁer. given argmaxyp. fully characterized data-generating distribution bayesian voting approach base learner viewed hypothesis made functional form conditional distribution given formally denoting strain training sample data-point denote means value hypothesis trained strain evaluated bayesian voting approach requires prior distribution that models probability hypothesis correct. using bayes rule readily obtains note ∏∈strain likelihood data hypothesis however quantity might reﬂect well quality hypothesis since likelihood training sample subject overﬁtting. give honest estimation could split training data sets model training computing neural networks validation usually aside tune hyper-parameters thus information fully exploited. expect using validation would provide good estimation likelihood finally would assess model using untouched testing set. observed that sample size large hypothesis typically tends much larger posterior probability others. later section validation large posterior weight usually dominated hypothesis weights proportional likelihood validation weight vector dominated dominated single algorithm would selector discrete super learner selector negative likelihood loss function idea stacking originally proposed concludes stacking works deducing biases generalizer respect provided learning set. also studied stacked regression using cross-validation construct ’good’ combination. predictions f··· linear combination weights super learner extension stacking. cross-validation based ensemble framework minimizes cross-validated risk combination. original paper demonstrated ﬁnite sample asymptotic properties super learner. literature shows application wide range topics e.g. survival analysis clinical trial mortality prediction combines base learners cross-validation. example -fold cross-validation base learners binary prediction. ﬁrst deﬁne cross-validated loss j-th base learner simplicity consider binary classiﬁcation task could easily generalized multi-class classiﬁcation regression. ﬁrst study simple version super learner single algorithms using negative log-likelihood loss function would drive weights base learners zero would reduce variance ensemble make interpretable. constrain necessary condition achieve oracle property theory oracle inequality requires bounded loss function lasso constraint highly advisable practice found imposing large leads better practical performance. small data sets recommended cross-validation compute optimal ensemble weight vector. however takes long time data library large. usually people aside validation instead cross-validation assess tune models deep learning. similarly instead optimizing v-fold cross-validated loss could optimize single-split cross-validation loss instead ensemble weights called single split super learner. figure shows details variation super learner. shows success single split super learner three large healthcare databases. study compute weights super learner minimizing single-split cross-validated loss. procedure necessitates almost additional computation forward pass validation images solving low-dimensional convex optimization. removes certain proportion activations training uses activations testing. could seen training multiple base learners ensemling prediction. discusses resnet state-of-the-art network structure could understood exponential ensembles shallow networks. however ensembles might highly biased meta-learner computes weights based prediction base learner training set. weights might biased base-learners might make objective prediction training set. contrast super learner computes honest ensemble weight based validation set. validation commonly used train/tune neural network. however usually used select tuning parameters image classiﬁcation data sets validation large order make validation stable. thus conjecture potential validation information fully exploited. super learner could considered neural network convolution validation scores base learners input. learns kernel either back-propagation directly solving convex optimization problem. cifar- data widely used benchmark data image recognition. contains classes natural images training images testing images. image image size classes data airplane automobile bird deer frog horse ship truck. class images training data images testing data. figure super learner convolution neural network perspective. base learners trained training convolutional layer trained validation set. simple structure avoids overﬁtting validation set. network network structure consists mlpconv layers multilayer perceptrons convolve input. layer made convolution layer larger kernel size followed convolution layer pooling layer. addition uses global average pooling layer replacement fully connected layers conventional neural networks. googlenet deep convolutional neural network architecture based inception module improved computational efﬁciency. inception module convolution applied dimension reduction expensive large convolutions. within inception module propagation splits ﬂows different convolution size concatenated. neural network structure using architecture small convolution ﬁlters ﬁrst second places localization classiﬁcation tracks imagenet challenge respectively. block made several consecutive convolutions followed pooling layer. number ﬁlters convolution increases network goes deeper. finally three fully connected layers softmax transformation. residual network network structure stacked multiple bottleneck building blocks. figure shows example called bottleneck building block stacked regular layer original study bottleneck building block made three convolutional layers kernel size similar googlenet uses convolution dimension reduction reduce computation. parameter-free identity shortcut starting layer ﬁnal output bottleneck block. solves degradation problem deep networks makes training deep neural network possible. later sections follow structure original paper cifar- data stack layers convolutions. sizes feature maps respectively layers feature size would layers including softmax layer. example resnet layers total. slightly modiﬁed training procedure original paper ilsvrc- competitions used momentum started learning rate decay divide every iterations. training regularized penalty weight dropout layers ﬁtst fully connected layer rate googlenet base learning rate weight decay momentum decreased learning rate every epochs. rate dropout layer last fully connected layer. residual network follow training procedures original paper applied weight decay momentum weight initialized following method applied batch normalization without dropout. learning rate started divided every iterations. trained model epochs. optimal classiﬁer super learner also include discrete negative log-likelihood loss error loss.. comparison list base learner achieved best performance testing empirical oracle. table shows prediction accuracy resnet different epochs. resne much shallower thus adaptive training smaller interval epoch notice great accuracy improvement around epoch learning rate decay. resnet substantively better naive averaging majority voting. earlier stage learners would worse performance causes deterioration performance naive averaging. performance majority voting even worse best base learner majority base learners under-optimized. experiment weights bocs dominated model gives best performance validation set. thus equivalent discrete super learner negative likelihood loss function. experiments performed well best base learner. subsequent experiments bocs showed similar dominated weight pattern. given practical equivalence discrete super learner don’t elaborate bocs report discrete super learner’s performance. unlike conventional machine learning algorithms deep neural networks solve high-dimensional non-convex optimization problem. mini-batch stochastic gradient descent momentum commonly used training. non-convexity networks structure different initialization training vary lot. studied distribution loss testing certain network structure trained multiple times sgd. shows distribution loss concentrated deeper neural network. suggest deep neural networks less sensitive randomness initialization training. ensemble learning would less helpful deeper nets. trained networks resnet respectively. table shows performance networks. studied performance meta-learners. shallow networks enjoyed improvement compared deeper networks ensembled super learner. similarity models show great improvement compared table prediction accuracy testing ensemble methods. algorithm candidates resnets structure trained several times differences come randomized initialization sgd. naive averaging. similarly majority voting work well might also diversity base learners. discrete negative log-likelihood loss successfully selected best single learner library discrete error loss selected slightly weaker one. suggests ﬁnite samples super learner using negative likelihood loss performs better w.r.t. prediction accuracy super learner uses prediction accuracy criterion. over-conﬁdent model loss classiﬁcation differentiable cross-entropy loss commonly used surrogate loss neural network training. could table cross-entropy usually negatively correlated prediction accuracy. however could networkin-network model much lower cross-entropy loss compared models interesting observe high-conﬁdence phenomenon network-in-network model predictions made high conﬁdence highconﬁdent networks usually achieve much smaller surrogate loss testing necessary smaller error loss. though networks suffered over-ﬁtting showed over-conﬁdence. addition higher training cross-entropy loss compared thus reasonable blindly attribute over-conﬁdence over-ﬁtting. several base learners suffer over-conﬁdence issue performance model averaging would seriously deteriorated unweighted average score/probability would dominated over-conﬁdent models. models over-conﬁdent unweighted average identical majority vote. empirically study impact over-conﬁdent network candidates ensemble methods candidates ensemble library resnet resnet resnet compare performance with/without adding over-conﬁdent net. table shows performance ensemble algorithms testing set. unweighted average model weakened over-conﬁdence made dominate others decrease prediction accuracy. naive average softmax less inﬂuenced scale networks different. majority vote algorithm inﬂuenced much extra candidate surprising. over-conﬁdent network weakened discrete negative log-likelihood loss inﬂuence discrete error loss. super learner successfully harnessed over-conﬁdent model adding helped increase prediction accuracy table prediction accuracy testing ensemble methods. algorithm candidates include resnet resnet resnet compare performance with/without over-conﬁdent network. hope ensemble method could learn models even though might base learners weaker overall performance compared learners library. experiment used under-trained googlenets weak candidates. original paper describe explicitly automatically train/tune network cifar data set. initial learning rate momentum decreased learning rate every epochs. give satisfactory performance prediction accuracy testing around avoid impact over-conﬁdence removed net. thus weakest base learner library achieved accuracy testing set. observe difference prediction accuracy googlenet around means googlenet model substantially weaker candidates. table prediction accuracy testing ensemble methods. algorithm candidates include resnet resnet resnet compared performance with/without under-optimized googlenets. learners would dominated number weak learner large. unweighted averaging also failed case. bocs remained unchanged likelihood validation still dominated base learner. super learner shows exciting success setting prediction accuracy remained stable extra weak learning. number base learners usually much smaller sample size usually apriori learner would achieve best performance encouraged apply rich library possible improve performance super learner. experiment simply networks mentioned library ensemble methods. table shows performance ensemble methods well base learner best performance. large proportion weak learners over-conﬁdent learners ensemble methods much worse performance compared super learner. another strength super learner simply putting potential base learners library super learner computes weights data-adaptively require tedious pre-selecting procedure based human experience. studied relative performance several widely used ensemble methods deep convolutional neural networks base learners cifar data commonly used benchmark image classiﬁcation. unweighted averaging proved surprisingly successful performance base learners comparable. outperformed majority voting almost experiments. however unweighted averaging proved sensitive overconﬁdent candidates. super leaner addressed issue simply optimizing weight validation data-adaptive manner. ensemble structure could considered convolution layer stacked output base learners. could adaptively assign weight base learners enables weak learner improve prediction. super learner proposed cross-validation based ensemble method. however since computationally intensive validation sets typically large image recognition tasks used validation neural networks computing weights super learner instead using conventional cross validation structure simple could easily extended. potential extension linear-weighted super learner would stacking several convolutions non-linear activation layers between. structure could mimic cascading/hierarchical ensemble small number parameters hope meta-learner would overﬁt validation thus would help improve prediction. however involves non-convex optimization results might stable. leave future work.", "year": 2017}