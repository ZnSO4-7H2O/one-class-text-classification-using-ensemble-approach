{"title": "Bridging belief function theory to modern machine learning", "tag": ["cs.AI", "cs.LG"], "abstract": "Machine learning is a quickly evolving field which now looks really different from what it was 15 years ago, when classification and clustering were major issues. This document proposes several trends to explore the new questions of modern machine learning, with the strong afterthought that the belief function framework has a major role to play.", "text": "machine learning quickly evolving ﬁeld looks really diﬀerent years classiﬁcation clustering major issues. document proposes several trends explore questions modern machine learning strong afterthought belief function framework major role play. user generated web-contents portable devices embedded computer vision capabilities machine learning data mining questions fundamental. result questions naturally penetrate neighboring research ﬁelds including belief function theory usual attend classiﬁcation session machine learning session conference devoted belief functions. however hard accept among various proposed approaches based become state-of-the-art methods knowledge spread beyond community. without doubt partly explained relative size scientiﬁc communities consideration although quickly growing relatively small respect statistics bayesian networks neural networks etc. however reason alone suﬃcient indeed topics instance information fusion bf-based methods well recognized methods based classical formalisms probabilities ontologies. report assume additional reason researchers focused progressively turned interests towards problems capture newest trends ﬁeld. fact used example researchers acknowledge ﬁrst perceptions clearly outdated. propose short review respective evolution well attempt perspective. course many senior researchers exercise futile broad view question. however knowledge recent referenced article available reader seeking starting point question links bft. document structured follow section brief recall evolution mainstream community provided. then section short summary earlier ages mid-s sketched well coarse description successful interactions times. afterward provide section synthetic overview revolution blew around early modiﬁed goals organization supporting community. seem picture world list section problems still interest current mainstream well potential interesting evolutions community adapt newly raised questions. considering interpretations often opposes demspter’s imprecise statistic view shafer smets singular view described however another sort various interpretations refer mathematical object mass function perceived generalization leads following taxonomy ∗the author would like thank fabio cuzzolin organized discussion panel status future belief function theory belief conference enriching brainstorm starting point work seen rejoinder discussion panel. probability-aﬃliated interpretation mass function object generalizes discrete probability measure probability masses necessarily known assumed belong interval. whatever origin imprecise probability i.e. either statistical theory hints subjective mathematical theory behind random sets set-aﬃliated interpretation mass function description real value ill-known variable enriched weightings one. course view conceptually closer fuzzy sets possibility theory artiﬁcial intelligence general. capacity-aﬃliated interpretation mass function particular type choquet’s capacity moebius transform totally monotone. interpretation strongly related multi-criteria optimization operational research historical perspective oldest probability-aﬃliated interpretation rooted imprecise statistics moving towards non-frequentist views inﬂuence shafer; shafer shenoy works link probability remains strong reference random explicit. later set-aﬃliated interpretation spread inﬂuence renown authors strong background data fusion fuzzy sets expert systems. among them smets constantly positioned work respect bayesian classiﬁers bayesian reasoning however transferable belief model non-probabilistic model which smet’s view compliant random sets. besides capacity-aﬃliated interpretation never really developed independently works operational research game theory. finally todays great majority belief function community accepted set-aﬃliated interpretation appears proceedings majority articles undertook interpretation. fully described miclet cornu´ejols short history full rapid strong evolutions date discipline originally part completely separated parent discipline make path appears strongly converge toward statistics optimization review original heuristics motivated bio-inspiration transformation systematic explorations golden symbolic learning ﬁnally opposition supervised unsupervised learning classiﬁcation clustering problems center attention mid-s. described probabilistic graphical models developed parallel period. fact omission really surprising community well separated community. fact also classically used problems data fusion system diagnosis involve learning. however long time bayesian networks amongst state-of-the-art supervised learning methods along others non-probabilistic methods hand unsupervised learning mainly non-probabilistic kohonen maps course k-means. latter interesting although probabilistic algorithm provides relaxed solution algorithm applied mixture gaussian models however parallel bezdek proposed established fuzzy version k-means named fuzzy c-means spite non-probabilistic motivation behaves similarly algorithm. clearly illustrates that spite diﬀerent culture similar tools proposed diﬀerent communities. another episode related multiple cultural anchors k-means recently showed established k-means discrete counterpart classical non-probabilistic method statistics developed multivariate analysis school nutshell mid-s connections although weakening remained appearent; problems interest related clustering classiﬁcation; various formalisms involved including probabilities none claims clear superiority. context naturally join trend numerous evidential versions classical algorithms successfully proposed rapidly become state-of-the-art references. among them signiﬁcant proportion proposed denoeux evidential k-nn evidential neural network late early basis early numerous derivations virtually classical algorithms proposed. among them smets’ series target identiﬁcation kalman ﬁlter prime importance reasons. first thrived generalized bayesian theorem proposed decade earlier theorem proposes link likelihood plausibility functions becomes possible derive algorithms framework parameters model observed data perfectly compatible language path taken denoeux order extend demspter’s algorithm cautiously stepping framework. second reason importance smet’s series kalman ﬁlter cultural defender constantly opposed bayesian views that major inﬂuence mid-s less less inﬂuenced probabilities depicted community mainly bayesian ﬁeld. context numerous researchers community including frozen decade-old past community would inspired bayesianism mainly focused classiﬁcation clustering; facts obviously hold anymore. works vapnik support vector machines statistical learning theory provided foundations major revolution transformed late which according st´ephane mallat still major impacts ﬁfteen years later. cornerstones revolution three them. ﬁrst obviously kernel trick deeply roots machine learning frameworks distance geometry riemannian analysis multivariate analysis second idea learning problem addressed minimization empirical risk finally third accept that depending dimensionality description space size dataset empirical risk minimization ill-posed regularizer involved optimization. ideas percolated community decade providing tools give unifying description wavelet transform based signal processing diﬀusion process kernel machines deep neural networks. combined ﬁrst works variable section penalty lead major breakthroughs sparse learning prime importance uncertainty theory communities main connection robust uncertainty principles proposed cand`es romberg todays mid-s anymore inter-disciplinary ﬁeld interfere diﬀerent theories ranging cognitive sciences probability solve supervised unsupervised problems background culture objectives changed. regarding objectives relate information retrieval social networks recommender systems feature extraction variable selection data factorization sublinear optimization. even improving clustering classiﬁcation method still deserves interest harder problems presented challenges receives focus. regarding background culture ﬁeld less interdisciplinary mainly considered sub-domain applied mathematics build optimization geometry multivariate statistics harmonic analysis. little room left subjective probabilities described also well illustrated applied mathematics background researchers recently hired labs. context appear particularly diﬃcult community adapt recent evolution order provide state-of-the-art developments. first interpretations opposite evolutions probability-aﬃliated view compatible statistics given room subjectivism ﬁnally preponderant set-aﬃliated view tailored data fusion problems. hand quit become tied functional analysis convex optimization. antagonism well illustrated following observation entries observations modeled points living vector space; hand entries assumed subjective opinions diﬀerent agents high semantics. similar major asset provide rich description various types uncertainty associated pieces information; hand classically interested modeling them rather blindly minimize loss function. nevertheless community still several cards play respect sequel section present them sorted according interpretation aﬃliate ﬁrst keep set-aﬃliated interpretations including restrict speciﬁc problems adapted. deﬁnitely easiest prevents change current mainstream community. however remains interest even path narrowing pressure data constraints. obviously must focus problems where ﬁrst stage several agents used dedicated learning tasks second stage cooperation combination expected. encompasses wide class problems classically faced computer vision among addressed ensemble learning idea combine capabilities several classiﬁers consensus decision robust. setting long explored framework well references) several researchers including however impact works limited lack theoretical performance guaranties boosting methods more precisely clustering classiﬁcation challenging problems still exist setting diﬀers original classically used instance classiﬁcation billions items millions classes computer vision problems classiﬁcation issue respect feature extraction problem precedes. setting various classiﬁers work diﬀerent feature spaces diﬀerent datasets order complementary knowledge. then classiﬁer used label examples useful classiﬁers improve performances. extended transfer learning problems models trained ﬁrst setting transposed similar settings settings required several agents either humans machines automatically evaluate level knowledge each provide communication scheme them improve another speciﬁcity consistency described seems adapted framework consider problem general case. even strictly speaking learning process would accounted cooperation framework would prime interest numerous tasks complex scene analysis bioinformatics etc. second option relies reversing purpose evolution theories back probability-aﬃliated interpretations root solid statistical foundations. even involved optimization problem formulation remains language statistics compliant dempster’s original view. setting could interesting address several questions. instance possible reformulate empirical risk minimization principle framework? minimizer remain convex several forms uncertainties distinctly accounted expression? apart rather general questions note recent works rely likelihood interpretation plausibility setting diﬀers already step direction. citation rates works acknowledge idea going back probability-aﬃliated interpretation makes sense. last solution accelerate evolution community probability-aﬃliated set-aﬃliated interpretations capacity-aﬃliated focuses interest. fact view much related optimization options provides terms modeling well terms solver could deﬁnitely interest basically would lead consider learning problems rely game theory setting multiple criteria optimized meantime. course would naturally lead models restrict totally monotone capacities types capacities. generally encompassing wider frameworks potentially enriching line already adopted researchers questions central hypothesis testing interval data regression based choquet’s capacities partial order ranking computation vapnik-chervonenkis dimension choquet integral etc. beyond numerous remaining questions worthy ﬁrst comment addressed fact evolution problem formulation remains language statistics question probabailistic interpretation. matter fact involvement risk function deﬁned expectation loss function justiﬁed approximated empirical risk basis assumption data i.i.d. deﬁnitely roots machine learning statistic multivariate analysis measure theory; even traditional hypothesis testing parametric probabilistic modeling involved even optimization methods used solve problem. another comment requires cited which ’solid foundations’ assertion seems repetition clich´e contrasting ’solid foundations’ probability statistics alleged lack foundations non-probabilistic approaches. never told ’solid foundations’ actually are. ’solid foundations’ statistical approaches described lenght vapnik’s statistical learning theory addition providing axiomatization problem provides metrics objectively evaluate experimental results well generalization bounds. never said non-probabilistic approaches lack foundations anonymous reviewer misread believe foundations proven adapt current problems already formalized literature. finally even modern mainstream less interested classiﬁcation/clustering problems natural application ﬁeld remain open questions would advantageously beneﬁt bft. among open questions based well-established interpretations aﬃliated probability theories requires move forwards accept wider interpretations inserted wider frameworks among open questions tentatively addressed leading researchers points directions providing successful ﬁrst results others remain unaddressed finally despite separating proﬁt applied mathematics still provides interesting playground researchers. importantly speciﬁc open questions even expect signiﬁcant contributions.", "year": 2015}