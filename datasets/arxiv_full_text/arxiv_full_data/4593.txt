{"title": "Expectation Propagation for approximate Bayesian inference", "tag": ["cs.AI", "cs.LG"], "abstract": "This paper presents a new deterministic approximation technique in Bayesian networks. This method, \"Expectation Propagation\", unifies two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. All three algorithms try to recover an approximate distribution which is close in KL divergence to the true distribution. Loopy belief propagation, because it propagates exact belief states, is useful for a limited class of belief networks, such as those which are purely discrete. Expectation Propagation approximates the belief states by only retaining certain expectations, such as mean and variance, and iterates until these expectations are consistent throughout the network. This makes it applicable to hybrid networks with discrete and continuous nodes. Expectation Propagation also extends belief propagation in the opposite direction - it can propagate richer belief states that incorporate correlations between nodes. Experiments with Gaussian mixture models show Expectation Propagation to be convincingly better than methods with similar computational cost: Laplace's method, variational Bayes, and Monte Carlo. Expectation Propagation also provides an efficient algorithm for training Bayes point machine classifiers.", "text": "paper presents imation technique method \"expectation previous techniques extension lief propagation tion bayesian networks. gation exact belief states useful limited class belief networks purely discrete. tation propagation retaining variance consistent applicable continuous mixture models convincingly computational ational bayes monte carlo. expectation propagation training section groundwork tion. assumed-density filtering approximate computing statistical dently proposed cial intelligence control approximations analytically rior perspective required made approximations. refine approximations general form expectation clutter problem algorithms variational importance troducing clutter). posterior typical curacy cost algorithms. absolute rior mean. cost measured operations tion. better ignores conditions defined fixed point. reverse fixed point recover obtain stationary assume terms bounded jective bounded below choose >..; second part fixed points case define 'best' fixed point minimum energy canonical scheme found canonical good reason namely approximating match exact posterior. example. considering reconsider", "year": 2013}