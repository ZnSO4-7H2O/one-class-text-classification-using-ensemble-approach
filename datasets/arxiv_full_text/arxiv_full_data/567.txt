{"title": "Discriminative Neural Sentence Modeling by Tree-Based Convolution", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "This paper proposes a tree-based convolutional neural network (TBCNN) for discriminative sentence modeling. Our models leverage either constituency trees or dependency trees of sentences. The tree-based convolution process extracts sentences' structural features, and these features are aggregated by max pooling. Such architecture allows short propagation paths between the output layer and underlying feature detectors, which enables effective structural feature learning and extraction. We evaluate our models on two tasks: sentiment analysis and question classification. In both experiments, TBCNN outperforms previous state-of-the-art results, including existing neural networks and dedicated feature/rule engineering. We also make efforts to visualize the tree-based convolution process, shedding light on how our models work.", "text": "paper proposes tree-based convolutional neural network discriminative sentence modeling. models leverage either constituency trees dependency trees sentences. treebased convolution process extracts sentences’ structural features features aggregated pooling. architecture allows short propagation paths output layer underlying feature detectors enables effective structural feature learning extraction. evaluate models tasks sentiment analysis question classiﬁcation. experiments tbcnn outperforms previous state-ofthe-art results including existing neural networks dedicated feature/rule engineering. also make efforts visualize tree-based convolution process shedding light models work. introduction discriminative sentence modeling aims capture sentence meanings classify sentences according certain criteria related various tasks interest attracted much attention community feature engineering—for example n-gram features dependency subtree features dedicated ones —can play important role modeling sentences. kernel machines e.g. exploited moschitti reichartz specifying certain measure similarity sentences without explicit feature representation. recent advances neural networks bring languages techniques understanding natural exhibited considerable potential. bengio mikolov propose unsupervised approaches learn word embeddings mapping discrete words real-valued vectors meaning space. mikolov extend approaches learn sentences’ paragraphs’ representations. compared human engineering neural networks serve automatic feature learning widely used neural sentence models convolutional neural networks recursive neural networks cnns extract words’ neighboring features effectively short propagation paths capture inherent sentence structures rnns encode extent structural information recursive semantic composition along parsing tree. however difﬁculties learning deep dependencies long propagation paths paper propose novel neural architecture discriminative sentence modeling called tree-based convolutional neural network models leverage different sentence parsing trees e.g. constituency trees dependency trees. model variants denoted c-tbcnn d-tbcnn respectively. idea tree-based convolution apply subtree feature detectors sliding entire parsing tree sentence; pooling aggregates extracted feature vectors taking maximum value dimension. merit tbcnns evaluated tasks sentiment analysis question classiﬁcation; models outperformed previous state-of-the-art results experiments. understand tbcnns work also visualize network plotting convolution process. make code results available project website. convolutional neural networks early used image processing turn effective natural languages well. figure depicts classic convolution process sentence ﬁxed-width-window feature detectors slide sentence output extracted features. window size x··· ne-dimensional word embeddings. output convolution evaluated current position rnc× parameters; activation function. semicolons represent column vector concatenation. convolution extracted features pooled ﬁxedsize vector classiﬁcation. convolution extract neighboring information effectively. features local—words convolution window interact other even though semantically related. kalchbrenner build deep convolutional networks local features high-level layers. similar deep cnns include models mean structural information used explicitly. recursive neural networks recursive neural networks proposed socher utilize sentence parsing trees. original version built upon binarized constituency tree. leaf nodes correspond words sentence represented nedimensional embeddings. non-leaf nodes sentence constituents coded child nodes recursively. node parent vector representations denoted parent’s representation composited parameters. process done recursively along tree; root vector used supervised classiﬁcation dependency parsing combinatory categorical grammar also exploited rnns’ skeletons build deep rnns enhance information interaction. improvements semantic compositionality include matrix-vector interaction tensor interaction suitable capturing logical information sentences negation exclamation. mation loss. thus rnns bury illuminating information complicated neural architecture. further back-propagation long path gradients tend vanish makes training difﬁcult long short term memory ﬁrst proposed modeling time-series data integrated rnns alleviate problem recurrent networks. variant class rnns recurrent neural network whose architecture rightmost tree. models meaningful tree structures also lost similar cnns. first sentence converted parsing tree either constituency dependency tree. corresponding model variants denoted ctbcnn d-tbcnn. node tree represented distributed real-valued vector. then design ﬁxed-depth subtree feature detectors called tree-based convolution window. window slides entire tree extract structural information sentence illustrated dashed triangle figure formally assume nodes convolution window x··· represented ne-dimensional vector. number feature detectors. output tree-based convolution window evaluated current subtree given following generic equation. extracted features thereafter packed ﬁxed-size vectors pooling maximum value dimension taken. finally fully connected hidden layer softmax output layer. designed architecture tbcnn models allow short propagation paths output layer position tree. therefore structural feature learning becomes effective. several main technical points tree-based convolution include represent hidden nodes vectors constituency trees? determine weights dependency trees nodes different numbers children? pool varying sized shaped features ﬁxed-size vectors? rest section explain model variants detail. particularly subsections address ﬁrst second problems; subsection deals third problem introducing several pooling heuristics. subsection presents training objective. figure illustrates example constituency tree leaf nodes words sentence non-leaf nodes represent grammatical constituent e.g. noun phrase. sentences parsed stanford parser; further constituency trees binarized simplicity. problem constituency trees nonleaf nodes vector representations word embeddings. strategy pretrain constituency tree equation pretraining vector representations nodes ﬁxed. consider tree-based convolution process c-tbcnn two-layer-subtree convolution window operates parent node direct children vector representations denoted convolution equation speciﬁc c-tbcnn tree-based convolution windows extended arbitrary depths straightforwardly. complexity exponential depth window linear number nodes. hence tree-based convolution compared cnns computational cost provided amount information process time. experiments convolution windows depth figure tree-based convolution c-tbcnn d-tbcnn. parsing trees correspond sentence loved dashed triangles illustrate shared-weight convolution window sliding tree. clarity positions drawn c-tbcnn. notice dotted arrows part neural connections; merely indicate topologies tree structures. specially edge dependency tree refers governed dependency type d-tbcnn dependency trees another representation sentence structures. nature dependency representation leads d-tbcnn’s major difference traditional convolution exist nodes different numbers child nodes. causes trouble associate weight parameters according positions window standard traditional convolution e.g. collobert weston c-tbcnn. based convolution also topologies varying size shape. dynamic pooling common technique dealing problem. propose several heuristics pooling along tree structure. generic design criteria pooling include nodes pooled slot neighboring viewpoint. slot similar numbers nodes expectation pooled thus equal amount information aggregated along different parts tree. following intuition propose pooling heuristics follows. global pooling. features pooled vector shown figure take maximum value dimension. simple heuristic applicable structure including c-tbcnn d-tbcnn. -slot pooling c-tbcnn. preserve information different parts constituency trees propose -slot pooling tree maximum depth pool nodes less laylower slot overcome problem extend notion convolution assigning weights according dependency types rather positions. believe strategy makes much sense dependency types reﬂect relationship governing word child words. concrete generic convolution formula d-tbcnn becomes weight parameter parent weight child grammatical relationship parent superscript indicates parameters d-tbcnn. note keep frequently occurred dependency types; others appearing rarely corpus mapped shared weight matrix. c-tbcnn d-tbcnn advantages d-tbcnn exploits structural features efﬁciently compact expressiveness dependency trees; c-tbcnn effective integrating global features underneath pretrained rnn. data samples offers rare combination entertainment education. idealistic love story brings latent -year-old romantic everyone. mysteries transparently obvious it’s slowly paced thriller. temperature center earth? state battle bighorn take place sentiment analysis task dataset sentiment analysis widely studied task discriminative sentence modeling. stanford sentiment treebank consists movie reviews. settings considered sentiment prediction ﬁne-grained classiﬁcation labels coarse-gained polarity classiﬁcation labels examples shown table standard split training validating testing containing sentences -class prediction. binary classiﬁcation contain neutral class. dataset phrases also tagged sentiment labels. rnns deal naturally recursive process. regard sub-sentences individual samples training like kalchbrenner mikolov training therefore entries total. validating testing whole sentences considered experiments. training details subsection describes training details dtbcnn hyperparameters chosen validation. c-tbcnn mostly tuned synchronously changes hyperparameters. c-tbcnn’s settings found website. nodes pooled slots lower left lower right according relative position respect root node. constituency tree completely obvious pool features slots comply aforementioned criteria time. therefore regard -slot pooling c-tbcnn hard mechanism temporarily. improvement addressed future work. k-slot pooling d-tbcnn. different constituency trees nodes dependency trees one-one corresponding words sentence. thus total order features deﬁned according corresponding word orders. kslot pooling adopt equal allocation strategy shown figure position word sentence extracted feature vector pooled j-th slot assess efﬁcacy pooling quantitatively section shall experimental results complicated pooling methods preserve information along tree structures extent effect large. tbcnns sensitive pooling methods. pooling information packed ﬁxed-size vectors hidden layer softmax layer predict probability target label classiﬁcation task. error function sample standard cross ground truth output softmax number classes. regularize model apply penalty dropout training details presented section d-tbcnn model number units convolution last hidden layer. word embeddings dimensional pretrained using wordvec english wikipedia corpus. slot pooling applied d-tbcnn. performance table compares models state-of-the-art results task sentiment analysis. class prediction d-tbcnn yields accuracy outperforming previous state-of-the-art result achieved based long-short term memory c-tbcnn slightly worse. achieves accuracy ranking third state-of-the-art list richard socher ﬁrst applies neural networks task thinks direct transfer binary classiﬁcation. followed strategy simplicity non-trivial deal neutral sub-sentences training train separate model. website reviews related work -class network transferred directly binary classiﬁcation estimated target probabilities reinterpreted classes. strategy enables take glance stability tbcnn models places difﬁcult position. nonetheless d-tbcnn model achieves accuracy ranking third list. controlled comparison—with shallow architectures basic interaction tbcnns variants consistently outperform rnns large extent also consistently outperform cnns results show structures important modeling sentences; tree-based convolution capture structural information effectively rnns. also observe d-tbcnn achieves higher performance c-tbcnn. suggests compact tree expressiveness important integrating global information task. question classiﬁcation evaluate tbcnn models question classiﬁcation task. dataset contains annotated sentences plus test samples trec also standard split like silva target lachose task evaluate models because number training samples rather small know tbcnns’ performance applied datasets different sizes. alleviate problem data sparseness dimensions convolutional layer last hidden layer respectively. back-propagate gradient embeddings task. dropout rate embeddings hidden layers dropped table compares models various methods. ﬁrst entry presents previous state-of-the-art result achieved traditional feature/rule engineering method utilizes features hand-coded rules. contrary tbcnn models single human-engineered feature rule. despite this c-tbcnn achieves similar accuracy compared feature engineering; d-tbcnn pushes state-of-the-art result best knowledge ﬁrst time neural networks beat dedicated human engineering question classiﬁcation task. table accuracies different pooling methods averaged random initializations. chose sensible hyperparameters manually advance make fair comparison. leads performance degradation vis-a-vis table reasonable protocol comparison tune hyperparameters setting compare highest accuracy. methodology however time-consuming depends largely quality hyperparameter tuning. alternative predeﬁne sensible hyperparameters report accuracy setting. experiment chose latter protocol hidden layers dimensional; penalty added. conﬁguration times different random initializations. summarize mean standard deviation table results imply complicated pooling better global pooling degree model variants. effect strong; models sensitive pooling methods mainly serve necessity dealing varying-structure data. experiments apply -slot pooling c-tbcnn -slot pooling d-tbcnn. comparing studies literature also notice pooling effective efﬁcient information gathering. irsoy cardie report epochs training deep achieves accuracy -class sentiment classiﬁcation. tbcnns typically trained within epochs. effect pooling extracted features tree-based convolution topologies varying size shape. propose section several heuristics pooling. subsection aims provide fair comparison among pooling methods. effect sentence lengths analyze sentence lengths affect models. sentences split groups length granularity long short sentences grouped together smoothing; numbers sentences group vary figure visualizing features related sentiment sentence. sample corresponds sentence dataset stunning dreamlike visual impress even viewers little patience euro-ﬁlm pretension. numbers brackets denote fraction node’s features gathered pooling layer figure presents accuracies versus lengths tbcnns. comparison also reimplemented achieving overall accuracy slightly worse reported socher thus think reimplementation fair comparison sensible. observe c-tbcnn d-tbcnn yield similar behaviors. consistently outperform scenarios. also notice tbcnns increases sentences contain words. result conﬁrms theoretical analysis section —for long sentences propagation paths rnns deep causing rnns’ difﬁculty information processing. contrast models explore structural information effectively tree-based convolution. information part tree propagate output layer short paths tbcnns capable sentence modeling especially long sentences. visualization visualization important understanding mechanism neural networks. tbcnns would like extracted features processed pooling layer ultimately related supervised task. show this trace back pooling layer’s features come from. dimension pooling layer chooses maximum value nodes pooled thus count fraction node’s features gathered pooling. intuitively node’s features related task fraction tends larger vice versa. applied global pooling information tracing sensible pooling slot. shown ﬁgure tree-based convolution effectively extract information relevant task interest. -layer windows corresponding visual impress viewers stunning dreamlike visual discriminative sentence’s sentiment. hence large fractions features convolution gathered pooling. hand words like will even known stop words mostly noninformative sentiment; hence features gathered. results consistent human intuition. observe tree-based convolution integrate information different words window. example word stunning appears windows window stunning itself window stunning dreamlike visual root node visual stunning acting child. window relevant ultimate sentiment window fractions versus even though root visual neutral sentiment. fact window larger fraction children’s paper proposed novel neural discriminative sentence model based sentence parsing structures. model built upon either constituency trees dependency trees example chosen deliberately. similar traits found entire gallery available website. also present d-tbcnn noticing dependency trees intrinsically suitable visualization since know meaning every node. variants achieved high performance sentiment analysis question classiﬁcation. d-tbcnn slightly better c-tbcnn experiments outperformed previous stateof-the-art results tasks. results show tree-based convolution capture sentences’ structural information effectively useful sentence modeling. references allan wade bolivar. retrieval novelty detection sentence level. proceedings annual international sigir conference research development informaion retrieval. collobert weston. uniﬁed architecture natural language processing deep neural networks multitask learning. proceedings international conference machine learning. boyd-graber claudino socher iii. neural network factoid question answering paragraphs. proceedings conference empirical methods natural language processing. kalchbrenner grefenstette blunsom. convolutional neural network modelling sentences. proceedings annual meeting association computational linguistics. lecun jackel bottou brunot cortes denker drucker guyon muller sackinger. comparison learning algorithms handwritten digit proceedings international conrecognition. ference artiﬁcial neural networks. marneffe maccartney manning. generating typed dependency parses phrase structure parses. proceedings language resource evaluation conference. mikolov sutskever chen corrado dean. distributed representations words phrases composiadvances neural information protionality. cessing systems. erhan manzagol bengio bengio vincent. difﬁculty training deep architectures effect unsupervised pre-training. proceedings international conference artiﬁcial intelligence statistics. moschitti. efﬁcient convolution kernels dependency constituent syntactic trees. proceedings european conference machine learning pages springer. zhao poupart. arxiv self-adaptive hierarchical sentence model. preprint arxiv. appear proceedints intenational joint conference artiﬁcial intelligence. paass. semantic relation extraction proceedkernels typed dependency trees. ings sigkdd international conference knowledge discovery data mining. socher huang pennin manning dynamic pooling unfolding recursive autoencoders paraphrase detection. advances neural information processing systems. pennington huang manning. semi-supervised recursive autoencoders preproceedings dicting sentiment distributions. conference empirical methods natural language processing. socher huval manning semantic compositionality prorecursive matrix-vector spaces. ceedings joint conference empirical methods natural language processing computational natural language learning. socher. perelygin chuang manning potts. recursive deep models semantic compositionality sentiment treebank. proceedings conference empirical methods natural language processing. srivastava hinton krizhevsky sutskever salakhutdinov. dropout simple prevent neural journal machine networks overﬁtting. learning research pages socher manning. improved semantic representations treestructured long short-term memory networks. arxiv preprint arxiv. appear proceedings annual meeting association computational linguistics.", "year": 2015}