{"title": "Extend natural neighbor: a novel classification method with  self-adaptive neighborhood parameters in different stages", "tag": ["cs.AI", "cs.LG"], "abstract": "Various kinds of k-nearest neighbor (KNN) based classification methods are the bases of many well-established and high-performance pattern-recognition techniques, but both of them are vulnerable to their parameter choice. Essentially, the challenge is to detect the neighborhood of various data sets, while utterly ignorant of the data characteristic. This article introduces a new supervised classification method: the extend natural neighbor (ENaN) method, and shows that it provides a better classification result without choosing the neighborhood parameter artificially. Unlike the original KNN based method which needs a prior k, the ENaNE method predicts different k in different stages. Therefore, the ENaNE method is able to learn more from flexible neighbor information both in training stage and testing stage, and provide a better classification result.", "text": "various kinds k-nearest neighbor based classification methods bases many well-established high-performance pattern-recognition techniques vulnerable parameter choice. essentially challenge detect neighborhood various data sets utterly ignorant data characteristic. article introduces supervised classification method extend natural neighbor method shows provides better classification result without choosing neighborhood parameter artificially. unlike original based method needs prior enane method predicts different different stages. therefore enane method able learn flexible neighbor information training stage testing stage provide better classification result. identify members predefined class called classification. therefore classification becomes fundamental problem especially pattern recognition data mining several effective algorithms successfully applied many real-world applications. setting classification kinds classifiers parametric classifiers nonparametric classifiers. coming data nonparametric classifiers received particular attention data distributions many classification problems either unknown difficult obtain practice. k-nearest neighbor classifiers typical representative nonparametric classifiers basic task used classify query object category nearest example k-nearest neighbor classifier extends idea taking nearest points assigning sign majority. classifier attracted many researchers make efforts applied various domains therefore witnessed considerable applications many different disciplines however still following main problems limit usage classifier efficiency classification heavily depends type distance measure especially applications data structure complicated corresponding distance measure computationally expensive classifier compare examples database query. become impractical huge database frequent queries. traditional k-nn adopts fixed query samples regardless geometric location related specialties. furthermore k-nearest neighbors distribute symmetrically around query sample neighborhood training spatially homogeneous. geometrical placement might important actual distance depict query samples neighborhood. therefore improve neighborhood based classifiers algorithms proposed. recently based mutual k-nearest neighborhood method tang propose extended nearest neighbor method classification makes two-way communication style unlike classic rule considers nearest neighbors test sample make classification decision method considers nearest neighbors test sample also consider test sample nearest neighbors. two-way communication style advantage also disadvantage method. advantage means classification decision method depended variable training data. disadvantage means problem parameter selection selection still exists. parameter based which size shape graph changed. makes overcome limitations classification work propose method called enan method keep advantage solve disadvantage method help natural neighbor method choosing optimum value training stage testing stage. rest paper outlined follows next section survey related works. present proposed method detail section followed series experiments section section concludes paper discussions future works. method makes prediction two-way communication style considers nearest neighbors test sample also consider test sample nearest neighbors. exploiting generalized class-wise statistics training data iteratively assuming possible class memberships test sample able learn global distribution therefore improving pattern recognition performance providing powerful technique wide range data analysis applications. recently team presented parameter-free nearest neighbor method called natural neighbor method inspired friendship human society could regarded belonging category scale free nearest neighbor method it’s effective method outlier detection natural neighbor method makes three contributions current state natural neighbor method create applicable neighborhood graph based local characteristics various data sets. neighborhood graph identify basic clusters data especially manifold clusters noises. method provide numeric result named natural neighbor eigenvalue replace parameter traditional method number nane dynamically chosen different data sets. original classification stages training stage testing stage. training stage developed calculate generalized class-wise statistic class build weighted graphs training samples vertices distances sample nearest neighbors edges. testing stage firstly assume test sample belongs class calculate generalized class-wise statistic class. assume belongs another class calculate class. class assumptions done classification decision made according eq.. original classification neighborhood parameter used calculate generalized class-wise statistic according eq.. therefore similar traditional methods classification result methods depends heavily firstly demonstrate effectiveness method. classification accuracies method significantly better method cases. more clear parameter choice problem still exist terrible selection also shapely decrease accuracy classification. nane method help based method choose parameter without priority knowledge. considering specific situations method’s stages method selects neighborhood parameters different ways. training stage method needs neighborhood measure distribution training data case nane reflects overall distribution data better choice. illustrate relationship data natural neighbor eigenvalue four simple examples artificial data sets shown fig. testing stage self-adaptive neighborhood sample accurate. fortunately idea natural neighbors number neighbors point fixed. natural neighborhood search process need limitation neighborhood parameters dynamic neighborhood value accurately reflect relationship data points data point according environmental characteristics find suitable neighbor. particular data neighbors goal construct efficient classification algorithm nearest neighbor classification help methods. enan method performs stages training stage testing stage steps explained below. training stage algorithm deals training data thus nane method used parameter generalized class-wise statistic calculation step. weighted graphs used calculate distance natural neighbor efficiently given test sample progress generalized classwise statistic calculation step algorithm stores weighted graphs transfer next stage. following description enan classification algorithm training stage. time complexity algorithm size training data set. algorithm firstly creates tree data time complexity step that complexity nane calculation value range nane must nane generally high dimensional irregular data sets nane less nane calculation step complexity class build weighted graph computational complexity last generalized class-wise statistic computation computational complexity testing stage goal algorithm determine class sample testing data belongs taking account diversity samples number every samples’ natural neighbors neighborhood parameter calculate generalized class-wise statistic. following description enan classification algorithm testing stage. time complexity algorithm size training data size testing data set. existence weighted graph sample testing data necessary calculate number natural neighbor training data computational complexity similarly time complexity generalized class-wise statistic calculation step algorithm demonstrate superiority proposed algorithm original algorithm classification section compare original algorithm enan method. best original algorithm searched range sqrt -fold cross-validation training set. algorithms experiments matlab table algorithm wins cases choices close best situations. overall enan algorithm achieves highest accuracy stability. recall that proposed algorithm completely neighborhood parameter free algorithm means longer need choose adaptive data advantage even important classification accuracy. result demonstrates universality enan algorithm characteristics neighborhood parameter free method exists nearest neighbor based algorithm. therefore chance innovate solve problems clustering outlier detection proposed based method based method. paper proposes novel parameter free classification method based extended nearest neighbor method parameter choosing problems training stage testing stage perfectly solved natural neighbor method uncorrelated ways. experimental results show classification result intuitively reflects characteristics data sets compared original algorithm different parameter algorithm increases accuracy classification result well adaption different kind data sets avoid neighborhood choosing problem.", "year": 2016}