{"title": "An Analysis of Visual Question Answering Algorithms", "tag": ["cs.CV", "cs.AI", "cs.CL"], "abstract": "In visual question answering (VQA), an algorithm must answer text-based questions about images. While multiple datasets for VQA have been created since late 2014, they all have flaws in both their content and the way algorithms are evaluated on them. As a result, evaluation scores are inflated and predominantly determined by answering easier questions, making it difficult to compare different methods. In this paper, we analyze existing VQA algorithms using a new dataset. It contains over 1.6 million questions organized into 12 different categories. We also introduce questions that are meaningless for a given image to force a VQA system to reason about image content. We propose new evaluation schemes that compensate for over-represented question-types and make it easier to study the strengths and weaknesses of algorithms. We analyze the performance of both baseline and state-of-the-art VQA models, including multi-modal compact bilinear pooling (MCB), neural module networks, and recurrent answering units. Our experiments establish how attention helps certain categories more than others, determine which models work better than others, and explain how simple models (e.g. MLP) can surpass more complex models (MCB) by simply learning to answer large, easy question categories.", "text": "figure good benchmark tests wide range computer vision tasks unbiased manner. paper propose dataset distinct tasks evaluation metrics compensate bias strengths limitations algorithms better measured. research began earnest late daquar dataset released including daquar major datasets released algorithms rapidly improved. popular dataset ‘the dataset’ best algorithms approaching accuracy results promising critical problems existing datasets terms multiple kinds biases. moreover existing datasets group instances meaningful categories easy compare abilities individual algorithms. example method excel color questions compared answering questions requiring spatial reasoning. color questions common dataset algorithm performs well spatial reasoning appropriately rewarded feat evaluation metrics used. contributions paper four major contributions aimed better analyzing comparing algorithms create benchmark dataset questions divided different categories based task solve; propose evaluation metrics compensate forms dataset bias; balance visual question answering algorithm must answer text-based questions images. multiple datasets created since late ﬂaws content algorithms evaluated them. result evaluation scores inﬂated predominantly determined answering easier questions making difﬁcult compare different methods. paper analyze existing algorithms using dataset called task driven image understanding challenge million questions organized different categories also introduce questions meaningless given image force system reason image content. propose evaluation schemes compensate over-represented question-types make easier study strengths weaknesses algorithms. analyze performance baseline state-of-the-art models including multi-modal compact bilinear pooling neural module networks recurrent answering units. experiments establish attention helps certain categories others determine models work better others explain simple models surpass complex models simply learning answer large easy question categories. open-ended visual question answering algorithm must produce answers arbitrary text-based questions images exciting computer vision problem requires system capable many tasks. truly solving would milestone artiﬁcial intelligence would signiﬁcantly advance human computer interaction. however datasets must test wide range abilities progress adequately measured. number yes/no object presence detection questions assess whether balanced distribution help algorithms learn better; introduce absurd questions force algorithm determine question valid given image. dataset re-train evaluate baseline state-of-the-art algorithms. found proposed approach enables nuanced comparisons algorithms helps understand beneﬁts speciﬁc techniques better. addition also allowed answer several questions algorithms generalization capacity algorithms hindered bias dataset?’ ‘does spatial attention help answer speciﬁc question-types?’ ‘how successful algorithms answering lesscommon questions?’ ’can algorithms differentiate real absurd questions?’ datasets natural images released daquar cocoqa fm-iqa dataset visualw visual genome fm-iqa needs human judges widely used discuss further. table shows statistics datasets. following others refer portion dataset containing natural images coco-vqa. detailed dataset reviews found aforementioned datasets biased. daquar coco-qa small limited variety question-types. visual genome visualw coco-vqa larger suffer several biases. bias takes form kinds questions asked answers people give them. coco-vqa system trained using question features achieves accuracy suggests questions predictable answers. without nuanced analysis challenging determine kinds questions dependent image. datasets made using mechanical turk annotators often object recognition questions e.g. ‘what image?’ elephant image?’. note latter example annotators rarely kind question unless object image. coco-vqa questions beginning ‘yes’ ground truth answer. question asked different images annotators instructed give opposite answers helped reduce language bias. however addition language bias datasets also biased distribution different types questions distribution answers within question-type. existing datasets performance metrics treat test instance equal value compute additional statistics basic questiontypes overall performance computed subscores exacerbates issues bias question-types likely biased also common. questions beginning ‘why’ ‘where’ rarely asked annotators compared beginning ‘is’ ’are’. example coco-vqa improving accuracy ‘is/are’ questions increase overall accuracy answering ‘why/where’ questions correctly increase accuracy inability existing evaluation metrics properly address biases algorithms trained datasets learn exploit biases resulting systems work poorly deployed real-world. related reasons major benchmarks released last decade simple accuracy evaluating image recognition related computer vision tasks instead metrics mean-per-class accuracy compensates unbalanced categories. example caltech even balanced training data simple accuracy fails address fact categories much easier classify others mean perclass accuracy compensates requiring system well category even amount test instances categories vary considerably. existing benchmarks require reporting accuracies across different question-types. even reported question-types coarse useful e.g. ‘yes/no’ ‘number’ ‘other’ coco-vqa. improve analysis algorithms categorize questions meaningful types calculate sub-scores incorporate evaluation metrics. previous works studied bias proposed countermeasures. yang dataset created study effect equal number binary questions cartoon images. found answering questions balanced dataset harder. work signiﬁcant limited yes/no questions approach using cartoon imagery cannot directly extended real-world images. goals paper determine kinds questions algorithm answer easily. shapes dataset proposed similar objectives. shapes small dataset consisting images composed arranging colored geometric shapes different spatial orientations. image yes/no questions resulting questions. although shapes serves important adjunct evaluation alone cannot sufﬁce testing algorithm. major limitation shapes images shapes representative real-world imagery. along similar lines compositional language elementary visual reasoning also proposes rendered geometric objects study reasoning capacities model. clevr larger shapes makes rendered geometric objects. addition shape color adds material property objects. clevr types questions attribute query attribute comparison integer comparison counting existence. shapes clevr speciﬁcally tailored compositional language approaches downplay importance visual reasoning. instance clevr question ‘what size cylinder left brown metal thing left sphere?’ requires demanding language reasoning capabilities limited visual understanding needed parse simple geometric objects. unlike three synthetic datasets dataset contains natural images questions. improve algorithm analysis comparison dataset explicitly deﬁned question-types evaluation metrics. past years multiple publicly released datasets spurred research. however biases issues evaluation metrics interpreting comparing performance systems opaque. propose benchmark dataset explicitly assigns questions distinct categories. enables measuring performance within category understand kind questions easy hard today’s best systems. additionally evaluation metrics further compensate biases. call dataset task driven image understanding challenge overall statistics example images dataset shown table fig. respectively. tdiuc question-types chosen represent classical computer vision tasks novel highlevel vision tasks require varying degrees image understanding reasoning. question-types number question-type tdiuc given table questions come three sources. first imported subset questions coco-vqa visual genome. second created algorithms generated questions coco’s semantic segmentation annotations visual genome’s objects attributes annotations third used human annotators certain question-types. following sections brieﬂy describe methods. imported questions coco-vqa visual genome belonging question-types except ‘object utilities affordances’. using large number templates regular expressions. visual genome imported questions word answers. coco-vqa imported questions word answers annotators agreed. color questions question would imported contained word ‘color’ answer commonly used color. questions classiﬁed activity sports recognition questions answer nine common sports ﬁfteen common activities question contained common verbs describing actions sports e.g. playing throwing etc. counting question begin ‘how many’ answer small countable integer categories determined using regular expressions. example classiﬁed question form ‘are sentiment understanding ‘what right of/left behind classiﬁed positional reasoning. table comparison previous natural image datasets tdiuc. coco-vqa explicitly deﬁned number question-types used much ﬁner granularity would possible individually classiﬁed. mc/oe refers whether open-ended multiple-choice evaluation used. similarly ‘what <object category> image?’ similar templates used populate subordinate object recognition questions. method used questions season weather well e.g. ‘what season this?’ rainy/sunny/cloudy?’ ‘what weather like?’ imported scene classiﬁcation. images coco dataset visual genome individual regions semantic knowledge attached them. exploit information generate questions using question templates. introduce variety deﬁne multiple templates question-type annotations populate them. example counting templates e.g. ‘how many <objects> there?’ ‘how many <objects> photo?’ etc. since coco visual genome different annotation formats discuss separately. sport recognition counting subordinate object recognition object presence scene understanding positional reasoning absurd questions created coco similar scheme used counting count number object instances image annotation. minimize ambiguity done objects covered area least pixels. subordinate object recognition create questions require identifying object’s subordinate-level object classiﬁcation based larger semantic category. this coco supercategories semantic concepts encompassing several objects common theme e.g. supercategory ‘furniture’ contains chair couch etc. image contains type furniture question similar ‘what kind furniture picture?’ generated answer ambiguous. using similar heuristics create questions identifying food electronic appliances kitchen appliances animals vehicles. object presence questions images objects area larger pixels produce question similar <object> picture?’ questions ‘yes’ answer. create negative questions questions coco objects present image. make harder prioritize creation questions referring absent objects belong supercategory objects present image. street scene likely contain trucks cars contain couches televisions. therefore difﬁcult answer truck?’ street scene answer couch?’ sport recognition questions detect presence speciﬁc sports equipment annotations questions type sport played. images must contain sports equipment particular sport. similar approach used create scene understanding questions. example toilet sink present annotations room bathroom appropriate scene recognition question created. additionally supercategories ‘indoor’ ‘outdoor’ questions photo taken. creating positional reasoning questions relative locations bounding boxes create questions similar ‘what left/right <object>?’ ambiguous overlapping objects employ following heuristics eliminate ambiguity vertical separation bounding boxes within small threshold; objects overlap half length counterpart; objects horizontally separated distance threshold determined subjectively judging optimal separation reduce ambiguity. tried generate above/below questions results unreliable. absurd questions test ability algorithm judge question answerable based image’s content. make these make list objects absent given image questions rest tdiuc absent objects exception yes/no counting questions. includes questions imported coco-vqa autogenerated questions manually created questions. make list possible questions would ‘absurd’ image uniformly sample three questions image. effect question repeated multiple times throughout dataset either genuine question nonsensical question. algorithm must answer ‘does apply’ question absurd. visual genome’s annotations contain region descriptions relationship graphs object boundaries. however annotations non-exhaustive duplicated makes using automatically make pairs difﬁcult. visual genome make color positional reasoning questions. methods used similar used coco additional precautions needed quirks annotations. additional details provided appendix. manual annotation creating sentiment understanding object utility/affordance questions cannot readily done using templates used manual annotation create these. twelve volunteer annotators trained generate questions used web-based annotation tool developed. shown random images coco visual genome could also upload images. post processing post processing performed questions sources. numbers converted text e.g. became two. answers converted lowercase trailing punctuation stripped. duplicate questions image removed. questions answers appeared least twice. dataset split train test splits train test. scene recognition sport recognition color attributes attributes activity recognition positional reasoning sub. object recognition absurd utility/affordance object presence counting sentiment understanding compensate skewed question-type distribution compute accuracy questiontypes separately. however also important ﬁnal uniﬁed accuracy metric. overall metrics arithmetic harmonic means across question-type accuracies referred arithmetic mean-per-type accuracy harmonic mean-per-type accuracy unlike arithmetic harmonic measures ability system high scores across question-types skewed towards lowest performing categories. also normalized metrics compensate bias form imbalance distribution answers within question-type e.g. repeated answer ‘two’ covers counting-type questions. this compute accuracy unique answer separately within question-type average together question-type. compute overall performance compute arithmetic normalized mean per-type harmonic n-mpt scores. large discrepancy unnormalized normalized scores suggests algorithm generalizing rarer answers. main goals research build computer vision systems capable many tasks instead expertise speciﬁc task reason argued kind visual turing test however simple accuracy used evaluating performance hard know system succeeds goal questiontypes questions others. skewed distributions question-types expected. test question treated equally difﬁcult assess performance rarer question-types compensate bias. propose multiple measures compensate bias skewed distributions. alternative formulations majority systems formulate classiﬁcation problem system given image question answers categories. almost systems features represent image either recurrent neural network bag-of-words model question. brieﬂy review systems focusing models compare experiments. comprehensive review image features come last hidden layer cnn. simple approaches often work well competitive complex attentive models spatial attention heavily investigated models systems weigh visual features based relevance question instead using global features e.g. last hidden layer cnn. example answer ‘what color bear?’ emphasize visual features around bear suppress features. system cvpr- workshop challenge. addition using spatial attention implicitly computes outer product image question features ensure elements interact. explicitly computing outer product would slow extremely high dimensional done using efﬁcient approximation. uses long short-term memory networks embed question. neural module network especially interesting compositional approach main idea compose series discrete modules executed collectively answer given question. achieve this variety modules e.g. find module outputs heat detecting arrange modules question ﬁrst parsed concise expression e.g. ‘what right car?’ parsed ;;). using expressions modules composed sequence answer query. multi-step recurrent answering units model another state-of-the-art method inference step consists complete answering block takes image question output previous lstm step. part larger lstm network progressively reasons question. trained multiple baseline models well state-ofthe-art methods tdiuc. methods predicts ‘yes’ questions. predicts repeated answer question ques linear softmax classiﬁer given question linear softmax classiﬁer given image linear classiﬁer given question image.. -layer question image features. without spatial attention. mcb-a spatial attention. minor modiﬁcations. minor modiﬁcations. ques provide information biases dataset. ques -dimensional skip-thought vectors embed question done image features ‘pool’ layer resnet- normalized unit length. layer softmax output layer. relu hidden layers units respectively. during training dropout used hidden layers. mcb-a used publicly available code train tdiuc. experimental setup hyperparamters kept unchanged default choices code except upgrading rau’s visual representation resnet-. results tdiuc models given table accuracy scores given question-types table scores normalized using meanper-unique-answer given appendix table inspecting table questiontypes comparatively easy scene recognition sport recognition object presence. high accuracy also achieved absurd discuss greater detail sec. subordinate object recognition moderately high despite large number unique answers. accuracy counting across methods despite large number training data. remaining question-types analysis needed pinpoint whether weaker performance lower amounts training data bias limitations models. next investigate much good performance bias answer distribution n-mpt compensates for. major aims compensate fact algorithms achieve high scores simply learning answer populated easier question-types. existing datasets earlier work shown simple baseline methods routinely exceed complex methods using simple accuracy tdiuc surpasses terms simple accuracy closer inspection reveals mlp’s score highly determined performance categories large number examples ‘absurd’ ‘object presence.’ using outperform mlp. inspecting normalized scores question-type shows even pronounced differences also reﬂected arithmetic n-mpt score presented table indicates prone overﬁtting. table results models. unnormalized accuracy question-type shown. overall performance reported using metrics. overall overall averages sub-scores providing clearer picture performance across question-types simple accuracy. overall arithmetic n-mpt harmonic nmpt normalize across unique answers better analyze impact answer imbalance normalized scores individual question-types presented appendix table denotes training without absurd questions. scene recognition sport recognition color attributes attributes activity recognition positional reasoning sub. object recognition absurd utility affordances object presence counting sentiment understanding similar observations made mcb-a compared outperforms mcb-a using simple accuracy scores lower metrics designed compensate skewed answer distribution bias. comparing unnormalized normalized metrics help determine generalization capacity algorithms given question-type. large difference scores suggests algorithm relying skewed answer distribution obtain high scores. found mcb-a accuracy subordinate object recognition drops unnormalized normalized scene recognition drops categories heavily skewed answer distribution; top- answers subordinate object recognition top- answers scene recognition cover questions respective question-types. shows question-types appear easy simply algorithms learning answer statistics. truly easy question-type similar performance unnormalized normalized metrics. example sport recognition shows drop compared drop counting despite counting number unique answers training data. comparing relative drop performance normalized unnormalized metric also compare generalization capability algorithms e.g. subordinate object recognition higher unnormalized score compared mcb-a however previous section models struggle correctly predict rarer answers. less repeated questions actually harder answer algorithms simply biased toward frequent answers? study this created subset tdiuc consisted questions answers repeated less times. call dataset tdiuc-tail train test questions. then trained full tdiuc dataset; tdiuc-tail. versions evaluated validation split tdiuc-tail. found trained tdiuc-tail outperformed trained tdiuc across questiontypes shows capable learning correctly predict rarer answers simply biased towards predicting common answers maximize overall accuracy. using normalized accuracy disincentivizes algorithms’ reliance answer statistics deploying system useful optimize directly n-mpt. sampled rest dataset high prior probability answered ‘does apply.’ corroborated ques model achieves high accuracy absurd; however questions genuine image achieves accuracy questions. good absurd performance achieved sacriﬁcing performance categories. robust system able detect absurd questions without failing others. examining accuracy real questions identical absurd questions quantify algorithm’s ability differentiate absurd questions real ones. found simpler models much lower accuracy questions compared complex models study this trained systems without absurd. results presented table trained without absurd questions accuracies categories increase considerably compared trained full tdiuc especially question-types used sample absurd questions e.g. activity recognition arithmetic accuracy model trained without absurd also substantially greater model trained absurd suggests properly discriminating between absurd real questions biased towards misidentifying genuine questions absurd. contrast capable model produces worse results absurd version trained without absurd shows much smaller differences shows capable identifying absurd questions. sec. skewed answer distribution impact generalization. effect strong even simple questions affects even sophisticated algorithms. consider mcb-a trained coco-vqa visual genome i.e. winner cvpr- workshop challenge. evaluated object presence questions tdiuc contains ‘yes’ ‘no’ questions correctly predicts ‘yes’ answers accuracy questions ‘no’ answer. however training tdiuc mcb-a able achieve ‘yes’ ‘no.’ mcb-a performed poorly learning biases coco-vqa dataset capable performing well dataset unbiased. similar observations balancing yes/no questions made datasets could balance simple categories like object presence extending idea categories challenging task undermines natural statistics breaking questions types assess types beneﬁt attention. comparing model without attention i.e. mcb-a. seen table attention helped improve results several question categories. pronounced increases color recognition attribute recognition absurd counting. question-types require algorithm detect speciﬁed object answered correctly. mcb-a computes attention using local features different spatial locations instead global image features. aids localizing individual objects. attention mechanism learns relative importance features. also utilizes spatial attention shows similar increments. compositional modular approaches lesser extent propose compositional approaches vqa. coco-vqa performed worse models using simple accuracy. hoped would achieve better performance models questions require logically analyzing image step-by-step manner e.g. positional reasoning. however perform better using n-mpt metric substantial beneﬁts speciﬁc question-types. limited quality ‘s-expression’ parser produces incorrect misleading parses many cases. example ‘what color jacket left?’ parsed ;;). expression fails parse ‘the man’ crucial element needed correctly answer question also wrongly interprets ‘left’ past tense leave. performs inference multiple hops because contains complete system learn solve different tasks step. since trained end-to-end need rely rigid question parses. showed good performance detecting absurd questions also performed well categories. introduced tdiuc dataset consists explicitly deﬁned question-types including absurd questions used perform rigorous analysis recent algorithms. proposed evaluation metrics compensate biases datasets. results show absurd questions evaluation metrics enable deeper understanding algorithm behavior.", "year": 2017}