{"title": "Fast and Robust Archetypal Analysis for Representation Learning", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "We revisit a pioneer unsupervised learning technique called archetypal analysis, which is related to successful data analysis methods such as sparse coding and non-negative matrix factorization. Since it was proposed, archetypal analysis did not gain a lot of popularity even though it produces more interpretable models than other alternatives. Because no efficient implementation has ever been made publicly available, its application to important scientific problems may have been severely limited. Our goal is to bring back into favour archetypal analysis. We propose a fast optimization scheme using an active-set strategy, and provide an efficient open-source implementation interfaced with Matlab, R, and Python. Then, we demonstrate the usefulness of archetypal analysis for computer vision tasks, such as codebook learning, signal classification, and large image collection visualization.", "text": "main objective rehabilitate pioneer unsupervised learning technique called archetypal analysis easy interpret providing good results prediction tasks. proposed alternative principal component analysis discovering latent factors high-dimensional data. unlike principal components factor learned archetypal analysis called archetype forced convex combination data points. associations archetypes data points useful interpretation. example clustering techniques provide associations data centroids. indeed common genomics cluster gene expression data several individuals interpret centroid looking common physiological traits among individuals cluster interestingly archetypal analysis related popular approaches sparse coding non-negative matrix factorization even though formulations independently invented around time. archetypal analysis indeed produces sparse representations data points approximating convex combinations archetypes; also provides non-negative factorization data matrix non-negative. natural question archetypal analysis gain success unlike sparse coding. believe lack efﬁcient available software limited deployment archetypal analysis promising applications; goal address issue. first develop efﬁcient optimization technique based active-set strategy then demonstrate approach scalable orders magnitude faster existing publicly available implementations. finally show archetypal analysis useful computer vision believe could many applications ﬁelds neurosciences bioinformatics natural language processing. ﬁrst show performs well sparse coding learning codebooks features visual recognition tasks signal classiﬁcation second show archetypal analysis provides simple effective visualizing large databases images. revisit pioneer unsupervised learning technique called archetypal analysis related successful data analysis methods sparse coding non-negative matrix factorization since proposed archetypal analysis gain popularity even though produces interpretable models alternatives. efﬁcient implementation ever made publicly available application important scientiﬁc problems severely limited. goal bring back favour archetypal analysis. propose fast optimization scheme using activeset strategy provide efﬁcient open-source implementation interfaced matlab python. then demonstrate usefulness archetypal analysis computer vision tasks codebook learning signal classiﬁcation large image collection visualization. unsupervised learning techniques widely used automatically discover underlying structure data. serve several purposes depending task considered. experimental sciences looking data representations automatically exhibit interpretable patterns example groups neurons similar activation population clusters genes manifesting similar expression topics learned text collections image processing computer vision unsupervised learning often used data modeling step subsequent prediction task. example natural image patches modeled sparse coding mixture gaussians yielding powerful representations image restoration. similarly local image descriptors encoded unsupervised learning methods producing successful codebooks visual recognition pipelines. interpretation probably crucial prediction tasks. however important pur∗lear team inria grenoble rhˆone-alpes laboratoire jean kuntzsince -norm related simplicial constraints ∆p—the non-negativity constraints aside— main difference sparse coding archetypal analysis fact archetypes convex combinations data points result vectors constrained simplex encourages sparse. then archetype becomes linear combination data points only useful interpreting moreover non-zero entries indicate proportions input data points related archetype another variant sparse coding called local coordinate coding also related archetypal analysis. variant dictionary elements encouraged close data points uses decompositions. then dictionary elements interpreted anchor points manifold representing data distribution. robust archetypal analysis applications desirable automatically handle outliers—that data points signiﬁcantly differ rest data. order make archetypal analysis robust propose following variant positive constant. whereas cost associated outliers original formulation large since grows quadratically huber cost grows linearly. section present effective iterative reweighted least-square strategy deal huber loss. formulation non-convex convex respect variables ﬁxed. thus natural block-coordinate descent scheme guaranteed asymptotically provide stationary point problem present strategy algorithm noticed ﬁxing variables column minimizing respect section present archetypal analysis formulation; section devoted optimization techniques; section presents successful applications archetypal analysis computer vision tasks section concludes paper. consider matrix rm×n column vector representing data point. archetypal analysis learns factorial representation looks archetypes rm×p geometrical constraints data vector well approximated convex combination archetypes archetype convex combination data points therefore given archetypes vector close product coefﬁcient vector simplex denotes frobenius norm; archetypes represented product solving challenging since optimization problem non-convex; issue addressed section interestingly formulation related approaches brieﬂy review here. similarly matrices archetypal analysis also non-negative non-negative. difference archetypal analysis latter involves simplicial constraints. sparse coding given ﬁxed archetypes rm×p data point approximated under constraint non-negative sums one. words -norm constrained sparsity-inducing effect thus archetypal analysis produces sparse approximations input data smooth optimization problem simplicial constraint. even though generic solvers could used signiﬁcantly faster convergence obtained designing dedicated algorithm leverage underlying sparsity solution propose active-set algorithm beneﬁt solution sparsity carefully implemented. indeed optimum often small subset variables non-zero. active-set algorithms seen aggressive strategy leverage property. given current estimate iteration deﬁne subset s.t. direction solving reduced problem denotes complement index then estimate obtained moving onto direction q—that choosing remains algorithm modiﬁes algorithm ﬁnds optimal solution strategy detailed algorithm opensource active-set solvers generic exist e.g. quadprog matlab found slow purpose. instead dedicated implementation proven similar form update carried line algorithm lines respectively update archetypes residual xba. thus algorithm cyclic block-coordinate algorithm guaranteed converge stationary point optimization problem e.g. main difﬁculty implement strategy efﬁciently solve quadratic programs simplex constraints discuss issue next section. much efﬁcient. precisely tricks inspired lasso solver spams toolbox initialize single variable; update iteration quantity aza)− using woodbury formula; implicitly working matrix without computing updating resutl iteration active-set algorithm computational complexity operations size iteration. like simplex algorithm solving linear programs maximum number iterations active-set algorithm exponential theory even though much smaller practice. approaches active-set algorithm could considered fast iterative shrinkage-thresholding algorithm penalty approach however experiments observed signiﬁcantly better performance active-set algorithm terms speed accuracy. optimization robust archetypal analysis introduced weight data point. typically becomes small outliers reducing importance objective function. denoting weight vector formulation following properties ﬁxing variables vector optimizing respect still obtain quadratic program simplicial constraints; true vectors ﬁxing optimizing respect closed form solution. thus natural block-coordinate descent scheme presented algorithm guaranteed converge stationary point. differences algorithms following time vector updated corresponding weight updated well solution solution actually maxxi then update vectors slightly involved. updating yields following optimization problem study efﬁciency archetypal analysis various applications. implementation coded interfaced python matlab. included toolbox spams number iterations archetypal analysis leads good performance experiments. comparison implementations best knowledge software packages python matrix factorization toolbox open-source library tackles several matrix factorization problems including archetypal analysis. performs alternate minimization scheme αi’s βj’s relies generic solver cvx. package archetypes reference implementation archetypal analysis widely used high-level programming language statistics. note algorithm implemented package deviates original archetypal analysis described originally intended methods matrices rm×n different sizes different numbers archetypes. unfortunately software packages suffer severe limitations able small datasets. report comparison figure computational times measured single core intel xeon report results package smallest dataset since diverged larger ones pymf several orders magnitudes slower implementation. also conducted experiment following optimization scheme pymf replacing solver alternatives mosek quadprog obtained similar conclusions. then study scalability implementation regimes package pymf unusable. report figure computational cost iteration method varying mnist dataset observe empirical complexity approximately linear allowing potentially learn large datasets samples linear thus main limitation approach. however limitation also shared classical sparse coding techniques empirical complexity regarding also greater figure experimental comparison implementations. left value objective function computational time dataset archetypes. method denoted arch. pymf slow appear graph r-package exhibits non-converging behavior. right experiment images mnist size archetypes. package diverged pymf times slower approach. figure scalability study. left computational time iteration varying sample size different numbers archetypes complexity implementation empirically linear right experiment varying ﬁxed sample sizes complexity linear image encoded descriptor invariant small deformations sift then unsupervised learning technique used deﬁning codebook visual patterns called visual words. image ﬁnally described computing histogram word occurrences yielding powerful representation discriminative tasks precisely typical methods learning codebook k-means sparse coding sift descriptors sparsely encoded using formulation image representation obtained maxpooling sparse codes explained spatial pyramid matching also used includes spatial information yielding better accuracy simple bags words many benchmark datasets. ultimately classiﬁcation task performed support vector machine linear kernel. thus natural wonder whether similar performance could achieved using archetypal analysis instead sparse coding. thus conducted image classiﬁcation experiment using software package simply replacing sparse coding component implementation archetypal analysis. many archetypes dictionary elements —that training samples call resulting method archetypal-spm. datasets —that caltech- scenes categorization purpose experiment demonstrate archetypal analysis able learn codebook good sparse coding better k-means. thus results similar methods represented state data sets slightly better nowadays involves different recognition pipeline. report results tables archetypal analysis seems perform well sparse coding. note kmeans-spm-χ uses χ-kernel gaussian kernel k-nearest neighbor classiﬁer even though state mnist achieves less test error results reported table remarkable several reasons method aahyper-parameter performs almost well sparse coding require choosing aa-all sc-all signiﬁcantly outperform k-nn perform similarly non-linear even though simple euclidean norm comparing digits; none methods table exploit fact xi’s fact images unlike sophisticated techniques convolutional neural networks figure neither helpful prediction archetypal analysis useful reducing computational cost test time. choice dictionary size driven trade-off. example usps using archetypes class yields similar results aa-all. table classiﬁcation error rates test mnist usps datasets. aa-all sc-all respectively mean data points used archetypes dictionary elements. uses gaussian kernel. data visualization become important topic especially regarding image databases internet videos focus section public images downloaded flickr website present methodology visualizing content different requests using robust archetypal analysis presented section example present section visualize request paris downloading images uploaded sorted relevance according flickr. ﬁrst compute dense sift descriptors images represent image using table classiﬁcation accuracy caltech- dataset. following experiment images class randomly chosen training rest testing. standard deviation obtained randomized experiments. table classiﬁcation accuracy scene- dataset. following experiment images randomly chosen training rest testing. standard deviation obtained randomized experiments. even though sparse coding unsupervised learning technique used directly classiﬁcation tasks digit recognition observed simple classiﬁcation rules based sparse coding yield impressive results classical datasets mnist usps. suppose learned training data dictionary rm×p every digit class using test digit observed. then classiﬁed ﬁnding class best represents vectors normalized. since want compare archetypal analysis sparse coding thus natural also consider corresponding archetype classiﬁcation rule archetypes learned every digit class. note archetypes made available training data convex dual equivalent nearest convex hull classiﬁer report results training data used archetypes table varying number archetypes class figure include comparison performance fisher vector shown good discriminative power classiﬁcation tasks. then learn archetypes. interestingly observe flickr request large number outliers meaning images tagged paris actually unrelated city. thus choose robust version archetypal analysis order reduce inﬂuence outliers. similar heuristics choosing robust statistics literature resulting data points -normalized. even though archetypes learned space fisher vectors displayable archetype interpreted sparse convex combination data points. figure represent archetypes learned approach; represented training images proportions indicated classical landmarks appear figure surprising since flickr contains large number vacation pictures. figure display several archetypes expect including ones soccer grafﬁtis food ﬂowers social gatherings. figure archetypes seem semantic meaning capture scene composition texture common dataset. present rest archetypes supplementary material results obtained requests london berlin. figure exploit symmetrical relation data archetypes. show four images decompose onto archetypes indicating values αi’s. decompositions trivial others high mean squared error badly represented archetypes others exhibit interesting relations texture architecture archetypes paper present efﬁcient active-set strategy archetypal analysis. providing ﬁrst scalable open-source implementation powerful unsupervised learning technique hope work useful applying archetypal analysis various scientiﬁc problems. particular shown promising applications computer vision performs well sparse coding prediction tasks provides intuitive visualization technique large databases natural images. also propose robust version useful processing datasets containing noise outliers both. work supported inria-uc berkeley associated team hyperion grant franceberkeley fund project gargantua funded program mastodons-cnrs microsoft research-inria joint centre.", "year": 2014}