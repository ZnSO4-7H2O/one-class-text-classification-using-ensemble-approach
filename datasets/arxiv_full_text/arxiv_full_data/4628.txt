{"title": "A parameter-free hedging algorithm", "tag": ["cs.LG", "cs.AI"], "abstract": "We study the problem of decision-theoretic online learning (DTOL). Motivated by practical applications, we focus on DTOL when the number of actions is very large. Previous algorithms for learning in this framework have a tunable learning rate parameter, and a barrier to using online-learning in practical applications is that it is not understood how to set this parameter optimally, particularly when the number of actions is large.  In this paper, we offer a clean solution by proposing a novel and completely parameter-free algorithm for DTOL. We introduce a new notion of regret, which is more natural for applications with a large number of actions. We show that our algorithm achieves good performance with respect to this new notion of regret; in addition, it also achieves performance close to that of the best bounds achieved by previous algorithms with optimally-tuned parameters, according to previous notions of regret.", "text": "study problem decision-theoretic online learning motivated practical applications focus dtol number actions large. previous algorithms learning framework tunable learning rate parameter barrier using online-learning practical applications understood parameter optimally particularly number actions large. paper offer clean solution proposing novel completely parameter-free algorithm dtol. introduce notion regret natural applications large number actions. show algorithm achieves good performance respect notion regret; addition also achieves performance close best bounds achieved previous algorithms optimally-tuned parameters according previous notions regret. paper consider problem decision-theoretic online learning proposed freund schapire dtol variant problem prediction expert advice problem learner must assign probabilities ﬁxed actions sequence rounds. assignment action incurs loss learner incurs loss equal expected loss actions round expectation computed according learner’s current probability assignment. regret action difference learner’s cumulative loss cumulative loss action. goal learner achieve sequence losses regret action lowest cumulative loss dtol general framework captures many learning problems interest. example consider tracking hidden state object continuous state space noisy observations look tracking dtol framework action path state space. loss action time distance observation time state action time goal learner predict path loss close action lowest cumulative loss. popular solution dtol problem hedge algorithm hedge action assigned probability depends cumulative loss action parameter also called learning rate. appropriately setting learning rate function iteration number actions hedge achieve regret upper-bounded iteration number actions. bound regret optimal lower-bound paper motivated practical applications tracking consider dtol regime number actions large. major barrier using online-learning practical problems large understood learning rate suggest figure notion regret. suppose action point line total losses given plot. regret ǫ-quantile difference learner’s total loss total loss worst action indicated interval measure setting ﬁxed function number actions however lead poor performance illustrate example section degradation performance particularly exacerbated grows larger. address simultaneously running multiple copies hedge multiple values learning rate choosing output copy performs best online way. however solution impractical real applications particularly already large. paper take step towards making online learning practical proposing novel completely adaptive algorithm dtol. algorithm called normalhedge. normalhedge simple easy implement round simply involves single line search followed updating weights actions. second issue using online-learning problems tracking large regret best action effective measure performance. problems tracking expects actions close action lowest loss. actions also loss measuring performance respect small group actions perform well extremely reasonable example figure paper address issue introducing notion regret natural practical applications. order cumulative losses actions lowest highest deﬁne regret learner ǫ-quantile difference cumulative loss learner ⌊ǫn⌋-th element sorted list. prove normalhedge regret ǫ-quantile actions achieved hedge optimally-tuned parameters. notice regret bound term involving dependence contrast hedge cannot achieve regret-bound nature uniformly normalhedge works assigning action potential; actions lower cumulative loss algorithm assigned potential regret action adaptive scale parameter adjusted round next depending loss-sequences. actions higher cumulative loss algorithm assigned potential weight assigned action round proportional derivative potential. also interpret hedge potential-based algorithm interpretation potential assigned hedge action proportional exp. potential used hedge differs signiﬁcantly use. although potential-based methods considered context online learning potential function novel best another useful property normalhedge hedge possess assigns zero weight action whose cumulative loss larger cumulative loss algorithm itself. words non-zero weights assigned actions perform better algorithm. applications expect small actions perform signiﬁcantly better actions. regret algorithm guaranteed small means algorithm perform better actions thus assign zero probability. proposed recent solutions dtol regret hedge best action upper bounded function loss best action function variations losses. bounds sharper bounds respect analysis directly yield kinds bounds. therefore leave open question ﬁnding adaptive algorithm dtol regret upper-bounded function depends loss best action. rest paper organized follows. section provide normalhedge. section provide example illustrates suboptimality standard online learning algorithms parameter properly. section discuss related work. section present outlines proof. proof details supplementary materials. consider decision-theoretic framework online learning. setting learner given access actions round learner chooses weight distribution actions action incurs loss learner incurs expected loss distribution addition tracking cumulative regrets action round algorithm also maintains scale parameter chosen average potential actions evaluated remains constant section present example illustrate setting parameters dtol algorithms function total number actions suboptimal. this compare performance normalhedge representative algorithms version hedge polynomial weights algorithm experiments example indicate performance algorithms suffer suboptimal setting parameters; hand normalhedge automatically adapts loss-sequences actions. main feature example effective number actions smaller total number actions notice without prior knowledge actions loss-sequences cannot determine effective number actions advance; result direct method hedge polynomial weights could parameters function example attempts model practical scenario often ﬁnds multiple actions loss-sequences almost identical. example tracking problem groups paths close together state space close loss-sequences. example indicates case performance hedge polynomial weights depend discretization state space however normalhedge comparatively unaffected discretization. example four parameters total number actions; effective number actions number good actions; indicates much better good actions compared rest. finally number rounds. instantaneous losses actions represented matrix loss action round entry matrix. construction matrix follows. first construct matrix based hadamard matrix matrix obtained hadamard matrix deleting constant stacking remaining rows negations repeating rows give losses actions time clear average action better other. therefore large enough losses typical algorithm eventually assign actions weight. except subtracted entry ﬁrst rows e.g. ﬁrst actions perform better losses given remaining large enough typical algorithm eventually recognize assign ﬁrst actions equal weights finally artiﬁcially replicate action times yield ﬁnal loss matrix actions replication actions signiﬁcantly affects behavior algorithms parameters respect number actions inﬂated compared effective number actions normalhedge parameters completely unaffected replication actions. ﬁgures normalhedge completely unaffected replication actions; matter many times actions replicated performance normalhedge stays exactly same. contrast increasing replication factor affects performance poly poly become sensitive changes total losses actions multiple good actions poly slower stabilize weights good actions. poly actually perform better using inﬂated value causes slight advantage single best action magniﬁed. however particular case anomaly; happen even note parameters poly function instead then performance would also depend replication factor therefore degradation performance poly solely suboptimality setting parameters. large amount literature various aspects dtol. hedge algorithm belongs general family algorithms called exponential weights algorithms; originally based littlestone warmuth’s weighted majority algorithm well-studied. standard measure regret works regret best action. original hedge algorithm regret bound hedge uses ﬁxed learning rate iterations requires function total number iterations result regret bound also holds ﬁxed algorithm guarantees regret bound best action uniformly using doubling trick. time-varying learning rates exponential weights algorithms considered there show ln/t using exponential weights round guarantees regret bounds bound provides better regret best action however method still susceptible poor performance illustrated example section moreover consider notion regret. though explicitly considered previous works exponential weights algorithms partly analyzed respect regret ǫ-quantile. ﬁxed hedge modiﬁed setting function regret ǫ-quantile bound hold uniformly ensure bound uniformly copies hedge learning rate function different value ﬁnal master copy hedge algorithm looks probabilities given subordinate copiesto give ﬁnal probabilities. however procedure adds additive factor regret quantile actions importantly procedure also impractical real applications might already working large actions. contrast solution normalhedge clean simple guarantee regret bound values uniformly without extra overhead. recent work provide algorithms signiﬁcantly improved bounds total loss best action small total variation losses small. bounds explicitly depend thus often sharper ones stress however methods different notion regret learning rates depend explicitly besides exponential weights another important class online learning algorithms polynomial weights algorithms studied algorithms require parameter; parameter depend number rounds depends crucially number actions weight assigned action round proportional +)p− polynomial weights share feature zero weight given actions performing worse algorithm although degree weight sparsity tied performance algorithm. finally derive time-adaptive variation follow-the- leader algorithm scaling perturbations parameter depends value theorem appears artifact analysis; divide sequence rounds phases length ﬁrst controlled value bound behavior algorithm phase separately. following corollary illustrates performance algorithm large values case effect ﬁrst phase essentially goes away. main idea behind proof lemma follows. first lemma show monotonic expression ratio derivatives double derivatives potential function next lemma corollary bound numerator denominator ratio. combining bounds gives proof lemma xi∈ett+ summation right-hand side non-negative summation left-hand side non-negative well. shows ﬁrst part lemma. second part follows re-arranging applying upper bound then finally ready prove lemma proof lemma start upper bound obtained lemma upper bound bound lemma ﬁner bound quantity proof lemma divide actions sets", "year": 2009}