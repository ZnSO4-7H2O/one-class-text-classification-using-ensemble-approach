{"title": "A Meta-Analysis of the Anomaly Detection Problem", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "This article provides a thorough meta-analysis of the anomaly detection problem. To accomplish this we first identify approaches to benchmarking anomaly detection algorithms across the literature and produce a large corpus of anomaly detection benchmarks that vary in their construction across several dimensions we deem important to real-world applications: (a) point difficulty, (b) relative frequency of anomalies, (c) clusteredness of anomalies, and (d) relevance of features. We apply a representative set of anomaly detection algorithms to this corpus, yielding a very large collection of experimental results. We analyze these results to understand many phenomena observed in previous work. First we observe the effects of experimental design on experimental results. Second, results are evaluated with two metrics, ROC Area Under the Curve and Average Precision. We employ statistical hypothesis testing to demonstrate the value (or lack thereof) of our benchmarks. We then offer several approaches to summarizing our experimental results, drawing several conclusions about the impact of our methodology as well as the strengths and weaknesses of some algorithms. Last, we compare results against a trivial solution as an alternate means of normalizing the reported performance of algorithms. The intended contributions of this article are many; in addition to providing a large publicly-available corpus of anomaly detection benchmarks, we provide an ontology for describing anomaly detection contexts, a methodology for controlling various aspects of benchmark creation, guidelines for future experimental design and a discussion of the many potential pitfalls of trying to measure success in this field.", "text": "andrew emmott oregon state university shubhomoy oregon state university thomas dietterich oregon state university alan fern oregon state university weng-keen wong oregon state university article provides thorough meta-analysis anomaly detection problem. accomplish ﬁrst identify approaches benchmarking anomaly detection algorithms across literature produce large corpus anomaly detection benchmarks vary construction across several dimensions deem important real-world applications point difﬁculty relative frequency anomalies clusteredness anomalies relevance features. apply representative anomaly detection algorithms corpus yielding large collection experimental results. analyze results understand many phenomena observed previous work. first observe effects experimental design experimental results. second results evaluated metrics area curve average precision. employ statistical hypothesis testing demonstrate value benchmarks. offer several approaches summarizing experimental results drawing several conclusions impact methodology well strengths weaknesses algorithms. last compare results trivial solution alternate means normalizing reported performance algorithms. intended contributions article many; addition providing large publicly-available corpus anomaly detection benchmarks provide ontology describing anomaly detection contexts methodology controlling various aspects benchmark creation guidelines future experimental design discussion many potential pitfalls trying measure success ﬁeld. research supported part defense advanced research projects agency contract wnf--c- part future life institute fli-rfp-ai program grant number opinions ﬁndings conclusions recommendations expressed material authors necessarily reﬂect views future life institute darpa army research ofﬁce government. introduction anomaly detection important inference task applications across many different domains including identifying novel threats computer security discovering novel astronomical phenomena detecting broken environmental sensors identifying machine component failures ﬁnding cancer cells normal tissue despite importance task ﬁeld statistical anomaly detection lacks standard methodology understanding evaluating proposed algorithms. published experiments evaluate algorithms application-speciﬁc case studies synthetic datasets. realistic publicly-available benchmark datasets. consequences this. first difﬁcult compare different algorithms assess progress ﬁeld. second difﬁcult understand various factors dimensions anomaly detection problems inﬂuence performance anomaly detection algorithms. makes difﬁcult experiments guide research algorithm development. building previous work provide thorough metaanalysis anomaly detection problem. study apply hope representative anomaly detection algorithms able provide robust comparison algorithms other article intended survey algorithms even deﬁnitive comparison algorithms herein. instead highlight common pitfalls associated evaluating success ﬁeld difﬁculty measuring progress ﬁeld. previous work develop test standardized evaluation methodology statistical anomaly detection. several important differences methodology evaluation results familiar previous work description differences appendix rather assume familiarity previous work body article describe methodology full. remainder article organized follows. ﬁrst reﬁne deﬁnition anomaly detection problem make clear domains considering study. next review assess existing approaches evaluation anomaly detection methods. based this section identify requirements experimental methodology identify four problem dimensions believe relevant anomaly detection applications. section present benchmarking methodology provide detailed procedures meeting requirements controlling problem dimensions. section present ﬁeld anomaly detection algorithms used study parameterization details noted appendix section identify evaluation metrics describe statistical hypothesis tests apply algorithm’s result benchmark. summary hypothesis tests discussion results follows section. section present straightforward ﬁndings comparing impact many measurable quantities respective control groups. later section offer several alternate views results intended illuminate certain properties algorithms well highlight difﬁculty comparing algorithms ﬁeld. section provide global discussion ﬁndings presented paper provide recommendations ﬁndings impact future work ﬁeld. task anomaly detection study following unsupervised anomaly detection setting. given collection data points d-dimensional real-valued vector. data points mixture nominal points anomalous points. however none points labeled. goal identify anomalous points. anomaly detection algorithm takes input data points produces output real-valued anomaly score point points higher scores believed anomalous. natural metrics quality anomaly predictions area curve average precision applications interested highest-ranked points case natural metrics precision recall applications might choose threshold classify points whose anomaly score exceeds threshold anomalies points nominal. settings common metrics precision recall accuracy error rate typically useful applications anomalies constitute small fraction data paper consider metrics. confronted anomaly detection problem main algorithmic approaches. model normal anomalous points. succeed good understanding kinds data points. however nature anomaly detection problems usually lack good understanding process generating anomalous points. computer security adversaries constantly changing attack strategies fact invalidates supervised learning studies based challenge astronomy goal discover surprising unexpected phenomena. machine failure seems murphy’s machines always ways fail. cancer difﬁcult model single disease rather whole constellation different mechanisms instead modeling processes generating anomalies second approach assume problem unsupervised detect anomalies relying statistical signals. speciﬁcally look statistical outliers hope outliers anomalies. refer assumption outliers-as-targets assumption. predominant assumption literature. nicely-articulated hawkins methodological perspective means constructing benchmark datasets anomaly detection sample probability distribution heavy tails. instead generate data combining different generating processes assess relationship outlier anomaly. existing experimental methodology anomaly detection research three kinds data employed analyze evaluate anomaly detection algorithms. first datasets drawn speciﬁc application problems second synthetic datasets third datasets constructed application-speciﬁc datasets useful. help understand evaluate algorithm reﬁnements needed achieve high performance particular application. however often datasets publicly available privacy security considerations synthetic datasets permit systematic manipulation properties however decades experience machine learning shown real data sets much complex idiosyncratic synthetic data undermines validity approach finally repurposing supervised classiﬁcation data sets desirable property different classes result different generating processes data retain idiosyncrasies real application however studies treated datasets without trying manipulate properties data. exception work scott subsampled anomaly class reduce relative frequency anomalies. another interesting case work generated anomalies permuting features among small subset data. cases supervised regression datasets repurposed treating data points extreme values anomalies propose combine idea repurposing supervised learning datasets idea systematically varying properties data. paper show take existing supervised learning datasets manipulate relative frequency anomalies degree difﬁculty individual anomaly points degree anomalies clustered degree features relevant versus irrelevant task. describing techniques ﬁrst collect list requirements anomaly detection benchmarks satisfy. requirements anomaly detection experiments discussed above although anomaly detection algorithms work searching statistical outliers goal identify points generated process distinct process generating normal points. distinction leads ﬁrst requirements benchmark datasets. requirement normal data points drawn real-world generating process. requirement anomalous data points also drawn realworld process distinct process generating normal points. anomalous points points tails normal distribution. example glasser lindauer’s synthetic anomaly generator requirement many benchmark datasets needed. employ small number datasets risk developing algorithms work problems. important presenting results many benchmarks time makes robust reliable reported results. corpus benchmarks presented study well excess needed reliable experiment section illustrate potential consequences using benchmarks. requirement benchmark datasets characterized terms well-deﬁned meaningful problem dimensions. applications anomaly detection often face different challenges across domains. experiments literature sometimes describe challenges propose strategies addressing them practical value real-world applications experiments acknowledge domain-speciﬁc challenges might addressing. currently established problem dimensions anomaly detection. identiﬁed four dimensions believe important consider beginning conversation. introduce concepts here; attempt measure problem dimension explained section point difﬁculty outliers-as-targets assumption breaks target points become harder distinguish normal points. aspect applying anomaly detection adversarial settings adversaries blend distribution normal points. propose point difﬁculty measure similarity anomalous data points normal ones. targets conﬁned extreme outliers extreme outliers anomalies anomalies interest confused normal points uninteresting outliers. phenomenon also referred swamping semantic variation common aspect many anomaly detection applications multiple processes generating anomalies. cyber-security setting many different kinds attacks many different methods stealing information. cancer detection many different biological processes result cancerous cells. hand many instances anomalies generating process cease appear statistical outliers all; anomalies often described clustered anomalies. propose semantic variation measure degree anomalies generated underlying process alternatively degree anomalies dissimilar other. anomaly points tightly clustered creates region high probability density defeat density estimation-based methods. phenomenon also called masking relative frequency fraction incoming data points anomalies interest. value problem dimension reliably reported literature already also called plurality contamination rate. little done examine impact results however. behavior anomaly detection algorithms often changes relative frequency. anomalies rare methods pretend training points normal model well. anomalies common methods attempt model anomalies well. experiments literature anomalies relative frequency high many security applications estimated relative frequencies range believe understanding impact relative frequency fundamental issue anomaly detection much data contaminated anomalies anomalies longer reliably detected? feature relevance/irrelevance application perspective natural tendency include feature could conceivably informative tendency also increases risk including features irrelevant task. good knowledge features germane task would obviate need anomaly detector all; know features describe targets able solve problem either trivial solution established supervised method. well-established irrelevant features degrade performance supervised learning methods many good algorithms identifying removing irrelevant features. believe irrelevant features even greater problem anomaly detection. statistical perspective irrelevant feature increases dimensionality space sample size required density estimation methods tends scale exponentially dimension. addition dimensionality data increases surface area volume containing data also increases geometric saying tails data lie. increases risk normal points fall tails distribution. benchmarking methodology goals study evaluate impact many factors experimental results including limited problem dimensions proposed section acknowledge corpus benchmarks generated study excess required good experimental design. important making strong claims effects experimental design experimental results requires control benchmark creation ways won’t always produce good experiment; guidelines regard given section select points given motherset construct benchmark. attempt vary control measure four proposed problem dimensions benchmark creation proceeds. task classiﬁcation regression. time series. instances least upper limit. features lower limit. values numeric only. categorical features ignored present. missing values criteria cover settings anomaly detection appropriate. instead focused common case high-dimensional continuous-valued independent identically distributed data. future work explore nominal ordinal features well structured settings time series network data criteria yielded collection datasets refer mothersets since produce thousands child benchmark datasets. selected mothersets following binary classiﬁcation magic gamma telescope miniboone particle identiﬁcation multi-class classiﬁcation steel plates faults sensor array drift image segmentation landsat satellite letter recognition optical recognition handwritten digits page blocks shuttle waveform yeast wine year prediction communities crime exception rule missing values. case features values missing majority points simply removed features data set. motherset control. study seeks validate many claims experimental design interest perform statistical hypothesis testing appropriate well provide clear baseline comparison ﬁnal results. well beyond scope study characterize mothersets ways might explain impact experimental results purview test claim selection motherset measurable impact all. also interest provide evidence claim benchmarks constructed synthetic data different caliber derived real world data. created synthetic motherset control group deﬁning normal versus anomalous data points central goal methodology normal anomalous points produced semantically distinct processes achieve this relabel point mother sets either candidate normal candidate anomaly informed semantics original set. employed following methods. binary classiﬁcation problems. datasets already binary classiﬁcation problems data already partitioned semantically-distinct groups. chose class candidate normal candidate anomaly class fewer instances chosen candidate anomaly class. ﬁnal benchmarks subsample candidate anomalies anomalies constitute small fraction data points. larger majority class easier achieve. case classes equal size class greater variance deﬁned candidate anomaly class; greater variance might give additional ﬂexibility selecting loosely tightly clustered anomalies benchmark construction time. data classes thresholding value. extent versus high values response correspond different generative processes create semantic distinction candidate normal candidate anomalous data points. expect points near median response high point difﬁculty points near extremes point difﬁculty. benchmarks derived regression problem sets allow ﬂexible control point difﬁculty. multi-class problems. multi-class datasets partitioned available classes sets goal maximizing difﬁculty telling apart. mother sets many classes impractical every partition classes search confusing binary problem employ approximation attempts maximize class confusion; method used detailed appendix controlling problem dimensions generate benchmark datasets next step methodology generate benchmark datasets motherset manipulating four dimensions point difﬁculty relative frequency clusteredness feature irrelevance section deﬁne continuous measure dimensions. deﬁne levels measure used construction. benchmark dataset corresponds choosing level factor mother iterate combination problem dimension levels construct benchmark datasets settings. describe below limitations mother datasets mean cannot always achieve datasets every desired setting. benchmarks constructed sampling points motherset time observing feasibility constraints selecting point violate desired problem dimension settings. constraints managed dimension detailed below. particular problem dimension creating control group dimension simply matter enforcing corresponding constraint. additionally benchmarks observe size constraints well. ensure variability among candidate normals used benchmark hard available candidate normals benchmark. also global points used single benchmark value chosen part accommodate algorithms scale well large data sets. point difﬁculty. propose deﬁne point difﬁculty based oracle knows true generating processes underlying normal anomalous points. using knowledge oracle estimate probability data point generated normal distribution data point generated anomalous distribution. consider point difﬁculty point estimated probability belongs class. point difﬁculty level entire benchmark summarized mean point difﬁculty points benchmark. arrive probability estimates applied kernel logistic regression oracle. speciﬁcally implemented algorithm described keerthi parameters chosen -fold cross-validation. large mothersets models underwent approximation subsampling points used deﬁne kernel space. relative frequency. setting easily controlled; desire benchmark contaminated anomalies simply ensure benchmark draws points candidate normals points candidate anomalies. deﬁned following relative frequency levels control group; benchmark drawn candidate anomalies. benchmark drawn candidate anomalies. benchmark drawn candidate anomalies. benchmark drawn candidate anomalies. benchmark drawn candidate anomalies. points benchmarks selected time regardless ground truth label. relative frequency setting used determine limit number candidate anomalies candidate normals sampled; class used selecting points ground truth setting longer feasible. construction possible feasible points limits reached. happens previously selected points removed benchmark appropriate relative frequency achieved. worth noting control group setting consider relative frequency often results benchmarks unrealistically large portion anomalies. however cannot make strong claims impact manipulating relative frequency don’t also demonstrate impact manipulating would prefer greater control measure maintaining feasibility problem dimensions sometimes odds trying maximize minimize quantity deﬁned three normalized clusteredness levels construction time control group; selecting point points would change current measure normalized clusteredness wrong side considered feasible. however would like greater variety among ﬁnal clusteredness measure selection points gives greater weight points would push measure zero. feature irrelevance. terms discussed earlier unknown relevant features motherset task detecting targeted outliers assume original data offers compact features purpose. feasibility constraint dimension. instead control measure feature irrelevance adding irrelevant features otherwise ﬁnished benchmark desired level feature irrelevance introduced. quantify amount feature irrelevance follows. first measure average distance pairs points benchmark dataset prior adding irrelevant features. irrelevant features average pairwise distance increased desired ratio. deﬁne four levels feature irrelevance control group; average distance ratio average distance ratio average distance ratio average distance ratio create irrelevant feature choose feature original motherset uniformly random. data point benchmark dataset choose value feature sampling uniformly values original data points. result feature marginal distribution original feature values carry information anomaly status data points. preserves idiosyncrasies real data allowing introduce noise. simplify process determining many irrelevant features needed compute estimate many extra features achieve desired average distance ratio. note expected distance vectors whose coordinates drawn random grows proportion dimensionality data. hence dataset already dimensions want estimate number dimensions needed increase average pairwise distance factor need anomaly detection algorithms part study applied eight anomaly detection algorithms entire corpus benchmarks. selection algorithms exhaustive believe good representation ﬁeld seeks cover classic state-of-the-art algorithms also trying cover several different solution approaches. section summarizes algorithm selected. speciﬁc implementation paramaterization details refer appendix density-based approaches straightforward generate outlier scores estimate probability distribution data points; less likely point likely anomaly. approaches strongest outliers-as-target assumption valid; features relevant task. robust kernel density estimation approach ﬁtting ﬂexible nonparametric probability density model distributions simple parameters point data build additive model combines them. methods known sensitive outliers complicated methods robust statistics introduced. describe strategies optimizing kernel density estimates applied study. ensemble gaussian mixture model another approach density estimation gaussian mixture model using algorithm. single robust like k-means clustering requires select value gaussian mixture components. improve robustness computed ensemble gmms many values discarded models data well combined predicted densities remaining models. above outlier scores produced based negative log-likelihood point according ﬁnal models. model-based approaches another anomaly detection strategy assume vast majority points normal thus constructing model data produce descriptive decision boundary. one-class one-class algorithm uses support-vector machine search kernel-space decision boundary separates fraction data kernel-space origin. outlier scores produced algorithm determined residual point projected onto decision surface. points outside decision boundary positive residuals interior points negative residuals. support vector data description proposed duin similar concept one-class support vector data description ﬁnds smallest hypersphere kernel space encloses data. above outlier scores produced algorithm determined residual point projected onto decision surface. nearest neighbors-based approaches often called distance-based approaches offer alternative name angle-based outlier detection actually distance computation anomaly score. like however assumes information local character space surrounding point found examining neighbors ways lets euclidian distance function proxy density estimation. local outlier factor well-known local outlier factor algorithm computes outlier score point computing average distance nearest neighbors. normalizes distance computing average distance neighbors nearest neighbors. roughly speaking point believed anomalous signiﬁcantly farther neighbors other. angle-based outlier detection angle-based outlier detection proposed kriegel full form algorithm cubic complexity follows. point consider pairs points compute angle relative point sample variance angles determines outlier score lower variances indicating anomalous points. run-time complexity simple approximations suggested authors. ﬁrst sample data reference computing angles. consider angles among nearest neighbors initial experiments found latter outperform former strategy employed study. projection-based approaches typically isolation forest stood alone among well-established isolation-based techniques instead group newer loda algorithm rely using information random projections data albeit much different ways. isolation forest isolation forest algorithm ranks data points anomalous easily isolated random axisparallel splits. isolation forest isolation trees isolation tree extremely random decision tree decision point determined choosing feature uniformly random choosing splitting threshold uniformly random minimum maximum observed values feature. intuition outliers points easily isolated random remaining data points small average isolation depth likely anomalies assigned high anomaly scores. lightweight online detector anomalies proposed pevny loda generates several weak anomaly detectors producing many random projections data computing density estimation histogram projection. outlier scores produced mean negative log-likelihood according histogram point. evaluation metrics hypothesis tests eight algorithms benchmark datasets results micro-experiments need evaluated. common evaluation metrics literature area curve average precision area precision-recall curve identical curve trapezoidal interpretation true positives. precision-at-k recall-at-k metrics also common risk application speciﬁc feel appropriate broad meta-analysis this note metrics might useful real-world setting. tested many algorithms many conditions exist many results seem poor. want certainty observed differences results random noise. performed hypothesis test micro-experiment output null hypothesis case algorithm’s output random alternative not. speciﬁcally treat random ranking points random variables parameters nanom nnorm compute quantiles interest distributions. conducting test signiﬁcance matter computing -quantile appropriate random variable. refer appendix overview treated random variables. intuitive view process algorithm achieves benchmark reject null hypothesis would mean probability least random ranking would achieve worse benchmark performed hypothesis tests micro-experiment result. failure context refers failing reject null hypothesis. summarize tests ﬁrst deﬁne notion benchmark failure benchmark instance algorithms failed. benchmark failure dependent algorithms used study still believe good indication benchmark evidence later conclusions. results hypothesis tests benchmark construction factors table summarizes global benchmark failure rates either column indicates benchmarks algorithms failed least metrics. synthetic abalone comm.and.crime concrete fault imgseg landsat letter.rec magic.gamma opt.digits pageb particle shuttle skin spambase wave wine yearp yeast appropriate signiﬁcance level study debatable. smaller trades away potential evidence greater conﬁdence results benchmarks consideration relevant. choose apply stringent threshold even though failure rate level rather high still leaves many benchmarks across factors interest. tables show benchmark failure rates across various benchmark construction criterion. boldface values indicate failure rate greater global average metric suggesting factor unsuitable benchmark construction least used caution. results hypothesis tests algorithm simple comparison algorithms table viii shows failure rate algorithm across entire corpus lowest failure rate metric boldfaced. defer drawing conclusions failure rates section point details now. motherset particular yeast failure rate approaching strong indication poor choice benchmark construction. also failure rates across relative frequency levels need interpreted carefully threshold rejecting null hypothesis impacted relative frequency. results summary examine factors contributing micro-experiment results closely. metric eliminate results failed benchmarks retain individual algorithm failures; long benchmark least success evidence failing algorithm could distinguished random behavior failure relevant. transformation metrics later section analyze micro-experiment results various linear models summarizing means metrics like constrained range problematic especially case constant expectation. remainder paper transform metrics extend range real numbers following ways auc– logit transform constant expectation normalize compute lift ratio expectation. commonly assumed expectation equivalent anomaly rate shows good approximation exactly correct. important approximation relative frequency case benchmarks. compute lift using exact expectation. ratio real numbers sensible transformation take lift impact benchmark construction criterion demonstrate impact benchmark construction methodology microexperiment results ﬁrst want examine results agnostic choice algorithm. examining results benchmarks least algorithm produced statistically signiﬁcant output know consider best result benchmark always choosing result better random high conﬁdence. benchmark construction factor well deﬁned control group compute mean difference performance-of-bestalgorithm level control group place conﬁdence interval around difference. results displayed figs. observe metrics logit meant compared other shown side side compact presentation. note fig. mothersets negative impact relative synthetic control group. course synthetic control group design synthetic control could produced several ways might generate challenging benchmarks general trend indicate using real data notably challenging synthetic problems. however yeast motherset statistically distinguish control group mostly disregarded nearly yeast benchmarks discarded motherset’s large failure rate; already less disqualiﬁed yeast consideration anyway. figs. show trend algorithm performance decreasing problem dimension’s relevant quantity increased. note fig. however easiest point difﬁculty setting distinguishable control setting. fig. suggest scattered anomalies easier clustered anomalies neither setting distinguishable control setting. discrete factor choice clustering algorithm adequate measure impact clusteredness algorithm performance show later clusteredness general statistically signiﬁcant impact. simple comparison algorithms again simple comparison algorithms table shows mean performance algorithm transformed metrics. best result metric bolded. analyzing results linear regression models discrete benchmark construction factors provide granular view overresults like include results algorithms best algorithm benchmark. constructed several ﬁxed mixed effect models explain logit across micro-experiment results. choice algorithm motherset constrained discrete factors four problem dimensions described numeric values; especially useful case clusteredness construction criterion capture clustered scattered anomalies are. already described measures clusteredness feature irrelevance section continuous; clusteredness apply transform feature irrelevance ratio. relative frequency point difﬁculty range apply logit transform them. gives real valued representations problem dimension suitable linear model. first build model metric using discrete construction factors real valued transformations compare models. metric comparison goodness-of-ﬁt measure inversely related mean squared error model ﬁgure merit models trying optimize. anova test models provides t-test variable model -test model itself. save space report individual values; results easily summarized every test p-value well table shows results ﬁrst four models. difference using construction factors real-valued problem dimensions large real-valued variables provide better computationally less expensive table shows coefﬁcients problem dimension base model. note negative means tends make benchmark difﬁcult. however scale coefﬁcients comparable original variables comparable scales. better evaluate importance variable predicting metrics construct simpler models exclude variables model measure difference measures relative base model. also construct model without four problem dimension variables measure impact problem dimensions aggregate. table shows results. boldfaced items greater loss algorithm variable suggests variable important ﬁnal outcome choice algorithm suggests important variable predicting results choice motherset important even four problem dimensions aggregate second largest impact. relative frequency alone third largest impact. choice algorithm important three remaining problem dimensions individually surprising list. best models respectable measures leave variance unexplained. constructed several mixed effect models could better explain results data. best models kept problem dimensions ﬁxed effects treated choice algorithm motherset random effect groups. member random effect group models interaction ﬁxed effects. additionally choice motherset also models interaction choice algorithm. models yield measures strong indication variables considering adequate explain micro-experiment outcomes. mixed effects models also provide coefﬁcients measuring effect problem dimension algorithm. similar figs. figs. show coefﬁcients across problem dimension. notably positive coefﬁcients svdd ocsvm algorithms. given poorest performing algorithms overall urge caution drawing conclusions this. further observe coefﬁcients part complex model tell whole story; offer insight strengths weaknesses algorithm. comparison algorithms isolation forest performing algorithm. fig. would suggest relative competitive algorithms isolation forest negatively affected introduction irrelevant features. contrast author’s claims isolation forest claim algorithm might fail larger feature space small subset features germane task. true least relative algorithms study show recreate table tables xiii considering benchmarks constructed feature irrelevance levels respectively. table xiii shows enormous improvement performance egmm rkde also abod. algorithms negatively affected irrelevant features according fig. contrast table considering benchmarks large portion irrelevant features isolation forest broadens lead outperforming algorithms even larger margin. novel observation itself also worth noting included irrelevant features study overall comparison algorithms would different. similarly table shows mean performance algorithm considering benchmarks highly clustered anomalies; abod clear winner comparison. also note nearest-neighbor method surpasses algorithms too. algorithms account local density possible explanation tangible advantage working clustered anomalies. also suggests benchmark creation strategy large impact ﬁnal comparison algorithms. latter point important recognize; imagine interest demonstrating superiority particular algorithm. keeping methodology otherwise intact could motherset selection affect ﬁnal outcome. table shows mean performance algorithm using mothersets abaloneparticlewine yearp. loda rkde beneﬁt selection most reporting evaluation metric would allow declare whichever algorithm preferred winner. don’t suspect dishonest behavior abounds literature table provides evidence assertion selection motherset choice evaluation metric great importance; published work anomaly detection address factors might impact reported results. trivial solution eliminating benchmarks based hypothesis testing attempted account benchmarks difﬁcult otherwise unrealistic composition additional concern benchmarks might trivially easy. often case anomaly detection literature reported results highly accurate whereas mean accuracy corpus benchmarks much lower. particular consider work presented authors share parameterization algorithm benchmark praise algorithm inne sometimes performing well parameter however understanding inne algorithm implication parameter choice reveal algorithm little approximating distance point mean data. acknowledge particular work smaller workshop publication serve warning benchmarks algorithms perform well might benchmarks trivially solved inclusion reported results might helpful. investigate phenomenon further trivial algorithm corpus benchmarks. algorithm simply computes arithmetic mean data assigns outlier score point based euclidean distance mean. performance algorithm normalize performance non-trivial algorithms. compute ratio given algorithm’s performance performance trivial algorithm. logit take natural ratios. case similar previous transformed metric except instead normalizing random expectation normalizing performance trivial solution. metric achieving perfect score benchmark also perfectly almost-perfectly solved trivial solution given much merit lower score signiﬁcant improvement trivial performance given much credit. table xvii shows mean performance algorithm metrics. isolation forest best overall. interestingly classic algorithm outperforms others context. scores demonstrate marginal improvement trivial solution. conclusions recommendations given evidence provided feel conﬁdent making several conclusions recommendations. acknowledge many conclusions possible explanation. section reveals uncommon benchmarks created methods common literature quality many algorithms cannot distinguish random ranking statistical hypothesis test. section observe many experiments literature danger easy hard believe many reported ﬁndings statistically insigniﬁcant feel poor form address matter. statistical hypothesis tests performed recommend creating benchmarks high point difﬁculty urge caution selecting data sets create benchmarks from. based particularly high failure rates reported table recommend using yearpyeast letter.rec datasets future experiments would beneﬁcial validate commonly used datasets. generally table shows benchmarks created binary mothersets lower failure rate suggesting beginning binary classiﬁcation dataset might preferable future work likely produce statistically signiﬁcant results. also recommend uncontrolled relative frequency table provide evidence support this. conﬁdence intervals used hypothesis tests functions relative frequency failure rates context informative. however fact failure rates uncontrolled level lower average provide evidence anomaly detection algorithms general capable picking amount signal long imbalance classes. reinforces results published results positive even reported anomaly rate high. benchmarks relative frequency high failure rate fig. suggests remainder benchmarks also relatively easy compared benchmarks. explained intuitively obvious benchmarks anomalies going much higher variance results meaning outstanding successes abject failures. based fig. recommend using synthetic datasets general further table suggests biggest factor impacting experimental results selection motherset. table demonstrates possible quietly tailor experiment favor particular algorithm. doubt dishonesty exists literature reported results derived somewhat arbitrary list datasets. possible positive ﬁndings literature lucky selection datasets. best protection simply many sources reasonable otherwise justify selection datasets used experiments. figs. table suggest deﬁned problems dimensions impact experimental results. reported mixed models section suggest choice motherset choice algorithm proposed problem dimensions capable predicting experimental results good accuracy. based able recommend using methodology controlling measuring problem dimensions. encourage work focuses speciﬁc contexts deﬁned problem dimensions especially maps contexts real-world applications. isolation forest performed best average good runtime properties recommend general use. however also recommend context impact choice algorithm. based fig. tables xiii observe conﬁdent feature space probability density estimates abod outperform isolation forest. large feature spaces unknown quality recommend isolation forest. also observe isolation forest loda scale well large data sets algorithms not. similarly density estimating methods rkde egmm scale well large number features. general svdd oc-svm performed poorly compared algorithms. made best effort parameterize well. appendix details parameterization works based however conclude poor algorithms rather difﬁcult parameterize correctly unable perform competitively algorithms study. however difﬁculty would reason recommend algorithm. among algorithms point difference performance among large observe isolation forest best overall point ﬁeld anomaly detection algorithms less solving benchmarks efﬁcacy. experimental design understanding impact different real world contexts seem importance given evidence study. section provides evidence over-positive results reported literature particularly helpful measuring progress. incremental improvements non-standardized environments demonstrate particular algorithm effective demonstrate particular breakthroughs ﬁeld. opinion average algorithms roughly measuring quantity producing results. worse table xvii might suggest often algorithms marginally better trivial solution. also opinion matter optics commonly reported metric reported results overwhelmingly positive. wellmastered benchmarks allow room improvement lack standard benchmarks enable recognition true breakthroughs ﬁeld. recmennatallah amer markus goldstein slim abdennadher. enhancing one-class support vector machines unsupervised anomaly detection. proceedings sigkdd workshop outlier detection description york doihttp//dx.doi.org/./. bache lichman. machine learning repository. http//archive.ics.uci.edu/ml tharindu bandaragoda ming ting david albrecht tony jason wells. efﬁcient anomaly detection isolation using nearest neighbour ensemble. data mining workshop ieee international conference ieee paulo cortez ant´onio cerdeira fernando almeida telmo matos jos´e reis. modeling wine preferences data mining physicochemical properties. decision support systems kaustav jeff schneider daniel neill. anomaly pattern detection categorical datasets. proceedings sigkdd international conference knowledge discovery data mining. ethan dereszynski thomas dietterich. spatiotemporal models anomaly detection dynamic environmental monitoring campaigns. transactions sensor networks andrew emmott shubhomoy thomas dietterich alan fern weng-keen wong. systematic construction anomaly detection benchmarks real data. corr abs/. http//arxiv.org/abs/. andrew emmott shubhomoy thomas dietterich alan fern weng-keen wong. systematic construction anomaly detection benchmarks real data. proceedings sigkdd workshop outlier detection description. huaming huang kishan mehrotra chilukuri mohan. online anomalous time series detection algorithm univariate data streams. recent trends applied artiﬁcial intelligence. springer deovrat kakde arin chaudhuri seunghyun kong maria jahja hansi jiang jorge silva. peak criterion choosing gaussian kernel bandwidth support vector data description. arxiv preprint arxiv. hans-peter kriegel peer kr¨oger erich schubert arthur zimek. loop local outlier probabilities. proceedings conference information knowledge management york doihttp//dx.doi.org/./. hans-peter kriegel arthur zimek others. angle-based outlier detection high-dimensional data. proceedings sigkdd international conference knowledge discovery data mining. lazarevic levent ertoz vipin kumar aysel ozgur jaideep srivastava. comparative study anomaly detection schemes network intrusion detection. proceedings siam conference data mining. matthew mahoney philip chan. analysis darpa/lincoln laboratory evaluation data network anomaly detection. recent advances intrusion detection. springer john mchugh. testing intrusion detection systems critique darpa intrusion detection system evaluations performed lincoln laboratory. transactions information system security kemal polat seral sahan halife kodaz salih gunes. classiﬁcation method breast cancer diagnosis feature selection artiﬁcial immune recognition system advances natural computation lipo wang chen yewsoon lecture notes computer science vol. springer berlin heidelberg doihttp//dx.doi.org/./ leonid portnoy eleazar eskin stolfo. intrusion detection unlabeled data using clustering. proceedings workshop data mining applied security senator henry alex memory goldberg william young brad rees robert pierce daniel huang matthew reardon jay-yoon danai koutra others. detecting insider threats real corporate database computer usage activity. duin. support vector data description. machine learning wagstaff lanza thompson dietterich gilmore. guiding scientiﬁc discovery explanations using demud. proceedings association advancement artiﬁcial intelligence aaai conference. yingchao xiao huangang wang zhang wenli methods selecting gaussian kernel parameters one-class application fault detection. knowledge-based systems zhang sconyers byington patrick orchard vachtsevanos. anomaly detection robust approach detection unanticipated faults. prognostics health management international conference doihttp//dx.doi.org/./phm.. jiong zhang mohammad zulkernine. anomaly based network intrusion detection unsupervised outlier detection. ieee international conference communications vol. ieee comparison previous work general goals conclusions previously published work similar study many differences benchmark construction evaluation results. identiﬁed problem dimensions relative frequencypoint difﬁculty clusteredness created corpus benchmarks varying properties across several values using mothersets study. added problem dimension feature irrelevance. studies used different battery algorithms current study; believe study uses representative algorithms evaluation. study also adds control group setting problem dimension including addition synthetic motherset. previous studies made convincing argument impact problem dimensions study explicitly demonstrates manipulating problem dimensions statistically signiﬁcant impact cases. previous studies evaluation done linear regression model observing coefﬁcients various factors evidence claims. similar methods applied here compare performance multiple models strengthen claims. study corpus benchmarks also evaluated rigorously. perform statistical hypothesis test algorithm’s output benchmark. benchmarks every algorithm fails differentiate random ranking discarded unsuitable. conversely trivial algorithm introduced help identify benchmarks might trivially easy. synthetic motherset synthetic motherset generated producing candidate normals candidate anomalies different multivariate distributions intention able manipulate problem dimensions ease. candidate normals drawn multivariate gaussian covariance matrix feature drawn standard normal distribution independently others. anomalies drawn uniformly hyper-cube deﬁned range choosing confusing partition classes heuristic procedure begins training random forest solve multi-class classiﬁcation problem. calculate amount confusion pair classes. data point random forest computes estimate predicted probability belongs class construct confusion matrix cell contains whose true class deﬁne graph node class edge weight equal probability data point class confused data point class vice versa. compute maximum weight spanning tree graph identify graph most-confusable relationships pairs classes. two-color tree adjacent nodes color. colors deﬁne classes points. approximately maximizes confusion candidate normal candidate anomaly data points also tends make classes diverse increases semantic variation sets. parameterizing battery algorithms rkde implemented approach described employed gaussian radial basis kernel kernel bandwidth selected jakkula heuristic. optimized hampel loss function additional parameters suggested authors. egmm reduce computational cost ﬁtting improve numerical stability process ﬁrst transformed benchmark dataset principle component analysis. selected principle components retain variance. generate members ensemble varied number clusters trying values value generated gmms training bootstrap replicates data randomly initializing replicate. computed average out-of-bag likelihood value discarded values whose average likelihood less times average likelihood best value purpose discard gmms data well. finally anomaly score computed point computing average surprise average negative probability number ﬁtted gmms density density assigned data point found preliminary experiments worked better using mean probability density ocsvm employed libsvm implementation chang available http//www.csie.ntu.edu.tw/∼cjlin/libsvm/. benchmark employed gaussian radial basis function kernel. selection kernel bandwidth done using method proposed parameter svdd implemented algorithm using simplex method. employed gaussian radial basis function kernel kernel bandwidth selection generate needed statistics tried different kernel bandwidths. employed package rlof available http//cran.open-source-solution.org/web/ packages/rlof/. chose dataset. smallest value would reliably datasets. treating random variables random ranking seen discrete parametric distribution parameters nnorm nanom distribution parametric only possible rankings meaning ﬁnite number possible scores given parameters. cases possible enumerate scores compute much probability mass score carries thus quantiles distributions computed. however larger values becomes computationally inefﬁcient. instead computed quantiles interest empirically. parameters present corpus produced million random ranking samples computed each estimated quantiles interest.", "year": 2015}