{"title": "BilBOWA: Fast Bilingual Distributed Representations without Word  Alignments", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "We introduce BilBOWA (Bilingual Bag-of-Words without Alignments), a simple and computationally-efficient model for learning bilingual distributed representations of words which can scale to large monolingual datasets and does not require word-aligned parallel training data. Instead it trains directly on monolingual data and extracts a bilingual signal from a smaller set of raw-text sentence-aligned data. This is achieved using a novel sampled bag-of-words cross-lingual objective, which is used to regularize two noise-contrastive language models for efficient cross-lingual feature learning. We show that bilingual embeddings learned using the proposed model outperform state-of-the-art methods on a cross-lingual document classification task as well as a lexical translation task on WMT11 data.", "text": "guages interested unsupervised distributed representations words capture important syntactic semantic information languages techniques succesfully applied wide range tasks across many different languages traditionally inducing representations involved training neural network language model slow train. however contemporary word embedding models much faster comparison scale train billions words single desktop machine models words represented learned realvalued feature vectors referred word embeddings trained large amounts text. models property similar embedding vectors learned similar words training. additionally vectors capture rich linguistic relationships male-female relationships verb tenses illustrated figure properties improve generalization embedding vectors used features wordsentence-level prediction tasks. distributed representations also induced different language-pairs serve effective learning linguistic regularities generalize across languages words similar distributional syntactic semantic properties languages represented using similar vectorial representations especially useful transferring limited label information high-resource low-resource languages demonstrated effective document classiﬁcation outperforming strong machine-translation baseline; well namedentity recognition machine translation introduce bilbowa simple computationally-efﬁcient model learning bilingual distributed representations words scale large monolingual datasets require word-aligned parallel training data. instead trains directly monolingual data extracts bilingual signal smaller raw-text sentence-aligned data. achieved using novel sampled bag-of-words cross-lingual objective used regularize noise-contrastive language models efﬁcient cross-lingual feature learning. show bilingual embeddings learned using proposed model outperform state-of-the-art methods cross-lingual document classiﬁcation task well lexical translation task data. text data freely available many languages labeled data e.g. text marked parts-of-speech named-entities expensive mostly available english. although several techniques exist learn hand-crafted features domain another general non-trivial come good features generalize well across tasks even harder across different languages. therefore desirable unsupervised techniques learn useful syntactic semantic features invariant tasks lanfigure monolingual embeddings shown capture syntactic semantic features noun gender verb tense goal crosslingual embeddings capture relationships across languages. since techniques fundamentally data-driven techniques quality learned representations improves size training data improves however discuss detail signiﬁcant drawbacks associated current bilingual embedding methods either slow train exploit parallel training data. former limits large-scale application techniques latter severely limits amount available training data furthermore introduces domain bias learning process since parallel data typically easily available certain narrow domains paper introduces bilbowa simple scalable technique inducing bilingual word embeddings trivial extension multilingual embeddings. model able leverage essentially unlimited amounts monolingual text. furthermore require word-level alignments instead extracts bilingual signal directly limited sample sentence-aligned raw-text parallel data uses align embeddings learned monolingual training data. contributions following introduce novel computationally-efﬁcient sampled cross-lingual objective employed align monolingual embeddings trained online setting. monolingual models scale large-scale training sets thereby avoiding training bias bilbowaloss considers sampled bag-of-words sentencealigned data training step scales extremely well also avoids need estimating experimentally evaluate induced cross-lingual embeddings document-classiﬁcation lexical translation task method outperforms current state-of-the-art methods training time reduced minutes hours compared several days prior approaches; monolingual word embedding algorithms learn useful features words text algorithms trained large datasets able predict words contexts appear. working intuitively understood mapping word learned vector embedded space updating vectors attempt simultaneously minimize distance word’s vector vectors words frequently co-occurs. result optimization process yields rich geometrical encoding distributional properties natural language words similar distributional properties cluster together. general nature features work well several prediction tasks cross-lingual setup goal learn features generalize well across different tasks different languages. goal therefore learn features word similar words language assigned similar embeddings additionally also want similar words across languages similar representations latter property allows learned embeddings features training discriminative classiﬁer predict labels language labelled data directly transfer language much labelled data. optimization perspective several approaches optimize objectives offline alignment simplest approach optimize monolingual objective separately enforce cross-lingual constraints separate disjoint ‘alignment’ step. alignment step consists learning transformation projecting embeddings words onto embeddings translation pairs obtained dictionary. shown viable approach learned linear projection embedding space other. extended simultanteously projected source target language embeddings joint space using canonical correlation analysis. advantage approach fast learn embedding alignments. main drawback approach clear single transformation capture relationships words source target languages improved results translation task seem point contrary furthermore accurate dictionary required language-pair method considers translation word ignores rich multi-sense polysemy natural languages. parallel-only alternatively leverage purely sentence-aligned parallel data train model learn similar representations aligned sentences. approach followed bicvm bilingual auto-encoder advantage approach fast using efﬁcient noise-contrastive training criterion like bicvm. main drawbacks method train limited parallel data expensive obtain necessarily written style register domain features might applied jointly-trained models another approach jointly optimize monolingual objectives cross-lingual objective enforced cross-lingual regularizer this deﬁne cross-lingual regularization term configure schematic proposed bilbowa model architecture inducing bilingual word embeddings. monolingual skipgram models jointly trained enforcing sampled l-loss aligns embeddings translation-pairs assigned similar embeddings languages. formulation captures intuition want learn representations model individual languages well regularizer encourages representations similar words related across languages. conceptually regularizer consists minimizing distance function vector representations learned words domains weighted similar i.e. distance denote learned embedding representations denote embedding learned word words weighted sure words across languages similar embedded nearby other. approach shown useful crucially advantages formulation enables train available monolingual data abundant less biased parallel-only approach since train data resembles data applying learned features disadvantage original model klementiev extremely slow train. training complexity stems authors implemized margin observed score noise scores. formulation scores computed sequences words idea taken step successfully applied bagof-word representations contexts continuous bagof-words skipgram models trained using negative sampling training objective objectives would yield comparable speedup could used architecture. work opted skipgram model trained using negative sampling since shown learn high-quality monolingual features. besides learning words language relate other equally important representations capture words languages relate other enforce using term equation general bilingual setting word similarities expressed matrix encodes translation score word language word other. rest discussion refer english french denote english-speciﬁc parameters using superscript french-speciﬁc parameters superscript. subscript indicates alignments ﬁxed captures relationships words english respect words french indeed also source main challenges formulation namely cross-lingual objective therefore penalizes euclidian distance words embedding spaces proportional alignment frequency. previous work approached step performing wordment monolingual cross-lingual objectives. monolingual objective train standard neural language model complexity output softmax layer grows output vocabulary size. therefore order evaluate model authors reduce output vocabulary frequent words. second reason slow training time cross-lingual objective considers interactions pairs words source target vocabulary training step scales product vocabularies. work address issues individually. bilbowa model discussed primary challenges existing bilingual embedding models computational complexity importantly strong domain bias introduced models train parallel data europarl. bilbowa model designed overcome issues order enable computationally-efﬁcient cross-lingual distributed feature learning large amounts monolingual text. schematic overview model shown figure main aspects first similar leverage advances monolingual feature learning algorithms replacing softmax objective efﬁcient noise-contrastive objective allowing monolingual training updates scale independently vocabulary size. second introduce novel computationallyefﬁcient cross-lingual loss considers sampled bag-of-words sentence-aligned data cross-lingual objective avoids need estimating word alignments moreover computation regularization term reduces words observed sample worst-case possible interactions training step naive case). learning monolingual features term since care language modelling feature learning alternative softmax noise-contrastive approach score valid observed combinations words randomly sampled unlikely combinations words. idea introduced collobert weston optifigure using global word-alignments aligning cross-lingual embeddings costly scales product vocabulary sizes. contrast bilbowa-loss approximates global loss averaging implicit local co-occurrence statistics limited sample parallel sentence-pairs. important note lengths sampled english french parallel sentences need equal importantly furthermore notice uniform alignment model training step word sampled english sentence updated towards words french sentence precompute simply updating english word towards mean french bag-of-words sentence-vector. speciﬁcally implement equation sampling parallel sentence-pair training step training step optimize following sampled approximate cross-lingual objective alignment step prior training learn alignment matrix however performing word alignment requires running giza++ fastalign software training word-alignment models. computationally costly also noisy. would like learn translation correspondences without utilizing word alignments. order that directly exploit parallel training data. main contribution work approximate costly term deﬁned equation terms global word-alignment statistics using cheaply-obtained local word co-occurrence statistics obtained raw-text parallel sentence-pairs main concept illustrated schematically figure discussed detail below. ﬁrst step notice since alignment weights normalized interpret alignment weights distribution write equation expectation distribution english french wordalignment probabilities word observed sentence denotes english french sampled sentencepair drawn parallel corpus. words bilbowa-loss minimizes sampled l-loss mean bag-of-words sentence-vectors parallel corpus. objective degenerate since embeddings would converge trivial solution coupled regularizer monolingual losses works well practice. sampling training sentences parallel document distribution objective efﬁciently approximates equation without actually compute word alignment weights aij. equation approximation equation illustrated figure really interested estimating global word-alignment statistics word-pair i.e. aij. however sampling words sentence-level local alignment statistics skewed words’ unigram frequencies occurrence given sentence since language strong zipﬁan distribution therefore practice equation over-regularizes frequent words. simple solution subsample words parallel sentences proportional unigram probabilities occurrence. words discard word parallel sentences probability depends unigram frequency occurrence. heavily discards frequent words effectively ﬂattens unigram distribution uniform distribution. idea closely related monolingual subsampling employed wordvec models although motivated different reason cross-lingual setting obtain better approximation global word-alignment statistics local sentencelevel co-occurrence statistics practice found useful learn ﬁner-grained crosslingual embeddings frequent words. better illustrate effect training jointly visualized top- frequent words english german using t-sne algorithm. illustrated figure show embeddings trained without subsampling blue embeddings words trained using parallel subsampling. visualization shows withsubsampling frequent words over-regularized cluster near origin. effect largely reduced proposed subsampling scheme. implemented model building popular open-source wordvec toolkit. implementation launches monolingual skipgram model separate thread language well cross-lingual thread. threads access shared embedding parameters asynchronously. training model make online figure joint t-sne visualization frequent english german words trained without parallel subsampling illustrating effect occurs without parallel subsampling frequent words overregularized towards origin. initial implementation synchronized updates threads found simply clipping individual updates thread sufﬁcient ensure training stability considerably improved training speed. monolingual training data freely available pretokenized wikipedia datasets cross-lingual training freely-available europarl corpus unlike approach however need perform word-alignment step ﬁrst. instead implementation trains directly parallel text ﬁles obtained after applying standard preprocessing scripts come data tokenize recase remove empty sentence-pairs. embedding matrices initialized drawing zero mean unit-variance gaussian distribution. skipgram negative sampling objectives require sample noise words training pair unigram distributions shown give good results. training update therefore occurs asynchronously across threads. monolingual threads select context-target -pair language sample noise words according unigram noise distributions. cross-lingual thread samples random pair parallel sentences parallel data. finally thread makes update parameters asynchronously according equation gradients easy section present experiments evaluate utility induced representations. evaluate embeddings cross-lingual document classiﬁcation task tests semantic transfer information across languages well word-level translation task tests ﬁne-grained lexical transfer. exact replication cross-lingual document classiﬁcation setup introduced klementiev evaluate cross-lingual embeddings. cldc task setup follows goal classify documents target language using labelled documents source language. speciﬁcally train averaged perceptron classiﬁer labelled training data source language attempt apply classiﬁer as-is target data documents represented tf-idf-weighted embedding vectors words appear documents. similar klementiev induce cross-lingual embeddings english-german language pair induced representations classify subset english german sections reuters rcv/rcv multilingual corpora pertaining four categories ccat ecat gcat mcat classiﬁcation experiments documents randomly selected rcv/ corpus third used test remainder divided training sets sizes separate held-out validation documents used development models. since setup exactly mirrors klementiev majority class baseline baselines namely glossed stronger baseline results summarized table order make results comparable results methods reported obtained using embedding dimensionality training data. ﬁrst lines english-german europarl data monolingual parallel training data. vocabulary size english german. since method motivated faster version model proposed klementiev note signiﬁcantly improve upon results training minutes versus original days yields total factor speedup. demonstrates bilbowa loss computationally-efﬁcient accurate approximation full cross-lingual objective implemented klementiev next compare method current state-of-the-art bilingual embedding methods. current state-of-the-art task reported using bilingual auto-encoder model hermann report bicvm model. shown model outperforms bicvm tasks outperforms baes german english yield current state-of-the-art result task runtime method also compares favorably methods. note even though bicvm method principle fast faster method reported training time slightly higher since trained iterations data. also evaluated induced cross-lingual embeddings word translation task used mikolov using publicly-available data. task authors extracted frequent words english-spanish data used online google translate service derive dictionaries translating source words target language since method requires translation-pairs training used ﬁrst frequent words learn translation matrix evaluated method remaining words used test set. translate source word ﬁnds nearest neighbours target language embedding space evaluate translation precision fraction target translations within top-k words returned using speciﬁc method. method require translation-pairs training simply test test-pairs. baselines methods described edit distance ranks words based edit-distance. word co-occurrence based distributional similarity word ﬁrst constructs word co-occurrence vector counts words co-occurs within -word window corpus. word-count vectors mapped source target language using dictionary. finally test word word similar vector tartable classiﬁcation accuracy training times proposed bilbowa method compared klementiev bilingual auto-encoders bicvm model exact replica reuters cross-lingual document classiﬁcation task. methods used induce -dimensional embeddings using training data. baseline results klementiev. table results translation task measured word translation accuracy evaluated top- top- words ranked method. cross-lingual embeddings induced distance embedded space used select word translation pairs. indicates improvement absolute precision previous state-of-the-art task results english-spanish translation tasks summarized table induced -dimensional embeddings using english spanish wikipedias europarl parallel data. model improves baselines mikolov al.’s method tasks gives noticeable improvement accuracy prediction. english spanish translation improve absolute word translation accuracy percent. spanish english task improve absolute word translation accuracy percent. indicates model able learn ﬁne-grained translation equivalences monolingual data using raw-text sentence-aligned parallel data despite lack word-level alignments training dictionaries. setting. motivated reason crosslingual setting helping uncover better approximation global alignment statistics observed local sentence-level co-occurrences. despite speedup model still much slower ofﬂine methods like translation matrix multilingual however results translation task suggest bilbowa learn ﬁner-grained cross-lingual relationships methods train much larger monolingual datasets parallel-only methods. goal learn high-quality general purpose bilingual embeddings always beneﬁcial leverage training data hence hybrid model like bilbowa might better choice parallel-only technique like bicvm baes. bilbowa model introduced paper utilizes sampled bag-of-words cross-lingual loss. main source signiﬁcant speedup klementiev model allowing training scale much larger datasets turn yields accurate features. found asynchronous implementation signiﬁcantly speeds training noticeable impact quality learned embeddings parallel subsampling improves accuracy learned features. getting asynchronous training work required clipping updates especially dimensionality embeddings gets larger. parallel subsampling makes training accurate especially frequent words therefore turns important monolingual crosslingual introduce bilbowa computationally-efﬁcient model inducing bilingual distributed word representations directly monolingual text limited amount parallel data without requiring word-alignments dictionaries. bilbowa combines advances training monolingual word embeddings particularly efﬁcient novel sampled cross-lingual objective. result required computations training step scales number words sentences thereby enabling efﬁcient large-scale cross-lingual training. achieve stateof-the-art results english-german cross-lingual document classiﬁcation whilst obtaining three orders magnitude speedup improve upon previous state english-spanish word-translation task. mikolov tomas quoc sutskever ilya. exploiting insimilarities among languages machine translation. ternational conference learning representations mikolov tomas sutskever ilya chen corrado greg dean jeff. distributed representations words phrases compositionality. advances neural information processing systems pennington jeffrey socher richard manning christopher glove global vectors word representation. proceedings empiricial methods natural language processing turian ratinov bengio word representations simple general method semi-supervised learning. proceedings annual meeting association computational linguistics association computational linguistics socher richard daniel manning christopher bilingual word embeddings phrase-based machine translation. proceedings conference empirical methods natural language processing references al-rfou’ rami perozzi bryan skiena steven. polyglot distributed word representations multilingual nlp. proceedings seventeenth conference computational natural language learning soﬁa bulgaria august association computational linguistics. http//www.aclweb.org/anthology/w-. bengio yoshua senecal j-s. adaptive importance sampling accelerate training neural probabilistic language model. neural networks ieee transactions blitzer mcdonald pereira domain adaptation structural correspondence learning. conference empirical methods natural language processing sydney australia chandar sarath lauly stanislas larochelle hugo khapra mitesh ravidran balaraman raykar vikas saha amrita. autoencoder approach learning bilingual word representations. proceedings nips inklementiev alexandre titov ivan bhattarai binod. ducing crosslingual distributed representations words. proceedings international conference computational linguistics bombay india december", "year": 2014}