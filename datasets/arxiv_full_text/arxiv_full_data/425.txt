{"title": "Gradient Descent Learns One-hidden-layer CNN: Don't be Afraid of  Spurious Local Minima", "tag": ["cs.LG", "cs.AI", "cs.CV", "math.OC", "stat.ML"], "abstract": "We consider the problem of learning a one-hidden-layer neural network with non-overlapping convolutional layer and ReLU activation function, i.e., $f(\\mathbf{Z}; \\mathbf{w}, \\mathbf{a}) = \\sum_j a_j\\sigma(\\mathbf{w}^\\top\\mathbf{Z}_j)$, in which both the convolutional weights $\\mathbf{w}$ and the output weights $\\mathbf{a}$ are parameters to be learned. We prove that with Gaussian input $\\mathbf{Z}$, there is a spurious local minimum that is not a global mininum. Surprisingly, in the presence of local minimum, starting from randomly initialized weights, gradient descent with weight normalization can still be proven to recover the true parameters with constant probability (which can be boosted to arbitrarily high accuracy with multiple restarts). We also show that with constant probability, the same procedure could also converge to the spurious local minimum, showing that the local minimum plays a non-trivial role in the dynamics of gradient descent. Furthermore, a quantitative analysis shows that the gradient descent dynamics has two phases: it starts off slow, but converges much faster after several iterations.", "text": "consider problem learning one-hidden-layer neural network non-overlapping convolutional weights output weights parameters learned. prove gaussian input spurious local minimum global mininum. surprisingly presence local minimum starting randomly initialized weights gradient descent weight normalization still proven recover true parameters constant probability also show constant probability procedure could also converge spurious local minimum showing local minimum plays non-trivial role dynamics gradient descent. furthermore quantitative analysis shows gradient descent dynamics phases starts slow converges much faster several iterations. deep convolutional neural networks achieved state-of-the-art performance many applications computer vision natural language processing reinforcement learning applied classic games like despite highly non-convex nature objective function simple ﬁrst-order algorithms like stochastic gradient descent variants often train networks successfully. success simple methods learning convolutional neural networks remains elusive optimization perspective. recently line research assumed input distribution gaussian showed stochastic gradient descent random initialization able train neural relu activation polynomial time. however results assume unknown layer {wj} ﬁxed vector. convolutional neural network unknown non-overlapping ﬁlter unknown output layer. ﬁrst layer ﬁlter applied nonoverlapping parts input passes relu activation function. ﬁnal output inner product output weight vector hidden layer outputs. convergence gradient descent learning convolutional neural network described figure ﬁlter size number non-overlapping patches success case failure case correspond convergence global minimum spurious paper take important step showing randomly initialized gradient descent learns simple non-linear convolutional neural network unknown layers knowledge work ﬁrst kind. formally consider convolutional case shared among diﬀerent hidden nodes. input sample e.g. image. generate patches size rp×k i-th column i-th patch generated known function assume sampled gaussian distribution overlap patches. assumption equivalent entry sampled gaussian distribution following paper mainly focus population loss allows network rescaled without changing function computed network. reported neyshabur desirable scaling-invariant learning algorithm stabilize training process. similar observations appeared matrix factorization literature. example show random initialization gradient descent converges target convolutional neural network probability least exploiting symmetry boost success probability additional deterministic restarts. further perhaps surprisingly prove objective function spurious local minimum constant probability using random initialization scheme gradient descent converge local minimum well. contrast previous works guarantees nonconvex objective functions whose landscape satisﬁes spurious local minima property result highlights conceptually surprising phenomenon core analysis series invariant qualitative characterizations gradient descent dynamics determines convergence global spurious local minimum. analysis emphasizes non-convex optimization problems need carefully characterize trajectory algorithm initialization. believe idea applicable non-convex problems. prediction error drops slowly. that signal becomes stronger gradient descent converges much faster linear rate prediction error drops quickly. paper organized follows. section introduce necessary notations analytical formulas gradient updates algorithm section provide main theorems performances algorithms implications. section give proof sketch main theorem laying technical insights learning one-hidden-layer cnn. conclude list future directions section place detailed proofs appendix. point view learning theory well known training neural network hard worst cases recently shamir showed assumptions target function input distribution needed optimization algorithms used practice succeed. additional assumptions many works tried design algorithms provably learn neural network polynomial time sample complexity however algorithms specially designed certain architectures cannot explain gradient based optimization algorithm works well practice. focusing gradient-based algorithms line research analyzed behavior gradient descent gaussian input distribution. tian showed population gradient descent able true weight vector random initialization one-layer one-neuron model. soltanolkotabi later improved result showing true weights exactly recovered empirical projected gradient descent enough samples linear time. brutzkus globerson showed population gradient descent recovers true weights convolution ﬁlter non-overlapping input polynomial time. zhong later work proved suﬃciently good initialization implemented tensor method gradient descent true weights one-hidden-layer fully connected convolutional neural network. yuan showed recover true weights one-layer resnet model relu activation assumption spectral norm true weights within small constant identity mapping. paper also follows line approach studies behavior gradient descent algorithm gaussian inputs. perturbed gradient descent methods second order information global minimum polynomial time. combined geometric analyses algorithmic results shown large number problems including tensor decomposition dictionary learning phase retrieval matrix sensing matrix completion matrix factorization solved polynomial time local search algorithms. motivates research studying landscape neural networks particular kawaguchi hardt zhou feng nguyen hein feizi showed conditions local minima global. recently showed using modiﬁed objective function satisfying properties above one-hidden-layer neural network learned noisy perturbed gradient descent. however nonlinear activation function number samples larger number nodes every layer usually case deep neural network natural objective functions like still unclear whether strict saddle locals global properties satisﬁed. paper show even one-hidden-layer neural network relu activation exists local minimum. however show randomly initialized local search achieve global minimum constant probability. preliminaries bold-faced letters vectors matrices. denote euclidean norm ﬁnite-dimensional vector. denote standard big-o big-theta notations hiding absolute constants. parameters t-th iteration optimal weights. i-th coordinate transpose i-th denote -dimensional unit sphere paper assume every patch vector gaussian random variables. following theorem gives explicit formula population loss. proof uses basic rotational invariant property polar decomposition gaussian random variables. section details. showed vanilla gradient descent converges minimizers convergence rates guarantees. recently gave exponential time lower bound vanilla gradient descent. paper give polynomial convergence guarantee vanilla gradient descent. remark second layer ﬁxed upon proper scaling formulas population loss gradient equivalent corresponding formulas derived however second layer ﬁxed gradient depends plays important role deciding whether converging global local theorem shows certain conditions initialization gradient descent converges global minimum. convergence phases beginning initial signal small convergence quite slow. iterations signal becomes stronger enter regime faster convergence rate. technical insights provided section small example makes close choose small step size depends inverse need large number iterations enter phase provide following initialization scheme ensures conditions required theorem large enough initial signal. together show randomly initialized gradient descent learns one-hidden-layer convolutional neural network polynomial time. proof ﬁrst part theorem uses symmetry unit sphere ball second part standard application random vector high-dimensional spaces. lemma example. techniques remark gaussian input assumption necessarily true practice although common assumption appeared previous papers also considered plausible result easily generalized rotation invariant distributions. however extending general distributional assumption e.g. structural conditions used remains challenging open problem. descent converge global minimum. perhaps surprisingly next theorem shows conditions underlying truth also pair makes gradient descent converge spurious local minimum. figure diﬀerence global minimum spurious local minimum. section list main ideas proving theorem section give qualitative high level intuition initial conditions suﬃcient gradient descent converge global minimum. section explain gradient descent phases. convergence global optimum relies geometric characterization saddle points series invariants throughout gradient descent dynamics. next lemma gives analysis stationary points. proof checking ﬁrst order condition stationary points using theorem arrive either global optimal point local minimum. recall gradient formula projection matrix onto complement therefore sign inner product plays crucial role dynamics algorithm inner product positive gradient update decrease angle negative angle using weight-normalization angle aﬀect prediction. therefore paper study dynamics following lemma quantitatively characterize shrinkage quantity iteration. paper give ﬁrst polynomial convergence guarantee randomly initialized gradient descent algorithm learning one-hidden-layer convolutional neural network. result reveals interesting phenomenon randomly initialized local search algorithm converge global minimum spurious local minimum events constant probability. give complete quantitative characterization gradient descent dynamics explain two-phase convergence phenomenon. list future directions. analysis focused population loss gaussian input. practice uses gradient descent empirical loss. concentration results useful generalize results empirical version. challenging question extend analysis gradient dynamics beyond rotationally invariant input distributions. proved convergence gradient descent structural input distribution assumptions one-layer convolutional neural network. would interesting bring insights setting. another interesting direction generalize result deeper wider architectures. speciﬁcally open problem conditions randomly initialized gradient descent algorithms learn one-hidden-layer fully connected neural network convolutional neural network multiple kernels. existing results often requires suﬃciently good initialization believe insights paper especially invariance principles section helpful understand behaviors gradient-based algorithms settings.", "year": 2017}