{"title": "Sentiment Analysis by Joint Learning of Word Embeddings and Classifier", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Word embeddings are representations of individual words of a text document in a vector space and they are often use- ful for performing natural language pro- cessing tasks. Current state of the art al- gorithms for learning word embeddings learn vector representations from large corpora of text documents in an unsu- pervised fashion. This paper introduces SWESA (Supervised Word Embeddings for Sentiment Analysis), an algorithm for sentiment analysis via word embeddings. SWESA leverages document label infor- mation to learn vector representations of words from a modest corpus of text doc- uments by solving an optimization prob- lem that minimizes a cost function with respect to both word embeddings as well as classification accuracy. Analysis re- veals that SWESA provides an efficient way of estimating the dimension of the word embeddings that are to be learned. Experiments on several real world data sets show that SWESA has superior per- formance when compared to previously suggested approaches to word embeddings and sentiment analysis tasks.", "text": "word embeddings representations individual words text document vector space often useful performing natural language processing tasks. current state algorithms learning word embeddings learn vector representations large corpora text documents unsupervised fashion. paper introduces swesa algorithm sentiment analysis word embeddings. swesa leverages document label information learn vector representations words modest corpus text documents solving optimization problem minimizes cost function respect word embeddings well classiﬁcation accuracy. analysis reveals swesa provides efﬁcient estimating dimension word embeddings learned. experiments several real world data sets show swesa superior performance compared previously suggested approaches word embeddings sentiment analysis tasks. representing words vector space allows quantiﬁcation relationships among words using distance angle measures. vector representations words useful performing several natural language processing tasks. general idea learning word embeddings estimate underlying probability distribution function word given corpus text documents. probabilistic models learning semantic word embeddings neural network based models current state wordvec algorithm derivatives unsupervised perform well trained billions text documents. results wordvec algorithm show addition capturing precise syntactic semantic information word embeddings obtained algorithms demonstrate linear structure particularly well suited performing analogy tasks. paper focuses sentiment analysis problem domains obtaining large amounts data problematic. typical example data obtained discussion forums part digital health intervention treatments. treatments demonstrated effectiveness substance disorders textual data obtained discussion forums rich sentiments determination pleasure anger fear etc. goal intervention treatment prevent relapse users timely intervention facilitated human moderators machine learning algorithms. though forum moderators monitor provide support participants struggling considerable labor involved reviewing deciding risk level text message. analyzing textual data sentiment efﬁcient algorithms developed predicting relapse. however challenges data amount unlabeled data small number active users modest number posts make on-line forum modest obtaining labels data hard need human moderated expertise judge certain post ‘positive/benign’ implies individual ‘negative/threat’ implying individual vulnerable likely relapse soon. contributions paper fold. first paper introduces supervised word embedding sentiment analysis algorithm iterative algorithm minimizes cost function classiﬁer word embeddings unit norm constraint word vectors. swesa uses document labels learning word embeddings. using document lables overcomes problem small-size training data allows learning meaningful word embeddings. contrast state algorithms like wordvec large amounts training data learn word embeddings unsupervised fashion. second word embeddings learned swesa polarity aware demonstrated extensive experiments standard data sets like imdb yelp amazon example ‘awful/good’ antonym pair returned swesa opposed ‘awful/could’ obtained wordvec. polarity aware word embeddings suitable perform word antonym tasks. addition swesa signiﬁcant improvement state-of-the-art word embeddings used sentiment analysis framework. word vector representations earliest vector representation words vector space models popular example latent semantic indexing works matrix co-occurence counts term frequency-inverse document frequency learn word embeddings. variants involve different measures co-occurence square root word counts logarithms etc. recent state-of-the-art neural network based language models weights neural network internal representation word. neural network models rich initial contributions successful modern incarnations neural network models lead wordvec algorithm uses energy-based techniques glove uses matrix factorization techniques main idea behind wordvec learn vector representations words maximize probability contiguous tuples occurring corpus time minimizing probability random c-tuples. furthermore wordvec paper posits probabilistic model based products word nearby words. model successfully produced efﬁcient word vector embeddings exhibit linear properties desirable applications word analogy tasks. latent variable probabilistic models extensions also used word embeddings. methods learn word embeddings unsupervised fashion. however using labeled data often help learning sentiment-aware word embeddings appropriate corpus hand. word embeddings used sentiment analysis tasks. sentiment analysis work propose probabilistic model captures semantic similarities among words across documents. model leverages document label information improve word vectors better capture sentiment contexts words occur. probabilistic model used similar latent dirichlet allocation document modeled mixture latent topics. word probabilities document modeled directly assuming given topic. supervised neural network based model proposed classify twitter data. proposed algorithm learns sentiment speciﬁc word vectors tweets making emoticons text guide sentiment words used text instead annotated sentiment labels. recursive neural tensor network proposed classiﬁes sentiment text varying length. learn sentiment long text model exploits compositionality text converting input text sentiment treebank format annotated sentiment labels. sentiment treebank based data introduced pang model performs particularly well longer texts exploitnotation throughout paper shall denote word vectors indicates size vocabulary. matrix word vectors rk×v classiﬁer learned represented weights word vectors document contained vector document label document indicated document represented matrix containing weight vectors vector vector containing document labels. given collection documents binary sentiments respectively learn classiﬁer given previously unseen document accurately estimate sentiment document. could class imbalance training data algorithm explicitly account class imbalance. problem approached introducing algorithm called swesa. swesa simultaneously learns word vector embeddings classiﬁer making document polarity/sentiment labels. representation documents within swesa motivated fact short texts like happy polarity sentence hinges words happy. result learning polarity aware word embeddings good vector representations documents achieved. instance example distance vectors would capture dissimilarities sentiment documents time reﬂecting similarities sentence structure. text documents framework represented weighted linear combination words given vocabulary. weights either term frequencies words within document term frequency-inverse document frequency weights provided input swesa experiments described section term frequencies. weighting scheme chosen mimic concept local context used wordvec family algorithms. global cooccurrence information leveraged using tf-idf weighting words documents. approach entirely unheard sentiment analysis tasks word embeddings considered features classiﬁcation algorithm swesa aims vector representations words extension text documents applying nonlinear transformation product results binary label indicating polarity document. mathematically assume that function order solve regularized negative likelihood minimization problem solved. optimization problem solved minimization problem objective function vector vector weights corresponding different words document mentioned previously testing swesa term frequencies different words certain document used regularization parameter classiﬁer cost associated misclassifying document positive class cost associated misclassifying document negative class. following heuristic suggested number positive documents corpus number negative documents corpus. scheme particularly useful dealing data sets imbalanced classes. order solve optimization problem line algorithm projected stochastic gradient descent sufﬁx averaging used. sufﬁx averaging last iterates obtained stochastic gradient descent averaged. sufﬁx averaging guarantees noise iterates reduced shown achieve almost optimal rates convergence minimization strongly convex functions. experiments section different initialization procedures used obtain ﬁrst method uses latent semantic analysis procedure form matrix word vectors given corpus text documents. second method uses wordvec algorithm form word vector matrix corpus. using balanced data sentiment given document captured document label framework binary label capture sentiments ‘positive/negative’ ‘threatening/benign’ depending data set. unit norm constraint optimization problem shown enforced word embeddings discourage degenerate solutions example absence constraint optimal typically vector zeros. note optimization problem bi-convex jointly convex optimization variables. algorithm shows algorithm solve optimization problem algorithm alternating minimization procedure initializes word embedding matrix alternates minimizing objective function w.r.t. weight vector word embeddings logistic regression model optimization problem assumes certain probability model minimizes negative loglikelihood norm constraints. while speciﬁc goal user might dictate appropriate choice probabilistic model large class classiﬁcation tasks sentiment analysis logistic regression model widely used. section assumed probability model interest logistic model. assumption minimization problem step algorithm standard logistic regression problem many specialized solvers devised problem implementation swesa standard off-the-shelf solver available scikit-learn package python used. high level swesa seen variation supervised dictionary learning problem within given labeled data unlabeled part data lies dimensional space goal learn dictionary size sparse encoding w.r.t. dictionary further label generated linear classiﬁer w.r.t i.e. learning problem estimate dictionary codes data point classiﬁer. swesa roughly mapped considering dictionary size column corresponds word embedding. however three main differences between swesa. input labeled dataset data point already represented vector. allows deﬁnition reconstruction error used algorithms designed sdl. contrast swesa labeled unstructured data direct vector representation learn vector representations data. result notion reconstruction error used apply swesa hence optimization formulation used signiﬁcantly different used sparse encoding data point learned whereas swesa sparse encoding considered known proportional number times word appears document. finally classiﬁer high-dimensional vector acts latent codes. problems matrix completion convergence properties alternating minimization studied. current analysis techniques might apply swesa mentioned differences conjecture similar ideas might useful convergence analysis swesa. standard methods like naive bayes one-hot encoding words hence fails capture semantic relationships words. contrast swesa learns word embeddings capture polarity. neural network models learn complicated functions data makes poor algorithmic tool presence limited data. ﬁxed small number. paper suggested spectrum matrix used determine typically required large enough capture intricacies data time small enough avoid ﬁtting. order best effective rank matrix calculated. effective rank matrix deﬁned smallest best rank-k approximation matrix satisﬁes indicates frobenius norm. notion effective rank intuitive meaning energy singular value matrix small relative entire spectrum. demonstrate choices good simple synthetic experiment performed. swesa synthetic data text documents split pairs training testing data sets. polarized vocabulary words built comprising positive negative neutral words. text document assigned negative label least words document negative. similarly text document labeled positive least words document positive. synthetic data unbalanced positive documents rest negative. matrix matrix term frequencies. since data relatively noise free value seen figure effective rank choice value average precision demonstrates fact deﬁnition effective rank provides good mechanism pick good values tensor network rntn proposed learns compositionality form text varying length performs classiﬁcation supervised fashion grained sentiment labels. since swesa aimed binary classiﬁcation rntn also used binary classiﬁcation framework. rntn shown perform better previously proposed recursive auto encoder hence swesa compared rae. two-step baseline introduced test effectiveness unsupervised embedding algorithms like wordvec features document sentiment classiﬁcation two-step performs following steps perform sentiment analysis. learn unigram word embeddings unsupervised fashion obtain document embeddings weighted linear combination. obtained document embeddings learn logistic regression classiﬁer sentiment analysis. swesa compared rntn sentiment-speciﬁc word embeddings competing neural network model developed three main reasons ssew algorithm developed speciﬁcally sentiment analysis twitter data uses emoticons tweets sentiment labels. contrast data sets considered emoticons usually absent. moreover structure language characteristics twitter data unlike datasets interest work making ssew unsuitable rntn algorithm handle texts varying length. contrast ssew limited tweets always less characters long. iii) well developed readily usable code available rntn ssew. swesa tested baselines four data sets balanced unbalanced. data split train-test data pairs. case unbalanced data ratio classes held consistent across training test data pairs. hyperparameter tuned training data cross validation. similarly number iterations swesa convergence determined running experiment training data range values value beyond signiﬁcant change difference consequent values objective function selected. since real data sets noisier compared synthetic data used section selected. average area curve precision scores test data sets reported. precision calculated ratio number true positives number true positives false positives tp+f area curve obtained applying trapezoidal rule calculate area curve. data sets used section tokenized textual characters removed. since data sets small unlike words tokenized data sets retained vocabulary. wordvec trained using hyperparameters similar default values similarly default hyperparameters reported used training rntn. results sentiment analysis task. figures show average precision average scores respectively baselines swesa four data sets three balanced data sets consist reviews food products movies respectively. review labeled ‘positive’ ‘negative’. data sets available download repository chess data consists documents obtained mobile phone based intervention treatment provides services recovery maintenance relapse prediction alcohol addicts unbalanced data number documents suggestive relapse user scores available rntn since possible determine prediction probabilities model. outnumbered users discussing sobriety. message labeled ‘threat’ suggesting relapse risk ‘not threat’ indicating well being. data proprietary study conducted neural network based baselines rntn wordvec based two-step baseline perform weakly opposed swesa baselines. observation consistent behavior neural network based algorithms small data sets. despite trained wikipedia corpus wordvec derived embeddings used two-step fail perform well swesa consistently four data sets achieving maximum precision amazon data set. data different initializations swesa achieve precision scores failure two-step wordvec attributed lack supervision training also disparity training test data. swesa learns meaningful embeddings text opposed methods like naive bayes word frequencies used obtain encodings documents. hence embeddings learned swesa better suited sentiment analysis. seen average precision achieved amazon data opposed average precision achieved swesa initialization. seen ﬁgures behavior consistent across balanced data sets chess data imbalanced classes. highlight qualitative performance swesa cosine similarity document representations swesa two-step evaluated. three reviews obtained two-step swesa similar sample review first reception sucks never bars ever. similar analysis performed holds consistently across data sets available supplemental material. shows swesa propagates document level polarity onto word embeddings helps sentiment analysis. failure pre-trained rntn neural network based rntns work well trained large data sets. work train rntn pang data movie reviews. table shows average precision obtained pre-trained rntn data sets. note difference average precision scores pre-trained rntn swesa considerably small given pretrained rntn trained dataset times size training data swesa. observation best illustrated amazon dataset average precision pre-trained rntn approximately swesa however pre-trained rntn particularly poorly chess data set. know difference language structure vocabulary training test data introduces error pre-trained rntn fails classify messages chess data set. also pre-trained rntn poor accounting class imbalance chess data precision scores messages classiﬁed extremely low. ﬁgure shows average precision scores obtained baselines swesa four data sets. error represents average precision score obtained running algorithms testing sets. sentiment labels. consequence word level polarity preserved vector space. given words ‘good’‘fair’ ‘awful’ antonym pair ‘good/awful’ determined calculating cosine similarity wgood wawf figure shows small sample word embeddings learned amazon data swesa wordvec. cosine similarity dissimilar words calculated owing assumptions word embeddings words depicted points unit circle. ﬁgure evident supervised algorithm like swesa projects document level polarity onto word level embeddings unsupervised algorithm like wordvec learns embeddings words virtue word co-occurrences fail embed polarity. important notice swesa learns word polarities using document polarities word polarities useful antonym tasks. unlike classical antonym tasks examples known antonym pairs provided setup pairs provided swesa able good discovering antonym pairs. example dissimilar word given word ‘excellent’ ‘poor’ learned swesa opposed ‘work’ learned wordvec. thus word antonym pairs obtained calculating cosine similarities. examples illustrate swesa captures sentiment polarity word embedding level despite limited data. paper introduces swesa novel iterative algorithm simultaneously learns polarity aware word embeddings classiﬁer perform sentiment analysis supervised learnfigure ﬁgure depicts word embeddings unit circle. dissimilar word pairs plotted based cosine angle respective word embeddings learned swesa wordvec. framework. swesa overcomes limitations posed small sized data sets neural network based learning algorithms. assumptions structure word embeddings within swesa preserve structural properties desirable embeddings typically obtained neural network based embedding algorithms. future work proposed geometric interpretation semantic relationships word embeddings used mine additional semantic relationships between words concepts data.", "year": 2017}