{"title": "Multimodal Sentiment Analysis with Word-Level Fusion and Reinforcement  Learning", "tag": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "abstract": "With the increasing popularity of video sharing websites such as YouTube and Facebook, multimodal sentiment analysis has received increasing attention from the scientific community. Contrary to previous works in multimodal sentiment analysis which focus on holistic information in speech segments such as bag of words representations and average facial expression intensity, we develop a novel deep architecture for multimodal sentiment analysis that performs modality fusion at the word level. In this paper, we propose the Gated Multimodal Embedding LSTM with Temporal Attention (GME-LSTM(A)) model that is composed of 2 modules. The Gated Multimodal Embedding alleviates the difficulties of fusion when there are noisy modalities. The LSTM with Temporal Attention performs word level fusion at a finer fusion resolution between input modalities and attends to the most important time steps. As a result, the GME-LSTM(A) is able to better model the multimodal structure of speech through time and perform better sentiment comprehension. We demonstrate the effectiveness of this approach on the publicly-available Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis (CMU-MOSI) dataset by achieving state-of-the-art sentiment classification and regression results. Qualitative analysis on our model emphasizes the importance of the Temporal Attention Layer in sentiment prediction because the additional acoustic and visual modalities are noisy. We also demonstrate the effectiveness of the Gated Multimodal Embedding in selectively filtering these noisy modalities out. Our results and analysis open new areas in the study of sentiment analysis in human communication and provide new models for multimodal fusion.", "text": "abstract increasing popularity video sharing websites youtube facebook multimodal sentiment analysis received increasing attention scientific community. contrary previous works multimodal sentiment analysis focus holistic information speech segments words representations average facial expression intensity develop novel deep architecture multimodal sentiment analysis performs modality fusion word level. paper propose gated multimodal embedding lstm temporal attention model composed modules. gated multimodal embedding alleviates difficulties fusion noisy modalities. lstm temporal attention performs word level fusion finer fusion resolution input modalities attends important time steps. result gme-lstm able better model multimodal structure speech time perform better sentiment comprehension. demonstrate effectiveness approach publicly-available multimodal corpus sentiment intensity subjectivity analysis dataset achieving state-ofthe-art sentiment classification regression results. qualitative analysis model emphasizes importance temporal attention layer sentiment prediction additional acoustic visual modalities noisy. also demonstrate effectiveness gated multimodal embedding selectively filtering noisy modalities out. results analysis open areas study sentiment analysis human communication provide models multimodal fusion. permission make digital hard copies part work personal classroom granted without provided copies made distributed profit commercial advantage copies bear notice full citation first page. copyrights components work owned others author must honored. abstracting credit permitted. copy otherwise republish post servers redistribute lists requires prior specific permission and/or fee. request permissions permissionsacm.org. icmi’ november glasgow copyright held owner/author. publication rights licensed association computing machinery. isbn ----//.... https//doi.org/./. concepts computing methodologies artificial intelligence; computer vision; natural language processing; machine learning; information systems sentiment analysis; reference format minghai chen wang paul liang tadas baltrušaitis amir zadeh louis-philippe morency. multimodal sentiment analysis word-level fusion reinforcement learning. proceedings international conference multimodal interaction york pages. https//doi.org/./. introduction multimodal sentiment analysis emerging field intersection natural language processing computer vision speech processing. sentiment analysis aims find attitude speaker writer towards document topic event sentiment expressed spoken words emotional tone delivery accompanying facial expressions. result helpful combine visual language acoustic modalities sentiment prediction combine cues different modalities previous work mainly focused holistic video-level feature fusion. done simple features calculated entire video representations verbal visual acoustic features simplistic fusion approaches mostly ignore structure speech focusing simple statistics videos combining modalities abstract level. cornerstone approach capturing full structure speech using time-dependent recurrent approach able properly perform modality fusion every timestep. understanding speech structure important major reasons local interactions modalities loud word uttered roots language acoustic modalities whether word accompanied smile roots language vision. considering local interaction helps dealing commonly researched problems natural language processing ambiguity sarcasm limited context providing information visual acoustic modalities. consider word crazy; word positive sentiment accompanied smile negative sentiment accompanied frown. time word great accompanied frown implies sarcastic speech. also limited context inference sentiment intensity difficult. example word good accompanied neutral nonverbal behavior could mean utterance positive; word accompanied smile could mean highly positive sentiment. global interactions modalities mostly established based temporal relations modalities. examples include delayed laughter speaker’s utterance words delayed smile speech pause. modalities also intramodal interactions characterized global structure speech. properly model structure speech questions need answered what modality look moment time? what moments speech important communication?. address first question model able gate useful modalities moment time. modality contain useful information modality noisy negatively affects performance model model able shut modality perform inference based information present modalities. address second question model able divert it’s attention moments communication polarized word uttered smile happens. paper introduce gated multimodal embedding lstm temporal attention model explicitly accounts questions using gated mechanism multimodal fusion time step temporal attention layer sentiment prediction. model able explore structure speech stateful recurrent mechanism perform fusion word level different modalities. gives model ability account local global interactions modalities. remainder paper follows. section review related work multimodal sentiment analysis. section give formal definition problems present approaches detail. section describe cmu-mosi dataset experimental methodology baseline models. results cmu-mosi dataset presented section detailed analysis model’s components provided section section concludes paper. related work deep learning approaches became extremely popular past years field multimodal machine learning specifically gotten unprecedented momentum multimodal models used sentiment analysis medical purposes detection suicidal risk ptsd depression emotion recognition image captioning media description question answering multimodal sentiment analysis written text modality wellstudied models predict sentiment language. early works used words n-gram representations derive sentiment individual words. approaches focused opinionated words applied complicated structures trees graphs structures aimed derive sentiment sentence based sentiment individual words compositions. recent works used dependency-based semantic analysis distributional representations sentiment convolutional architecture semantic modeling sentences however primarily working spoken text rather written text gives opportunity integrate additional audio visual modalities. modalities helpful providing additional information sometimes noisy. multimodal sentiment analysis integrates verbal nonverbal behavior predict user sentiment. though various multimodal datasets sentiment labels exist cmu-mosi dataset first dataset opinion level sentiment labels. recent multimodal sentiment analysis approaches focus deep neural networks including convolutional neural networks multiple-kernel learning select-additive learning learns generalizable features across speakers. uses unimodal deep neural network three modalities explores effectiveness early fusion late fusion. features three modalities similar work fuse multimodal information work simply uses concatenation approach video level propose justify advanced methods multimodal fusion. besides sentiment speakers’ attributes persuasion passion confidence could also analyzed similar methods proposes ensemble classification approach combines different perspectives classifying multimodal data first perspective assumes inter-modality conditional independence second perspective explicitly captures correlation modalities recognized clustering based kernel similarity approach. methods also applied multimodal sentiment analysis. compared previous work method major advantages. best knowledge model first word level modality fusion means align word corresponding video frames audio segments. secondly also first propose attention layer input gate controller trained reinforcement learning approach problem noisy modalities. lstm temporal attention second component sentiment prediction model captures temporal interactions multimodal embedding layer. component learns global interactions modalities sentiment prediction. lstm temporal attention gated multimodal embedding passed input lstm time step lstm forget gate used learn global temporal information multimodal input data input forget output gates lstm lstm memory cell lstm output linearly transform respectively lstm gate space operator denotes hadamard product attention model similar selectively combine temporal information input modalities adaptively predicting important time steps towards sentiment video clip. expect relevant information sentiment high attention weights. example person crying laughing information relevant sentiment opinion higher importance neutral word movie. attention mechanism also allows modalities complimentary information. cases language helpful model adaptively focus presence sentiment related non-verbal behaviors facial gestures tone voice. mathematically soft attention layer sequence lstm hidden outputs. obtained multiplying hidden layer time step shared vector passing sequence softmax function. attention units used weight importance time step’s hidden layer final sentiment prediction. suppose represents matrix hidden units lstm final sentiment prediction obtained embedding layer lstm temporal attention model. gated multimodal embedding layer performs selective multimodal fusion time step using input modality gates lstm temporal attention performs sentiment prediction attention time step. together modules combine form gated multimodal embedding lstm temporal attention section ends training details gme-lstm. gated multimodal embedding first component gated multimodal embedding performs multimodal fusion learning local interactions modalities. suppose dataset contains video clips containing opinion mapped sentiment intensity. video clip contains time steps time step corresponds word. video clip also labeled ground truth sentiment value align words corresponding video audio segment using penn phonetics forced aligner software computes alignment speech audio file verbatim text transcript. following alignment word level time step obtain aligned feature vectors language acoustic visual modalities respectively. problem previous models noisy modalities multimodal fusion since textual modality negatively affected visual audio modalities. result useful textual features ignored corresponding visual acoustic feature noisy important information lost. solution introduce on/off input gate controller determines acoustic visual modality time step contribute overall prediction. reason apply input gate controller acoustic/visual features always letting textual features language modality much reliable multimodal sentiment analysis visual acoustic. also visual acoustic modalities noisy unreliable since audio visual feature extraction done automatically using methods additional noise. representing audio visual inputs time step respectively. controller weights determining on/off audio modalities weights determining on/off visual modalities. controllers implemented deep neural networks take respectively. binary output controllers mimics accepting rejecting modality based noise level. inputs concatenate features three different modalities visual audio text form inputs word level lstm temporal attention described next section. extracting features alignment word level exploit temporal correlation among different modalities time model less affected impact noisy modalities figure architecture gme-lstm model visual modality. controller visual modality selectively allows visual inputs pass. fc-relu fully-connected layer rectified linear unit activation. obtaining sentiment prediction loss eb−l reward signal train visual input gate controller studied reinforcement learning policy gradient methods balance exploration optimization randomly sampling many possible outputs controller optimizing best performance. specifically reinforce algorithm used iteratively update take visual input gate controller example detailed training algorithm visual input gate controller shown algorithm acoustic input gate trained manner. function represents dense layer non-linear activation. select mean absolute error loss function. though mean square error popular choice loss function common criteria sentiment analysis training details gme-lstm train gme-lstm need know output decisions controller affects performance lstm model. given weights gate controller input data controller decides whether reject input not. rejected inputs replaced accepted inputs changed. obtain inputs x′at x′vt train lstm inputs loss validation set. seen indicator well controller affects performance model. note lower implies better performance reward signal train controllers. expected reward represented total number time steps dataset. sentiment prediction reward signal non-convex non-differentiable respect parameters since changes outputs change discrete manner. straightforward gradient descent methods explore possible regions function. form problem experimental methodology section describe experimental methodology including dataset data splits training validation testing input features preprocessing experimental details finally review baseline models compare results cmu-mosi dataset test multimodal corpus sentiment intensity subjectivity analysis dataset collection online videos speaker expressing opinions towards movie. video split multiple clips clip contains opinion expressed sentences. clip sentiment label continuous value representing speaker’s sentiment towards certain aspect movie. figure depicts snapshot cmu-mosi dataset. cmu-mosi dataset consists videos labeled clips training performed labeled clips. video cmu-mosi dataset different speaker. train test sets defined trains videos/ clips validates videos/ clips tests videos/ clips speaker dependent contamination experiments model generalizable learns speaker-independent features. input features text video audio input modalities task. text inputs pre-trained word embeddings convert transcripts videos cmu-mosi dataset word vectors. dimensional word embedding trained billion tokens common crawl dataset. audio inputs covarep extract acoustic features including mel-frequency cepstral coefficients pitch tracking voiced/unvoiced segmenting features glottal source parameters peak slope parameters maxima dispersion quotients. video inputs facet openface extract features including facial action units facial landmarks head pose gaze tracking features figure snapshot cmu-mosi dataset text visual audio features aligned. example bottom figure first scene labeled text speaker currently saying word aligned video clip speaking word looks excited. implementation details training select best features facet covarep using univariate linear regression tests. selected facet covarep features linearly normalized maximum absolute value training set. lstm model number hidden units lstm maximum sequence length lstm units relu fully connected layer. model trained using adam learning rate loss function. gme-lstm model visual audio controllers implemented neural network hidden layer units sigmoid activation. number samples generated controller training step sampled lstm model trained using adam learning rate loss function. input gate controller trained using adam learning rate sal-cnn multimodal sentiment analysis model attempts prevent identitydependent information learned improve generalization based accurate indicators sentiment. results section summarize results multimodal sentiment analysis. table compare proposed approaches previous state-of-the-art multimodal well language-based baseline models sentiment analysis multimodal section table shows performance proposed approaches compared multimodal baseline methods. model proposed gme-lstm well version without gated controller lstm outperform multimodal single modality sentiment analysis models. gmelstm model gives best result achieved across models improving upon state binary classification accuracy mae. since gme-lstm able attend time using soft attention well input modality using gated multimodal embedding layer surprise model outperforms others. language section table shows lstm single modality language obtains slightly worse performance language-based methods. methods complicated language models dependency-based parse tree. however combining cues audio video careful multimodal fusion gme-lstm immediately outperforms language-based multimodal baseline models. jump performance shows good temporal attention multimodal fusion model benefit addition input modalities models did. discussion section analyze usefulness model’s different components demonstrating temporal attention layer gated multimodal embedding input modalities crucial towards multimodal fusion sentiment prediction. table sentiment prediction results test using different text-based multimodal methods. numbers reported binary classification accuracy f-score best scores highlighted bold ∆sot shows improvement state-of-the-art. results rntn parenthesized because model trained stanford sentiment treebank dataset much larger cmu-mosi. table sentiment prediction results test using lstm model different combinations modalities. numbers reported binary classification accuracy f-score best scores highlighted bold. text modality provides better sentiment prediction using unimodal audio visual modalities. acoustic visual modality noisy. provide additional modalities lstm model without attention performance improve significantly. using three modalities actually leads slightly worse performance f-score compared using fewer input modalities. allows deduce audio video features probably noisy figure successful case although highest weighted word extracted transcript want ambiguous sentiment lstm leverages visual modality speaker looks disappointed make prediction video sentiment much closer ground truth table sentiment prediction results test using lstm model different combinations modalities. numbers reported binary classification accuracy f-score best scores highlighted bold. hurt model’s performance multimodal fusion carefully performed. temporal attention improves sentiment prediction. hand lstm model table shows adding modalities improves sentiment regression classification. lstm consistently outperforms lstm across modality combinations. hypothesize using temporal attention model assign largest attention weights time steps modalities give strong consistent sentiment predictions abandon noisy frames altogether. result temporal attention improves sentiment prediction despite presence noisy acoustic visual modalities. successful cases lstm model. obtain better insight model’s performance provide successful cases demonstrate contribution temporal attention layer. studying attention weights lstm find words/time steps model focuses following examples successful cases look textual modality alone. line represents single transcript bold word indicates word model assigns highest attention weight highlighted words words good indicators positive negative sentiment. lstm model combines word meanings audio visual indicators. figure figure examples lstm model successful modalities. examples lstm model able leverage word level alignment audio visual modalities overcome ambiguity corresponding aligned word. lstm model able determine overall video sentiment greater accuracy compared lstm model without attention. word level fusion enables fine grained multimodal analysis. model indeed capturing meaning words implicitly classifying based sentiment positive negative neutral. neutral words model correctly looks aligned visual audio modalities make prediction. therefore model learning indicators sentiment facial gestures tone voice well. benefit word figure successful case although highest weighted word extracted transcript lines ambiguous sentiment lstm leverages visual modality speaker looks make prediction video sentiment much closer ground truth gated multimodal embedding analysis gated multimodal embedding helps multimodal fusion. lstm model still susceptible noisy modalities. table shows gme-lstm model outperforms lstm model table sentiment prediction results test using lstm lstm gme-lstm multimodal models. numbers reported binary classification accuracy f-score best scores highlighted bold. figure successful case across entire video speaker’s facial features rather monotonic except frame smiled brightly visual input gate rejects visual input time steps after allows frame pass since speaker displaying obvious facial gestures. prediction much closer ground truth compared without input gate controller metrics indicating value attending modalities using gated multimodal embedding gme-lstm model correctly selects helpful modalities. obtain better insight effect gated multimodal embedding layer successful example shown figure input gate controller visual modality correctly identifies frames obvious facial expressions displayed rejects blank expression. gme-lstm model correctly rejects noisy modalities. revisit failure case lstm model speaker covering mouth word gives best sentiment prediction cute lstm model focusing uninformative time step makes poor sentiment prediction. words model confused added visual audio modalities uninformative noisy. found gated multimodal embedding correctly rejects noisy visual input time step cute gme-lstm model gives sentiment prediction closer ground truth good example gme-lstm model directly tackles problem motivated development issue noisy modalities hurts performance multimodal fusion carefully performed. specifically gme-lstm model able learn visual modality mismatched textual modality recognizing visual modality noisy corresponding word good indicator positive speaker sentiment. figure successful case lstm extracts wrong word sentiment extracting little instead better word cute upon inspection speaker covering mouth word cute\" spoken leads less attention weight word cute since modalities consistently strong frame. result lstm model makes prediction video sentiment away ground truth however gated multimodal embedding correctly rejects noisy visual input time step cute including gated multimodal embedding improves sentiment prediction back closer ground truth. conclusion paper proposed gated multimodal embedding lstm temporal attention model multimodal sentiment analysis. approach first it’s kind perform multimodal fusion word level. furthermore build model suitable complex structure speech introduce selective word-level fusion modalities using gating mechanism trained using reinforcement learning. attention model divert focus model important moments speech. stateful nature model allows long interactions captured different modalities. show state performance mosi dataset bring qualitative analysis model able deal various challenges understanding communication dynamics. references basant agarwal soujanya poria namita mittal alexander gelbukh amir hussain. concept-level sentiment analysis dependency-based semantic parsing novel approach. cognitive computation stanislaw antol aishwarya agrawal jiasen margaret mitchell dhruv batra lawrence zitnick devi parikh. visual question answering. proceedings ieee international conference computer vision. tadas baltrušaitis chaitanya ahuja louis-philippe morency. arxiv preprint tadas baltrušaitis peter robinson louis-philippe morency. openface open source facial behavior analysis toolkit. applications computer vision ieee winter conference ieee chatterjee park l.-p. morency scherer. combining perspectives classifying multimodal data recognizing speaker traits. proceedings international conference multimodal interaction gilles degottex john kane thomas drugman tuomo raitio stefan scherer. covarep—a collaborative voice analysis repository speech technologies. acoustics speech signal processing ieee international conference ieee jeffrey donahue lisa anne hendricks sergio guadarrama marcus rohrbach subhashini venugopalan kate saenko trevor darrell. long-term recurrent convolutional networks visual recognition description. proceedings ieee conference computer vision pattern recognition. felix gers jürgen schmidhuber fred cummins. learning forget continual prediction lstm. neural computation sepp hochreiter jürgen schmidhuber. long short-term memory. neural comput. https//doi.org/./neco... imotions. facial expression analysis. goo.gl/rhjn mohit iyyer varun manjunatha jordan boyd-graber daumé iii. deep unordered composition rivals syntactic methods text classification.. kalchbrenner edward grefenstette phil blunsom. convolutional neural network modelling sentences. arxiv preprint arxiv. diederik kingma jimmy adam method stochastic opti navonil majumder soujanya poria alexander gelbukh erik cambria. deep learning-based document modeling personality detection text. ieee intelligent systems louis-philippe morency rada mihalcea payal doshi. towards multimodal sentiment analysis harvesting opinions web. proceedings international conference multimodal interfaces. sunghyun park shim moitreya chatterjee kenji sagae louisphilippe morency. computational analysis persuasiveness social multimedia novel dataset multimodal prediction approach. proceedings international conference multimodal interaction york https//doi.org/./. veronica perez-rosas rada mihalcea louis-philippe morency. utterance-level multimodal sentiment analysis. association computational linguistics sofia bulgaria. http//ict.usc.edu/pubs/utterance-level% multimodal%sentiment%analysis.pdf soujanya poria basant agarwal alexander gelbukh amir hussain newton howard. dependency-based semantic parsing concept-level text analysis. international conference intelligent text processing computational linguistics. springer soujanya poria erik cambria alexander gelbukh. deep convolutional neural network textual features multiple kernel learning utterance-level multimodal sentiment analysis. soujanya poria erik cambria alexander gelbukh. deep convolutional neural network textual features multiple kernel learning utterance-level multimodal sentiment analysis. proceedings conference empirical methods natural language processing emnlp lisbon portugal september http//aclweb.org/anthology/d/d/ d-.pdf soujanya poria chaturvedi erik cambria federica bisio. sentic improving semantic similarity aspect-based sentiment analysis. neural networks international joint conference ieee soujanya poria alexander gelbukh dipankar sivaji bandyopadhyay. fuzzy clustering semi-supervised learning–case study construction emotion lexicon. mexican international conference artificial intelligence. springer soujanya poria haiyun peng amir hussain newton howard erik cambria. ensemble application convolutional neural networks multiple kernel learning multimodal sentiment analysis. neurocomputing stefan scherer gale lucas jonathan gratch albert skip rizzo louisphilippe morency. self-reported symptoms depression ptsd associated reduced vowel space screening interviews. ieee transactions affective computing https//doi.org/./taffc. richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts recursive deep models semantic compositionality sentiment treebank. proceedings conference empirical methods natural language processing vol. citeseer lucia specia stella frank khalil sima’an desmond elliott. shared task multimodal machine translation crosslingual image description. proceedings first conference machine translation berlin germany. association computational linguistics. ucbvislab. pfa-vislab. https//github.com/ucbvislab/pfa-vislab michel valstar jonathan gratch björn schuller fabien ringeval dennis lalanne mercedes torres torres stefan scherer giota stratou roddy cowie maja pantic. avec depression mood emotion recognition workshop challenge. proceedings international workshop audio/visual emotion challenge. verena venek stefan scherer louis-philippe morency albert rizzo john pestian. adolescent suicidal risk assessment clinician-patient interaction. ieee transactions affective computing haohan wang aaksha meghawat louis-philippe morency eric xing. select-additive learning improving cross-individual generalization multimodal sentiment analysis. arxiv preprint arxiv. martin wöllmer felix weninger tobias knaup björn schuller congkai kenji sagae louis-philippe morency. youtube movie reviews sentiment analysis audio-visual context. ieee intelligent systems bishan yang claire cardie. extracting opinion expressions semimarkov conditional random fields. proceedings joint conference empirical methods natural language processing computational natural language learning. association computational linguistics quanzeng hailin zhaowen wang chen fang jiebo luo. image captioning semantic attention. proceedings ieee conference computer vision pattern recognition. zhou stefen scherer david devault jonathan gratch giota stratou louisphilippe morency justine cassell. multimodal prediction psychological disorders learning verbal nonverbal commonalities adjacency pairs. semdial dialdam proceedings workshop semantics pragmatics dialogue. amir zadeh tadas baltrušaitis louis-philippe morency. convolutional experts constrained local model facial landmark detection. computer vision pattern recognition workshops ieee conference ieee amir zadeh rowan zellers pincus louis-philippe morency. mosi multimodal corpus sentiment intensity subjectivity analysis online opinion videos. arxiv preprint arxiv. amir zadeh rowan zellers pincus louis-philippe morency. multimodal sentiment intensity analysis videos facial gestures verbal messages. ieee intelligent systems qiang mei-chen kwang-ting cheng shai avidan. fast human detection using cascade histograms oriented gradients. computer vision pattern recognition ieee computer society conference vol. ieee", "year": 2018}