{"title": "Robust Estimation via Robust Gradient Estimation", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "We provide a new computationally-efficient class of estimators for risk minimization. We show that these estimators are robust for general statistical models: in the classical Huber epsilon-contamination model and in heavy-tailed settings. Our workhorse is a novel robust variant of gradient descent, and we provide conditions under which our gradient descent variant provides accurate estimators in a general convex risk minimization problem. We provide specific consequences of our theory for linear regression, logistic regression and for estimation of the canonical parameters in an exponential family. These results provide some of the first computationally tractable and provably robust estimators for these canonical statistical models. Finally, we study the empirical performance of our proposed methods on synthetic and real datasets, and find that our methods convincingly outperform a variety of baselines.", "text": "provide computationally-eﬃcient class estimators risk minimization. show estimators robust general statistical models classical huber ǫ-contamination model heavy-tailed settings. workhorse novel robust variant gradient descent provide conditions gradient descent variant provides accurate estimators general convex risk minimization problem. provide speciﬁc consequences theory linear regression logistic regression estimation canonical parameters exponential family. results provide ﬁrst computationally tractable provably robust estimators canonical statistical models. finally study empirical performance proposed methods synthetic real datasets methods convincingly outperform variety baselines. robust estimation rich history statistics seminal contributions tukey huber hampel several others. classical analysis statistical estimators statistical guarantees derived strong model assumptions cases guarantees hold absence arbitrary outliers deviations model assumptions. strong model assumptions rarely practice development robust inferential procedures various associated statistical concepts inﬂuence function breakdown point huber ǫ-contamination model assess robustness estimators. despite progress however statistical methods strongest robustness guarantees instance based non-convex -estimators tournaments notions depth computationally intractable. paper present class estimators computationally tractable strong robustness guarantees. estimators propose obtained robustifying classical algorithms risk minimization applicable wide-range parametric statistical models parameter estimation cast within framework. contrast classical work instance m-estimation attempt replace risk minimization objective robust counterpart instead focus making canonical gradient-based optimization usual risk minimization objective robust. shift perspective enables uniﬁed treatment diﬀerent statistical models leads computationally tractable estimators leads estimators strong robustness guarantees. outliers arbitrary deviations model assumptions; i.e. typically assumed zi’s independent identically distributed according distribution many analyses risk minimization assume follows sub-gaussian distribution otherwise well-controlled tails order appropriately control deviation population risk empirical counterpart. general results specialized obtain results variety models notions robustness focus developing estimators robust canonical classes deviations model assumptions distribution allows arbitrary outliers correspond gross corruptions subtle deviations assumed model. model equivalently viewed model mis-specﬁcation total variation metric. setting interested developing estimators weak moment assumptions. assume distribution obtain samples ﬁnite low-order moments heavy tailed distributions arise frequently analysis ﬁnancial data large-scale biological datasets contrast classical analyses empirical risk minimization setting empirical risk uniformly close population risk methods directly minimize empirical risk perform poorly ﬁrst contribution introduce class robust estimators risk minimization estimators based robustly estimating gradients population risk computationally tractable design. building prior work robust mean estimation huber model heavy-tailed model design robust gradient estimators population risk main insight general risk minimization setting gradient population risk simply multivariate mean vector leverage prior work mean estimation design robust gradient estimators. able signiﬁcantly generalize applicability mean estimation methods general parametric models. estimators practical second contribution conduct extensive numerical experiments real simulated data proposed estimators. provide guidelines tuning parameter selection compare proposed estimators several competitive baselines across diﬀerent settings according various metrics estimators consistently perform well. finally provide rigorous robustness guarantees estimators propose variety canonical statistical models including linear regression logistic regression estimation canonical parameters exponential family. contributions direction two-fold building prior work provide general result stability gradient descent risk minimization showing favorable cases gradient descent quite tolerant inaccurate gradient estimates. subsequently concrete settings provide careful analysis quality gradient estimation aﬀorded proposed gradient estimators combine results obtain guarantees ﬁnal estimates. broadly discuss sequel work suggests estimators based robust gradient estimation oﬀer variety practical conceptual statistical computational advantages robust estimation. extensive work broadly area robust statistics references therein) focus section lines work related paper. classical work already developed several estimators known optimally robust variety inferential tasks including hypothesis testing mean estimation general parametric estimation non-parametric estimation however major drawback classical line work estimators strong robustness guarantees computationally intractable remaining ones heuristics optimal recently ﬂurry research theoretical computer science designing provably robust estimators computationally tractable achieving near-optimal contamination dependence special classes problems. proposed algorithms practical rely ellipsoid algorithm require solving semideﬁnite programs slow modern problem sizes. build work study practical robust mean covariance estimators distributions appropriately controlled moments. complementary line recent research focused providing minimax upper lower bounds performance estimators ǫ-contamination model without constraint computational tractability. ǫ-contaminated model arbitrary work settings contamination distribution restricted various ways. example recent work high-dimensional statistics studied problems like principal component analysis linear regression assumption corruptions evenly spread throughout dataset. another line research focused designing robust estimators heavy tailed distribution setting. approaches relax sub-gaussian sub-exponential distributional assumptions typically imposed target distribution allow heavy tailed distribution. approaches category robust mean estimators exhibit sub-gaussian type concentration around true mean distributions satisfying mild moment assumptions. median-of-means estimator catoni’s mean estimator popular examples robust mean estimators. sabato median-of-means estimator develop alternative heavy tails. although estimator strong theoretical guarantees computationally tractable noted authors performs poorly practice. recent work brownlees replace empirical mean empirical risk minimization framework catoni’s mean estimator perform risk minimization. authors provide risk bounds similar bounds achieve sub-gaussian distributional assumptions. however estimator easily computable authors provide practical algorithm compute estimator. recent works lerasle oliveira lugosi mendelson similar ideas derive estimators perform well theoretically heavy-tailed situations. however approaches involve optimization complex objectives computationally tractable algorithms exist. emphasize contrast work works focus robustly estimating population risk directly lead computable estimator. instead consider robustly estimating gradient population risk. complemented gradient descent algorithm leads naturally computable estimator. conclude section brief outline remainder paper. section provide background risk minimization huber heavy-tailed noise models. section introduce class estimators provide concrete algorithms ǫ-contaminated heavy-tailed setting. section study empirical performance estimator variety tasks datasets. complement empirical results theoretical guarantees sections defer technical details appendix. finally conclude section discussion open problems. goal risk minimization minimize population risk given samples {zi}n whereas parameter estimation interested estimating unknown parameter samples work assume population risk convex ensure tractable minimization. moreover order ensure identiﬁability parameter impose standard regularity conditions population risk. properties deﬁned terms error ﬁrst-order taylor approximation population risk i.e. deﬁning assume starting point techniques develop paper classical projected gradient descent method empirical risk minimization. given data {zi}n empirical risk minimization estimates unknown parameter minimizer empirical risk i.e. step size projection operator onto despite simplicity gradient descent method robust general convex losses. furthermore empirical risk minimizer poor estimator presence outliers data since depends sample mean outliers data eﬀect sample mean lead sub-optimal estimates. observation large body research focuses developing robust m-estimators favorable statistical properties often computationally intractable. goals work develop general statistical estimation methods robust following models huber’s ǫ-contamination model heavy-tailed model. brieﬂy review notions robustness. heavy-tailed model assumed data follows heavy-tailed distribution heavy-tailed distributions various possible characterizations paper consider characterization gradients. given i.i.d observations objective estimate minimizer population risk. conceptual standpoint classical analysis risk-minimization relies uniform concentration empirical risk around true risk fails heavy-tailed setting necessitating estimators analyses problem gradient estimation reduced multivariate mean estimation problem goal robustly estimate true mean samples {∇l}n given sample-size conﬁdence parameter deﬁne gradient estimator deﬁnition function gradient estimator functions probability least ﬁxed estimator satisﬁes following inequality subsequent sections develop conditions obtain gradient estimators strong control functions huber heavy-tailed models. furthermore investigating stability gradient descent develop sufﬁcient conditions functions gradient descent inaccurate gradient estimator still returns accurate estimate. perform projected gradient descent. order avoid complex statistical dependency issues arise analysis gradient descent theoretical results consider sample-splitting variant algorithm iteration performed fresh batch samples assume number gradient iterations speciﬁed a-priori accordingly deﬁne discuss methods selecting impact sample-splitting later sections. conﬁrmed experiments sample-splitting viewed device introduced theoretical convenience likely eliminated complex uniform arguments next consider notions robustness described section derive speciﬁc gradient estimators models using framework described above. although major focus work huber contamination heavy-tailed models class estimators general restricted notions robustness. ﬂurry recent interest designing mean estimators which huber contamination model robustly estimate mean random vector. results focused case uncorrupted distribution gaussian isotropic interested robust mean oracles general distributions. proposed robust mean estimator general distributions satisfying weak moment assumptions leverage existence estimator brieﬂy describe main idea behind algorithm mean estimator algorithm builds upon fact one-dimension relatively easy estimate gradient robustly. higher-dimension crucial insight eﬀect contamination mean uncontaminated distribution eﬀectively one-dimensional provided accurately estimate direction along mean shifted. direction diﬀerence sample mean gradient true gradient true gradient estimated using robust d-mean algorithm along gradient-shift direction non-robust sample-gradient orthogonal direction since contamination eﬀect gradient orthogonal direction. order identify gradient shift direction recursive singular value decomposition based algorithm. stage recursion ﬁrst remove gross-outliers truncation algorithm subsequently identify subspaces using clean subspace contamination small eﬀect mean another subspace contamination potentially larger eﬀect. simple sample-mean estimator clean subspace recurse computation subspace. building work lemma appendix provide careful analysis gradient estimator. design gradient estimators heavy-tailed model leverage recent work designing robust mean estimators setting. robust mean estimators build classical work alon nemirovski yudin jerrum so-called median-of-means estimator. problem one-dimensional mean estimation catoni lerasle oliveira propose robust mean estimators achieve exponential concentration around true mean distribution bounded second moment. work require mean estimators multivariate distributions. several recent works extend median-of-means estimator general metric spaces. paper geometric median-of-means estimator originally proposed analyzed setup contamination level next generate clean covariates corresponding clean responses using simulate outlier distribution drawing covariates setting responses total number samples i.e. sample size increases dimension. scaling used ensure statistical error absence contamination roughly optimally robust method error close metric measure parameter error ℓ-norm. also study convergence properties proposed method diﬀerent contamination levels code provided implement gradient estimator. updates current estimates solving active set. bhatia shown superiority torrent convex-penalty based outlier techniques hence compare methods. plugin estimator implemented using algorithm estimate mean vector performs poorly huber contamination model contaminated. error robust plugin estimator increases dimension. investigate theoretically section error plugin estimator grows norm corroborates corollary section metric measure classiﬁcation error separate test set. study error changes convergence properties proposed method diﬀerent contamination levels results figures show qualitatively similar results linear regression setting i.e. error proposed estimator degrades gracefully contamination level gradient descent iterates converge linearly. figure observe logistic regression perform poorly. logistic regression completely ﬂips labels error close whereas linear outputs random hyperplane classiﬁer ﬂips label roughly half dataset. setup experiment show eﬃcacy algorithm attempting reconstruct face images corrupted heavy occlusion occluding pixels play role outliers. data cropped yale dataset dataset choose face images subject taken mild illumination conditions computed eigenface eigenfaces. given corrupted face image subject goal best reconstruction/approximation true face. remove scaling eﬀects normalized images range. image person used metric root mean square error original reconstructed image evaluate performance algorithms. also compute best possible reconstruction original face image using eigenfaces. methods torrent baselines. wang implemented popular robust estimators ransac huber loss etc. showed poor performance. wang proposed alternate robust regression algorithm called self scaled regularized robust regression showed equivalence ℓ-penalized method. also compare best possible rmse obtained reconstructing un-occluded image using eigenfaces. results table shows mean rmse best proposed gradient descent based method recovered images cases closer un-occluded original image. figure shows case none methods succeed reconstruction. figure robust face recovery results top; order original image occluded image best possible recovery given basis. bottom; order reconstructions using proposed algorithm torrent ordinary least squares consider heavy-tailed model present experimental results synthetic real world datasets comparing gradient descent based robust estimator described algosetup. covariate sampled zero-mean isotropic gaussian distribution. entry /√p. noise sampled pareto distribution mean zero variance tail parameter tail parameter determines moments pareto random variable. speciﬁcally moment order exists hence smaller heavy-tailed distribution. setup keep dimension ﬁxed vary always maintain sample-size least simply solution. also study erm-gd performs gradient descent equivalent using empirical mean gradient oracle framework. also compare robust estimation techniques sabato duchi namkoong experiments iterative techniques convergence. results. reduce variance plots presented next section averaged results repetitions. figure shows beneﬁts using gmom erm. figure plot excess risk erm-gd gmom number iterations. upon convergence gmom much lower population risk erm. expected erm-gd converges erm. however population risk erm-gd ﬁrst iterations much lower risk suggesting early stopping. next figure plot scaled excess risk gmom increases. gmom always better even number samples times dimension figure plot relative eﬃciency gmom shows percentage improvement excess risk gmom decreases noise level decreases. behavior expected noiseless setting methods would similar behavior. similar study relative eﬃciency heavy-tailedness noise distribution. noted before increased moments exist underlying distribution. figure shows noise distribution becomes heavy-tailed beneﬁt using gmom erm. dependence conﬁdence level. figure shows performance gmom estimator various values seen choice little eﬀect performance estimator. however notice small values performance gmom degrades. practice either cross validation validation choosing theory convergence projected gradient descent section analyze gradient estimators deﬁned algorithms sections respectively. finally sections present consequences general theory canonical examples huber contamination heavy-tailed models. drawn zero-mean distribution normal distribution variance heavy-tailed distribution student-t pareto distribution. suppose covariates mean covariance huber contamination model impossible obtain consistent estimators gradient risk non-vanishing bias caused contaminated samples. hence turn attention understanding behavior projected gradient descent biased inexact gradient estimator form present main result deﬁne notion stability gradient estimator plays role convergence gradient descent. defer proof result appendix. theorem provides general result risk minimization parameter estimation. concrete instantiation given gradient estimator risk pair ﬁrst study distribution gradient risk estimate bound ﬁrst term decreasing second term increasing suggests given need enough iterations ﬁrst term bounded second. hence number iterations smallest positive integer that section analyze gradient estimator heavy-tailed setting described algorithm following result shows gradient estimate exponential concentration around true gradient mild assumption gradient distribution bounded second moment. proof follows analysis geometric median-of-means estimator minsker denote trace matrix turn attention examples introduced earlier present speciﬁc applications theorem parametric estimation huber contamination model. shown lemma need added assumption true gradient distribution bounded fourth moments suggests need additional assumptions. make assumptions explicit defer technical details appendix. asymptotic setting number samples huber gradient estimator corresponding maximum allowed contamination level says well-conditioned covariance matrix higher contamination level tolerate. plugin estimation linear regression true parameter written closed form e−e. non-iterative estimate separately estimate using robust covariance mean oracles respectively. assumption kθ∗k would make estimator vacuous kθ∗k scales dimension kθ∗k. disadvantage plugin estimation inescapable known minimax results robust mean estimation show dependence kθ∗k unavoidable leads equivalence theorem theorem setting. following section instantiate theorem logistic regression compare contrast results existing methods. plugin estimation since true parameter minimizer negative loglikelihood know implies eθ∗]. shows true parameter obtained inverting operator whenever possible. robust estimation framework robust mean suﬃcient statistics estimate eθ∗]. instantiate estimator using mean estimator estimate eθ∗] logarithmically dimension dependency dimension facet using estimator gradient estimation. using better oracles improve performance. next would like point diﬀerence maximum allowed contamination three models. logistic regression exponential family linear regression diﬀerences large part diﬀering variances gradients naturally depend underlying risk function. scaling variance gradients linear regression also provides insights limitations algorithm gradient estimators. appendix provide upper bound contamination level based initialization point which algorithm would work gradient estimator. paper introduced broad class estimators showed estimators strong robustness guarantees huber’s ǫ-contamination model heavy-tailed distributions. estimators leverage robustness gradient descent together observation risk minimization statistical models gradient risk takes form simple multivariate mean robustly estimated using recent work robust mean estimation. estimators based robust gradient descent work well practice many cases outperform robust estimators. several avenues future work including developing better understanding robust mean estimation. improvement robust mean estimation would immediately translate improved guarantees estimators propose general parametric models. finally would also interest understand extent could replace gradient descent optimization methods accelerated gradient descent newton’s method. note however although methods faster rates convergence classical risk minimization setting setup stability crucial warrants investigation.", "year": 2018}