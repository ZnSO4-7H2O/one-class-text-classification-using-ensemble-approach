{"title": "New Advances and Theoretical Insights into EDML", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "EDML is a recently proposed algorithm for learning MAP parameters in Bayesian networks. In this paper, we present a number of new advances and insights on the EDML algorithm. First, we provide the multivalued extension of EDML, originally proposed for Bayesian networks over binary variables. Next, we identify a simplified characterization of EDML that further implies a simple fixed-point algorithm for the convex optimization problem that underlies it. This characterization further reveals a connection between EDML and EM: a fixed point of EDML is a fixed point of EM, and vice versa. We thus identify also a new characterization of EM fixed points, but in the semantics of EDML. Finally, we propose a hybrid EDML/EM algorithm that takes advantage of the improved empirical convergence behavior of EDML, while maintaining the monotonic improvement property of EM.", "text": "edml recently proposed algorithm learning parameters bayesian networks. paper present number advances insights edml algorithm. first provide multivalued extension edml originally proposed bayesian networks binary variables. next identify simpliﬁed characterization edml implies simple ﬁxed-point algorithm convex optimization problem underlies characterization reveals connection edml ﬁxed point edml ﬁxed point vice versa. thus identify also characterization ﬁxed points semantics edml. finally propose hybrid edml/em algorithm takes advantage improved empirical convergence behavior edml maintaining monotonic improvement property edml recently proposed algorithm learning parameters bayesian network incomplete data edml procedurally similar expectation maximization edml shown certain advantages theoretically practically. theoretically edml certain specialized cases provably converge iteration whereas require many iterations solve learning problem. empirically preliminary experimental evaluation suggested edml could better parameter estimates fewer iterations. vide simple extension edml bayesian networks multivalued variables whereas edml initially proposed bayesian networks binary variables. also show convex optimization problem underlies binary version edml remains convex multivalued case. next identify simpliﬁed characterization edml facilitates number theoretical observations edml. example characterization implies simple ﬁxed-point iterative algorithm solving convex optimization problems underlying edml. moreover show ﬁxed-point algorithm monotonically improves solutions convex optimization problems correspond approximate factorization posterior network parameters. armed characterization edml identify surprising connection edml particular show ﬁxed point edml ﬁxed point vice versa. observation number implications. first provides perspective ﬁxed points based semantics edml originally inspired approximate inference algorithm bayesian networks subsumed inﬂuential loopy belief propagation algorithm degenerate case second suggests hybrid edml/em algorithm seeks take advantage desirable properties each improved convergence behavior edml monotonic improvement property upper case letters denote variables lower case letters denote values. variable sets denoted bold-face upper case letters instantiations bold-face lower case letters generally denote variable distribution induced network structure parameters case complete data maximum likelihood parameters unique easily obtainable. learning parameters harder data incomplete algorithm typically employed. starts initial parameters called seed successively improves iteration. uses update equation ψx|u. ψx|u equation reduces computing parameters. moreover using ψx|u leads parameters laplace smoothing. common technique deal problem insuﬃcient counts laplace smoothing experiments. algorithm example suﬃcient implement iteration guaranteed never decrease likelihood estimates across iterations. also converges every local maxima given starts appropriate seed. common multiple seeds keeping best local maxima ﬁnds. recent treatments parameter learning bayesian networks related methods. edml recent method learning bayesian network parameters incomplete data based bayesian learning formulates estimation terms computing posterior distributions network parameters. given bayesian network constructs corresponding meta network parameters explicated make sparse enough exact inference followed compensation scheme attempts improve quality approximations obtained edge-deleted network. adaptation inference method meta networks shown figure speciﬁc techniques employed augment edge θx|u−→x auxiliary variable u−→x equivalence edge. followed deleting equivalence edge. technique yielded disconnected meta network classes subnetworks called parameter islands network islands. deleting edges proposed leads introducing auxiliary nodes bayesian network deleted edge. moreover approximate inference edge deletion follows deletion process compensation scheme searches appropriate cpts auxiliary nodes. turns search cpts done iteratively amenable intuitive interpretation shown particular cpts corresponded soft evidence network parameters network island contributes piece soft evidence network parameter. second cpts corresponded updated parameter estimates parameter island contributes estimate underlying parameter set. interpretation basis form edml shown algorithm particular version edml introduced assumes network variables binary. binary edml shall call iterates known meta networks tend complex exact inference algorithms especially dataset large enough. basic insight behind edml adapt speciﬁc approximate inference scheme meta networks goal computing parameter estimates. particular original derivation edml adapted approximate inference algorithm proposed edges deleted bayesian network like does producing estimates iteration. however edml iterations viewed phases. ﬁrst phase example data used compute piece soft evidence parameter second phase pieces soft evidence pertaining parameter used compute estimate process repeats convergence criteria met. aside optimization task edml computational complexity. contribution paper extension binary edml handles multivalued variables well. principle extension turns straightforward depicted algorithm however issues require discussion. ﬁrst concerns speciﬁcation soft evidence multivalued variables. second conﬁrming optimization problem corresponding parameter island remains strictly concave therefore admitting unique solutions. consider issues next. values except value consistent equivalent hard evidence favor hand inconsistent values providing neutral evidence. nutshell complete example provides either hard evidence relevant; neutral evidence irrelevant. consider standard learning problem depicted figure here variable takes values distribution speciﬁed parameter parameter represents corresponding probability suppose dirichlet prior parameter exponents greater one. standard learning problem compute estimates parameter given independent observations variable estimates known unique case corresponding closed form. particular edml based variant learning problem given soft observations variable instead hard observations. variant shown figure observation child used emulate soft evidence represent soft evidence simply choose multivalued edml given algorithm works follows. start initial parameter estimates like iterate performing steps iteration. ﬁrst step example dataset used compute soft evidence variable equation algorithm using soft evidence learning problem parameter θx|u given figure learning sub-problem context equation algorithm solution learning sub-problem provides next estimate parameter θx|u. process repeats. principle appropriate optimization algorithm could used solve learning sub-problems. mentioned earlier however edml update requires varying number local iterations. indeed observed parameter sets require several hundred local iterations depending seed equation convergence criteria used. another important observation edml secondary seeds compared needed start equation beginning global iteration edml. experiments seed equation using parameter estimates obtained previous global iteration edml. note however choice secondary seeds subject signiﬁcantly beneﬁt research. theorem suggests convergent iterative algorithm solving convex optimization problem equation algorithm first start initial parameter estimates iteration iteration update compute parameters previous iteration. point parameters iteration change next iterations converged ﬁxed point. theorem together theorem shows updates convergent unique maximum posterior. given equation think types iterations edml local global. global iteration corresponds executing lines algorithm similar iteration. within global iteration local iterations correspond evaluations equation note parameter θx|u local iterations meant optimal values parameter set. moreover number local iterations parameter θx|u diﬀerent depending soft evidence pertaining depending equation seeded particular parameter edml updates time complexity viewpoint update implies exactly local iteration parameter since equation needs evaluated parameter set. well known facts ﬁxed points precisely stationary points log-likelihood function property number implications capable converging every local maxima log-likelihood assuming algorithm seeded appropriately. edml viewed functions take network parameterization returns annetwork parameterization algorithm seeded initial parameters ﬁrst iteration algorithm produces next parameters generally iteration algorithm produces parameters parameters ﬁxed point algorithm. also algorithm converged following results. proof theorem rests observations. first using update equation given line algorithm immediately gets ﬁxed points characterized following equation theorem edml share ﬁxed points. result fairly surprising considering theoretical practical diﬀerences algorithms. example certain specialized situations identiﬁed edml converges optimal parameter estimates single global iteration whereas require many iterations converge estimates hand edml guaranteed monotonically improve estimates global iteration carefully examining edml updates quantities needs perform single local iteration quantities needed perform update hence without additional computational eﬀort obtain updates side eﬀect computing edml updates algorithms share ﬁxed points thus makes sense consider hybrid algorithm takes advantage improved theoretical practical beneﬁts edml monotonic improvement property additionally computing update much overhead computing edml update evaluating updates respect posterior incurs non-trivial cost. shall following section however improved convergence behavior edml enables hybrid algorithm realize improvements terms faster convergence terms time ﬁrst experiments show simple edml compared often better estimates fewer global iterations. second experiments show hybrid edml/em algorithm better estimates less time well fewer global iterations. following networks alarm andes asia diagnose pigs spect water winpts. network spect naive bayes network induced dataset repository class variable attributes. network diagnose evaluation. networks commonly used benchmarks. using networks simulated data sets certain size made data incomplete randomly selecting certain percentage nodes hidden cases network generated data sets random. combination network percentage hidden nodes generated data constitutes learning problem. edml seeded randomly generated parameters. local edml iterations seeded estimates previous global iteration. first study behavior edml compared respect global iterations every learning problem edml global iterations identify best estimates achieved either them purpose evaluation. global iteration edml thus propose simple hybrid algorithm. global iteration edml also compute updates simultaneously. evaluate update choose increases posterior most. guaranteed improve posterior hybrid algorithm also trivially guaranteed figure error parameter estimates iterations. going right x-axis increasing iterations. going y-axis increasing error. edml depicted solid line dashed black line. curves left right pairs networks andes asia diagnose alarm. pair curves represents selection diﬀerent datasets size improve current estimates improving posterior difference posterior current estimates best posterior found either algorithm considered error. error measured every global iteration edml decreases table summarizes results showing percentage global iterations algorithm less error other. global iterations algorithm less error factor decreases error algorithm average computed considered relative improvement edml respectively. table shows results three diﬀerent breakdowns diﬀerent networks diﬀerent hiding percentages average. edml obtain better estimates much fewer global iterations. interestingly case even though edml guaranteed improve estimates global iteration does. another interesting observation decreasing percentage hidden nodes widens edml favor edml. surprising though since approximate inference scheme edml based becomes accurate observations. particular local optimization problems edml solves exactly independently become independent observations figure highlights selection error curves given edml diﬀerent learning problems. cases shown edml error goes zero much faster ﬁrst experiments showed edml obtain better estimates signiﬁcantly fewer global iterations global edml iteration however costly global iteration edml performs local iterations needed solve convex optimization problem associated parameter set. thus edml potentially take time converge cases therefore reducing overall beneﬁt terms time edml still perform favorably time-wise compared show figure parameter estimates time. going right x-axis increasing time going y-axis increasing map. hybrid edml depicted solid line dashed black line edml blue dotted dashed line. curves left right following problems alarm hiding winpts hiding water hiding ﬁrst convergence second hybrid edml/em achieves quality parameter estimates summarize results table shows percentage learning problems algorithm faster other. cases hybrid edml/em algorithm faster average percentage decreases execution time reported speedup speedup cases faster given results suggest hybrid edml/em used better estimates less time. speciﬁcally average hybrid method decreases execution time factor cases rest cases faster factor roughly particularly interesting light non-trivial overhead associated hybrid edml/em evaluate edml estimates order select better estimate. comparison edml alone average faster factor times cases whereas faster edml alone factor remaining cases. played critical role learning probabilistic graphical models bayesian networks however learning remains challenging variety situations particularly hidden variables; e.g. recently characterizations also interesting connections loopy belief propagation related algorithms although focus approximate inference probabilistic graphical models message-passing less learning. slow convergence also recognized particularly presence hidden variables. cases coupled algorithms gradient ascent traditional algorithms optimization; e.g. variety techniques accelerating proposed literature; e.g. work partially supported grant n--- grant iis- grant iis-. work also based upon research performed collaborative facilities renovated funds grant award funded american recovery reinvestment", "year": 2012}