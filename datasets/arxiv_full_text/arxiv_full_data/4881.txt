{"title": "$\\ell_1$ Regularized Gradient Temporal-Difference Learning", "tag": ["cs.AI", "cs.LG"], "abstract": "In this paper, we study the Temporal Difference (TD) learning with linear value function approximation. It is well known that most TD learning algorithms are unstable with linear function approximation and off-policy learning. Recent development of Gradient TD (GTD) algorithms has addressed this problem successfully. However, the success of GTD algorithms requires a set of well chosen features, which are not always available. When the number of features is huge, the GTD algorithms might face the problem of overfitting and being computationally expensive. To cope with this difficulty, regularization techniques, in particular $\\ell_1$ regularization, have attracted significant attentions in developing TD learning algorithms. The present work combines the GTD algorithms with $\\ell_1$ regularization. We propose a family of $\\ell_1$ regularized GTD algorithms, which employ the well known soft thresholding operator. We investigate convergence properties of the proposed algorithms, and depict their performance with several numerical experiments.", "text": "paper study temporal diﬀerence learning linear value function approximation. well known learning algorithms unstable linear function approximation oﬀ-policy learning. recent development gradient algorithms addressed problem successfully. however success algorithms requires well chosen features always available. number features huge algorithms might face problem overﬁtting computationally expensive. cope diﬃculty regularization techniques particular regularization attracted signiﬁcant attentions developing learning algorithms. present work combines algorithms regularization. propose family regularized algorithms employ well known soft thresholding operator. investigate convergence properties proposed algorithms depict performance several numerical experiments. fundamental problem reinforcement learning learn long-term expected reward i.e. value function consequently used determining good control policy sutton barto general setting large inﬁnite state space exact representation actual value function often inhibitively computationally expensive hardly possible. overcome diﬃculty function approximation techniques employed estimating value function sampled trajectories. quality learned policy depends signiﬁcantly chosen function approximation technique. paper consider technique linear value function approximation. value function represented approximated linear combination features basis functions. features generated sampled states either heuristic constructions e.g. bradtke barto keller kernel-based approaches e.g. taylor parr common approach generates ﬁrstly vast number features often much larger number available samples chooses automatically relevant features approximate actual value function. unfortunately approaches fail completely overﬁtting. cope situation regularization techniques necessarily employed. simple regularization penalizes smoothness learned value function e.g. farahmand work focus regularization. regularization often produces sparse solutions thus serve method automatic feature selection linear value function approximation. work focuses development temporal diﬀerence learning algorithms bradtke barto recent active researches applying regularization learning various number eﬀective algorithms e.g. loth kolter johns geist scherrer hoﬀman important notice minimization extensively studied areas compressed sensing image processing many eﬃcient minimization algorithms developed cand´es romberg zibulevsky elad recently advanced minimization algorithms adapted learning i.e. dantzig selector based algorithm geist orthogonal matching pursuit based algorithm developed painter-wakeﬁeld parr hand learning algorithms known unstable linear value function approximation oﬀ-policy learning. observing fact original forms algorithms true gradient descent methods class intrinsic gradient learning algorithms linear value function approximation developed proven stable sutton however important know success algorithms might limited fact family requires well chosen features. words algorithms potential danger overﬁtting. contribution present work development family regularized algorithms referred gtd-ist algorithms. convergence properties proposed algorithms investigated perspective stochastic optimization. paper outlined follows. section brieﬂy introduce general setting learning provide preliminaries objective functions. section presents framework regularized learning algorithms investigates convergence properties. section several numerical experiments depict practical performance proposed algorithms compared several existing regularized algorithms. finally conclusion drawn section tuple possible states environment actions agent ×a×s conditional transition probabilities state transitions state state given action reward function assigning immediate reward state discount factor. state space large inﬁnite exact representation value function often practically unfeasible. function approximation thus great demand estimating actual value function. popular approach construct features parameter vector. setting learning parameter updated time step i.e. state transition associated reward here consider simple one-step learning linear function approximation i.e. framework learning. parameter updated follows appropriate objective function accurately measures correctness current value function approximation i.e. current approximation away actual solution. subsection recall three popular objective functions learning. cost function often referred mean squared bellman error ideally minimum msbe function admits good value function approximation. unfortunately well known that practice performance approximation depends ﬁrst part section present general framework gradient algorithms minimizing regularized objective functions. second subsection develops regularized stochastic gradient algorithms online setting investigates convergence properties perspective stochastic optimization. objective function iterative soft thresholding algorithm nowadays classic algorithm minimizing cost function interpreted extension classical gradient algorithm. high popularity skip derivation algorithm refer zibulevsky elad references therein reading. refer family algorithms td-ist algorithms. note employed developing ﬁxed point algorithms painter-wakeﬁeld parr whereas work focus developing intrinsic gradient algorithm. td-ist algorithms presented previous subsection applicable batch setting. real applications certainly favorable working online. stochastic gradient descent algorithms developed straightforwardly minimize regularized objective functions. approximations sutton investigate convergence properties proposed algorithms requires results duchi singer develops general framework analyzing empirical loss minimization regularizations. adapt result corollary duchi singer current setting follows. step size parameter. refer corresponding algorithm gtd-ist algorithm. convergence properties gtd-ist algorithm characterized following corollary. easily seen regularized function strictly convex matrix invertible. solution global minimum condition i.i.d sequence uniformly bounded second moments ensures holds true constant finally applying fact stochastic approximation quasi-stationary estimate term sutton refer corresponding regularized algorithms employ updates gtd-ist tdc-ist algorithms respectively. surprises share similar convergence properties gtd-ist algorithm. experiment apply proposed algorithms random walk problem chain environment consisting seven states. exists action transition probability going right left equal. reward assigned rightmost state terminal state whereas rewards zero everywhere else. features consist binary encoding states additional noisy features simply gaussian noise. setting three diﬀerent experiments. experiment compares performance proposed regularized algorithms un-regularized counterparts. figure shows learning curves three learning algorithms namely together regularized versions. evident based algorithms outperform original unregularized versions respectively. experimental results demonstrate eﬀectiveness based learning algorithms. second experiment investigates recovery behavior convergence speed proposed algorithms unfavorable initializations. here consider simple gtd-ist algorithm. parameter vector initialized ones noisy features zeros good features. words experiment starts initialization selecting features. results figure show third experiment compare gtd-ist algorithms algorithm painter-wakeﬁeld parr lars-td algorithm kolter results figure imply that without noise three gtd-ist algorithms outperforms algorithm consistently. closer look result zoomed-in window figure shows lars-td algorithm performs best presence noise. might fact larstd algorithm updates every episodes using samples available. nevertheless without surprise timing experiment shows table lars-td algorithm performs much slower online algorithms. test performance gtd-ist algorithms oﬀ-policy learning employ well-known star example proposed baird consists seven states state considered center. outer states agent choose actions either solid action takes center state probability dotted action takes states equal probability. reward state transitions equal zero states represented tabular features described original setting. noisy gaussian features state representation. behavior policy chooses solid action probability dotted otherwise estimation policy chooses always dotted action. learning curves figure shows gtdist gtd-ist algorithms outperform original counterparts consistently. work combines recently developed methods regularization proposes family gtd-ist algorithms. investigate convergence properties proposed algorithms perspective stochastic optimization. preliminary experiments demonstrate proposed family gtd-ist algorithms outperform original counterparts existing regularized algorithms. aware advanced developments community sparse representation project employ state-of-the-art algorithms sparse representation example algorithms usually known slow compared advanced minimization algorithms. applying eﬃcient minimization algorithms beck teboulle learning great interests future work. work partially supported international graduate school science engineering technische universit¨at m¨unchen germany. authors would like thank christopher painter-wakeﬁeld providing matlab implementation algorithm.", "year": 2016}