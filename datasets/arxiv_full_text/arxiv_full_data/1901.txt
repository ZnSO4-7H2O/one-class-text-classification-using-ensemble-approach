{"title": "Combining Language and Vision with a Multimodal Skip-gram Model", "tag": ["cs.CL", "cs.CV", "cs.LG"], "abstract": "We extend the SKIP-GRAM model of Mikolov et al. (2013a) by taking visual information into account. Like SKIP-GRAM, our multimodal models (MMSKIP-GRAM) build vector-based word representations by learning to predict linguistic contexts in text corpora. However, for a restricted set of words, the models are also exposed to visual representations of the objects they denote (extracted from natural images), and must predict linguistic and visual features jointly. The MMSKIP-GRAM models achieve good performance on a variety of semantic benchmarks. Moreover, since they propagate visual information to all words, we use them to improve image labeling and retrieval in the zero-shot setup, where the test concepts are never seen during model training. Finally, the MMSKIP-GRAM models discover intriguing visual properties of abstract words, paving the way to realistic implementations of embodied theories of meaning.", "text": "extend skip-gram model mikolov taking visual information account. like skip-gram multimodal models build vector-based word representations learning predict linguistic contexts text corpora. however restricted words models also exposed visual representations objects denote must predict linguistic visual features jointly. mmskip-gram models achieve good performance variety semantic benchmarks. moreover since propagate visual information words improve image labeling retrieval zero-shot setup test concepts never seen model training. finally mmskip-gram models discover intriguing visual properties abstract words paving realistic implementations embodied theories meaning. distributional derive vector-based representations meaning patterns word co-occurrence corpora. dsms effectively applied variety semantic tasks however compared human semantic knowledge purely textual models like traditional symbolic systems severely impoverished suffering lack grounding extra-linguistic modalities observation development multimodal distributional semantic models enrich linguistic vectors perceptual information often form visual features automatically induced image collections. text-based approaches tasks directly require access visual knowledge also general semantic benchmarks however current mdsms still number drawbacks. first generally constructed ﬁrst separately building linguistic visual representations concepts merging them. obviously different humans learn concepts hearing words situated perceptual context. second mdsms assume linguistic visual information available words generalization knowledge across modalities. third latter assumption full linguistic visual coverage current mdsms paradoxically cannot applied computer vision tasks image labeling retrieval since generalize images words beyond training set. introduce multimodal skip-gram models mdsms address issues above. models build upon effective skip-gram approach mikolov constructs vector representations learning incrementally predict linguistic contexts target words occur corpus. extension subset target words relevant visual evidence natural images presented together corpus contexts model must learn predict visual representations jointly linguistic features. joint objective encourages propagation visual information representations words direct visual evidence available training. resulting multimodally-enhanced vectors achieve remarkably good performance traditional semantic benchmarks application zero-shot image labeling retrieval scenario. interestingly indirect visual evidence also affects representation abstract words paving ground-breaking cognitive studies novel applications computer vision. large literature multimodal distributional semantic models. focus representative systems. bruni propose straightforward approach mdsm induction textimage-based vectors words constructed independently mixed applying singular value decomposition concatenation. empirically superior model proposed silberer lapata advanced visual representations relying images annotated highlevel visual attributes multimodal fusion strategy based stacked autoencoders. kiela bottou adopt instead simple concatenation strategy obtain empirical improvements using state-of-the-art convolutional neural networks extract visual features skip-gram model text. related systems take twostage approach derive multimodal spaces tested concepts textual visual labeled training data available learn text images jointly using topic models shown empirically weak bruni early-acquired concrete words larger vocabulary. however subject-generated features surrogate realistic perceptual information test model small-scale simulations word learning. hill korhonen whose evaluation focuses perceptual information affects different word classes less effectively similarly howell integrate perceptual information form subject-generated features text image annotations skipgram model. inject perceptual information merging words expressing perceptual features corpus contexts amounts linguisticcontext re-weighting thus making impossible separate linguistic perceptual aspects induced representation extend model non-linguistic features. instead authentic image analysis proxy perceptual information design robust incorporate easily extendible signals feature norm brain signal vectors recent work so-called zero-shot learning address annotation bottleneck image labeling looks imagetext-based vectors different perspective. instead combining visual linguistic information common space aims learning mapping imagetext-based vectors. mapping induced annotated data used project images objects seen training onto linguistic space order retrieve nearest word vectors labels. multimodal word vectors better-suited purely text-based vectors task similarity structure closer images. however traditional mdsms canused setting cover words manually annotated training images available thus defeating generalizing purpose zero-shot learning. show below multimodal vectors hampered restriction indeed bring signiﬁcant improvement purely text-based linguistic representations zero-shot setup. figure cartoon mmskip-gram-b. linguistic context vectors actually associated classes words tree single words. skipgram obtained ignoring visual objective mmskip-gram-a ﬁxing identity matrix. visual representation concepts denote visual representation also encoded vector thus make skip-gram multimodal adding second visual term original linguistic objective extend equation follow lling text-based skip-gram oblvision term forces word representations take visual information account. note word associated visual information systematically case e.g. determiners non-imageable nouns also generally word visual data available lvision socher methods rely necessarily limited collections captioned images sources multimodal evidence whereas automatically enrich large corpus images induce general-purpose multimodal word representations could used input embeddings systems speciﬁcally tuned caption processing. thus work complementary line research. skip-gram model start reviewing standard skip-gram model mikolov version use. given text corpus skip-gram aims inducing word representations good predicting context words surrounding target word. mathematically maximizes objective function words training corpus size window around target determining context words predicted induced representation following mikolov implement subsampling option randomly discarding context words inverse function frequency controlled hyperparameter probability core part objective equation given softmax context target vector representations word respectively size vocabulary. normalization term equation requires time complexity. considerable speedup achieved using hierarchical version equation adopted here. resentations thus aligning dimensions linguistic vector visual making linguistic representation concept move closer visual representation. maximize similarity max-margin framework commonly used models connecting language vision precisely formulate visual objective lvision minus sign turns loss cost margin target multimodally-enhanced word representation learn corresponding visual vector ranges visual representations words randomly sampled distribution random visual representations negative samples encouraging similar visual representation words. sampling distribution currently uniform number negative samples controlled hyperparameter multi-modal skip-gram model visual objective mmskip-gram-a drawback assuming direct comparison linguistic visual representations constraining equal size. mmskip-gram-b lifts constraint including extra layer mediating between linguistic visual representations learning layer equivalent estimating cross-modal mapping matrix linguistic onto visual representations jointly induced linguistic word embeddings. extension straightforwardly implemented substituting equation word representation u→vuwt cross-modal mapping matrix induced. avoid overﬁtting also regularization term overall objective relative importance controlled hyperparamer text corpus wikipedia dump comprising approximately tokens. train multimodal models visual information words entry imagenet occur least times corpus concreteness score according turney average tokens text corpus associated visual representation. construct visual representation word sample pictures imagenet entry extract -dimensional vector picture using caffe toolkit together pre-trained convolutional neural network krizhevsky vector corresponds activation layer network. finally average vectors pictures associated word deriving aggregated visual representations. hyperparameters skip-gram mmskip-gram models hidden layer size facilitate comparison mmskipgram-a mmskip-gram-b since former requires equal linguistic visual dimensionality keep ﬁrst dimensions visual vectors. linguistic objective hierarchical softmax huffman frequency-based encoding tree setting frequency subsampling option window size without tuning. following hyperparameters tuned text corpus mmskip-gram-a mmskip-gram-b λ=.. approximating human judgments benchmarks widely adopted test dsms multimodal extensions measure well model-generated scores approximate human similarity judgments pairs words. together various benchmarks covering diverse aspects meaning gain insights effect perceptual information different similarity facets. speciﬁcally test general relatedness pairs) e.g. pickles related hamburgers semantic similarity pairs; semsim silberer lapata pairs) e.g. pickles similar onions well visual similarity pairs semsim different human ratings) e.g. pickles look like zucchinis. alternative multimodal models compare models several recent alternatives. test vectors made available kiela bottou similarly derive textual features skip-gram model visual representations extracted dataset convolutional neural network concatenate textual visual features normalizing unit length centering zero mean. also test vectors performed best evaluation bruni based textual features extracted b-token corpus sift-based bag-of-visual-words visual features extracted collection. bruni colleagues fuse weighted concatenation components svd. reimplement methods textual visual embeddings concatenation finally present comparison results semsim vissim reported silberer lapata obtained stacked-autoencoders architecture textual features extracted wikipedia strudel algorithm attribute-based visual features extracted imagenet. benchmarks contain fair amount words direct visual evidence. interested assessing models terms fuse linguistic visual evidence available robustness lack full visual coverage. thus evaluate settings. visual-coverage columns table report results subsets compared models access direct visual information words. report results full sets models propagate visual information that consequently meaningfully tested results state-of-the-art visual features alone perform remarkably well outperforming purely textual model tasks achieving best absolute performance visual-coverage subset simlex-. regarding multimodal focusing visual-coverage subsets) mmskip-gram models perform well tasks comparable results variants. performance also good full data sets consistently outperform skip-gram they’re points state-of-the-art correlation achieved baroni corpus larger extensive tuning. mmskip-gram-b close state simlex- reported resource creators impressively mmskip-gram-a reaches performance level silberer lapata model semsim vissim data sets despite fact latter full visual-data coverage uses attribute-based image representations requiring supervised learning attribute classiﬁers achieve performance semantic tasks comparable higher features finally multimodal models bring large performance gain purely linguistic model visual similarity improvement consistently large also benchmarks conﬁrming multimodality leads better semantic models general help capturing different types similarity defer work better understanding relation multimodal grounding different similarity relations table provides qualitative insights injecting visual information changes structure semantic space. skip-gram neighbours donuts places might encounter them whereas multimodal models relate take-away food ranking visually-similar pizzas top. example shows multimodal table spearman correlation model-generated similarities human judgments. right columns report correlation visual-coverage subsets first block reports results out-of-the-box models; second block visual textual representations alone; third block implementation multimodal models. models pick taxonomically closer neighbours concrete objects since often closely related things also look similar particular multimodal models squirrels offer birds prey nearest neighbours. direct visual evidence used induce embeddings remaining words table thus inﬂuenced vision propagation. subtler systematic changes observe cases suggest indirect propagation non-damaging respect purely linguistic representations actually beneﬁcial. concrete mural concept multimodal models rank paintings portraits less closely related sculptures tobacco models rank cigarettes cigar coffee mmskip-gram-b avoids arguably less common crop sense cued corn. last examples show multimodal models turn embodiment level representation abstract words. depth neighbours suggest concrete marine setup abstract measurement sense picked mmskip-gram neighbours. chaos rank demon concrete agent chaos replace abstract notion despair equally gloomy imageable shadows destruction multimodal representations induced models better suited purely textbased vectors label retrieve images. particular given quantitative qualitative results collected suggest models propagate visual information across words apply image labeling retrieval challenging zeroshot setup refer here conciseness’ sake image labeling/retrieval visual vectors aggregated representations images tasks we’re modeling consist precisely labeling pictures denoting object retrieving corresponding given name object. setup take test words visual vectors for. multimodal models re-trained without visual vectors words using hyperparameters above. tasks search correct word label/image conducted whole word/visual vectors. image labeling task given visual vector representing image onto word space label image word corresponding nearest vector. perform vision-tolanguage mapping train ridge regression fold cross-validation test task given linguistic/multimodal vector onto visual space retrieve nearest image. skip-gram ridge regression training regime labeling task. multimodal models since maximizing similarity visual representations already part training objective extra mapping function. mmskipgram-a directly look nearest neighbours learned embeddings visual space. mmskip-gram-b mapping function induced learning word embeddings. results image labeling skip-gram outperformed multimodal models conﬁrming models produce vectors directly applicable vision tasks thanks visual propagation. interesting results however achieved image retrieval essentially task multimodal models implicitly optimized could applied without speciﬁc training. strategy directly querying nearest visual vectors mmskip-gram-a word embeddings works remarkably well outperforming higher ranks skip-gram requires ad-hoc mapping function. suggests multimodal fold tune ridge three estimate mapping matrix test last fold. enforce strict zero-shot conditions exclude test fold labels occurring lsvrc employed train krizhevsky extract visual features. embeddings inducing general enough achieve good performance semantic tasks discussed above encode sufﬁcient visual information direct application image analysis tasks. especially remarkable word vectors testing matched visual representations model training time thus multimodal propagation. best performance achieved mmskip-gram-b conﬁrming claim matrix acts multimodal mapping function. already seen depth chaos examples table indirect inﬂuence visual information interesting effects representation abstract terms. latter received little attention multimodal semantics hill korhonen concluding abstract nouns particular beneﬁt propagated perceptual information representation even harmed information forced still embodied theories cognition provided considerable evidence abstract concepts also grounded senses since word representations produced mmskip-gram-a including pertaining abstract concepts directly used search near images visual space decided verify experimentally near images relevant concrete words precisely focused words sampled across norms concreteness spectrum kiela includes concrete abstract nouns also adjectives verbs even grammatical terms words relatively high concreteness ratings particularly imageable e.g. smell pain sweet. word extracted nearest neighbour picture mmskip-gram-a representation matched random picture. pictures selected labeled distinct words since much common concrete abstract words directly represented image picture searching nearest neighbour excluded picture labeled word interest present crowdflower survey presented test word associated images asked subjects pictures found closely related word. collected minimally judgments word. subjects showed large agreement conﬁrming understood task behaved consistently. quantify performance terms proportion words number votes nearest neighbour picture signiﬁcantly chance according two-tailed binomial test. signiﬁcance adjusting p-values holm correction running statistical tests. results table indicate that half cases nearest picture word mmskipgram-a representation meaningfully related word. expected often case concrete abstract words. still also observe table subjects’ preference nearest visual neighbour words kiela random pictures. figure merit percentage proportion signiﬁcant results favor nearest neighbour across words. results reported whole well words concreteness rating median. unseen column reports results words exposed direct visual evidence training discarded. words columns report cardinality. signiﬁcant preference model-predicted nearest picture fourth abstract terms. whether word exposed direct visual evidence training course making difference factor interacts concreteness abstract words matched images training. limit evaluation word representations exposed pictures training difference concrete abstract terms still large becomes less dramatic words considered. figure shows four cases subjects expressed strong preference nearest visual neighbour word. freedom theory strikingly agreement view embodied theories abstract words grounded relevant concrete scenes situations. together example illustrates visual data might ground abstract notions surprising ways. cases borrow howell visual propagation abstract words intuitively something like trying explain abstract concept like love child using concrete examples scenes situations associated love. abstract concept never fully grounded external reality inherit meaning concrete concepts related. multimodal vectors induce also display interesting intrinsic property related hypothesis grounded representations abstract words complex concrete ones since abstract concepts relate varied composite situations natural corollary idea visually-grounded representations abstract concepts diverse think dogs similar images speciﬁc dogs come mind. also imagine abstract notion freedom nature related imagery much varied. recently kiela proposed measure abstractness exploiting intuition. however rely manual annotation pictures google images deﬁne ad-hoc measure image dispersion. conjecture representations naturally induced models display similar property. particular entropy multimodal vectors expression varied information encode correlate degree abstractness corresponding words. figure shows indeed difference entropy concrete abstract words kiela set. test hypothesis quantitatively measure correlation entropy concreteness words kiela set. figure shows entropies mmskip-gram-a representations generated mapping mmskip-gram-b vectors onto visual space achieve high correlation evidence multimodal learning grounding representations concrete abstract words meaningful ways. introduced multimodal extensions skipgram. mmskip-gram-a trained directly optimizing similarity words visual representations thus forcing maximum interaction modalities. mmskip-gram-b includes extra mediating layer acting crossmodal mapping component. ability models integrate propagate visual information resulted word representations performed well semantic vision tasks could used input systems beneﬁting prior visual knowledge results abstract words suggest models might also help tasks metaphor detection even retrieving/generating pictures abstract concepts. incremental nature makes well-suited cognitive simulations grounded language acquisition avenue research plan explore further.", "year": 2015}