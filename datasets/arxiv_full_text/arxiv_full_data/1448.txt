{"title": "Traversing Knowledge Graphs in Vector Space", "tag": ["cs.CL", "cs.AI", "cs.DB", "stat.ML"], "abstract": "Path queries on a knowledge graph can be used to answer compositional questions such as \"What languages are spoken by people living in Lisbon?\". However, knowledge graphs often have missing facts (edges) which disrupts path queries. Recent models for knowledge base completion impute missing facts by embedding knowledge graphs in vector spaces. We show that these models can be recursively applied to answer path queries, but that they suffer from cascading errors. This motivates a new \"compositional\" training objective, which dramatically improves all models' ability to answer path queries, in some cases more than doubling accuracy. On a standard knowledge base completion task, we also demonstrate that compositional training acts as a novel form of structural regularization, reliably improving performance across all base models (reducing errors by up to 43%) and achieving new state-of-the-art results.", "text": "figure propose performing path queries lincoln/parents/location parallel low-dimensional vector space. here entity sets represented real vectors edge traversal driven vector-to-vector transformations however missing vector space models original strength knowledge bases ability support compositional queries example might ethnicity abraham lincoln’s daughter would formulated path query knowledge graph would like method answer efﬁciently generalizing missing facts even missing hypothetical entities paper present scheme answer path queries knowledge bases compositionalizing broad class vector space models used knowledge base completion high level interpret base vector space model implementing soft edge traversal operator. operator recursively applied predict paths. interpretation suggests compositional training objective encourages better modeling path queries knowledge graph used answer compositional questions what languages spoken people living lisbon?. however knowledge graphs often missing facts disrupts path queries. recent models knowledge base completion impute missing facts embedding knowledge graphs vector spaces. show models recursively applied answer path queries suffer cascading errors. motivates compositional training objective dramatically improves models’ ability answer path queries cases doubling accuracy. standard knowledge base completion task also demonstrate compositional training acts novel form structural regularization reliably improving performance across base models achieving state-of-the-art results. introduction broad-coverage knowledge bases freebase support rich array reasoning question answering applications known suffer incomplete coverage example freebase entity lincoln ethnicity. elegant solution incompleteness using vector space representations controlling dimensionality vector space forces generalization facts example would hope infer tad’s ethnicity ethnicity parents. empirical ﬁndings first show compositional training enables answer path queries least length substantially reducing cascading errors present base vector space model. second somewhat surprisingly compositional training also improves upon state-of-the-art performance knowledge base completion special case answering unit length path queries. therefore compositional training also seen form structural regularization existing models. give formal deﬁnition task answering path queries knowledge base. entities binary relations. knowledge graph deﬁned triples form example triple freebase knowledge base completion. knowledge base completion task predicting whether given edge belongs graph not. formulated path query candidate answer section show compositionalize existing models answer path queries. start motivating example section present general technique section suggests compositional training objective described section finally illustrate technique several models section experiments. example common vector space model knowledge base completion bilinear model model learn vector entity matrix rd×d relation given query recursively. model begins entity vector sequentially applies traversal operators vwri traversal operation results vector representing entities reached point traversal finally applies membership operator section show compositional training leads substantially better results path query answering knowledge base completion. section provide insight why. composable models many possible candidates example could one’s favorite neural network mapping here focus composable models recently shown achieve state-of-the-art performance knowledge base completion. bilinear-diag. bilinear-diag model yang special case bilinear model relation matrices constrained diagonal. alternatively model viewed variant transe multiplicative interactions entity relation vectors. models compositionalized. important point models naturally composable—for example latent feature model riedel neural tensor network socher approaches scoring functions combine involve intermediate vector representing alone without decompose according compositional training score function naturally suggests compositional training objective. denote path query training examples path lengths ranging minimize following max-margin objective sequence traversals. another perspective traversal operator trained transformation preserves information vector might needed subsequent traversal steps. adagrad optimize general non-convex. initialization scale mini-batch size step size cross-validated models. initialize parameters i.i.d. gaussians variance every entry mini-batch size examples step size models. example sample negative entities training entity vectors constrained unit ball clipped gradients median observed gradients update exceeded times median. ﬁrst train path queries length convergence train path queries convergence. guarantees model masters basic edges composing form paths. training path queries explicitly parameterize inverse relations. bilinear model initialize transe initialize −wr. bilinear-diag found initializing exact inverse numerically unstable instead randomly initialize i.i.d gaussians variance every entry. additionally bilinear model replaced objective since yielded slightly higher accuracy. models implemented using theano section describe standard knowledge base completion datasets. consist single-edge queries call base datasets. section generate path query datasets base datasets. wordnet freebase subsets exhibit substantial differences inﬂuence model performance. freebase subset almost bipartite edges taking form person relation property wordnet source target entities arbitrary words. wordnet freebase contain many relations almost perfectly correlated inverse relation. example wordnet contains part part freebase contains parents children. test time query edge easy answer inverse triple observed training set. following socher account excluding trivial queries test set. given base knowledge graph generate path queries performing random walks graph. view compositional training form regularization approach allows generate extremely large amounts auxiliary training data. procedure given below. generate path query test repeat procedure except using graph gfull gtrain plus test edges base dataset. remove queries test also appeared training set. statistics path query datasets presented table evaluate models derived section tasks path query answering knowledge base completion. tasks show compositional training strategy proposed section leads substantial performance gains standard single-edge training. also compare directly results socher demonstrating previously inferior models match outperform state-of-the-art models compositional training. evaluation metric. numerous metrics used evaluate knowledge base queries including hits mean rank. evaluate hits well normalized version mean rank mean quantile better accounts total number candidates. query quantile correct answer fraction incorrect answers ranked quantile ranges optimal. mean quantile deﬁned average quantile score examples dataset. illustrate normalization important consider queries relation gender. model predicts incorrect gender every query would receive mean rank fairly good absolute terms whereas mean quantile would rightfully penalizing model. ﬁnal note several queries freebase path dataset type-match trivial sense type matching candidates correct answers query. case mean quantile undeﬁned exclude queries evaluation. lower half table shows surprisingly compositional training also improves performance knowledge base completion across almost models metrics datasets. wordnet transe beneﬁts most reduction error. freebase bilinear beneﬁts most reduction error. deduction induction. table takes deeper look performance path query answering. divided path queries subsets deduction induction. deduction subset consists queries source tartraining graph gtrain speciﬁc query never seen training. queries answered performing explicit traversal training graph subset tests model’s ability approximate underlying training graph predict existence path collection single edges. induction subset consists queries. means least edge missing paths following source target training graph. hence subset tests model’s generalization ability robustness missing edges. performance deduction subset dataset disappointingly models trained single-edge training struggle answer path queries even edges path query seen training time. compositional training dramatically reduces errors sometimes doubling mean quantile. section analyze might possible. compositional training performance harder induction subset also much stronger. even edges missing along path models able infer them. interpretable queries. although path datasets consists random queries datasets contain large number useful interpretable queries. results illustrative examples shown table table path query answering knowledge base completion. compare performance single-edge training compositional training mean quantile hits %red percentage reduction error. table path query performance selection interpretable queries. compare bilinear single bilinear comp. meanings query what professions institution?; what religion parents?; what ethnicities people country what types parts have?; transitive what type of?. comparison socher here measure performance task terms accuracy metric socher evaluation involves sampled negatives hence noisier mean quantile makes results directly comparable socher results show previously inferior models socher proposed parametrizing entity vector average vectors words entity pretraining word vectors using method turian table reports results using approach conjunction compositional training. initialized models word vectors pennington found compositionally trained models outperform neural tensor network wordnet slightly behind freebase. strategy averaging word vectors form entity vectors applied compositional models signiﬁcantly better wordnet slightly better freebase. worth noting many domains entity names lexically meaningful word vector averaging section understand compositional concreteness everything described terms bilinear model. refer compositionally trained model comp model trained single-edge training single. tempting think single accurately modeled individual edges graph accurately model paths result edges. intuition turns incorrect revealed single’s relatively weak performance path query dataset. hypothesize cascading errors along path. given edge path single-edge training encourages closer incorrect however achieved margin push closer remaining discrepancy noise gets added step path traversal. illustrated schematically figure observe phenomenon empirically examine well model handles intermediate step path query. measuring reconstruction quality vector produced traversal operation. since intermediate stage valid path query deﬁne average quantile figure cascading errors visualized transe. node represents position entity vector space. relation parent ideally simple horizontal translation traversal introduces noise. circle expect tad’s parent square expect tad’s grandparent dotted lines show error grows larger traverse farther away tad. compositional training pulls entity vectors closer ideal arrangement. given nature cascading errors might seem reasonable address problem adding term objective explicitly encourages close possible motivation tried adding term objective bilinear model term objective transe. experimented different settings range case additional term improve single’s performance path query single edge dataset. results suggest compositional training effective combat cascading errors. table reveals comp also performs better single-edge task knowledge base completion. somewhat surprising since single trained training distributionally matches test whereas comp not. however comp’s better performance path queries suggests must another factor play. high level training paths must providing form structural regularization reduces cascading errors. indeed paths knowledge graph proven important features predicting existence single edges example consider following horn clause parents location place birth empirically verify comp also better meeting criterion perform following path type relation deﬁne dist angle corresponding matrices natural measure wrxt computes matrix inner product hence matrix small distance produce nearly scores entity pairs. comp better capturing correlation between would expect prec high compositional training shrink dist more. conﬁrm hypothesis enumerated possible paths length examined proportional reduction dist caused compositional training knowledge base completion vector space models. many models proposed knowledge base completion including reviewed section dong demonstrated models improve quality relation extraction serving graph-based priors. riedel showed models also directly used open-domain relation extraction. compositional training technique orthogonal improvement could help composable model. distributional compositional semantics. previous works explored compositional vector space representations context logic sentence interpretation. socher matrix associated word sentence used recursively modify meaning nearby constituents. grefenstette exfigure reconstruction quality step query lincoln/parents/place birth/ −/profession. comp experiences place birth signiﬁcantly less degradation path length increases. correspondingly highest scoring entities computed step using comp signiﬁciantly accurate given single correct entities bolded. states parent location place birth body horn clause expresses path comp models path better better able knowledge infer head horn clause. generally consider horn clauses form path type relation predicted. focus horn clauses high precision deﬁned sampled random walks social networks training examples different goal classify nodes network. bordes embed paths relation vectors question answering. approach unique modeling denotation intermediate step path query using information regularize spatial arrangement entity vectors. introduced task answering path queries incomplete knowledge base presented general technique compositionalizing broad class vector space models. experiments show compositional training leads state-ofthe-art performance path query answering knowledge base completion. several ideas paper regularization augmenting dataset paths representing sets low-dimensional vectors context-sensitive performing function composition using vectors. believe three could greater applicability development vector space models knowledge representation inference. acknowledgments would like thank gabor angeli fruitful discussions anonymous reviewers valuable feedback. gratefully acknowledge support google natural language understanding focused program national science foundation graduate research fellowship grant dge.", "year": 2015}