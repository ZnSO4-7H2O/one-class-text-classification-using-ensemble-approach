{"title": "New Error Bounds for Solomonoff Prediction", "tag": ["cs.AI", "cs.LG", "I.2.6; F.1.3; E.4; F.2"], "abstract": "Solomonoff sequence prediction is a scheme to predict digits of binary strings without knowing the underlying probability distribution. We call a prediction scheme informed when it knows the true probability distribution of the sequence. Several new relations between universal Solomonoff sequence prediction and informed prediction and general probabilistic prediction schemes will be proved. Among others, they show that the number of errors in Solomonoff prediction is finite for computable distributions, if finite in the informed case. Deterministic variants will also be studied. The most interesting result is that the deterministic variant of Solomonoff prediction is optimal compared to any other probabilistic or deterministic prediction scheme apart from additive square root corrections only. This makes it well suited even for difficult prediction problems, where it does not suffice when the number of errors is minimal to within some factor greater than one. Solomonoff's original bound and the ones presented here complement each other in a useful way.", "text": "solomonoﬀ sequence prediction scheme predict digits binary strings without knowing underlying probability distribution. call prediction scheme informed knows true probability distribution sequence. several relations universal solomonoﬀ sequence prediction informed prediction general probabilistic prediction schemes proved. among others show number errors solomonoﬀ prediction ﬁnite computable distributions ﬁnite informed case. deterministic variants also studied. interesting result deterministic variant solomonoﬀ prediction optimal compared probabilistic deterministic prediction scheme apart additive square root corrections only. makes well suited even diﬃcult prediction problems suﬃce number errors minimal within factor greater one. solomonoﬀ’s original bound ones presented complement useful way. induction process predicting future past precisely process ﬁnding rules data using rules guess future data. induction principle subject long philosophical controversies. highlights epicurus’ principle multiple explanations occams’ razor principle bayes’ rule conditional probabilities solomonoﬀ elegantly uniﬁed aspects formal theory inductive inference. theory allows prediction digits binary sequences without knowing true probability distribution contrast call informed scheme true distribution known. ﬁrst error estimate also given solomonoﬀ years later states total means squared distance prediction probabilities solomonoﬀ informed prediction bounded kolmogorov complexity true distribution. corollary theorem ensures solomonoﬀ prediction converges informed prediction computable sequences limit. result justifying solomonoﬀ prediction long sequences complexity. another natural question relations total number expected errors solomonoﬀ prediction total number prediction errors informed scheme. unfortunately bound terms satisfactory way. example exclude possibility inﬁnite even ﬁnite. want prove upper bounds terms ensuring corollary case cannot happen. hand theorem much convergence solomonoﬀ informed prediction. solomonoﬀ’s bounds complement nice way. preliminary section give notations strings conditional probability distributions strings. furthermore introduce kolmogorov complexity universal probability take care make latter true probability measure. section deﬁne general probabilistic prediction scheme solomonoﬀ informed prediction special cases. give several error relations prediction schemes. bound error diﬀerence −eµ| solomonoﬀ informed prediction central result. relations simple interesting consequences known results euclidean bound. section study deterministic variants solomonoﬀ informed prediction. give similar error relations probabilistic case prediction schemes. interesting consequence system optimal compared probabilistic deterministic prediction scheme apart additive square root corrections only. excellent introduction kolmogorov complexity solomonoﬀ induction consult book vit´anyi article short course. historical surveys inductive reasoning/inference found greek letters probability measures underline arguments indicate probability arguments. probability sequence starts x...xn. drop index clear arguments also need conditional probabilities derived bayes’ rule. prefer notation preserves order words contrast standard notation ﬂips extend deﬁnition conditional case following convention arguments underlined argument probability variable nonunderlined arguments represent conditions. convention bayes’ rule following look ﬁrst equation states probability string x...xn− followed equal probability string starts x...xn divided probability string starts x...xn−. second equation ﬁrst applied times. choose universal monotone turing machine unidirectional input output tapes bidirectional work tape. deﬁne preﬁx kolmogorov complexity length shortest program outputs string universal semi-measure deﬁned probability output universal turing machine starts provided fair coin ﬂips input tape. easy equivalent formal deﬁnition minimal programs outputs string starting might non-terminating. important universality property majorizes every computable probability measure multiplicative factor depending kolmogorov complexity function like deﬁned length shortest self-delimiting coding turing machine computing function. unfortunately probability measure binary strings. programs output followed neither stop printing continue forever without output. drawback easily corrected. deﬁne universal probability measure deﬁning ﬁrst conditional probabilities every inductive inference problem brought following form given string give guess continuation assume strings continued drawn according probability distribution. section consider probabilistic predictors next string. true probability measure string probability system predicts successor x...xn−. interested probability next itself. want system output either probabilistic strategies useful game theory called mixed strategies. keep ﬁxed compare diﬀerent interesting quantities probability making error predicting given x<n. probability system predict ρ=−ρ. happens probability analogously probability making wrong prediction step another popular keep sacriﬁce axioms probability theory. reason although computable least enumerable. hand interested conditional probabilities derived longer enumerable anyway reason stick still computable limit approximable. known natural choice call informed prediction scheme. probability high system predicts high probability. unknown could universal distribution deﬁned known solomonoﬀ prediction interested upper bound µ-expected number errors ξ-predictor. might also interested probability diﬀerence predictions step µξ-predictor total absolute diﬀerence power reason directly study relations alone follow ﬁnite ﬁnite. assume could choose would ﬁnite would inﬁnite without violating theorems prominent n→∞−→ probability page however neither settles question. following show ﬁnite causes ﬁnite e∞ξ. independent xk+n. exchanged transforms product inside logarithm. last equality used second form bayes rule universality i.e. ﬁnal inequality yielded basis error estimates. theorem binary sequences xx... drawn probability ﬁrst bits. ρ-system predicts deﬁnition probability error probability prediction µ-expected total number errors ﬁrst predictions following error relations hold universal solomonoﬀ informed general predictions first ensures ﬁniteness number errors solomonoﬀ prediction informed prediction makes ﬁnite number errors. especially case deterministic case. solomonoﬀ prediction makes ﬁnite number errors computable sequences. complicated probabilistic environments even ideal informed system makes inﬁnite number errors relation well-known euclidean bound upper bound theorem remains ﬁnite enµ/ρ ensures convergence individual prediction probabilities relation shows system makes least half errors system. relation improves lower bounds together upper bound says excess errors compared larger. result plausible since knowing means additional information saves making errors. information content quantiﬁed terms relative entropy relation states prediction scheme less half errors system whatever take ensures optimality apart factor combining ensures optimality solomonoﬀ prediction apart factor additive square root corrections note even comparing computability counts whereas might even uncomputable probabilistic predictor. optimality within factor might suﬃcient applications especially ﬁnite enµ/n inacceptable others. next section consider deterministic prediction factor occurs. proof theorem ﬁrst inequality follows directly deﬁnition triangle inequality. second inequality start modestly constants satisfy linear inequality last section several relations derived number errors universal ξ-system informed µ-system arbitrary ρ-systems. probabilistic predictors sense given output certain probabilities. section interested systems whose output input deterministically distinguish case true distribution known unknown. probabilistic scheme studied system. given probabilistic predictor easy construct deterministic predictor following probability predicting larger deterministic predictor always chooses analogously deﬁne analogously last section draw binary strings randomly distribution deﬁne probability system makes erroneous prediction step total µ-expected number errors ﬁrst predictions theorem binary sequences drawn probability ﬁrst bits. ρ-system predicts deﬁnition probability deterministic system always predicts otherwise. error probability prediction total µ-expected number errors ﬁrst predictions following relations hold best prediction scheme possible compared probabilistic deterministic prediction error expectation enθµ smaller every single step hence total number errors also. surprising nearly obvious system always predicts highest probability. known system always preferred prediction scheme even informed prediction system. combining leads bound number prediction errors deterministic variant solomonoﬀ prediction. computable prediction scheme fewer errors system whatever take apart example. consider critical example. want predict outcome colored black white faces white black. game becomes interesting second complementary black four white sides. dealer throws dice uses according deterministic rule. stake every round; return every correct prediction. selection strategy reﬂected complicated prediction system reaches winning zone thousand rounds. number rounds really small expected proﬁt round order magnitude smaller return. leads constant orders magnitude size front stated otherwise large stochastic noise makes diﬃcult extract signal i.e. structure rule furthermore bound turnaround value true expected turnaround might smaller. proved several error bounds solomonoﬀ prediction terms informed prediction terms general prediction schemes. theorem corollary summarize results probabilistic case theorem corollary deterministic case. shown probabilistic case asymptotically bounded twice number errors prediction scheme. deterministic variant solomonoﬀ prediction factor absent. well suited even diﬃcult prediction problems error probability eθξ/n converges rapidly minimal possible error probability eθµ/n. reduced problem showing since latter quadratic symmetric maximum thus suﬃcient check boundary values non-negative putting everything together proved", "year": 1999}