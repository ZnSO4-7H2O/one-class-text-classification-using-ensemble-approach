{"title": "Feature selection with test cost constraint", "tag": ["cs.AI", "cs.LG"], "abstract": "Feature selection is an important preprocessing step in machine learning and data mining. In real-world applications, costs, including money, time and other resources, are required to acquire the features. In some cases, there is a test cost constraint due to limited resources. We shall deliberately select an informative and cheap feature subset for classification. This paper proposes the feature selection with test cost constraint problem for this issue. The new problem has a simple form while described as a constraint satisfaction problem (CSP). Backtracking is a general algorithm for CSP, and it is efficient in solving the new problem on medium-sized data. As the backtracking algorithm is not scalable to large datasets, a heuristic algorithm is also developed. Experimental results show that the heuristic algorithm can find the optimal solution in most cases. We also redefine some existing feature selection problems in rough sets, especially in decision-theoretic rough sets, from the viewpoint of CSP. These new definitions provide insight to some new research directions.", "text": "feature selection important preprocessing step machine learning data mining. real-world applications costs including money time resources required acquire features. cases test cost constraint limited resources. shall deliberately select informative cheap feature subset classiﬁcation. paper proposes feature selection test cost constraint problem issue. problem simple form described constraint satisfaction problem backtracking general algorithm eﬃcient solving problem medium-sized data. backtracking algorithm scalable large datasets heuristic algorithm also developed. experimental results show heuristic algorithm optimal solution cases. also redeﬁne existing feature selection problems rough sets especially decision-theoretic rough sets viewpoint csp. deﬁnitions provide insight research directions. keywords feature selection cost-sensitive learning constraint satisfaction problem backtracking algorithm heuristic algorithm decision-theoretic rough sets. many data mining approaches employ feature selection techniques speed learning improve model quality techniques especially important datasets tens hundreds thousands features attribute reduction special type feature selection problems studied rough society. reduct feature subset jointly suﬃcient individually necessary preserve certain information data decision making often addressed information positive region respect decision class objective classical reduct problem minimal reduct since simpler representation often provides better generalization ability according occam’s razor principle. feature selection problems ﬁnding feature subsets maximal margin maximal stability minimal space etc. problems assume data already stored datasets available without charge. however data free real-world applications. test costs money time resources obtain feature values objects. example takes time money obtain medical data patient context would like select cheapest reduct consideration parallel test assumption motivated minimal test cost reduct problem recently number algorithms developed deal problem related issues also identiﬁed addressing numerical features observational errors test costs relationships problems searching cheapest feature subset preserves suﬃcient information classiﬁcation. nevertheless available resource usually limited users sacriﬁce necessary information keep test cost budget. paper introduces feature selection test cost constraint problem formulate issue. upper bound available resource serves constraint. fstc problem general fact problems coincide constraint less test cost optimal reduct. constraint tight suﬃciency condition cannot cannot obtain reduct. problem falls feature selection instead attribute reduction. paper fstc problem deﬁned viewpoint constraint satisfaction problem words deﬁned four aspects namely input output constraint optimization objective. deﬁnition simpler easier comprehend deﬁned viewpoint family furthermore redeﬁne classical reduct problem minimal reduct problem viewpoint. show feature selection problems rough sets including decisiontheoretic rough sets viewed extensions minimal reduct problem four aspects. viewpoint gives insight meaningful research trends concerning feature selection broader sense. fact similar viewpoints including optimization viewpoint attribute reduction dtrs discussed compared them presented systematic. develop backtracking algorithm fstc problem small medium-sized datasets. backtracking algorithms natural eﬀective approaches csps obtaining optimal solutions. however seldom employed deal feature selection problems rough theory discernibility matrix based approaches popular possible reason people deﬁned attribute reduction problems explicitly csps. exhaustive algorithm backtracking algorithm time complexity exponential respect number features. also develop heuristic algorithm polynomial time complexity large datasets. employ addition-deletion approach design heuristic function based information gain often employed similar problems similar proposed prefer cost features λ-weighting user speciﬁed parameter. diﬀerence algorithm employed lies stopping criteria. improve performance algorithm employ competition strategy strategy diﬀerent feature subsets obtained setting diﬀerent values best selected. strategy trade quality result time. importantly strategy user involved setting instead values valid dataset speciﬁed algorithm. four open datasets employed study performance algorithms. experimental results show backtracking algorithm eﬃcient medium-sized data. takes less second obtain optimal feature subset mushroom dataset contains features objects. backtracking algorithm approximately times faster sesra based another deﬁnition problem. heuristic algorithm stably eﬃcient backtracking one. help competition strategy heuristic algorithm optimal solution cases. rest paper organized follows section presents problem deﬁnition. classical reduct problem minimal test cost reduct problem also redeﬁned. section proposes backtracking heuristic algorithms. experimental results four datasets discussed section section studies existing feature selection problems rough society viewpoint csp. interesting problems also brieﬂy discussed. finally section presents concluding remarks research directions. section reviews three feature selection problems rough sets. classical rough sets last concerned test cost problems redeﬁned csps. moreover propose problem called feature selection test cost constraint. deﬁnition indicates reduct jointly suﬃcient individually necessary preserving particular property decision system words constraint named suﬃciency necessity respectively. consequently problem obtaining reduct deﬁned style follows. problem optimization objective typical csp. note constraint namely suﬃciency. indicate necessity constraint met. fact necessity constraint derived optimization objective. easily prove contradiction. superﬂuous features size feature subset cannot minimal. words problem deﬁnition simpliﬁed viewed csp. test cost important issue many applications. built hierarchy test-cost-sensitive decision systems present simple model used deﬁning problem paper. diﬀerences problem problem ﬁrst input test cost external information. second optimization objective minimize test cost instead number features. sometimes given limited resources obtain feature values. proposed issue optimal sub-reduct address issue. positive region instead conditional information entropy deﬁne respective concepts. deﬁnition equation ensures constraint met; equation ensures informative feature subset selected; equation ensures test cost minimized. problem constructing called optimal sub-reducts test cost constraint problem unfortunately deﬁnition rather prolonged hard read. next follow style problem present following problem. note objectives equally important. primary secondary objectives respectively. fact problem osrt problem. however problem deﬁnition simpler easier comprehend. phenomenon indicates form appropriate kind problems. comparing problems observe following diﬀerences. first constraint expressed test cost instead positive region. second ﬁrst objective problem maximize positive region. third objective problem becomes secondary objective problem objective considered primary achieved. section presents algorithms. backtracking algorithm heuristic algorithm. backtracking algorithm always produces optimal solution problem. heuristic algorithm eﬃcient large datasets however feature subset obtained optimal. backtracking algorithm natural solution csp. rough society people seldom employ algorithm attribute reduction. partly form problem deﬁnition shown deﬁnition backtracking algorithm fstc problem illustrated algorithm algorithm backtracking algorithm fstc problem input selected feature subset feature index lower bound output results stored global variable method backtracking algorithm lines check constraint. feature subsets violating constraint simply discarded. lines indicate positive region current feature subset namely suﬃciency condition fstc problem coincides problem. case need address problem. lines osb| devoted optimization objective. serves ﬁrst objective. serves second; checked osb. implementation coser analyze time complexity. number feature subsets |c|. worst case checked. hand feature subset never checked twice. therefore number backtracking steps namely number time backtracking method invoked bounded |c|. indicated line time need compute feature subset feature. computation involves splitting dataset design algorithm often closely related problem deﬁnition. algorithm easily obtained problem similarly sesra algorithm three main steps indicated deﬁnition phenomenon shows inﬂuence problem viewpoint problem deﬁnition algorithm design. backtracking algorithm scalable. indicated equation time exponential respect number features worst case. hence need design heuristic algorithms large datasets. adopt well known addition-deletion approach design algorithm since deletion approach ineﬃcient large datasets frequently. tested approach four datasets listed voting tic-tac-toe datasets feature alone produces positive region therefore approach fails given test cost setting. feasible heuristic information information gain generally feature subset less information entropy tends produce bigger positive region. therefore employ information gain paper design algoalgorithm listed algorithm algorithm ﬁrst constructs feature subset meeting constraint minimal information entropy lines lines necessary however help speeding algorithm. redundant features removed viewpoint positive region lines algorithm successful example remove dataset algorithm also fails. make matter worse decision tree encounters problem. might drawback heuristic algorithms compared exhaustive ones. fortunately extreme case seldom happens applications. many datasets tested algorithm never fails construct feature subset. adopt competition strategy working follows. first speciﬁes values obtains corresponding feature subsets using algorithm ﬁnally chooses feature subset maximal positive region minimal test cost. since feature subsets produced diﬀerent values compete winner strategy called competition strategy number notes make. counting number features decision included. missing values treated particular value. equal itself unequal value. animal name feature useful dataset simply remove datasets library provide test cost information. statistical purposes need produce them. diﬀerent test cost distributions correspond diﬀerent applications. three distributions namely uniform distribution normal distribution pareto distribution discussed simplicity paper employs uniform distribution generate random test cost according deﬁnition tci-ds diﬀerent test cost settings diﬀerent. sense produce many tci-ds needed given need know eﬃciency backtracking algorithm three viewpoints. ﬁrst average time complexity. need know whether number backtracking steps exponential respect number features. second time taken small medium-sized data. fact diagnosis data particular disease hospital contain thousands instances. datasets optimal solution always required. third time compared exhaustive approaches. backtracking algorithm compared sesra sesra∗ proposed sesra based deﬁnition sesra∗ enhanced version. table shows number backtracking steps namely many times backtracking method invoked. denote number. size backtracking tree hence also upper bound voting dataset sometimes therefore maximal close indicates sometimes exponential respect |c|. contrast mushroom dataset table compares performance backtracking algorithm sesra sesra∗ terms time. backtracking algorithm takes mushroom voting datasets respectively. words appropriate many real applications. moreover backtracking algorithm stably outperforms sesra sesra∗. time taken tic-tac-toe mushroom datasets compared sesra. results show advantage viewpoint. convenience time heuristic algorithm also listed table heuristic algorithm always eﬃcient exhaustive algorithms. eﬃciency diﬀerence becomes signiﬁcant time exhaustive algorithms long. moreover eﬃciency depends dataset size compare performance three approaches mentioned section three based algorithm ﬁrst approach called nonweighting approach implemented setting second approach general results depicted fig. observe following. first approach without taking considering test cost performs poorly. cases cannot optimal feature subset. second specify appropriately namely results acceptable. likely optimal feature subset. however discussed earlier often idea specify third performance competition strategy much better two. cases produces optimal feature subset. moreover user know optimal setting word extra computation resource consumed competition strategy worthwhile. problems provide viewpoint feature selection. existing feature selection problems rough sets viewed extensions problem following aspects input output constraint optimization objective. analyze aspect follows. conditional features numeric. numeric data quite diﬀerent symbolic ones employed pawlak rough sets coverings instead partitions formed according feature sets. covering-based rough sets deal reduction coverings. neighborhood rough model generates neighborhood systems data. data uncertain. uncertainty data caused noise observational error error range based covering rough model proposed deal observational error. another well known data model might interval-valued fuzzy sets studied rough sets external information features feature subsets information subjective expressed user preference. example features ranked user even directly speciﬁed expert information objective. example weight test cost feature number possible extensions weight computation feature subset. additive average maximal minimal extensions data models concerning test cost relationships among features deﬁned. external information classiﬁcation. widely adopted information might misclassiﬁcation cost dtrs consider loss functions concerning diﬀerent classiﬁcations. classiﬁcations correspond positive negative boundary rules. cost misclassiﬁcations correct classiﬁcations. second extensions concerning output. people considered generalized reduct problems attribute value reduction discretization symbolic value partition since features transformed combined problems called feature extraction instead third extensions concerning constraint. many still expressed form problem however deﬁnitions positive region diﬀerent change input data model. others expressed diﬀerent forms. computation positive region follows variable precision rough model bayesian rough model userspeciﬁed parameter indicate admissible classiﬁcation error. pawlak rough sets viewed special case variable precision rough sets extension inspired fruitful research works concerning probabilistic rough sets β-lower distribution β-upper distribution closely studied. computation positive region follows neighborhood rough model error range based covering rough model neighborhood rough model positive regions also rely user speciﬁed parameter distance threshold. error range based covering rough model positive regions also rely error ranges data. error ranges determined testing instruments therefore objective. tional information entropy respect conditional information entropy constraint stricter positive region constraint. feature subset meeting positive region constraint meet conditional information entropy constraint. reverse hold. constraints equivalent decision system consistent minimize cost. test cost sensitive decision systems objective minimize total test cost misclassiﬁcation cost sensitive decision systems objective minimize average misclassiﬁcation cost risk decision systems test cost misclassiﬁcation cost objective minimize total cost features values likely selected. features however weaker generalization ability features less values. objective help amend drawback. domains features size objective coincides problem maximize stability. dynamic reducts stable process decision table sampling. decision rules computed dynamic reducts reliable. parallel reducts follow idea. maximize margin. margin geometric measure evaluating conﬁdence classiﬁer respect decision unlike metric positive region conditional information entropy problems mentioned longer reduct problems. input changed indiscernibility relation exist. consider weaker relations similarity relation constraint changed positive region computed computed pawlak approach reducts subject conditional information entropy constraint pawlak reduct. optimization objective changed optimal reduct minimal. feature subset minimal total cost reduct all. feature selection dtrs test cost. note external information dtrs cannot expressed misclassiﬁcation matrix. test cost also external information. considering external information problem interesting challenging. feature selection positive region constraint. even simpler representation require positive region preserved certain degree. example feature subset positive region original. note problem diﬀerent variable precision rough model deﬁnition positive region changed. motivations however quite similar deal overﬁtting issue. minimal test cost feature selection positive region constraint. problem diﬀers last objective feature subset least cost. hybrid last problem problem. also viewed dual problem fstc problem. problems combinations existing extensions involve extensions. observe number possible combinations many certain application areas. words much research issues opened viewpoint. paper proposed feature selection problem concerning test cost constraint. problem called fstc wide application area resource aﬀord often limited. backtracking heuristic algorithms designed experimental results showed eﬃciency backtracking algorithm compared existing ones eﬀectiveness competition strategy based λ-weighted heuristic algorithm. noted competition strategy know optimal setting instead specify values valid dataset. important contribution paper viewpoint feature selection rough sets. viewpoint feature selection problems natural generalizations minimal reduct problem. viewpoint helps identify meaningful problems following aspects input output constraint optimization objective. summary paper indicated important research trends concerning feature selection beyond rough sets. work supported part national natural science foundation china grant natural science foundation fujian province china grant nos. science technology project fujian province china grant", "year": 2012}