{"title": "Pattern representation and recognition with accelerated analog  neuromorphic systems", "tag": ["q-bio.NC", "cs.NE", "stat.ML"], "abstract": "Despite being originally inspired by the central nervous system, artificial neural networks have diverged from their biological archetypes as they have been remodeled to fit particular tasks. In this paper, we review several possibilites to reverse map these architectures to biologically more realistic spiking networks with the aim of emulating them on fast, low-power neuromorphic hardware. Since many of these devices employ analog components, which cannot be perfectly controlled, finding ways to compensate for the resulting effects represents a key challenge. Here, we discuss three different strategies to address this problem: the addition of auxiliary network components for stabilizing activity, the utilization of inherently robust architectures and a training method for hardware-emulated networks that functions without perfect knowledge of the system's dynamics and parameters. For all three scenarios, we corroborate our theoretical considerations with experimental results on accelerated analog neuromorphic platforms.", "text": "abstract—despite originally inspired central nervous system artiﬁcial neural networks diverged biological archetypes remodeled particular tasks. paper review several possibilites reverse architectures biologically realistic spiking networks emulating fast lowpower neuromorphic hardware. since many devices employ analog components cannot perfectly controlled ﬁnding ways compensate resulting effects represents challenge. here discuss three different strategies address problem addition auxiliary network components stabilizing activity utilization inherently robust architectures training method hardwareemulated networks functions without perfect knowledge system’s dynamics parameters. three scenarios corroborate theoretical considerations experimental results accelerated analog neuromorphic platforms. artiﬁcial neural networks rank among successful classes machine learning models superﬁcial similarities sensory processing pathways cortex notwithstanding difﬁcult biologically realistic spiking neural networks. nevertheless argue reverse mapping worthwhile reasons. first could help understand information processing brain assuming follows similar computational principles. second enables machine learning applications fast low-power neuromorphic architectures speciﬁcally developed mimic biological neuro-synaptic dynamics. manuscript discuss several ways answer consider challenge neuromorphic architectures analog components possible design spiking architectures training methods amenable neuromorphic implementation remain functionally performant despite substrate-inherent imperfections? speciﬁcally review three different approaches ﬁrst based recent insights networks spiking neurons constructed sample predeﬁned joint probability distributions distributions learned data networks automatically build internal generative model straightforward pattern recognition memory recall practical problems arise hardware dynamics parameter ranges incompatible target speciﬁcations network inevitably distort sampled distribution. ﬁrst approach involves addition auxiliary network components order make robust hardware-induced distortions second restricts network topology endows immunity effects demonstrate effectiveness approaches spikey neuromorphic system third strategy maps traditional feedforward architectures trained ofﬂine backpropagation algorithm network spiking neurons neuromorphic device here good performance additional learning phase parameters trained hardware loop using abstract network description approximation parameter updates. show approach restore network functionality despite incomplete knowledge gradient along parameters need descend. experiments performed brainscales neuromorphic system networks small compared used contemporary machine learning applications showcase potential using accelerated analog neuromorphic systems pattern representation recognition. particular used neuromorphic systems operate times faster biological archetypes thereby signiﬁcantly speeding training practical application. following neural network activity interpreted sampling underlying probability distribution binary random variables mapping spikes states deﬁned spike times neuron τref absolute refractory period using leaky integrateand-ﬁre neurons poisson background noise used achieve high-conductance state stochastic fig. sampling neurons. exemplary membrane potential traces mapping refractory/non-refractory neuron states states binary rvs. exemplary structure subset units biases connected weights highlighted exemplify neuromorphic network structure subplot sketch sampling subnetworks representing binary rvs. subnetwork consists principal neuron associated synﬁre chain implements refractoriness coupling sampling units exemplary spike activity sampling unit membrane potential target sampled distribution spikey chip. evolution kullback-leibler divergence sampled target distribution multiple experimental runs. time given biological units. logistic function represents noise-free membrane potential neuron. parameters controlled intensity background noise. appropriate settings synaptic weights bias parameters networks trained sample boltzmann distributions particular requires neurons instantaneously transmit states postsynaptic partners. physical system assumption necessarily violated degree since signal transmission never instantaneous. particular case accelerated neuromorphic hardware synaptic transmission delays become even problematic order magnitude state-encoding refractory times themselves. furthermore required equivalence post-synaptic potential durations refractory states violated either unstable. spikey example refractory times relative spike-to-spike variations στref /τref kinds timing mismatch pose fundamental problem implementation spiking accelerated analog substrates. here alleviate issue substrate-induced timing mismatches using recurrent network structure represents small subnetwork called sampling unit. subnetworks built refractory times well controlled addition intra-unit refractory states inter-unit state communication across network inseparably coupled sampling units consist single principle neuron small synﬁre chain excitatory inhibitory populations stage project populations following stage thereby relaying activity pulse forward direction. project backwards ensuring neurons previous stages spike once. additionally last also project onto large weights. therefore elicits spike sequentially pull membrane potential close inhibitory reversal potential prohibiting ﬁring long synﬁre chain active pulse reached ﬁnal synﬁre stage pulls pn’s membrane potential back equilibrium value. total duration pseudo-refractory period controlled synﬁre chain length parameters. addition controlling refractoriness synﬁre chains also mediate interaction pns. connections synﬁre chain simply mirror connections guarantees match effective interaction durations pseudo-refractory periods. correct synapse parameter settings determined iterative training procedure results hardware emulation seen fig. network four sampling units trained spikey sample target boltzmann distribution. training network needs biological time achieve good match sampled target distribution. considering hardware acceleration factor happens wall-clock time. discussed previous section sampling networks ostensibly sensitive different types hardwareinduced timing mismatch. subsection discuss sampling network model made robust imposing hierarchy onto network structure equivalent moving general restricted addition making operation robust discuss below hierarchization distinct advantage signiﬁcantly speeding training. fig. robustness structure hierarchical networks. hierarchical spiking network emulating rbm. effects hardware-induced distortions classiﬁcation rate network. test image presented duration green training data blue test data brown mean value range distortions measured spikey. error bars represent trial-to-trial variations. synaptic transmission delays. spike-to-spike variability refractory times. membrane time constant. synaptic weight discretization. comparison classiﬁcation rates three scenarios software simulation ideal distortion-free case software simulation combined hardware-induced distortions measured spikey hybrid emulation hidden layer spikey light colors training data dark colors test data. emulate construct hierarchical network model layers visible layer representing data hidden layer learns particular motifs data label layer classiﬁcation network trained contrastive learning rule speciﬁc inﬂuence various hardware-induced distortion mechanisms ﬁrst studied complementary software simulations. simulations show classiﬁcation accuracy network essentially unaffected types timing mismatch discussed above even amplitudes much larger measured neuromorphic substrate order facilitate meaningful comparison hardware experiments distortion mechanisms studied. upper limit membrane conductance prevent neurons entering high-conductance state thereby distorting activation functions away ideal logistic shape consequently modifying sampled distribution. however within range achievable spikey effect classiﬁcation accuracy remains small largest effect stems discretization synaptic weights resolution bits spikey robustness hierarchical architecture timing mismatches consequence training procedure information within network. training effect creating steep energy landscape deep energy minima corresponding particular learned digits represent strong attractors system placed classiﬁcation clamping visible layer. throughout duration attractor visible neurons represent pixels constant intensity encoded spiking probability thereby entering quasi-rate-based information representation regime. therefore information provide hidden layer unaffected temporal shifts zero-mean noise. outnumber hidden neurons effectively control state hidden layer. hidden layer neurons unaffected timing mismatches interconnected. secondorder lateral interactions indeed distorted mediated label neurons relative strength weak play critical role. ﬁndings corroborated experiments spikey system’s limitations used hybrid approach visible label layers implemented software hidden layer running spikey. ideal undistorted case network classiﬁcation performance test set. reduced distortive effects simultaneously present software simulations. comparison hybrid emulation showed performance closely matched software results within trial-to-trial variability. stress result direct-tohardware mapping additional training compensate hardware-induced distortions sec. used training procedure based optimize hardware-emulated sampling network. simple contrastive learning rules yield good classiﬁcation performance networks spiking neurons anclass highly successful learning algorithms based error backpropagation. this however requires precise knowledge gradient cost function respect network parameters difﬁcult achieve analog hardware. propose training method hardwareemulated networks circumvents problem using cost function gradient respect parameters approximation true gradient respect hardware parameters similar method previously used network training digital neuromorphic device essential advantage employed neuromorphic platforms provided accelerated dynamics. despite possible losses performance compared precisely tunable software solutions accelerated analog neuromorphic systems potential vastly outperform classical simulations neural networks terms speed energy consumption invaluable advantage online learning complex real world data sets. network sec. example already faster equivalent software simulations several orders magnitude. studied networks serve proof principle scalable larger network sizes. future research address whether results obtained small networks still hold training tasks increase complexity. furthermore generative properties described hierarchical networks remain studied. another major step forward taken training take place entirely hardware thereby rendering sequential reconﬁgurations individual experiments unnecessary. future generations used systems feature onboard plasticity processor units early-stage experiments already showing promising results petrovici schroeder breitwieser grübl schemmel meier robustness structure fast inference neuromorphic device hierarchical networks ijcnn schmitt klähn bellec grübl güttler hartel neuromorphic hardware loop training deep spiking network brainscales wafer-scale system ijcnn buesing bill nessler maass neural dynamics sampling model stochastic computation recurrent networks spiking neurons plos computational biology vol. petrovici bill bytschok schemmel meier stochastic inference spiking neurons high-conductance state physical review vol. leng petrovici martel bytschok breitwieser spiking neural networks superior generative discriminative models cosyne abstracts salt lake city february pfeil grübl jeltsch müller müller petrovici schmuker networks universal neuromorphic computing substrate frontiers neuroscience vol. schemmel brüderle grübl hock meier millner wafer-scale neuromorphic hardware system large-scale neural modeling proceedings ieee iscas in-the-loop training. structure feed-forward rate-based fig. deep spiking network. schematic training procedure hardware loop. classiﬁcation accuracy training step. left software training phase right hardware in-the-loop training phase. here denote target network state label layer respectively runs samples within minibatch learned parameters translated feed-forward spiking neural network here brainscales wafer-scale system used network emulation. hardware imperfections classiﬁcation accuracy dropped mapping network hardware substrate. second training phase hardware-emulated network trained loop several iterations. parameter updates calculated using gradient descent rule activation layers measured hardware. rationale behind approach activation function unit sufﬁciently similar neuron allow using computed gradient approximation true hardware gradient. seen fig. assumption validated post-training performance hardware-emulated network training iterations classiﬁcation accuracy increased back reviewed three strategies emulating performant spiking network models analog hardware. proposed methods tackled problems induced substrateinherent imperfections different angles. three strategies implemented evaluated different analog hardware systems.", "year": 2017}