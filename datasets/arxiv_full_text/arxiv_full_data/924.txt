{"title": "Churn analysis using deep convolutional neural networks and autoencoders", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "Customer temporal behavioral data was represented as images in order to perform churn prediction by leveraging deep learning architectures prominent in image classification. Supervised learning was performed on labeled data of over 6 million customers using deep convolutional neural networks, which achieved an AUC of 0.743 on the test dataset using no more than 12 temporal features for each customer. Unsupervised learning was conducted using autoencoders to better understand the reasons for customer churn. Images that maximally activate the hidden units of an autoencoder trained with churned customers reveal ample opportunities for action to be taken to prevent churn among strong data, no voice users.", "text": "true corporation public company limited bangkok thailand department electrical engineering faculty engineering king mongkut's university technology thonburi bangkok thailand international business machines corporation singapore customer temporal behavioral data represented images order perform churn prediction leveraging deep learning architectures prominent image classification. supervised learning performed labeled data million customers using deep convolutional neural networks achieved test dataset using temporal features customer. unsupervised learning conducted using autoencoders better understand reasons customer churn. images maximally activate hidden units autoencoder trained churned customers reveal ample opportunities action taken prevent churn among strong data voice users. deep learning convolutional neural networks demonstrated superior performance many image processing tasks order leverage advances predict churn take pro-active measures prevent represent customers images. specifically construct dimensional array normalized pixels column type behavior tracked type behavior include data usage amount frequency voice calls voice minutes messages etc. training testing data image also accompanied label churned churned. analysis examine prepaid customers particular. order determine labels specific dates image first define churn last call predictor window according customer’s lifetime-line best understood viewing fig. right left. first item churn assessment window chosen days. customer registers activity within days label active/not-churned. fig. green circle demarks label first top-most customer ltl. customer activity time frame label churned. second third ltls fig. next define last call latest call occurring -day last call window fig. call within window exclude customer analysis consider customer churned long able take pro-active retention measures. look days back last call define predictor window. used predictor window analyses here conceivable vary time frame yield improved results. note exact dates predictor window depend customer’s usage behavior want protocol prepare unlabeled data actual prediction. creating training testing images customer according customer method explained above feed deep cnns similar used successfully image classification. architecture shown fig. call dl-. architecture consists consecutive convolutional layers followed pooling layer fullyconnected layer units softmax output units binary classification. first convolutional layer involves four filters size pans across usage behavior column period seven days. chose seven days analyze customers’ weekly patterns across usage behavior type time. filter maintains shared weights biases throughout convolution commonly employed image processing. outputs convoluted second convolutional layer filters size across usage behavior features output first convolutional layer. filter intended analyze customers’ usage across variables given time. convolutions pooling layer size applied intended assist translational invariance next fully-connected layer flattens prepares data softmax output binary classifier. training testing architecture end-to-end yields results superior chaid decision tree model judging area-under-the-curve benchmark receiver operating curve commonly accepted benchmark comparing models; accounts true false positives note trained epochs using binary cross-entropy loss function rectified linear unit activation functions stochastic gradient descent backpropagation batch sizes adaptive learning rates comparing spss chaid model model although cases exhibit overfitting deep learning implementation superior training testing. tested various deep learning hyperparameters architectures found best results dl-. includes features topup count/amount comprises convolutional layer dropout followed pooling layer convolutional layer pooling layer fully-connected layer units dropout fully-connected layer units dropout fully-connected layer units dropout softmax output units binary classification. fully connected layers dropout appears reduce overfitting evident aucs training testing datasets table training less test significantly higher. note even though epochs used epochs still superior dl-. parameters identical dl-. discussed supervised learning order predict churn. understand customer behavioral patterns elucidate reasons churning apply unsupervised learning approaches autoencoders. autoencoders neural networks inputs outputs identical. used dimensionality reduction data performed better principal components analysis training autoencoder dataset used previously produce images maximally activate hidden units obtain dimensionally-reduced information. assume input norm constrained images entire customer base shown fig. image columns represent data represent voice calls download/upload volume/duration columns represent in/out. interpret real customer’s image approximately reconstructable linear superposition base images. evident second base image daily incoming marketing messages primary component customers. three different base images regarding data suggest data usage varies among customers therefore requires components represent. interested preventing customer churn train autoencoder subset customers churn. shown fig. found three distinguishing base images customers churn. case columns represent topup frequency/amount columns represent voice calls represent data download/upload third image suggests many customers churn consistent topup data usage outgoing throughout entire -day period also voice usage. leads believe many customers simply abandoning socially tied phone number. possible action prevent churn offer voice incentives promotions. also possible customers receiving adequate marketing messages maintain activity service. onclusion deep convolutional neural networks autoencoders prove useful predicting understanding churn telecommunications industry outperforming simpler models decision tree modeling. since temporal features used customer input images developed augmented features improve efficacy. another strategy improve pre-train weights deep convolutional neural network using stacked convolutional autoencoders demonstrated complex type model involving thousands variables possible. churn important problem address many industries internetsubscriptions-based services expect approach widely applicable adopted ways beyond covered here. urther implementation details deep learning computations performed dell poweredge ubuntu operating system installed. docker used deploy various systems development. computations performed open-source libraries including theano tensorflow keras training testing dataset together consists million customers randomly split respectively. churn rate consistent across datasets. ontributions true corporation provided data hardware. a.w. hardware software conceived autoencoder models scaled experiments million customers composed manuscript. c.b. software conceived customer image approach spss chaid model initial testing. a.w. c.b. o.l. r.p. contributed ideas reviewed manuscript. krizhevsky alex ilya sutskever geoffrey hinton. \"imagenet classification deep convolutional neural networks.\" advances neural information processing systems. szegedy christian yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. \"going deeper convolutions.\" proceedings ieee conference computer vision pattern recognition. russakovsky olga deng jonathan krause sanjeev satheesh sean zhiheng huang \"imagenet large scale visual recognition challenge.\" international journal computer vision scherer dominik andreas müller sven behnke. \"evaluation pooling operations convolutional architectures object recognition.\" artificial neural networks–icann springer berlin heidelberg hanley james barbara mcneil. \"the meaning area receiver operating characteristic curve.\" radiology bradley andrew \"the area curve evaluation machine learning algorithms.\" pattern recognition buja andreas werner stuetzle shen. \"loss functions binary class probability estimation classification structure applications.\"working draft november rumelhart david geoffrey hinton ronald williams. \"learning representations backpropagating errors.\" cognitive modeling zeiler matthew \"adadelta adaptive arxiv. hinton geoffrey nitish srivastava alex krizhevsky ilya sutskever ruslan salakhutdinov. \"improving neural networks preventing co-adaptation feature detectors.\" arxiv preprint arxiv. hinton geoffrey ruslan salakhutdinov. \"reducing dimensionality data neural networks.\" science http//deeplearning.stanford.edu/wiki/index.php/visualizing_a_trained_autoencoder visualizing trained autoencoder. ufldl tutorial masci jonathan ueli meier cireşan jürgen schmidhuber. \"stacked convolutional autoencoders hierarchical feature extraction.\" artificial neural networks machine learning– icann springer berlin heidelberg bergstra james olivier breuleux frédéric bastien pascal lamblin razvan pascanu guillaume desjardins joseph turian david warde-farley yoshua bengio. \"theano math expression compiler.\" proceedings python scientific computing conference vol. bastien frédéric pascal lamblin razvan pascanu james bergstra goodfellow arnaud bergeron nicolas bouchard david warde-farley yoshua bengio. \"theano features speed improvements.\" arxiv preprint arxiv. abadi tensorflow large-scale machine learning heterogeneous systems software available tensorflow.org. chollet françois https//github.com/fchollet/keras keras github", "year": 2016}