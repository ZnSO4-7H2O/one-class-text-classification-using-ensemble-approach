{"title": "Distributional semantics beyond words: Supervised learning of analogy  and paraphrase", "tag": ["cs.LG", "cs.AI", "cs.CL", "cs.IR", "H.3.1; I.2.6; I.2.7"], "abstract": "There have been several efforts to extend distributional semantics beyond individual words, to measure the similarity of word pairs, phrases, and sentences (briefly, tuples; ordered sets of words, contiguous or noncontiguous). One way to extend beyond words is to compare two tuples using a function that combines pairwise similarities between the component words in the tuples. A strength of this approach is that it works with both relational similarity (analogy) and compositional similarity (paraphrase). However, past work required hand-coding the combination function for different tasks. The main contribution of this paper is that combination functions are generated by supervised learning. We achieve state-of-the-art results in measuring relational similarity between word pairs (SAT analogies and SemEval~2012 Task 2) and measuring compositional similarity between noun-modifier phrases and unigrams (multiple-choice paraphrase questions).", "text": "several efforts extend distributional semantics beyond individual words measure similarity word pairs phrases sentences extend beyond words compare tuples using function combines pairwise similarities component words tuples. strength approach works relational similarity compositional similarity however past work required hand-coding combination function different tasks. main contribution paper combination functions generated supervised learning. achieve state-of-the-art results measuring relational similarity word pairs measuring compositional similarity nounmodiﬁer phrases unigrams harris firth hypothesized similar contexts tend words appear similar meanings. hypothesis foundation distributional semantics words represented context vectors. similarity words calculated comparing corresponding context vectors distributional semantics highly effective measuring semantic similarity individual words. eighty multiplechoice synonym questions test english foreign language distributional approach recently achieved accuracy however difﬁcult extend distributional semantics beyond individual words word pairs phrases sentences. moving beyond individual words various types semantic similarity consider. focus paraphrase analogy. paraphrase similarity meaning pieces text analogy similarity semantic relations sets words common study paraphrase sentence level prefer concentrate simplest type paraphrase bigram paraphrases unigram. example house paraphrase kennel. experiments concentrate noun-modiﬁer bigrams noun unigrams. analogies terms domain terms another domain familiar analogy solar system rutherfordbohr atomic model involves several terms domain solar system domain atomic model thing longer raw; decorate thing longer plain. semantic relations cook similar semantic relations decorate plain. following experiments focus proportional analogies. approaches beyond space words representation computed compared phrases sentences valcombining multiple third weighted inference rules integrate distributional similarity formal logic fourth single space integrates formal logic vectors taking second approach turney introduced dual-space model space measuring domain similarity another function similarity similarities beyond individual words calculated functions combine domain function similarities component words. dual-space model applied measuring compositional similarity relational similarity experiments tested sensitivity word order dual-space model performed signiﬁcantly better competing approaches limitation past work dual-space model combination functions handcoded. main contribution show handcoding eliminated supervised learning. ease reference call approach supersim modiﬁcation supersim speciﬁc task achieve better results previous hand-coded models. compositional similarity compares contiguous phrases sentences whereas relational similarity require contiguity. tuple refer contiguous noncontiguous word sequences. ilarity word pairs train supersim quadruples labeled positive negative examples analogies. example proportional analogy hcook decorate plaini labeled positive example. quadruple represented feature vector composed domain function similarities dual-space model features based corpus frequencies. supersim uses support vector machine learn probability quadruple consists word pair analogous word pair probability interpreted degree relational similarity given word pairs. also approach paraphrase supervised tuple classiﬁcation. measure compositional similarity beween m-gram n-gram train learning algorithm -tuples positive negative examples paraphrases. supersim learns estimate probability triple consists compositional bigram synonymous unigram instance phrase tank synonymous aquarium; tank aquarium high compositional similarity. triple hﬁsh tank aquariumi represented using features used analogy. probability triple interpreted degree compositional similarity given bigram unigram. review related work section general feature space learning relations compositions presented section experiments relational similarity described section section reports results compositional similarity. section discusses implications results. consider future work section conclude section semeval task concerned measuring degree relational similarity word pairs task examined degree semantic equivalence sentences. areas research mostly independent although socher turney present uniﬁed perspectives tasks. relational analysis) measures relalra spond word pairs columns correspond patterns connect pairs large corpus. holistic approach distributional similarity since word pairs opaque wholes; component words separate representations. compositional approach analogy representation word word pair represented composing representations member pair. given vocabulary words compositional approach requires representations handle possible word pairs holistic approach requires representations. holistic approaches scale required nine days run. bollegala answered analogy questions support vector machine trained quadruples here. however feature vectors holistic hence scaling problems. herda˘gdelen baroni used support vector machine learn relational similarity. feature vectors contained combination holistic compositional features. measuring relational similarity closely connected classifying word pairs according semantic relations semantic relation classiﬁcation focus semeval task semeval task beyond words many researchers take ﬁrst approach described single vector space used individual words phrases approach given words context vectors mitchell lapata experiment many different vector operations element-wise multiplication performs well. bigram represented however element-wise multiplication commutative bigrams vector experiments test order sensitivity element-wise multiplication performs poorly treat bigram unit single word construct context vector occurrences large corpus. holistic approach representing bigrams performs well limited bigrams speciﬁed advance scale many possible bigrams although holistic approach scale generate holistic bigram vectors train supervised regression model given bigram observed corpus regression model predict holistic vector observed separately. show section idea adapted train supersim without manually labeled data. socher take second approach described sentences compared combining multiple pairwise similarity values. construct variable-sized similarity matrix element similarity i-th phrase sentence j-th phrase other. since supervised learning simpler ﬁxed-sized feature vectors variable-sized similarity matrix reduced smaller ﬁxed-sized matrix allow comparison pairs sentences varying lengths. socher represent words phrases pair consisting vector matrix. vector captures meaning word phrase matrix captures word phrase modiﬁes meaning another word phrase combined. apply matrix–vector representation compositions relations. turney represents words vectors vector domain space vector function space. domain vector captures topic ﬁeld word function vector captures functional role word. dual-space model applied compositions relations. extend dual-space model turney ways hand-coding replaced supervised learning sets features augment domain function space. moving supervised learning instead hand-coding makes easier introduce features. dual-space model parameterized similarity measures provided input values handcrafted functions. task required different hand-crafted functions. parameters similarity measures tuned using customized grid search algorithm. grid search algorithm suitable integration supervised learning algorithm. insight behind supersim that given appropriate features supervised learning algorithm replace grid search algorithm hand-crafted functions. fearepresent tuple four tures based frequencies large corpus. ﬁrst type feature logarithm second type frequency word. positive pointwise mutual information third fourth similarities words domain function space following experiments ppmi matrix turney domain function matrices turney three matrices word frequency data based corpus collection pages gathered university sites containing words. three matrices word–context matrices rows correspond terms frequency corpus. deﬁne log+). corpus freq zero thus also zero. frequency features feature word n-tuple. second features consists positive pointwise mutual information values pair words n-tuple. ppmi matrix turney although computed singular value decomposition project vectors lower-dimensional space need original high-dimensional columns features. ppmi matrix rows columns density term wordnet corresponding ppmi matrix. unigram wordnet corresponding columns ppmi matrix marked left right. suppose corresponds i-th ppmi matrix corresponds j-th column marked left. value i-th j-th column ppmi matrix ppmi positive pointwise mutual information co-occurring corpus ﬁrst word left ignoring intervening stop words corresponding matrix ppmi value zero. turney estimated ppmi sampling corpus phrases containing looking left sampled phrases sampling process ppmi necessarily equal ppmi. example suppose rare word common word. ppmi sample phrases containing relatively likely phrases. ppmi sample phrases containing less likely phrases containing although theory ppmi equal ppmi likely unequal given limited sample. n-tuple select pairs generate features pair ppmi ppmi. thus ppmi values second features. third features consists domain space similarity values pair words n-tuple. domain space designed capture topic word. turney ﬁrst constructed frequency matrix rows correspond terms wordnet columns correspond nearby nouns. given term corpus sampled phrases containing phrases processed part-of-speech tagger identify nouns. noun closest noun left right frequency count i-th j-th column incremented. hypothesis nouns near term characterize topics associated term. term domain space represented vector ukσp parameter speciﬁes number singular values truncated singular value decomposition; number latent factors low-dimensional representation term generate deleting columns corresponding smallest singular values. parameter raises singular values power goes zero factors smaller singular values given weight. effect making similarity measure discriminating similarity words domain space computed extracting vectors ukσp correspond words calculating cosine. optimal performance requires tuning parameters task following experiments avoid directly tuning generating features variety values allowing supervised learning algorithm decide features use. pairs pair generate domain similarity features varies steps varies steps number values number values therefore features nknp pair xji. thus nnknp domain space similarity values third features. fourth features consists function space similarity values pair words n-tuple. function space designed capture functional role word. similar domain space except context based verbal patterns instead nearby nouns. hypothesis functional role word characterized patterns relate word nearby verbs. table summarizes four sets features size function number words given tuple. values considered constants. table shows number elements feature vector varies total number features believe acceptable growth scale comparing sentence pairs. four sets features hierarchical relationship. frequency features based counting isolated occurrences word corpus. ppmi features based direct co-occurrences words; ppmi greater zero words actually occur together corpus. domain function space capture indirect higherorder co-occurrence truncated values high even actually co-occur corpus. conjecture higher orders hierarchy would provide improved similarity measures. supersim learns classify tuples representing features. supersim uses sequential minimal optimization support vector machine implemented weka kernel normalized third-order polynomial. weka provides probability estimates classes ﬁtting outputs logistic regression models. section presents experiments learning relational similarity using supersim. training datasets consist quadruples labeled positive negative examples analogies. table shows feature vectors elements. experiment three datasets collection ﬁve-choice questions college entrance exam modiﬁed ten-choice variation questions table example question ﬁve-choice questions. ﬁve-choice question yields labeled quadruples combining stem choice. quadruple hword language note musici labeled positive four quadruples labeled negative. since learning works better balanced training data symmetries proportional analogies positive positive quadruple three positive quadruples thus ﬁve-choice question provides four positive four negative quadruples. ten-fold cross-validation apply supersim questions. folds constructed eight quadruples question kept together fold. answer question testing fold learned model assigns probability choices guesses choice highest probability. supersim achieves score correct table gives rank supersim list results analogy questions. scores ranging signiﬁcantly different supersim’s score according fisher’s exact test conﬁdence level. however supersim answers addition symmetries proportional analogies asymmetries. quadruple positive negative. example hword language note musici good analogy hword music note languagei not. words basic units language notes basic units music words necessary music notes necessary language. turney used asymmetry convert ﬁve-choice questions tenchoice questions. choice expanded stem resulting quadruple order shufﬂed choice pair ﬁvechoice question generated choice quadruples ten-choice question. nine quadruples negative examples quadruple consisting stem pair followed solution pair positive example. purpose ten-choice questions test ability measures relational similarity avoid asymmetric distractors. ten-choice questions compare hand-coded dual-space approach supersim. also questions perform ablation study four sets features supersim. ﬁve-choice questions symmetries proportional analogies three positive examples training ten-choice questions supersim’s score compared ﬁve-choice questions drop hand-coded dual-space model scores compared ﬁve-choice questions drop difference supersim handcoded dual-space model signiﬁcant according fisher’s exact test conﬁdence level. advantage supersim need hand-coding. results show supersim avoid asymmetric distractors. table shows impact different subsets features percentage correct answers ten-choice questions. included features marked ablated features marked results show frequency ppmi features helpful relational similarity. also domain space function space needed good results. semeval task dataset based semantic relation classiﬁcation scheme bejar consisting high-level categories relations seventy-nine subcategories paradigmatic examples subcategory. instance subcategory taxonomic category class inclusion three paradigmatic examples ﬂowertulip emotionrage poemsonnet. algorithm reference buap duluth-v duluth-v duluth-v utd-svm rink harabagiu utd-nb rink harabagiu rnn- mikolov utd-lda rink harabagiu supersim jurgens used amazon’s mechanical turk create semeval task dataset phases. ﬁrst phase turkers expanded paradigmatic examples subcategory average forty-one word pairs subcategory total pairs. second phase word pair ﬁrst phase assigned prototypicality score indicating similarity paradigmatic examples. challenge semeval task guess prototypicality scores. supersim trained ﬁve-choice questions evaluated semeval task test dataset. given word pair created quadruples combining word pair paradigmatic examples subcategory. used supersim compute probabilities quadruple. guess prototypicality score given word pair average probabilities. spearman’s rank correlation coefﬁcient turkers’ prototypicality scores supersim’s scores averaged sixty-nine subcategories testing set. supersim highest spearman correlation achieved date semeval task section presents experiments using supersim learn compositional similarity. datasets consist triples nounmodiﬁer bigram noun unigram. triples labeled positive negative examples paraphrases. table shows feature vectors elements. experiment distractors designed difﬁcult current approaches composition. example fantasy world represented element-wise multiplication context vectors fantasy world likely guess fantasy world fairyland seven-choice question yields seven labeled triples combining stem choice. triple hfantasy world fairylandi labeled positive triples labeled negative. positive example negative. example world fantasy paraphrase fairyland. second dataset constructed applying shufﬂing transformation convert sevenchoice questions fourteen-choice questions second dataset designed table shows percentage testing questions answered correctly datasets. vector addition element-wise multiplication sensitive word order perform poorly fourteen-choice questions. datasets supersim performs signiﬁcantly better approaches except holistic approach according fisher’s exact test conﬁdence level. holistic approach noncompositional. stem bigram represented single context vector generated treating bigram unigram. noncompositional approach cannot scale realistic applications holistic approach cannot applied fourteenchoice questions bigrams questions correspond terms wordnet hence correspond vectors matrices turney found necessary hand-code soundness check algorithms given stem choice hand-coded check assigns minimal score choice need handcode checking supersim. learns automatically training data avoid choices. table shows effects ablating sets features performance supersim fourteen-choice questions. ppmi features important; themselves achieve correct although features needed reach domain space features reach second highest performance used alone reduce performance combined features; however drop signiﬁcant according fisher’s exact test signiﬁcance level. since ppmi features play important role answering noun-modiﬁer questions take closer look them. table twelve ppmi features triple noun-modiﬁer bigram noun unigram. split twelve features three subsets subset pair words example subset four features ppmi ppmi ppmi ppmi. table shows effects ablating subsets. supersim uses training questions learn recognize bigram paraphrase unigram; learns expert knowledge implicit wordnet synsets. would advantageous able train supersim less reliance expert knowledge. past work adjective-noun bigrams shown holistic bigram vectors train supervised regression model output regression model vector representation bigram approximates holistic vector bigram; approximates vector would treating bigram unigram. supersim generate vectors output still holistic bigram vectors training. table shows seven-choice training question generated without using wordnet synsets. choices form bigrams represent holistic bigram vectors; pretend unigrams. call bigrams pseudounigrams. supersim concerned difference pseudo-unigrams true unigrams. question table treated question table generate holistic training questions randomly selecting noun-modiﬁer bigrams wordnet stems questions avoiding bigrams appear stems solution testing questions. pseudo-unigram corresponds matrices section stem bigram. term wordnet corresponds vector. corresponding vectors enable treat bigrams wordnet unigrams. distractors component unigrams stem bigram pseudounigrams share component word stem construct holistic training questions used wordnet source bigrams ignored rich information wordnet provides bigrams synonyms hypernyms hyponyms meronyms glosses. table compares holistic training standard training testing standard testing cases. signiﬁcant drop performance holistic training performance still surpasses vector addition element-wise multiplication hand-coded dual-space model since holistic questions generated automatically without human expertise experimented increasing size holistic training dataset growing questions increments performance fourteen-choice questions holistic training standard testing varied correct clear trend down. signiﬁcantly different performance holistic training questions seems likely drop performance holistic training instead standard training difference nature standard questions holistic questions currently investigating issue. expect able close performance future work improving holistic questions. however possible fundamental limits holistic training. supersim performs slightly better hand-coded dual-space model relational similarity problems performs much better compositional similarity problems ablation studies suggest ppmi features effect ten-choice performance large effect fourteen-choice noun-modiﬁer paraphrase performance advantage supervised learning handcoding facilitates adding features. clear modify hand-coded equations dual-space model noun-modiﬁer composition include ppmi information. supersim approaches distributional semantics beyond words attempted address relational compositional similarity strength approach works well kinds similarity. given promising results holistic training noun-modiﬁer paraphrases plan experiment holistic training analogies. consider proportional analogy hard hard time good good time hard time good time pseudo-unigrams. human analogy trivial supersim access surface form term. supersim concerned analogy much analogy hard difﬁculty good fun. strategy automatically converts simple easily generated analogies complex challenging analogies suited training supersim. also suggests noun-modiﬁer paraphrases used solve analogies. perhaps evaluate quality candidate analogy searching term good paraphrases. example consider analogy mason stone carpenter wood. paraphrase mason stone worker carpenter wood worker. transforms analogy stone worker stone wood worker wood makes easier recognize relational similarity. another area future work extending supersim beyond noun-modiﬁer paraphrases measuring similarity sentence pairs. plan adapt ideas socher task. dynamic pooling represent sentences varying size ﬁxed-size feature vectors. using ﬁxedsize feature vectors avoids problem quadratic growth enables supervised learner generalize sentences varying length. ablation experiments suggest domain function spaces provide important features relational similarity ppmi values provide important features noun-modiﬁer compositional similarity. explaining another topic future research. paper presented supersim uniﬁed approach analogy paraphrase supersim treats problems supervised tuple classiﬁcation. supervised learning algorithm standard support vector machine. main contribution supersim four types features representing tuples. features work well analogy paraphrase task-speciﬁc modiﬁcations. supersim matches state analogy questions substantially advances state semeval task challenge noun-modiﬁer paraphrase questions. answering questions minutes instead days. unlike dual-space model supersim requires handcoded similarity composition functions. since hand-coding easy features supersim. much work remains done incorporating logic scaling sentence four approaches described supersim instance second approach extending distributional semantics beyond words comparing word pairs phrases sentences combining multiple pairwise similarity values. perhaps main signiﬁcance paper provides evidence support general approach.", "year": 2013}