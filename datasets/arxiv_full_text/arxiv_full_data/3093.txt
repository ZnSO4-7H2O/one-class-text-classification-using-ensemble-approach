{"title": "Training Effective Node Classifiers for Cascade Classification", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Cascade classifiers are widely used in real-time object detection. Different from conventional classifiers that are designed for a low overall classification error rate, a classifier in each node of the cascade is required to achieve an extremely high detection rate and moderate false positive rate. Although there are a few reported methods addressing this requirement in the context of object detection, there is no principled feature selection method that explicitly takes into account this asymmetric node learning objective. We provide such an algorithm here. We show that a special case of the biased minimax probability machine has the same formulation as the linear asymmetric classifier (LAC) of Wu et al (2005). We then design a new boosting algorithm that directly optimizes the cost function of LAC. The resulting totally-corrective boosting algorithm is implemented by the column generation technique in convex optimization. Experimental results on object detection verify the effectiveness of the proposed boosting algorithm as a node classifier in cascade object detection, and show performance better than that of the current state-of-the-art.", "text": "real-time object detection inherently involves searching large number candidate image regions small number objects. processing single image example require interrogation well million scanned windows order uncover single correct detection. imbalance data impact detectors applied also training process. impact reﬂected need identify discriminative features within large over-complete feature set. cascade classiﬁers proposed potential solution problem imbalance data received signiﬁcant attention speed accuracy. work propose principled method train boosting-based cascade classiﬁers. boosting-based cascade approach object detection introduced viola jones received signiﬁcant subsequent attention also underpins current state-of-the-art viola jones approach uses cascade increasingly complex classiﬁers aims achieve best possible classiﬁcation accuracy achieving extremely false negative rate. classiﬁers seen forming nodes degenerate binary tree whereby negative result single node classiﬁer terminates interrogation current patch. viola jones adaboost train node classiﬁer order abstract cascade classiﬁers widely used realtime object detection. diﬀerent conventional classiﬁers designed overall classiﬁcation error rate classiﬁer node cascade required achieve extremely high detection rate moderate false positive rate. although reported methods addressing requirement context object detection principled feature selection method explicitly takes account asymmetric node learning objective. provide algorithm here. show special case biased minimax probability machine formulation linear asymmetric classiﬁer design boosting algorithm directly optimizes cost function lac. resulting totally-corrective boosting algorithm implemented column generation technique convex optimization. experimental results object detection verify eﬀectiveness proposed boosting algorithm node classiﬁer cascade object detection show performance better current state-of-the-art. shen wang paisitkriangkrai hengel australian centre visual technologies school computer science university adelaide australia e-mail chunhua.shenadelaide.edu.au work part supported australian research council future fellowship achieve best possible classiﬁcation accuracy. false negative rate achieved subsequently adjusting decision threshold desired false negative rate achieved. process cannot guaranteed produce best detection performance given false negative rate. all. conjecture improvement gained node learning objective explicitly taken account steps. thus propose boosting algorithms implement idea verify conjecture. preliminary version work published shen assumption node cascade classiﬁer makes independent classiﬁcation errors detection rate false positive rate entire respectively represents detection rate classiﬁer corresponding false positive rate number nodes. pointed equations suggest node learning objective node extremely high detection rate moderate false positive rate values cascade nodes drawback standard adaboost approach boosting take advantage cascade classiﬁer’s special structure. adaboost minimizes overall classiﬁcation error particularly minimize number false negatives. sense features selected adaboost optimal purpose rejecting many negative examples possible. viola jones proposed solution problem asymboost modifying loss function greatly penalize false negatives. asymboost achieves better detection rates adaboost still addresses node learning goal indirectly cannot guaranteed achieve optimal solution. explicitly studied node learning goal proposed linear asymmetric classiﬁer fisher linear discriminant analysis adjust weights features selected adaboost asymboost experiments indicated post-processing technique node learning objective better translated improved detection rates. viola jones’ framework boosting used select features time train strong classiﬁer. al.’s work separates tasks adaboost asymboost used select features; second step used construct strong classiﬁer adjusting weights selected features. node learning objective considered second step. ﬁrst step—feature selection— node learning objective explicitly considered starting theory minimax probability machines derive simpliﬁed version biased minimax probability machine formulation linear asymmetric classiﬁer thus show underlying connection lac. importantly interpretation weakens restrictions acceptable input data distribution imposed lac. develop boosting-like algorithms directly minimizing objective function linear asymmetric classiﬁer results algorithm label lacboost. also propose fisherboost basis fisher rather lac. methods used identify feature optimally achieves node learning goal training cascade classiﬁer. knowledge ﬁrst attempt design feature selection method. lacboost fisherboost share similarities lpboost sense column generation—a technique originally proposed large-scale linear programming typically lagrange dual problem solved iteration column generation. instead solve primal quadratic programming problem special structure entropic gradient used solve problem eﬃciently. compared general interior-point based solvers much faster. apply lacboost fisherboost object detection better performance observed other methods particular pedestrian detection fisherboost achieves state-of-the-art comparing methods listed three benchmark datasets. results conﬁrm conjecture show eﬀectiveness lacboost fisherboost. methods immediately applied asymmetric classiﬁcation problems. observed many cases even performs better lac. experiments also observed similar phenomena. paisitkriangkrai empirically showed lda’s criterion used achieve better detection results. explanation works well object detection missing literature. demonstrate context object detection seen regularized version approximation. proposed lacboost/fisherboost algorithm differs traditional boosting algorithms minimize loss function. opens possibilities designing boosting-like algorithms special purposes. also extended column generation optimizing nonlinear optimization problems. next review related work context real-time object detection using cascade classiﬁers. ﬁeld object detection made signiﬁcant progress last decade especially seminal work viola jones. three components contribute ﬁrst robust real-time object detection framework approach received signiﬁcant subsequent attention. number alternative cascades developed including soft cascade waldboost dynamic cascade and-or cascade multi-exit cascade joint cascade recently proposed rate constraint embedded cascade work adopted multi-exit cascade pham eﬀectiveness eﬃciency demonstrated pham multi-exit cascade improves classiﬁcation performance using results weak classiﬁers applied patch reaching decision node tree thus n-th node classiﬁer uses results weak classiﬁers associated node addition improving cascade structure number improvements made learning algorithm building node classiﬁers cascade. example fast forward feature selection accelerate training procedure also showed used deliver better classiﬁcation performance. pham cham recently proposed online asymmetric boosting considerably reduces training time required exploiting feature statistics pham cham also designed fast method train weak classiﬁers. zhang proposed floatboost discards redundant weak classiﬁers adaboost’s greedy selection procedure. masnadi-shirazi vasconcelos proposed cost-sensitive boosting algorithms applied diﬀerent cost-sensitive losses means gradient descent. shum also proposed klboost aiming select features maximize projected kullback-leibler divergence select feature weights minimizing classiﬁcation error. promising results also reported logitboost employs logistic regression loss gentleboost uses adaptive newton steps additive model. multiinstance boosting introduced object detection require precisely labeled locations targets training data. features also designed improving detection performance. viola jones’ haar features suﬃciently discriminative detecting complex objects like pedestrians multi-view faces. covariance features histogram oriented gradients proposed context eﬃcient implementation approaches developed each. shape context also exploit integral images applied human detection thermal images local binary pattern descriptor variants shown promising performance human detection recently eﬀort spent combining complementary features including simple concatenation combination heterogeneous local features boosted cascade classiﬁer bayesian integration intensity depth motion features mixture-of-experts model fig. cascade classiﬁers. ﬁrst standard cascade viola jones second multi-exit cascade proposed pham classiﬁed true detection nodes true targets. rest paper organized follows. brieﬂy review concept minimax probability machine derive simpliﬁed version biased minimax probability machine section linear asymmetric classiﬁcation connection minimax probability machine discussed section section show design boosting algorithms rewriting optimization formulations fisher lda. boosting algorithms applied object detection section conclude paper section following notation used. matrix denoted bold upper-case letter column vector denoted bold lower-case letter denoted column identity matrix size clear context. column vectors respectively. denote component-wise inequalities. deﬁne matrix rm×n entry label predicted weak classiﬁer datum element order simplify notation eliminate superscript training therefore column matrix consists note boosting algorithms entirely depends matrix directly interact training examples. following discussion thus largely focus matrix write vector obtained multiplying matrix vector entry represent coeﬃcients selected weak classiﬁer margin training datum vector margins training data lower bound classiﬁcation accuracy test data. problem transformed convex problem specifically second-order cone program thus solved eﬃciently formulation assumes classiﬁcation problem balanced. attempts achieve high recognition accuracy assumes losses associated mis-classiﬁcations identical. however many applications case. acceptable classiﬁcation accuracy less important class. resulting decision hyperplane prioritizes classiﬁcation important class less important class biased thus expected perform better biased classiﬁcation applications. huang showed iteratively solved solving sequence socps using fractional programming technique. clearly signiﬁcantly computationally demanding solve next show re-formulate simpler quadratic program based recent theoretical results section interested simplifying problem special case important application object detection following discussion simplicity consider although algorithms developed also apply theoretical results show that worst-case constraint written diﬀerent forms follows arbitrary symmetric symmetric unimodal gaussian distributions biased based general form four cases shown appendix i.e. equation arbitrary distributions impose constraints upon distributions however take advantage structural information whenever available. example shown that face detection problem weak classiﬁer outputs well approximated gaussian distribution. words constraint arbitrary distributions utilize type priori information hence many problems considering arbitrary distributions simplifying conservative. since biased assume constraints distribution family fail exploit structural information. consider special case easy worst-case constraint becomes simple linear constraint symmetric symmetric unimodal well gaussian distributions pointed result immediate consequence symmetry worst-case distributions forced probability mass arbitrarily away sides mean. case information covariance neglected. override symbol here denotes output vector weak classiﬁers datum cast node linear classiﬁer feature space constructed binary outputs weak classiﬁers. node cascade classiﬁer wish maximize detection rate maintaining false positive rate moderate level problem represents node learning goal. boosting algorithms adaboost used feature selection methods used learn linear classiﬁer binary features chosen boosting advantage approach considers asymmetric node learning explicitly. negative data respectively. empirically veriﬁed approximately gaussian cascade face detector. discuss issue detail section shen theoretically proved assumption weak classiﬁers independent margin adaboost follows gaussian distribution long number weak classiﬁers suﬃciently large. section verify theoretical result performing normality test nodes diﬀerent number weak classiﬁers. please refer appendix proof theorem derived biased algorithm diﬀerent perspective. reveal assumption symmetric distributions needed arrive simple unconstrained formulation. compared approach used information simply optimization problem. importantly shown next section unconstrained formulation enables design boosting algorithm. close connection algorithm linear asymmetric classiﬁer resulting problem exactly removing inequality constraint leads problem solvable eigendecomposition. thus shown results generalized gaussian distributions assumed symmetric distributions. shown starting biased minimax probability machine able obtain optimization formulation shown much weakening underlying assumption before propose lacboost fisherboost however provide brief overview lac. proposed linear asymmetric classiﬁcation post-processing step training nodes cascade framework. stated guaranteed reach optimal solution assumption gaussian data distributions. know gaussianality condition relaxed. selected meaning dimension optimization variable also extremely large. semi-inﬁnite quadratic program show column generation used solve problem. make column generation applicable need derive speciﬁc lagrange dual primal problem. exactly standard adaboost lpboost producing best weak classiﬁer iteration. weak classiﬁer minimum weighted training error. summarize lacboost/fisherboost algorithm algorithm simply changing algorithm used train either lacboost fisherboost. note obtain actual strong classiﬁer need include oﬀset i.e. ﬁnal tion algorithm cost function minimize classiﬁcation error. ﬁnds projection direction data maximally separated. simple line search optimal moreover training cascade need tune oﬀset anyway shown optimal value dual problem would decrease. accordingly optimal value primal problem decreases optimal value zero duality gap. moreover primal cost function convex therefore converges global minimum. derive lagrange dual quadratic problem although interested variable need keep auxiliary variable order obtain meaningful dual problem. lagrangian case rank-deﬁcient inverse exist actually zero eigenvalue corresponding eigenvector ones. easy zero. simply regularize small positive constant. actually diagonally dominant matrix strict diagonal dominance. strict diagonal dominance gershgorin circle theorem strictly diagonally dominant matrix must invertible. optimality conditions problem viewed regularized lpboost problem. compared hard-margin lpboost diﬀerence regularization term cost function. duality primal dual zero. words solutions coincide. instead solving directly calculates violated constraint iteratively current solution adds constraint optimization problem. theory column violates dual feasibility added. speed convergence dual problem standard problem. special structure exploit. show primal problem belongs special class problems eﬃciently solved using entropic/exponentiated gradient descent appendix details algorithm. fast solver extremely important training object detector since need solve thousand problems. compared standard solvers like mosek much faster. makes possible train detector using almost amount time using standard adaboost majority time spent weak classiﬁer training bootstrapping. ﬁers vertical horizontal decision stumps. fisherboost emphasizes positive samples negative samples. result decision boundary fisherboost similar gaussian distribution decision boundary adaboost. second equation obtained fact dual problem’s constraints optimum must exist least equality holds. largest edge weak classiﬁers. summary using solve primal ﬁrst illustrate performance fisherboost asymmetrical synthetic data large number negative samples compared positive ones. fig. demonstrates subtle diﬀerence classiﬁcation boundaries adaboost fisherboost. observed fisherboost places emphasis positive samples negative samples ensure positive samples would classiﬁed correctly. adaboost hand treat positive negative samples equally. might fact adaboost optimizes overall classiﬁcation accuracy. ﬁnding consistent results reported earlier experiment fisherboost lacboost compared several asymmetric boosting algorithms namely adaboost fisher post-processing asymboost cost-sensitive adaboost rate constrained boosting results adaboost also presented baseline. algorithm train strong classiﬁer consisting weak classiﬁers along coeﬃcients. threshold determined false positive rate test every method experiment repeated times average detection rate positive class reported. fisherboost duct experiments. ﬁrst experiment training enforce target detection rate second experiment; training data train model enforce target detection rate. target detection rate barrier coeﬃcient number iterations halving tested performance algorithms real-world data sets including machine learning vision data sets categorized usps data sets classes even digits digits. faces face data sets randomly extract negative patches background images. apply principle component analysis preserve total variation. data dimension uiuc els. projected data capture variation ﬁnal dimension indoor/outdoor scene divide -scene data used groups indoor outdoor scenes. centrist feature descriptors build visual code words using histogram intersection kernel image represented spatial hierarchy manner. image consists sub-windows. total feature dimensions image. classiﬁers trained remove negative data retaining almost positive data. compare detection rates table experiments fisherboost demonstrates best performance data sets. however lacboost perform well expected. suspect poor performance might partially numerical issues cause overﬁtting. discuss detail section experiments eight asymmetric boosting methods evaluated multi-exit cascade fisherboost/lacboost adaboost alone lda/lac post-processing asymboost alone lda/lac postprocessing. also implemented viola-jones’ face detector baseline furthermore face detector also compared state-of-the-art including cascade design methods i.e. waldboost floatboost boosting chain extension rcecboost algorithm training multi-exit cascade summarized algorithm ﬁrst illustrate validity adopting fisher post-processing improve node learning objective cascade classiﬁer. described above assume margin training data associated node classiﬁer cascade exhibits gaussian distribution. demonstrate assumption face detection task fig. fig. shows normal probability plot margins positive training data ﬁrst three node classiﬁers multi-exit classiﬁer. ﬁgure reveals larger number weak classiﬁers used closely margins follow gaussian table test errors real-world data sets. experiments times boosting iterations. average detection rate standard deviation false positives reported. best average detection rate shown boldface. fig. normality test face data’s margin distribution nodes nodes contains weak classiﬁers respectively. data plotted theoretical normal distribution data follows normal distribution model form straight line. curves deviated straight line indicate departures normality. larger number weak classiﬁers closely margin follow gaussian distribution. several multi-exit cascades trained various algorithms described above. order ensure fair comparison used number multi-exit stages number weak classiﬁers. multi-exit cascade consists exits weak classiﬁers. indices exit nodes predetermined simplify training procedure. portant parameter chosen rameter using cross-validation. instead train node cascade candidate choose best training accuracy. exit negative examples misclassiﬁed current cascade discarded negative examples bootstrapped background images pool. total billions negative examples extracted pool. positive training data validation data keep unchanged training process. experiments performed workstation intel xeon cpus ram. takes hours train multi-exit cascade adaboost asymboost. fisherboost lacboost takes less hours train complete distribution. this infer lac/lda postprocessing thus lacboost fisherboost expected achieve better performance larger number weak classiﬁers used. therefore apply lac/lda within later nodes multi-exit cascade nodes contain weak classiﬁers. choose multiexit property eﬀectiveness reported compared multiexit cascade lda/lac post-processing conventional cascade lda/lac post-processing performance improvement observed. since multi-exit cascade makes previous weak classiﬁers earlier nodes would meet gaussianity requirement better conventional cascade classiﬁer. fig. face detectors compared asymmetric boosting methods state-of-theart including cascade design methods mit+cmu frontal face test data using curves asym mean features selected using adaboost asymboost respectively. implements viola jones’ cascade using adaboost multiexit means multi-exit cascade curves compared methods quoted original papers compared methods ranked legend based average detection rates. multi-exit cascade. words algorithm takes less hour solve primal problem estimation computational complexity suppose number training examples number weak classiﬁers iteration cascade training complexity solving primal using iterations needed eg’s convergence. complexity training weak classiﬁer number haar-feature also experimentally observed speedup standard solvers. solve primal deﬁned using mosek qp’s size variables. accuracy tolerance mosek takes seconds seconds standard desktop. times faster. moreover iteration training cascade take advantage last iteration’s solution starting small perturbation previous receiver operating characteristic curves fig. show entire cascade’s performance. average detection rate used rank compared methods mean detection rates sampled evenly false positives. note multiple factors impact cascade’s performance however including classiﬁer cascade structure bootstrapping etc. fig. demonstrate superior performance fisherboost asymmetric boosting methods face detection task. also lacboost perform worse fisherboost. observed post-processing outperform post-processing cases either. experiments pedestrian detection minor modiﬁcation visual features used. evaluate approach inria data training consists cropped mirrored pedestrian images large resolution background images. test consists images containing annotated pedestrians nonpedestrian images. training sample scaled malization applied feature vector. furthermore integral histograms speed computation iteration randomly sample possible blocks training weak classiﬁer. used weighted linear discriminant analysis weak classiﬁers used linear support vector machines weak classiﬁers also used weak classiﬁers here. experiment cascade classiﬁers number nodes weak classiﬁers. reason described face detection section fisherboost/lacboost al.’s lda/lac post-processing applied cascade node onwards instead ﬁrst node. positive examples remain nodes negative examples later nodes obtained bootstrap approach. parameter fisherboost carefully selected experiment. ideally cross-validation used pick best value using independent cross-validation data set. since many labeled positive training data inria data positive examples validation. collect additional input training examples ordered labels dmin minimum acceptable detection rate node; fmax maximum acceptable false positive rate node; target overall false positive rate. generate weak classiﬁer update weak classiﬁers’ linear coeﬃcient using lacboost fisherboost. adjust threshold current boosted strong classiﬁer also compared methods boosted greedy sparse considered state-of-the-art. fisherboost lacboost outperform bgslda adaboost/asymboost detection rate. note bgslda uses standard cascade. fig. performance fisherboost better considered cascade design methods. however since parameters cascade structure carefully tuned method guarantee optimal tradeoﬀ accuracy speed. believe boosting method cascade design strategy compensate other. actually authors also incorporate costsensitive boosting algorithms e.g. cost-sensitive adaboost asymexperiment compare fisherboost stateof-the-art pedestrian detectors several public data sets. authors compare various pedestrian detectors conclude combining multiple discriminative features often signiﬁcantly boost performance pedestrian detection. surprising since similar conclusion drawn object recognition task. clearly pedestrian detector relies solely feature unlikely outperform using combination features. fig. fisherboost lacboost compared cascade pedestrian detectors inria data set. cascades trained number weak classiﬁers nodes using features. legend detectors sorted based log-average detection rates. fisherboost performs best compared cascades. pixels scales octave performance diﬀerent cascade detectors evaluated using protocol described technique known pairwise maximum suppression applied suppress less conﬁdent detection windows. conﬁdence score needed detection window input pairwise maximum suppression. work conﬁdence simply calculated mean decision scores last nodes cascade. curves plotted fig. log-average detection rate used summarize overall detection performance mean detection rates sampled evenly positions general fisherboost outperforms cascade detectors. similar previous experiments postprocessing improve performance adaboost. however observe fisherboost post-processing better generalization performance lacboost post-processing. discuss issue experiments. cation pixel intensity ﬁrst order intensity derivatives second order intensity derivatives edge orientation. pixel mapped -dimensional feature image. calculate correlation coeﬃcients block concatenate features previously computed features. feature encodes gradient histogram also information correlation deﬁned statistics inside spatial layout similar previous experiment project features line using weighted linear discriminant analysis. except features training test implementations previous pedestrian detection experiments. ﬁrst compare fisherboost baseline detectors trained adaboost. ﬁrst baseline detector trained conventional cascade second baseline detector trained multi-exit cascade detectors trained covariance features inria training set. results inria test sets using protocol reported fig. similar previous results fisherboost outperforms baseline detectors. covariance features capture relationship different image statistics shown perform well previous experiments. however discriminative features also used instead e.g. haar-like features local binary pattern selfsimilarity low-level features fig. performance pedestrian detector compared baseline detectors state-of-the-art detectors publicly available pedestrian data sets. detector uses covariance features. performances ranked using log-average detection rates legend. detector performs best inria data second best tud-brussels data sets. note best latter data sets either used many features used sophisticated part-based model. detector compared existing pedestrian detectors listed inria tud-brussels data sets. tudbrussels data sets since sizes ground-truths smaller inria training upapplying pedestrian detector. curves logaverage detection rates reported fig. data fisherboost outperforms compared detectors. tud-brussels data detector second best inferior multiftr+motion uses discriminative features ours. inria data fisherboost’s performance also ranked second worse part-based detector uses much complex model training process believe combining discriminative features e.g. features used overall detection performance method improved. summary despite simple plus covariance features fisherboost pedestrian detector still achieves finally report average number features evaluated scanning window table compare fisherboost implementation adaboost traditional cascade adaboost multimajor bottleneck pedestrian detector lies feature extraction part. implementation make multi-threading speed runtime pedestrian detector. using cores intel able speed average processing time less second frame. believe using special purpose hardware graphic processing unit speed detector signiﬁcantly improved. impact varying number weak classiﬁers next experiment vary number weak classiﬁers cascade node evaluate impact ﬁnal detection performance. train three diﬀerent pedestrian detectors inria data set. limit maximum number weak classiﬁers multi-exit node ﬁrst nodes trained using adaboost subsequent nodes trained using fisherboost. fig. shows curves diﬀerent detectors. although observe performance improvement number weak classiﬁers increases improvement minor compared signiﬁcant increase average number features required detection window. experiment indicates robustness fisherboost number weak classiﬁers multi-exit cascade. note fisher used previous experiments pedestrian detection. impact training fisherboost early node previous section conjecture fisherboost performs well margin follows gaussian distribution. result apply fisherboost later node multi-exit cascade experiment show possible start training fisherboost ﬁrst node cascade. achieve this train additional weak classiﬁers ﬁrst node conduct experiment training fisherboost detectors. ﬁrst detector fisherboost applied ﬁrst node onwards. number weak classiﬁers node etc. second detector apply adaboost ﬁrst nodes apply fisherboost third node onwards. number weak classiﬁers node etc. detectors node criterion i.e. node discard least background samples. conﬁgurations kept same. report performance detectors fig. results fisher performs slightly better fisher based results classiﬁers early nodes cascade heuristically chosen large number easy negative patches quickly discarded. words ﬁrst nodes signiﬁcantly aﬀect eﬃciency visual detector play signiﬁcant role ﬁnal detection performance. actually always apply simple classiﬁers remove large percentage negative windows speed detection. fig. performance comparison. vary number weak classiﬁers multi-exit node. weak classiﬁers used node accuracy slightly improved. start training fisherboost ﬁrst node hogcov-fisher achieve slightly better detection rate hogcov-fisher. table compare performance fisherboost varying number weak classiﬁers multi-exit node. average features required detection window log-average detection rates inria pedestrian dataset reported. weak classiﬁers multi-exit node used slightly improved accuracy achieved price features evaluated. tried diﬀerent combinations classes’ covariance matrices calculating within-class matrix nonnegative constant. easy correspond respectively. found setting negative data symmetric. practice requirement perfectly satisﬁed especially ﬁrst several node classiﬁers. explain cases improvement signiﬁcant. however explain works; sometimes performs even better ﬁrst glance means explicitly considers imbalanced node learning objective. plausible explanation either proposition object detection problems fisher linear discriminant analysis viewed regularized version linear asymmetric classiﬁer. words linear discriminant analysis already considered asymmetric learning objective. fisherboost regularization equivalent -norm penalty primal variable objective function problem section norm regularization avoids over-ﬁtting increases robustness fisherboost. similar penalty also used machine learning algorithms ridge regression object detection face pedestrian detection considered here covariance matrix negative class close scaled identity matrix. theory negative data anything target. look oﬀ-diagonal elesummary lda-like approaches perform better laclike approaches object detection main reasons. ﬁrst reason regularized version lac. second reason negative data necessarily symmetrically distributed. particularly latter nodes bootstrapping forces negative data visually similar positive data. case ignoring negative data’s covariance information likely deteriorate detection performance. fig. covariance matrix ﬁrst weak classiﬁers selected fisherboost non-pedestrian data. approximated scaled identity matrix. average magnitude diagonal elements times larger oﬀ-diagonal elements. experiment evaluate impact regularization parameter varying value balances ratio positive negative class’s covariance matrices i.e. σ+δς; also classiﬁers trained remove negative data retaining almost positive data. compare detection rate table first general observe performance improvement small positive value. since setting happens coincide objective criterion classiﬁer also inherits node learning goal context object detection. second diﬀerent datasets theory parameter cross validated setting point hypothesis naturally arises regularization really reason lacboost underperforms fisherboost applying forms regularization lacboost would also likely improve lacboost. last experiment tries verify hypothesis. regularize matrix adding appropriately scaled identity matrix ˜δi. discussed section numerical stability point view diﬃculties arise rank-deﬁcient causing dual solution non-uniquely deﬁned. issue much worse lacboost lowerblock i.e. zero matrix. case well-deﬁned problem obtained replacing ˜δi. interpreted corresponding primal-regularized lacboost. also proposed entropic gradient descent eﬃciently implement fisherboost lacboost. proposed algorithms easy implement applied asymmetric classiﬁcation tasks computer vision. future design asymmetric boosting algorithms exploiting asymmetric kernel classiﬁcation methods compared stage-wise adaboost parameter-free boosting algorithms need tune parameter.", "year": 2013}