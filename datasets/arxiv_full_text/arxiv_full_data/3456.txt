{"title": "Hyperplane Clustering Via Dual Principal Component Pursuit", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "We extend the theoretical analysis of a recently proposed single subspace learning algorithm, called Dual Principal Component Pursuit (DPCP), to the case where the data are drawn from of a union of hyperplanes. To gain insight into the properties of the $\\ell_1$ non-convex problem associated with DPCP, we develop a geometric analysis of a closely related continuous optimization problem. Then transferring this analysis to the discrete problem, our results state that as long as the hyperplanes are sufficiently separated, the dominant hyperplane is sufficiently dominant and the points are uniformly distributed inside the associated hyperplanes, then the non-convex DPCP problem has a unique global solution, equal to the normal vector of the dominant hyperplane. This suggests the correctness of a sequential hyperplane learning algorithm based on DPCP. A thorough experimental evaluation reveals that hyperplane learning schemes based on DPCP dramatically improve over the state-of-the-art methods for the case of synthetic data, while are competitive to the state-of-the-art in the case of 3D plane clustering for Kinect data.", "text": "state-of-the-art methods clustering data drawn union subspaces based sparse low-rank representation theory. existing results guaranteeing correctness methods require dimension subspaces small relative dimension ambient space. assumption violated example case hyperplanes existing methods either computationally intense lack theoretical support main theoretical contribution paper extend theoretical analysis recently proposed single subspace learning algorithm called dual principal component pursuit case data drawn union hyperplanes. gain insight expected properties non-convex problem associated dpcp develop geometric analysis closely related continuous optimization problem. transferring analysis discrete problem results state long hyperplanes sufﬁciently separated dominant hyperplane sufﬁciently dominant points uniformly distributed inside associated hyperplanes non-convex dpcp problem unique global solution equal normal vector dominant hyperplane. suggests sequential hyperplane learning algorithm ﬁrst learns dominant hyperplane applying dpcp data. order avoid hard thresholding points sensitive choice thresholding parameter points weighted according distance hyperplane second hyperplane computed applying dpcp weighted data experiments corrupted synthetic data show dpcp-based sequential algorithm dramatically improves similar sequential algorithms learn dominant hyperplane state-of-the-art single subspace learning methods finally plane clustering experiments real point clouds show k-hyperplanes dpcp-based scheme computes normal vector cluster dpcp instead classic competitive state-of-the-art approaches subspace clustering. past ﬁfteen years model union linear subspaces also called subspace arrangement gained signiﬁcant popularity pattern recognition computer vision often replacing classical model single linear subspace associated well-known principal component analysis variety algorithms attempt cluster collection data drawn subspace arrangement giving rise challenging ﬁeld subspace clustering techniques iterative point cloud analysis hybrid system identiﬁcation even though ways hyperplane clustering simpler general subspace clustering since e.g. dimensions subspaces equal known a-priori modern self-expressiveness-based subspace clustering methods elhamifar vidal principle apply case require small relative subspace dimensions. theoretical point view appropriate methods hyperplane clustering algebraic subspace clustering gives closed-form solutions means factorization differentiation polynomials. however main drawback exponential complexity number hyperplanes ambient dimension makes impractical many settings. another method theoretically justiﬁable clustering hyperplanes spectral curvature clustering based computing d-fold afﬁnity d-tuples points dataset. case characterized combinatorial complexity becomes cumbersome large even though possible reduce complexity comes cost signiﬁcant performance degradation. hand intuitive classical method k-hyperplanes alternates assigning clusters ﬁtting normal vector cluster perhaps practical method hyperplane clustering since simple implement robust noise complexity depends maximal allowed number iterations. however sensitive outliers guaranteed converge local minimum; hence multiple restarts general required. median k-flats shares similar objective function uses -norm instead norm attempt gain robustness outliers. minimizes objective function stochastic gradient descent scheme searches directly basis subspace makes slower converge hyperplanes. finally note single subspace learning method ransac reaper applied sequential fashion learn union hyperplanes ﬁrst learning ﬁrst dominant hyperplane removing points lying close learning second dominant hyperplane dpcp single subspace method high relative dimensions. recently method introduced context single subspace learning outliers called dual principal component pursuit aims recovering orthogonal complement subspace presence outliers. since orthogonal complement hyperplane one-dimensional dpcp particularly suited hyperplanes. dpcp searches normal hyperplane solving non-convex minimization problem sphere alternatively recursion linear programming relaxations. assuming dataset normalized unit -norm consists points uniformly distributed great circle deﬁned hyperplane together arbitrary points uniformly distributed sphere tsakiris vidal gave conditions normal hyperplane unique global solution non-convex problem well limit point recursion linear programming relaxations latter reached ﬁnite number iterations. contributions. motivated robustness dpcp outliers–dpcp shown method capable recovering normal hyperplane presence outliers inside -dimensional ambient space –one could naively hyperplane clustering recovering normal hyperplane time treating points hyperplanes outliers. however scheme a-priori guaranteed succeed since outliers clearly structured contrary theorems correctness tsakiris vidal assume outliers uniformly distributed sphere. precisely theoretical bridge paper show long hyperplanes sufﬁciently separated dominant hyperplane sufﬁciently dominant points uniformly distributed inside associated hyperplanes non-convex dpcp problem unique global solution equal normal vector dominant hyperplane. suggests sequential hyperplane learning algorithm ﬁrst learns dominant hyperplane weights points according distance hyperplane. dpcp applied weighted data yields second dominant hyperplane experiments corrupted synthetic data show dpcp-based sequential algorithm dramatically improves similar sequential algorithms learn dominant hyperplane state-of-the-art single subspace learning methods finally plane clustering experiments real point clouds show k-hyperplanes dpcp-based scheme computes normal vector cluster dpcp instead classic competitive state-of-the-art approaches notation. positive integer vector otherwise. unit sphere vectors principal angle unique angle denotes vector ones stands left-hand-side stands right-hand-side. finally denote card cardinality paper organization. rest paper organized follows. review priorart generic hyperplane clustering. discuss theoretical contributions paper; proofs given describes algorithmic contributions paper contains experimental evaluations proposed methods. ransac. traditional clustering points lying close hyperplane arrangement means random sampling consensus algorithm attempts identify single hyperplane time. speciﬁcally ransac alternates randomly selecting points counting number points dataset within distance hyperplane generated selected points. certain number trials reached ﬁrst hyperplane selected admits largest number points dataset within distance points removed second hyperplane obtained reduced dataset similar fashion naturally ransac sensitive thresholding parameter addition efﬁciency depends probability randomly selected points close underlying hyperplane probability depends large well balanced unbalanced clusters are. small ransac likely succeed trials. true clusters highly dominant i.e. since case identifying likely achieved trials. hand large order magnitude exponentially many trials required ransac becomes inefﬁcient. k-hyperplanes another popular method hyperplane clustering so-called k-hyperplanes proposed bradley mangasarian attempts minimize non-convex objective function hyperplane assignment point i.e. point assigned hyperplane normal vector estimated i-th hyperplane. non-convexity typical perform optimization alternating assigning clusters i.e. given assigning closest hyperplane ﬁtting hyperplanes i.e. given segmentation {sj} computing best hyperplane cluster means cluster. iterative reﬁnement hyperplanes clusters method sometimes also called iterative hyperplane learning theoretical guarantees limited convergence local minimum ﬁnite number steps. even though alternating minimization computationally efﬁcient practice several restarts typically used order select best among multiple local minima. fact higher ambient dimension restarts required signiﬁcantly increases computational burden moreover robust noise outliers since update normal vectors done means standard -based) pca. notice almost identical objective except distances points assigned hyperplanes appear without square. makes optimization problem harder zhang propose solve means stochastic gradient approach requires multiple restarts does. even though conceptually expected robust outliers aware theoretical guarantees surrounding corroborate intuition. moreover considerably slower since searches directly basis hyperplanes rather normals hyperplanes. note designed speciﬁcally hyperplanes rather general case unions equi-dimensional subspaces. addition trivial adjust search orthogonal complement subspaces would efﬁcient approach hyperplanes. algebraic subspace clustering originally proposed vidal precisely purpose provably clustering hyperplanes problem time handled either intuitive ransac k-hyperplanes. idea behind polynomial degree data number hyperplanes polynomial indeterminates. absence noise polynomial shown scale form normal vector hyperplane reduces problem factorizing product linear factors elegantly done vidal data contaminated noise ﬁtted polynomial need longer factorizable; apparent difﬁculty circumvented vidal shown gradient polynomial evaluated point good estimate normal vector hyperplane lies closest using insight obtain hyperplane clusters applying standard spectral clustering angle-based afﬁnity matrix spectral curvature clustering another conceptually distinct method ones discussed whose main idea build d-fold tensor follows. d-tuple distinct points dataset value tensor polar curvature points explicit formula) tuning parameter. intuitively polar curvature multiple volume simplex points becomes zero points hyperplane points hyperplane larger volume becomes. obtains hyperplane clusters unfolding tensor afﬁnity matrix upon spectral clustering applied. main bottleneck computational since ransac/kh hybrids. generally speaking single subspace learning method robust outliers handle subspaces high relative dimensions used perform hyperplane clustering either ransac-style kh-style scheme combination both. example method takes dataset hyperplane compute ﬁrst dominant hyperplane remove points dataset lying close compute second dominant hyperplane alternatively start random guess hyperplanes cluster data according distance hyperplanes hyperplane cluster even though large variety single subspace learning methods exist e.g. references lerman zhang methods potentially able handle large relative dimensions particular hyperplanes. addition ransac paper consider possibilities i.e. reaper dpcp described next. reaper. recently proposed single subspace learning method admits interesting theoretical analysis so-called reaper reaper inspired nonconvex optimization problem whose principle minimize euclidean distances points single ddimensional linear subspace matrix appearing thought product rd×d contains columns orthonormal basis non-convex lerman relax convex semi-deﬁnite program optimization problem actually solved reaper; orthogonal projection matrix associated obtained projecting solution onto space rank orthogonal projectors. limitation reaper semi-deﬁnite program become prohibitively large even moderate values difﬁculty circumvented solving iteratively reweighted least squares fashion convergence objective value neighborhood optimal value established lerman dual principal component pursuit similarly ransac reaper dpcp another recently proposed single subspace learning method applied hyperplane clustering. idea dpcp identify single hyperplane maximal respect data maximal hyperplane deﬁned property must contain maximal number points dataset i.e. hyperplane notice maximal hyperplane characterized solution problem non-convex non-smooth optimization problem sphere. case noise dataset consists inlier points drawn single hyperplane normal vector together outlier points general position i.e. unknown permutation matrix unique maximal hyperplane coincides certain uniformity assumptions data points abundance inliers tsakiris vidal asserted unique sign global solution i.e. combinatorial problem non-convex relaxation share unique global minimizer. moreover shown additional assumption principal angle initial estimate large sequence ˆnk} generated recursion linear programs converges sign ﬁnitely many iterations. alternatively attempt solve problem means irls scheme similar fashion reaper. even though theory developed approach experimental evidence tsakiris vidal indicates convergence irls scheme global minimizer methods. general large variety clustering methods adapted perform hyperplane clustering list means exhaustive; rather contains methods intelluctually closest proposal paper. important examples compare paper statistical-theoretic mixtures probabilistic principal component analyzers well information-theoretic agglomerative lossy compression extensive account methods reader referred vidal section develop main theoretical contributions paper concerned properties non-convex minimization problem well recursion linear programs context hyperplane clustering. speciﬁcally particularly interested developing conditions every global minimizer non-convex problem normal vector hyperplanes underlying hyperplane arrangement. towards insightful study associated continuous problem obtained replacing ﬁnite cluster within hyperplane uniform measure unit sphere hyperplane main result direction theorem next introducing certain uniformity parameters measure deviation discrete quantities continuous counterparts adapt analysis discrete case interest furnishes theorem discrete analogue theorem gives global optimality conditions non-convex dpcp problem finally theorem gives convergence guarantees linear programming recursion proofs results deferred data model problem hyperplane clustering consider given collection rd×n points unit sphere hyperplane points orthogonal normal vector i.e. position mean things. first mean linear relations among points ones induced membership hyperplanes. particular every points coming form basis points come least distinct hihi linearly independent. second mean points uniquely deﬁne hyperplane arrangement sense arrangement hyperplanes contains veriﬁed computationally checking scale homogeneous polynomial degree data vidal tsakiris vidal details. assume every precisely points denoted notation unknown permutation matrix indicating hyperplane membership points unknown. moreover assume ordering refer dominant hyperplane. preparations problem hyperplane clustering stated follows given data number hyperplanes associated normal vector according hyperplane membership. turns certain important insights regarding problem respect hyperplane clustering gained examining associated continuous problem. problem note ﬁrst note distance line spanned line spanned thus every global minimizer problem minimizes weighted distances span span span thought representing weighted median lines. medians riemmannian manifolds particular grassmannian manifold active subject research however aware work literature deﬁnes median means work studies advantage working instead solution continuous problem depends solely weights assigned hyperplane recall principal angle i.e. arrangement well geometry arrangement captured principal angles contrast solutions discrete problem also depend distribution points perspective understanding problem unique solution coincides normal dominant hyperplane essential understanding potential hyperplane clustering. towards next provide series results pertaining ﬁrst conﬁguration examine hyperplanes. case weighted geometric median lines spanned normals hyperplanes always corresponds normals theorem arrangement hyperplanes weights global minimizers satisﬁes notice problem recovers normal dominant hyperplane irrespectively separated hyperplanes since according proposition principal angle play role. continuous problem equally favorable recovering normal vectors global minimizers dual situation arrangement consists perfectly separated hyperplanes asserted next theorem. theorems hard prove since hyperplanes objective shown strictly concave function orthogonal hyperplanes objective separable. contrast problem becomes considerably harder non-orthogonal hyperplanes. even characterizing global minimizers function geometry weights seems challenging. nevertheless three hyperplanes equiangular weights equal symmetry conﬁguration allows analytically characterize median function angle arrangement. theorem equiangular hyperplane arrangement weights global minimizers satisﬁes following phase transition proposition whose proof uses nontrivial arguments spherical algebraic geometry particularly enlightening since suggests global minimizers associated normal vectors underlying hyperplane arrangement hyperplanes sufﬁciently separated otherwise seem capturing median hyperplane arrangement. striking similarity results regarding fermat point planar spherical triangles however symmetry theorem removed requiring principal angles or/and weights equal proof technique longer applies problem seems even harder. even intuitively expects interplay angles weights arrangement which hyperplanes sufﬁciently separated sufﬁciently dominant unique global minimizer equal next theorem formalizes intuition. provide intuition meaning quantities theorem begin with ﬁrst term precisely equal second term lower bound objective function discards hyperplane moving admits nice geometric interpretation lower bound small principal angle critical point interestingly means larger larger minimum angle shows critical hyperplanes distinct must sufﬁciently separated finally ﬁrst term second term smallest objective value corresponds simply guarantees global minimizer normals irrespectively θij. finally condition consistent condition requires close hihj sufﬁciently separated again always satisﬁed irrespectively choosing sufﬁciently large since positive term deﬁnition depends manifesting terms close separated used relatively assigned weight removing term objective function corresponds identiﬁed removing associated points re-apply theorem remaining hyperplanes obtain conditions recovering notice conditions independent rather relative assigned weight always satisﬁed large enough finally recursive application theorem furnish conditions sequentially recovering normals however point conditions theorem readily seen stronger necessary. example already know theorem arrangement orthogonal i.e. problem unique minimizer soon ni∀i contrary theorem applied case requires unnecessarily large i.e. condition becomes special case reduces clearly artifact techniques used prove theorem weakness problem terms global optimality properties. improving proof technique theorem open problem. turn attention discrete problem hyperplane clustering dpcp i.e. problems case points described ﬁrst step analysis deﬁne certain uniformity parameters serve link continuous discrete domains. towards note quantity technical necessary deﬁnition. deﬁnition positive integer deﬁne maximum circumradius among circumradii polytopes distinct integers circumradius closed bounded minimum radius among spheres contain set. deﬁne quantity interest theorem strictly stronger theorem surprise since already remarked solution depends geometry weights arrangement also distribution data points note contrast condition theorem appears sides condition theorem nevertheless assumption equivalent positivity quadratic polynomial whose leading coefﬁcient positive hence always satisﬁed sufﬁciently large another interesting connection theorem theorem former seen limit version latter dividing letting inﬁnity keeping ratio ni/n ﬁxed recalling recover conditions theorem next consider linear programming recursion conceptual level main difference linear programming recursion continuous discrete problems respectively behavior depends highly initialization intuitively closer likely recursion converge likelihood becoming larger larger precise technical statement follows. theorem ˆnk} sequence generated linear programming recursion means simplex method initial estimate principal anmin mini> {θi}. small equal suppose enough i.e. quantities appearing theorem harder interpret theorem still give intuition meaning. begin with inequalities represent distinct requirements enforced proof combined guarantee limit point satisﬁed either sufﬁciently large sufﬁciently small avoid pathological situations required negative less natural enforce positive. precisely achieved inequality quite natural condition itself initial estimate needs closer normal well-distributed data inside positive. avoid situations positivity polynomial contradicts relation important leading coefﬁcient positive second requirement satisﬁed large enough thus compatible turns positive data sufﬁciently well distributed captured condition theorem even latter condition sufﬁcient; instead well-distributed data inside next notice conditions theorem directly comparable theorem indeed case global minimizer non-convex problem recursions converge simply close fact must closer i.e. similarly theorems separated hyperplanes hihj easier satisfy condition contrast needs sufﬁciently separated since otherwise becomes large. intuitive explanation less separated rest hyperplanes less resolution linear program distinguishing increase resolution needs either select close select large. acute reader recall quantity appearing becomes larger becomes separated nevertheless inconsistency issues controlling size i.e. increase arbitrarily increase. another look consistency condition depend hence always satisfy selecting large enough. section prove theorems associated continuous problem well theorems associated discrete non-convex minimization problem recursion linear programs respectively. lemma global solution must plane span problem becomes planar i.e. well assume hyperplane arrangement line arrangement note partition arcs among these length strictly less denote next recall continuous objective function hyperplanes written global solution suppose replace −b−b operation change neither arrangement objective. replacement finally suppose neither inside replacing either leads consequently without loss generality assume lies moreover subject rotation perhaps exchanging assume aligned positive x-axis angle measured counter-clockwise lies global solution lemma arrangement equiangular planes angle weights global minimizer either proof ±b±b±b statement clearly holds since sin. suppose must satisfy equations together allowing take value zero must satisfy lemma arrangement equiangular planes angle weights spherical center radius global minimizer must either boundary interior proof first notice boundary global minimizer. already seen proposition vertices principal angle suppose least would give smaller objective hence without loss generality assume addition lemma assume without loss generality vector small joins since must case principal angle less equal conclude sign assume without loss generality i.e. angles either non-negative non-positive. proof lemma know either ﬁrst case angles less equal lemmas show global minimizer problem choice corresponding excluded since always results smaller objective moving staying plane results decreasing angles consequently assume problem becomes unconstrained objective since leading coefﬁcient always negative function always true interval strictly negative. consequently must show long true every non-negative squaring must show descartes rule signs precisely positive root. fact root equal since leading coefﬁcient positive must either everywhere negative everywhere real roots i.e. positive. since conclude long everywhere negative long positive i.e. done. angle matrix denote matrix arises taking absolute values element matrix known σmax σmax. hence result follows recalling |cos| cos. global solution suppose sake contradiction hi∀i i.e. consequently differentiable must satisfy repeat convenience theorem global minimizer suppose sake contradiction hi∀i show exists lower bound course contradiction. towards ﬁrst order optimality condition written lagrange multiplier sign subdifferential function |·|. since points general hyperplane spanned points points come contain remaining points consequently lemma orthogonal precisely αjξj decomposed along index based hyperplane membership instance replace term superscript denotes association hyperplane repeating possible re-indexing note angle well-deﬁned since hypothesis effectively says never drops min. straightforward check hypothesis contradiction. words must equal sign proves ﬁrst part theorem. second part follows noting condition guarantees mini> first follows theory simplex method obtained simplex method satisfy conclusion lemma appendix lemma guarantees {nk} converges critical point problem ﬁnite number steps; denote point words satisfy equation unit norm. last relation contradicted hypothesis i.e. none show suppose sake contradiction colinear i.e. since satisﬁes part proof theorem according principal angle become less consequently using concentration model obtain since core dpcp single subspace learning method well learn hyperplanes ransac used learn hyperplane entire dataset remove points close learn second hyperplane main weakness technique well known consists sensitivity thresholding parameter necessary order remove points. alleviate need knowing good threshold propose replace process removing points process appropriately weighting points. particular suppose solve dpcp problem entire dataset obtain unit -norm vector instead removing points close hyperplane normal vector weight another hyperplane clustering dpcp modify classic k-subspaces computing normal vector cluster dpcp. call resulting method ihl-dpcp; algorithm worth noting since dpcp minimizes -norm distances points hyperplane consistency dictates stopping criterion ihl-dpcp governed points distance point assigned hyperplane words global objective function minimized ihl-dpcp median k-flats solving dpcp problem recall dpcp problem appears algorithms non-convex. tsakiris vidal described four distinct methods solving brieﬂy review here. ﬁrst method ﬁrst proposed sp¨ath watson consists solving recursion linear programs using standard solver gurobi refer method dpcp-r standing relaxed dpcp second approach called dpcp-irls solve using standard iteratively reweighted leastsquares technique algorithm third method ﬁrst proposed solve approximately applying alternative minimization denoised version refer method dpcp-d standing denoised dpcp; algorithm finally fourth method relaxed denoised dpcp replaces problem recursion denoised version synthetic data dataset design. begin evaluating experimentally sequential hyperplane learning algorithm using synthetic data. coordinate dimension data inspired major applications hyperplane arrangements appear. particular recall figure sequential hyperplane learning clustering accuracy function number hyperplanes relative-dimension data balancing white corresponds black two-view geometry works correspondences pairs points. correspondence treated point itself equal tensor product corresponding points thus coordinate dimension consequence choose well choice a-posteriori justiﬁed sufﬁciently larger clustering problem becomes challenging. choice randomly generate hyperplanes sample hyperplane follows. choice total number points dataset number points sampled hyperplane αi−ni− parameter controls balancing clusters means clusters perfectly balanced smaller values lead less balanced clusters. experiment speciﬁed size cluster points cluster sampled zero-mean unit-variance gaussian distribution support corresponding hyperplane. make experiment realistic corrupt points hyperplane adding white gaussian noise standard deviation support direction orthogonal hyperplane. moreover corrupt dataset adding outliers sampled standard zero-mean unit-variance gaussian distribution support entire ambient space. different versions algorithm. dpcp algorithms terminate either maximal number iterations dpcp-r iterations dpcp-r-ddpdp-d dpcp-irls reached algorithm converges within accuracy also compare reaper analog algorithm computation normal vector done irls version reaper instead dpcp. dpcp algorithms maximal number iterations convergence accuracy finally compare ransac predominant method clustering hyperplanes ambient dimensions fairness implement version ransac involves weighting scheme algorithm instead weighting points uses normalized weights discrete probability distribution data points; thus points close already computed hyperplanes probability selected. moreover control running time ransac slow dpcp-r latter slowest among four dpcp algorithms. results. since results single ﬁgure show mean clustering accuracy independent experiments fig. ransac reaper dpcp-r dpdp-irls values well fig. methods accuracy normalized range corresponding black color corresponding white. first observe performance almost methods improves clusters become unbalanced intuitively expected smaller dominant i-th hyperplane respect hyperplanes likely identiﬁed iteration sequential algorithm. exception intuitive phenomenon ransac appears insensitive balancing data. ransac conﬁgured relatively long amount time approximately equal running time dpcp-r turns compensates unbalancing data since many different samplings take place thus leading approximately constant behavior across different fact notice ransac best among methods mean clustering accuracy ranging hand ransac’s performance drops dramatically move higher coordinate dimensions hyperplanes. example mean clustering accuracy ransac drops fact probability sampling points hyperplane decreases increases. secondly proposed algorithm using dpcp-r uniformly best method slight exception clustering accuracy ranges opposed ransac latter case. fact dpcp variants superior ransac reaper challenging scenario dpcp-r dpcp-irls dpcp-d dpcp-r-d gave accuracy respectively opposed ransac reaper. table reports running times seconds readily seen dpcpr slowest among methods remarkably dpcp-d reaper fastest among methods difference approximately orders magnitude dpcp-r. however above none methods performs nearly well dpcp-r. perspective dpcp-irls interesting since seems striking balance running time performance. moving vary outlier ratio mean clustering accuracy independent trials shown fig. fig. experiment hierarchy methods clear algorithm using dpcp-r using dpcp-irls best second best methods respectively rest methods perform equally poorly. example challenging scenario outliers dpcp-r gives accuracy next best method dpcp-irls accuracy; scenario ransac reaper give accuracy respectively dpcp-r-d dpcp-d give respectively. moreover dpcp-r dpcp-irls give accuracy methods give plane clustering real kinect data dataset objective. section explore various iterative hyperplane clustering algorithms using benchmark dataset nyudepthv silberman dataset consists rgbd data instances acquired using microsoft kinect sensor. instance nyudepthv corresponds indoor scene consists data together depth data pixel i.e. total depth values. turn depth data used reconstruct point cloud associated scene. experiment point clouds learn plane arrangements segment pixels corresponding images based plane membership. important problem robotics estimating geometry scene essential successful robot navigation. manual annotation. coarse geometry indoor scenes approximately described union planes many points scene planes thus viewed outliers. moreover always clear many planes select planes are. fact nyudepthv contain ground truth annotation based planes rather scenes annotated semantically view object recognition. reason among total scenes manually annotated scenes dominant planes well-deﬁned capture scene; example figs. speciﬁcally images dominant planar regions manually marked image pixels within regions declared inliers remaining pixels declared outliers. planar region ground truth normal vector computed using dpcp-r. finally planar regions considered distinct manual annotation merged absolute inner product corresponding normal vectors pre-processing. computational reasons hyperplane clustering algorithms directly original point cloud rather weighted subset corresponding superpixel representation image. particular image segmented superpixels entire point sub-cloud corresponding superpixel replaced point geometric center superpixel. account fact planes associated indoor scene afﬁne i.e. pass common origin work homogeneous coordinates i.e. append fourth coordinate point representing superpixel normalize unit -norm. finally weight assigned representative point equal number pixels underlying superpixel. role weight regulate inﬂuence point modeling error algorithms. ﬁrst algorithm test sequential ransac algorithm identiﬁes plane time. secondly explore family variations algorithm based dpcp reaper ransac. particular ihlsvd indicates classic algorithm computes normal vectors singular value decomposition minimizes objective ihldpcp-r-d ihl-dpcp-d ihl-dpcp-irls denote variations dpcp according algorithm depending method used solve dpcp problem similarly recall iterative hyperplane clustering mean process estimating hyperplanes assigning point closest hyperplane reﬁning hyperplanes associated cluster points cluster re-assigning points hyperplanes table plane clustering error subset real kinect dataset nyudepthv. number ﬁtted planes. refer clustering error without spatial smoothing respectively. third method explore hybrid algebraic subspace clustering ransac first vanishing polynomial associated computed ransac instead traditional way; ensures robustness outliers. spectral clustering applied angle-based afﬁnity associated yields clusters. finally iteration ihl-ransac reﬁnes clusters yields normal vector cluster post-processing. algorithms described above generic hyperplane clustering algorithms. hand know nearby points point cloud high chance lying plane simply indoor scenes spatially coherent. thus associate spatially smooth image segmentation algorithm normal vectors algorithm produced minimize conditional-random-field type energy function given plane label point unary term measures cost assigning point plane normal pairwise term measures similarity points chosen parameter indexes neighbors indicator function. unary term deﬁned euclidean distance point plane normal pairwise also values thresholding parameter shl-ransac also denote rest parameters dpcp reaper section convergence accuracy algorithms moreover algorithms conﬁgured allow maximal number random restarts iterations restart parameter mean distance points representing neighboring superpixels. parameter inverse twice maximal row-sum pairwise matrix {w}; achieve balance unary pairwise terms. evaluation. recall none algorithms considered section explicitly conﬁgured detect outliers rather assigns every point plane. thus compute clustering error follows. first restrict output labels algorithm indices dominant ground-truth cluster measure restricted labels identical done computing ratio restricted labels different dominant label. dominant label disabled similar error computed second dominant ground-truth plane finally clustering error taken weighted errors associated dominant plane weights proportional size ground-truth cluster. evaluate algorithms several different settings. first test well algorithms cluster data ﬁrst dominant planes equal total number annotated planes scene. second report clustering error spatial smoothing i.e. without reﬁning clustering minimizing spatial smoothing. former case denoted indicating graph-cuts takes place latter indicated finally account randomness ransac well random initialization average clustering errors independent experiments. notice spatial smoothing improves clustering accuracy considerably gc); e.g. clustering error traditional ihl-svd ground-truth planes drops spatial smoothing employed. moreover intuitively expected clustering error increases ﬁtting planes required; e.g. case error ihl-svd increases planes next note remarkable insensitivity dpcp-based methods ihl-dpcp-d ihl-dpcp-r-d variations parameter sharp contrast shl-ransac sensitive e.g. shl-ransac best method error increases respectively. interestingly hybrid ihl-ransac signiﬁcantly robust; fact terms clustering error best method. hand looking lower part table conclude average rest methods similar behavior. figs. show segmentation results scenes without spatial smoothing. remarkable that even though segmentation fig. contains artifacts expected lack spatial smoothing quality actually good dominant planes correctly identiﬁed. indeed applying spatial smoothing drops error methods studied theoretically algorithmically application recently proposed single subspace learning method dual principal component pursuit problem clustering data close union hyperplanes. gave theoretical conditions nonconvex cosparse problem associated dpcp admits unique global solution equal normal vector underlying dominant hyperplane. proposed sequential parallel hyperplane clustering methods synthetic data dramatically improved upon state-of-the-art methods ransac reaper competitive latter case learning unions planes real kinect data. future research directions include analysis presence noise generalizations unions subspaces arbitrary dimensions even scalable algorithms applications deep networks. work supported grants authors thank prof. daniel robinson applied mathematics statistics department johns hopkins university many useful conversations well many comments helped improve manuscript. section state three results important mathematical analysis already known sp¨ath watson detailed proofs found tsakiris vidal matrix full rank following. lemma suppose problem minb ˆnk= orthogonal linearly independent points accordance lemma sequence {nk} converges critical point problem minbb=", "year": 2017}