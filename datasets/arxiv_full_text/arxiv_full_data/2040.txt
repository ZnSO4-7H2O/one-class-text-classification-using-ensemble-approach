{"title": "Learning Bayesian Network Structure from Massive Datasets: The \"Sparse  Candidate\" Algorithm", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Learning Bayesian networks is often cast as an optimization problem, where the computational task is to find a structure that maximizes a statistically motivated score. By and large, existing learning tools address this optimization problem using standard heuristic search techniques. Since the search space is extremely large, such search procedures can spend most of the time examining candidates that are extremely unreasonable. This problem becomes critical when we deal with data sets that are large either in the number of instances, or the number of attributes. In this paper, we introduce an algorithm that achieves faster learning by restricting the search space. This iterative algorithm restricts the parents of each variable to belong to a small subset of candidates. We then search for a network that satisfies these constraints. The learned network is then used for selecting better candidates for the next iteration. We evaluate this algorithm both on synthetic and real-life data. Our results show that it is significantly faster than alternative search procedures without loss of quality in the learned structures.", "text": "learning bayesian networks often cast optimization ta�k find structure tistically learning tools address optimization using standard heuristic search space extremely procedures candidates problem becomes critical sets large either number stance.�. number attributes. paper. introduce fa�ter learning restricting achieves par­ restricts space. iterative ents variable belong small sub­ candidates. search network satisfies work used selecting next iteration. synthetic show significantly search procedures without loss quality learned structures. recent years growing interest learning structure bayesian networks data somewhat generalizing finding structure. constraint estimate attributes data. usually done using statistical hypothesis exhibits cies. examples approach include sec­ approach poses learning optimization problem. start defining statistically motivated score scribes fitness possible structure data. scores include scores learner's ta�k find struc­ np-hard ture maximizes problem thus need resort heuristic meth­ ods. although approach effi­ cient sensitive common opinion optimization approach better tool learning structure search techniques lated annealing search procedures ample \"generic\" apply knowledge expected structure network learned. example greedy hill-climbing search procedures local changes step apply leads biggest provement score. usual choice \"local\" changes edge addition approximately number variables. becomes acute learn ma�sive data sets. since evaluation collecting candidates various statistics data becomes expensive grows. collect form pa�s data. although recent techniques ity. still expect trivial statistics mains large number attributes possible search eliminated statistical greedy hill-climbing possible removed consideration independent parent course heuristic since marginally strong dependence many domains however mutual information construction gorithm like network consider mutual ever authors cally motivated score. methods mutual information cue. paper within procedure score. provide well ma�sive general cues data restrict willing possible parents variable. potential parents possible since many domains many parents.) score respect restrictions. search niques ca�e perform fa�ter search space significantly show ca�es find best scoring network satisfying constraints graph encodes joint probability network formally first component acyclic variables conditional able independent ents second component sents parameters contains possible denotes particular instantiation graph discussed parents fies unique structure. move change. tions made previous parents la�t move. example hill-climbing change results local maximum. sarily example tice; e.g. advance changes one-arc hill-climbing caching computed counts avoid unnecessary data. cache also allows marginalize counts. thus cache compute summing values usually much fa�ter making pa�s data. dom­ inating factors computational complete data number pa�ses actually training large training section outline framework sparse candidate gorithm fairly intuitive. \"strong dependency\" network. strength dependency often mea�ured using mutual infor­ tween variables mation correlation fact restricting net· work graph tree chow liu's algorithm exactly that. mea�ures mutual information pairs variables selects maximal spanning tree required mutual information mea�ures dependency focus attention find variables parents restrict promising candidate search networks variables parents gives smaller search space hope find good structure choose candidate mitted them. thus mistake initial find inferior ba�ic procedure consider candidate next iteration. would chosen candidate able weaker dependency general form shown figure framework defines whole cla�s algo­ rithms depending choose candidates restrict step perform search max· imize step. choice methods steps mostly independent another. examine detail next sections. another issue stopping types stopping terion terminates candidate since score monotonically bounded function teed stop. however candidate able continue provement score. also enter non-terminating cycle therefore improvement example easily select network thus select parents ba<>ed however know adds information over choice take account example shows general tion iterative select candidates hopefully relevance mutual information? notice empty network estimated thus initial tual information discrepancy joint would expect network parent quite different would mea�ure highly relevant would good approximation even \"weak\" parents opportunity didates score ba�ed semantics recall bayesian non-descendants. whether conditional given holds. holds dependent current parent hand hold either parent descendant general tions usually requires cost calculating iteration. reduced limiting attention large enough mutual information mutual information lected statistics learning small candidate sets section examine problem finding con­ maximal score. strained bayesian network attaining first show introduction sets proves efficiency standard greedy hill-climbing. heuristic sparse structure rial a�pect. proposition follows slight modification finding optimal unconstrained standard though mrbn np-hard even standard heuristics computationally mation compared unconstrained fact complexity needed. tremely large. searching hope better chance finding high-scoring work. although search space size mrbn remains exponential space bayesian networks domain. this note even restrict search bayesian networks parents possible parent sets variable. hand mrbn possible acyclicity change order magnitude sets). sufficient candidate compute input. parents computed marginal­ ization duce divide conquer section binatorial efficiently straints. details bayesian focus underlying specified takes linear decomposition search nent separately. connected problem section components. separator decompose must consider ever goal find small \"bottlenecks\" cycles ways breaking rithms global erwise parents attain compose ciently graph decomposition decomposition component strongly simplest lows cycles components. contains directed maximal strongly clear must disjoint vertices strongly vertices nents. every cycle contained component. within variables. component table summary results domain. report mea�ured number instances) distribution. terms execution statistics disc discrepancy score score ba�ed mea�ure. hill-climbing addition tion procedure possible biggest improvement. convergence. dure augmented keeps list la�t candidates applying applies change results tabu list actually minate procedure failed far. termination scoring structure reported figure graphs showing performance different algorithms show plots score running time measured terms score candidate size candidate points curve sparse candidate iteration. instances \"alarm\" network network ha.� used studies structure pers treated common benchmark field. network contains variables values values values. note although consider ma.�sive search procedure. future plan synthetic data larger networks. table mea�ure score networks found error respect generating distribu­ tions. results rithm particular heuristic networks comparable score found greedy hill climbing. small scale experiment.� sparse candidate records. finally mains examined data text. used data contains mately each) represent vector containing attributes data sets different subsets vocabulary. removing common stop word�. sorting words ba.�ed frequency whole data set. data sets included group designator set) common words. randomly selected total data set. ca.� attributes score selection learn networks rea�onably found greedy hill-climbing time half number sufficient statistics. larger attributes pect consider data sets larger number attributes lack greedy hill-climbing memory store collected stage range scores shown figure. a�sess time would take reach score net­ works found methods seems slower even conservative discrepancy score mea�ure. note initial statistics since tion adds modest number statistics calculate ini­ tially significant efficiency. restricting small number find high-scoring networks efficiently. showed dates taking getting higher scoring networks effects experiments. procedure lead dramatic time. comes small sometimes lead higher scoring networks. second showed restricting variable small group candidate parents sometimes theoretical gorithm. knowledge find polynomial time learning algorithm net­ works in-degree argument might also practical ramifications. showed even exact algorithm pensive guide finding good approxi­ mate solutions. heuristic algorithm sparse candidate structural complete data. setup cost finding statistics much higher since instead counting number stances number stances. requested leads sig­ nificant saving time. similar cost issues occur variant algorithm probabilistic procedure crucial component analysis thousands", "year": 2013}