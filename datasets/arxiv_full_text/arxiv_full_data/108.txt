{"title": "Listen, Attend and Spell", "tag": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "abstract": "We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits characters as outputs. The network produces character sequences without making any independence assumptions between the characters. This is the key improvement of LAS over previous end-to-end CTC models. On a subset of the Google voice search task, LAS achieves a word error rate (WER) of 14.1% without a dictionary or a language model, and 10.3% with language model rescoring over the top 32 beams. By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0%.", "text": "present listen attend spell neural network learns transcribe speech utterances characters. unlike traditional dnn-hmm models model learns components speech recognizer jointly. system components listener speller. listener pyramidal recurrent network encoder accepts ﬁlter bank spectra inputs. speller attentionbased recurrent network decoder emits characters outputs. network produces character sequences without making independence assumptions between characters. improvement previous end-toend models. subset google voice search task achieves word error rate without dictionary language model language model rescoring beams. comparison state-of-the-art cldnn-hmm model achieves deep neural networks improvements various components speech recognizers. commonly used hybrid dnn-hmm speech recognition systems acoustic modeling dnns also produced signiﬁcant gains pronunciation models words phoneme sequences language modeling recurrent models shown improve speech recognition accuracy rescoring n-best lists traditionally components acoustic pronunciation language models trained separately different objective. recent work area attempts rectify disjoint training issue designing models trained end-to-end speech directly transcripts main approaches connectionist temporal classiﬁcation sequence sequence models attention approaches limitations address assumes label outputs conditionally independent other; whereas sequence sequence approach applied phoneme sequences trained end-to-end speech recognition. paper introduce listen attend spell neural network improves upon previous attempts network learns transcribe audio sequence signal word sequence character time. unlike previous approaches make independence assumptions label sequence rely hmms. based sequence sequence learning framework attention consists encoder recurrent neural network named listener decoder named speller. listener pyramidal converts level speech signals higher level features. speller converts higher level features output utterances specifying probability distribution sequences characters using attention mechanism listener speller trained jointly. approach fact pyramidal model listener reduces number time steps attention model extract relevant information from. rare out-of-vocabulary words handled automatically since model outputs character sequence character time. another advantage modeling characters outputs network able generate multiple spelling variants naturally. example phrase triple model produces triple beams model like trouble producing diverse transcripts utterance conditional independence assumptions frames. experiments components necessary work well. without attention mechanism model overﬁts training data signiﬁcantly spite large training three million utterances memorizes training transcripts without paying attention acoustics. without pyramid structure encoder side model converges slowly even month training error rates signiﬁcantly higher errors report here. problems arise acoustic signals hundreds thousands frames makes difﬁcult train rnns. finally reduce overﬁtting speller training transcripts sampling trick training improvements achieves subset google voice search task without dictionary language model. combined language model rescoring achieves wer. comparison google state-of-the-art cldnn-hmm system achieves data even though deep networks successfully used many applications recently mainly used classiﬁcation mapping ﬁxed-length vector output category structured problems mapping variable-length sequence another variable-length sequence neural networks combined sequential models hidden markov models conditional random fields drawback combining approach resulting models cannot easily trained end-to-end make simplistic assumptions probability distribution data. sequence sequence learning framework attempts address problem learning variable-length input output sequences uses encoder sequential variable-length input ﬁxed-length vector. decoder uses vector produce variable-length output sequence token time. training model feeds groundtruth labels inputs decoder. inference model performs beam search generate suitable candidates next step predictions. sequence sequence models improved signiﬁcantly attention mechanism provides decoder information produces output tokens output step last hidden state decoder used generate attention vector input sequence encoder. attention vector used propagate information encoder decoder every time step instead once original sequence sequence model attention vector thought skip connections allow information gradients effectively rnn. sequence sequence framework used extensively many applications machine translation image captioning parsing conversational modeling generality framework suggests speech recognition also direct application formally describe accepts acoustic features insection input seputs emits english characters outputs. quence ﬁlter bank spectra features yseos) c··· spacecommaperiodapostropheunk} output sequence characters. special start-of-sentence token end-ofsentence tokens respectively. listen attend spell model consists sub-modules listener speller. listener acoustic model encoder whose operation listen. speller attentionbased character decoder whose operation attendandspell. listen function transforms original signal high level representation attendandspell function consumes produces probability distribution character sequences figure listen attend spell model listener pyramidal blstm encoding input sequence high level features speller attention-based decoder generating characters listen operation uses bidirectional long short term memory pyramid structure. modiﬁcation required reduce length length input input speech signals hundreds thousands frames long. direct application blstm operation listen converged slowly produced results inferior reported here even month training time. presumably operation attendandspell hard time extracting relevant information large number input time steps. circumvent problem using pyramid blstm similar clockwork successive stacked pblstm layer reduce time resolution factor typical deep btlm architecture output i-th time step j-th layer computed follows model stack pblstms bottom blstm layer reduce time resolution times. allows attention model extract relevant information smaller number times steps. addition reducing resolution deep architecture allows model learn nonlinear feature representations data. figure visualization pblstm. pyramid structure also reduces computational complexity. next section show attention mechanism features computational complexity thus reducing speeds learning inference signiﬁcantly. describe attendandspell function. function computed using attention-based lstm transducer every output step transducer produces probability distribution next character conditioned characters seen previously. distribution function decoder state context decoder state function previous state previously emitted character context ci−. context vector produced attention mechanism. speciﬁcally characterdistribution softmax outputs characters layer lstm. time step attention mechanism attentioncontext generates context vector encapsulating information acoustic signal needed generate next character. attention model content based contents decoder state matched contents representing time step generate attention vector used linearly blend vectors create speciﬁcally decoder timestep attentioncontext function computes scalar energy time step using vector scalar energy converted probability distribution times steps using softmax function. used listen attendandspell functions trained jointly end-to-end speech recognition. sequence sequence methods condition next step prediction previous characters maximizes probability however inference groundtruth missing predictions suffer model trained resilient feeding predictions time steps. ameliorate effect trick proposed training instead always feeding ground truth transcript next step prediction sometimes sample previous character distribution inputs next step predictions system deep network appear type pretraining would required. however experiments found need pretraining. particular attempted pretrain listen function context independent context dependent phonemes generated conventional gmm-hmm system. softmax network attached output units listener used make multi-frame phoneme state predictions improvements. also attempted phonemes joint objective target found improvements. decoding performed simple left-to-right beam search algorithm similar maintain partial hypotheses starting start-of-sentence token. timestep partial hypothesis beam expanded every possible character likely beams kept. token encountered removed beam added complete hypothesis. dictionary optionally added constrain search space valid words however found necessary since model learns spell real words almost time. vast quantities text data compared amount transcribed speech utterances. language models trained text corpora alone similar conventional speech systems rescore beams language model. model small bias shorter utterances normalize probabilities number characters |y|c hypothesis combine language model probability used dataset approximately three million google voice search utterances experiments. approximately hours utterances randomly selected held-out validation set. data augmentation performed using room simulator adding different types noise reverberations; noise sources obtained youtube environmental recordings daily events increased amount audio data times. -dimensional log-mel ﬁlter bank features computed every used acoustic inputs listener. separate utterances representing approximately hours data used test data. noisy test data also created using corruption strategy applied training data. training sets anonymized hand-transcribed representative googles speech trafﬁc. text normalized converting characters lower case english alphanumerics punctuations space comma period apostrophe kept tokens converted unknown token. mentioned earlier utterances padded start-of-sentence end-of-sentence tokens. state-of-the-art model dataset cldnn-hmm system described cldnn system achieves clean test noisy test set. however note cldnn uses unidirectional cldnns would certainly beneﬁt even bidirectional cldnn architecture. listen function used layers pblstm nodes blstm operates input. reduced time resolution times. spell function used layer lstm nodes each. weights initialized uniform distribution asynchronous stochastic gradient descent used training model learning rate used geometric decay utterances used distbelief framework replicas minibatch utterances. order speed training sequences grouped buckets based frame length model trained using groundtruth previous characters results validation stopped improving. took approximately weeks. model decoded using beam width achieved clean test noisy test without dictionary language model. found constraining beam search dictionary impact wer. rescoring beams n-gram language model used cldnn system using language model weight improved results clean noisy test sets respectively. note convenience decode language model rather rescored beams. possible gains could achieved using language model decoding. mentioned section mismatch training testing. training model conditioned correct previous characters testing mistakes made model corrupt future predictions. trained another model sampling previous character distribution probability improved results clean noisy test sets respectively language model rescoring used. language model rescoring achevied clean noisy test sets respectively. table summarizes results. table comparison clean noisy google voice search task. cldnn-hmm system state-of-the-art system listen attend spell models decoded beam size language model rescoring applied beams sampling trick applied bridge training inference. tional ﬁlters could lead improved results reported improve performance relative clean speech relative noisy speech compared non-convolutional architectures figure alignments character outputs audio signal produced listen attend spell model utterance much would woodchuck chuck. content based attention mechanism able identify start position audio sequence ﬁrst character correctly. alignment produced generally monotonic without need location based priors. content-based attention mechanism creates explicit alignment characters audio signal. visualize attention mechanism recording attention distribution acoustic sequence every character output timestep. figure visualizes attention alignment characters ﬁlterbanks utterance much would woodchuck chuck. particular utterance model learnt monotonic distribution without location priors. words woodchuck chuck acoustic similarities attention mechanism slightly confused emitting woodchuck dilution distribution. attention model also able identify start utterance properly. investigate correlation performance model width beam search without language model rescoring. figure shows effect decode beam width clean test set. consistent improvements increasing beam width observe signiﬁcant beneﬁts. beam width language model rescoring. rescoring beams oracle produces clean test noisy test set. figure effect decode beam width clean google voice search task. reported wers without dictionary language model language model rescoring oracle different beam widths. ﬁgure shows good results obtained even relatively small beam size. measure performance model function number words utterance. expect model poorly longer utterances limited number long training utterances distribution. hence surprising longer utterances larger error rate. deletions dominate error long utterances suggesting missing words. surprising short utterances perform quite poorly. here substitutions insertions main sources errors suggesting model split words apart. figure also suggests model struggles generalize long utterances trained distribution shorter utterances. possible location-based priors help situations reported study performance model rare words. recall metric indicate whether word appears utterance regardless position figure reports recall word test distribution function word frequency training distribution. rare words higher variance lower recall frequent words typically higher figure correlation error rates number words utterance. reported without dictionary language model language model rescoring oracle clean google voice search task. data distribution respect number words utterance overlaid ﬁgure. performs poorly short utterances despite abundance data. also fails generalize well longer utterances trained distribution shorter utterances. insertions substitutions main sources errors short utterances deletions dominate error long utterances. recall. word occurs times training however recall even language model rescoring. word frequently mis-transcribed suggests improvements needed language model. contrast word walkerville occurs training recall suggests recall word depends frequency training acoustic uniqueness. experiments observed learn multiple spelling variants given acoustics. table shows beams utterance includes triple seen model produces triple within four beams. decoder able generate varied parses next step prediction model makes assumptions probability distribution using chain rule decomposition. would difﬁcult produce differing transcripts using conditional independence assumptions conditionally independent conventional dnn-hmm systems would require spellings pronunciation dictionary generate spelling permutations. also seen model produced even though acoustically different presumably language model overpowers acoustic signal case. training corpus common phrase suspect language model figure correlation word frequency training distribution recall test distribution. general rare words report worse recall compared frequent words. also surprised model capable handling utterances repeated words despite fact uses content-based attention. table shows example utterance repeated word. since implements content-based attention expected lose attention decoding steps produce word less times number times word spoken. seen example even though seven repeated three times model successfully outputs seven three times. hints location-based priors needed repeated contents. eight nine four minus seven seven seven eight nine four minus seven seven seven eight nine four nine seven seven seven eight nine four minus seven seventy seven eight nine four nine seven seven seven presented listen attend spell attention-based neural network directly transcribe acoustic signals characters. based sequence sequence framework pyramid structure encoder reduces number timesteps decoder attend trained end-to-end main components. ﬁrst component listener pyramidal acoustic encoder transforms input sequence high level feature representation. second component speller decoder attends high level features spells transcript character time. system concepts phonemes rely pronunciation dictionaries hmms. bypass conditional independence assumptions show learn implicit language model generate multiple spelling variants given acoustics. improve results used samples softmax classiﬁer decoder inputs next step prediction training. finally showed language model trained additional text used rerank hypotheses. thank tara sainath babak damavandi helping data language models helpful comments. also thank andrew ashish agarwal samy bengio eugene brevdo greg corrado andrew jeff dean rajat monga christopher olah mike schuster noam shazeer ilya sutskever vincent vanhoucke google brain team helpful comments suggestions technical assistance.", "year": 2015}