{"title": "Clustering hidden Markov models with variational HEM", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "The hidden Markov model (HMM) is a widely-used generative model that copes with sequential data, assuming that each observation is conditioned on the state of a hidden Markov chain. In this paper, we derive a novel algorithm to cluster HMMs based on the hierarchical EM (HEM) algorithm. The proposed algorithm i) clusters a given collection of HMMs into groups of HMMs that are similar, in terms of the distributions they represent, and ii) characterizes each group by a \"cluster center\", i.e., a novel HMM that is representative for the group, in a manner that is consistent with the underlying generative model of the HMM. To cope with intractable inference in the E-step, the HEM algorithm is formulated as a variational optimization problem, and efficiently solved for the HMM case by leveraging an appropriate variational approximation. The benefits of the proposed algorithm, which we call variational HEM (VHEM), are demonstrated on several tasks involving time-series data, such as hierarchical clustering of motion capture sequences, and automatic annotation and retrieval of music and of online hand-writing data, showing improvements over current methods. In particular, our variational HEM algorithm effectively leverages large amounts of data when learning annotation models by using an efficient hierarchical estimation procedure, which reduces learning times and memory requirements, while improving model robustness through better regularization.", "text": "hidden markov model widely-used generative model copes sequential data assuming observation conditioned state hidden markov chain. paper derive novel algorithm cluster hmms based hierarchical algorithm. proposed algorithm clusters given collection hmms groups hmms similar terms distributions represent characterizes group cluster center i.e. novel representative group manner consistent underlying generative model hmm. cope intractable inference e-step algorithm formulated variational optimization problem eﬃciently solved case leveraging appropriate variational approximation. beneﬁts proposed algorithm call variational demonstrated several tasks involving time-series data hierarchical clustering motion capture sequences automatic annotation retrieval music online hand-writing data showing improvements current methods. particular variational algorithm eﬀectively leverages large amounts data learning annotation models using eﬃcient hierarchical estimation procedure reduces learning times memory requirements improving model robustness better regularization. keywords hierarchical algorithm clustering hidden markov model hidden markov mixture model variational approximation time-series classiﬁcation hidden markov model probabilistic model assumes signal generated double embedded stochastic process. discretetime hidden state process evolves markov chain encodes dynamics signal observation process encodes appearance signal time conditioned current state. hmms successfully applied variety ﬁelds including speech recognition music analysis identiﬁcation online hand-writing recognition analysis biological sequences clustering time series data paper clustering hmms. precisely interested algorithm that given collection hmms partitions clusters similar hmms also learning representative cluster center concisely appropriately represents cluster. similar standard k-means clustering except data points hmms instead vectors various applications motivate design clustering algorithms ranging hierarchical clustering sequential data hierarchical indexing fast retrieval reducing computational complexity estimating mixtures hmms large datasets clustering hmms eﬃciently estimated many small subsets data compact mixture model data. however little work clustering therefore applications. existing approaches clustering hmms operate directly parameter space grouping hmms according suitable pairwise distance deﬁned terms parameters. however parameters non-linear manifold simple application k-means algorithm succeed task since assumes real vectors euclidean space. addition approach would additional complication parameters particular generative model unique i.e. permutation states leads generative model. solution proposed jebara ﬁrst constructs appropriate similarity matrix hmms clustered applies spectral clustering. approach proven successful group hmms similar clusters directly address issue generating cluster centers. cluster still represented choosing given hmms e.g. spectral clustering procedure maps closest spectral clustering center. however suboptimal applications clustering example hierarchical estimation annotation models. another distance distributions suitable spectral clustering divergence practice approximated sampling sequences model computing log-likelihood instead paper propose cluster hmms directly respect probability distributions represent. probability distributions hmms used throughwhole clustering algorithm construct initial embedding clustering output distributions hmms marginalized hidden-state distributions avoid issue multiple equivalent parameterizations hidden states. derive hierarchical expectation maximization algorithm that starting collection input hmms estimates smaller mixture model hmms concisely represents clusters input hmms algorithm generalization algorithm algorithm considered special case mixture delta functions input. main diﬀerence e-step. algorithm computes suﬃcient statistics given observed data algorithm calculates expected sufﬁcient statistics averaged possible observations generated input probability models. historically ﬁrst algorithm designed cluster gaussian probability distributions algorithm starts gaussian mixture model components reduces another fewer components mixture components reduced represents i.e. clusters group original gaussian mixture components. recently chan derived algorithm cluster dynamic texture models probability distributions. applied successfully construct hierarchies eﬃcient image indexing cluster video represented estimate gmms mixtures large datasets semantic annotation images video music extend framework gmms mixtures hmms additional marginalization hidden-state processes required dtms. however gaussians allow tractable inference e-step longer case hmms. therefore work derive variational formulation algorithm leverage variational approximation derived hershey make inference e-step tractable. resulting algorithm clusters hmms also learns novel hmms representative centers cluster. resulting vhem algorithm generalized handle classes graphical models exact computation e-step standard would intractable leveraging similar variational approximations e.g. general case hmms emission probabilities continuous exponential family distributions. compared spectral clustering algorithm vhem algorithm several advantages make suitable variety applications. first vhem algorithm capable clustering well learning novel cluster centers manner consistent underlying generative probabilistic framework. addition since require sampling steps also scalable memory requirements. consequence vhem hmms allows eﬃcient estimation mixtures large datasets using hierarchical estimation procedure. particular intermediate mixtures ﬁrst estimated parallel running algorithm small independent portions dataset. ﬁnal model estimated intermediate models using vhem algorithm. vhem based maximumlikelihood principles drives model estimation towards similar optimal parameter values performing maximum-likelihood estimation full dataset. addition averaging possible observations compatible input models e-step vhem provides implicit form regularization prevents over-ﬁtting improves robustness learned models compared direct application algorithm full dataset. note that contrast vhem construct kernel summary contributions paper three-fold derive variational formulation algorithm clustering hmms generates novel centers representative cluster; evaluate vhem variety clustering annotation retrieval problems involving time-series data showing improvement current clustering methods; iii) demonstrate experiments vhem eﬀectively learn hmms large sets data eﬃciently standard improving model robustness better regularization. respect previous work vhem algorithm originally proposed remainder paper organized follows. review hidden markov model hidden markov mixture model section present derivation vhem-hm algorithm section followed discussion section finally present experimental results sections generated double embedded stochastic process observation time depends state discrete hidden variable sequence hidden states evolves ﬁrst-order markov chain. hidden variables take values evolution hidden process encoded state transition matrix ]ββ=...s entry p|xt probability transitioning state state initial state distribution tion here assume emission density time-invariant modeled reduce clutter assume hmms number hidden states emission probabilities mixture components. derivation could easily extended general case though. algorithms clustering hmms serve wide range applications hierarchical clustering sequential data hierarchical indexing fast retrieval reducing computational complexity estimating mixtures hmms large weakly-annotated datasets clustering hmms eﬃciently estimated many small subsets data compact mixture model data. work derive hierarchical algorithm clustering hmms respect probability distributions. approach problem clustering hmms reducing input mixture large number components mixture fewer components. note diﬀerent hmms input mixture allowed method estimating reduced mixture model generate samples input mixture perform maximum likelihood estimation i.e. maximize log-likelihood samples. however avoid explicitly generating samples instead maximize expectation log-likelihood respect input mixture model thus averaging possible samples input mixture model. dependency samples replaced marginalization respect input mixture model. marginalization tractable gaussians longer case hmms. therefore work derive variational formulation algorithm specialize case leveraging variational approximation proposed hershey note proposed alternative mcmc sampling computation divergence hmms used learning context far. base hidden markov mixture model components. goal vhem algorithm reduced hidden markov mixture model components represents well. likelihood random sequence given soft clustering original components groups cluster membership encoded assignment variables represent responsibility reduced mixture component base mixture component i.e. ˆzij finally take expectation virtual samples estimation carried eﬃcient manner requires knowledge parameters base model without need generating actual virtual samples. always indexing individual gaussian components emissions base reduced models respectively. individual gaussian components reduced model. finally reduced mixture expectations also short-hand model notation imply conditioning model. addition expectations assumed taken respect output variable unless otherwise speciﬁed. example eyτ|z=im. table summarizes notation used derivation including variable names model parameters short-hand notations probability distributions expectations. bottom table also summarizes variational lower bound variational distributions introduced subsequently. respect uses large numbers turn virtual samples expectation base model components diﬀerent objective function derive vhem algorithm. estimate maximizing eventually lead estimate maximizing allows strictly preserve variational lower bound would otherwise ruined applying large numbers general approach deal maximum likelihood estimation presence hidden variables algorithm traditional formulation algorithm presented alternation expectation step maximization step work take variational perspective views step maximization step. variational e-step ﬁrst obtains family lower bounds log-likelihood indexed variational parameters optimizes variational parameters tightest bound. corresponding m-step maximizes lower bound respect model parameters. advantage variational formulation readily allows useful extensions algorithm replacing diﬃcult inference e-step variational approximation. practice achieved restricting maximization variational e-step smaller domain lower bound tractable. derive lower bound expected log-likelihood cost function derivation proceed successively applying lower bound expected log-likelihood term arises. result nested lower bounds. ﬁrst deﬁne following three lower bounds expected log-likelihood lower bound looking individual term likelihood mixture assignment component hence introducing variational distribution fact i.i.d. samples. observation log-likelihood essentially mixture distribution hence expectation cannot calculated directly. instead lower bound likelihood given observation variable hidden variable state sequence therefore introduce variational distribution state sequence depends particular sequence note emission distribution hence observation variable hidden variable therefore introduce variational distribution conditioned observation arising component apply note standard algorithm derived assignment probabilities based expected log-likelihoods components hms). variational algorithm expectations replaced lower bounds number times hmms modeling sequences generated expected number transitions state state modeling sequences generated note covariance matrices reduced models include additional outer-product term acts regularize covariances base models. regularization eﬀect derives e-step averages possible observations base model. proposed vhem-hm algorithm clusters hmms directly distributions represent learns novel cluster centers compactly represent structure cluster. application vhem-hm algorithm hierarchical clustering hmms. particular vhem-hm algorithm used recursively cluster centers produce bottom-up hierarchy input hmms. since cluster centers condense structure clusters represent vhem-hm algorithm implicitly leverage rich information underlying structure clusters expected impact positively quality resulting hierarchical clustering. another application vhem eﬃcient estimation data using hierarchical estimation procedure break learning problem smaller pieces. first data split small portions intermediate hmms learned portion standard then ﬁnal model estimated intermediate models using vhem-hm algorithm. vhem standard based similar maximum-likelihood principles drives model estimation towards similar optimal parameter values performing estimation directly full dataset. however compared direct estimation vhem-hm memorytime-eﬃcient. first longer requires storing memory entire data parameter estimation. second need evaluate likelihood samples iteration converges eﬀective estimates shorter times. note even parallel implementation could eﬀectively handle high memory requirements parallel-vhem still require fewer resources parallel-em. addition hierarchical procedure estimation intermediate models easily parallelized since learned independently other. finally hierarchical estimation allows eﬃcient model updating adding data. assuming previous intermediate models saved re-estimating requires learning intermediate models data followed running vhem again. since estimation intermediate models typically computationally intensive vhem stage reusing previous intermediate models lead considerable computational savings re-estimating result models generalize better lastly virtual samples vhem implicitly generates maximum-likelihood estimation need length actual input data estimating intermediate models. making virtual sequences relatively short positively impact time vhem iteration. achieved without loss modeling accuracy show section scalar length virtual sequences. case corresponds bhattacharyya aﬃnity. approach indirectly leverages probability distributions represented hmms proven successful grouping hmms similar clusters several limitations. first spectral clustering algorithm cannot produce novel cluster centers represent clusters suboptimal several applications clustering. example implementing hierarchical clustering spectral embedding space clusters represented single points embedding space. fail capture information local structure clusters that using vhem-hm would encoded novel cluster centers. hence expect vhem-hm produce better hierarchical clustering spectral clustering algorithm especially higher levels hierarchy. because building level vhem leverage information lower levels encoded cluster centers. simple extension ppk-sc obtain cluster center select input spectral clustering algorithm maps closest spectral clustering center. however method cluster centers limited existing input hmms instead hmms optimally condense structure clusters. therefore expect novel cluster centers learned vhem-hm better represent clusters. involved hybrid solution learn cluster centers vhem-hm obtaining clusters ppk-sc using vhem-hm algorithm summarize hmms within ppk-sc cluster single hmm. however expect vhem-hm algorithm learn accurate clustering models since jointly learns clustering centers optimizing single objective function second drawback spectral clustering algorithm construction inversion similarity matrix input hmms costly operation number large therefore expect vhem-hm computationally eﬃcient spectral clustering algorithm since directly operating probability distributions hmms require construction initial embedding costly matrix operation large kernel matrices. finally jebara note exact computation cannot carried eﬃciently unless diﬀerent values jebara propose approximate alternative kernel function eﬃciently computed; alternative kernel function however guaranteed invariant diﬀerent equivalent representations hidden state process note spectral clustering algorithms similar applied kernel matrices based aﬃnity scores distributions similarity jebara examples found earlier work hmm-based clustering time-series juang rabiner lyngso bahlmann burkhardt panuccio particular juang rabiner propose approximate log-likelihood distributions computing log-likelihood real samples generated model other. extensions juang rabiner proposed zhong ghosh yang work pursue comparison various similarity functions implement spectral clustering based similarity showed superior). hmms also clustered sampling number time-series hmms base mixture applying algorithm cluster time-series. despite simplicity approach would suﬀer high memory time requirements especially dealing large number input hmms. first generated samples need stored memory. second evaluating likelihood generated samples iteration computationally intensive prevents algorithm converging eﬀective estimates acceptable times. contrary vhem-hm eﬃcient computation memory usage replaces costly sampling step expectation. additional problem sampling that simple application algorithm time-series generated input assigned diﬀerent clusters output consequence resulting clustering necessary consistent since case corresponding input clearly assigned single cluster. experiments circumvent problem deﬁning appropriate constrains assignment variables. eﬃciently solved jebara projunction tree algorithm pose alternative kernel applies power operation terms i.e. rather entire terms joint probabilities vhem algorithm similar spirit bregman-clustering banerjee algorithms base clustering kl-divergence divergence expected log-likelihood diﬀer entropy term aﬀect clustering. main diﬀerences setting expected log-likelihood computable closed form hence vhem uses approximation; vhem-hm clusters random processes whereas limited single random variables. next sections validate points raised discussion experimental evaluation using vhem-hm algorithm. particular consider clustering experiments section density estimation automatic annotation retrieval section application exploits beneﬁts vhem. first show vhem-hm accurate clustering ppk-sc particular higher levels hierarchical clustering experiment synthetic data similarly annotation retrieval results section favor vhem-hm ppk-sc standard suggesting vhem-hm robust effective density estimation. finally experiments running time vhem-hm compares favorably clustering algorithms; ppk-sc suffers long delays number input hmms large standard algorithm considerably slower. demonstrates vhem-hm eﬃcient clustering hmms. section present empirical study vhem-hm algorithm clustering hierarchical clustering hmms. clustering hmms consists partitioning input hmms groups similar hmms. hierarchical clustering involves organizing input hmms multi-level hierarchy levels applying clustering recursive begin experiment hierarchical clustering input hmms clustered estimated sequence motion capture data then present simulation study clustering synthetic hmms first provide overview diﬀerent algorithms used study. hmms applying spectral clustering. work jebara considered hmms single gaussian emissions always give satisfactory results experiments. hence extended allowing emissions derived similarity general case using preliminary experiments found best performance integrating sequences length finally also extend jebara construct multi-level hierarchies using hierarchical k-means spectral clustering embedding. samples generated input hmms rather expectation sample observation sequences estimate em-hm algorithm smyth modiﬁed single assignment variable sample obtain consistent clustering. many real-life applications goal cluster collection time series i.e. observed sequences. although input data collection hmms case still clustered vhem-hm algorithm ﬁrst modeling sequence using hmms input vhem-hm algorithm. time-series data input also possible clustering approaches model sequence hmm. hence hierarchical motion clustering experiments also compare following algorithms clusters time-series data directly second clusters time series modeling sequence dynamic texture model levels) proceed bottom fashion build level simply reclustering given time series smaller number clusters using extend algorithm single assignment variable sequences within cluster immediately lower level hierarchy. modiﬁcation preserves hierarchical clustering property sequences cluster remain together higher levels. dynamical systems i.e. dynamic textures hierarchical clustering performed using hierarchical algorithm mixtures analogous vhem-hm. main diﬀerence that hem-dtm time-series modeled continuous state space unimodal observation model whereas vhem-hm uses discrete state space multimodal observations several metrics quantitatively compare results diﬀerent clustering algorithms. first calculate rand-index measures correctness proposed clustering given ground truth clustering. intuitively index measures consistent cluster assignments ground truth second consider log-likelihood used smyth evaluate clustering. measures well clustering input data. time series given input data compute log-likelihood clustering log-likelihoods input sequence cluster center assigned. input data consists hmms evaluate log-likelihood clustering using expected log-likelihood observations generated input cluster center assigned. ppk-sc cluster center estimated running vhem-hm algorithm hmms assigned cluster. note log-likelihood particularly appropriate compare vhem-hm shem-hm em-hm hem-dtm since explicitly optimize however unfair ppk-sc since method optimizes similarity log-likelihood. consequence also measure cluster-compactness directly related ppk-sc optimizes for. cluster-compactness average intra-cluster pair-wise similarity. performance metric favors methods produce clusters high intra-cluster similarity. figure example hierarchical clustering mocap dataset vhem-hm ppk-sc. diﬀerent colors represent diﬀerent motion classes. vertical bars represent clusters colors indicating proportions motion classes cluster numbers x-axes representing clusters’ indexes. level clusters motion sequence. levels clusters respectively. vhem almost clusters level populated examples single motion class. error vhem clustering portion soccer basket probably actions involve sequence movement shot pause. moving hierarchy vhem algorithm clusters similar motions classes together level creates dichotomy motion classes. ppk-sc also clusters motion sequences well level incorrectly aggregates soccer quite diﬀerent dynamics. level clustering obtained ppk-sc harder interpret vhem. experiment test vhem algorithm hierarchical motion clustering motion capture data i.e. time series representing human locomotions actions. hierarchically cluster collection time series ﬁrst model time series cluster hmms hierarchically. since summarizes appearance dynamics particular motion sequence represents structure encoded hierarchy hmms directly applies original motion sequences. jebara uses similar approach cluster motion sequences applying ppk-sc cluster hmms. however extend study hierarchies multiple levels. experiment motion capture datasets mocap dataset vicon physical action dataset mocap dataset motion examples spanning diﬀerent classes example sequence -dimensional vectors representing -coordinates body markers tracked spatially time. figure illustrates typical examples. built hierarchy levels. ﬁrst level formed hmms learned individual motion example next three levels contain hmms. perform hierarchical clustering vhem-hm ppk-sc emhm shem-hm hem-dtm quence consists time series -dimensional vectors representing -coordinates body markers captured using vicon tracker. dataset includes normal aggressive activities performed human subjects single time. build hierarchy levels starting hmms ﬁrst level using next four levels. experiment repeated times vhem-hm ppk-sc using diﬀerent random initializations algorithms. similar experiments varied number levels hierarchy number clusters level noted similar relative performances various clustering algorithms datasets. example hierarchical clustering mocap dataset vhem-hm illustrated figure ﬁrst level vertical represents motion sequence diﬀerent colors indicating diﬀerent ground-truth classes. second level clusters shown vertical bars colors indicating proportions motion classes cluster. almost clusters populated examples single motion class demonstrates vhem group similar motions together. note error vhem clustering portion soccer examples basket. probably caused similar dynamics actions consist sequence movement shot pause. moving hierarchy vhem algorithm clusters similar motion classes together example walk walk clustered together level highest level creates dichotomy rest motion classes. desirable behavior kinetics sequences considerably diﬀerent rest. right figure experiment repeated ppk-sc. ppk-sc clusters motion sequences properly table hierarchical clustering mocap dataset using vhem-hm ppk-sc shemhm em-hm hem-dtm. number brackets shem-hm represents number real samples used. computed rand-index data log-likelihood cluster compactness level hierarchy registered time learn hierarchical structure. diﬀerences rand-index levels statistically signiﬁcant based paired t-test conﬁdence incorrectly aggregates soccer level even though quite diﬀerent dynamics. furthermore highest level hierarchical clustering produced ppk-sc harder interpret vhem. table presents quantitative comparison ppk-sc vhem-hm level hierarchy. vhem-hm lower rand-index ppk-sc level vhem-hm higher rand-index level level terms cluster-compactness observe similar results. particular vhem-hm higher cluster-compactness ppk-sc level overall keeping mind ppk-sc explicitly driven ppk-similarity vhem-hm algorithm results considered strongly favor vhem-hm addition data log-likelihood vhem-hm higher ppk-sc level hierarchy. suggests novel cluster centers learned vhem-hm motion capture data better spectral cluster centers. conclusion supported results density estimation experiments sections note higher hierarchy clearly eﬀect manifested. comparing methods em-hm generally lower rand-index vhem-hm ppk-sc em-hm directly clusters original motion sequences vhem-hm ppk-sc implicitly integrate possible virtual variations original motion sequences results robust clustering procedures. addition em-hm considerably longer running times vhem-hm ppk-sc since needs evaluate likelihood training sequences iteration levels. results table favor vhem-hm shem-hm empirically validate variational approximation vhem uses learning. example using samples running shem-hm takes orders magnitude time vhem-hm still achieve performance competitive vhem-hm. eﬃcient closed-form expression averaging possible virtual samples vhem approximates suﬃcient statistics virtually unlimited number observation table hierarchical clustering vicon physical action dataset using vhem-hm ppk-sc. performance measured terms rand-index data log-likelihood cluster-compactness level. diﬀerences rand-index levels statistically signiﬁcant based paired t-test conﬁdence test failed level sequences without need using real samples. additional regularization eﬀect improves robustness learned cluster centers. contrast shemhm uses real samples requires large number learn accurate models results signiﬁcantly longer running times. finally table also report hierarchical clustering performance hem-dtm. vhem-hm consistently outperforms hem-dtm terms rand-index data log-likelihood. since vhem-hm hem-dtm based hierarchical algorithm learning clustering indicates hmm-based clustering models appropriate dt-based models human mocap data. note that ppk-sc also hmm-based lower rand-index hem-dtm level suggests ppk-sc optimally cluster hmms. table presents results using vhem-hm ppk-sc cluster vicon physical action dataset. algorithms performs similarly terms rand-index lower levels hierarchy higher levels vhem-hm outperforms ppk-sc. addition vhem-hm registers higher data loglikelihood ppk-sc level hierarchy. this again suggests learning cluster centers vhem-hm algorithm retains information clusters’ structure ppk-sc. finally compared vhem-hm ppk-sc produces clusters compact terms similarity. however necessarily imply better agreement ground truth clustering evinced rand-index metrics. synthesized generating random sequence length nid) estimating parameters corrupted version note procedure adds noise observation space. number noisy versions noise variance collection original hmms created follows. number always number hidden states hmms emission distributions single one-dimensional gaussians length vhem-hm ppk-sc algorithms used cluster synthesized hmms groups quality resulting clusterings measured rand-index cluster-compactness expected log-likelihood discovered cluster centers respect original hmms. expected log-likelihood computed using lower bound original hmms assigned likely cluster center. results averages trials. figure reports performance metrics varying number noisy versions original hmms noise variance experimental settings. majority settings clustering produced vhem-hm superior produced ppk-sc considered metrics exception experiment where noise variance ppk-sc best terms rand-index cluster compactness. interesting note performance vhem-hm ppk-sc generally larger values believe because limited number input hmms available ppk-sc produces embedding lower quality. aﬀect vhem-hm since clusters distribution space embedding. results suggest that clustering hmms directly distribution space vhemhm generally robust ppk-sc performance instead depends quality underlying embedding. experiment evaluate vhem-hm estimating annotation models contentbased music auto-tagging. generative time-series model allow account timbre well longer term temporal dynamics modeling musical signals. therefore music annotation retrieval applications expected prove eﬀective frequency cepstral coeﬃcients half-overlapping windows audio signal augmented ﬁrst second instantaneous derivatives. song represented collection audio fragments sequences audio features using dense sampling overlap. content probability distribution. i.e. estimate audio fragments songs database associated using hierarchical estimation procedure based vhem-hm. speciﬁcally database ﬁrst processed song level using algorithm learn components song audio fragments. then song-level labeled pooled together form large vhem-hm algorithm used reduce ﬁnal tag-model components number training songs particular tag). given tag-level models song represented vector posterior probabilities extracting features song computing likelihood features tag-level model applying bayes’ rule. test song annotated top-ranking tags smn. retrieve songs given query collection songs ranked tag’s probability smns. compare vhem-hm three alternative algorithms estimating models ppk-sc ppk-sc-hybrid em-hm. three alternatives number mixture components models ppk-sc methods leverage work jebara learn models place vhem-hm algorithm second stage hierarchical estimation procedure. found necessary implement ppk-sc approaches song. songs structural parts intro verse chorus solo bridge outro. experiment able successfully estimate accurate models shemhm. particular shem-hm requires generating appropriately large number real samples produce accurate estimates. however computational limits cluster able test shem-hm using small number samples. preliminary experiments registered performance slightly chance level training times still twice longer vhem-hm. comparison vhem-hm shem-hm density estimation reader refer experiment section online hand-writing classiﬁcation retrieval. level component since computational cost constructing initial embedding scales poorly number input hmms. ppk-sc ﬁrst applies spectral clustering song-level hmms selects cluster centers hmms closest spectral cluster centers spectral embedding. ppk-sc-hybrid hybrid method combining ppk-sc clustering vhem-hm estimating cluster centers. speciﬁcally spectral clustering cluster centers estimated applying vhem-hm hmms assigned resulting clusters. words ppk-sc ppk-sc-hybrid spectral clustering summarize collection song-level hmms centers forming model. mixture weight component proportional number hmms assigned cluster. em-hm models estimated directly audio fragments relevant songs using em-hm algorithm. empirically found that runtime requirements em-hm must non-overlapping audiofragments evenly subsample average resulting sequences used vhem-hm. note that however em-hm still using actual song data believe reasonable comparison vhem methods roughly similar resources based projections running densely sampled song data would require roughly hours time opposed hours vhem-hm. would extremely cpu-intensive given computational limits cluster. vhem algorithm hand learn considerable amounts data still maintaining runtime memory requirements. finally also compare state-of-the-art models music tagging hemdtm based diﬀerent time-series model hem-gmm bag-of-features model using gmms. methods eﬃcient hierarchical estimation based algorithm obtain tag-level models. example consider learning tag-level songs corresponds audio fragments. using hierarchical estimation procedure ﬁrst model song individually song-level save song models then pool song models large reduce smaller tag-level using vhem-hm algorithm. auto-taggers operate audio features extracted half-overlapping windows hemgmm uses mfccs ﬁrst second instantaneous derivatives hem-dtm uses -bins mel-spectral features grouped audio fragments consecutive features. |wh| number test songs ground truth annotations |wa| number times annotation system uses automatically tagging song |wc| number times correctly used precision recall f-score positives top-k ranking. averages precision point ranking song correctly retrieved. reported metrics averages tags least examples result -fold cross-validation. table report performance various algorithms annotation retrieval dataset. looking overall runtime vhem-hm eﬃcient algorithm estimating distributions music data requires runtime em-hm runtime ppk-sc. vhem-hm algorithm capitalizes ﬁrst stage song-level estimation eﬃciently eﬀectively using song-level learn ﬁnal models. note runtime ppk-sc corresponds setting registered running time four times longer signiﬁcant improvement performance. strongly improving ranked lists evinced higher scores. relative em-hm vhem-hm beneﬁt regularization learning eﬃciently leverage music data condensed song hms. vhem-hm also outperforms ppk-sc approaches metrics. ppk-sc discards considerable information clusters’ structure selecting original hmms approximate cluster. signiﬁcantly aﬀects accuracy resulting annotation models. vhem-hm hand generates novel cluster centers summarize clusters. allows retain accurate information ﬁnal annotation models. ppk-sc-hybrid achieves considerable improvements relative standard ppk-sc relatively additional computational costs. demonstrates vhemhm algorithm eﬀectively summarize smaller model information contained several hmms. addition observe vhem-hm still outperforms ppk-sc-hybrid suggesting former produces accurate cluster centers density estimates. fact vhem-hm couples clustering learning cluster centers entirely based maximum likelihood estimating annotation models. ppk-sc-hybrid contrary separates clustering parameter estimation optimizes diﬀerent metrics consequence phases mismatched centers learned vhem best representatives clusters according aﬃnity. finally vhem-hm compares favorably auto-taggers based generative models. first vhem-hm outperforms hem-gmm model temporal information audio signal metrics. second performances vhem-hm hem-dtm statistically diﬀerent based paired t-test conﬁdence except annotation precision vhemhm scores signiﬁcantly higher. since hem-dtm based linear dynamic systems model stationary time-series linear subspace. contrast vhem-hm uses hmms discrete states emissions hence better adapt non-stationary time-series non-linear manifold. diﬀerence illustrated experiments vhem-hm outperforms hem-dtm human mocap data non-linear dynamics perform similarly music data audio features often stationary short time frames. experiment investigate performance vhem-hm algorithm estimating class-conditional distributions automatic classiﬁcation retrieval on-line hand-writing data. diﬀerences performance statistically signiﬁcant based paired t-test conﬁdence. ppk-sc-hybrid vhem-hm algorithm converges quickly since component learned beneﬁt clever initialization handwriting motion primitives example trajectory diﬀerent characters correspond single pen-down segment. data captured wacom tablet consists -coordinates force. data numerically diﬀerentiated gaussian smoothed half data used training half held testing. hand-writing examples training estimate series class-conditional distributions character using hierarchical estimation vhemhm algorithm. first character partition relevant training data groups sequences learn group using baum-welch algorithm. next estimate class-conditional distribution character aggregating relevant hmms summarizing components using vhem-hm algorithm using character-level bayes’ rule hand-writing example test compute posterior probabilities characters. example classiﬁed character largest posterior probability. retrieval given query character examples test ranked character’s posterior probability. repeated experiment using ppk-sc shem-hm estimate classiﬁcation models intermediate hmms. finally considered em-hm algorithm directly uses training sequences learn class-conditional since vhem-hm shem-hm em-hm iterative algorithms studied varying stopping criterion. particular algorithms terminated relative variation value objective function consecutive iterations finally measure classiﬁcation retrieval performance test using classiﬁcation accuracy average per-tag also report total training time includes time used learn intermediate hmms. experiments consisted trials diﬀerent random initializations algorithms. note choosing lower value plays role making clustering algorithm reliable. using fewer virtual samples equates attaching smaller virtual probability masses input hmms leads less certain assignments input hmms clusters determines mixing initial iterations algorithm reduces risk prematurely specializing cluster original hmms. eﬀect desirable since input hmms estimated smaller number sequences therefore noisier less reliable. similar experiment used number iterations stopping criterion registered vhem-hm performs better ppk-sc metrics. learning novel cluster centers vhem-hm estimates distributions representative relevant intermediate hmms hence relevant training sequences. em-hm best classiﬁcation vhem-hm performs better retrieval evinced scores. terms training time vhem-hm ppk-sc times faster em-hm. particular ppk-sc fastest algorithm since small number input hmms allows build spectral clustering embedding eﬃciently. version based actual sampling performs better vhem-hm classiﬁcation vhem-hm higher retrieval scores. however training time shem-hm approximately times longer vhem-hm. order reliably estimate reduced models shem-hm algorithm requires generating large number samples computing likelihood iteration. contrast vhem-hm algorithm eﬃciently approximates suﬃcient statistics virtually unlimited number samples without need using real samples. changes contrast vhem-hm shemhm consistently improve metrics algorithm converges results suggest regularization eﬀect hierarchical estimation finally elaborate results compare experiments music annotation retrieval first character trajectory data number training sequences associated class small compared dataset. result em-hm algorithm able process data achieve good classiﬁcation performance. however em-hm still needs evaluate likelihood original sequences iteration. leads slower iterations results total training time times longer vhem-hm second character trajectory data controlled data since class corresponds single character examples writer. consequence less variation intermediate hmms several summarize cluster well providing good candidate cluster centers ppk-sc. conclusion ppk-sc faces limited loss information selecting initial hmms represent cluster achieves reasonable performances. generation virtual samples vhem-hm controlled parameters number virtual sequences length section investigate impact parameters annotation retrieval performance cal. given nvntk constant number training songs number mixture components song-level starting parameter varied keeping ﬁxed annotation retrieval performance dataset calculated described section performances varying close metrics. example average vary small ranges similarly varying number virtual sequences limited impact performance well. demonstrates vhem-hm fairly robust choice parameters. note e-step vhem-hm algorithm averages possible observations compatible input models also choose value number virtual samples controls virtual mass input hmms thus certainty cluster assignments. finally tested vhem-hm music annotation retrieval using virtual sequences length audio fragments used song level i.e. compared registered increase total running time corresponding improvement performance. thus experimental setting making virtual sequences relatively short positively impacts running time without reducing quality learned models. paper presented variational algorithm clustering hmms respect probability distributions generating novel center represent cluster. experimental results demonstrate eﬃcacy algorithm various clustering annotation retrieval problems involving time-series data showing improvement current methods. particular using two-stage hierarchical estimation procedure learn many smaller subsets data summarize compact model data vhem-hm algorithm estimates annotation models data eﬃciently standard also improves model robustness better regularization. speciﬁcally averaging possible virtual samples prevents over-ﬁtting improve generalization learned models. moreover using relatively short virtual sequences positively impacts running time algorithm without negatively aﬀecting performance practical applications. addition noted vhem-hm algorithm robust choice number length virtual samples. experiments implemented ﬁrst stage hierarchical estimation procedure partitioning data non-overlapping subsets particular partitioning data song level practical advantage. since individual songs relevant several tags estimation song executed single time song database re-used vhem estimation associated models. positive impact computational eﬃciency. depending particular application however slightly diﬀerent implementation ﬁrst stage better suited. example estimating large amount training data could procedure necessarily cover data inspired kleiner size training data ﬁrst estimate intermediate many little bootstrap subsamples data size summarize intermediate ﬁnal using vhem-hm algorithm. future work plan extend vhem-hm case hmms share large universal background model emission distributions commonly used speech would allow faster training would require faster implementation inference addition plan derive algorithm hmms discrete emission distributions compare performance work presented extension large background model. e.c. thanks yingbo song providing code assistance experiments motion clustering. authors acknowledge support google inc. e.c. g.r.g.l. also wish acknowledge support qualcomm inc. yahoo inc. hellman fellowship program national science foundation a.b.c. supported research grants council hong kong special administrative region china g.r.g.l. acknowledges support alfred sloan foundation. research supported part ucsd fwgrid project research infrastructure grant number eia-. bahlmann burkhardt. measuring similarity bayes probability error application online handwriting recognition. document analysis recognition proceedings. sixth international conference pages ieee carneiro a.b. chan p.j. moreno vasconcelos. supervised learning semantic classes image annotation retrieval. ieee transactions pattern analysis machine intelligence", "year": 2012}