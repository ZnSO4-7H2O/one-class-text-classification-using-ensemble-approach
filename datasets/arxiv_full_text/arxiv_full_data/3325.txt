{"title": "Loss Functions for Top-k Error: Analysis and Insights", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "In order to push the performance on realistic computer vision tasks, the number of classes in modern benchmark datasets has significantly increased in recent years. This increase in the number of classes comes along with increased ambiguity between the class labels, raising the question if top-1 error is the right performance measure. In this paper, we provide an extensive comparison and evaluation of established multiclass methods comparing their top-k performance both from a practical as well as from a theoretical perspective. Moreover, we introduce novel top-k loss functions as modifications of the softmax and the multiclass SVM losses and provide efficient optimization schemes for them. In the experiments, we compare on various datasets all of the proposed and established methods for top-k error optimization. An interesting insight of this paper is that the softmax loss yields competitive top-k performance for all k simultaneously. For a specific top-k error, our new top-k losses lead typically to further improvements while being faster to train than the softmax.", "text": "hand compare schemes direct multiclass losses extensive experiments other present theoretical discussion regarding calibration top-k error. based insights suggest families loss functions top-k error. smoothed versions top-k hinge losses top-k versions softmax loss. discuss advantages disadvantages convex losses provide efﬁcient implementation based stochastic dual coordinate ascent evaluate battery loss functions datasets different tasks ranging text classiﬁcation large scale vision benchmarks including ﬁne-grained scene classiﬁcation. systematically optimize report results separately top-k accuracy. interesting message would like highlight softmax loss able optimize top-k error measures simultaneously. contrast multiclass also reﬂected experiments. finally show top-k variants smooth multiclass softmax loss improve top-k performance speciﬁc related work. top-k optimization recently received revived attention advent large scale problems top-k error multiclass classiﬁcation promotes good ranking class labels example closely related precisionk metric information retrieval counts fraction positive instances among top-k ranked examples. essence approaches enforce desirable ranking items classic approaches optimize pairwise ranking svmstruct ranknet larank alternative direction proposed usunier described general family convex loss functions ranking classiﬁcation. loss functions consider also falls family. weston introduced wsabie optimizes approximation ranking-based loss bayesian approach suggested order push performance realistic computer vision tasks number classes modern benchmark datasets signiﬁcantly increased recent years. increase number classes comes along increased ambiguity class labels raising question top- error right performance measure. paper provide extensive comparison evaluation established multiclass methods comparing top-k performance practical well theoretical perspective. moreover introduce novel top-k loss functions modiﬁcations softmax multiclass losses provide efﬁcient optimization schemes them. experiments compare various datasets proposed established methods top-k error optimization. interesting insight paper softmax loss yields competitive top-k performance simultaneously. speciﬁc top-k error topk losses lead typically improvements faster train softmax. number classes rapidly growing modern computer vision benchmarks typically also leads ambiguity labels classes start overlap. even humans error rates top- performance often quite high previous research focuses minimizing top- error address top-k error optimization paper. interested cases achieving small top-k error reasonably small minimization speciﬁc top-k error. argued one-versus-all scheme performs top- top- accuracy variations based ranking losses recently shown minimization top-k hinge loss leads improvements top-k performance compared multiclass ranking-based formulations. paper study topk error optimization wider perspective. contributions. study problem top-k error optimization diverse range learning tasks. consider existing methods well propose novel loss functions minimizing top-k error. brief overview methods given table proposed convex top-k losses develop efﬁcient optimization scheme based sdca also used training softmax loss. methods evaluated empirically terms top-k error whenever possible terms classiﬁcation calibration. discover softmax loss proposed smooth top- astonishingly competitive top-k errors. small improvements obtained top-k losses. consider multiclass problems classes consists examples training along corresponding labels denote permutation unless stated otherwise reorders components vector descending order i.e. consider linear classiﬁers experiments loss functions formulated general setting function learned prediction test time done maxy∈y resp. top-k predictions. linear case predictors form rd×m stacked weight matrix convex loss function regularization parameter. consider following multiclass optimization problem minw maximal corresponds taking largest conditional probabilities yields bayes optimal top-k error since relative order within {pτj irrelevant top-k error classiﬁer sets coincide bayes optimal. note assumed w.l.o.g. clear pτk+ likely classes rest. general ties resolved arbitrarily long guarantee largest components correspond classes yield maximal optimization zero-one loss leads hard combinatorial problems. instead standard approach convex surrogate loss upper bounds zero-one error. mild conditions loss function optimal classiﬁer w.r.t. surrogate yields bayes optimal solution zero-one loss. loss called classiﬁcation calibrated known statistical learning theory necessary condition classiﬁer universally bayes consistent introduce notion calibration top-k error. deﬁnition loss function called top-k calibrated possible data generating measures standard multiclass problem often solved using one-vs-all reduction binary classiﬁcation problems. every class trained versus rest yields classiﬁers {fy}y∈y. hinge logistic losses correspond logistic regression respectively. show schemes top-k calibrated simultaneously. lemma reduction top-k calibrated bayes optimal function convex margin-based loss strictly monotonically increasing function show hinge top-k calibrated construct example problem classes note every class bayes optimal binary classiﬁer hence predicted ranking labels arbitrary produce bayes optimal top-k error. multiclass hinge loss crammer singer softmax loss popular losses multiclass problems. latter also known crossentropy multiclass logistic loss often used last layer deep architectures multiclass hinge loss shown competitive large-scale image classiﬁcation however known calibrated top- error. next show top-k calibrated implicit reason top-k calibration schemes softmax loss estimate probabilities bayes optimal classiﬁer. loss functions allow called proper. refer references therein detailed discussion. established logistic regression softmax loss top-k calibrated interested deﬁning loss functions top-k error? reason calibration asymptotic property bayes optimal functions obtained pointwise. picture changes linear classiﬁers since obviously cannot minimized independently point. indeed bayes optimal classiﬁers cannot realized linear functions. particular convexity softmax multiclass hinge losses leads phenomena errk) happens adds bias working rigid function classes linear ones. loss functions introduce following modiﬁcations losses goal alleviating phenomenon. recently introduced top-k versions multiclass hinge loss second version based family ranking losses introduced earlier notation direct comparison refer ﬁrst version second ones vector y-th basis vector deﬁned componentwise top-k hinge losses j-th largest component shown tighter upper bound top-k error however losses performed similarly experiments. following simply refer top-k hinge top-k loss. follows multiclass hinge loss classiﬁcation calibrated maxy∈y bayes optimal classiﬁer reduces constant. moreover even loss top-k calibrated predicted order remaining classes need optimal. losses reduce multiclass hinge loss therefore unlikely top-k calibrated even though currently neither prove disprove multiclass hinge loss calibrated non-smooth allow estimate class conditional probabilities family smooth top-k hinge losses based moreau-yosida regularization technique used smooth binary hinge loss interestingly smooth binary hinge loss fulﬁlls conditions lemma leads top-k calibrated scheme. hope smooth top-k hinge loss becomes top-k calibrated well. smoothing works adding quadratic term conjugate function becomes strongly convex. smoothness loss among things typically leads much faster optimization discuss section analytic expression evaluation requires computing projection onto top-k simplex done time shown non-analytic nature smooth top-k hinge losses currently prevents proving top-k calibration. top-k entropy loss shown synthetic data top- top- error optimization limited linear classiﬁers lead completely different solutions. softmax loss primarily aiming top- performance produces solution reasonably good top- error achieved top- error. reasoning motivated adapt softmax loss top-k error optimization. inspired conjugate top-k hinge loss introduce section top-k entropy loss. recall conjugate functions multiclass top-k differ effective domain conjugate function same. instead standard simplex conjugate top-k hinge loss deﬁned subset top-k simplex. suggests construct novel losses speciﬁc properties taking conjugate existing loss function modifying essential domain enforces desired properties. motivation comes interpretation dual variables forces every training example pushes decision surface direction given ground truth label. absolute value dual variables determines magnitude forces optimal values often attained boundary feasible therefore reducing feasible limit maximal contribution given training example. proof. provide derivation convex conjugate softmax loss already given without proof. also highlight constraint easily missed computing conjugate re-stated explicitly lemma softmax loss example smoothing applied top-k hinge loss yields following smooth top-k hinge loss smoothing done similarly proposition smoothing parameter. smooth top-k hinge loss conjugate obtain γ-strongly conadd regularizer conjugate loss stated proposition. mentioned primal smooth top-k hinge loss obtained convex conjugate /γ-smooth. obtain formula compute based euclidean projection onto top-k simplex. deﬁnition conjugate top-k entropy loss obtained version could obtained replacing using instead defer future work. closed-form solution primal top-k entropy loss evaluate follows. proposition top-k entropy loss deﬁned note cannot satisfy conditions choice dual variables therefore implies constraint might active note however view active either dimensional problem. consider case constraint active below. major limitation softmax loss top-k error optimization cannot ignore highest scoring predictions yields high loss even top-k error zero. seen rewriting problem also present top-k hinge losses considered inherent limitation convexity. origin problem fact ranking based losses based functions αjfπj function convex sequence monotonically non-increasing implies convex ranking based losses weight highest scoring classiﬁers would like less weight them. drop ﬁrst highest scoring predictions sacriﬁcing convexity loss deﬁne truncated top-k entropy loss follows indexes corresponding smallest components )j=y. loss seen smooth version top-k error small whenever top-k error zero. below show loss top-k calibrated. note algorithm terminates iterations since overall complexity therefore compute actual loss note empty i.e. violated constraints top-k entropy loss coincides softmax loss directly given otherwise section brieﬂy discuss proposed smooth top-k hinge losses top-k entropy loss optimized efﬁciently within sdca framework primal dual problems. rd×n matrix training examples corresponding gram matrix rd×m matrix primal variables rm×n matrix dual variables regularization parameter. primal fenchel dual objective functions given convex conjugate sdca proceeds randomly picking variable modifying achieve maximal increase dual objective turns update step equivalent proximal problem seen regularized projection onto essential domain convex conjugate. important ingredient sdca framework convex conjugate show multiclass loss functions consider fact depend differences enforces certain constraint conjugate function. lemma unless proof. proof follows directly already reproduced proof proposition softmax loss. formulated simpliﬁed lemma since additionally required y-compatibility show hold e.g. softmax loss. obtained removing y-th coordinate vector show performing update step smooth top-k hinge loss equivalent projecting certain vector computed prediction scores onto essential domain top-k simplex added regularization biases solution orthogonal proposition respectively loss conjugate proposition top-k svmα loss nonconvex solutions obtained softmax loss initial points optimize gradient descent. however resulting optimization problem seems mildly nonconvex same-quality solutions obtained different initializations. section show synthetic experiment advantage discarding highest scoring classiﬁer loss becomes apparent. loss derived similarly using resulting projection problem biased continuous quadratic knapsack problem discussed supplement smooth top-k hinge losses converge signiﬁcantly faster nonsmooth variants show scaling experiments below. explained theoretical results convergence rate sdca. also similar observations smoothed binary hinge loss. update step top-k ent. discuss optimization proposed top-k entropy loss sdca framework. note top-k entropy loss reduces softmax loss thus sdca approach used gradient-free optimization softmax loss without tune step sizes learning rates. note optimization problem similar difﬁcult solve presence logarithms objective. propose tackle problem using lambert function introduced below. lambert function. lambert function deﬁned inverse function widely used many ﬁelds taking logarithms sides deﬁning equation obtain therefore given equation form directly solve closed-form crux problem function transcendental like logarithm exponent. exist highly optimized implementations latter argue done lambert develop intuition concerning lambert function exponent brieﬂy discuss function behaves different values illustration provided figure directly equation behavior changes dramatically depending whether large positive large negative number. ﬁrst case linear part dominates logarithm function approximately linear; better approximation second case function behaves like exponent this write ete−x note therefore approximations initial points order householder method also used single iteration already sufﬁcient full float precision iterations needed double. solve present similar derivation already done problem above. main difference encounter lambert function optimality conditions. re-write problem figure synthetic data unit circle visualization top- top- predictions smooth top- optimizes top- error impedes top- error. trunc. top- entropy loss ignores top- scores optimizes directly top- errors leading much better top- result. section demonstrate synthetic experiment proposed top- losses outperform top- losses aims optimal top- performance. dataset three classes shown inner circle figure sampling. first generate samples subdivided segments. segments unit length except segment length sample uniformly random segments according following class-conditional probabilities class class class finally data rescaled mapped onto unit circle. samples different classes plotted next better visibility signiﬁcant class overlap. visualize top-/ predictions colored circles sample points parameter training/validation/testing tune range results table runtime. compare wall-clock runtime top- multiclass smooth multiclass softmax loss objectives figure plot relative duality d)/p validation accuracy versus time best performing models ilsvrc obtain substantial improvement convergence rate smooth top- compared non-smooth baseline. moreover top- accuracy saturates passes training data justiﬁes fairly loose stopping criterion lrmulti cost epoch signiﬁcantly higher compared top- svms difﬁculty solving suggests smooth top- svmα obtain competitive performance lower training cost. also compare implementation lrmulti spams optimization toolbox denoted lrmulti provides efﬁcient implementation fista note rate convergence sdca competitive fista noticeably better conclude approach competitive state-of-the-art faster computation would lead speedup. gradient-based optimization. finally note proposed smooth top-k hinge truncated top-k entropy losses easily amenable gradient-based optimization particular training deep architectures computation gradient straightforward smooth top-k hinge loss table top-k accuracy various datasets. ﬁrst line reference state-of-the-art dataset reports top- accuracy except numbers aligned top-k. compare one-vs-all multiclass baselines top-k svmα well proposed smooth top-k svmα column provide results model optimizes corresponding top-k accuracy general different top- top-. first note top- baselines perform similar top- performance except svmmulti top- show better results. next top- losses improve accuracy improvement signiﬁcant nonconvex top- enttr loss close optimal solution dataset. top- enttr tight bound top- error ignores top- errors loss. unfortunately similar signiﬁcant improvements observed real-world data sets tried. goal section provide extensive empirical evaluation top-k performance different losses multiclass classiﬁcation. evaluate loss functions introduced datasets various problem domains detailed statistics datasets given table please refer table overview methods naming convention. broad selection results also reported paper. ranking based losses perform well comparison here. solvers. liblinear one-vs-all baselines svmova lrova; code top-k svm. extended latter support smooth top-k svmγ top-k ent. multiclass loss baselines svmmulti lrmulti correspond respectively top- top- ent. nonconvex top-k enttr lrmulti solution initial point perform gradient descent line search. cross-validate hyperparameters range extending optimal value boundary. features. aloi letter news datasets features provided libsvm datasets. aloi randomly split data equally sized training test sets preserving class distributions. letter dataset comes separate validation used flowers ilsvrc matconvnet extract outputs last fully connected layer imagenet-vgg-verydeep- model pre-trained imagenet achieves state-of-the-art results image classiﬁcation indoor places places-vggnet- model pretrained places outperforms imagenet pre-trained model scene classiﬁcation tasks further results found paper. cases obtain similar behavior terms ranking considered losses discussed below. discussion. experimental results given table several interesting observations make. schemes perform quite similar multiclass approaches conﬁrms earlier observations schemes performed worse aloi letter. therefore seems safe recommend multiclass losses instead schemes. comparing softmax multiclass losses clear winner top- performance softmax consistently outperforms multiclass top-k performance might strong property softmax top-k calibrated please note trend uniform across datasets particular also ones features coming convnet. smooth top-k hinge top-k entropy losses perform slightly better softmax compares corresponding top-k errors. however good performance truncated top-k loss synthetic data transfer real world datasets. might relatively high dimension feature spaces requires investigation. fine-tuning experiments. also performed number ﬁne-tuning experiments original network trained epochs smooth top-k hinge truncated top-k entropy losses. motivation full end-to-end training would beneﬁcial compared training classiﬁer. results reported table first note setting slightly different feature extraction step matconvnet non-regularized bias term caffe next top-k speciﬁc losses able improve performance compared reference model top- loss achieves best table top-k accuracy reported caffe large scale datasets ﬁne-tuning approximately epoch places epochs ilsvrc. ﬁrst line reference performance ﬁne-tuning. top-.. performance places however experiments also observed similar improvements ﬁne-tuning standard softmax loss achieves best performance ilsvrc conclude safe choice multiclass problems seems softmax loss yields competitive results top-k errors. interesting alternative smooth top-k hinge loss faster train achieves competitive performance. wants optimize directly top-k error improvements possible using either smooth top-k top-k entropy losses. done extensive experimental study top-k performance optimization. observed softmax loss smooth top- hinge loss competitive across top-k errors considered primary candidates practice. top-k loss functions improve results slightly especially targeting particular top-k error performance measure. finally would like highlight optimization scheme based sdca top-k entropy loss also includes softmax loss independent interest.", "year": 2015}