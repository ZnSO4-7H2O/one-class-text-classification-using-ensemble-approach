{"title": "Identifying Finite Mixtures of Nonparametric Product Distributions and  Causal Inference of Confounders", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We propose a kernel method to identify finite mixtures of nonparametric product distributions. It is based on a Hilbert space embedding of the joint distribution. The rank of the constructed tensor is equal to the number of mixture components. We present an algorithm to recover the components by partitioning the data points into clusters such that the variables are jointly conditionally independent given the cluster. This method can be used to identify finite confounders.", "text": "component mixing weights permutations z-values. paper focus determining model identiﬁable. used infer existence hidden common cause observed variables reconstruct confounder. remainder paper organized follows section method proposed determine i.e. number mixture components section discusses established results identiﬁability parameters section presents algorithm determining parameters section uses ﬁndings previous sections confounder identiﬁcation. finally experiments provided section various methods proposed literature select number mixture components mixture model b¨ohning seidel rasmussen iwata however impose diﬀerent kind assumptions conditional independence assumption model assuming model kasahara shimotsu proposed nonparametric method requires discretization observed variables provides lower bound following present method determine without making parametric assumptions component distributions. propose kernel method identify ﬁnite mixtures nonparametric product distributions. based hilbert space embedding joint distribution. rank constructed tensor equal number mixture components. present algorithm recover components partitioning data points clusters variables jointly conditionally independent given cluster. method used identify ﬁnite confounders. latent variable models widely used model heterogeneity populations. following assume observed variables jointly conditionally independent given latent class. example given medical syndrome diﬀerent symptoms might conditionally independent. convariable domain rkhs product satisfying reproducing property consequently kernel thus deﬁnes satisfying i.e. corresponds product denote probability distributions following mean deﬁne hilbert space embedding given data points identiﬁable distribution goal cluster data points distribution points label close empirical distribution every mixture comprobabilistic mixture models clustering methods used identify mixture components b¨ohning seidel rasmussen iwata however impose diﬀerent kind assumptions conditional independence assumption model assuming model levine proposed expectationmaximization algorithm nonparametric estimation parameters given known. algorithm uses kernel smoothing operator. choose common kernel bandwidth components otherwise iterative algorithm guaranteed increase iteration another. stated also chauveau fact adaptive bandwidth problematic especially distributions components diﬀer signiﬁcantly. proposed method clic assigns observations mixture components claim single data point assigned correctly instead yield variables jointly conditionally independent given cluster order recover components. measure conditional independence given cluster hilbert schmidt independence criterion measures hilbert space distance kernel embeddings joint distribution random variables product marginal distributions. test mutual independence. that perform multiple tests namely etc. bonferroni correction. cluster consider test statistic hsic test leads smallest p-value matrix easily rank approximations truncated singular value decomposition dropping svs. ﬁnding low-rank approximation tensor ill-posed problem grouping variables sets without loss generality formally obtain case vector-valued variables. pure called pure values probability distribution allman eters whenever weak conditions using theorem kruskal finally kasahara shimotsu provided complementary identiﬁability results diﬀerent condiregard negative logarithms pvalues null hypothesis independence objective function. proposed algorithm iterative. ﬁrst randomly assign every data point mixture component. every iteration perform greedy search randomly divide data disjoint sets points. then select sets consider possible assignments set’s points clusters assignment optimizes objective function accepted points assigned clusters eventually repeat procedure disjoint sets constitutes iteration algorithm. every iteration test conditional independence given cluster. algorithm stops iteration following happens observe independence clusters data point changed cluster assignment upper limit iterations reached. clear objective function monotonously decreasing. algorithm succeed producing conditionally independent variables diﬀerent reasons e.g. incorrect estimation previous step convergence local optimum. case clic reports unable appropriate clusters. along iterations kernel test independence updates bandwidth according data points belonging current cluster note however case algorithm section there obliged common bandwidth information mixture components. parameter allows trade-oﬀ speed avoiding local optima clic would global optimum step would require checking cluster assignments. hand leads faster algorithm stuck local optima. experiments used since encounter many problems local optima. considering constant computational complexity clic every iteration. algorithm includes pseudocode clic. drawing causal conclusions observed statistical dependences without able intervene system always relies assumptions link statistics causality least disputed causal markov condition stating every variable conditionally independent non-descendant given parents respect directed acyclic graph formalizes causal relations. focus causal inference problems observed variables causally suﬃcient i.e. statistical dependences also hidden common causes observed variables. assumption linear relationships variables non-gaussian distributions confounders identiﬁed using independent component analysis results linear case presented silva non-linear case additive noise janzing fast causal inference exclude confounding pairs variables given many variables observed. finally reconstruction binary confounders assumption pure conditionals presented janzing section results previous sections infer existence identify ﬁnite confounder explains dependences observed variables. assume latent variables caused observed variables unlike previous methods make explicit assumptions distribution variables. instead postulate diﬀerent assumption namely conditional variable given parents full rank causal inference interested inferring unknown underlying causal dag. following theorem uses theorem infer causal structure based rank embedding joint distribution. identiﬁed according section note single common cause could result merging many common causes vector-valued variable thus ﬁrst glance seems lose generality assuming common cause. however assumption then excludes case consists components strength causal arrows sample size. then faced issue that based theorem would always infer fig. depicts possible causal number values equal estimated rank. however lower rank conﬁdent indeed existence confounder renders observed variables conditionally independent hand high rank also direct causal links observed variables continuous confounders. that consider theorem appropriate inferring existence confounder small number values would lead rank. however admit considered high well deﬁned. example much high rank values ﬁrst step determining number mixture components common select bandwidth every median distance between data points dimension empirical data. however approach would usually result overestimation bandwidth especially case many mixture components partially account this compute bandwidth every median distance points neighborhood every point sample. neighborhood found nearest neighbors point computed using variables apart estimate rank report estimated rank normal distribution standard deviation t-distribution degrees freedom beta distribution alpha beta mixture normal distributions variance each. distance components dimension distributed according gaussian mean standard deviation chose distance mixtures experiments cover different levels overlap components comparison additionally generated data observed variables connected also direct causal links thus conditionally dependent given confounder that ﬁrst generated data according fig. above shifted simulate diidentifying number mixture compotion i.e. nents empirical rank estimation depend strength causal arrows kernel bandwidth selection sample size estimate rank keeping large eigenvalues. figures illustrate histograms estimated number components ﬁrst simulated data respectively. ﬁgure contains histogram every value observe increases method becomes sensitive underestimating number components behavior explained common sigma selection data dimension high overlap distributions hand increases method becomes robust estimating correctly grouping variables allows multiple rank estimations. estimated rank values provide evidence causal fig. true course stated also sec. diﬃcult deﬁne considered rank. next performed experiments using ﬁrst simulated data evaluate performance proposed method method levine algorithm using gaussian mixture model following refer methods clic levine respectively. data point latter methods output posterior probabilities clusters sample obtain cluster assignments. figure illustrates cluster assignments three methods simulated dataset always identiﬁable). note permutations colors ambiguity labels identiﬁcation problem. however incorrectly identiﬁes single component distinct components. clear assumes data generated gaussian mixture model model opposed clic levine methods. ture component squared maximum mean discrepancy distance hilbert space embeddings distributions. test statistics described gretton since designed compare independent samples whereas samples overlapping observations. account permutations measure cluster permutations select minimum clusters. figures report squared results three methods diﬀerent combinations point corresponds squared cluster experiments. results provided cases number components correctly identiﬁed previous step. clic method unable appropriate clusters experiments without claiming comparison exhaustive infer clic levine methods perform signiﬁcantly better since impose conditional independence. higher improves since clusters less overlapping. however computational time clic higher compared methods. further applied framework breast cancer wisconsin data machine learning repository dataset consists features breast masses along classiﬁcation benign malignant sample size dataset selected feaally independent condition class applied method three features succeeded correctly inferring number mixture components figure depicts ground truth breast data input results clic levine fig. corresponding squared mmds. observe levine method performs poorly dataset. condition fourth feature person applied method three features succeeded correctly inferring number mixture components figure depicts ground truth input results clic levine fig. corresponding squared mmds. observe levine methods perform poorly dataset. finally applied method database cause-eﬀect pairs. includes pairs variables known causal structure. since exists direct causal arrow expect rank inﬁnite given assumptions even exist hidden confounders not. however estimated rank ﬁnite data always ﬁnite magnitude strongly depending strength causal arrow sample size mentioned sec. figure depicts cause-eﬀect pairs sample size various depaper introduced kernel method identify ﬁnite mixtures nonparametric product distributions. method used infer existence identify hidden common cause observed variables. experiments simulated real data performed evaluation proposed approach. practice method appropriate identiﬁcation confounder small number values.", "year": 2013}