{"title": "Ranking to Learn: Feature Ranking and Selection via Eigenvector  Centrality", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "In an era where accumulating data is easy and storing it inexpensive, feature selection plays a central role in helping to reduce the high-dimensionality of huge amounts of otherwise meaningless data. In this paper, we propose a graph-based method for feature selection that ranks features by identifying the most important ones into arbitrary set of cues. Mapping the problem on an affinity graph-where features are the nodes-the solution is given by assessing the importance of nodes through some indicators of centrality, in particular, the Eigen-vector Centrality (EC). The gist of EC is to estimate the importance of a feature as a function of the importance of its neighbors. Ranking central nodes individuates candidate features, which turn out to be effective from a classification point of view, as proved by a thoroughly experimental section. Our approach has been tested on 7 diverse datasets from recent literature (e.g., biological data and object recognition, among others), and compared against filter, embedded and wrappers methods. The results are remarkable in terms of accuracy, stability and low execution time.", "text": "abstract. accumulating data easy storing inexpensive feature selection plays central role helping reduce high-dimensionality huge amounts otherwise meaningless data. paper propose graph-based method feature selection ranks features identifying important ones arbitrary cues. mapping problem afﬁnity graph features nodes solution given assessing importance nodes indicators centrality particular eigenvector centrality gist estimate importance feature function importance neighbors. ranking central nodes individuates candidate features turn effective classiﬁcation point view proved thoroughly experimental section. approach tested diverse datasets recent literature compared ﬁlter embedded wrappers methods. results remarkable terms accuracy stability execution time. introduction data collection technologies advance computer power grows torrent data generated almost every ﬁeld computers used volume velocity variety complexity datasets continuously increasing pattern recognition methodologies become indispensable order extract useful information huge amounts otherwise meaningless data. feature selection long existing methods deals problems objective select minimal subset attributes allows problem clearly deﬁned. choosing minimal subset features irrelevant redundant features removed according reasonable criteria original task achieved equally well better. techniques partitioned three classes wrappers classiﬁers score given subset features; embedded methods inject selection process learning classiﬁer; ﬁlter methods analyze intrinsic properties data ignoring classiﬁer. filters also robust overﬁtting. potential interactions among elements joint latter ﬁnal subset features selected provided. cases operations performed sequentially cases selection carried usually subset selection supervised ranking case methods supervised not. np-hard features total goal select optimal subset evaluating overview). ﬁlters features ﬁrst considered individually ranked subset extracted examples mutual information relief-f inf-fs unsupervised mrmr conversely wrapper embedded methods subsets features sampled evaluated ﬁnally kept ﬁnal output instance svm-rfe work propose novel graph-based feature selection algorithm ranks features according graph centrality measure main idea behind method problem afﬁnity graph model pairwise relationships among feature distributions weighting edges connecting them. novelty proposed method terms state assigns score importance feature taking account features mapped nodes graph bypassing combinatorial problem methodologically sound fashion. indeed eigenvector centrality differs measurements since node feature receiving many links necessarily high eigenvector centrality. reason nodes equivalent relevant others reasonably endorsements important nodes count noteworthy another important contribution work scalability method. indeed centrality measurements implemented using reduce paradigm makes algorithm prone possible distributed version approach extensively tested benchmarks cancer classiﬁcation prediction genetic data prostate leukemia lymphoma handwritten recognition generic feature selection datasets object recognition compare proposed method data comparing seven efﬁcient approaches different conditions overcoming terms ranking stability classiﬁcation accuracy. finally provide open portable library feature selection algorithms integrating methods uniform input output formats facilitate large scale performance evaluation. feature selection library interfaces fully documented. library integrates directly matlab popular language machine learning pattern recognition research. rest paper organized follows. brief overview related literature given section mostly focusing comparative approaches consider work. feature selection algorithm described section graph construcfig. wrapper models involve optimizing predictor part selection process. tend give better results ﬁlter methods usually computationally less expensive wrappers. tion weighting presented section section respectively employed eigenvector centrality discussed section section contains experimental evaluations results. finally conclusions provided section since mid-s domains explored used features. situation changed considerably past years papers explore domains hundreds tens thousands features. approaches proposed address challenging tasks involving many irrelevant redundant cues often comparably training examples. among used strategies relief-f iterative randomized supervised approach estimates quality features according well values differentiate data samples near other; discriminate among redundant features performance decreases data. similar problems affect svm-rfe wrapper method selects features sequential backward elimination manner ranking high feature strongly separates samples means linear svm. batti developed mutual information-based feature selection criterion features selected greedy manner. given existing selected features step locates feature maximizes relevance class. selection regulated proportional term measures overlap information candidate feature existing features. authors proposed graph-based ﬁlter approach feature selection constructs graph node corresponds feature edge weight corresponding mutual information features connected edge. method performs dominant clustering select highly coherent features selects features based multidimensional interaction information another effective fast ﬁlter method fisher method computes score feature ratio interclass separation intraclass variance features evaluated independently ﬁnal feature selection occurs aggregating ranked ones. widely used ﬁlters based mutual information dubbed considers selection criterion mutual information distribution values given feature membership particular class. fig. filter methods selection features independent classiﬁer used. rely general characteristics training data select features independence predictor. mutual information provides principled measuring mutual dependence variables used number researchers develop information theoretic feature selection criteria. even last case features evaluated independently ﬁnal feature selection occurs aggregating ranked ones. maximum-relevance minimum-redundancy criterion efﬁcient incremental search algorithm. relevance scores assigned maximizing joint mutual information class variables subset selected features. computation information high-dimensional vectors impractical time required becomes prohibitive. face problem mrmr propose estimate mutual information continuous variables using parzen gaussian windows. estimate based heuristic framework minimize redundancy uses series intuitive measures relevance redundancy select features. note equivalent number features. selecting features unsumifs pervised learning scenarios much harder problem absence class labels would guide search relevant information. scenario compare approach recent unsupervised graph-based ﬁlter dubbed inf-fs inf-fs formulation feature node graph path selection features higher centrality score important feature. assigns score importance feature taking account possible feature subsets paths graph. another unsupervised method laplacian score importance feature evaluated power locality preserving. order model local geometric structure method constructs nearest neighbor graph. algorithm seeks features respect graph structure. finally embedded method include feature selection concave minimization selection process injected training linear programming technique. building graph given features build undirected graph vertices corresponding variable codiﬁes edges among features. adjacency matrix associated deﬁne nature weighted edges element represents pairwise potential term. potentials represented binary function nodes design function crucial operation. work weight graph according good reasonable criteria related class separation address classiﬁcation problem. words want rank features according well discriminate classes. hence draw upon best-practice propose ensemble different measures capturing relevance redundancy proposing kernelized-based adjacency matrix. continuing discussion note feature distribution normalized mean standard deviation respectively assumed i-th feature considering samples c-th class. higher discriminative i-th feature. however natural generalization score multi-class framework given given class labels natural want keep features related lead classes. therefore mutual information obtain good feature ranking score high features highly predictive class. boost performance introduce second feature-evaluation metric based standard deviation capturing amount variation dispersion features average follows loading coefﬁcient generic entry accounts much discriminative feature jointly considered; time considered weight edge connecting nodes graph i-th node models i-th feature distribution graph theory perspective identifying important nodes corresponds individuate indicators centrality within graph ﬁrst used graph theory study accessibility nodes example. idea compute suitably large sums entries measure accessibility vector entries equal accessibility index node would thus entries i-th total number paths length node nodes graph. problem method integer seems arbitrary. however count longer longer paths measure accessibility converges index known eigenvector centrality measure basic idea behind calculate eigenvector associated largest eigenvalue. values representative strongly node connected nodes. since limit approaches large positive number converges limit increase ratio components converges therefore marginalizing columns sufﬁciently large corresponds calculate principal eigenvector matrix figure illustrates example three random planar graphs. graphs made nodes fig. eigenvector centrality plots three random planar graphs. left simple gaussian distribution central nodes peripheral part distribution expected. central right plots complicated distributions node receiving many links necessarily high eigenvector centrality. best viewed color. weighted euclidean distance pair points. example high scoring nodes ones farther mean peculiarity eigenvector centrality node important linked important nodes work eigenvector centrality allows individuate candidate features turn effective classiﬁcation point view since indicators centrality characterize global prominence feature graph. summarizing gist eigenvector centrality compute centrality node function centralities neighbors. experiments results datasets comparative approaches datasets chosen letting proposed method deal diverse scenarios shown table details consider problems dealing training samples many features unbalanced classes classes severely overlap whose samples noisy complex scenes object classiﬁed located many outliers lastly consider shift problem samples used test congruent training data. table lists methods comparison whose details found sec. note type ﬁlters wrappers embedded methods class supervised unsupervised additionally report computational complexity computational complexity approach term computation mean values among samples every feature concerns construction matrix computation leading eigenvector costs number much smaller selected within algotable list approaches considered experiments speciﬁed according type class complexity complexity number samples number initial features multiplicative constant number iterations case iterative algorithms number classes. indicates computational complexity speciﬁed reference paper. section proposes tests pascal voc- dataset. object recognition voc- suitable tool testing models therefore reference benchmark assess strengths weaknesses using approach regarding classiﬁcation task. reason compare approach state-of-theart methods reported table experiment considers features cues extracted deep convolutional neural network architecture selected pre-trained model called deep convnets performs favorably state classiﬁcation detection imagenet large-scale visual recognition challenge -dimension activations last layer image descriptors voc- edition contains images split train validation test sets labeled twenty object classes. one-vs-rest classiﬁer class learnt evaluated independently performance measured mean average precision across classes. fisher table varying cardinality selected features. image classiﬁcation results achieved terms average precision scores selecting ﬁrst features total table serves analyze empirically clarify well important features ranked high several algorithms. amount features used experiments total. results signiﬁcant method achieved best performance terms mean average precision followed unsupervised ﬁlter methods inf-fs. methods comparison observe high variability classiﬁcation accuracy; indeed results show method robust classes application ﬁelds like biology inconceivable devise analysis procedure comprise step. clear example found analysis expression microarray data expression level thousands genes simultaneously measured. within scenario tested proposed approach four well-known microarray benchmark datasets two-class problems. results reported table testing protocol adopted experiment consists splitting dataset training testing. order fair evaluation feature ranking calculated using training samples applied testing samples. classiﬁcation performed using linear svm. setting best parameters used -fold cross validation training data. procedure repeated several times results averaged trials. results reported terms receiver operating characteristic curves. widely used measurement summarizes curve area curve useful comparing algorithms independently application. hence classiﬁcation results datasets used show proposed approach produces superior results cases. overall performance indicates approach robust others changing data still produces high quality rankings. quality feature subset measured estimate classiﬁcation accuracy chosen classiﬁer trained candidate subset. stability ranking important aspect task knowledge discovery. rationale behind fact estimate quality candidate subsets usually depends many training/testing split data. therefore different sequences features returned repeated runs approaches. case important deﬁne numerous different subsets features approximately equal quality otherwise presenting user subset misleading. assessed fig. kuncheva stability indices method comparison presented. ﬁgure reports stability varying cardinality selected features different benchmarks. stability selected features using kuncheva index stability measure represents similarity rankings generated different splits dataset. similarity sequences size seen number elements common kuncheva index takes values higher value larger number commonly selected features sequences. index shown figure comparing approach methods. valid alternative stability index based jensen-shannon divergence proposed range indicates completely random rankings means stable rankings. unlike kuncheva measure metric suitable different algorithm outcomes partial sublists well least studied partial ranked lists. since case work full ranked lists feature selection algorithms considered study produce permutations original features preferred widely used kuncheva index. proposed method shows cases high stability whereas highest performance achieved. gina sparse input variables consisting features. balanced data instances belonging positive class. results obtained gina indicate proposed approach overcomes methods comparison select useful features data high-complexity dimensionality. madelon artiﬁcial dataset part nips feature selection challenge. represents two-class classiﬁcation problem continuous input variables. method fisher-s inf-fs relieff ec-fs table varying cardinality selected features. different datasets classiﬁcation. performance obtained ﬁrst features. difﬁculty problem multivariate highly non-linear. results reported table gives proof classiﬁcation performance approach attained test sets gina madelon. techniques deﬁnitely represent important class preprocessing tools eliminating uninformative features strongly reducing dimension problem space allows achieve high performance useful practical purposes domains high speed required. reliability validity order assess difference performance statistically signiﬁcant t-tests used comparing accuracies. statistical tests used determine accuracies obtained proposed approach signiﬁcantly different others test assessing whether data come normal distributions unknown equal variances lilliefors test. results obtained comparing results produced method trials given distributions proposed method current competitor size two-sample t-test applied obtaining test decision null hypothesis data vectors comes independent random samples normal distributions equal means equal unknown variances. results show statistical signiﬁcant effect performance conclusion paper present idea solving feature selection eigenvector centrality measure. design graph features nodes weighted kernelized adjacency matrix draws upon best-practice feature selection assigning scores according well features discriminate classes. method estimates indicators centrality identifying important features within graph. results remarkable proposed method extensively tested different datasets selected different scenarios cases achieve performances competitors selected recent literature feature selection. approach also robust stable different splits training data performs effectively ranking high relevant features competitive complexity. study also points many future directions; focusing investigation different implementations parallel computing data analysis focusing investigation different relations among features. finally provide open portable library feature selection algorithms integrating methods uniform input output formats facilitate large scale performance evaluation. feature selection library interfaces fully documented. library integrates directly matlab popular language machine learning pattern recognition research.", "year": 2017}