{"title": "Neural Networks Should Be Wide Enough to Learn Disconnected Decision  Regions", "tag": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "abstract": "In the recent literature the important role of depth in deep learning has been emphasized. In this paper we argue that sufficient width of a feedforward network is equally important by answering the simple question under which conditions the decision regions of a neural network are connected. It turns out that for a class of activation functions including leaky ReLU, neural networks having a pyramidal structure, that is no layer has more hidden units than the input dimension, produce necessarily connected decision regions. This implies that a sufficiently wide layer is necessary to produce disconnected decision regions. We discuss the implications of this result for the construction of neural networks, in particular the relation to the problem of adversarial manipulation of classifiers.", "text": "recent literature important role depth deep learning emphasized. paper argue sufﬁcient width feedforward network equally important answering simple question conditions decision regions neural network connected. turns class activation functions including leaky relu neural networks pyramidal structure layer hidden units input dimension produce necessarily connected decision regions. implies sufﬁciently wide layer necessary produce disconnected decision regions. discuss implications result construction neural networks particular relation problem adversarial manipulation classiﬁers. deep learning becomes state many application domains computer vision natural language processing speech recognition theoretical understanding success steadily growing still plenty questions little understanding. particular question construct network e.g. choice activation function number layers number hidden units layer etc. little guidance limited understanding implications choice e.g. design hidden units extremely active area research many deﬁnitive guiding theoretical principles. quote recent book deep learning nevertheless recently progress understanding choices. connected feedforward networks universal approximators sense even single hidden layer network standard non-polynomial activation function like sigmoid approximate arbitrarily well every continuous function compact domain order explain success deep learning much recent effort spent analyzing representation power neural networks perspective depth basically show exist functions computed efﬁciently deep networks linear polynomial size require exponential size shallow networks. highlight power depth show number linear regions relu network form input space grows exponentially depth. another measure expressivity so-called trajectory length proposed show complexity functions computed network along one-dimensional curve input space also grows exponentially depth. previous work show existence depth efﬁciency cannot show often holds functions interest taken ﬁrst step address problem. particular studying special type networks called convolutional arithmetic circuits also known sum-product networks authors show besides measure zero functions realized deep network polynomial size require exponential size order realized even approximated shallow network. later show property however longer holds convolutional rectiﬁer networks represent empirically successful deep learning architecture practice. cently shown neural networks relu activation function wide enough order universal approximation property depth increases. particular authors show class continuous functions compact cannot arbitrarily well approximated arbitrarily deep network maximum width network larger input dimension moreover shown recently loss surface fully connected networks convolutional neural networks well behaved sense almost local minima global minima exists layer hidden units number training points. paper study question conditions network decision regions neural network connected respectively potentially disconnected. decision region class subset predicts class. similar study feedforward networks threshold activation functions show initial layer width order disconnected decision regions. empirical level recently argued decision regions caffe network imagenet connected. paper analyze feedforward networks continuous activation functions currently used practice. show line previous work almost networks pyramidal structure width layers smaller input dimension produce connected decision regions. show result tight providing explicit counterexamples case conclude guiding principle construction neural networks layer wider input dimension strong assumption bayes optimal classiﬁer connected decision regions. interestingly result holds leaky relu max{t able show standard relu max{t whereas result relu generalization straightforward even activations form max{l afﬁne functions different slopes. discuss also implications connected decision regions regarding generation adversarial samples provide another argument favor larger width. consider paper feedforward neural networks multi-class classiﬁcation. input dimension number classes. number layers layers indexed respectively corresponds input layer hidden layer output layer width layer consistency assume activation function every hidden layer following functions applied componentwise. deﬁne feature layer computes every input feature vector layer rnk−×nk weight matrix layer please note output layer linear usually done practice. consider following activation functions continuous strictly monotonically increasing. true proposed activation functions hold relu max{t hand argued recent literature following variants preferred relu deal better vanishing gradient problem outperform relu prediction performance leaky relu section prove results connectivity decision regions classiﬁer. require activation function continuous strictly monotonically increasing. main theorem holds feedforward networks arbitrary depth requires additionally second theorem holds hidden layer networks requirements activation function. show general pyramidal feedforward neural networks produce connected decision regions width hidden layers smaller equal input dimension. prove lemma following properties connected sets continuous functions useful. proposition continuous function. connected also connected set. proof pick must exist since connected holds deﬁnition exists continuous curve consider curve holds moreover continuous continuous. thus holds connected deﬁnition proposition minkowski connected subsets connected set. proof exists since connected sets exist continuous curves consider continuous curve holds every implies every elements connected continuous curve thus must connected set. apart property connectivity also consider property openness considering pre-image given network. recall following standard result topology proposition continuous function. open also open. recall standard result calculus showing certain restricted conditions inverse continuous mapping exists well continuous. proposition continuous strictly monotonically increasing. inverse mapping exists continuous. stated assumptions inverse mapping exists lemma continuous. since image connected continuous ˆσ−. thus proposition connected. moreover open proposition holds every ﬁrst term image open connected afﬁne mapping thus open connected second term linear subspace also connected closed. proposition minkowski connected sets connected. thus connected set. moreover pre-image open continuous function also open proposition thus open connected set. following lemma ingredient following proofs. allows show pre-image open connected hidden layer network open connected. using fact deep networks seen composition individual layers allow transfer result deep networks. lemma function deﬁned deﬁned bijective continuous strictly monotonically increasing linear deﬁned rm×n full rank open connected also open connected set. proof proposition holds h−). componentwise function inverse mapping given inverse mappings components show following decisions regions feedforward networks pyramidal maximal width smaller input dimension produce connected decision regions. assume activation functions fulﬁlled leaky relu. theorem width layers feedforward network network satisfy continuous strictly monotonically increasing function every layer weight matrices full rank. every decision region open connected subset every proof deﬁnition holds every since continuous bijection assumption follows also continuous bijection. moreover holds full rank connected set. thus apply lemma subsequently composed functions every obtains connected set. therefore open connected every next theorem holds networks hidden layer allows general activation functions continuous strictly monotonically increasing leaky relu softplus sigmoid activation functions. decision regions connected hidden layer maximal width smaller input dimension theorem hidden layer network satisfy continuous strictly monotonically increasing function weight matrices full rank. every decision region open connected subset every sets open convex sets intersection convex open well. thus connected set. rest argument folˆ lows using lemma noting proposition note theorem makes assumptions structure layers network. thus result holds weight matrices full rank maximal width statement theorem true decision regions disconnected. reason simply rank matrices particular ﬁrst layer reduce effective dimension input. illustrate effect small analytical example argue nevertheless practice extremely difﬁcult rank weight matrices. equal class probabilities figure illustration. note bayes optimal decision region class disconnected. moreover easy verify hidden layer network leaky relu perfectly data neural networks fully connected layers convolutional layers. moreover theorem holds regardless parameters network attained trained otherwise long weight matrices full rank. quite weak condition practice rank matrices lebesgue measure zero. even optimal weight parameters data generating distribution would rank unlikely trained weight parameters rank statistical noise training sample optimization noise usage stochastic gradient descent variants ﬁnally practice people early stopping thus even optimal solution training would rank theorem covers activation functions like leaky relu sigmoid softplus. moment unclear result might hold also general class activation functions treated theorem problem lemma compute pre-image even though sets connected intersection connected sets need connected. avoided theorem using initial convex intersection convex sets convex thus also connected. show result tight sense give empirical example neural network single hidden layer hidden units produces disconnected regions. note result complements show universal approximation property considers networks width least arbitrary depth. theorem indicates result also hold leaky relu approximation arbitrary functions implies approximation arbitrary decisions regions clearly requires able disconnected decision regions. taking results together seems rather obvious general guiding principle construction hidden layers least ﬁrst layer units input dimension rather unlikely bayes optimal decision regions connected. indeed true decision regions disconnected using network smaller width might still perfectly data since learned decision regions connected exists path true decision regions used potential adversarial manipulation. disconnected decision regions class red. thus theorems indeed hold weight matrices full rank. nevertheless practice unlikely rank weights illustrate figure show decision regions trained classiﬁer indeed connected decision regions. statistical noise training well noise optimization procedure common practice early stopping training neural networks. single image digit mnist dataset create artiﬁcial dataset underlying data generation probability measure similar one-dimensional structure embedded pixel space achieved using rotation onedimensional degree freedom. blue class rotate images degree range illustration. class generate images uniformly. note dataset effective degree freedom bayes optimal decision regions disconnected. train hidden layer network hidden units equal input dimension leaky relu activation function training error zero resulting weight matrices full rank. thus conditions theorem fulﬁlled decision region class connected even though bayes optimal decision region disconnected. happen establishing connection around blue class. test sampling source image part class target image part consider simply line segment images sample densely dividing equidistant parts. corresponding images classiﬁed high conﬁdence. illustrate complete path source target image figure color indicates images classiﬁed plot also conﬁdence turning output network probabilities using softmax. experiment illustrates exists continuous path disconnected parts class even stays time high conﬁdence regions. want claim intermediate images look much like images class blue somehow related thus could seen adversarial samples blue class. point want make might think order avoid adversarial manipulation solution simple classiﬁer capacity. think rather opposite true sense classiﬁer rich enough model true underlying data generating distribution able model true decision boundaries. particular classiﬁer able realize disconnected decision regions order avoid paths input space connect different disconnected regions bayes optimal classiﬁer. could argue problem synthetic example corresponding digits obviously whole image space nevertheless classiﬁer prediction possible images. output classes varying number hidden units well leaky relu activation function cross-entropy loss training objective. optimization momentum train epochs learning rate reduce factor every epochs. cases resulting weight matrices full rank. show training error decision regions trained networks figure grid size case figure manually chosen clearly connected/disconnected components decision regions. first observe hidden units network satisﬁes condition theorem thus learn connected regions also clearly ﬁgure basically gets linear separator. however three hidden units network produce disconnected decision regions shows theorems tight sense width already sufﬁcient produce disconnected components whereas results width less decision regions connected. number hidden units increases observe network produces easily disconnected decision regions expected. figure decision regions training error hidden layer networks varying number hidden units example given figure shown theorem decision region connected however already gets disconnected decision regions shows theorem tight. figure ﬁgure illustrates trajectory source image figure target image figure parametrized caption image represents conﬁdence image predicted class corresponding parameter. could handled introducing background class would even important classiﬁer produce disconnected decision regions requires minimal width network. figure positive class refers images rotated angles uniformly randomly sampled sample figure negative class refers images either illustrate sample sets figure figure respectively. shown deep neural networks need general width larger input dimension order learn disconnected decision regions. remains open problem current requirement removed. result resolve question choose network architecture practice provides least guideline choose width network. moreover result experiments show narrow networks produce high conﬁdence predictions path connecting true disconnected decision regions could used attack networks using adversarial manipulation. shelhamer donahue karayev long grishick guadarrama darrell caffe convolutional architecture fast feature embedding. int. conference multimedia", "year": 2018}