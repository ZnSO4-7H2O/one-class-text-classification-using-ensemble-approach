{"title": "Hybrid Generative/Discriminative Learning for Automatic Image Annotation", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Automatic image annotation (AIA) raises tremendous challenges to machine learning as it requires modeling of data that are both ambiguous in input and output, e.g., images containing multiple objects and labeled with multiple semantic tags. Even more challenging is that the number of candidate tags is usually huge (as large as the vocabulary size) yet each image is only related to a few of them. This paper presents a hybrid generative-discriminative classifier to simultaneously address the extreme data-ambiguity and overfitting-vulnerability issues in tasks such as AIA. Particularly: (1) an Exponential-Multinomial Mixture (EMM) model is established to capture both the input and output ambiguity and in the meanwhile to encourage prediction sparsity; and (2) the prediction ability of the EMM model is explicitly maximized through discriminative learning that integrates variational inference of graphical models and the pairwise formulation of ordinal regression. Experiments show that our approach achieves both superior annotation performance and better tag scalability.", "text": "raises automatic machine learning tremendous challenges requires modeling data ambiguous input output e.g. images containing multiple objects labeled multiple semantic tags. even challenging number candidate tags usually huge image related them. paper presents hybrid generative-discriminative classiﬁer simultaneously address extreme overﬁtting-vulnerability data-ambiguity issues tasks aia. particularly exponential-multinomial mixture model established capture input output ambiguity meanwhile encourage prediction sparsity; prediction ability model explicitly maximized discriminative learning integrates variational inference graphical models pairwise formulation ordinal regression. experiments show approach achieves superior annotation performance better scalability. introduction exponential growth internet photographs videos automatic image annotation increasingly important indexing managing retrieving multimedia data. usually includes types tasks image annotation assigns descriptive metadata given image; region annotation annotates object within given image appropriate textual tags. raises tremendous challenges machine learning algorithms. firstly image usually contains multiple objects consistent scene recognize object thus requires modeling input-ambiguous systems i.e. input example contains correlated instances individual output interest. triggers nontrivial application multi-instance learning addition huge image volumes extremely diverse scene topics prohibitive employ skilled experts manually label training data sufﬁciently large support statistical learning. instead access partially labeled images e.g. photos captions specifying names persons photo name goes face. learning ambiguously labeled multi-instance data opens frontier machine learning i.e. modeling systems/data ambiguous input output motivating recently emerged framework multi-label multi-instance classiﬁcation perhaps even challenging that real world task number candidate tags usually huge image related them. indeed scene topics internet images/videos extremely diverse noun-phrase valid tag. makes naive classiﬁer extremely vulnerable overﬁtting relevant tags could easily eclipsed crowd huge number irrelevant ones; data could extremely scarce training classiﬁer corresponding individual tag. example considering setting consists evenly distributed tags even labeled training images examples expected accuracy baseline random guessing classiﬁer. reduce risk overﬁtting desirable pursue classiﬁers scalable class size. however little progress made topic. paper abstract generic learning task called many-class multi-label multi-instance classiﬁcation aims learning decision rules data ambiguous input output involve massive classes. typically data example consists multiple instances associated several huge number classes paper present hybrid generative-discriminative classiﬁer address extreme data ambiguity overﬁtting vulnerability issues involved particularly exponential-multinomial mixture model established able capture input output ambiguity data meanwhile encourage prediction sparsity automatically rule irrelevant classes furthermore maximize discriminant ability handle small-sample problem incurred massive classes well derive inference learning algorithms based principle margin maximization leading learning formulation integrating variational inference graphical model pariwise preference formulation ordinal regression efﬁciently solved convex optimization. compared traditional learning discriminative learning formulation explicitly optimizes predication performance also enjoys additional advantages stronger supervision perfect data balance. system builds multi-instance multi-label corpora segmenting image instances using words caption labels. region represented using standard bag-of-discretefeature framework example analogous document instance paragraph feature word. experiments image caption collections alipr labelme show algorithm achieves superior annotation accuracy also better tag-scalability i.e. performance robustness label size increasing training size decreasing. related works automatic image annotation usually addressed using machine translation approaches image-caption pairs viewed bi-lingual texts machine translation techniques applied align textual vocabulary visual vocabulary paper instead learning loose correspondences tags visual words attempt learn classiﬁer able directly discriminate semantic tags based visual content/context image regions. critical challenges data ambiguity overﬁtting vulnerability massive tags. substantial efforts made address former issue. traditional multi-class image classiﬁcation assigns single entire image thus captures neither input output ambiguity. multi-label classiﬁcation annotating image tags captures output ambiguity applicable task whereas multiinstance classiﬁcation annotates image region single captures input ambiguity thus cannot annotate multitag images. quite recently several approaches developed multi-label multi-instance classiﬁcation naturally address input output ambiguity. although approaches observed achieve superior performances ambiguity modeling none addresses massive-tag issue fundamental challenge real-world tasks. result applicable task moderate number classes. paper attempt simultaneously address data ambiguity massive-tag issues. model builds dirichlet-bernoulli alignment model capture data ambiguity. dirichlet-multinomial mixture model similar slda topic model supervised topics limitation classiﬁcation prediction turns smoothed classes making performance deteriorate exponentially number classes increases. paper take measures improve tag-scalability. first instead using dirichlet prior smooth prediction among tags adopt exponential prior encourage prediction sparsity. signiﬁcantly restricts number active tags given image ensuring classiﬁer effectively identify relevant tags automatically ruling irrelevant ones. secondly unlike traditional learning model trained discriminatively explicitly maximize prediction accuracy. also noticed surge needs sparse topic models machine learning community. believe model good start. simpliﬁed version model seamlessly integrates important learning tools topic modeling sparse coding enjoys advantages both outstanding interpretability topic models discovering topics intuitively comprehensible extraordinary performance sparse-coding learning predictive topics. indeed sparsity turns highly-preferable property learning algorithms. objective convex solution space areas nonsparse learning algorithms usually lead local optima under-learned models. sparsity prior/regularization/constraint solutions become distinguishable hence lead better generalization ability. informationtheoretic viewpoint sparser might also imply smaller description length. traditional multiclass classiﬁcation given labeled examples yn}n point vector space point moderatesized goal decision rule many-class multi-label multi-instance classiﬁcation task number classes extremely large example well label neither points sets given labeled training yn}n example contains instances label consists class labels goal decision rule denotes power make following assumptions section develop bayesian hierarchical model model data ambiguity model built recent advances probabilistic topic models modeling document admixture latent topics topic models allow document associated multiple topics different proportions thus provide principled capture ambiguity data. however topics discovered topic model essentially multinomial distributions words. although textual data possible interpret discovered topics based human expertise image data multinomial distributions visual features generally incomprehensible. here natural language tags explicit topics allows learn classiﬁers ambiguously labeled data also enables discriminative learning. also using exponential rather dirichlet prior model seamlessly integrates important learning tools i.e. topic modeling sparse coding develop bayesian hierarchical model based aforementioned assumptions. exponentialmultinomial mixture model assume example generated following process sample θ∼exp instances generate example-level label model exponential distribution used model property label parsimoniousness prior parameter. instance-level class indicator binary c-vector -of-c code generated multinomial distribution parameter ˜θ=θ/||θ||. example-level label also binary c-vector pattern belongs c-th class otherwise. assume example-level label generated costsensitive voting process based instance-level labels. denote average instance-level label assignments degraded soft-max regression model assume instance described discrete features although quite straightforward substitute instance models e.g. mixtures gaussian. bag-of-feature representation images popularized computer vision simplicity robustness effectiveness achieved building vocabulary visual codewords e.g. clustering representative image patches dictionary discrete features therefore multinomial model denotes likelihood observing labeled samples yn}n discriminative formulation enjoys additional advantages exploits stronger supervision suppose training example relevant tags irrelevant ones pairwise formulation extends supervision size relieves data imbalance issue typically massive relevant tags example occupy tiny proportion thus formulation suffers severe supervision imbalance whereas max-margin formulation perfectly balanced i.e. priori pair equally positive negative lagragian multipliers. last terms involve marginal probabilities requires integration latent variables exact computation tractable therefore mean-ﬁeld variational method derive upper bound approximate lagrangian objective optimize upper bound instead. overall learning algorithm optimization e-step uses variational method approximate lagrangian whereas m-step turn optimizes learn model parameters. introduced full-factorized variational disn tribution mult kullback-leibler divergence posterior distribution latent variables. denote entropy variational lower bound eq]+hq. number classes typically large parameters could high dimensions. usually leads severe problems overﬁtting maximum likelihood estimators used. cope problem adopt fully bayesian treatment posing conjugate priors variables. particularly assume i.i.d. generated gamma distribution ∼gamma .βc} sampled dirichlet distribution ∼dir overall model depicted graphical representation figure paper casting topics explicitly class labels eligible discriminative learning. develop maximum margin algorithms inference learning leading hybrid generative/discriminative approach. taking advantage max-margin learning’s well-known ability handling small-sample problem hope relieve overﬁtting issue incurred massive classes also hope improve classiﬁcation accuracy directly maximizing discriminative ability using max-margin optimization. none concerns could otherwise addressed learning. formulation based margin-maximization principle used training svms. algorithm integration variational inference pariwise ordinal regression attempting parameters place decision boundaries relevant irrelevant label pair apart possible. learning problem formulated follows components nonzero values therefore effectively reduces number candidate classes given example relevant. considering instance-level labels sampled according example-level label generated based exponential membership acting like ﬁlter automatically identiﬁes relevant labels ruling large number irrelevant ones. optimization decomposable three parameters since involved likelihood learn parameters approximate empirical bayesian procedure i.e. maximizing variational likelihood keeping variational parameters ﬁxed. speciﬁcally taking account gamma prior have paper svmperf cuttingplane solver. note although involves huge number constraints little proportion actually active lagrangian multipliers become exact zero optima. also cutting-plane optimization progressively adds violated constraints guarantee working always controllable size. prediction consider task mentioned task i.e. label prediction given example ﬁrst variational inference procedure approximate label distribution ˆθw/||ˆθw||. figure image annotation performance tag-scalability comparison. top-k accuracy top- accuracy size. top- accuracy training size. results alipr. results labelme. note irrelevant tags λcρc− would lead exact zero response also note that exponential term exp) remn moved updating since label testing example unobserved. evaluate method data sets popular online annotation engines alipr labelme respectively. alipr data consists images total number unique tags average tags image. labelme data contains images tags tags image. cover sufﬁciently diverse scenes e.g. indoor urban village road landscape. image-caption pair image segmented n-cuts algorithm region used instance word caption used label. geometry-free bag-of-discrete-feature model adopted represent region simplicity robustness good performance figure region annotation performance comparison top-k accuracy annotated regions captioned images images without captions results obtained labelme data set. particularly randomly sample ﬁxed-size local patches image characterize patch using -dimensional sift descriptor encode descriptor using hard membership nearest codebook center. image analogous ﬁxed-length document region paragraph represented histogram visual word counts. visual vocabulary built running k-mean algorithm vector quantize descriptors patches sampled randomly chosen images. snow mountain cloud boat harbor water boat snow mountain snow water mountain plane mountain plane house water indoor water mountain snow plant bridge corrlda model machine-translation-based method shown outperform many methods class mle-trained dirichlet-multinomial mixture model multi-label multi-instance classiﬁcation. trade-off parameters tuned -fold cross validation procedure. reported results testing accuracy also estimated -fold cross validation. ﬁrst apply approaches image-level annotation i.e. image caption prediction. models output probabilistic predictions top-k accuracy evaluation metric micro-averaged measure computed top-ranked k-sublist predictions. results illustrated figure firstly observe classiﬁcation based approaches generally achieves better performance machine-translation based approach validating beneﬁcial solving classiﬁcation task. among three classiﬁcation approaches hybrid approach achieves consistently best accuracies data sets. interestingly trained classiﬁers alipr emmm performs better head predictions slightly worse tail predictions whereas labelme whose size bigger emmm performs almost consistently predictions indicating model might better tag-scalability. worth noting measures turns signiﬁcantly improve performances particularly terms accuracy encouraging prediction sparsity model alone improves performance alipr labelme; max-margin learning hybrid approach gains improvements emmm. annotation results system satisfactorily good. intuitive demonstration figure shows example images top- annotations algorithm. annotations hybrid approach test label-scalability algorithm. also shown figure top- accuracy algorithm trained ﬁxed-size image increasing-number tags; decreasing-size image ﬁxed number tags. expected performances deteriorate exponentially cases. contrast although hybrid approach also based classiﬁcation performance stable. fact scalability emmd much better even compared machine translation based approach corrlda. ﬁnally compare three algorithms region-level annotation. conduct task captioned images uncaptioned images. note alipr data include region-level labels hence cannot used assess performances task. results labelme data depicted figure ﬁrst setting hybrid approach outperforms competitors consistently; latter performs best top-relevant predictions comparably tail predictions still quite encouraging predictions usually important upon ﬁnal tags ﬁnally decided. also observe classiﬁcation-based algorithms emmd emmm substantially outperform machine-translation-based approach corrlda suggesting former powerful disambiguating scenes. automatic image annotation involves modeling data extremely ambiguous sparse critically challenging existing learning algorithms. paper investigated generic task many-class multi-label multi-instance classiﬁcation devised hybrid generative-discriminative learning approach tasks aia. proposed approach includes bayesian hierarchical model able capture input output ambiguity meanwhile encourage prediction sparseness discriminative learning formulation integrates variational inference pairwise ordinal regression maximize prediction power. tested approach real-world benchmarks showed satisfactory annotation performance well superior scalability size. limitation current model that assuming instance exchangeability account context correlations example apple likely mean computer rather fruit appears scene together mouse. plan explore context correlation future work. also scale evaluation limited availability labeled data plan extend real-world scale soon larger labeled data available. also plan empirically compare model state-of-art annotation algorithms provide picture existing annotation methods.", "year": 2012}