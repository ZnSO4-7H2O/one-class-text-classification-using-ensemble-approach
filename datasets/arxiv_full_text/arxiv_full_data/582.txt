{"title": "Generating Text with Deep Reinforcement Learning", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "We introduce a novel schema for sequence to sequence learning with a Deep Q-Network (DQN), which decodes the output sequence iteratively. The aim here is to enable the decoder to first tackle easier portions of the sequences, and then turn to cope with difficult parts. Specifically, in each iteration, an encoder-decoder Long Short-Term Memory (LSTM) network is employed to, from the input sequence, automatically create features to represent the internal states of and formulate a list of potential actions for the DQN. Take rephrasing a natural sentence as an example. This list can contain ranked potential words. Next, the DQN learns to make decision on which action (e.g., word) will be selected from the list to modify the current decoded sequence. The newly modified output sequence is subsequently used as the input to the DQN for the next decoding iteration. In each iteration, we also bias the reinforcement learning's attention to explore sequence portions which are previously difficult to be decoded. For evaluation, the proposed strategy was trained to decode ten thousands natural sentences. Our experiments indicate that, when compared to a left-to-right greedy beam search LSTM decoder, the proposed method performed competitively well when decoding sentences from the training set, but significantly outperformed the baseline when decoding unseen sentences, in terms of BLEU score obtained.", "text": "introduce novel schema sequence sequence learning deep qnetwork decodes output sequence iteratively. enable decoder ﬁrst tackle easier portions sequences turn cope difﬁcult parts. speciﬁcally iteration encoder-decoder long short-term memory network employed input sequence automatically create features represent internal states formulate list potential actions dqn. take rephrasing natural sentence example. list contain ranked potential words. next learns make decision action selected list modify current decoded sequence. newly modiﬁed output sequence subsequently used input next decoding iteration. iteration also bias reinforcement learning’s attention explore sequence portions previously difﬁcult decoded. evaluation proposed strategy trained decode thousands natural sentences. experiments indicate that compared left-to-right greedy beam search lstm decoder proposed method performed competitively well decoding sentences training signiﬁcantly outperformed baseline decoding unseen sentences terms bleu score obtained. introduction many real-world problems effectively formulated sequence sequence learning. important applications include speech recognition machine translation text rephrasing question answering. example last three expressed mapping sentence words another sequence words. major challenge modeling tasks variable length sequences often known a-priori. address that encoder-decoder long short-term memory architecture recently shown effective idea lstm encode input sequence resulting ﬁxed dimensional vector representation. subsequently another lstm deployed decode output sequence using newly created vector lstm’s initial state. decoding process essentially recurrent neural network language model decoding schema based recurrent language models naturally left-to-right decoding procedure aims obtain output sequence maximal probability select list sequence candidates post-processing. paper propose alternative strategy training end-to-end decoder. speciﬁcally employ deep q-network embrace iterative decoding strategy. detail input sequence ﬁrst encoded using encoder-decoder lstm network. process automatically generates informative features represent internal states list potential actions dqn. next employed iteratively decode output sequence. consider rephrasing natural sentence. list potential actions contain ranked word candidates. scenario learns make decision word selected list modify current decoded sequence. newly edited output sequence subsequently used input next decoding iteration. inspired recent success attention mechanisms also bias reinforcement learning’s attention iteration explore sequence portions previously difﬁcult decoded. decoded sequence last iteration used ﬁnal output model. unlike left-to-right decoding schema able learn ﬁrst focus easier parts sequence resulted information help solve difﬁcult portions sequence. example sentence testing data decoded encoder-decoder lstms click read york times successfully corrected click read york times second iteration. evaluation proposed strategy trained encode decode thousands natural sentences. experimental studies indicate proposed method performed competitively well decoding sentences training compared left-to-right greedy beam search decoder lstms signiﬁcantly outperformed baseline decoding unseen sentences terms bleu score obtained. context reinforcement learning decoding sequential text need overcome challenge arise large number potential states actions. mainly ﬂexible word ordering sentence existence large number words synonyms modern dictionaries. best knowledge work ﬁrst decode text using dqn. particular employ lstms generalize informative features text represent states also create list potential actions text dqn. intuitively application also effect generating synthetic sequential text training networks dqn’s exploration strategy training. background reinforcement learning deep q-network reinforcement learning commonly used framework learning control policies computer algorithm so-called agent interacting environment given internal states predeﬁned actions agent takes action state following certain policies rules result state receive reward agent maximize cumulative reward sequence actions. action forms transition tuple markov decision process practically environment unknown partially observed sequence state transition tuples used formulated environment. q-learning popular form model-free technique used learn optimal action-value function measure action’s expected long-term reward agent. typically q-value function relies possible state-action pairs often impractically obtained. work around challenge approximate using parameterized function parameter often learned features generalized states actions environment promisingly beneﬁting recent advance deep learning techniques shown able effectively generate informative features wide ranges difﬁcult problems mnih introduced deep q-network approximates q-value function non-linear deep convolutional network also automatically creates useful features represent internal states agent interacts environment discrete iteration taking maximize long term reward. starting random q-function agent continuously updates q-values taking actions obtaining rewards consulting current q-value function. iterative updates derived bellman equation expectation often computed transition tuples involved agent taking action state requires informative representation internal states. playing video games infer state representations directly pixels screens using convolutional network however text sentences instance contain sequential nature text also variable length. lstm’s ability learn data long range temporal dependencies varying lengths makes natural choice replace convolutional network application here. next brieﬂy describe lstm network. long short-term memory recurrent neural networks deploying recurrent hidden vector recurrent neural networks compute compositional vector representations sequences arbitrary length. network learns complex temporal dynamics mapping length input sequence sequence hidden states networks compute hidden state vector recursive application transition function popular variant rnns namely lstms designed overcome vanishing gradient issue rnns thus better modeling long term dependencies sequence. addition hidden unit lstm includes input gate forget gate output gate memory cell unit vectors following purposes. memory cell unit self-connection capable considering pieces information. ﬁrst previous memory cell unit modulated forget gate. here forget gate embraces hidden states adaptively reset cell unit self-connection. second piece information function current input previous hidden state modulated input gate. intuitively lstm learn selectively forget previous memory consider current input. similarly output gate learns much memory cell transfer hidden state. additional cells enable lstm preserve state long periods time figure iteratively decoding lstm; encoder-decoder lstm network depicted gray-ﬁlled rectangles bottom; top-left graphical illustration bidirectional lstms; dash arrow line right indicates iteration loop. generating sequence deep q-network employ encoder-decoder lstm network presented automatically generate informative features learn q-value function approximate long term rewards. learning algorithm depicted figure algorithm generating state representations lstms encoder-decoder lstm network depicted gray-ﬁlled rectangles figure descriptive purpose named state generation function context dqn. detail given natural sentence tokens ﬁrst encode sequence using lstm reading tokens timestep time reaching sentence encode process results ﬁxed dimensional vector representation whole sentence namely hidden layer vector used initial state another lstm decoding generate target sequence process hidden vectors delstm also conditioned input consequently delstm creates sequence hidden states time step. next hidden vectors softmax function produce distribution possible classes thus creating list word probabilities time step i.e. prov parameter decoder-encoder lstms namely stategf function tuned maximize probability correct decoding sentence given source sentence using following training objective straight forward effective method decoding search suggested deploy simple left-to-right beam search. decoder maintains small number incomplete sentences. timestep decoder extends partial sentence beam every possible word vocabulary. suggested beam size works well. feeding state generate function ensen result decoded sentence desen. decoding discussed next employs iteration strategy denote sentence sequence pair enseni deseni; indicates i-th iteration dqn. iteratively decoding sequence deep q-network decoding iteration considers sentence pair namely enseni deseni internal state. also ranked words list time step delstm treated potential actions dqn. lists learns predict actions taken order accumulate larger long time reward. detail hidden vector delstm neural network neural networks learn approximate q-value function given dqn’s current state contains enseni deseni well word probability list time step delstm. take action q-value outputs. consider iteration current state modiﬁed accordingly. deseni modiﬁed replacing process results decoded sentence word time step namely replacing namely deseni+ next similarity target sentence current decoded sentence deseni+ evaluated bleu thus transition tuple next iteration newly generated sentence deseni+ generate next decoded sentence deseni+. training optimal weight matrix neural networks. q-network trained minimizing sequence loss functions iteration algorithm generating text deep q-network initialize replay memory initialize enlstm delstm random weights pretraining encoder-decoder lstms epoch training q-value function epoch esa|s target q-value reward parameters ﬁxed previous iteration. words trained predict expected future reward. updates parameters performed following gradient learning q-value function agent chooses action highest order maximize expected future rewards decoding sequences. quite often trade-off exploration exploitation strategy employed agent. following ǫgreedy policy agent perform random action probability inspired recent success attention mechanisms bias reinforcement learning’s attention explore sequence portions difﬁcult decoded. random actions chance picked tokens decoded incorrectly previous iterations. bidirectional lstms decoding would like information entire input sequence i.e. figure attain goal deploy bidirectional lstms speciﬁcally speciﬁc time step given sequence bidirectional lstm enables hidden states summarize time step past future sequence. network deploys separate hidden layers precess data directions left right another right left time step hidden state bidirectional lstm concatenation forward backward hidden states forwards output layer. equation implemented follows reward calculated based closeness target sentence decoded output sentence takes action. compute similarity sentence pair using popular score metric statistical translation. speciﬁcally obtain bleu score sentences. measure score difference current iteration previous iteration. difference positive reward assigned; negative reward; otherwise zero. note that since conduct sentence level comparison adopt smoothed version bleu unlike bleu smoothedbleu avoids giving zero score even -gram matches sentence pair. separating state generation function experiments suggest separating state generation function networks beneﬁcial. deterministic network generating states sequence pair. given input pair encoder-decoder lstms network namely state generation function stategf always decoded output sequence. empirical studies indicate important successfully training decoding text. intuitive explanation follows. using approximate q-value function intuitively equals train network moving targets network’s targets depend network itself. suppose given input feed stategf would generate different output sequence time dqn. scenario network also deal moving state function involving text high dimensionality. intuitively agent living changing environment result difﬁcult learn predict q-value since states rewards unstable change even input feed. pre-training state generation function empirical techniques employed ensure deterministic network generating states dqn. firstly deploy pre-training technique. speciﬁcally pre-train state generation function stategf input sequence enlstm’s input target sequence delstm’s input. training converges networks’ weights ﬁxed training network starts. secondly training input sequence enlstm decoded sequence previous iteration used delstm input stage portions figure updated. reward errors networks backpropagated state generation functions. updating replay memory sampling studies also indicate that performing updates q-value function using transitions current training sentence causes network strongly overﬁt current input sentence. result sentence training always predict previous sentence used training. avoid correlation issue replay memory strategy applied updating dqn. updated transition tuples different current input sequence. action takes save transition tuple replay memory pool including enseni deseni deseni+ updating randomly sample transition tuple replay memory pool. sophisticated replay memory update could applied here; would like leave future work. example priority sampling replay technique transitions large rewards chance chose. case bias selection transitions high bleu scores. importance supervised softmax signal also conduct experiments without supervised tmax error network. whole network including lstms receive error signals q-value predictions. observed that without supervised signal difﬁcult learn. intuition follows. firstly discussed before decoding text typically involves large number potential states actions challenge learn optimal policy moving state generation function moving q-value target function. secondly potential actions namely word probability list output delstm changing unreliable complicate learning dqn. simultaneously updating softmax q-value error training update discussed previously also update state generation functions i.e. encoder-decoder lstms. found network could easily bias state generation functions since tmax error signal strong reliable thus sufﬁciently tuned. course could bias towards learning would introduce tricky parameter tuning. addition indeterministic state generation function again. experiments task dataset experimental task train network regenerate natural sentences. given sentence input network ﬁrst compresses ﬁxed vector vector used decode input sentence. words algorithm same. experiment randomly select sentences length billion word corpus train model sentences select best model validation data consist sentences. test model seen sentences unseen sentences. seen test randomly sampled training set. training testing detail computational reason used one-layer lstm encoder-decoder lstms well backward lstm memory cells dimensional word embeddings. used softmax words output delstm. initialized lstms parameters uniform distribution including word vectors. used adaptive stochastic gradient descent without momentum starting learning rate although lstms tend suffer vanishing gradient problem exploding gradients. thus employ gradient norm clip technique threshold used regularization dropout avoid overﬁtting networks. ﬁrst pretrain encoder-decoder lstms target sentence input. training converges start train dqn. training turn drop encoder-decoder lstms deterministic network generate states lists word probabilities dqn. addition scale epsilon iterations. words actions beginning training random became greedy towards training. sentence length allow edit sentence iterations namely taking actions decoding. sentence decoded iteration saved replay memory capacity discount factor also bleu score threshold indicating decoding success initial states bi-directional lstms used ﬁxed vector generated lstm encoder. testing phase also sentence steps. also experiment used word probability lists potential actions dqn. since maximal length sentence experiment output nodes. namely choose words corresponding time step delstm action take action indicates modiﬁcation needed. compared strategy encoder-decoder lstm network used machine translation. baseline decoder searches likely output sequence using simple left-to-right beam search technique. suggested beam size worked well. adopt approach decoding baseline. experiments nvidia titanx memory. report average smoothedbleu score testing sentences. steadily decreasing average bleu score gradually increasing. curves stabilized iterations. also right subﬁgure figure indicates that reward obtained negative beginning training gradually moved positive reword zone. negative rewards expected dqn’s actions random beginning training. gradually knew decode sentences receive positive rewards. shown right ﬁgure training converged epochs. results indicate state generation functions easy trained. table show testing results terms average smoothedbleu obtained seen unseen sentences. observe that although results achieved seen data slightly better baseline lstms network unseen data meaningfully outperformed baseline. analysis suggests follows. seen data decoder tended agree lstm decoder. time decision modiﬁcation. unseen data dqn’s exploration strategy allows learn many noisy data lstms networks decoder able tolerate better noise generalize well unseen data. intuitively application also effect generating synthetic sequential text training decoder exploration component. also conducted experiments observe behaviors exploration; considered unseen testing data set. enabled follow ǫ-greedy policy respectively. words allowed agent choose best actions according q-value function time. experimental results terms bleu score obtained presented figure figure conclude exploration strategy testing time help dqn. results indicate allowing explore testing time decreased predictive performance terms bleu score obtained recently deep q-network shown able successfully play atari games trained variant q-learning learns control strategies using deep neural networks. main idea deep learning automatically generate informative features represent internal states environment software agent lives subsequently approximate non-linear control police function learning agent take actions. addition playing video games employing reinforcement learning learn control policies text also investigated. applications include interpreting user manuals navigating directions playing text-based games also recently employed learn memory access patterns rearrange given words unlike works research aims decode natural text dqn. addition employ encoder-decoder lstm network generalize informative features text represent states also create list potential actions text dqn. deploy deep q-network embrace iterative decoding strategy sequence sequence learning. encoder-decoder lstm network employed automatically approximate internal states formulate potential actions dqn. addition incorporate attention mechanism reinforcement learning’s exploration strategy. exploration intuitively enables decoding network learn many synthetic sequential text generated decoding stage. evaluate proposed method sentence regeneration task. experiments demonstrate approach’s promising performance especially decoding unseen sentences terms bleu score obtained. paper also presents several empirical observations terms model design order successfully decoding sequential text dqn. future allowing pick words list time step delstm would studied. furthermore would like experiment sophisticated priority sampling techniques training. particular interested applying approach statistical machine translation.", "year": 2015}