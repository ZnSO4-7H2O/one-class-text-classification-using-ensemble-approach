{"title": "Graph Attention Networks", "tag": ["stat.ML", "cs.AI", "cs.LG", "cs.SI"], "abstract": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).", "text": "present graph attention networks novel neural network architectures operate graph-structured data leveraging masked self-attentional layers address shortcomings prior methods based graph convolutions approximations. stacking layers nodes able attend neighborhoods’ features enable specifying different weights different nodes neighborhood without requiring kind costly matrix operation depending knowing graph structure upfront. address several challenges spectral-based graph neural networks simultaneously make model readily applicable inductive well transductive problems. models achieved matched state-of-theart results across four established transductive inductive graph benchmarks cora citeseer pubmed citation network datasets well proteinprotein interaction dataset convolutional neural networks successfully applied tackle problems image classiﬁcation semantic segmentation machine translation underlying data representation grid-like structure. architectures efﬁciently reuse local ﬁlters learnable parameters applying input positions. however many interesting tasks involve data represented grid-like structure instead lies irregular domain. case meshes social networks telecommunication networks biological networks brain connectomes. data usually represented form graphs. several attempts literature extend neural networks deal arbitrarily structured graphs. early work used recursive neural networks process data represented graph domains directed acyclic graphs graph neural networks introduced gori scarselli generalization recursive neural networks directly deal general class graphs e.g. cyclic directed undirected graphs. gnns consist iterative process propagates node states equilibrium; followed neural network produces output node nevertheless increasing interest generalizing convolutions graph domain. advances direction often categorized spectral approaches non-spectral approaches. hand spectral approaches work spectral representation graphs successfully applied context node classiﬁcation. bruna convolution operation deﬁned fourier domain computing eigendecomposition graph laplacian resulting potentially intense computations non-spatially localized ﬁlters. issues addressed subsequent works. henaff introduced parameterization spectral ﬁlters smooth coefﬁcients order make spatially localized. later defferrard proposed approximate ﬁlters means chebyshev expansion graph laplacian removing need compute eigenvectors laplacian yielding spatially localized ﬁlters. finally kipf welling simpliﬁed previous method restricting ﬁlters operate -step neighborhood around node. however aforementioned spectral approaches learned ﬁlters depend laplacian eigenbasis depends graph structure. thus model trained speciﬁc structure directly applied graph different structure. hand non-spectral approaches deﬁne convolutions directly graph operating groups spatially close neighbors. challenges approaches deﬁne operator works different sized neighborhoods maintains weight sharing property cnns. cases requires learning speciﬁc weight matrix node degree using powers transition matrix deﬁne neighborhood learning weights input channel neighborhood degree extracting normalizing neighborhoods containing ﬁxed number nodes monti presented mixture model cnns spatial approach provides uniﬁed generalization architectures graphs. recently hamilton introduced graphsage method computing node representations inductive manner. technique operates sampling ﬁxed-size neighborhood node performing speciﬁc aggregator approach yielded impressive performance across several large-scale inductive benchmarks. attention mechanisms become almost facto standard many sequence-based tasks beneﬁts attention mechanisms allow dealing variable sized inputs focusing relevant parts input make decisions. attention mechanism used compute representation single sequence commonly referred self-attention intra-attention. together recurrent neural networks convolutions self-attention proven useful tasks machine reading learning sentence representations however vaswani showed self-attention improve method based rnns convolutions also sufﬁcient constructing powerful model obtaining state-of-the-art performance machine translation task. inspired recent work introduce attention-based architecture perform node classiﬁcation graph-structured data. idea compute hidden representations node graph attending neighbors following self-attention strategy. attention architecture several interesting properties operation efﬁcient since parallelizable across nodeneighbor pairs; applied graph nodes different degrees specifying arbitrary weights neighbors; model directly applicable inductive learning problems including tasks model generalize completely unseen graphs. validate proposed approach four challenging benchmarks cora citeseer pubmed citation networks well inductive protein-protein interaction dataset achieving matching state-of-the-art results highlight potential attention-based models dealing arbitrarily structured graphs. sharing neural network computation across edges reminiscent formulation relational networks vain wherein relations objects agents aggregated pair-wise employing shared mechanism. similarly proposed attention model connected works duan denil neighborhood attention operation compute attention coefﬁcients different objects environment. related approaches include locally linear embedding memory networks selects ﬁxed number neighbors around data point learns weight coefﬁcient neighbor reconstruct point weighted neighbors. second optimization step extracts point’s feature embedding. memory networks also share connections work particular interpret neighborhood node memory used compute node features attending values updated storing features position. section present building block layer used construct arbitrary graph attention networks directly outline theoretical practical beneﬁts limitations compared prior work domain neural graph processing. start describing single graph attentional layer sole layer utilized throughout architectures used experiments. particular attentional setup utilized closely follows work bahdanau —but framework agnostic particular choice attention mechanism. input layer node features number nodes number features node. layer produces node features order obtain sufﬁcient expressive power transform input features higher-level features least learnable linear transformation required. initial step shared linear transformation parametrized weight matrix applied every node. perform self-attention nodes—a shared attentional mechanism computes attention coefﬁcients indicate importance node features node general formulation model allows every node attend every node dropping structural information. inject graph structure mechanism performing masked attention—we compute nodes neighborhood node graph. experiments exactly ﬁrst-order neighbors make coefﬁcients easily comparable across different nodes normalize across choices using softmax function experiments attention mechanism single-layer feedforward neural network parametrized weight vector applying leakyrelu nonlinearity fully expanded coefﬁcients computed attention mechanism expressed represents transposition concatenation operation. obtained normalized attention coefﬁcients used compute linear combination features corresponding them serve ﬁnal output features every node whj) employed model parametrized weight vector applying leakyrelu activation. right illustration multihead attention node neighborhood. different arrow styles colors denote independent attention computations. aggregated features head concatenated averaged obtain stabilize learning process self-attention found extending mechanism employ multi-head attention beneﬁcial similarly vaswani speciﬁcally independent attention mechanisms execute transformation equation features concatenated resulting following output feature representation represents concatenation normalized attention coefﬁcients computed k-th attention mechanism corresponding input linear transformation’s weight matrix. note that setting ﬁnal returned output consist features node. specially perform multi-head attention ﬁnal layer network concatenation longer sensible—instead employ averaging delay applying ﬁnal nonlinearity then graph attentional layer described subsection directly addresses several issues present prior approaches modelling graph-structured data neural networks nodes. eigendecompositions similar costly matrix operations required. time complexity single attention head computing features expressed |e|f number input features numbers nodes edges graph respectively. complexity baseline methods graph convolutional networks applying multi-head attention multiplies storage parameter requirements factor individual heads’ computations fully independent parallelized. opposed gcns model allows assigning different importances nodes neighborhood enabling leap model capacity. furthermore analyzing learned attentional weights lead beneﬁts interpretability case machine translation domain attention mechanism applied shared manner edges graph therefore depend upfront access global graph structure nodes several desirable implications recently published inductive method hamilton samples ﬁxed-size neighborhood node order keep computational footprint consistent; allow access entirety neighborhood performing inference. moreover technique achieved strongest results lstm -based neighborhood aggregator used. assumes existence consistent sequential node ordering across neighborhoods authors rectiﬁed consistently feeding randomly-ordered sequences lstm. technique suffer either issues—it works entirety neighborhood assume ordering within mentioned section reformulated particular instance monet speciﬁcally setting pseudo-coordinate function represent features node concatenation; weight function softmax) would make monet’s patch operator similar ours. nevertheless note that comparison previously considered monet instances model uses node features similarity computations rather node’s structural properties able produce version layer leverages sparse matrix operations reducing storage complexity linear number nodes edges enabling execution models larger graph datasets. however tensor manipulation framework used supports sparse matrix multiplication rank- tensors limits batching capabilities layer currently implemented appropriately addressing constraint important direction future work. depending regularity graph structure place gpus able offer major performance beneﬁts compared cpus sparse scenarios. also noted size receptive ﬁeld model upper-bounded depth network techniques skip connections could readily applied appropriately extending depth however. lastly parallelization across graph edges especially distributed manner involve redundant computation neighborhoods often highly overlap graphs interest. performed comparative evaluation models wide variety strong baselines previous approaches four established graph-based benchmark tasks achieving matching state-of-the-art performance across them. section summarizes experimental setup results brief qualitative analysis model’s extracted feature representations. transductive learning utilize three standard citation network benchmark datasets—cora citeseer pubmed —and closely follow transductive experimental setup yang datasets nodes correspond documents edges citations. node features correspond elements bag-of-words representation document. node class label. allow nodes class used training—however honoring transductive setup training algorithm access nodes’ feature vectors. predictive power trained models evaluated test nodes additional nodes validation purposes cora dataset contains nodes edges classes features node. citeseer dataset contains nodes edges classes features node. pubmed dataset contains nodes edges classes features node. inductive learning make protein-protein interaction dataset consists graphs corresponding different human tissues dataset contains graphs training validation testing. critically testing graphs remain completely unobserved training. construct graphs used preprocessed data provided hamilton average number nodes graph node features composed positional gene sets motif gene sets immunological signatures. labels node gene ontology collected molecular signatures database node possess several labels simultaneously. transductive learning transductive learning tasks compare strong baselines state-of-the-art approaches speciﬁed kipf welling includes label propagation semi-supervised embedding manifold regularization skip-gram based graph embeddings iterative classiﬁcation algorithm planetoid also directly compare model gcns well graph convolutional models utilising higher-order chebyshev ﬁlters monet model presented monti inductive learning inductive learning task compare four different supervised graphsage inductive methods presented hamilton provide variety approaches aggregating features within sampled neighborhood graphsage-gcn graphsage-mean graphsage-lstm graphsage-pool transductive approaches either completely inappropriate inductive setting assume nodes incrementally added single graph making unusable setup test graphs completely unseen training transductive learning transductive learning tasks apply two-layer model. architectural hyperparameters optimized cora dataset reused citeseer. ﬁrst layer consists attention heads computing features followed exponential linear unit nonlinearity. second layer used classiﬁcation single attention head computes features followed softmax activation. coping small training sizes regularization liberally applied within model. training apply regularization furthermore dropout applied layers’ inputs well normalized attention coefﬁcients similarly observed monti found pubmed’s training size required slight changes architecture applied output attention heads strengthened regularization otherwise architecture matches used cora citeseer. inductive learning inductive learning task apply three-layer model. ﬁrst layers consist attention heads computing features followed nonlinearity. ﬁnal layer used classiﬁcation attention heads computing features each averaged followed logistic sigmoid activation. training sets task sufﬁciently large found need apply regularization dropout—we have however successfully employed skip connections across intermediate attentional layer. utilize batch size graphs training. strictly evaluate beneﬁts applying attention mechanism setting also provide results constant attention mechanism used architecture—this assign weight every neighbor. models initialized using glorot initialization trained minimize cross-entropy training nodes using adam optimizer initial learning rate pubmed datasets. cases early stopping strategy cross-entropy loss accuracy micro-f score validation nodes patience epochs. transductive tasks report mean classiﬁcation accuracy test nodes method runs reuse metrics already reported kipf welling monti state-of-the-art techniques. speciﬁcally chebyshev ﬁlterbased approach provide maximum reported performance ﬁlters orders order fairly assess beneﬁts attention mechanism evaluate model computes hidden features attempting relu activation reporting better result runs inductive task report micro-averaged score nodes unseen test graphs averaged runs reuse metrics already reported hamilton table summary results terms micro-averaged scores dataset. graphsage∗ corresponds best graphsage result able obtain modifying architecture. const-gat corresponds model architecture constant attention mechanism techniques. speciﬁcally setup supervised compare supervised graphsage approaches. evaluate beneﬁts aggregating across entire neighborhood provide best result able achieve graphsage modifying architecture features computed layer features used aggregating neighborhoods). finally report -run result constant attention model fairly evaluate beneﬁts attention mechanism gcn-like aggregation scheme results successfully demonstrate state-of-the-art performance achieved matched across four datasets—in concordance expectations discussion section speciﬁcally able improve upon gcns margin cora citeseer respectively suggesting assigning different weights nodes neighborhood beneﬁcial. worth noting improvements achieved dataset model improves w.r.t. best graphsage result able obtain demonstrating model potential applied inductive settings larger predictive power leveraged observing entire neighborhood. furthermore improves w.r.t. const-gat directly demonstrating signiﬁcance able assign different weights different neighbors. effectiveness learned feature representations also investigated qualitatively—and purpose provide visualization t-sne -transformed feature representations extracted ﬁrst layer model pre-trained cora dataset representation exhibits discernible clustering projected space. note clusters correspond seven labels dataset verifying model’s discriminative power across seven topic classes cora. additionally visualize relative strengths normalized attention coefﬁcients properly interpreting coefﬁcients require domain knowledge dataset study left future work. presented graph attention networks novel convolution-style neural networks operate graph-structured data leveraging masked self-attentional layers. graph attentional layer utilized throughout networks computationally efﬁcient allows assigning different importances different nodes within neighborhood dealing different sized neighborhoods depend knowing entire graph structure upfront—thus addressing many theoretical issues previous spectral-based approaches. models leveraging attention successfully achieved matched state-of-the-art performance across four well-established node classiﬁcation benchmarks transductive inductive several potential improvements extensions graph attention networks could addressed future work overcoming practical problems described subsection able handle larger batch sizes. particularly interesting research direction would taking advantage attention mechanism perform thorough analysis model interpretability. moreover extending method perform graph classiﬁcation instead node classiﬁcation would also relevant application perspective. finally extending model incorporate edge features would allow tackle larger variety problems. figure t-sne plot computed feature representations pre-trained model’s ﬁrst hidden layer cora dataset. node colors denote classes. edge thickness indicates aggregated normalized attention coefﬁcients nodes across eight attention heads authors would like thank developers tensorflow received funding european union’s horizon research innovation programme propag-ageing grant agreement acknowledge support following agencies research funding computing support cifar canada research chairs compute canada calcul qu´ebec well nvidia generous support. special thanks benjamin fabian jansen kindly pointing issues previous iteration paper; michał dro˙zd˙zal useful discussions feedback support; ga´etan marceau reviewing paper prior submission. references mart´ın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg man´e rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda vi´egas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng. tensorflow large-scale machine learning heterogeneous systems https//www.tensorflow.org/. software available tensorﬂow.org. dzmitry bahdanau kyunghyun yoshua bengio. neural machine translation jointly learning align translate. international conference learning representations mikhail belkin partha niyogi vikas sindhwani. manifold regularization geometric framework learning labeled unlabeled examples. journal machine learning research joan bruna wojciech zaremba arthur szlam yann lecun. spectral networks locally connected networks graphs. international conference learning representations kyunghyun bart merri¨enboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. arxiv preprint arxiv. djork-arn´e clevert thomas unterthiner sepp hochreiter. fast accurate deep network learning exponential linear units international conference learning representations micha¨el defferrard xavier bresson pierre vandergheynst. convolutional neural networks graphs fast localized spectral ﬁltering. advances neural information processing systems david duvenaud dougal maclaurin jorge iparraguirre rafael bombarell timothy hirzel al´an aspuru-guzik ryan adams. convolutional networks graphs learning molecular ﬁngerprints. advances neural information processing systems jonas gehring michael auli david grangier yann dauphin. convolutional encoder model neural machine translation. corr abs/. http//arxiv. org/abs/.. xavier glorot yoshua bengio. understanding difﬁculty training deep feedforward neural networks. proceedings thirteenth international conference artiﬁcial intelligence statistics kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition guyon luxburg bengio wallach fergus vishwanathan garnett curran associates http//papers.nips.cc/paper/ -vain-attentional-multi-agent-predictive-modeling.pdf. simon j´egou michal drozdzal david v´azquez adriana romero yoshua bengio. hundred layers tiramisu fully convolutional densenets semantic segmentation. workshop computer vision vehicle technology cvprw federico monti davide boscaini jonathan masci emanuele rodol`a svoboda michael bronstein. geometric deep learning graphs manifolds using mixture model cnns. arxiv preprint arxiv. mathias niepert mohamed ahmed konstantin kutzkov. learning convolutional neural networks graphs. proceedings international conference machine learning volume adam santoro david raposo david barrett mateusz malinowski razvan pascanu peter battaglia timothy lillicrap. simple neural network module relational reasoning. arxiv preprint arxiv. nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research aravind subramanian pablo tamayo vamsi mootha sayan mukherjee benjamin ebert michael gillette amanda paulovich scott pomeroy todd golub eric lander gene enrichment analysis knowledge-based approach interpreting genome-wide expression proﬁles. proceedings national academy sciences ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez lukasz kaiser illia polosukhin. attention need. arxiv preprint arxiv. xiaojin zoubin ghahramani john lafferty. semi-supervised learning using gaussian ﬁelds harmonic functions. proceedings international conference machine learning", "year": 2017}