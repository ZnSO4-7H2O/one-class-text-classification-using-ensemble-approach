{"title": "Learning What Data to Learn", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \\emph{\\textbf{N}eural \\textbf{D}ata \\textbf{F}ilter} (\\textbf{NDF}), to explore automatic and adaptive data selection in the training process. In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks. Taking neural network training with stochastic gradient descent (SGD) as an example, comprehensive experiments with respect to various neural network modeling (e.g., multi-layer perceptron networks, convolutional neural networks and recurrent neural networks) and several applications (e.g., image classification and text understanding) demonstrate that NDF powered SGD can achieve comparable accuracy with standard SGD process by using less data and fewer iterations.", "text": "machine learning essentially sciences playing data. adaptive data selection strategy enabling dynamically choose different data various training stages reach effective model efﬁcient way. paper propose deep reinforcement learning framework call neural data filter explore automatic adaptive data selection training process. particular takes advantage deep neural network adaptively select ﬁlter important data instances sequential stream training data future accumulative reward maximized. contrast previous studies data selection mainly based heuristic strategies quite generic thus widely suitable many machine learning tasks. taking neural network training stochastic gradient descent example comprehensive experiments respect various neural network modeling several applications demonstrate powered achieve comparable accuracy standard process using less data fewer iterations. training data plays critical role machine learning. data selection strategy along training process could signiﬁcantly impact performance learned model. example appropriate strategy removing redundant data lead better model using less computational efforts. another example previous studies curriculum learning self-paced learning reveal principles tailoring data based ‘hardness’ favor model training; easy data instances important early model training later harder training examples tend effective improve model since easy ones bring minor changes. facts reveal feed training samples machine learning system nontrivial feeding data totally random order always good choice. explore better data selection strategy training previous works including curriculum learning self-paced learning adopt simple heuristic rules shufﬂing sequence length train language model abandoning training instances whose loss values larger human-deﬁned threshold human-deﬁned rules little restricted certain tasks cannot generalized broader learning scenarios since different learning tasks yield different optimal data selection rules even learning task need data various properties optimize different training stages. therefore remains open problem automatically dynamically allocate appropriate training data different stages machine learning? solution problem design twofold intuitive principles hand data selection strategy general enougg naturally applied different learning scenarios without particularly human-designed efforts; hand strategy forward-looking choice every step along training leads better long-term reward rather temporarily ﬁtting current stage. following principles propose data selection framework based deep reinforcement learning framework based data selection model acts teacher training process target model student teacher responsible providing student appropriate training data. teacher-student framework make possible models long-term reward along training process also well generalized machine learning scenarios since reinforcement learning approach model data selection mechanism parametric policy adaptive work ﬂexible state spaces cover signals used previous work automatically optimized end-to-end way. better elaborate proposal focus applying mini-batch stochastic gradient descent widely used optimize machine learning model training process. mini-batch sequential process mini-batches data {d··· arrive sequentially random order. mini-batch data arriving t-th time step consisting training instances. given loss gradient w.r.t. current model respec. tively denoted then mini-batch updates model follows assuming mini-batch proposed method aims dynamically determining instances mini-batch used training abandoned receiving training instances. speciﬁcally deep reinforcement learning determine whether/how ﬁlter given mini-batch training data call neural data filter illustrated figure training base machine learning model casted markov decision process state characterizing current state training process composed parts mini-batch arrived data current parameters trainee i.e. {dtwt}. time step receives representation current state outputs action specifying instances ﬁltered according policy afterwards remaining data determined used update trainee’s state generate reward conversely leveraged feedback updating policy. apply training various types neural networks including training data different domains including image text. experimental studies demonstrates faster convergence speed caused baselines. analysis shows welldesigned data selection policy beneﬁts deep neural network training fully explored community. automatically learnt data selection policy based reinforcement learning quite general surpassing human designed efforts task. rest paper organized follows. section context mini-batch algorithms introduce details neural data filter including language perform training data ﬁltration policy gradient algorithms learn ndf. section taking deep neural network training example empirically verify effectiveness ndf. discuss related work section conclude paper last section. section assuming training machine learning models mini-batch introduce mathematical details neural data filter summary aims ﬁlter certain amount training data within mini-batch high-quality training data remained better convergence speed training achieved. achieve that introduced section figure cast training markov decision process termed sgd-mdp. sgd-mdp traditional sgd-mdp composed tuple illustrated represents space actions. data ﬁltration task {am}m batch size denotes whether ﬁlter m-th data instance not. ﬁltered instances effects base model training. state transition probability determined factors uniform distribuwe consider data instances within mini-batch independent other therefore statement simplicity context clear used denote remain/ﬁlter decision single data instance i.e. similarly notation sometimes represent state training instance. whole process training listed algorithm particular take similar generalization framework proposed randomly sample subset training data train policy policy gradient method apply data ﬁltration model training process whole dataset detailed algorithm train policy introduced next subsection. input training data randomly sample subset training data optimize policy network based policy gradient apply full dataset train base machine learning model sgd. output base machine learning model. state-action value function parameterizes policy. since non-differentiable w.r.t. reinforce montecarlo policy gradient algorithm optimize quantity equation tion sequentially arrived training batch data; optimization process speciﬁed gradient descent principle randomness comes stochastic factors training dropout samples action policy function parameters learnt. policy binary classiﬁcation model logistic regression deep neural network. example sigmoid function feature vector effectively represent state discussed below. state features designing state feature vector effectively efﬁciently represent sgd-mdp state. since state includes arrived training data current base model state adopt three categories features compose data features contain information data instance label category length sentence linguistic features text segments gradients histogram features data features commonly used curriculum learning base model features include signals reﬂecting well current machine learning model trained. collect several simple features passed mini-batch number average historical training loss historical validation accuracy. proven effective enough represent current model status. features represent combination data model. using features target represent important arrived training data current model. mainly three parts signals classiﬁcation tasks predicted probabilities class; loss value data appears frequently self-paced learning algorithms advanced variants adadelta momentum-sgd perform base model training task. self-paced learning refers ﬁltering training data hardness reﬂected loss value. mathematically speaking training data satisfying ﬁltered threshold grows smaller larger training process. implementation improve robustness following widely used trick common implementation ﬁlter training data using loss rank mini-batch rather absolute loss value ﬁlter data instances largest training loss values within m-sized mini-batch linearly drops training. learnt shown algorithm state features constructed according principles described state features section experiments deﬁne reward following accuracy threshold episode record ﬁrst mini-batch index accuracy held-out exceeds reward log) pre-deﬁned maximum iteration number. note terminal reward exists. many ways deﬁne reward explore future work. three-layer neural network data ﬁltration policy function. weight values network uniformly initialized bias terms except bias last-layer initialized goal ﬁltering much data early age. adam leveraged optimize policy. policy achieves best terminal reward episodes applied ﬁnal policy full dataset reduce estimation variance moving average historical reward values previous episodes resampled reward time-step discount factor. reduce high variance gradient estimation equation variance reduction technique substracting reward baseline function details shown subsection sample data ﬁltration action data instance {d··· {am}m state corresponding update base machine learning model gradient descent based selected data receive reward computed section taking neural networks training example demonstrate improves sgd’s convergence performance large margin. experiments conducted three commonly used neural networks multi-layer perceptron convolutional neural networks recurrent neural networks image text classiﬁcation tasks. randdrop. conduct comprehensive comparison record ratio ﬁltered data instances epoch randomly ﬁlter data epoch according logged ratio. form baseline referred randdrop. strategies unﬁltered make sure base neural network model updated until un-trained selected data instances accumulated. guarantee updating neural network parameters batch sizes every strategy thus convergence speed determined quality selected data different model update frequencies since data ﬁltration within mini-batch lead smaller batch size. model implemented theano tesla training/testing process. data ﬁltration strategy every task report test accuracy respect number effective training instances. demonstrate robustness different hyper-parameters plot curve hyper-parameter conﬁguration. concretely speaking vary validation threshold reward computation. test different speeds embrace training data training process. speed characterized pre-deﬁned epoch number means training data gradually included among ﬁrst epochs. experimental curves reported average results repeated runs. ﬁrst test different data ﬁltration strategies multilayer perceptron network training image recognition task. dataset used mnist consists training testing images handwritten digits categories momentum-sgd mini-batch size used perform model training. figure test accuracy curves different data ﬁltration strategies mnist dataset. different hyper-parameter settings included validation accuracy threshold respectively conﬁgured randdrop uses ﬁltered data information output x-axis records number effective training instances. mnist dataset. tanh acts activation function hidden layer. subset contains randomly selected images whole training instances serves held-out validation dev. train policy network episodes control training episode early stopping based validation accuracy. leverages threelayer neural network model size policy network ﬁrst layer node number dimension state features tanh function activation function middle layer. accuracy curves different data ﬁltration strategies test plotted figure figure observe achieves best convergence speed signiﬁcantly better policies. particular achieve fairly good classiﬁcation accuracy uses much less training data without data selection mechanism select important data model training reﬂected curves scattered dots. better understand behaviors figure record number instances ﬁltered epoch curves denote number ﬁltered instances corresponding difference hardness levels measured rank loss values among data mini-batch. ﬁgure clearly observed data selection strategy quite different first training goes data ﬁltered opposite spl. second hard data tend selected training easy ones training sampled training data contains images among randomly selected images held-out provide reward signal. conﬁgurations training state features policy network structure optimization algorithm almost mnist experiments except train policy episodes since resnet training cifar computationally expensive. figure ﬁltered data numbers epoch mnist training. different curves denote number ﬁltered data corresponding different hardness levels indicated ranks loss ﬁltered data instance within minibatch. concretely speaking category rank values number training instances mini-batch buckets. bucket denotes hardest data instances whose loss values largest among mini-batch bucket easiest among batch smallest loss values. probably ﬁltered. believe result well demonstrates training mnist favors critical data brings fairly larger effects model training whereas less informative data instances smaller loss values comparatively redundant negligible. subsection conduct experiments larger vision dataset mnist powerful classiﬁcation model mlp. speciﬁcally cifar widely used dataset image classiﬁcation contains images size categorized classes. dataset partitioned training images test images. furthermore data augmentation applied every training image padding pixels side randomly sampling crop. resnet well-known effective model image recognition adopted perform classiﬁcation cifar-. based public lasagne implementation containing layers. mini-batch size momentum-sgd used optimization algorithm. following learning rate scheduling strategy original paper initial learning rate multiply factor k-th k-th model update. training test accuracy reaches figure test accuracy curves training resnet cifar- different data ﬁltration policies. hyper-parameter hyper-parameter randdrop uses ﬁltered data information output ndf.. figure records curves test accuracy varying number effective training data instances using different data ﬁltration strategies. again outperforms strategies indicated fact achieve classiﬁcation accuracy spends roughly half training data used without data selection policy addition performs almost data ﬁltered tiny gain training instances. however cannot catch ndf. similar figure plot number instances ﬁltered varying training epochs different hardness levels figure clearly observe similar data ﬁltration pattern mnist quite different since training data ﬁltered learning process hard data instances tend kept addition image recognition task also test data selection mechanisms text related tasks. basic setting using recurrent neural network imdb movie review conduct sentiment classiﬁcation. figure number instances ﬁltered epoch cifar training. similar figure separate ranks loss values buckets denote training data different hardness levels. dataset binary sentiment classiﬁcation dataset consisting movie review comments positive/negative sentiment labels evenly separated train/test set. sentences imdb dataset signiﬁcantly long average word token number frequent words selected dictionary others replaced special token unk. apply lstm sentence taking randomly initialized word embedding vectors input last hidden state lstm logistic regression classiﬁer predict sentiment label size word embedding size hidden state mini-batch size adadelta used perform lstm model training. test accuracy roughly reproducing result previous work training training data randomly sample learn data ﬁltration policy. episode number early stop validation used control training process episode. conﬁgurations repeat used policy network training mnist. detailed results shown figure ﬁgure following observations first significantly boosts convergence training lstm. much less data achieves satisfactory classiﬁcation accuracy. example achieve test accuracy needs training data plain adadelta second signiﬁcantly outperforms randdrop baseline demonstrating effectiveness learnt policies. last self-paced learning helps initialization lstm. figure test accuracy curves different data ﬁltration strategies imdb sentiment classiﬁcation dataset. hyperparameter hyper-parameter randdrop uses ﬁltered data information output however seems help training middle phase. using reinforcement learning achieves better long-term convergence faster initialization although initialization effective spl. better understand advantages brought ndfreinfofce figure also give curves recording number ﬁltered data instances epoch. observe data selection pattern learnt signiﬁcantly different mnist cifar. particularly learnt data selection mechanism similar since hard data larger loss values probably ﬁltered early lstm training gradually included model update along training process. shown better initialization brought compared data selection training easy data early phase helps accelerating lstm initialization identiﬁed difﬁcult problem lstm training long sequences point view necessary start learning easy data. model grown mature state stronger cahowever global point view figure training lstm imdb data favors easier data. except aforementioned difﬁculty lstm training conjecture another reason behavior compared image data natural language contains noise residing sentences labels. data large loss values might imply high noise levels eliminated model training. good data selection mechanism effectively accelerate model convergence. example data selection policy learnt help training various neural networks achieve fairly good performances signiﬁcantly smaller number training data without data section. different tasks datasets favor different data selection policies indicated figures sense heuristic data selection rule cannot cover different scenarios. contrast acts adaptive successfully handle diverse scenarios. covers information state features indicate importance data instance obtains target data selection policy based reinforcement learning. furthermore sensitive setting hyper-parameters. policies trained wide range values lead satisfactory performances. typically works shallow model training involve frequent model updated. however training deep models provide good data selection mechanism simple heuristic rule. plenty previous works talk data scheduling strategies machine learning. remarkable example curriculum learning showing data order easy instances hard ones a.k.a. curriculum beneﬁts learning process. measure hardness typically determined heuristic understandings data comparison self-paced learning quantiﬁes hardness loss data. training instances loss values larger threshold neglected gradually increases training process ﬁnally training instances play effects. apparently viewed data ﬁltration strategy considered paper. recently revival deep neural networks researchers noticed importance data scheduling deep learning. example simple batch selection strategy based loss values training data proposed speeding neural network training. leverages bayesian optimization optimize curriculum function training distributed word representations. sachan xing investigate several hand-crafted criteria data ordering solving question answering tasks based dnn. computer vision hard example mining approach tailored training object detection network proposed work differs signiﬁcantly works ﬁlter data randomly arrived mini-batches training process save computational efforts rather actively select mini-batch feedforward process untrained data quite computationally expensive; leverage reinforcement learning automatically derive optimal data selection policy according feedback training process rather naive heuristic rules task. latter limited timeconsuming show example complicated rules accelerate mnist training fail cifar. sense belongs category meta learning a.k.a. learning learn proposed neural data filter data ﬁltration based deep reinforcement learning applies deep neural networks reinforcement learning particular belongs policy based reinforcement learning seeking search directly optimal control policy. reinforce actorcritic representative policy gradient algorithms difference actor-critic adopts value function approximation reduce high variance policy gradient estimator reinforce. paper introduced neural data filter reinforcement learning framework adaptively perform training data selection machine learning. experiments several deep neural networks training mini-batch demonstrated boosts convergence training process. hand shown reinforcement learning based adaptive approach effective general various machine learning tasks; hand would like inspire community explore data selection/scheduling machine learning especially training deep neural networks. future work ﬁrst goal provide efﬁcient training example optimality tightening actor-critic collect frequent reward signals learning pure scratch eliminate feed-forward step current design obtain state features. apply reinforcement learning based teacher-student framework strategy design problems machine learning hyper-parameter tuning structure learning distributed scheduling hope providing better guidance controlled training process. references andrychowicz marcin denil misha gomez sergio hoffman matthew pfau david schaul freitas nando. learning learn gradient descent gradient descent. arxiv preprint arxiv. bengio yoshua louradour j´erˆome collobert ronan proceedings weston jason. curriculum learning. annual international conference machine learning dalal navneet triggs bill. histograms oriented gradients human detection. computer vision pattern recognition cvpr ieee computer society conference volume ieee frank yang schwing alexander peng jian. learning play faster deep reinforcement learning optimality tightening. proceedings international conference learning representations conference track jiang meng deyu mitamura teruko hauptmann alexander easy samples ﬁrst self-paced reranking zero-example multimedia search. proceedings international conference multimedia jiang meng deyu shoou-i zhenzhong shan shiguang hauptmann alexander. self-paced learning diversity. advances neural information processing systems yong grauman kristen. learning easy things ﬁrst self-paced visual category discovery. computer vision pattern recognition ieee conference ieee loshchilov ilya hutter frank. online batch selection proceedings faster training neural networks. international conference learning representations workshop track maas andrew daly raymond pham peter huang andrew potts christopher. prolearning word vectors sentiment analysis. ceedings annual meeting association computational linguistics human language technologies june mnih volodymyr kavukcuoglu koray silver david graves alex antonoglou ioannis wierstra daan riedmiller martin. playing atari deep reinforcement learning. arxiv preprint arxiv. tsvetkov yulia faruqui manaal ling wang macwhinney brian dyer chris. learning curriculum bayesian optimization task-speciﬁc word repreproceedings annual sentation learning. meeting association computational linguistics august wang yiren tian fei. recurrent residual learning sequence classiﬁcation. proceedings conference emnlp association computational linguistics november weaver nigel. optimal reward baseline gradient-based reinforcement learning. proceedings seventeenth conference uncertainty artiﬁcial intelligence morgan kaufmann publishers inc. mnih volodymyr badia adria puigdomenech mirza mehdi graves alex lillicrap timothy harley silver david kavukcuoglu koray. asynchronous methods deep reinforcement learning. arxiv preprint arxiv. sachan mrinmaya xing eric. easy questions ﬁrst? case study curriculum learning question answering. proceedings annual meeting association computational linguistics august schmidhuber jurgen. evolutionary principles selfreferential learning. learning learn meta-meta-... hook.) diploma thesis institut informatik tech. univ. munich shrivastava abhinav gupta abhinav girshick ross. training region-based object detectors online hard example mining. proceedings ieee conference computer vision pattern recognition silver david huang maddison chris guez arthur sifre laurent driessche george schrittwieser julian antonoglou ioannis panneershelvam veda lanctot marc mastering game deep neural networks tree search. nature spitkovsky valentin alshawi hiyan jurafsky daniel. baby steps leapfrog less unsupervised dependency parsing. annual conference north american chapter association computational linguistics srivastava nitish hinton geoffrey krizhevsky alex sutskever ilya salakhutdinov ruslan. dropout simple prevent neural networks overﬁtjournal machine learning research ting. sutskever ilya martens james dahl george hinton geoffrey. importance initialization momentum deep learning. dasgupta sanjoy mcallester david proceedings international conference machine learning", "year": 2017}