{"title": "AOSO-LogitBoost: Adaptive One-Vs-One LogitBoost for Multi-Class Problem", "tag": ["stat.ML", "cs.AI", "cs.CV"], "abstract": "This paper presents an improvement to model learning when using multi-class LogitBoost for classification. Motivated by the statistical view, LogitBoost can be seen as additive tree regression. Two important factors in this setting are: 1) coupled classifier output due to a sum-to-zero constraint, and 2) the dense Hessian matrices that arise when computing tree node split gain and node value fittings. In general, this setting is too complicated for a tractable model learning algorithm. However, too aggressive simplification of the setting may lead to degraded performance. For example, the original LogitBoost is outperformed by ABC-LogitBoost due to the latter's more careful treatment of the above two factors.  In this paper we propose techniques to address the two main difficulties of the LogitBoost setting: 1) we adopt a vector tree (i.e. each node value is vector) that enforces a sum-to-zero constraint, and 2) we use an adaptive block coordinate descent that exploits the dense Hessian when computing tree split gain and node values. Higher classification accuracy and faster convergence rates are observed for a range of public data sets when compared to both the original and the ABC-LogitBoost implementations.", "text": "paper presents improvement model learning using multi-class logitboost classiﬁcation. motivated statistical view logitboost seen additive tree regression. important factors setting coupled classiﬁer output sum-to-zero constraint dense hessian matrices arise computing tree node split gain node value ﬁttings. general setting complicated tractable model learning algorithm. however aggressive simpliﬁcation setting lead degraded performance. example original logitboost outperformed abc-logitboost latter’s careful treatment factors. paper propose techniques address main diﬃculties logitboost setting adopt vector tree enforces sum-to-zero constraint adaptive block coordinate descent exploits dense hessian computing tree split gain node values. higher classiﬁcation accuracy faster convergence rates observed range public data sets compared original abc-logitboost implementations. boosting successful binary multi-class classiﬁcation among popular variants particularly focusing logitboost paper. originally logitboost motivated statistical view boosting algorithms consists three components loss function model optimization algorithm. case logitboost logit loss additive tree models stage-wise optimization respectively. important factors logitboost setting. firstly posterior class probability estimate must normalised order logit loss. leads coupled classiﬁer output i.e. sum-to-zero classiﬁer output. secondly dense hessian matrix arises deriving tree node split gain node value ﬁtting. challenging design tractable optimization algorithm fully handles factors. consequently simpliﬁcation and/or approximation needed. friedman proposes scalar regression tree class strategy. breaks coupling classiﬁer output boosting iteration model updating collapses independent regression tree ﬁttings denotes number classes. sum-to-zero constraint dropped hessian approximated diagonally. unfortunately friedman’s prescription turns drawbacks. later improvement abclogitboost shown outperform logitboost terms classiﬁcation accuracy converrest paper organised follows section ﬁrst formulate problem setting logitboost give details approach. section compare approach logitboost. section experimental results terms classiﬁcation errors convergence rates reported range public datasets. begin basic setting logitboost algorithm. k-class classiﬁcation consider example training yi}n denotes feature value denotes class label. class probabilities conditioned denoted learned training set. test example known unknown predict class label using bayes rule maxk make optimization feasible model needed describe depends example linear model used traditional logit regression generalized additive model adopted logitboost figure newly added tree boosting iteration -class problem. class pair selected tree node. internal node pair computing split gain; terminal nodes node vector updating. feature space partitioned tree regions region coordinates updated based corresponding class pair shown gence rate abclogitboost’s careful handling problems logitboost setting. iteration sum-to-zero constraint enforced scalar trees ﬁtted classes. remaining class called base class selected adaptively iteration hence acronym also hessian matrix approximated reﬁned manner original logitboost computing tree split gain ﬁtting node value. paper propose novel techniques address challenging aspects logitboost setting. approach vector tree added iteration. allow dimensional sum-to-zero vector ﬁtted tree node. permits explicitly formulate computation node split gain node value ﬁtting dimensional constrained quadratic optimization arises subproblem inner loop split seeking ﬁtting tree. avoid diﬃculty dense hessian propose subproblems coordinates adaptively selected updating hence name aoso logitboost. figure gives overview approach. section show ﬁrst order second order approximation loss reduction good measure quality selected class pair. following formulation abc-logitboost although derived somewhat diﬀerent framework thus shown special case aoso-logitboost less ﬂexible tree model. section compare diﬀerences approaches detail provide intuition second point explicitly give node split gain. suppose internal node training examples feature re-index examples according sorted feature values. need index maximizes node gain deﬁned loss reduction division n′-th examples minimizers index sets respectively. generally searching applies features. best division resulting largest recorded perform actual node split. note arises context outer loop number features. however na¨ıve summing losses incurs additional factor complexity ﬁnally results unacceptable complexity single boosting iteration. workaround newton descent method gradient hessian incrementally computed. respectively gradient vector hessian matrix dropping constant odeloss irrelevant taylor expansion w.r.t. second order typically represented scalar regression trees real adaboost.mh implementation single vector tree paper adopt single vector tree. restrict binary tree split must vertical coordinate axis formally solving tree model equivalent determining parameters rj}j m-th iteration. subsection show problem reduces solving collection convex optimization subproblems numerical method. following friedman’s logitboost settings newton descent. also show gradient hessian computed incrementally. properties shown indicate singular unconstrained newton descent applicable here rank could high prohibits application standard fast quadratic solver designed rank hessians. following propose address problem block coordinate descent technique successfully used training svms variable choose coordinates i.e. class pair update keeping others ﬁxed. suppose chosen s-th coordinate free variables plugging yields unconstrained dimensional quadratic problem regards scalar variable diagonal matrix diag. compute approximated node loss thanks additive form incrementally/decrementally computed constant time split searching proceeds training example next. therefore computation eliminates complexity na¨ıve summing losses. minimise give properties taken account seeking solver. begin sum-to-zero constraint. probability estimate logit loss must non-zero sum-to-one ensured link function link turn means unchanged adding arbitrary constant component result single example loss invariant moving along all- vector real adaboost.mh second order approximation necessary special form exponential loss absence sum-to-zero constraint exists analytical solution node loss simply setting derivative also computation incremental/decremental. since loss design adaboost.mh main interests discuss further. methods selecting proposed. based ﬁrst order approximation. free variables rest ﬁxed suﬃciently small ﬁxed length small enough constant. ﬁrst order approximation so-called base class selected exhaustive search iteration i.e. trying possible involves growing trees. reduce time complexity also proposed methods. selected every several iterations intuitively class leads largest loss reduction last iteration. abc-logitboost sum-to-zero constraint explicitly considered deriving node value node split gain scalar regression tree. indeed paper although derived using slightly diﬀerent motivation. sense abc-logitboost seen special form aoso-logitboost since tree class pair ﬁxed every node selected adaptively aoso trees added iteration tree added iteration aoso since changes made abc-logitboost immediate question happens make one? happens vector tree added iteration single class pair selected root node shared tree nodes {pi}n updated soon tree added aoso. tried unfortunately degraded performance observed combination results reported here. datasets pokerk pokerk pokerk pokerk pokerkt pokerkt covertypek covertypek letter letterk letterk letterk pendigits zipcode isolet optdigits mnistk m-basic m-image m-rand m-noise m-noise m-noise m-noise m-noise m-noise subscript emphasize k-th tree built independently trees although simpliﬁes mathematics aggressive approximation turns harm classiﬁcation accuracy convergence rate shown li’s experiments section compare aoso-logitboost abc-logitboost shown outperform original logitboost li’s experiments test aoso datasets used listed table section datasets bottom mnist datasets many variations detailed descriptions). exhaust learning ability logitboost boosting stop either training converges approaches implemented maximum number reached. test errors last iteration simply reported since obvious over-ﬁtting observed. default large datasets adopt criteria except maximum iterations maoso mabc number classes. note tree added iteration aoso added abc. thus correction compares maximum number trees aoso abc. important tuning parameters logitboost number terminal nodes shrinkage factor reported results logitboost number combinations. report corresponding results aosologitboost combinations. following intend show nearly combinations aoso-logitboost lower classiﬁcation error faster convergence rates abc-logitboost. table summary test classiﬁcation errors. lower bold. middle panel except pokerkt pokerkt chosen validation right panel overall best. dash means unavailable relative improvements -values given. datasets pokerk pokerk pokerk pokerk pokerkt pokerkt covertypek covertypek letter letterk letterk letterk pendigits zipcode isolet optdigits mnistk m-basic m-rotate m-image m-rand m-noise m-noise m-noise m-noise m-noise m-noise pokerkt. therefore summarized classiﬁcation errors simply except pokerkt pokerkt errors reported using other’s test validation set. based criteria summarize aoso middle panel table test errors well improvement relative given. right panel table provide comparison best results achieved combinations corresponding results available also tested statistical signiﬁcance aoso abc. assume classiﬁcation error rate subject binomial distribution. denote number errors number tests estimate error rate variance ˆp/n. subsequently approximate binomial distribution gaussian distribution perform hypothesis test. p-values reported table aoso) outperforms state-of-the-art classiﬁer deep learning. m-image dbn- aoso). paper’s extended version details. shows aoso’s improvement deserve eﬀorts. recall stop boosting procedure either maximum number iterations reached converges fewer trees added boosting stops faster convergence lower time cost either training test-", "year": 2011}