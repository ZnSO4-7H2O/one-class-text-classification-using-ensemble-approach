{"title": "Accelerating Hessian-free optimization for deep neural networks by  implicit preconditioning and sampling", "tag": ["cs.LG", "cs.CL", "cs.NE", "math.OC", "stat.ML", "65K05, 90C15, 90C90"], "abstract": "Hessian-free training has become a popular parallel second or- der optimization technique for Deep Neural Network training. This study aims at speeding up Hessian-free training, both by means of decreasing the amount of data used for training, as well as through reduction of the number of Krylov subspace solver iterations used for implicit estimation of the Hessian. In this paper, we develop an L-BFGS based preconditioning scheme that avoids the need to access the Hessian explicitly. Since L-BFGS cannot be regarded as a fixed-point iteration, we further propose the employment of flexible Krylov subspace solvers that retain the desired theoretical convergence guarantees of their conventional counterparts. Second, we propose a new sampling algorithm, which geometrically increases the amount of data utilized for gradient and Krylov subspace iteration calculations. On a 50-hr English Broadcast News task, we find that these methodologies provide roughly a 1.5x speed-up, whereas, on a 300-hr Switchboard task, these techniques provide over a 2.3x speedup, with no loss in WER. These results suggest that even further speed-up is expected, as problems scale and complexity grows.", "text": "space iterations required solution approximate hessian within iteration secondly proposes using ﬁxed amount data iterations gradient krylov subspace iteration computations. purpose research explore algorithmic strategies reduction amount time spent gradient krylov subspace computations reducing amount data needed training well reducing number krylov subspace iterations. paper exploit speciﬁc instance krylov subspace solvers consumed symmetric positive deﬁnite matrices known conjugate gradient solvers. simplicity term conjugate gradient speciﬁc krylov subspace technique used estimate hessian. however algorithms propose reducing training time generic work ﬂexible krylov subspace solver variant. preconditioning context linear algebra refers process transforming system equations solved readily example preconditioning extensively used reduce iterations obtaining appropriate preconditioner given problem challenging. first type preconditioner works best problem speciﬁc. second principle possible design preconditioning strategies reduce computational burden consequent solution phase radically computational investment attaining preconditioner might offset beneﬁt. thus critical identify proper balance computational efforts invested preconditioning invested consequent solution phase. optimization problem computationally intractable construct hessian explicitly. quasi-newton approaches construct approximation hessian limited memory versions form approximations implicitly. work propose using quasi-newton l-bfgs method preconditioner solver. rationale quasi-newton approaches exploit underlying structure linear system postulated structural assumptions complementary. therefore combination methods typically effective dependence upon solely. reason l-bfgs used directly optimization dnns l-bfgs crudely approximates curvature matrix whereas method makes implicitly available exact curvature matrix allows identiﬁcation directions extremely curvature. l-bfgs preconditioning suggested before numerical simulations. extend upon work demonstrate l-bfgs serves effective preconditioner cg-based training dnns large-scale speech recognition data. furthermore unlike used typical ﬁxed approach make important observation non-ﬁxed point preconditioners proposed l-bfgs cannot used stahessian-free training become popular parallel second order optimization technique deep neural network training. study aims speeding hessian-free training means decreasing amount data used training well reduction number krylov subspace solver iterations used implicit estimation hessian. paper develop l-bfgs based preconditioning scheme avoids need access hessian explicitly. since l-bfgs cannot regarded ﬁxed-point iteration propose employment ﬂexible krylov subspace solvers retain desired theoretical convergence guarantees conventional counterparts. second propose sampling algorithm geometrically increases amount data utilized gradient krylov subspace iteration calculations. english broadcast news task methodologies provide roughly speed-up whereas switchboard task techniques provide speedup loss wer. results suggest even further speed-up expected problems scale complexity grows. second order optimization techniques extensively explored problems involving pathological curvature deep neural network training problems. fact demonstrated success second order technique known hessian-free optimization dnns various image recognition tasks. addition successfully applied optimization technique dnns speech recognition tasks. second order methods including l-bfgs krylov subspace descent also shown great success training. second order methods particularly important sequencetraining dnns provides relative improvement cross-entropy trained sequence training must information time-sequential lattices corresponding utterances sequence training performed using utterance randomization rather frame randomization. mini-batch stochastic gradient descent often used training frame randomization performs better utterance randomization however sequence-training must accomplished utterance level second order methods perform much better second order methods compute gradient large batch utterances compared utterance mini-batch research employ optimization techniques sequence training drawbacks method training slow requiring weeks training hour switchboard task using parallel machines. reasons training slow. firstly great number krylov subblely standard iterative schemes thus ensure stable predictable convergence propose ﬂexible variants methods variants avoid failures breakdowns standard counterparts susceptible second introduce sampling strategy amount data used gradient calculations gradually increased. optimization problems gradient-based methods typically operate within popular regimes stochastic approximation methods select small sample size estimate gradient. methods often decrease objective function loss quickly initial training iterations albeit during later iterations movement objective function slow. spectrum sample approximation techniques compute gradient large sample data. computation expensive gradient estimates much reliable objective function progresses well later training iterations. study propose hybrid method captures beneﬁts stochastic sample approximation methods increasing amount sampled data used gradient calculations. sampling amount data used gradient calculations explored observed variance batch gradient determine amount data gradient calculations. alternatively explored geometrically increasing amount data used logistic regression conditional random ﬁeld problems. beneﬁt approach schedule selecting data given ahead time need compute expensive gradient variance. paper extend idea training compare sampling approach initial experiments conducted english broadcast news task preconditioning allows speedup reducing number iterations. furthermore gradient sampling provide roughly additional improvement training time. total combining sampling preconditioning speedup ideas able reduce overall training time factor second extend preconditioning sampling ideas larger switchboard task proposed techniques provide speedup loss accuracy. pseudo-code algorithm closely follows gradients computed training data. gauss-newton matrix-vector products computed sample taken time cg-minimize called. loss computed held-out set. cg-minimize uses minimize starting search direction function returns series steps used line search procedure. parameter update based armijo rule backtracking line search. distributed computation computer gradients curvature matrix-vector products done using master/worker architecture problems technique used algorithms used obtain approximate solution hessian require many iterations. figure indicates training iterations increase training time iteration fact dominated iterations. section discuss reduce number iterations using preconditioning. nd-order optimization techniques require computation hessian order determine search direction form formulation hessian approximation gradient objective function iteration. aforementioned method used solve search direction. speciﬁcally gaussnewton matrix solve hkdk −gk. mentioned above principle l-bfgs used optimization training problem. reason l-bfgs used optimization neural networks practice l-bfgs crudely approximates curvature systems whereas describing speedups made hessian-free algorithm brieﬂy summarize algorithm training described denote network parameters denote loss function denote gradient loss respect parameters denote search direction denote matrix characterizing curvature loss around central idea optimization iteratively form quadratic approximation loss minimize approximation using krylov subspace methods conjugate gradient access curvature matrix implicitly matrix-vector products form products computed efﬁciently neural networks algorithm search truncated based upon relative improvement approximate loss. curvature matrix often chosen gauss-newton matrix positive deﬁnite. avoid breakdown negative curvature positive deﬁnite approximation enforced converts system favorable structure. preconditioning makes problem easier solve reduces number iterations. deﬁne preconditioner preconditioned involves following transformation problem dk). preconditioner required symm− explored using diaogonal elements fisher information matrix preconditioner training dnns. using diagonal matrix elements limited ability precondition system mainly beneﬁcial matrix suffers scaling issues. addition explored using jacobi pre-conditioner computed batch data like curvature-vector products thus requiring master/worker data-parallelization architecture. speciﬁc speech problem found jacobi preconditioner costly compute offset reductions iterations. l-bfgs preconditioner propose powerful compared diagonal matrix preconditioners improves spectral properties system rather merely tackling potential scaling issues. furthermore require data parallelization. l-bfgs preconditioner described follows. iteration produces sequence iterates sequence residuals using statistics vectors stored iterations speciﬁed user. statistics saved l-bfgs matrix deﬁned using steps algorithm l-bfgs matrix used preconditioner variety different methodologies choose statistics estimating l-bfgs matrix. adopt strategy proposed namely using vectors evenly distributed throughout estimate l-bfgs matrix. implies preconditioner changes different iterations. requirement preconditioner needs ﬁxed iterations inconvenient since obtain l-bfgs statistics improve estimate preconditioner. changing preconditioner requires using ﬂexible approach speciﬁcally instead using equivalent fletcher-reeves updating formula non-preconditioned polak-ribi`ere variant required opposed approach taken ﬂexible approach. computation search direction computationally excessive requiring great number iterations. thus quasi-newton methods preconditioning implicit systems sensible structural assumptions l-bfgs complementary. section below describe l-bfgs algorithm detail using preconditioner ﬂexible l-bfgs quasi-netwton optimization method uses limited memory technique approximate hessian inverse. specifically instead computing hessian directly often large dense matrix l-bfgs algorithm stores small number vectors used rank approximation hessian. l-bfgs algorithm outlined algorithm iterative methods used solve search direction precondiminimizing following problem tioning typically involves process transformation applied upon system equations return another problem technique used gradient computed using data ﬁxed data sample. section explore reducing amount data used gradient computations. speciﬁcally explore hybrid technique ﬁrst starts small amount data similar stochastic approximation methods gradually increases amount sampled data similar sample approximation methods. following section detail different hybrid methods. bounded bounds decreasing. fact links sampling errors directly expected rate convergence. approach require computing statistics along sampling strategy used select linked directly expected convergence rate uses geometrically increasing sample size. adopt strategy gradient iteration samples iteration. speciﬁcally given initial sample size sample size iteration given equation geometric factor tuned development set. initial experiments conducted english broadcast news task results reported ears devf set. recipe outlined extract acoustic features. hybrid trained using speaker-adapted vtln+fmllr features input context frames around current frame. observed -layer hidden units layer sixth softmax layer output targets appropriate architecture tasks. explore behavior preconditioning sampling training smaller task ﬁrst moving larger switchboard task. timing experiments study core intel xeon x.ghz cpu. matrix/vector operations training multi-threaded using intel mkl-blas. machines exclusively reserved training reliable training time estimates. section compare preconditioning preconditioning preconditioning explore behavior different number statistics used estimate l-bfgs preconditioned namely table shows total time spent total number training iterations achieve loss. addition figure provides closer look cumulative time methods. figure indicates preconditioning methods require less time particularly number total iterations increases manifests signiﬁcant reduction time iterations also results loss moving much slower method explained increased iterations table appears cost-efﬁcient choice given task terms iteration runtime terms loss reduction roughly faster baseline method. proposes method increase sample size based variance estimates obtained computation batch gradient. algorithm described follows. denote output true output loss predicted true values deﬁned yi). loss training size deﬁned losses individual training examples shown equation denoting gradients full subset losses respectively algorithm ensures descent made every iteration must admit descent direction true objective function expressed equation paper explore sampling approach within framework. given input utterance output gradients training frames utterance i.e. therefore compute variance gradient estimate requires passes utterance compute since makes algorithm computationally expensive compute average gradient utterance i.e. variance statistics become sum-squared utterances training shown equation requires pass network utterance. sampling approach proposed uses sampling statistics approximate descent condition need estimate variance adds notable computational complexity gradient computation. contrast framework discussed provides expected guarantee descent iteration long sampling errors geometric sampling geometric factor tuned held-out gradient found gradient allowed best tradeoff reduction amount training data used training time. geometric factor corresponds seeing roughly total data used gradient calculations roughly total training iterations completed. variance sampling equation tuned smaller favors larger sample size. figure shows percentage data accessed gradient geometric variance methods iteration three different values notice variance methods access training data beginning relative geometric method. reason beginning training little data available reliable variance estimate larger sample size preferred. variance method provided best tradeoff training time data accessed. similar also used estimating amount data used figure shows cumulative time gradient calculation iteration full gradient/cg sampling approaches sampling approaches tuned provide best tradeoff training time amount data accessed. geometric method quicker variance sampling method particularly accesses less data early training iterations shown figure overall geometric method provides roughly reduction training time. possible technique starts geometric sampling switches variance sampling enough data obtained reliable variance estimate might provide speedups. trade-off loss overall training time baseline method preconditioning including gradient sampling. overall pc+gradient+cg sampling offers fastest training time compared baseline. table shows training time corresponding baseline speedup methods. training time reduced hours hours roughly speedup loss accuracy. analyzing behavior preconditioning sampling speedups smaller -hour broadcast news task section explore training speed improvements larger -hour switchboard task. explore dnns performance hours conversational american english telephony data switchboard corpus. development done hub’ testing done report performance separately switchboard fisher portions set. similar training features speaker-adapted using vtln fmllr techniques. input features -frame context around current frame. similar hidden layers containing sigmoidal units output targets. results without speedups reported sequence training. performance baseline speedup techniques shown table since using l-bfgs stats performed well smaller -hour task used switchboard task preconditioning. addition increased amount training data associated larger task found using smaller sample size gradient iteration calculations still allowed appropriate estimate statistics. since parallel machines compared possible exclusively reserve machines timing calculations. therefore training time estimated calculating total number accessed data points training correlated timing. table shows total accessed data points baseline speedup techniques. notice larger dataset able decrease fraction data used gradient conjugate gradient calculations achieve even larger speedup baseline loss accuracy. suggests even speedups possible data size grows. paper explored using l-bfgs pre-conditioner geometric sampling approach accelerate training. approaches combined provided roughly speedup broadcast news task speedup switchboard task loss accuracy. anticipate even larger speedup attained informed selection quasi-newton horesh schweiger s.r. arridge d.s. holder large-scale non-linear reconstruction algorithms electrical impedance tomography human head world congress medical physics biomedical engineering magjarevic j.h. nagel eds. vol. ifmbe proceedings springer berlin heidelberg kingsbury sainath soltau scalable minimum bayes risk training deep neural network acoustic models using distributed hessian-free optimization proc. interspeech barrett berry chan demmel donato dongarra eijkhout pozo romine vorst templates solution linear systems building blocks iterative methods edition siam philadelphia", "year": 2013}