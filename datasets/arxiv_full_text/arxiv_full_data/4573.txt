{"title": "Inducing Probabilistic Programs by Bayesian Program Merging", "tag": ["cs.AI", "cs.LG"], "abstract": "This report outlines an approach to learning generative models from data. We express models as probabilistic programs, which allows us to capture abstract patterns within the examples. By choosing our language for programs to be an extension of the algebraic data type of the examples, we can begin with a program that generates all and only the examples. We then introduce greater abstraction, and hence generalization, incrementally to the extent that it improves the posterior probability of the examples given the program. Motivated by previous approaches to model merging and program induction, we search for such explanatory abstractions using program transformations. We consider two types of transformation: Abstraction merges common subexpressions within a program into new functions (a form of anti-unification). Deargumentation simplifies functions by reducing the number of arguments. We demonstrate that this approach finds key patterns in the domain of nested lists, including parameterized sub-functions and stochastic recursion.", "text": "report outlines approach learning generative models data. express models probabilistic programs allows capture abstract patterns within examples. choosing language programs extension algebraic data type examples begin program generates examples. introduce greater abstraction hence generalization incrementally extent improves posterior probability examples given program. motivated previous approaches model merging program induction search explanatory abstractions using program transformations. consider types transformation abstraction merges common subexpressions within program functions deargumentation simpliﬁes functions reducing number arguments. demonstrate approach ﬁnds patterns domain nested lists including parameterized sub-functions stochastic recursion. abstraction anti-uniﬁcation refactoring programs uniﬁcation summary deargumentation compactly representing noisy data merging similar variables inducing recursive functions inducing noisy data constructors deargumentation program induction patterns look ﬁgure might describe image series trees tree large brown base number green branches variable length branch ending ﬂower either yellow red. recognizing patterns important aspect intelligence human machine. approach pattern recognition problem learning generative models observed examples. wish description process gave rise examples form description rich enough language capture abstract patterns—in report probabilistic programming language. build representation explore family algorithms learning probabilistic programs data. generative models play prominent role modern machine learning wide variety applications. trade-oﬀ variety patterns model class able capture feasibility learning models class much machine learning focused studying classes models limited expressiveness order develop tractable algorithms modeling large data sets. investigation takes diﬀerent approach explores learning might proceed expressive class models focus identifying abstract patterns small amounts data. represent generative models programs probabilistic programming language. probabilistic program represents probability distribution evaluation program results sample distribution. implement programs subset probabilistic programming language church programs parameterized functions recursion allow natural representation long-range dependencies recursive patterns. frame searching space models terms bayesian model merging demonstrate approach interesting patterns simple domain colored trees. main components approach follows represent data algebraic data type generative models probabilistic programming language extends data type guide search program space using bayesian posterior probability. algorithm begins transforming data large program generates examples explores program space identifying repeated structure large program transforming program make sharing explicit; search moves formalized family useful program transformations. probabilistic programs learned manner understood generative models reasoning models formulated terms probabilistic inference. illustrate ideas colored trees proceed note report report status update working system containing detailed code illustrative examples. documents progress made believe useful generally. report completed academic work. particular little situate work within context previous work provides little high-level discussion aims illustrative examples rather compelling applications. bayesian model merging framework searching space generative models order model accurately generates observed data. main idea search model space training set—it severely overﬁts initial data. therefore generate alternative model hypotheses using program transformations collapse model structure often result better generalizations. technique successfully applied learning artiﬁcial probabilistic grammars represented extend bayesian model merging models expressed rich class probabilistic programs. allows represent complex patterns beyond context free grammars wide range transformations including transformations result lossy compression input data. remainder document describe following parts required implement bayesian program merging data representation program language represent data terms algebraic data type gives form initial programs using type constructors. language programs consists type constructors lambda abstraction additional operators. language probabilistic hence programs correspond distributions observations. program structure corresponds regularities observations. search objective posterior probability probabilistic programs objective search program space posterior probability program given observed data. posterior decomposes prior based program length likelihood estimate using selective model averaging. search moves program transformations describe types program transformation search moves abstraction deargumentation. transformations collapse program structure often increases prior probability program improves generalization unobserved data. assume represent data using algebraic data type. assumption gives starting point program induction since data directly translated program derivation data type speciﬁcation. figure shows expressions langauge together corresponding visual representations. representing data rudimentary programs. order capture interesting patterns need expressive language. following subset church bayesian program merging tree domain ﬁrst step bayesian program merging data incorporation. data incorporation creation initial model going example training creating expression evaluates example combine programs single expression draws uniformly list. implementation data unannotated s-expressions easily converted expressions terms tree type constructors ))). report assume readability data tree expression form. important question leave future work perform data incorporation data less structured example given terms feature vectors. calling incorporate-data program shown ﬁgure results following program generative model joint probability distribution latent states observed data. represent joint distribution program probabilistic programming language like church structure program i.e. decomposition functions control capture regularities data. illustrate idea using probabilistic program generates images ﬁgure diﬀerent parts program correspond diﬀerent patterns described earlier speciﬁed grammar). programs combination data constructors control operations lambda abstractions. call program shown ﬁrst determines number branches tree creates large body node connects branches. branch function recursively connects series small nodes together ends call ﬂower function passing colors. ﬂower function creates node three identically colored petal nodes. program structure reﬂects patterns ﬂower three petals body large base. compositionality language captures relational patterns fact branches ﬂowers. given programs represent structures goal transform initial program generated data incorporation structured form. program transformations generate functions. call functions abstractions. represent using name argument variables pattern i.e. s-expression makes body function. following code snippet function creates readable unique symbols function variable names. wrap programs another data type keep track additional information search. motivate section search detail. basic idea avoid recomputation program’s likelihood transformations aﬀect program’s semantics. probabilistic programs correspond probability distributions observed data i.e. compute likelihood given observation program. given process generating programs bayes theorem compute posterior probability program prior biases search towards smaller programs. increasing constant gives prior weight calculating posterior means minimizing program size important criterion. program’s size number symbols function bodies well main body. computing likelihood diﬃcult part posterior probability computation. intuitively think likelihood tracking good particular program producing target data. important search since gives precise quantitative information whether adjust hypothesis program. however since large number possible settings random choices program make determining choices lead observed data diﬃcult. furthermore given data point could multiple settings generate data point; correctly compute likelihood need take account them. often cannot compute quantity exactly limited computational resources. following describe stochastic approximation computation list data. fact likelihood estimation procedure stochastic means ﬁnal model learned implementation bayesian program merging diﬀer even runs input data. case programs generate list-structured data estimate likelihood generating samples using sequential monte carlo method generates discrete structure examples extending sample forcing generate continuous parameters applying selective model averaging samples. begin factoring problem generating data problem generating data points computation result target tree time. product probabilities random choices made single evaluation corresponds probability single possible generating tree. since multiple ways program generate given tree probability distinct parse compute lower bound true likelihood take advantage fact directly compute probability sample gaussian given parameters. therefore modify gaussian functions program output mean variance particular node instead sampling value distribution. code also changes uniform-choice syntactic construct uniform-draw current church implementations provide. code below smc-core forces program generate desired data. detailed description smc-core beyond scope report. short method incremental forward sampler separate continuous choices since forward sampling would probability generating observed real values. start program constructed data incorporation want propose changes posterior probability improves. achieve using program transformations isolate patterns compress program thereby increasing prior probability. following describe transformations abstraction deargumentation. abstraction aims create functions based syntactic patterns program replace patterns calls newly created functions. removes duplication code acts proxy recognizing repeated computation. terms bayesian model merging transformation merges structure model potentially leads models better generalization properties also interpret process ﬁnding partial symmetries work bokeloh successful domain inverse-procedural modeling. following code fragment implements procedure. function compressions ﬁnds abstractions formed anti-unifying pairs subexpressions condensed form program ﬁlter duplicate abstractions create compressed programs replacing occurrences abstraction bodies original program figure expression represented tree. anti-uniﬁcation recursively ﬁnds common subtrees syntax trees. matching tree itself three partial matches transformation refactors program subexpressions partially match program function contains common parts matching subexpressions body. example shown above subexpressions partially match common subexpression function created using common subexpression original subexpressions replaced exists abstraction transformation pair subexpressions partial match. case following pairs subexpressions partial match process ﬁnding partial match expressions called anti-uniﬁcation. understand process terms syntax trees expressions. every s-expression tree lists sublists s-expression make interior nodes primitive elements lists leaves. tree ﬁgure corresponds expression partial match expressions ﬁnding common subtree tree representations. using lambda abstractions created partial matches subexpressions program attempt refactor program possibly compressing given abstraction take abstraction body replace subexpressions match pattern function call. apply replacement functions well body insert deﬁnition abstraction body generate refactored program replace pattern matches abstraction expression recursively matching body using uniﬁcation. match exists return function call match return matches replaced subexpressions. non-matching primitive return illustrate refactoring using expression partial match resulting anti-uniﬁcation subexpressions refactor original expression terms creating function replacing occurrences body original expression. example replacement another general apply function wherever possible refactored program problem determining whether match expression abstraction known uniﬁcation described anti-uniﬁcation process creating abstraction given pair expressions. uniﬁcation opposite process. return value successful uniﬁcation list assignments arguments applied arguments results uniﬁcation algorithm recursively checks whether body lists size. uniﬁcation returns list containing uniﬁcation subexpressions. size list uniﬁcation returns false. expressions primitives uniﬁcation returns true equal false otherwise. case function expression uniﬁcation variable return assignment i.e. variable along expression passed uniﬁcation. variable repeatedly occurs assigned diﬀerent values diﬀerent places also return false. otherwise uniﬁcation succeeds return assignment unique variable function equivalent addition function numbers therefore refactoring program terms function compress original expression. example refactoring compress expression expression pair abstraction program transformation identiﬁes repeated computation program ﬁnding syntactic patterns. program generated using data incorporation syntactic patterns directly correspond patterns observed data. formalization notion pattern seem limited ﬁrst worth contemplating central role lambda abstraction lambda calculus expressiveness language. abstraction process steps first create abstractions common subexpressions program using anti-uniﬁcation. second compress program using abstractions replacing instances abstractions function calls uniﬁcation. important note given program usually many possible abstraction transformations corresponding diﬀerent repeated patterns. illustrate abstraction using tree example ﬁrst used section data incorporation program anti-uniﬁcation ﬁnds possible abstractions. examples show abstraction results best compression abstraction results smallest program second abstraction corresponds ﬂower-like pattern. pattern could capture even structure replacing variables petal colors ﬁxed value since similar. instead explaining data drawn multiple gaussians slightly diﬀerent means could explain data generated single gaussian. second program transformation deargumentation addresses issue. deargumentation program transformation takes function program changes deﬁnition removing function arguments. wherever removed argument used within instead independent sample constant distribution. value depends values original argument overall program; depending argument values replacements create diﬀerent program transformations. following demonstrate transformation useful compactly representing continuous values distorted noise identifying replicated arguments inducing recursive structure. know system modeling noisy want treat expressions essentially identical i.e. unifying instead. achieve eﬀect using deargumentation transform replacement-function called noisy-number-replacement. function replaces variable within abstraction mean values instances. apply deargumentation transform flower function variable identify instances going replace variable mean abstraction flower changed using remove-abstraction-variable applications flower exist within program bigger impact simpliﬁcation. transform aﬀect likelihood data program. since search based posterior probability incorporating prior likelihood trade improvements model complexity data principled way. applied deargumentation transform flower argument used values less similar would program gives lower likelihood data program generated removing figure replacing arguments stochastic choices. argument takes similar values instantiations replacing stochastic choice result samples almost indistinguishable original program values dissimilar using gaussian centered mean values result samples look unlike samples original program abstraction creates functions whose arguments represent expression parts diﬀer speciﬁc abstraction instances. arguments take similar values instantiations might underlying generative process. take advantage potential redundancy merging arguments. achieved using deargumentation transform following replacement-function figure merging arguments. arguments take similar values instantiations merging argument results samples almost indistinguishable original program instantiations dissimilar samples look unlike samples original program variables take similar values applications consider same. above deﬁne items similar list structure numbers. still consider objects similar even numbers diﬀerent values allow resulting likelihood ﬁlter models explain data. detect matching variables ﬁrst choose abstraction choose variable abstraction check whether variables abstraction matching instance values. variable matches return deﬁnition variable removed. illustrate deargumentation transform discovers compactly represents recursive patterns. replacement-function transform recursion-replacement redeﬁnes variable random choice recursive call non-recursive call. redeﬁnition useful whenever argument values passed function calls itself. probability recursion depends many argument instances fact calls function. every program generated terminate probability one. approach inducing recursive programs limited view interesting illustration recursion identiﬁed captured. remove argument using deargumentation remove-abstraction-variable replacement function. changes deﬁnition drawn uniform distribution instances former argument following demonstrate noisy data constructors make possible represent datasets much compactly. describe instance deargumentation transform therefore infer constructors show example process. consider observations shown ﬁgure without noisy data constructors—i.e. gaussian inside calls color—we would represent observations using program observations representative generative process want model paid large penalty terms model size small gain explanatory power. noisy data constructors provide similar gain much lower cost program size previously used noisy data constructors part incorporate-data. show gain compactness noisy constructors allows infer them. make minor adjustments incorporate-data noisy-number-replacement. first change node-data->expression helper function used incorporate-data automatically call gaussian. instead constructs deterministic nodes second move creation noisy data constructors deargumentation replacement function noisy-number-replacement. instead returning sample mean function returns call gaussian uses sample mean variance parameters generate observations example inducing noisy data constructors following program sample trees. tree consists three nodes small variance color third node. figure shows observations property. bayesian program merging using search moves described compresses program representation function three-node structure. function generates value third node makes stochastic noise process. discussion hints representational beneﬁts probabilistic data constructors probabilistic programs must postpone complete treatment implications design decision future work. deargumentation solves following problem given values function argument takes course running overall generative process want determine whether compact stochastic program generate values—if case remove argument deﬁnition compact program within function. phrased terms clear problem addressed deargumentation problem program induction. rephrase deargumentation moves presented induction steps limited particular sets programs case noisy-number deargumentation induce programs generate occurring number values restricting search programs corresponding one-dimensional gaussian distributions. case identical variable deargumentation assume generative process values variable identical other hence induce program variable simply referring ﬁrst one. search program space interested programs high posterior probability. many diﬀerent search strategies possible report limit beam search using posterior probability search heuristic. main part search performed depth-iter-transformations recursively applies program transformations best programs given search depth ﬁlters results best programs next depth. reduce amount computation required choosing best programs level search separating program transformations preserve semantics not. marking programs based transformation type allows reuse previously computed likelihoods programs created semantics preserving transformation. illustrate program induction method using small examples drawn domain colored trees. examples following shape present probabilistic program show observations sampled program highlight patterns visible original program observed trees show patterns bayesian program merging recovers. results representative running bayesian program merging examples detailed results given diﬀer stochastic nature likelihood estimation. create initial program represents ﬂowers using data incorporation compress learn function takes arguments creates ﬂower petals similar color. system parameters beam width depth size observations generated original recursive program reaches length threshold trade-oﬀ prior likelihood favors recursive programs small size. adjust threshold occurs using size constant prior. example bayesian program merging induce model vine identical ﬂowers. demonstrates multiple types program transformations applying data. system parameters beam width depth example demonstrates learning parameterized function recursion applying functions multiple places within program. functions shown generate ﬂowerlike patterns tree consists branches ends ﬂower diﬀerent color. system beam width depth setting results trade-oﬀ prior likelihood make introducing recursion worthwhile. expect system could learn similar program provide training examples branch instances since would increase amount program compression. bayesian model merging produces program structure similar original generating program. plays similar role flower function taking single color argument creating three nodes size passed color. function creates branch ends ﬂower either brown petals. presented bayesian program merging approach inducing generative models data. central idea approach ﬁrst translate data program without abstractions compress program identifying repeated computations. perform compression using program transformations goal maximizing posterior probability program given data. many possible improvements system described paper present themselves. include sophisticated search strategies eﬃcient ways computing likelihood sophisticated robust methods program transformation. furthermore considerations section suggest systematic development general framework possible uniting many individual search moves proposed. many barriers overcome probabilistic program induction compete state-ofthe-art machine learning algorithms real world problems. potential capturing rich patterns limited dependence human engineering makes worthy pursuit.", "year": 2011}