{"title": "Short-term Memory of Deep RNN", "tag": ["cs.LG", "cs.AI", "math.DS", "stat.ML"], "abstract": "The extension of deep learning towards temporal data processing is gaining an increasing research interest. In this paper we investigate the properties of state dynamics developed in successive levels of deep recurrent neural networks (RNNs) in terms of short-term memory abilities. Our results reveal interesting insights that shed light on the nature of layering as a factor of RNN design. Noticeably, higher layers in a hierarchically organized RNN architecture results to be inherently biased towards longer memory spans even prior to training of the recurrent connections. Moreover, in the context of Reservoir Computing framework, our analysis also points out the benefit of a layered recurrent organization as an efficient approach to improve the memory skills of reservoir models.", "text": "abstract. extension deep learning towards temporal data processing gaining increasing research interest. paper investigate properties state dynamics developed successive levels deep recurrent neural networks terms short-term memory abilities. results reveal interesting insights shed light nature layering factor design. noticeably higher layers hierarchically organized architecture results inherently biased towards longer memory spans even prior training recurrent connections. moreover context reservoir computing framework analysis also points beneﬁt layered recurrent organization eﬃcient approach improve memory skills reservoir models. deep learning attractive area research constant growth particular neuro-computing ﬁeld study deep neural networks composed multiple non-linear layers proved able learn feature representations progressively higher levels abstraction leading eminent performance e.g. vision tasks. extending beneﬁts depth recurrent neural networks intriguing research direction recently gaining increasing attention context study deep rnns pointed hierarchically organized recurrent models potentiality developing multiple time-scales representations input history internal states great help e.g. dealing text processing tasks recently studies area reservoir computing shown ability developing structured state space organization indeed intrinsic property layered architectures study deep networks hand allowed development eﬃciently trained deep models learning temporal domain hand paved studies properties deep rnns dynamics even absence learning recurrent connections. aspect prominent relevance study dynamical models represented analysis memory abilities. paper exploiting ground provided deep framework explicitly address problem analyzing short-term memory capacity individual layers deep recurrent architectures. contributing highlight intrinsic diversiﬁcation transient state dynamics hierarchically constructed recurrent networks investigation aims shedding light bias layering architectural design. framed area analysis also intended provide insights process reservoir network construction. consider deep rnns whose recurrent architecture obtained stacked composition multiple non-linear recurrent hidden layers illustrated fig. state computation proceeds following hierarchical network organization lowest layer highest one. speciﬁcally time step ﬁrst recurrent layer network external input successive layer activation previous one. dynamical system perspective deep implements inputdriven discrete-time non-linear dynamical system state evolution layer ruled state transition function denote input dimension assume sake simplicity hidden following layer contains recurrent units. respectively indicate external input state i-th hidden layer time step based notation state ﬁrst layer updated according following equation rnr×nr collects weights inter-layer connections layer layer rnr×nr recurrent weight matrix layer note tanh non-linearity used element-wise applied activation function recurrent units bias terms omitted ease notation. also worth observing that although deep recurrent dynamics globally evolve whole locally layer state information coming previous level actually acts independent input information encodes history external input present time step. recently introduced deep framework according recurrent part deep architecture left untrained initialization subject stability constraints speciﬁcally network initialized weights uniform distribution re-scaled control layer values denotes spectral radius matrix argument quantities hyper-parameters model inﬂuence state dynamics typically small values order guarantee stable regime standard initialization approach also trained networks. notice framework allows hand investigate ﬁxed characterization state dynamics successive levels deep network hand study bias layering deep recurrent architectures. output computation implemented using output layer size though diﬀerent choices possible state-output connection settings following analysis aims consider output modules individually applied layer recurrent network. enables study separately characteristics state behavior emerging diﬀerent levels architecture. linear output modules outx matrices layer output computed trained layer individually using direct method framework setting also ensures investigate short-term memory abilities deep architectures resorting memory capacity task aims measuring extent past input events recalled present state activations. speciﬁcally recurrent system tested ability reconstruct delayed versions stationary uni-variate driving input signal layer computed squared correlation coeﬃcient follows activation output unit trained reconstruct signal state layer respectively denote covariance variance operators. order maximally exercise memory capability systems used i.i.d. input signals uniform distribution i.e. unstructured temporal stream carry information previous inputs task considered time-step long sequence ﬁrst time steps used training remaining assessment. considered deep rnns recurrent layers containing recurrent units. input inter-layer weights rescaled weights recurrent connections re-scaled spectral radius layers i.e. values ranging note that considered experimental settings higher values network dynamics tend exhibit chaotic behavior shown previous works terms local lyapunov exponents although recurrent dynamics chaotic regimes generally interesting practical point view paper consider also cases scope analysis. choice independently generated networks realizations averaged achieved results realizations. practical assessment values useful recall basic theoretical result provided states nr-dimensional recurrent system driven i.i.d. uni-variate input signal upper bounded accordingly considered maximum value delay equal twice size state space i.e. suﬃcient account correlations practically involved experimental settings. fig. shows values achieved correspondence progressively higher layers architecture diﬀerent cases considered network initialization. results clearly point recurrent networks ordered regime higher layers deep architecture naturally biased toward progressively longer short-term memory abilities. networks chaotic regime higher layers tend show poorer performance shown fig. peak correspondence case score improves layer layer. interestingly results also highlight eﬀectiveness layering striking advantage convenient process networks architectural design. memory nr-dimensional reservoir indeed easily improved using underlying stack recurrent layers ﬁlter external input signal. note improvement comes price modestly increased cost state computation cost output training remains same. inquire memory structure developed layers deep rnns analyzing values increasing delays. fig. shows forgetting curves individual layers i.e. values function obtained case plot fig. clearly reveals diversiﬁcation memory spans components deep recurrent architecture higher layers able store information past inputs longer times. layer memory recall almost null delay dynamics developed layer lead value zero even delay also input signals smaller delays better reconstructed lower layers higher layers characterized peak tends shift right slope forgetting curve tends increasingly smoother. besides highlighted diversiﬁcation short-term memory spans among successive layers deep architecture also interesting characterizing intrinsic richness state representations globally developed deep recurrent system. paper provided computational analysis short-term memory deep rnns. resorted task mean quantify memory state dynamics successive levels deep recurrent system. results clearly showed higher layers hierarchically organized architecture inherently featured even prior learning recurrent connections improved ability latch input information longer time spans. analysis provided paper also revealed interesting insights diversiﬁcation memory structure developed within deep stacked dynamics showing higher layers tend forget past input history slowly smoothly compared lower ones. furthermore framed within deep framework results provided evidence support practical beneﬁt layered recurrent organization improve memory skills reservoir networks cost-eﬀective fashion. overall though studies research direction certainly demanded believe outcomes provided paper contribute better understand characterize bias layering deep recurrent neural models.", "year": 2018}