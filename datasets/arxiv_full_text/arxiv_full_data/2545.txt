{"title": "Auxiliary Deep Generative Models", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Deep generative models parameterized by neural networks have recently achieved state-of-the-art performance in unsupervised and semi-supervised learning. We extend deep generative models with auxiliary variables which improves the variational approximation. The auxiliary variables leave the generative model unchanged but make the variational distribution more expressive. Inspired by the structure of the auxiliary variable we also propose a model with two stochastic layers and skip connections. Our findings suggest that more expressive and properly specified deep generative models converge faster with better results. We show state-of-the-art performance within semi-supervised learning on MNIST, SVHN and NORB datasets.", "text": "lars maaløe casper kaae sønderby søren kaae sønderby winther department applied mathematics computer science technical university denmark bioinformatics centre department biology university copenhagen deep generative models parameterized neural networks recently achieved state-ofthe-art performance unsupervised semisupervised learning. extend deep generative models auxiliary variables improves variational approximation. auxiliary variables leave generative model unchanged make variational distribution expressive. inspired structure auxiliary variable also propose model stochastic layers skip connections. ﬁndings suggest expressive properly speciﬁed deep generative models converge faster better results. show state-of-theart performance within semi-supervised learning mnist svhn norb datasets. stochastic backpropagation deep neural networks approximate bayesian inference made deep generative models practical large scale problems typically assume mean ﬁeld latent distribution latent variables independent. assumption might result models incapable capturing dependencies data. paper show deep generative models expressive variational distributions easier optimize better performance. increase ﬂexibility model introducing auxiliary variables allowing complex latent distributions. demonstrate beneﬁts increased ﬂexibility achieving state-of-the-art performance semisupervised setting mnist recently signiﬁcant improvements within semi-supervised classiﬁcation tasks. kingma introduced deep generative model semisupervised learning modeling joint distribution data labels. model difﬁcult train end-to-end layer stochastic latent variables coupled pretrained feature extractor performs well. lately ladder network virtual adversarial training improved performance end-toend training. paper train deep generative models multiple stochastic layers. auxiliary deep generative models utilize extra auxiliary latent variables increase ﬂexibility variational distribution also introduce slight change adgm -layered stochastic model skip connections skip deep generative model models trainable end-to-end offer state-of-the-art performance compared semisupervised methods. paper ﬁrst introduce data demonstrate that recently kingma rezende coupled approach variational inference deep learning giving rise powerful probabilistic models constructed inference neural network generative neural network approach perceived variational equivalent deep auto-encoder acts encoder decoder. however difference models ensures efﬁcient inference continuous distributions latent space automatic relevance determination regularization kl-divergence. furthermore gradients variational upper bound easily deﬁned backpropagation network. keep computational requirements variational distribution usually chosen diagonal gaussian limiting expressive power inference model. paper propose variational auxiliary variable approach improve variational distribution generative model extended variables original model invariant marginalization variational distribution hand used marginal hierarchical speciﬁcation allows latent variables correlated maintaining computational efﬁciency fully factorized models sec. demonstrate expressive power inference model ﬁtting complex multimodal distribution. variational auto-encoder recently introduced powerful method unsupervised learning. latent variable generative model data parameterized deep neural network parameters parameters inferred maximizing variauvae inference model parameterized second deep neural network. inference generative parameters jointly trained optimizing stochastic gradient ascent reparameterization trick backpropagation gaussian latent variables figure probabilistic graphical model adgm semisupervised learning. incoming joint connections variable deep neural networks parameters auxiliary variables propose extend variational distribution auxiliary variables marginal distribution complicated posteriors order unchanged generative model required joint mode gives back original marginalization thus auxiliary variables used algorithm gibbs sampling previously considered variational learning agakov barber concurrent work ranganath proposed make parameters variational distribution stochastic leads similar model. important note order fall back original model require agakov barber app. auxiliary lower bound becomes main focus paper auxiliary approach build semi-supervised models learn classiﬁers labeled unlabeled data. encompass class information introduce extra latent variable generative model deﬁned pppθpθ weight generative discriminative learning. parameter nl+nu scaling constant number labeled data points number unlabeled data points. objective function labeled unlabeled data kingma proposed model stochastic layers unable make converge endto-end instead resorted layer-wise training. preliminary analysis also found model pθpθpp failed converge trained end-to-end. auxiliary model made two-layered stochastic model simply reversing arrow fig. would expect auxiliary model works well terms convergence performance two-layered model pθpθpp work even better ﬂexible generative model. variational distribution unchanged qφqφqφ. call skip deep generative model test alongside auxiliary model benchmarks sdgm adgm parameterized neural networks auxiliary inference model latent inference model classiﬁcation model generative model generative model neural networks consists fully connected hidden layers denoting output layer hidden layers rectiﬁed linear activation functions. compute approximations stochastic variables place independent output layers forward-pass propagating input neural network auxiliary variable class label latent features respectively. multinomial distribution treated latent variable unlabeled data points. study experimented categorical labels however method applies distributions latent variable categorical gaussian discrete continuous observations deep neural networks parameters inference model deﬁned qφqφqφ diag)) order model gaussian distributions deﬁne separate outputs deterministic layer deep neural network µφ∨θ φ∨θ. outputs able approximate expectations applying reparameterization trick. point adgm auxiliary unit introduce latent feature extractor inference model giving richer mapping classiﬁer compute probabilities unlabeled data part class retrieve cross-entropy error estimate labeled data used cohesion variational lower bound deﬁne good objective function order train model end-to-end. benchmark experiments parameterized deep neural networks fully connected hidden layers. pair hidden layers size generative model pppθpθ adgm sdgm augmented unchanged inference models bined extra resulting data points. test size trained small norb dataset consisting training samples equal amount test samples distributed across classes animal human plane truck car. normalized norb images following miyato using image pairs resulting vectorized input labeled subsets consisted evenly distributed labeled samples. batch size svhn norb half batch labeled samples. avoid phenomenon modeling discretized values real-valued estimation added uniform noise pixel value. normalized norb dataset svhn dataset standard deviation color channel. datasets assumed gaussian distributed generative models section present examples shed light auxiliary variables improve distribution thereafter investigate unsupervised generative log-likelihood performance followed semi-supervised classiﬁcation performance several benchmark datasets. demonstrate state-of-the-art performance show adding auxiliary variables increase classiﬁcation performance convergence speed beyond gaussian latent distributions variational auto-encoders inference model parameterized fully factorized gaussian. demonstrate auxiliary model complicated posterior distributions latent space. consider potential model exp)/z leads bound fig. shows true posterior fig. shows density plot samples trained adgm. similar ﬁndings rezende mohamed demonstrate using normalizing ﬂows complicated posterior distributions. frequent solution found optimization shown equivalent modes. mode solution identical values bound expected simpler single mode solution easier infer. parameters initialized using glorot bengio scheme. expectation variables performed monte carlo sampling using reparameterization trick average exact enumeration mnist dataset combined training examples validation examples. test remained used batch size half batch always labeled samples. labeled data chosen randomly distributed evenly across classes. speed training removed columns standard deviation resulting input size epoch normalized mnist images binarized sampling bernoulli distribution mean parameter pixel intensities. figure true posterior prior approximation qφqφ adgm. prediction half-moon data epochs labeled data points class. plot principal component corresponding auxiliary latent space. table unsupervised test log-likelihood permutation invariant mnist normalizing ﬂows importance weighted auto-encoder variational gaussian process ladder denoting ﬁnetuning procedure sønderby importance weighted samples training number stochastic latent layers evaluate negative log-likelihood layered avae. found warm-up crucial activation auxiliary variables. table shows log-likelihood scores permutation invariant mnist dataset. methods directly comparable except ladder since training performed differently. however give good indication expressive power auxiliary variable model. avae performing better normalizing ﬂows importance weighted auto-encoder sample results also comparable ladder latent layers variational gaussian process shown burda sønderby increasing samples annealing learning rate likely increase log-likelihood. exemplify power semisupervised learning generated synthetic dataset consisting half-moons sin)) sin) added gaussian noise. training contains samples divided batches labeled data points class test contains samples. good semi-supervised model able learn data manifold half-moons together limited labeled information build classiﬁer. adgm converges close classiﬁcation error epochs much faster equivalent model without auxiliary variable converges epochs. investigating auxiliary variable ﬁnds discriminating internal representation data manifold thereby aids classiﬁer table semi-supervised test error benchmarks mnist svhn norb randomly labeled evenly distributed data points. lower section demonstrates benchmarks contribution article. table shows performance adgm sdgm mnist dataset. adgm’s convergence around fast point convergence speed declines ﬁnally reaching sdgm shows close similar performance proves stable speeding convergence advanced generative model. achieved best results mnist performing multiple monte carlo samples good explorative estimate models ability comprehend data manifold words close posterior distribution possible evaluate generative model. fig. show sdgm trained labeled data points learned separate style class information. shows random samples generative model. figure mnist analogies. forward propagating data point generate samples class label generating sample class label random generated gaussian noise; hence inference model. fig. demonstrate information contribution stochastic unit sdgm measured average test kl-divergence variational distribution prior. units little information content close prior distribution kl-divergence term thus close number clearly activated units quite tail slightly active units similar results reported burda still evident information ﬂowing variables though. fig. shows clustering auxiliary space adgm sdgm respectively. figure sdgm trained labeled mnist. kldivergence units latent variables calculated difference approximated value prior. principal component auxiliary latent space. adgm sdgm powerful deep generative models relatively simple neural network architectures. trainable end-to-end since follow principles variational inference multiple improvements consider optimizing models like using importance weighted bound adding layers stochastic variables. furthermore proposed models using gaussian latent distribution model easily extended distributions approaching stability issues adgm training gaussian input distributions temperature weighting discriminative stochastic learning kl-divergence estimating variational lower bound similar problems gaussian input distributions oord restrict dataset ordinal values order apply softmax function output generative model discretization data also possible solution. another potential stabilizer batch normalization ensure normalization output batch fully connected hidden layer. downside semi-supervised variational framework summing classes order evaluate variational bound unlabeled data. computationally costly operation number classes grow. sense ladder network advantage. possible extension sample calculating unlabeled lower bound result gradients high variance. framework implemented fully connected layers. vaes proven work well convolutional layers could promising step improve performance. finally since expect variational bound found auxiliary variable method quite tight could interest whether bound used classiﬁcation bayes classiﬁer manner introduced novel framework making variational distributions used deep generative models expressive. examples benchmarks investigated framework uses auxiliary variables learn better variational approximations. finally demonstrated framework gives state-of-the-art performance number semi-supervised benchmarks trainable end-to-end. order investigate whether stochasticity auxiliary variable network depth essential models performance constructed adgm deterministic auxiliary variable. furthermore also implemented model kingma using exact hyperparameters learning adgm. fig. shows adgm outperforms model adgm deterministic auxiliary variables. found convergence model highly unstable; shown best obtained. table sdgm outperforms relative reduction error rate svhn dataset. also tested model performance omitted svhn extra training. achieved classiﬁcation error improvements norb dataset signiﬁcant svhn adgm slightly worse sdgm slightly better vat. svhn model trains around classiﬁcation error epochs followed decline convergence speed. norb dataset signiﬁcantly smaller dataset sdgm converges around epochs. also trained norb dataset single images opposed image pairs achieved classiﬁcation error around epochs. appendix study theoretical optimum auxiliary variational bound found functional derivatives variational objective. practice resort restricted deep network parameterized distributions. analysis nevertheless shed light properties optimum. without loss generality consider auxiliary latent results extended full semi-supervised setting without changing overall conclusion. variational bound auxiliary model solution optimization respect simply δ-function value optimizes variational bound z-model. fall back model without auxiliary also noted agakov barber tested uninformed auxiliary model semisupervised learning benchmarks competitive results mnist benchmarks. attribute factors semi-supervised learning additional classiﬁcation cost generic form objective thank durk kingma shakir mohamed helpful discussions. research supported novo nordisk foundation danish innovation foundation nvidia corporation donation titan tesla gpus. references agakov barber auxiliary variational method. neural information processing volume lecture notes computer science pages springer berlin heidelberg. bastien lamblin pascanu bergstra goodfellow bergeron bouchard bengio theano features speed improvements. deep learning unsupervised feature learning workshop neural information processing systems. glorot bengio understanding difﬁculty training deep feedforward neural networks. proceedings international conference artiﬁcial intelligence statistics pages ioffe szegedy batch normalization accelerating deep network training reducing internal covariate shift. proceedings international conference machine learning pages lecun bottou bengio haffner gradient-based learning applied document recognition. proceedings ieee computer society conference computer vision pattern recognition pages lecun huang bottou learning methods generic object recognition invariance pose lighting. proceedings ieee computer society conference computer vision pattern recognition pages netzer wang coates bissacco reading digits natural images deep learning unsupervised feature learning. unsupervised feature learning workshop neural information processing systems", "year": 2016}