{"title": "Learning Undirected Graphical Models with Structure Penalty", "tag": ["cs.AI", "cs.LG"], "abstract": "In undirected graphical models, learning the graph structure and learning the functions that relate the predictive variables (features) to the responses given the structure are two topics that have been widely investigated in machine learning and statistics. Learning graphical models in two stages will have problems because graph structure may change after considering the features. The main contribution of this paper is the proposed method that learns the graph structure and functions on the graph at the same time. General graphical models with binary outcomes conditioned on predictive variables are proved to be equivalent to multivariate Bernoulli model. The reparameterization of the potential functions in graphical model by conditional log odds ratios in multivariate Bernoulli model offers advantage in the representation of the conditional independence structure in the model. Additionally, we impose a structure penalty on groups of conditional log odds ratios to learn the graph structure. These groups of functions are designed with overlaps to enforce hierarchical function selection. In this way, we are able to shrink higher order interactions to obtain a sparse graph structure. Simulation studies show that the method is able to recover the graph structure. The analysis of county data from Census Bureau gives interesting relations between unemployment rate, crime and others discovered by the model.", "text": "undirected graphical models learning graph structure learning functions relate predictive variables responses given structure topics widely investigated machine learning statistics. learning graphical models stages problems graph structure change considering features. main contribution paper proposed method learns graph structure functions graph time. general graphical models binary outcomes conditioned predictive variables proved equivalent multivariate bernoulli model. reparameterization potential functions graphical model conditional odds ratios multivariate bernoulli model oﬀers advantage representation conditional independence structure model. additionally impose structure penalty groups conditional odds ratios learn graph structure. groups functions designed overlaps enforce hierarchical function selection. able shrink higher order interactions obtain sparse graph structure. simulation studies show method able recover graph structure. analysis county data census bureau gives interesting relations unemployment rate crime others discovered model. undirected graphical models graph deﬁned nodes links nodes. fact associated multivariate response variables speciﬁes conditional independence structure among them. example link nodes indicates pairwise interaction clique three nodes indicates third order interaction ijk. functions formulate eﬀects predicative variables responses interactions. graphical models used many applications. conditional random fields extensions e.g. dynamic well known natural language processing community. crfs achieve great success sequentially structured text modeling interaction labels nodes conditioned ﬂexibly features also numerous applications graphical intuition utilizing graph structure responses related others not. however many cases graph pre-determined domain knowledge. example duan proposed collective model labeling music signals fully connected graph called collective conditional random ﬁelds. semantic categories genre instrument production rhythm etc. possible links necessary e.g. production instrument. estimating parameters relations lead over-ﬁtting. therefore graph structure learning important aspect relation discovery multivariate response applications multi-task learning. many papers focused graphical model selection issue. meinshausen buhlmann peng studied sparse covariance estimation gaussian outcomes without input features. covariance matrix determines dependence structure gaussian distribution sparsity speciﬁes linkage gaussian markov random fields. case non-elliptical distribution distribution discrete random variables. ravikumar focused graph structure selection ising model based l-regularized logistic regression. gave suﬃcient conditions consistently estimating neighborhood nodes withinput features. however marginally independent response variables become dependent conditioning ignoring predicative variables lead inconsistent estimation graph structure. best knowledge previous work addressed issue learning graph structure functions associated graph time. paper ﬁrst contribution proof equivalence general graphical model bivariate outcomes multivariate bernoulli model. functions represent eﬀects predicative variables responses interactions formulated model endowed advantage interpreting graph structure. follows sparsity links graphical models functions constant zero means certain responses conditionally independent. therefore impose structure penalty groups functions overlaps obtain sparse estimation graph structure. groups designed enforce sparsity functions shrink higher order interactions appear lower components entered model. paper organized follows section introduces graphical models relation multivariate bernoulli model. section discusses model learning graph structure functions graph structure penalty. experiments shown discussed section section gives conclusion future work. norm sample size number covariates number response/output dimensional response/output power except empty highest level interaction considered model paper number assumption paper element used indexing augmented responses model parameters dimension {ω|v subgraph rooted containing descendants penalty weight penalty structure subgradient group normalization factor. distribution factorized according cliques graph. clique nodes fully connected subgraph. potential function depends predicative variables shared across response variables. example application model relations multiple clinical responses aﬀected person’s genetic variables environmental variables purpose eﬃcient computation usually maximal cliques graph. maximal clique clique properly contained clique graph. diﬀerent representations non-maximal cliques converted maximal clique representation redeﬁnition potential functions furthermore reﬂect graph structure long suﬃcient. example general choice given graphical model {ω}. conditional independence response variables implicitly formulated restrictions potential functions. theorem example details. markov property states nodes clique conditionally independent given nodes. example conditionally independent given variables block path therefore maximal cliques factorizes graph speciﬁes conditional independence model. figure cliques case conditionally independent given figure graphical model examples. model triangle clique indicate third order interaction. additionally pairwise interaction conditionally independent given model another interacted random variables independent nodes. model form -node clique connected model form strongly connected component independent nodes given graph structure potential functions convenient characterize distribution graph. however graph unknown advance estimating potential functions give direct inference graph structure diﬀerent representations diﬀerent choices cliques graph mentioned above. even assuming general case conditional independence nodes cannot represented simple form makes learning graph structure diﬃcult. multivariate bernoulli model represent graphical model whose nodes bernoulli random variables i.e. parameterization model suitable learning graph structure. show later equivalent binary outcomes. distribution model here following notations. power counting empty elements convenient follows relabel elements e.g. interchangeably without speciﬁcation. free parameters denote simple notation. denotes deﬁne augmented response example graph nodes parameters potential function. usually restrict make model identiﬁable. free parameters. similarly also free parameters model many applications assumption graphical models graph structure large cliques. equivalent sparsity higher order interactions model theorem impose sparse penalty dependence structure shrink higher order interactions. |ψk| components total. denote number components paper consider learning full model |ψk|. suppose function reproducing kernel hilbert space kernel general penalized likelihood model objective obtain sparse estimation cliques structure penalty consider pairwise links. link graphical model means example figure conditionally independent means zero. objective similar sparse covariance matrix estimation gaussian data neighborhood selection lasso however model deal higher order covariance structures exist gaussian data. addition consider graph structure responses alone also functions predicative variables satisfy intuition penalty designed shrink large cliques graph. suppose true model interaction clique zero penalty designed shrink zero. idea viewed group lasso overlaps. group lasso leads selection variables groups. consistent estimation groups exclusive union whole set. jacob considered penalty groups arbitrary overlaps. zhao general framework hierarchical variable selections overlapping groups adopt functions. consider penalty guided structure figure guiding graph nodes abuse notation element index node edge |ω|+ |ω|. domain knowledge applied design diﬀerent guiding structure. jenatton discussed deﬁne groups achieve diﬀerent nonzero patterns. paper focus situation function space {}⊕hω refers constant function space rkhs linear kernel. function kcωk. here form vector length ω∈ψk denote hence objective positive scalar chosen adaptively. slight abuse notation denote vector parameters step. algorithm summarized framework solving following analysis wright show proximal linearization algorithm converge negative log-likelihood loss function plus group lasso type penalties overlaps. however solving group lasso overlaps trivial non-smoothness singular point. recent years several papers addressed problem. jacob duplicated variables design matrix appear group overlaps solved problem group lasso without overlaps. xing reparameterized group norm additional dummy variables. alternatively optimized model parameters dummy variables iteration. method performs eﬃciently quadratic loss function gaussian data. optimizing alternatively sets parameters might scale well penalized logistic regression. subgradient unit ball certain subspace subspaces perpendicular other. thus sv’s separable closed form solution cannot obtained. solve proximal subproblem smoothing convex dual problem. note equivalent matrix whose columns {s|s feasible region since lower semicontinuous upper semicontinuous exists saddle point exchangeable. solution minimizing following proof show convex lipschitz continuous. diﬀerential αk˜cet vector ones. hence solved existing gradient methods. accelerated gradient descent method implemented general distribution family graphical model according graph structure. example model response variables triangular clique independent response variables. case conditional odds ratios estimate. true model non-zero functions model functions nonzero. model functions nonzero. predictive variables independently generated multivariate gaussian distribution mean variance parameters estimate parameters uniformly sampled intercepts main eﬀects second higher order interactions randomly selected proportional probability equation generate datasets graph structure figure evaluate learning accuracy. sample size dataset table count function number runs replications recovered recovered functions true model considered true positive; others true model false positive. since main eﬀects always detected correctly listed table. structure penalty eﬃcient recognizing strong interactions responses interaction performance higher order interactions aﬀected complex graph structures e.g. model compared gacv bgacv tends achieve sparse results general large penalties degrees freedom estimated model. contrary gacv discover true positive functions cost higher false positive rate. figure show learning results terms true positive rate increasing sample size experimental setting before. figure true positive rate graph structure learning increasing sample size diﬀerent tuning methods gacv bgacv. curves converged samples. larger sample size required achieve convergence since functions estimate. bgacv conservative selecting non-zero functions. tprs improving along increasing sample size. compared model algorithm needs substantially larger sample size achieve high model gacv achieves better true positive rate four graphical models. tuning method obtain less sparse models compared bgacv. county data u.s. census bureau validate method. includes demographic data counties united states covering population employment votes etc. delete counties missing value columns interested data entries total. outcomes study summarized table vote coded republican candidate presidential election. dichotomize rest response variables national mean selected threshold. third column table gives percentage positive data. features model percentage housing unit change; government expenditure; population percentage ethnic groups people foreign born people people people high school education people bachelor degree; birth rate; death rate. experiment ﬁrst standardize data mean variance then estimated graphical model every ﬁxed adjusting regularization parameter discover interactions entering model. graph structure shown figure ﬁrst number edge indicates order link entering model second corresponding unemployment rate plays important role graph. strongly related poverty crimes. population change negatively related violent crimes well unemployment rate. analyze link vote pchange recovered method links. marginal correlation second smallest absolute value correlation matrix. partial correlation method taken example show links response variables discovered without considering link vote pchange tenth recovered link using package space. reason taking features account dependence structure response variables change. main contribution case percentage housing unit change population percentage people part ﬁtted model shown below main eﬀects suggest increase housing units counties tend increase population vote republican. increase people counties tend lose population still likely vote republican. interaction function reveals housing units increase counties likely positive results vote pchange. tendency counteracted increase people responses less likely take positive values. structure penalty multivariate bernoulli model eﬃciently learn graph structure indicates conditional independence response variables. paper consider linear models conditional odds ratios. extended smoothing spline anova model freedom choosing kernels. theorem holds natually. also interesting penalty improve prediction power compared large margin methods. another extension selection features sparsity function requires sparsity within group. graph structure would change diﬀerent selected features. application discover relations multiple symptoms clinical responses eﬀected environmental genetical covariates. smoking could signiﬁcant many diseases interactions covariates taking vitamin might related subset symptoms. result investigate sparse penalties within function.", "year": 2011}