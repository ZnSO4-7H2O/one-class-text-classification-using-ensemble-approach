{"title": "Kernel Feature Selection via Conditional Covariance Minimization", "tag": ["stat.ML", "cs.AI", "cs.LG", "stat.ME"], "abstract": "We propose a framework for feature selection that employs kernel-based measures of independence to find a subset of covariates that is maximally predictive of the response. Building on past work in kernel dimension reduction, we formulate our approach as a constrained optimization problem involving the trace of the conditional covariance operator, and additionally provide some consistency results. We then demonstrate on a variety of synthetic and real data sets that our method compares favorably with other state-of-the-art algorithms.", "text": "propose framework feature selection employs kernel-based measures independence subset covariates maximally predictive response. building past work kernel dimension reduction formulate approach constrained optimization problem involving trace conditional covariance operator additionally provide consistency results. demonstrate variety synthetic real data sets method compares favorably state-of-the-art algorithms. feature selection important problem statistical machine learning common method dimensionality reduction encourages model interpretability. large data sets becoming ever prevalent feature selection seen widespread usage across variety real-world tasks recent years including text classiﬁcation gene selection microarray data face recognition work consider supervised variant feature selection entails ﬁnding subset input features explains output well. practice reduce computational expense downstream learning removing features redundant noisy simultaneously providing insight data features remain. feature selection algorithms generally divided three main categories ﬁlter methods wrapper methods embedded methods filter methods select features based intrinsic properties data independent learning algorithm used. wrapper methods specialized contrast aiming features optimize performance speciﬁc predictor finally embedded methods multipurpose techniques incorporate feature selection prediction single problem often optimizing objective combining goodnessof-ﬁt term penalty number parameters recent work kernel methods successfully applied paradigms kernel feature selection methods advantage capturing nonlinear relationships features labels. many previous approaches ﬁlter methods based hilbert-schmidt independence criterion proposed gretton measure dependence. instance song proposed optimize hsic greedy algorithms features. masaeli proposed hilbert-schmidt feature selection optimizes hsic continuous relaxation. later work yamada proposed hsic-lasso dual augmented lagrangian used global optimum. also wrapper methods embedded methods using kernels. methods weights features optimize original kernelized loss function together penalty weights example proposed margin-based algorithms svms select features kernel space. lastly allen proposed kernel iterative feature extraction applied kernel svms kernel ridge regression embedded method. paper propose trace conditional covariance operator criterion feature selection. oﬀer theoretical motivation choice show method interpreted ﬁlter method wrapper method certain class learning algorithms. also show empirical estimate criterion consistent sample size increases. finally conclude empirical demonstration algorithm comparable better several popular feature selection algorithms synthetic real-world tasks. formulating feature selection domain covariates domain responses given independent identically distributed samples generated unknown joint distribution together integer goal select total features best predict full features denote subset features. ease notation identify features also identify formulate problem supervised feature selection perspectives below. ﬁrst perspective motivates algorithm ﬁlter method. second perspective oﬀers interpretation wrapper method. viewing problem perspective dependence would ideally like identify subset features size remaining features conditionally independent responses given however achievable small. therefore quantify extent remaining conditional dependence using metric minimize subsets appropriate size. formally function mapping subsets non-negative reals satisﬁes following properties class functions loss function speciﬁed user. example might linear functions might squared error e.g. univariate regression problem. hope solve following problem conditional covariance operator provides measure conditional dependence random variables. ﬁrst proposed baker studied used suﬃcient dimension reduction fukumizu provide brief overview operator properties here. criterion derivation denote rkhs supported subset features cardinality take vector components otherwise. deﬁne kernel suppose kernel permutation-invariant. permutation denoting every cardinality generates rkhs supported call rkhs show trace conditional covariance operator interpreted dependence measure long rkhs large enough. rkhs characteristic one-to-one. bounded equivalent saying dense probability measure following lemma whose proof given appendix lemma bounded characteristic also characteristic. proof postponed appendix. generic result place narrow focus problems univariate responses including univariate regression binary classiﬁcation multi-class classiﬁcation. case regression assume supported take linear kernel equivalently interpreted linear kernel assuming one-hot encoding obtain following corollary theorem includes identity function subset features. moreover equality holds x|xt hence univariate case problem supervised feature selection reduces minimizing trace conditional covariance operator subsets features controlled cardinality regression setting equation implies residual error regression also characterized trace conditional covariance operator using linear kernel formally following observation given fact trace conditional covariance operator characterize dependence prediction error regression empirical estimate objective. given samples empirical estimate given entry kernel matrix sample features kernel space responses linear matrix sample response. without loss generality assume column zero-mean objective becomes proof provided appendix. comparable result given fukumizu consistency dimension reduction estimator minimization takes place ﬁnite proof considerably simpler. finding global optimum np-hard generic kernels exhaustive search computationally intractable number features large. therefore approximate problem interest continuous relaxation previously done past work feature selection objective optimized using projected gradient descent represents ﬁrst tractable approximation. solution relaxed problem converted back solution original problem setting largest values remaining values initialize uniform vector order avoid corners constraint early stages optimization. computational issues removing inequality constrant. hard constraint requires nontrivial projection step detailed duchi instead replace soft constraint move objective. letting hyperparameter gives rise modiﬁed problem removing matrix inverse. matrix inverse objective function expensive operation. light this ﬁrst deﬁne auxiliary variable equality constraint nεnin)−y rewrite objective note multiply using kernel approximation. rahimi recht propose method approximating kernel evaluations inner products random feature vectors random depending choice kernel decomposition rn×d then deﬁning similarly centered kernel matrix written woodbury matrix identity write substituting objective function scaling removing constant term resulting identity matrix gives approximate optimization problem. modiﬁcation reduces complexity optimization step choice formulation. remark three approximations beyond initial relaxation independently used omitted allowing number possible objectives constraint sets. explore conﬁgurations experimental section below. section evaluate approach synthetic real-world data sets. compare several strong existing algorithms including recursive feature elimination minimum redundancy maximum relevance bahsic ﬁlter methods using mutual information pearson’s correlation author’s implementation bahsic scikit-learn scikit-feature packages rest algorithms. -dimensional -way classiﬁcation. consider corners -dimensional hypercube group tuples leaving sets vectors paired negations {v−v}. given class point generated figure plots show median rank true features function sample size simulated data sets. lower median ranks better. dotted line indicates optimal median rank. ﬁrst data represents standard nonlinear binary classiﬁcation task. second data multi-class classiﬁcation task feature independent combination three features joint eﬀect third data arises additive model nonlinear regression. data dimensions total true features. since identity features known evaluate performance feature selection algorithm computing median rank assigns real features lower median ranks indicating better performance. given enough samples would expect value come close optimal lower bound experimental setup follows. generate independent copies data sample sizes ranging record median ranks assigned true features algorithm. process repeated total times results averaged across trials. kernel-based methods gaussian kernel expx ˜x/) linear kernel take median pairwise distance samples scaled since number true features known provide input algorithms require initial experiments basic version algorithm section number desired features ﬁxed regularization parameter needs chosen. classiﬁcation tasks regression task selecting values using cross-validation. results shown figure binary -way classiﬁcation tasks method outperforms algorithms succeeding identifying true features using fewer samples others require close even fail converge. additive nonlinear model several algorithms perform well method best across sample sizes. experiments show algorithm comparable better several widely-used feature selection techniques selection synthetic tasks adept capturing several kinds nonlinear relationships covariates responses. compared particular closest relative bahsic backward-elimination algorithm based hilbert–schmidt independence criterion algorithm often produces higher quality results fewer samples even succeeds non-additive problem bahsic fails converge. also rerun experiments ﬁrst approximations described section above. comparable results attained either approximate objective note algorithm robust changes previous section found method feature selection excelled identifying nonlinear relationships variety synthetic data sets. turn attention collection real-word tasks studying performance method nonlinear approaches used conjunction kernel downstream classiﬁcation. carry experiments standard benchmark tasks feature selection website repository summary data sets provided table data sets drawn several domains including gene data image data voice data span low-dimensional high-dimensional regimes. every task algorithm evaluated obtain ranks features. performance measured training kernel features computing resulting accuracy measured -fold cross-validation. done total number features larger otherwise. cases regularization constant gaussian kernel previous section selected features. algorithm across experiments number desired features otherwise. results shown figure compared three popular methods nonlinear feature selection i.e. mrmr bahsic method strongest performer large majority cases sometimes substantial margin case tox-. method occasionally outperformed beginning number selected features small either ties overtakes leading method instance. remark method consistently improves upon performance related bahsic method suggesting objective based conditional covariance powerful based hilbert-schmidt independence criterion. work propose approach feature selection based minimizing trace conditional covariance operator. idea select features maximally account dependence response covariates. accomplish relaxing intractable discrete formulation problem obtain continuous approximation suitable gradient-based optimization. demonstrate eﬀectiveness approach multiple synthetic real-world experiments ﬁnding often outperforms state-of-the-art approaches including another competitive kernel feature selection method based hilbert-schmidt independence criterion.", "year": 2017}