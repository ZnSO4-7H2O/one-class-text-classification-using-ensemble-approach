{"title": "Non-parametric Power-law Data Clustering", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "It has always been a great challenge for clustering algorithms to automatically determine the cluster numbers according to the distribution of datasets. Several approaches have been proposed to address this issue, including the recent promising work which incorporate Bayesian Nonparametrics into the $k$-means clustering procedure. This approach shows simplicity in implementation and solidity in theory, while it also provides a feasible way to inference in large scale datasets. However, several problems remains unsolved in this pioneering work, including the power-law data applicability, mechanism to merge centers to avoid the over-fitting problem, clustering order problem, e.t.c.. To address these issues, the Pitman-Yor Process based k-means (namely \\emph{pyp-means}) is proposed in this paper. Taking advantage of the Pitman-Yor Process, \\emph{pyp-means} treats clusters differently by dynamically and adaptively changing the threshold to guarantee the generation of power-law clustering results. Also, one center agglomeration procedure is integrated into the implementation to be able to merge small but close clusters and then adaptively determine the cluster number. With more discussion on the clustering order, the convergence proof, complexity analysis and extension to spectral clustering, our approach is compared with traditional clustering algorithm and variational inference methods. The advantages and properties of pyp-means are validated by experiments on both synthetic datasets and real world datasets.", "text": "abstract—it always great challenge clustering algorithms automatically determine cluster numbers according distribution datasets. several approaches proposed address issue including recent promising work incorporate bayesian nonparametrics k-means clustering procedure. approach shows simplicity implementation solidity theory also provides feasible inference large scale datasets. however several problems remains unsolved pioneering work including power-law data applicability mechanism merge centers avoid over-ﬁtting problem clustering order problem e.t.c.. address issues pitman-yor process based k-means proposed paper. taking advantage pitman-yor process pyp-means treats clusters differently dynamically adaptively changing threshold guarantee generation power-law clustering results. also center agglomeration procedure integrated implementation able merge small close clusters adaptively determine cluster number. discussion clustering order convergence proof complexity analysis extension spectral clustering approach compared traditional clustering algorithm variational inference methods. advantages properties pyp-means validated experiments synthetic datasets real world datasets. power-law data ubiquitous real world. examples include social networks facebook topics forums citations among published papers. kind data differs traditional ones large ﬂuctuations occur tails distributions. increased attentions received recent years detection powerlaw phenomena characterization structure kind data. clustering essential technique data structure learning owing capability grouping data collections automatically. however challenge employing clustering power-law data lies difﬁculties inferring cluster number well determining cluster sizes. previous decades various clustering methods proposed dealing different kinds data. however them including classic k-means mixture models spectral clustering mean shift etc. assume cluster number kind prior information provided users value usually unknown user. initial approaches proposed handle unknown cluster number problem. however address problem model selection criteria leads dilemma selection criteria. bayesian non-parametric learning fast growing research topic recent years utilized effective approach address parameter selection issue. core idea treat required parameters e.g. cluster number hyper-distribution employ inference methods learn posterior probability latent variables given observations. demonstrates signiﬁcance contributions parameter inference. however often suffers difﬁculty designing learning schemes based conjugate assumption well computational complexity induced inference sampling address problem method called dp-means proposed bridge classic k-means clustering non-parametric dirichlet process gaussian mixture model taking advantage asymptotic zero-covariance property gaussian mixture models dp-means naturally introduces ﬁxed threshold determine whether data point belong existing cluster cluster created provides uniﬁed view combine bayesian non-parametric methods hard clustering algorithms address scale learning large datasets. however several issues remain unsolved promising pioneering work. method designed powerlaw data. global threshold clusters result clusters similar sizes. mechanism merge closed centers needed. algorithm result many small clusters. merged closed enough. clustering order inﬂuences result. strategies discussed address issue. paper proposes novel clustering approach pitmanyor process-means clustering power-law data. modiﬁed pitman-yor process ﬁrst proposed approximate power-law data structure hard partition. unlike ﬁxed threshold proposed dp-means modiﬁed pitman-yor process introduces deregulated threshold whose value changes accordance cluster denotes probability assigning data point cluster result regarded kind soft clustering different traditional hard clustering assigning single point multiple clusters probabilities. actually asymptotic link k-means clustering well-known result speciﬁcally covariance matrices mixture component assumed ǫid×d becomes center agglomeration procedure also proposed adaptively determine cluster number. address issue clustering procedure result isolated small clusters other check inter-distance pair cluster centers combine distance clusters smaller value. prevent clustering result ﬁtting power-law distribution taking account real data distribution. heuristic furthest ﬁrst strategy discussed address data order issue. prove cluster number stops increasing arbitrary order remained data points result clustering result. extend newly proposed dp-means modiﬁed pitman-yor process based k-means algorithm address power-law data generalization able cluster power-law dataset normal dataset. introduce heuristic furthest ﬁrst strategy address data order issue clustering procedure. prove convergence pyp-means calculate complexity algorithm. also extend method spectral clustering expand approach multiple clustering algorithms. remaining part paper organized follows. section introduces related work gaussian mixture models derivatives pitman-yor process. modiﬁed pitman-yor process towards power-law data represented section iii. followed section detail discusses proposed approach pyp-means including main implementation strategy data order issue center agglomeration procedure avoid overﬁtting. discussion pyp-means’ convergence proof complexity analysis extension spectral clustering found section section introduces experimental results different datasets prove effectiveness work. followed last section draws conclusion paper. k-means suffering selection cluster numbers. address problem dirichlet process introduced k-means recently. taking advantage dirichlet process distance threshold generated prevent data points assigned cluster distances exceed threshold. data point fails assigned clusters cluster created taking cluster center. speciﬁcally dirichlet process denoted hyper-parameter basement distribution hyper-parameter alfa written form base measurement used generate gaussian distribution gibbs sampling take address process. probability used gibbs sampling written gibbs sampling assigns label cluster generated gaussian distribution within ﬁnite steps local minimum achieved. data points assigned corresponding clusters distances centers smaller given threshold. dp-means treat component equally. universal threshold clusters. clustering result tends contain cluster similar sizes. however clusters real world dataset usually vary lot. typically obey power-law distributions. therefore would convenient method able generate reasonable clustering results satisfy power-law distribution. though wonderful work systematic learning unsolved problems including increasing cluster number problem clustering order problem complexity analysis e.t.c. focused. practical issues including power-law data approximating parameter adjustment also needs investigated. pitman-yor process generalization dirichlet process. py-process discount parameter added increase probability class generation. discount parameter tuning effect becomes suitable model depict power-law data. process degenerate classic dirichlet process p´olya scheme used explain pyprocess’ generative paradigm technical perspective. scheme objects interests represented colored balls contained urn. beginning empty. balls uncolored. pick ﬁrst ball paint certain color urn. following steps pick ball time color urn. color ball allocated according following probability. process continues balls painted urn. size cluster ﬁxed joint probability unchanged refers exchangeability. py-process preserves dirichlet process’ ’rich richer’ property process assigning colors balls. larger size balls certain color greater probability ball painted color. thanks discount parameter probability generating color py-process greater easily proven py-process draws colors data points power-law scheme. therefore would promising py-process incorporated clustering algorithms help address power-law data. power-law data also named heavy-tailed behavior data represents case frequency size data cluster obey exponential distribution i.e. small sized subsets cluster data coming reality life wide range data obeys power-law data including frequencies words languages populations cities intensities earthquakes. situations kind ﬁndings power-law data would considered noisy defective. however time interesting part whole observations. cluster number constrained avoid minimal distance problem. shorten threshold value accordance cluster number hidden clusters could discovered clusters would also compact larger threshold. stage main implementations pypmeans clustering quite similar dp-means. however differences come ﬂuctuated threshold clustering procedure stepwise/adaptive density checking procedure. regarding difﬁculties power-law data clustering traditional clustering methods tend group small size clusters major clusters simply treat noisy data points. un-proper trivial clusters still important whole data structure. many soft clustering methods including py-process forward effectively mining kind data. received good results however still suffer complexity problem implementation high conditions required. best knowledge little work done hard clustering scenario equivalence connected classic k-means clustering. proposition revised allocation paradigm still keeps exchangeability property i.e. joint probability data affected orders given cluster size ﬁxed. power-law distribution. contrast ordinary data clustering encounters difﬁculties trivial cluster discovery cluster number determination related imbalanced problem. contrary clustering order λ-out data points would affect center’s determination sequence. generated different centers would surely affect data belonging. explore complex situation heuristic search method called furthest ﬁrst employed here. start re-clustering choose data point whose shortest distance existed centers largest i.e. maxi{di|di minc dik} cluster center. remove data point recursively re-clustering. beneﬁt furthest ﬁrst avoid generating clusters cluster number stopped increasing. arbitrary order remained data points would affect clustering performance. saves computational cost deﬁning centers. proof assume shortest distances remaining corresponding variables come center threshold becomes ﬁxed. similar apriori rules remaining data points {x}r belong existed clusters. thus selection order generate cluster dp-means cluster generated λ-out data encountered however never disappeared even gets much closer another cluster. could result overﬁtting problem dividing dense clusters parts. hand special overﬁtting results whole implementation consists three procedures data partition center recalculation center agglomeration. data partition procedure shares similarities existed dp-means divides data λ-in data λ-out data. λ-in data clustering method according usual k-means λ-out data’s clustering employs adaptive determine cluster would detail discussed later. center-recalculation procedure corresponding step k-means. center agglomeration procedure avoid many trivial clusters. details would discussed later. clustering order data affect performance pyp-means suffering problem discuss problem accordance stages main implementation λ-in data clustering re-clustering λ-out data. following simple prototype illustrates idea clearly. four data points clusterd. assume threshold previously clustered individual clusters. according agglomeration procedure combine clusters since distance centers satisﬁes condition employ procedure clusters would remain same leading unsatisﬁed result. detail implementation time cluster centers re-calculated agglomeration procedure. checking pair cluster centers satisﬁes condition effectively prevent situations. guaranteeing local minimum value within ﬁnite steps vital pyp-means. approach goal ﬁrst showing objective function strictly decreases iteration. partitioning data points stage distance λdata newly belonging cluster center would increased conﬁrmed λ-out data cluster centers shrinkage cost penalty value more increases penalty value λ−c·θ decreases reduces objective function more. according proposition objective function strictly decrease increases. thus inequality also applies leads fact contradict assumptions. thus assumption success conclusion. proposed pyp-means scalable number data points ﬁnal cluster number computational complexity analyzed follows. three major computational steps iteration considered follows. λ-out data data size re-clustering process involves sort operation quickest complexity worst case λ-out data centers complexity cost would label assigning process complexity would agglomeration procedure. procedure needs thus assume clustering process needs iterations converge total computational complexity algorithm usually small subset algorithm algorithm computational feasible. however threshold small values leading larger computational cost would heavy. classical determination orthonormal matrices spectral theory states selects eigenvectors objective function reaches maximum ﬁxed clusters. ﬂexible value problem objective function reaches maximum selected matrices eigenvectors non-negative eigenvalues corresponding cadjusted matrix getting relaxed cluster indicator matrices cluster rows data points using k-means clustering according standard spectral clustering method take corresponding result ﬁnal clustering result. experimental evaluation conducted three types datasets grouped synthetic dataset benchmarking dataset communities’ criminal dataset datasets preprocessed normalizing feature dimension interval furthermore clustering process algorithms repeated times setting average value taken ﬁnal result. experiments computer intel xeno .-ghz microsoft windows algorithms coded matlab. sufﬁcient comparison proposed pyp-means compared three baseline algorithms k-means clustering dp-means dirichlet process variational learning parameters algorithms accordingly. kmeans clustering pre-deﬁned cluster number true number synthetic data random initialization strategy starting partition; dp-means pyp-means using parameter setting described later; variational simulation results figure shows running result synthetic dataset. experiments running cases cluster number corresponding score accuracy score recorded. figure easy dp-means pypmeans satisﬁed results cluster number small however clusters generated dpmeans falls accuracy pyp-means receives much better performance accuracy. also note pypmeans receives better performance k-means clustering cases even later true cluster number. parameter learning part parameter value taken discovery rate employed denote cluster number uncovered. default pyp-means. detail result shows figure ﬁgure smaller threshold would larger discovered cluster number. quite reasonable smaller threshold would lead smaller cluster size larger cluster number. also proposed pyp-means discovery relative accurate cluster number less however dp-means discover perform well cluster number case. cluster number learning shows corresponding cluster number discovered using parameter setting previous experiment. since k-means always take true cluster number prior information. check three methods’ discovery rate comparison. cluster number’s increase discovery rate slowly decrease. however pyp-means receives better performance existed dp-means facing large cluster number situation. running time test running time methods tested validate complexity analysis methods comparable test self-parameter comparable test. figure pyp-means runs approximate time dp-means. even large scale case validating clustering results always non-trivial task. presence true labels synthetic data employ accuracy measure effectiveness proposed methods deﬁned follows focused synthetic dataset manually contain power-law behavior learning procedure. would like investigate multiple aspects method including clustering accuracy score performance relationships threshold discovered cluster number running time e.t.c.. synthetic data generation synthetic data derived generation algorithm power-law property reﬂected specially assigning data points ﬁrst clusters remaining others also cluster number varies cover larger cases. cluster distributed according -dimensional gaussian distribution uniform distributed random centers. practical parameter setting employ method value. ﬁrst roughly estimate cluster number initialize center cluster mean. iteratively select data point largest distance generated center. maximum value distance identiﬁed value experiment. value experimentally detail discussions determination discussed later. benchmarking dataset benchmarking dataset study types datasets selected study power-law dataset normal dataset clusters equal sized clusters contrast power-law since power-law dataset limited manually make removing data points certain clusters datasets. maximum likelihood estimation power-law density function parameter used curve detail power-law behavior denoted dataset constitute nearly attributes. attributes varies many aspects community excluding clearly unrelated attributes. class label total number violent crimes population. experiment crime rate continuous variable ranges manually discrete values certain number intervals gets related labels. tune parameter value experimentally receive better performance value default table detail outcomes benchmarking data experiments. c.n. denotes cluster number method produced. results given normal datasets method pyp-means performances better least good dp-means k-means clustering cases. usually cluster number usually small kind dataset leading discount parameter function little process. power-law dataset pyp-means receive better result dp-means. ability automatically learn threshold plays vital role learning. although methods loses datasets could still validated valued. novel modiﬁed pitman-yor process based method proposed address power-law data clustering problem. discount parameter py-process slightly adjusted power-law data perfectly depicted. also introduce center agglomeration procedure leading adaptively determining number clusters. further extend work spectral clustering case address sophisticated situations. issues also well discussed here including convergence complexity analysis practical issues including reliable data clustering order. greatly strengthen solidness reality applicability method. selim ismail k-means-type algorithms generalized convergence theorem characterization local optimality pattern analysis machine intelligence ieee transactions redmond baveja data-driven software tool enabling cooperative information sharing among police departments european journal operational research vol. brown pocock m.-j. zhao luj´an conditional likelihood maximisation unifying framework information theoretic feature selection mach. learn. res. vol. mar.", "year": 2013}