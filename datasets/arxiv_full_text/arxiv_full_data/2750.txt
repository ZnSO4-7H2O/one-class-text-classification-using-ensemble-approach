{"title": "Regularization approaches for support vector machines with applications  to biomedical data", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "The support vector machine (SVM) is a widely used machine learning tool for classification based on statistical learning theory. Given a set of training data, the SVM finds a hyperplane that separates two different classes of data points by the largest distance. While the standard form of SVM uses L2-norm regularization, other regularization approaches are particularly attractive for biomedical datasets where, for example, sparsity and interpretability of the classifier's coefficient values are highly desired features. Therefore, in this paper we consider different types of regularization approaches for SVMs, and explore them in both synthetic and real biomedical datasets.", "text": "support vector machine widely used machine learning tool classiﬁcation based statistical learning theory. given training data ﬁnds hyperplane separates different classes data points largest distance. standard form uses l-norm regularization regularization approaches particularly attractive biomedical datasets where example sparsity interpretability classiﬁer’s coefﬁcient values highly desired features. therefore paper consider different types regularization approaches svms explore synthetic real biomedical datasets. support vector machine supervised learning discriminative binary classiﬁer formally deﬁned separating hyperplane. words given labeled training data vector predictor variables denotes class label algorithm outputs optimal categorizes examples according decision rule sign). hyperplane hyperplane chosen separates classes data points largest distance. solving problem exercise convex optimization. case separable data popular setup given non-negative slack variables allow points wrong side soft margin well decision boundary therefore describing overlap classes cost parameter controls amount overlap. note solving lagrangian dual problem obtain simpliﬁed problem efﬁciently solvable quadratic programming algorithms known dual form svm. alternatively formulate problem regularization framework using loss penalty criterion loss function arbitrary function hilbert space functional measures \"roughness\" generalization extension nonlinear kernel svms becomes straightforward. case norm rkhs functions generated positive deﬁnite kernel consequently previous section employed l-norm penalty equations shrinking magnitude coefﬁcients l-norm penalty reduces variance estimated coefﬁcients thus achieve better prediction accuracy. however l-norm penalty cannot produce sparse coefﬁcients hence cannot automatically perform variable selection. major limitation applying classiﬁcation high-dimensional data variable selection essential providing reasonable interpretations. usually case biomedical data obtaining good classiﬁer sufﬁce also want know variables relevant classiﬁcation problem therefore expand traditional l-norm formulation include sparsity variable selection using different regularization approaches testing real biomedical datasets. simplicity consider linear kernel results easily extended kernels. several algorithms exist solve problem. example expressed primal form solved using standard linear programming software package solved coordinate descent algorithm squared hinge loss). similar l-norm penalty norm also shrinks ﬁtted coefﬁcients toward zero hence also reducing variance ﬁtted coefﬁcients. however case owing mathematical properties norm making sufﬁciently large cause ﬁtted coefﬁcients exactly zero. therefore lasso penalty promotes sparsity hence kind feature selection case penalty. unfortunately l-norm penalty suffers important disadvantage terms feature selection several highly correlated input variables data relevant output variable lasso penalty tend pick shrink rest zero. words norm fails \"grouped selection\". particularly important context biomedical datasets. example let’s consider gene panel diagnosis multigenic disease requires abnormally high expression levels genes particular genes regulating several biological pathways. ideal classiﬁer able automatically include whole group relevant genes. however correlation among genes high l-norm select gene relevant genes corresponding coefﬁcients relevant genes zero. overcome limitation apply elastic penalty discussed section despite disadvantage l-norm penalty also presents major advantage it’s extension multi-class classiﬁcation problems straightforward. specially advantageous since svms inherently two-class classiﬁers. traditional k-class classiﬁcation svms one-vs-all approach class build classiﬁer positive examples points class negative examples points class classiﬁer. then decision rule simply argmaxcfc. unfortunately method multiple disadvantages. example number classes becomes large binary classiﬁcation becomes highly unbalanced small fraction instances class. case non-separable classes class smaller fraction instances tends ignored leading degrading performances. addition this perspective feature selection feature relevant classes selected binary classiﬁcation. presence many irrelevant features usually results necessary features therefore adverse effect classiﬁcation interpretability. alternatively instead combining separately trained binary classiﬁers single optimization problem formulated consider classes solved once done crammer singer l-norm regularization case. here propose solve alternative all-in-one multi-class optimization problem l-norm regularization based ﬁrst step generalize binary hinge loss multi-class case. −fc)]+ here later done multi-class then multi-class penalty expressed primal form number classes consideration. shen show formulation natural interpretation minimizing fc)]+. then expressed linear programming optimization problem −βcj otherwise. |βck| changes necessary eliminate absolute value operation solved linear programming software package multi-class optimization problem results linear decision functions decision rule simply argmaxcfc. regularization parameters. here role l-norm penalty allow variable selection whereas role l-norm penalty help groups correlated variables selected together highly correlated input variables similar ﬁtted coefﬁcients called grouping effect highly desirable feature biomedical applications many cases presence disease characterized increase decrease variable likely highly correlated. therefore grouping elastic select correlated variables together therefore improving interpretability nonzero classiﬁer coefﬁcients. show grouping property elastic hinge loss let’s ﬁrst note hinge loss lipschitz continuous constant then solution consider another coefﬁcients hinge loss. demonstrates grouping property elastic net. note also holds therefore grouping effect l-norm penalty. finally note exist multiple algorithmic implementations solve elastic svm. example zhou developed parallel solver utilizes gpus multi-core cpus. another regularization approach based k-support norm proposed sparsity regularization method balances norms linear function similar elastic net. k-support norm based convex hull k-support norm tighter convex relaxation elastic therefore better convex constraint elastic seeking sparse l-norm linear predictor combines uniform shrinkage penalty largest components sparse shrinkage penalty smallest components. k-support norm computed regularization parameter dimension feature space. note negatively correlates sparsity. algorithm also uses hinge loss previous examples leads sparse correlated subsets selected features. implemented using nesterov’s accelerated gradient descent algorithm table classiﬁcation results breast tumor diagnosis dataset containing input features computed digitalized images needle aspirates binary labels malignant/benign. dataset right contaminated additional features drawn distribution. relevant coefﬁcients indicate number non-zero coefﬁcients correspond original features whereas non-relevant coefﬁcients refer ones corresponding additional noise features. results show averages repetitions together standard deviation. figure coefﬁcients elastic trained gene expression microarray containing genes patients either acute myeloid leukemia acute lymphoblastic leukemia. sparsity analysis demonstrate different sparsity properties regularization approaches wisconsin breast cancer dataset containing total instances features computed digitized images needle aspirates breast mass labels malignant cancer benign tumors. dataset divided training test performed -fold cross-validation training optimize hyperparameters classiﬁer. results shown table show average results repetitions indicate k-support leads best performance dataset followed elastic approaches lead sparse coefﬁcients non-zero coefﬁcients respectively coefﬁcients. addition this table also shows results dataset \"contaminated\" additional noise features drawn distribution. case number relevant non-zero coefﬁcients stays algorithms regularizations select considerably non-relevant features. cases k-support elastic svms lead best performances higher sparsity. addition this tested regularization approaches oligonucleotide array dataset containing probes genes bone marrow samples acute lymphoblastic leukemia acute myeloid leukemia. figure shows resulting coefﬁcients elastic demonstrates better interpretability sparse coefﬁcient selection elastic obtaining sparse coefﬁcients examine genes relevant classiﬁcation task. case regularization possible. grouping effect illustrate concept grouping effect generate synthetic data follows. generate training instances classes instance p-dimensional vector class normal distribution mean either covariance matrix lower triangular matrix. then generate multiple separate calls scalar gaussian generator. finally compute desired distribution mean covariance note classes elements highly correlated trained different svms training examined resulting coefﬁcients different combinations hyperparameters. results shown figure indicate lnorm kept variables model l-norm variable selection failed identify group correlated variables elastic successfully selected relevant variables shrunk coefﬁcients figure comparison regularization paths different svms simple synthetic dataset variables highly correlated whereas correlated. blue lines indicate coefﬁcient values correlated variables whereas read lines indicate coefﬁcient values synthetic dataset examine performance multi-class l-regularized consider four category classiﬁcation problem. first sample instances then randomly assign four classes instances class. finally linear transformation performed xij; classes respectively different values example features relevant classiﬁcation whereas remaining features redundant. hyperparameters different algorithms tested tuned discrete using leave-one-out cross-validation. accuracy algorithms examined different test constructed training set. optimal test errors well number selected features averaged repetitions reported table results show lmsvm outperforms algorithms terms number number features selected values number non-zero coefﬁcients close addition this lmsvm also resulted classiﬁcation errors gene expression dataset multi-class algorithm also tested real dataset containing gene expression data genes patients leukemia. training data consisted samples three classes samples acute myeloid leukemia samples b-cell acute lymphoblastic leukemia samples t-cell acute lymphoblastic leukemia test contained samples b-all t-all. tuned algorithm hyperparametrs using leave-one-out cross-validation. results showed multi-class algorithm lead test error non-zero coefﬁcients. contrast methods select large number non-zero coefﬁcients. example although elastic resulted test error well produced non-zero coefﬁcients. therefore results table average test errors number features selected binary classiﬁer number features selected multi-class classiﬁers together standard errors simulation replications. provided lmsvm offer better interpretability and. interestingly four genes selected lmsvm individually jointly identiﬁed predictor genes t-all b-all aml. project examined variety regularization approaches binary multi-class classiﬁcation support vector machines. sparsity properties regularization approaches demonstrated synthetic real biomedical datasets showing sparse solutions lead better interpretability learning results. furthermore showed mathematically grouping property elastic demonstrated empirically synthetic dataset. unfortunately limited scope project couldn’t explore interesting regularization approaches could offer interesting features biomedical data classiﬁcation group lasso sparse multiple kernel learning svms. remain topics future exploration. references andreas argyriou rina foygel nathan srebro. sparse prediction k-support norm. nips thomas cormen charles leiserson ronald rivest clifford stein. introduction algorithms. katerina gkirtzou jean-françois deux guillaume bassez aristeidis sotiras alain rahmouni thibault varacca nikos paragios matthew blaschko. sparse classiﬁcation based markers neuromuscular disease categorization. international workshop machine learning medical imaging vladimir vapnik. statistical learning theory. wiley wang zou. doubly regularized support vector machine. statistica sinica lifeng wang xiaotong shen yuan zheng. l-norm multi-class support vector machines. quan zhou wenlin chen shiji song jacob gardner kilian weinberger yixin chen. reduction", "year": 2017}