{"title": "Evolutionary Generative Adversarial Networks", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Generative adversarial networks (GAN) have been effective for learning generative models for real-world data. However, existing GANs (GAN and its variants) tend to suffer from training problems such as instability and mode collapse. In this paper, we propose a novel GAN framework called evolutionary generative adversarial networks (E-GAN) for stable GAN training and improved generative performance. Unlike existing GANs, which employ a pre-defined adversarial objective function alternately training a generator and a discriminator, we utilize different adversarial training objectives as mutation operations and evolve a population of generators to adapt to the environment (i.e., the discriminator). We also utilize an evaluation mechanism to measure the quality and diversity of generated samples, such that only well-performing generator(s) are preserved and used for further training. In this way, E-GAN overcomes the limitations of an individual adversarial training objective and always preserves the best offspring, contributing to progress in and the success of GANs. Experiments on several datasets demonstrate that E-GAN achieves convincing generative performance and reduces the training problems inherent in existing GANs.", "text": "generative adversarial networks eﬀective learning generative models real-world data. however existing gans tend suﬀer training problems instability mode collapse. paper propose novel framework called evolutionary generative adversarial networks stable training improved generative performance. unlike existing gans employ pre-deﬁned adversarial objective function alternately training generator discriminator utilize diﬀerent adversarial training objectives mutation operations evolve population generators adapt environment also utilize evaluation mechanism measure quality diversity generated samples well-performing generator preserved used training. e-gan overcomes limitations individual adversarial training objective always preserves best oﬀspring contributing progress success gans. experiments several datasets demonstrate e-gan achieves convincing generative performance reduces training problems inherent existing gans. generative adversarial networks main groups methods used learn generative models complicated real-world data. well using generator synthesize semantically meaningful data standard signal distributions gans train discriminator distinguish real samples training dataset fake samples synthesized generator. confronter generator aims deceive discriminator producing ever realistic samples. training procedure continues generator wins adversarial game; discriminator cannot make better decision randomly guessing whether particular sample fake real. gans recently successfully applied image generation image editing video prediction many tasks although gans already produce visually appealing samples various applications often diﬃcult train. data distribution generated distribution substantially overlap generator gradients point less random directions even result vanishing gradient issue. gans also suﬀer mode collapse i.e. generator assigns probability mass small region space addition appropriate hyper-parameters network architectures critical conﬁgurations gans. unsuitable settings many recent eﬀorts gans focused overcoming training diﬃculties developing various adversarial training objectives. typically assuming optimal discriminator given generator learned diﬀerent objective functions generator measure distance data distribution generated distribution diﬀerent metrics. original uses jensen-shannon divergence metric. number metrics introduced improve gan’s performance least-squares absolute deviation kullback-leibler divergence wasserstein distance however according theoretical analyses experimental results minimizing distance pros cons. example although measuring kullback-leibler divergence largely eliminates vanishing gradient issue easily results mode collapse likewise wasserstein distance greatly improves training stability non-convergent limit cycles near equilibrium exploit advantages suppress weaknesses diﬀerent metrics devise framework utilizes diﬀerent metrics jointly optimize generator. improve training stability generative performance. build evolutionary generative adversarial network treats adversarial training procedure evolutionary problem. speciﬁcally discriminator acts environment population generators evolve response environment. adversarial iteration discriminator still trained recognize real fake samples. however method acting parents generators undergo diﬀerent mutations produce oﬀspring adapt environment. diﬀerent adversarial objective functions minimize diﬀerent distances generated distribution data distribution leading diﬀerent mutations. meanwhile given current optimal discriminator measure quality diversity samples generated updated oﬀspring. finally according principle survival ﬁttest poorly-performing oﬀspring removed remaining well-performing oﬀspring preserved used training. based evolutionary paradigm optimize gans proposed e-gan overcomes inherent limitations individual adversarial training objectives always preserves best oﬀspring produced diﬀerent training objectives contribute progress success gans. experiments several datasets demonstrate advantages integrating diﬀerent adversarial training objectives e-gan’s convincing performance image generation. section ﬁrst review previous gans devoted reducing training instability improving generative performance. brieﬂy summarize evolutionary algorithms deep neural networks. generative adversarial networks provides excellent framework learning deep generative models capture probability distributions given data. compared generative models easily trained alternately updating generator discriminator using back-propagation algorithm. many generative tasks gans produce better samples generative models however problems still exist gans training process. original training generator equal minimizing jensen-shannon divergence data distribution generated distribution easily resulted vanishing gradient problem. solve issue generator then designed speciﬁed network architectures proposed several heuristic tricks improve training stability. meanwhile energy-based least-squares improved training stability employing diﬀerent training objectives. although methods partly enhanced training stability practice network architectures training procedure still required careful design maintain figure original framework. generator discriminator play two-player adversarial game. updating gradients generator received adaptive objective depends discriminator-generator balance. recently wasserstein variant wgangp proposed minimize wasserstein- distance generated data distributions. since wasserstein- distance continuous everywhere diﬀerentiable almost everywhere minimal assumptions methods convincingly reduce training instability. however measure wasserstein- distance generated distribution data distribution asked enforce lipschitz constraint discriminator restrict critic capability result optimization diﬃculties last twenty years evolutionary algorithms achieved considerable success across wide range computational tasks including modeling optimization design inspired natural evolution essence evolutionary algorithm equate possible solutions individuals population produce oﬀspring variations select appropriate solutions according ﬁtness recently evolutionary algorithms introduced solve deep learning problems. minimize human participation designing deep algorithms automatically discover conﬁgurations many attempts optimize deep learning hyper-parameters design deep network architectures evolutionary search evolutionary algorithms also demonstrated capacity optimize deep neural networks moreover proposed novel evolutionary strategy alternative popular mdp-based reinforcement learning techniques achieving strong performance benchmarks. last least evolutionary algorithm proposed compress deep learning models automatically eliminating redundant convolution ﬁlters section ﬁrst review formulation. then introduce proposed e-gan algorithm. illustrating e-gan’s mutations evaluation mechanism discuss advantage proposed framework. finally conclude entire e-gan training process. generative network taking noisy sample input generative network outputs data whose distribution supposed close data distribution pdata. meanwhile discriminative network employed distinguish discriminator population individual represents possible solution parameter space generative network evolutionary process expect population gradually adapts environment means evolved generator generate ever realistic samples eventually learn real-world data distribution. shown fig. evolution step consists three sub-stages variation given individual population utilize variation operators produce oﬀspring gθ···}. speciﬁcally several copies individual—or parent—are created thus discriminative network continually provide adaptive losses drive population generator evolving produce better solutions. next illustrate discuss proposed variation evaluation operators detail. employ asexual reproduction diﬀerent mutations produce next generation’s individuals speciﬁcally mutation operators correspond diﬀerent training objectives attempt narrow distances generated distribution data distribution diﬀerent perspectives. section introduce mutations used work. analyze corresponding properties mutations suppose that evolutionary step optimal discriminator pdata+pg according already learned according theoretical analysis given optimal discriminator minimax mutation aims minimize jensen-shannon divergence data distribution generated distribution. although minimax game easy explain theoretically analyze performance practice disappointing primary problem generator’s vanishing gradient. support distributions lies manifolds constant leading vanishing gradient problem also illustrated fig. discriminator rejects generated samples high conﬁdence data distribution meaning discriminator cannot completely distinguish real fake samples minimax mutation provides eﬀective gradients continually narrows data distribution generated distribution. compared minimax mutation heuristic mutation saturate discriminator rejects generated samples. thus heuristic mutation avoids vanishing gradient provides useful generator updates however according given optimal discriminator minimizing heuristic mutation equal minimizing jsd] i.e. inverted minus algorithm e-gan. default values require batch size discriminator’s updating steps iteration number parents number mutations adam hyper-parameters hyper-parameter evaluation function. least-squares mutation inspired lsgan least-squares objectives utilized penalize generator deceive discriminator. work formulate least-squares mutation saturates eventually approaching zero. therefore similar heuristic mutation least-squares mutation avoid vanishing gradient discriminator signiﬁcant advantage generator. meanwhile compared heuristic mutation although least-squares mutation assign extremely high cost generate fake samples also assign extremely cost mode dropping partly avoids mode collapse note that diﬀerent gan-minimax gan-heuristic lsgan employs diﬀerent loss optimize discriminator. shown supplementary material optimal discriminator lsgan equivalent ours. therefore although employ discriminator environment distinguish real generated samples suﬃcient provide adaptive losses mutations described above. evolutionary algorithm evaluation operation measuring quality individuals. determine evolutionary direction devise evaluation function measure demonstrated heuristic objective suﬀers mode collapse since assigns high cost generating performance evolved individuals typically focus generator properties quality diversity generated samples. first simply feed generator produced images discriminator observe average value output name quality ﬁtness score note discriminator constantly upgraded optimal training process reﬂecting quality generators evolutionary step. generator obtains relatively high quality score generated samples deceive discriminator generated distribution approximate data distribution. besides generative quality also attention diversity generated samples attempt overcome mode collapse issue optimization. recently proposed gradient-based regularization term stabilize optimization suppress mode collapse. observation generator collapses small region discriminator subsequently label collapsed points fake obvious countermeasure gradient value updating utilized measure diversity generated samples. updated generator obtains relatively high diversity score corresponds small discriminator gradients generated samples tend spread enough avoid discriminator obvious countermeasures. thus mode collapse issue suppressed discriminator change smoothly helps improve training stability. balances measurements generative quality diversity. overall relatively high ﬁtness score leads higher training eﬃciency better generative performance. figure experiments cifar- dataset. cifar- inception score generator iterations wall-clock time graph selected mutations e-gan training process regarded evolutionary population discriminator acts environment. evolutionary step generators updated diﬀerent objectives accommodate current environment. according principle survival ﬁttest well-performing children survive participate future adversarial training. unlike two-player game ﬁxed static adversarial training objective conventional gans e-gan allows algorithm integrate merits diﬀerent adversarial objectives generate competitive solution. thus training evolutionary algorithm largely suppresses limitations individual adversarial objectives also harnesses advantages search better solution. evaluate e-gan synthetic datasets three image datasets cifar- lsun bedroom celeba tasks network architectures based dcgan brieﬂy introduced here details found supplementary material. default hyper-parameter values listed algorithm experiments. note number parents means child retained evolutionary step. hand reduces e-gan’s computational cost thereby accelerating training. other experiments show e-gan already achieves impressive performance stability even survivor step. ﬁrst experiment adopt experimental design proposed trains gans gaussian mixture distributions. mode collapse issue accurately measured synthetic datasets since clearly observe data distribution generated distribution. shown fig. employ challenging distributions evaluate e-gan mixture gaussians arranged circle mixture gaussians arranged grid. ﬁrst compare proposed evolutionary adversarial training framework using individual adversarial objective train method iterations report plots fig. results show individual adversarial objectives suﬀer mode collapse greater lesser degree. however combining diﬀerent objectives evolution framework model performance largely improved accurately target distributions. demonstrates evolutionary procedure proposed evaluation mechanism recognize well-performing updatings promote population better evolutionary direction. evaluating model sample quality convergence speed important criteria. train diﬀerent gans cifar- plot inception scores course training network architecture based dcgan used methods. shown fig. -left e-gan higher inception score less training steps. meanwhile e-gan also shows comparable stability goes convergence. comparison conventional gans expose diﬀerent limitations instability convergence slow convergence invalid mentioned above diﬀerent objectives measure distance generated data distributions diﬀerent metrics diﬀerent pros cons. here utilizing evolutionary framework e-gan overcomes limitations individual adversarial objectives also outperforms gans furthermore although e-gan takes time iteration achieves comparable convergence speed terms wall-clock time training e-gan recorded selected objective evolutionary step beginning training heuristic mutation least-square mutation selected frequently minimax mutation. fact minimax mutation hard provide eﬀective gradients discriminator easily recognize generated samples. along generator approaching convergence ever minimax mutations employed number selected heuristic mutations falling. aforementioned minus jsds heuristic mutation tend push generated distribution away data distribution lead training instability. however e-gan beyond heuristic mutation options mutation architecture robustness another advantage e-gan. demonstrate training stability method train diﬀerent network architectures lsun bedroom dataset compare several existing works. addition baseline dcgan architecture choose three additional architectures corresponding diﬀerent training challenges limiting recognition capability discriminator i.e. -conv--fc leakyrelu discriminator; limiting expression capability generator i.e. batchnorm constant number ﬁlters generator; reducing network capability generator discriminator together i.e. remove generator discriminator architecture test diﬀerent methods dcgan lsgan standard wgan wgan-gp e-gan. method used default conﬁgurations recommended respective studies train model iterations. shown fig. e-gan generates reasonable results even methods failed. images addition given well-trained generator evaluate performance embedding latent space noisy vectors fig. ﬁrst select pairs generated faces record corresponding latent vectors images pair diﬀerent attributes gender expression hairstyle age. then generate novel samples linear interpolating pairs generated samples seamlessly change semantically meaningful face attributes. experiment demonstrates generator training merely memorize training samples learns meaningful projection latent noisy space face images. meanwhile also shows generator trained e-gan suﬀer mode collapse shows great space continuity. paper present evolutionary framework training deep generative models. reduce training diﬃculties improve generative performance devise evolutionary algorithm evolve population generators adapt dynamic environment contrast conventional gans evolutionary paradigm allows proposed e-gan overcome limitations individual adversarial objectives preserve best oﬀspring iteration. experiments show e-gan improves training stability models achieves convincing performance several image generation tasks. future works focus exploring relationship environment evolutionary population improving generative performance.", "year": 2018}