{"title": "Conditional Probability Tree Estimation Analysis and Algorithms", "tag": ["cs.LG", "cs.AI"], "abstract": "We consider the problem of estimating the conditional probability of a label in time $O(\\log n)$, where $n$ is the number of possible labels. We analyze a natural reduction of this problem to a set of binary regression problems organized in a tree structure, proving a regret bound that scales with the depth of the tree. Motivated by this analysis, we propose the first online algorithm which provably constructs a logarithmic depth tree on the set of labels to solve this problem. We test the algorithm empirically, showing that it works succesfully on a dataset with roughly $10^6$ labels.", "text": "consider problem estimating conditional probability label time number possible labels. analyze natural reduction problem binary regression problems organized tree structure proving regret bound scales depth tree. motivated analysis propose ﬁrst online algorithm provably constructs logarithmic depth tree labels solve problem. test algorithm empirically showing works succesfully dataset roughly labels. central question paper eﬃciently estimate conditional probability label given observation virtually approaches solving problem require time. commonly used one-against-all approach tries predict probability label versus labels requires time training example. another common approach learn scoring function convert conditional probability estimate motivation dealing computational difﬁculty usual one—we want capability solve otherwise unsolvable problems. example experiments involves probabilistic prediction problem roughly labels examples solution intractable. section provide ﬁrst online supervised learning algorithm trains predicts computation example. algorithm require knowledge advance; adapts naturally labels encountered. prediction algorithm uses binary tree regressors used node predict conditional probability true label left right. probability leaf estimated product appropriate conditional probability estimates path root leaf. experiments linear regressors trained stochastic gradient descent. diﬃcult part algorithm constructing tree itself. number labels large becomes critical construct easily solvable binary problems nodes. section introduce treeconstruction rule desirable properties. first always results depth also encourages natural problems minimizing expected loss nodes. technique used algorithm also useful prediction problems multiclass classiﬁcation. test algorithm empirically datasets improves performance naive tree-building approaches competes prediction performance common oneagainst-all approach exponentially slower. finally analyze broader logarithmic time probability estimation methods. section prove tree based approach squared loss bounded tree depth squared times average squared loss node regressors used. contrast pecoc approach squared loss bounded times average squared loss uses computation. suggests tradeoﬀ computation squared loss multiplier. section demany methods used solve conditional probability estimation problems achieve logarithmic dependence ones know batch constructed regression trees treenet slow consider datasets scale interest incapable reasonably dealing labels appearing time. mnih hinton constructed special purpose tree-based algorithm language modeling perhaps similar previous work. algorithm specialized word prediction substantially slower since involves many iterations training data. however general analysis provide section applies algorithm. regard empirical success algorithm evidence tree-based approaches merit investigation. section states analyses methods logarithmic time probabilistic prediction given tree structure. section gives algorithm building tree structure. analysis ﬁrst section sufﬁciently general applies second. common deﬁne observable squared loss equation replaced consider regret respect common deﬁnition since well known diﬀerence observable squared loss minimum possible observable squared loss equal therefore regret squared loss interchangeably paper. well known squared loss strictly proper scoring rule thus uniquely minimized analysis focuses squared loss bounded proper scoring rule. boundedness implies convergence guarantees hold weaker assumptions unbounded proper scoring rules loss. non-leaf node associated regression problem predicting probability label given observation left subtree conditioned following procedure shows transform multiclass examples binary examples non-leaf node tree. righti left subtree node otherwise. theorem proved following core lemma. node path root label deﬁne conditional probability label consistent next step given previous steps consistent. similarly deﬁne conditional probability tree computationally tractable could hope robust could hope for. example pecoc approach yields squared loss multiplier independent number labels. approach robust tree requiring less computation pecoc? provide construction trades extremes pecoc conditional probability tree. essential idea shift binary tree k-way tree pecoc regressors used node tree estimate probability child conditioned reaching node. simplicity assume power power node tree lemma bounds power adversary disturb probability estimate function adversary’s regret. similarly lemma bounds power adversary induce overall misestimate function adversary’s power disturb estimates within node path. proof. since code prediction algorithm symmetric respect inclusion assume without loss generality every subset thus every entry pecoc output estimate pecoc construction deﬁned binary matrix column label deﬁning regression problem. regression problem corresponding predict probability given correct label subset free parameter entire algorithm. rule indicates place labels side fewer current labels resulting perfectly balanced tree. direction chosen always currently favored regressor. trade-oﬀ objectives provided values extremes. section analyze algorithm throughout section tree node consideration total number leaves node number left right note rule symmetric respect also deﬁne analysis section applies binary tree motivates creation trees small depth small regret nodes. leaves question which tree use? give online tree construction algorithm several useful properties. particular algorithm doesn’t require prior knowledge labels takes computation example labels. algorithm guarantees tree maximum depth using decision rule trades depth ease prediction. algorithm builds maintains tree whose leaves one-to-one correspondence labels seen far. node tree associated regressor given sample consider cases. already exists label leaf tree associated root-to-leaf path conditional probability tree algorithms previous section train test minor modiﬁcation training regressor leaf train example exist tree algorithm still traverses tree leaf using decision rule computes direction nonleaf node encountered. leaf reached necessarily corresponds label convert non-leaf node left child right child regressor node duplicated regressor created trained example describe decision rule used decide non-leaf node encountered traversal. first denote number children left node number right. current prediction associated node regressor favors right subtree input otherwise left subtree. regressor favors side smaller number elements direction chosen. regressor favors side elements algorithm faces dilemma. hand sending label right would result highly balanced tree hand would result training sample disagreeing current regressor’s prediction. resolution deﬁne objective function proceeding inductively total depth -leaf tree lr-leaf subtrees total depth plus total depth plus since convex function worst case comes unequal split applying inductive hypothesis total depth conducted experiments datasets. purpose ﬁrst experiment show conditional probability tree competes prediction performance existing exponentially slower approaches. this derive label probability prediction problem publicly available reuters proof. prove inductively result follows symmetrically. non-leaf node starts left right child satisﬁes claim. given satisfy claim prove leaf added next values cases. dataset second experiment fullscale test system exponentially slower approach intractable seriously consider. proprietary dataset consists webpages associated advertisements derived problem predict probability would displayed webpage. dataset split training test set. training test sample form algorithms train training produce probabilistic rule maps pairs form numbers range interpret approximation algorithms evaluated test computing algorithms allowed continue learning tested however predictions used computed training sample type evaluation called progressive validation accurately measures performance online algorithm. particular unbiased estimate algorithm’s performance assumption pairs identically independently distributed. motivating applications algorithm expect labels appear throughout learning process requires learning occur continually online fashion. thus turning learning computing test loss less natural. nevertheless reuters dataset veriﬁed test loss progressive validation quite similar. advertising dataset measures drastically diﬀerent large number labels appear test set. algorithm executed three treebuilding construction methods random tree uniform random left/right decisions made leaf encountered balanced tree according algorithm general tree according algorithm binary regression problems used vowpal wabbit simple linear regressor trained stochastic gradient descent. essential enabling feature hashing trick allows represent linear regressors sparse feature space reasonable amount ram. reuters dataset consists documents assigned categories. total approximately categories appear data. split data training documents test documents opposite original use. document formed example form follows. vector uses words representation weighted normalized tf-idf scores exactly done paper label categories assigned chosen uniformly random category assigned doc. compared one-against-all algorithm standard approach reducing multi-class regression binary regression. one-against-all approach regresses probability category versus categories. given base training example example used train regressor category indicator function. predictions test example done according learning algorithm used training binary regressors approaches incremental gradient descent squared loss. algorithm several versions different learning rates chosen coarse grid picked setting yielded smallest training error. algorithm performed similar search one-against-all approach used pass training data used passes. note even additional pass much faster training fact requires training regressors example whereas one-againsttrains regressor category. machine took seconds train one-againsttook seconds. progressive validation compute average squared loss test results appearing following table conﬁdence intervals computed hoeﬀding’s inequality note problem much advantage using algorithm using random tree. since aren’t many labels many examples structure tree important. conﬁrmed running algorithm various diﬀerent random trees observing little variability squared loss.", "year": 2009}