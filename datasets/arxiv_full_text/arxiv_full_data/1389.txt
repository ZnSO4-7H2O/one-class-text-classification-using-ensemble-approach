{"title": "Deep neural networks are robust to weight binarization and other  non-linear distortions", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "Recent results show that deep neural networks achieve excellent performance even when, during training, weights are quantized and projected to a binary representation. Here, we show that this is just the tip of the iceberg: these same networks, during testing, also exhibit a remarkable robustness to distortions beyond quantization, including additive and multiplicative noise, and a class of non-linear projections where binarization is just a special case. To quantify this robustness, we show that one such network achieves 11% test error on CIFAR-10 even with 0.68 effective bits per weight. Furthermore, we find that a common training heuristic--namely, projecting quantized weights during backpropagation--can be altered (or even removed) and networks still achieve a base level of robustness during testing. Specifically, training with weight projections other than quantization also works, as does simply clipping the weights, both of which have never been reported before. We confirm our results for CIFAR-10 and ImageNet datasets. Finally, drawing from these ideas, we propose a stochastic projection rule that leads to a new state of the art network with 7.64% test error on CIFAR-10 using no data augmentation.", "text": "recent results show deep neural networks achieve excellent performance even when training weights quantized projected binary representation. here show iceberg networks testing also exhibit remarkable robustness distortions beyond quantization including additive multiplicative noise class non-linear projections binarization special case. quantify robustness show network achieves test error cifar- even effective bits weight. furthermore common training heuristic— namely projecting quantized weights backpropagation—can altered networks still achieve base level robustness testing. speciﬁcally training weight projections quantization also works simply clipping weights never reported before. conﬁrm results cifar- imagenet datasets. finally drawing ideas propose stochastic projection rule leads state network test error cifar- using data augmentation. deep neural networks trained using backpropagation shown perform exceptionally well wide range classiﬁcation tasks typically networks high precision representation weights training inference. considering inference tasks long standing goal reduce precision weights without sacriﬁcing performance lowering network’s computational memory footprint. practical applications include network compression running networks faster efﬁciently conventional hardware running networks specialized hardware designed speciﬁcally reduced precision representations remarkably slew recent work shown using three values weights dnns approach state performance popular benchmarks basic approach apply gradient descent using high precision weights training project weights quantization function forward/backward passes. high precision weights able accumulate small gradient updates—computed respect projected ones—allowing network explore discrete conﬁgurations continuous setting. argued rather reasonably training procedure essential learning precision representations work suggest nuanced view. shift focus idea projecting quantized weights training leads networks robust quantization general phenomenon projecting weights certain functions lead networks robust function entire class distortions. patient given vaccine ﬁnds inoculated measles mumps malaria well. show many networks perform well weights binarized also perform well kinds distortions. distortions include additive multiplicative noise well applying non-linear distortions weights. report using weight projections quantization training also lead robust networks. furthermore show examples standard backprop weight clipping learn base level robustness although performance slightly reduced. based observations propose stochastic projection rule inspired binaryconnect except rule projects weights random intervals rule results state performance cifar- binary case non-binary case. organization paper follows reviewing related work section describe training algorithm section next section algorithm train networks cifar- dataset exploring different combinations weight projections weight clipping parameters. help frame results ﬁrst delve curious ﬁnding section speciﬁcally show network trained using quantized weights also test error non-quantized weights suggesting robust weight distortions. following lead section uncover network robust distortions beyond weight quantization. section tease apart aspects training lead robust networks report ﬁrst time non-quantized projections also lead robustness. stochastic projection rule explored section sections check ﬁndings hold imagenet. section puts forth theoretical ideas backprop able robust solutions. finally conclude discuss future directions section current work related ﬂurry recent research dnns precision weights. major advance discovery backprop-based learning method quantizes weights propagations. training method introduced impressive results cifar- developed context neuromorphic hardware proposed similar rule contrastive divergence. build work exploring general class weight projections restricted quantization. another recent approach approximates weights using binary values training formulated constrained optimization problem promising results imagenet expect ﬁndings consistent results however left future work. approaches developed probabilistic interpretation discrete weights however extended convnets datasets larger mnist. consider sets tensors input activations weights indexed layer output layer computed using convolution operation example typical relu activation proj projection operator described below. notational simplicity specify fully-connected layers convolutions consider neuron biases. work explore dnn’s various projections weights used training testing. projections deﬁned table scalar functions extended operate tensors element-wise. denote i-th element tensor wki. introduce layer normalize weights interval certain projections. factor normalizing across entire layer seems crude compared ﬁlter-wise normalization found cases lead similar results. worth pointing power projection generalizes sign none since power sign power none. procedure train described algorithm similar binaryconnect except allow arbitrary weight projections. differences algorithm standard backprop ﬁrst project weights using proj computing forward pass compute gradients respect projected weights apply updates iii) clip weights update layer clip value deﬁned standard deviation initialized weights scaled global factor unless otherwise noted. algorithm reduces standard backprop projection none testing like training also performed particular projection however important note testing training projections independently speciﬁed. often refer projections applied testing distortions. algorithm training weight projection. proj projection table parameter clip boundaries loss function number layers. input minibatch inputs targets current weights learning rate output updated weights project weights proj standard forward pass computed respect projected weights backward propagation update weights update color images categories experiments data augmentation. train adam learning rule learning rate batch size square hinge loss; batch normalization results obtained tensorflow except control network used caffe experimental follows using training pre-processed global contrast normalization whitening pylearn trained networks epochs using conv layers networks named according training parameters. example tr-sign-c speciﬁes network trained using sign projection clipping append clipping seventh network downloaded pre-trained caffe model comparison. training evaluate network’s test error different distortions weights batch norm parameters re-computed training data distortion. tests speciﬁed using similar naming convention example te-power refers test power projection. results te-none te-sign te-round summarized table along comparisons prior work. surprising results table analyze time next sections. table test error cifar-. rows networks trained using different weight projections clipping parameters bottom three rows literature; columns tests using different weight distortions. interesting result networks comparable test errors te-none te-sign. surprising consider figure trained using sign projection training loss computed respect sign weights high precision weights directly. network performs well evaluated using either binarized weights high precision ones result would expected weight distributions converged values however case example weights corresponding ﬁlters noticeably different quantization errors present throughout layers despite differences activity network still converges similar patterns propagates layers demonstrating surprising insensitivity exact weight values. based observations next explore networks also robust non-linear distortions beyond weight binarization. investigate premise networks perform well binary weights also perform well many types weight distortions. here focus networks trained using quantization-based weight projections allowed weight states training discrete. speciﬁcally consider tr-sign-c tr-stoch-c three distortions te-addnorm te-multunif te-power. nin-ctrl also shown comparison. case adding gaussian noise weight observe test error increases however tr-sign-c tr-stoch-c signiﬁcantly resilient nin-ctrl. particular tr-stoch-c achieves test error even corresponds bits weight using signal noise analysis second moments weight noise distributions respectively. case figure cifar- network trained using sign projection conv speciﬁed x-y-nin-nout nin-nout. test scenarios te-none te-sign four weight histograms post training shown right. test error training evaluated every epochs te-none te-sign. insets show weights corresponding ﬁlters post training. average absolute differences sign layer. correlation coefﬁcient neuron activity layer minibatch forward passes evaluated using using sign. multiplicative noise applied weight trend similar tr-stoch-c tr-sign-c resilient noise compared control finally te-power weight normalized raised power multiplied sign. remarkably tr-stoch-c tr-sign-c also robust types distortions. surprising networks never explicitly trained using visualize projection distorts weights different values show tr-sign-c’s weight distribution conv note lower pushes weights bi-modal distribution approaches sign. contrast nin-ctrl sensitive distortions error distortion near bringing results together appears backprop ﬁnding paths solutions generally robust weight distortions speciﬁc quantizations used training. section attempt uncover aspects training lead robust networks. ﬁrst experiment projections weight quantization also work. accordingly trained network using power projection drawn minibatch. network converges solution similarly robust tr-sign-c remarkable considering projected weights undergo stochastic non-linear distortion training step. conﬁrms training weight projections quantization also works opens door trying exotic projections next tried removing weight projections altogether. trained networks tr-none-c tr-none-nc putting networks battery tests observe tr-none-c robust tr-none-nc although exhibit basic trends. notably tr-none-c still achieves test error even weights quantized binary even though never trained binary weights. later figure dnns robust different types weight distortions. networks trained using different projections clip values. training network tested gaussian multiplicative noise applied weights distortion weight raised power weight histograms conv tr-sign-c shown weights projected using power four values section hypothesize weight clipping viewed type regularization help explain results. curious result tr-none-nc also exhibits base level robustness nin-ctrl not—even though trained using standard backprop without clipping. conﬂicts previous ﬁndings crude quantization lead good performance believe typically process tuning best score often leads non-robust networks. case however networks ﬁrst tuned using weight projections training. therefore remove weight projections training rest parameters still regime backprop discovers robust solutions. learning regime appears delicate take practical view rest paper focus backprop weight clipping weight projections. inspired stoch projection ﬁrst introduced constructed stochastic projection rule stochm. stoch weight randomly projected probability probability stochm derives similar idea projects weight rationale interval state data without data augmentation best knowledge. network also exhibits high degree robustness interesting note using training expected value projected weight longer figure versions alexnet trained without weight projections tr-none-nc weight clipping tr-none-c uses weight clipping test error computed every iterations projections none round sign. weight clipping alone network becomes robust weight quantizations. whether results extend beyond cifar- moved imagenet dataset training images classes alexnet convolution fully connected layers modiﬁed batch norm layers. experiments matconvnet using momentum batch size benchmarking tested whether weight projections needed obtain robust networks. accordingly alexnets trained without projections iterations without weight clipping weight clipping cases top- error reported te-none te-round te-sign. focusing network tr-none-nc te-none reaches test error te-round te-sign signiﬁcantly worse. previous cifar- result tr-none-nc hold imagenet. network tr-none-c tells different story network similar performance te-none te-round te-sign respectively. conﬁrms previous observation weight clipping training inﬂuence network robustness. although case peak error iterations higher te-none compared network tr-none-nc; help mitigate effect clipping scheduling increase clip values training later experiments. also observe that te-round performs better te-sign. benchmarked tr-stochm-c imagenet. network trained iterations learning rate dropped iterations also increased clip values scaling global clip factor iterations effect keeping te-none te-round scores sync also allowing network reach lower error. probed network’s robustness using te-power compare test error recent models binary weights figure alexnet trained stochm robust power distortions. test error alexnet different weight distortions compared recent alexnet models binary weights first explore idea imposing constraints weights regularizer similar dropconnect idea ﬁrst suggested case weight binarization. here examine imposing weight clipping also regularizer context proximal methods; excellent review proximal methods. consider minimizing vector containing weights loss regularizer. convex proximal gradient method trivial case proxg proximal gradient method reduces standard gradient descent. case weight clipping assuming layer clip value proxg max−) functions operate element-wise corresponds words applying weight clipping update understood imposing type regularization penalizes weights outside unit ball norm. empirically non-convex case regularization appears reduces sensitivity distortions. future work could also operations weights commonly used proximal methods; preliminary work vein. second consider stochastic weight projection. stochastic projection function parametrized vector maps real-valued weight projection. extend operate vectors element-wise. used without projection forward backward steps gradient descent traversing cost surface however weights projected computing forward backward steps backprop access sampling distribution several minibatches gradient descent traversing alternative error surface hence solution obtained minimizing edataeφθ necessarily provides robustness distortions weights. furthermore surface edataeφθ smoothed-over version surface edata smoothness controlled distribution noise source underlying φθ’s stochasticity. training typically backprop estimates gradient edata takes presumed stochasticity data account produce smoothed estimate. case backprop estimates gradient edataeφθ additionally sampling weight space neighborhood provides additional gradient smoothing even deterministic extent. thus error surface gradient smoother explain empirical results observed paper. testing objective function using either directly deterministic projection yield best results. future work envision cooling noise source underlying φθ’s stochasticity training progress eventually example draw respect normal distribution moreover also interested extending domain include vectors weight projection function weight. instance recent work already used projection depends norm weights ﬁlter although work slightly different context ours. taking step back appears cases weight gradients distorted relative gradients standard backprop. viewing distortions type noise bridge ﬁndings recent work suggests adding explicit gradient noise results better overall performance expand previous work demonstrating networks trained backprop become robust speciﬁc weight distortions binary weights. show imposing certain weight distortions training leads regime network becomes robust distortion entire family distortions well. based observation proposed novel rule stochastically projects weight random interval based current value. hypothesize rule similar stochastic projection rule binaryconnect optimizing weight values directly instead optimizing neighborhood weight vector. practice rule leads state performance cifar- binary non-binary weighted networks. ﬁnding network achieve cifar- bits weight also practical interest. potential application weights implemented noisy devices could implications neuromorphic computing. recently research binary weights extended also include binary neuron activations training networks similar binary weight case namely binary activations imposed training. hypothesize neuron outputs models also robust distortions. conﬁrmed would suggest imposing activation constraints could improve performance. cannot help speculate built-in robustness noise synapses neurons inherent characteristic brain prove invaluable opening directions deep learning neuromorphic computing.", "year": 2016}