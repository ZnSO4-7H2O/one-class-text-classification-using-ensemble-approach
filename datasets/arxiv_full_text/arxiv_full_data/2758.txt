{"title": "The Case for Meta-Cognitive Machine Learning: On Model Entropy and  Concept Formation in Deep Learning", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Machine learning is usually defined in behaviourist terms, where external validation is the primary mechanism of learning. In this paper, I argue for a more holistic interpretation in which finding more probable, efficient and abstract representations is as central to learning as performance. In other words, machine learning should be extended with strategies to reason over its own learning process, leading to so-called meta-cognitive machine learning. As such, the de facto definition of machine learning should be reformulated in these intrinsically multi-objective terms, taking into account not only the task performance but also internal learning objectives. To this end, we suggest a \"model entropy function\" to be defined that quantifies the efficiency of the internal learning processes. It is conjured that the minimization of this model entropy leads to concept formation. Besides philosophical aspects, some initial illustrations are included to support the claims.", "text": "machine learning usually deﬁned behaviourist terms external validation primary mechanism learning. paper argue holistic interpretation ﬁnding probable efﬁcient abstract representations central learning performance. words machine learning extended strategies reason learning process leading so-called meta-cognitive machine learning. such facto deﬁnition machine learning reformulated intrinsically multiobjective terms taking account task performance also internal learning objectives. suggest model entropy function deﬁned quantiﬁes efﬁciency internal learning processes. conjured minimization model entropy leads concept formation. besides philosophical aspects initial illustrations included support claims. machine learning often approached behaviourist perspective external feedback form reinforcement signal major driving force improvement. though method lead many successes confronted interesting unsolved challenges like tackling overﬁtting providing comprehensibility building reusable abstractions concept formation among many problem behaviourist approaches ignore central importance internal processes considering learning. model internals often regarded means achieve higher performance. analogous studying human behaviour however appreciating mechanisms learning boils question when really learnt? paper argue computer learnt when possible analogy better understand statements found software engineering. considering code performs speciﬁc task care functionality also execution speed/efﬁciency so-called non-functional requirements. furthermore carefully modularized design probably reﬂects understanding endless enumeration if-else clauses. different humans course machines measurable. provides unique opportunity study nature learning principle time improving machine intelligence. claiming model complexity/efﬁciency subject past research efforts. contrary many techniques design principles attempted improve exactly properties like occam’s razor bayesian structure learning pruning prototypes compact information regularization strategy reduce energy weight sharing rnns cnns decrease model complexity etc. indeed whole evolution deep learning seen speciﬁc approach quest models structured organizing training layer-wise fashion focus mainly training algorithms designing model architectures adapted kinds deep structures similar efforts multiobjective machine learning techniques considered means improve performance rather goal however believe minimizing model’s structural complexity optimizing efﬁciency representation means improve performance central pillar machine intelligence leads concept formulation made explicit. sense vision aligns kurzweil claimed theory behind deep learning. model reﬂects hierarchy natural phenomenon you’re trying learn figure basic example concept learnt. function represents anti-symmetric function variables expect good model reﬂect anti-symmetry model structure parameters. laid case operational deﬁnition machine learning made. forward conjecture optimization model entropy leads concept formation. last conclusions steps operationalize concepts formulated. conventional wisdom depicts machine learning optimization parametric model respect performance measure. view clearly reﬂected facto deﬁnition machine learning mitchell computer program said learn experience respect class tasks performance measure performance tasks measured improves experimental data traditional machine learning techniques typically exploit shallow-structured often ﬁxed architectures. nevertheless general consensus learning higherorder concepts problematic solution issue somehow connected deep architectures create ever higher forms abstraction. experimental research well neurological evidence organization brain supports ﬁnding limitation architecture complexity preferred primarily behaviour could understood training complex adaptive architectures leads explosion complexity. recently. recent advanced so-called deep learning focused training algorithms adapted kinds deep architectures heuristic strategies attain speciﬁc structural properties like sparse coding lead higher forms abstractions. exception studies interpretability structural properties mainly considered by-product side effect applied training mechanisms. though organization complexity model topologies acknowledged crucial current approaches mainly limited analysing data space i.e. implemented regression functions decision boundaries problem approach. consider neural network algorithm needs learn simple concept like function depicted fig. inﬁnite number neural networks similar identical decision boundaries constructed shown fig. external point view discriminate models describing difference models occur terms model internals. course weight space represents model neural network related data space performs calculations data. words sparse coding perfect example this. without sparse coding although information intrinsically present data neural networks become intractable train extremely volatile complex decision surface. perspective follow observations made bengio representation learning. interesting phenomena information entanglement case model space lower dimensionality complexity data space. projection data onto high-dimensional space using sparse coding then advantage representations likely linearly separable least less nonlinear. hand model complexity increased considerably neural network becomes untrainable using traditional techniques because dimensionality search space explodes. deep learning techniques tackle issue among techniques pre-initializing model-space particular layer maximum-likelihood/minimal-energy state. though energy-based approach proven successful choice divide train network layer-wise rather arbitrary. instead would like kind process optimizes model structure according data leads representation learning. example driving weights stimulate formation sparse representations promote disentanglement. vision learning machines transforms implementation execution strategies optimize external performance inner energetic properties model well model entropy. words figure model internals matter. example zero-test-error neural network topologies characterize gate. inputs outputs deﬁned domain figure shows minimal entropy model non-minimal entropy one. networks identical decision surface therefore identical generalization noise-robustness properties. however surfaces constructed substantially different model structures. many advantages moving away purely extrinsic deﬁnition machine learning towards cognitively inspired operational deﬁnition start artiﬁcial distinction supervised unsupervised learning evaporates particular model still optimized towards minimal entropy rather minimal error. also opens door towards learning symbolic data. multi-objective approach allows deep comparison different machine learning models even different kinds algorithms. allows focused approach towards deep learning techniques lastly opens opportunities quantize biases machine learning algorithms. extrinsic performance determines well model scores performing externally deﬁned task. often performance counts also additional desirable properties like noiseimmunity smoothness generalization unseen data etc. qualities measured terms externally deﬁned problem. data modelling efﬁciency model requires less energy represent data learnt probably demonstrates superior insight. modelling efﬁciency thus interpreted likelihood particular model generated particular data. optimization margin support vector machines example. maximum likelihood models related maximal entropy models. models attempt minimize a-priori assumptions imposed model data. minimal entropy models maximal entropy modelling contradictory similar. latter case entropy measured data space metaspace model parameters especially presence uncertainty data modelling efﬁciency essential improve robustness model. elaborate later sections. model entropy considering model complexity dates back beginning computational modelling makes indeed sense believe models lower entropy complexity without losing expressive power provide superior abstraction. therefore makes sense optimize model structures minimal entropy respect data. model entropy determined network topology also value patterns weights values interaction data tempting confuse minimizing model entropy methods assuming smoothness input/output data distribute impose structural constraints data. clearly connected crucial difference. minimal entropy models elevate abstraction model given data assume data entropy case example methods referred algorithms already implicitly employ kind optimization strategies. usage prototypes nearestneighbours strategies compact information implicitly improve modelling efﬁciency class sample determined distance closest neighbour; regularization strategy reducing energy consumption; weight sharing decrease complexity/entropy model; penalty functions constrain model complexity; occam’s razor pruning decision trees. computer program said learn experience performance task measured improves experience model entropy modelling efﬁciency model likelihood decreases respect training data associated experience similar energy function traditional deep learning entropy function deﬁned measures complexity process construct higher-order abstract representations represents efﬁciency learning processes. existing research explored similar ideas which example neural networks characterized impact topology reduction information theoretic content network however crucial realize complexity depends model characteristics like topology weight patterns properties sample conﬁgurations though could tempted think entropy ﬁxed given training data case process abstraction. indeed abstract representations produced ones using computation thus complexity calculation matters. side complexity computation depends data representation well. this course line previous investigations nature complexity example cartesian coordinate system decision boundary double spiral problem nonlinear polar coordinate system linear requires hidden layer neurons coding requires two. representation thus clearly impacts model entropy. according wikipedia concept abstract idea representing fundamental characteristics represents. concepts arise abstractions generalisations experience result transformation existing ideas. thus makes sense hypothesize minimization entropy accomplishes capture structural identities concepts outside world example model structure optimal entropy discriminate hand-written numbers probable reﬂect true nature concepts. empirical evidence supports interpretation especially context so-called one-shot generalization finding constructing entropy function easy task subject fundamental research. interesting ﬁnding effective strategies minimize function prove even harder deeper investigation complexity models needed. today often ad-hoc solutions chosen like enforcing structural limitations pruning regularization weight sharing kernel tricks sparse representations etc. conclusion paper argue holistic view machine learning takes distance prevailing behaviourist perspective external validation major force learning. work centred around conjectures learning explained extrinsic terms data representation model computation considered sides coin. result structural properties model data space modelling higher abstractions. machine learning regards strategies optimize performance model structure data modelling efﬁciency/likelihood. result machine learning intrinsically multi-objective process trades task performance model complexity data modelling efﬁciency/energy model likelihood. deﬁnition learning needs reﬂect vision computer program said learn experience performance task measured improves experience model entropy modelling efﬁciency model likelihood decreases respect training data associated experience future work includes research entropy function scientiﬁc quantitative validation postulated hypotheses prototype examples. later phase investigated proposed extensions improve existing deep learning techniques. figure double spiral learning problem starting different input data abstraction complexity model depends properties model data. computation representations sides coin.", "year": 2017}