{"title": "Growing Regression Forests by Classification: Applications to Object  Pose Estimation", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "In this work, we propose a novel node splitting method for regression trees and incorporate it into the regression forest framework. Unlike traditional binary splitting, where the splitting rule is selected from a predefined set of binary splitting rules via trial-and-error, the proposed node splitting method first finds clusters of the training data which at least locally minimize the empirical loss without considering the input space. Then splitting rules which preserve the found clusters as much as possible are determined by casting the problem into a classification problem. Consequently, our new node splitting method enjoys more freedom in choosing the splitting rules, resulting in more efficient tree structures. In addition to the Euclidean target space, we present a variant which can naturally deal with a circular target space by the proper use of circular statistics. We apply the regression forest employing our node splitting to head pose estimation (Euclidean target space) and car direction estimation (circular target space) and demonstrate that the proposed method significantly outperforms state-of-the-art methods (38.5% and 22.5% error reduction respectively).", "text": "paper accepted publication eccv title paper changed k-ary regression forests continuous pose direction estimation growing regression forests classiﬁcation applications object pose estimation. abstract. work propose novel node splitting method regression trees incorporate regression forest framework. unlike traditional binary splitting splitting rule selected predeﬁned binary splitting rules trial-and-error proposed node splitting method ﬁrst ﬁnds clusters training data least locally minimize empirical loss without considering input space. splitting rules preserve found clusters much possible determined casting problem classiﬁcation problem. consequently node splitting method enjoys freedom choosing splitting rules resulting eﬃcient tree structures. addition euclidean target space present variant naturally deal circular target space proper circular statistics. apply regression forest employing node splitting head pose estimation direction estimation demonstrate proposed method signiﬁcantly outperforms state-of-the-art methods regression successfully applied various computer vision tasks head pose estimation object direction estimation human body pose estimation facial point localization require continuous outputs. regression mapping input space target space learned training data. learned mapping function used predict target values data. computer vision input space typically high-dimensional image feature space target space low-dimensional space represents high level concepts present given image. complex input-target relationship non-linear regression methods usually employed computer vision tasks. among several non-linear regression methods regression forests shown eﬀective various computer vision problems regression forest ensemble learning method combines several regression trees strong regressor. regression trees deﬁne recursive partitioning input space leaf node contains model predictor. training stage trees grown order reduce empirical loss training data. regression forest regression tree independently trained using random subset training data prediction done ﬁnding average/mode outputs trees. node splitting algorithm binary splitting commonly employed regression trees however limitations regarding partitions input space. biggest limitation standard binary splitting splitting rule node selected trial-and-error predeﬁned splitting rules. maintain search space manageable typically simple thresholding operations single dimension input chosen. limitations resulting trees necessarily eﬃcient reducing empirical loss. overcome drawbacks standard binary splitting scheme propose novel node splitting method incorporate regression forest framework. node splitting method clusters training data least locally minimize empirical loss ﬁrst found without being restricted predeﬁned splitting rules. splitting rules preserve found clusters much possible determined casting problem classiﬁcation problem. by-product procedure allows node tree child nodes adding level ﬂexibility model. also propose adaptively determine number child nodes splitting. unlike standard binary splitting method splitting procedure enjoys freedom choosing partitioning rules resulting eﬃcient regression tree structures. addition method euclidean target space present extension naturally deal circular target space proper circular statistics. refer regression forests employing node splitting algorithm employing adaptive determination number child nodes akrf. test akrf pointing’ dataset head pose estimation epfl multi-view dataset direction estimation observe proposed methods outperform state-of-the-art error reduction pointing’ error reduction epfl multi-view dataset. also akrf signiﬁcantly outperform general regression methods including regression forests standard binary splitting. number inherently regression problems head pose estimation body orientation estimation addressed classiﬁcation methods assigning diﬀerent pseudo-class label roughly discretized target value increasing number pseudo-classes allows precise prediction however classiﬁcation problem becomes diﬃcult. becomes problematic dimensionality target space increases. apply k-means clustering target space automatically discretize target space assign pseudo-classes. solve classiﬁcation problem rule induction algorithms classiﬁcation. though somewhat sophisticated approaches still suﬀer problems discretization. diﬀerence method approaches discussed approaches pseudo-classes ﬁxed determined either human clustering algorithms approach pseudo-classes adaptively redetermined node splitting regression tree training. similarly method converts node splitting tasks local classiﬁcation tasks applying algorithm joint input-output space. since clustering applied joint space method suitable tasks high dimensional input space. fact experiments limited tasks upto dimensional input space method performs poorly compared baseline methods. work similar method proposed chou applied k-means like algorithm target space locally optimal partitions regression tree learning. however method limited case input categorical variable. although limit continuous inputs formulation general applied type inputs choosing appropriate classiﬁcation methods. regression widely applied head pose estimation tasks. used kernel partial least squares regression learn mapping features head poses. fenzi learned local feature generative model using networks estimated poses using inference. works considered direction estimation tasks direction ranges modiﬁed regression forests binary splitting minimizes cost function speciﬁcally designed direction estimation tasks. applied supervised manifold learning used networks learn mapping point learnt manifold target space. following subsections ﬁrst explain abstracted regression tree algorithm followed presentation standard binary splitting method normally employed regression tree training. describe details splitting method. algorithm adaptively determine number child nodes presented followed modiﬁcation method circular target space necessary direction estimation tasks. lastly regression forest framework combining regression trees presented. regression trees grown recursively partitioning input space disjoint partitions starting root node corresponds entire input space. node splitting stage splitting rules prediction models partition determined minimize certain loss typical choice prediction model constant model determined mean target value training samples partition. however higher order models linear regression also used. throughout work employ constant model. partitioning corresponding child nodes created training sample forwarded child nodes. child node split number training samples belonging node larger predeﬁned number. essential component regression tree training algorithm splitting nodes. recursive nature training stage suﬃces discuss splitting root node training data available. subsequent splitting done subset training data belonging node exactly manner. standard binary regression trees ﬁxed two. splitting rule deﬁned pair index input dimension threshold. thus binary splitting rule corresponds hyperplane perpendicular axes. among predeﬁned splitting rules minimizes overall selected trial-and-error. major drawback splitting procedure splitting rules determined exhaustively searching best splitting rule among predeﬁned candidate rules. essentially reason simple binary splitting rules deﬁned thresholding single dimension considered training stage. since candidate rules severely limited selected rules necessarily best among possible ways partition input space. order overcome drawbacks standard binary splitting procedure propose splitting procedure rely trial-and-error. graphical illustration algorithm given fig.. node splitting target space number clusters. note similarity objective functions eq.. diﬀerence clusters indirectly determined splitting rules deﬁned input space clusters directly determined k-means algorithm without taking account input space. preserves much possible. task equivalent k-class classiﬁcation problem aims determining cluster training data based although classiﬁcation method used work employ lregularized l-loss linear one-versus-rest approach. formally solve following optimization cluster using liblinear unlike standard binary splitting splitting rules limited hyperplanes perpendicular axes clusters found without restricted predeﬁned splitting rules input space. furthermore splitting strategy allows node child nodes employing adding level ﬂexibility model. note larger generally results smaller value however since following classiﬁcation problem becomes diﬃcult larger necessarily lead better performance. since parameter need determine value time consuming cross-validation step. order avoid cross-validation step achieving comparative performance propose method adaptively determine node based sample distribution. work employ bayesian information criterion measure choose also used diﬀerent formulation. designed balance model complexity likelihood. result target distribution complex larger number selected target distribution simple smaller value selected. contrast non-adaptive method ﬁxed number used regardless complexity distributions. fig. illustration proposed splitting method clusters training data found target space k-means input partitions preserving found clusters much possible determined splitting needed mean computed constant estimate colored samples. yellow stars represent means. note color points change misclassiﬁcation. splitting needed clusterling applied colored samples separately target space. k-means clustering assume underling probability distribution assume data generated mixture isotropic weighted gaussians shared variance. unbiased estimate shared variance computed direction estimation object cars pedestrians unique target variable periodic namely represent direction angle. thus target space naturally represented unit circle riemannian manifold deal target space special treatments needed since euclidean distance inappropriate. instance distance shorter manifold. method direction estimation problems naturally addressed modifying k-means algorithm computation bic. remaining steps kept unchanged. k-means clustering method consists computing cluster centroids hard assignment training samples closest centroid. finding closest centroid circle trivially done using length shorter distance. periodic nature variable arithmetic mean appropriate computing centroids. typical compute mean angles ﬁrst convert angle point unit circle. arithmetic mean computed plane converted back angular value. speciﬁcally given direction angles mean direction computed using k-means algorithm node splitting essentially means employ distance loss function eq.. although squared shorter length might appropriate direction estimation task constant time algorithm mean minimizes also explained shortly deﬁnition mean coincides maximum likelihood estimate mean certain probability distribution deﬁned circle. note that second term quantity euclidean norm mean vector obtained converting angle point unit circle. similar derivation euclidean case assume data generated mixture weighted mises distributions shared mean k-th mises distribution mean k-th cluster obtained k-means clustering. shared value obtained solving following equation regression forest ﬁnal regression model. regression forest ensemble learning method regression ﬁrst constructs multiple regression trees random subsets training data. testing done computing mean outputs regression tree. denote ratio regression forest standard binary regression trees additional randomness typically injected. ﬁnding best splitting function node randomly selected subset feature dimensions considered. regression forest regression trees always consider feature dimensions. however another form randomness naturally injected randomly selecting data points initial cluster centroids k-means algorithm. test eﬀectiveness akrf euclidean target space head pose estimation task. adopt pointing’ dataset dataset contains head images subjects subject series images diﬀerent poses represented pitch yaw. first compare akrf general regression methods using image features. choose standard binary regression forest kernel \u0001-svr kernels widely used various computer vision tasks. ﬁrst series images subjects used training second series images used testing. performance measured mean absolute error degree. akrf terminate node splitting number training data associated leaf node less number trees combined akrf determined -fold cross-validation training set. kernel implementation provided author \u0001-svr libsvm package parameters kernel \u0001svr also determined -fold cross-validation. seen table akrf work signiﬁcantly better regression methods. table compares akrf prior art. since previous works report -fold cross-validation estimate whole dataset also follow protocol. akrf advance state-of-the-art reduction average respectively. fig. shows eﬀect average along average akrf. experiment cross-validation process successfully selects best performance. akrf works better second best overall training time much faster akrf since cross-validation step determining value necessary. train single regression tree akrf takes takes cross-validation training ﬁnal model. reference takes train single tree finally estimation results akrf second sequence person shown fig.. test akrf circular target space epfl multi-view dataset dataset contains sequences images cars various directions. sequence contains images instance car. total images dataset. image comes bounding specifying location ground truth direction car. direction ranges input features multiscale features parameters previous experiment extracted pixels image patches algorithm evaluated using ﬁrst sequences training remaining sequences testing. table compare krf-circle akrf-circle previous work. also include performance kernel \u0001-svr kernels using features. kernel \u0001-svr ﬁrst direction angles points unit circle train regressors using mapped points target values. testing phase point coordinate ﬁrst estimated mapped back angle atan. parameters determined leave-one-sequencecross-validation training set. performance evaluated mean absolute error measured degrees. addition percentile absolute errors percentile reported following convention prior works. seen table krf-circle akrf-circle work much better existing regression methods. particular improvement brf-circle notable. methods also advance state-of-the-art reduction previous best method respectively. fig. show akrf-circle computed sequence testing set. performance varies signiﬁcantly among diﬀerent sequences fig. shows representative results worst three sequences testing notice failure cases fig. representative results worst three sequences testing set. numbers image ground truth direction estimated direction failure cases ﬂipping error. paper proposed novel node splitting algorithm regression tree training. unlike previous works method rely trial-and-error process best splitting rules predeﬁned rules providing ﬂexibility model. combined regression forest framework methods work signiﬁcantly better state-of-the-art methods head pose estimation direction estimation tasks. acknowledgements. research supported muri grant oﬃce naval research n---.", "year": 2013}