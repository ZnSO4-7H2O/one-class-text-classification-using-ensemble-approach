{"title": "Regularizing and Optimizing LSTM Language Models", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.", "text": "recurrent neural networks long short-term memory networks serve fundamental building block many sequence learning tasks including machine translation language modeling question answering. paper consider speciﬁc problem word-level language modeling investigate strategies regularizing optimizing lstmbased models. propose weight-dropped lstm uses dropconnect hidden-tohidden weights form recurrent regularization. further introduce nt-asgd variant averaged stochastic gradient method wherein averaging trigger determined using non-monotonic condition opposed being tuned user. using regularization strategies achieve state-of-the-art word level perplexities data sets penn treebank wikitext-. exploring effectiveness neural cache conjunction proposed model achieve even lower state-of-the-art perplexity penn treebank wikitext-. effective regularization techniques deep learning subject much research recent years. given over-parameterization neural networks generalization performance crucially relies ability regularize models sufﬁciently. strategies dropout batch normalization found great success ubiquitous feed-forward convolutional neural networks. naïvely applying approaches case recurrent neural networks highly successful however. many recent works hence focused extension regularization strategies rnns; brieﬂy discuss below. naïve application dropout rnn’s hidden state ineffective disrupts rnn’s ability retain long term dependencies ghahramani propose overcoming problem retaining dropout mask across multiple time steps opposed sampling binary mask timestep. another approach regularize network limiting updates rnn’s hidden state. approach taken semeniuta wherein authors drop updates network units speciﬁcally input gates lstm lieu units themselves. reminiscent zone updates hidden state fail occur randomly selected neurons. instead operating rnn’s hidden states regularize network restrictions recurrent matrices well. done either restricting capacity matrix element-wise interactions forms regularization explicitly upon activations batch normalization recurrent batch normalization layer normalization introduce additional training parameters complicate training process increasing sensitivity model. work investigate regularization strategies highly effective also used modiﬁcation existing lstm implementations. weight-dropped lstm applies recurrent regularization dropconnect mask hidden-to-hidden recurrent weights. strategies include randomized-length backpropagation time embedding dropout activation regularization temporal activation regularization modiﬁcations required lstm implementation regularization strategies compatible black libraries nvidia cudnn many times faster naïve lstm implementations. deﬁned training algorithm used required good minimizer loss function also converge minimizer rapidly. choice optimizer even important context regularized models since strategies especially dropout impede training process. stochastic gradient descent variants adam rmsprop amongst popular training methods. methods iteratively reduce training loss scaled gradient steps. particular adam found widely applicable despite requiring less tuning hyperparameters. context word-level language modeling past work empirically found outperforms methods ﬁnal loss also rate convergence. agreement recent evidence pointing insufﬁciency adaptive gradient methods given success especially within language modeling domain investigate averaged known superior theoretical guarantees. asgd carries iterations similar instead returning last iterate solution returns average iterates past certain tuned threshold threshold typically tuned direct impact performance method. propose variant asgd determined non-monotonic criterion show achieves better training outcomes compared sgd. propose dropconnect recurrent hidden hidden weight matrices require modiﬁcations rnn’s formulation. dropout operation applied weight matrices forward backward pass impact training speed minimal standard implementation used including inﬂexible highly optimized black lstm implementations nvidia’s cudnn lstm. performing dropconnect hidden-to-hidden weight matrices within lstm prevent overﬁtting occurring recurrent connections lstm. regularization technique would also applicable preventing overﬁtting recurrent weight matrices cells. weights reused multiple timesteps individual dropped weights remain dropped entirety forward backward pass. result similar variational dropout applies dropout mask recurrent connections within lstm among popular methods training deep learning models across various modalities including computer vision natural language processing reinforcement learning. training deep networks posed non-convex optimization problem subscript denotes iteration number denotes stochastic gradient computed minibatch data points. demonstrably performs well practice also possesses several attractive theoretical properties linear convergence saddle point avoidance better generalization performance speciﬁc task neural language modeling traditionally without momentum found outperform algorithms momentum adam adagrad rmsprop statistically signiﬁcant margin. motivated observation investigate averaged improve training process. asgd analyzed depth theoretically many surprising results shown including asymptotic second-order convergence asgd takes steps identical equation instead returning last iterate solui=t total numtion returns iterations user-speciﬁed averaging trigger. despite theoretical appeal asgd found limited practical training deep networks. part unclear tuning guidelines learning-rate schedule averaging trigger averaging triggered soon efﬁcacy method impacted triggered late many additional iterations needed converge solution. section describe non-monotonically triggered variant asgd obviates need tuning further algorithm uses constant learning rate throughout experiment hence tuning necessary decay scheduling. neighborhood around solution. case certain learning-rate reduction strategies step-wise strategy analogously reduce learning rate ﬁxed quantity point. common strategy employed language modeling reduce learning rates ﬁxed proportion performance model’s primary metric worsens stagnates. along lines could make triggering decision based performance model validation set. however instead averaging immediately validation metric worsens propose non-monotonic criterion conservatively triggers averaging validation metric fails improve multiple cycles; algorithm given choice triggering irreversible conservatism ensures randomness training play major role decision. analogous strategies also proposed learning-rate reduction algorithm introduces additional hyperparameters logging interval non-monotone interval found setting number iterations epoch worked well across various models data sets. such setting ntasgd experiments following section demonstrate achieves better training outcomes compared sgd. addition regularization optimization techniques above explored additional regularization techniques aimed improve data efﬁciency training prevent overﬁtting model. given ﬁxed sequence length used break data ﬁxed length batches data efﬁciently used. illustrate this imagine given elements perform backpropagation ﬁxed backpropagation time window element divisible never elements backprop into matter many times traverse data set. indeed backpropagation window element receives equal element’s index. data inefﬁcient preventing data ever able improve recurrent fashion resulting remaining elements receiving partial backpropagation window compared full possible backpropagation window length probability probability high value approaching spreads starting point bptt window beyond base sequence length. select sequence length according base sequence length standard deviation. jitters starting point doesn’t always fall speciﬁc word divisible these sequence length efﬁciently uses data ensuring given enough epochs elements data experience full bptt window ensuring average sequence length remains around base sequence length computational efﬁciency. training rescale learning rate depending length resulting sequence compared original speciﬁed sequence length. rescaling step necessary sampling arbitrary sequence lengths ﬁxed learning rate favors short sequences longer ones. linear scaling rule noted important training large scale minibatch without loss accuracy component unbiased truncated backpropagation time standard dropout binary dropout mask sampled every time dropout function called. dropout masks sampled even given connection repeated input lstm timestep receiving different dropout mask input lstm variant this variational dropout samples binary dropout mask upon ﬁrst call repeatedly locked dropout mask repeated connections within forward backward pass. propose using dropconnect rather variational dropout regularize hidden-to-hidden transition within variational dropout dropout operations speciﬁcally using dropout mask inputs outputs lstm within given forward backward pass. example within minibatch uses unique dropout mask rather single dropout mask used examples ensuring diversity elements dropped out. following ghahramani employ embedding dropout. equivalent performing dropout embedding matrix word level dropout broadcast across word vector’s embedding. remaining non-dropped-out word embeddings scaled probability embedding dropout. dropout occurs embedding matrix used full forward backward pass means occurrences speciﬁc word disappear within pass equivalent performing variational dropout connection one-hot embedding embedding lookup. weight tying shares weights embedding softmax layer substantially reducing total parameter count model. technique theoretical motivation prevents model learn one-to-one correspondence input output resulting substantial improvements standard lstm language model. natural language processing tasks pretrained trained word vectors relatively dimensionality—frequently dimensions size. previous lstm language models dimensionality word vectors dimensionality lstm’s hidden state. even reducing word embedding size beneﬁcial preventing overﬁtting easiest reduction total parameters language model reducing word vector size. achieve this ﬁrst last lstm layers modiﬁed input output dimensionality respectively equal reduced embedding size. l-regularization often used weights network control norm resulting model reduce overﬁtting. addition decay used individual unit activations difference outputs different time steps; strategies labeled activation regularization temporal activation regularization respectively penalizes activations signiﬁcantly larger means regularizing network. concretely deﬁned dropout mask output timestep scaling coefﬁcient. falls broad category slowness regularizers penalize model producing large changes hidden state. penn treebank data long central data experimenting language modeling. data heavily preprocessed contain capital letters numbers punctuation. vocabulary also capped unique words quite small comparison modern datasets results large number vocabulary tokens. wikitext- sourced curated wikipedia articles approximately twice size data set. text tokenized processed using moses tokenizer frequently used machine translation features vocabulary words. capitalization punctuation numbers retained data set. experiments three-layer lstm model units hidden layer embedding size loss averaged examples timesteps. embedding weights uniformly initialized interval weights initialized performed better smaller sizes nt-asgd. completion asgd hot-started ﬁne-tuning step improve solution. ﬁne-tuning step terminate using nonmonotonic criterion detailed algorithm carry gradient clipping maximum norm initial learning rate experiments. random bptt length probability probability values used dropout word vectors output lstm layers output ﬁnal lstm layer embedding dropout respectively. weight-dropped lstm dropout applied recurrent weight matrices. increase input dropout account increased vocabulary size. experiments values respectively embedding softmax weights. hyperparameters chosen trial error expect improvements possible ﬁne-grained hyperparameter search conducted. results abbreviate approach awd-lstm asgd weight-dropped lstm. present single-model perplexity results models competitive models table respectively. data sets improve state-of-the-art vanilla lstm model beating state approximately unit units comparison recent state-of-the-art models model uses vanilla lstm. zilly propose recurrent highway network extends lstm allow multiple hidden state updates timestep. zoph reinforcement learning agent generate cell tailored speciﬁc task language modeling cell complex lstm. independently work melis apply extensive hyperparameter search lstm based language modeling implementation analyzing sensitivity based language models hyperparameters. unlike work modiﬁed lstm caps input gate adam rather asgd skip connections lstm layers black hyperparameter tuner exploring models settings. particular interest hyperparameters tuned individually data compared work shared almost hyperparameters including embedding hidden size data sets. this used less model parameters model found shallow lstms layers worked best like work melis underlying lstm architecture highly effective compared complex custom architectures well tuned hyperparameters used. approaches used work melis complementary would worth exploration. mikolov zweig mikolov zweig cache mikolov zweig mikolov zweig rnn-lda mikolov zweig rnn-lda cache zaremba lstm zaremba lstm ghahramani variational lstm ghahramani variational lstm ghahramani variational lstm ghahramani variational lstm charcnn merity pointer sentinel-lstm grave lstm grave lstm continuous cache pointer inan variational lstm augmented loss inan variational lstm augmented loss zilly variational zoph cell zoph cell melis -layer skip connection lstm table single model perplexity validation test sets penn treebank language modeling task. parameter numbers estimates based upon understanding model reference merity models noting tied weight tying embedding softmax weights. model awd-lstm stands asgd weight-dropped lstm. inan variational lstm inan variational lstm augmented loss grave lstm grave lstm continuous cache pointer melis -layer lstm melis -layer skip connection lstm table single model perplexity wikitext-. models noting tied weight tying embedding softmax weights. model awd-lstm stands asgd weight-dropped lstm. substantial improvements underlying neural language model remained open question effective pointer augmentation especially improvements weight tying mutually exclusive ways. neural cache model added pre-trained language model negligible cost. neural cache stores previous hidden states memory cells uses simple convex combination probability distributions suggested cache language model prediction. cache model three hyperparameters memory size cache coefﬁcient combination ﬂatness cache distribution. tuned validation trained language model obtained require training themselves making quite inexpensive use. tuned values hyperparameters respectively. tables show model improves perplexity language model much perplexity points points smaller gains reported grave used lstm without weight tying still substantial drop. given simplicity neural cache model lack trained components results suggest existing neural language models remain fundamentally lacking failing capture long term dependencies remember recently seen words effectively. understand impact pointer model speciﬁcally validation perplexity detail contribution word cache model’s overall perplexity table compute total difference loss function value between lstm-only lstm-with-cache models target words validation portion wikitext- data set. present results difference opposed mean since latter undesirably overemphasizes infrequently occurring words cache helps signiﬁcantly ignores frequently occurring words cache provides modest improvements cumulatively make strong contribution. largest cumulative gain improving handling <unk> tokens though instances. second best improvement approximately ﬁfth gain given <unk> tokens meridian word occurs times. indicates cache still helps signiﬁcantly even relatively rare words demonstrated churchill blythe sonic. cache beneﬁcial handling frequent word categories punctuation stop words language model table total difference loss given word results instances validation data wikitext- continuous cache pointer introduced. right column contains words twenty best improvements left column twenty deteriorated table present values validation testing perplexity different variants best-performing lstm model. variant removes form optimization regularization. ﬁrst variants deal optimization language models rest deal regularization. model using learning rate reduced using nonmonotonic fashion signiﬁcant degradation performance. stands empirical evidence regarding beneﬁt averaging iterates. using monotonic criterion instead also hampered performance. similarly removal ﬁne-tuning step expectedly also degrades performance. step helps improve estimate minimizer resetting memory previous experiment. process ﬁne-tuning repeated multiple times found little beneﬁt repeating once. table model ablations best lstm models reporting results validation test penn treebank wikitext-. ablations split optimization regularization variants sorted according achieved validation perplexity wikitext-. pivotal ensuring state-of-the-art performance. extreme perplexity jump removing hiddento-hidden lstm regularization provided weightdropped lstm. without hidden-to-hidden regularization perplexity rises substantially points. line previous work showing necessity recurrent regularization state-of-the-art models also experiment static sequence lengths hypothesized would lead inefﬁcient data usage. also worsens performance approximately perplexity unit. next experiment reverting matching sizes embedding vectors hidden states. signiﬁcantly increases number parameters network leads degradation almost perplexity points attribute overﬁtting word embeddings. could potentially improved aggressive regularization computational overhead involved substantially larger embeddings likely outweighs advantages. finally experiment removal embedding dropout ar/tar weight decay. cases model suffers perplexity increase points hypothesize insufﬁcient regularization network. work discuss regularization optimization strategies neural language models. propose weight-dropped lstm strategy uses dropconnect mask hidden-to-hidden weight matrices means prevent overﬁtting across recurrent connections. further investigate averaged nonmonontonic trigger training language models show outperforms signiﬁcant margin. investigate regularization strategies including variable bptt length achieve state-of-the-art perplexity wikitext- data sets. models outperform custom-built cells complex regularization strategies preclude possibility using optimized libraries nvidia cudnn lstm. finally explore neural cache conjunction proposed model show improves performance thus attaining even lower state-of-the-art perplexity. regularization optimization strategies proposed demonstrated task language modeling anticipate would generally applicable across sequence learning tasks.", "year": 2017}