{"title": "A modular architecture for transparent computation in Recurrent Neural  Networks", "tag": ["cs.NE", "cs.AI", "cs.CL", "cs.FL", "cs.SC"], "abstract": "Computation is classically studied in terms of automata, formal languages and algorithms; yet, the relation between neural dynamics and symbolic representations and operations is still unclear in traditional eliminative connectionism. Therefore, we suggest a unique perspective on this central issue, to which we would like to refer as to transparent connectionism, by proposing accounts of how symbolic computation can be implemented in neural substrates. In this study we first introduce a new model of dynamics on a symbolic space, the versatile shift, showing that it supports the real-time simulation of a range of automata. We then show that the Goedelization of versatile shifts defines nonlinear dynamical automata, dynamical systems evolving on a vectorial space. Finally, we present a mapping between nonlinear dynamical automata and recurrent artificial neural networks. The mapping defines an architecture characterized by its granular modularity, where data, symbolic operations and their control are not only distinguishable in activation space, but also spatially localizable in the network itself, while maintaining a distributed encoding of symbolic representations. The resulting networks simulate automata in real-time and are programmed directly, in absence of network training. To discuss the unique characteristics of the architecture and their consequences, we present two examples: i) the design of a Central Pattern Generator from a finite-state locomotive controller, and ii) the creation of a network simulating a system of interactive automata that supports the parsing of garden-path sentences as investigated in psycholinguistics experiments.", "text": "computation classically studied terms automata formal languages algorithms; relation neural dynamics symbolic representations operations still unclear traditional eliminative connectionism. therefore suggest unique perspective central issue would like refer transparent connectionism proposing accounts symbolic computation implemented neural substrates. study ﬁrst introduce model dynamics symbolic space versatile shift showing supports real-time simulation range automata. show g¨odelization versatile shifts deﬁnes nonlinear dynamical automata dynamical systems evolving vectorial space. finally present mapping nonlinear dynamical automata recurrent artiﬁcial neural networks. mapping deﬁnes architecture characterized granular modularity data symbolic operations control distinguishable activation space also spatially localizable network itself maintaining distributed encoding symbolic representations. resulting networks simulate automata real-time programmed directly absence network training. discuss unique characteristics architecture consequences present examples design central pattern generator ﬁnite-state locomotive controller creation network simulating system interactive automata supports parsing garden-path sentences investigated psycholinguistics experiments. keywords automata theory recurrent artiﬁcial neural networks representation theory nonlinear dynamical automata neural symbolic computation versatile shift relation symbolic computation neural dynamics pertinent problems computational neuroscience artiﬁcial intelligence cognitive science. hand symbolic computation generically codiﬁed terms production systems formal languages algorithms automata hand neural dynamics artiﬁcial neural networks described nonlinear evolution laws approaches connect diﬀerent realms research back seminal paper mcculloch pitts networks idealized two-state neurons behave logic gates. furthermore fundamental work kleene minsky demonstrated equivalence networks ﬁnite-state automata thus digital computers later examples connectionist modeling symbolic computation speech perception production models trace mcclelland elman nettalk sejnowski rosenberg important step achieved elman introducing simple recurrent networks prediction devices letters words syntactic categories sentences found number successful applications linguistics cognitive science formal grammars employed generation training sets. training grammatical relations emerged connectivity activation patterns network’s hidden layer could examined clustering principal component analysis problem similar approaches based eliminative connectionism theoretical stance aiming elimination symbolic representations connectionist models emerging representations comparable metric space empirical methods clustering allow inferences syntactic structural relationships symbolic training data. even case contemporary deep-learning reservoir computing approaches featuring large networks randomly recurrently connected nonlinear units reason another branch research call transparent connectionism developed framework vector symbolic architectures here explicitly starts symbolic data structures processes ﬁrst decomposed socalled ﬁller-role bindings used create vectorial images tensor product representations serve training patterns subsequent connectionist modeling. contrast eliminative connectionism representations emerge training great extent opaque representations vsas completely transparent resolved step encoding procedure. depending structure chosen vector space arrives diﬀerent kinds integrated connectionist/symbolic architectures g¨odelizations one-dimensional representations ﬁeld real numbers proper vectorial representations ﬁnite-dimensional vector spaces functional representations inﬁnite-dimensional vector spaces importantly siegelmann sontag used combination g¨odelization localist ﬁnite-dimensional representation prove recursive anns rational weights ramp activation turing machine partial recursive function endowed speciﬁc localist architecture. moreover siegelmann sontag showed r-ann consisting units simulate universal turing machine recent work cabessa extends results r-anns realm interactive computation framework studying systems interact environment throughout computation proving r-anns equivalent power interactive tms. very-large-scale reservoir-like neural network approaches also rely ingredient neural engineering framework employs semantic pointers addressing symbolic representations activation space recent work interface reservoir computing connectionist/symbolic approaches contrast present work focuses parsimonious implementations building upon seminal results siegelmann sontag work moore shown nonlinear dynamical automata piecewise-aﬃne linear dynamical systems unit square simulate dynamics real-time machine represented generalized shift dotted sequences. work ﬁrst extend moore’s results showing support real-time simulation range models computation including limited turing machines achieve relaxing deﬁnition leads novel expressive shift versatile shift enables parsimonious real-time emulation symbolic computation range models. show dynamics mapped dynamics unit square g¨odelization. finally present mapping r-anns symbolic models computation distinguish data operations data control operations. example automata implement symbolic operations control look-up table data string encoding so-called conﬁguration automaton. grammars term rewriting systems operations instead deﬁned substitution/rewriting rules symbolic string application rules controlled conditions. perform symbolic computation vectorial space preserving formulation division data operations data control. basing construction derive architecture also preserves division thus obtaining networks transparent modular parsimonious. importantly operations embedded within architecture propose herein distinguishable activation space also spatially localized still relying distributed representation symbolic data. granular modularity architecture brought relation diﬀerentiates approach previous work important consequences constructive mapping interactive automata networks r-anns possibility correlational studies electrophysiological data discuss subsequent sections. illustrate approach means examples. ﬁrst example construct central pattern generator ﬁnite-state automaton gait patterns quadruped animals neuronal sequential activations cpgs usually modeled networks coupled nonlinear oscillators undergo symmetry-breaking bifurcations changes driving input show construction although symbolically inspired allows investigation similar bifurcation scenarios. additionally results example relevant design cpgs control robotic locomotion second example show approach ideally suited tackle mapping interactive machines neural networks separation network architecture data transformations control. makes straightforward construct r-anns simulating networks automata e.g. share states organized complex hierarchies bound interactions conditions application symbolic transformations. demonstrate constructing interactive automata network implements diagnosis repair parser syntactic language processing subsequently mapping r-ann performing computation. able derive vectorial observables network; speciﬁcally compute synthetic event-related brain potentials discuss relation event-related potentials measured experiments involving garden-path sentences local ﬁeld potentials linear transformation layer machine conﬁguration layer nonlinear dynamical automaton principal component analysis push-down automaton present section outlines general method allows mapping range models computation r-anns. figure summarize complete mapping procedure accompany exposition. construction two-step process. ﬁrst deﬁne versatile shift emulates model computation subsequently encode dynamics unit square g¨odelization obtaining two-dimensional piecewise aﬃne-linear unit square i.e. nda. second step mapped onto ﬁrst-order r-ann endowed architecture captures nda’s three components state encoding symbolic data model computation; aﬃne-linear transformations encoding operations data; iii) switching rule selects relevant aﬃne-linear transformation apply given state thus implementing control symbolic operations. next theoretical methods employed discussed detail. presentation various objects formal language theory automata theory essentially follow well-established deﬁnitions hopcroft ullman sipser theory symbolic dynamics tool study dynamical systems based discretization time space order interpret trajectories vectorial space discrete sequences inﬁnite strings symbols. importantly theoretical apparatus also used opposite mapping sequences strings vectorial space. start redeﬁning representation strings symbols dotted sequence. figure automaton mapped recurrent artiﬁcial neural network representation machine conﬁgurations dotted sequences allows mapping machine transition function action versatile shift upon said sequences simulating computation performed automaton. g¨odel encoding acts bridge symbolic vectorial representation automaton’s dynamics enables representation aﬃne-linear nonlinear dynamical automaton finally generates r-ann speciﬁc network architecture internal dynamics operates mnemonic sign indicating index right. shift space i.e. shifts symbols place left similarly possible deﬁne inverse shift shifting symbols place right notice shifting dotted sequence left right resembles movement read-write head turing machine tape order fully attain power turing machines moore endows shift space three additional maps depend content within determines number left shifts right shifts shift maps symbols within onto symbols symbols outside mapped onto auxiliary symbol finally composition operator overwrites symbols within images changing outside according moore’s proof turing machine realized mgs. since turing machines programmed simulate computation carried model lower equal computational power ﬁnite-state automata push-down automata paper moore actually deﬁnes ﬁnite integers need consecutive introduces second ﬁnite integers domain eﬀect indicate cells rewritten nevertheless always possible given arbitrary construct equivalent deﬁned here; thus decided propose simpliﬁed deﬁnition. implies also described terms equivalent gss. practice however simulating automata turing machines lead rather complicated machine tables even simplest symbolic algorithms thus unnecessarily complicated shift spaces. fact diﬀerent automata implement diﬀerent atomic operations turing machine require multiple computation steps simulate single computation step another automaton even automaton computationally less powerful. therefore introduce novel shift space shall henceforth refer versatile shift allow represent automata conﬁguration dynamics dotted sequences straightforward parsimonious fashion simulating real-time. construction essentially relies redeﬁnition concept dotted sequence. above used mnemonic symbol without functional implication. introduce meta-symbol concatenated words v.v. denote dotted words. moreover notion dotted sequence follows. symbols wαvwβ dotted word deﬁnition indices inherited dotted word thus explicitly prescribed. whereas rewrite symbol endowed general rewriting operation substituting dotted words dotted words equal diﬀerent lengths adds expressiveness allowing parsimonious real-time simulation range automata figure diﬀerence substitution operation generalized versatile shifts. figure show example substitutions respectively generalized shift versatile shift. generalized shift rewrite symbol dotted word domain dependence versatile shift substitute dotted word arbitrary dotted word. point worth noting endowing rewriting capability extends direction semi-thue systems universal model computation introduced axel thue rewriting systems play important role example algebraic speciﬁcations abstract data structures equational programming program transformation automated theorem proving conditional successive application ﬁnite rewrite rules transforms given symbolic structure. discuss range automata simulated real-time choosing appropriate dotted sequence representations machine conﬁgurations constructing reproduce machine’s operations conditional application. finite-state machines. ﬁnite-state machine model computation introduced mcculloch pitts widely used describe systems many application ﬁelds ranging computer science engineering biology name few. every step computation ﬁnite states change state result incoming input signal. formally deﬁned -tuple mfsm ﬁnite control states input alphabet starting state accept states transition function deﬁned follows computation step reads current state consumes current input symbol transitions state prescribed transition function. possible encode conﬁgurations dotted sequences push-down automata context-free grammars. push-down automaton computing machine sequential access input manipulate stack memory popping pushing symbols formally deﬁned -tuple mpda ﬁnite control states stack alphabet automata computation neural networks outside scope work follows discuss deterministic pdas. determinism thus implied point transition function deﬁned follows computation step consumes input symbol pushes pops symbol stack changes state prescribed transition function applied current state currently read input symbol current top-of-stack symbol particular current content stack transitions form recognize class languages generated context-free grammars cgfs thus equivalent power. speciﬁes language i.e. strings alphabet deﬁning words constructed moving distinguished starting symbol applying substitution rules string unsubstitutable symbols reached. formally deﬁned -tuple non-terminal symbols terminal symbols substitution rules distinguished start symbol. particular rule written example deﬁne generates language balanced round square brackets. language. illustration purposes example derivation would lex. always possible construct given top-down recognizers. examples presented later text make top-down recognizers process locally unambiguous non-left-recursive cfgs. tdrs subclass simulate rule expansion accept languages generated non-left-recursive cfgs. given leftrecursive possible construct parse strings belonging context-free language generated grammar. input string constructed language generated grammar computation empty stack input said accept string empty stack. speciﬁcally interested tdrs process locally unambiguous cfgs additional property needing state perform computation. construct locally unambiguous non-left-recursive suﬃcient deﬁne function following non-terminal string terminals non-terminals tdr’s state. note deﬁnition endow tdrs additional capability pushing strings stack rather single symbols. respectively unconsumed input content stack automaton reverse order time similarly simpler needed simulate pdas constructed tdr’s transition function deﬁning domain dependence turing machines. turing machine automaton read-write random access two-sided inﬁnite tape central theory computation thought powerful enough model physically realizable computation in-built tape ﬁnite-state controller endowed read-write head follows instructions encoded transition function. step computation given current state current symbol read read-write head controller determines transition function writing symbol current memory location shift read-write head memory location left right current deﬁned -tuple containing blank symbol input alphabet starting state ‘halting’ states reached computation {lr} partial transition given computation step content tape together position read-write head current controller state deﬁne machine conﬁguration. possible encode conﬁgurations dotted sequences follows describes part tape left read-write head describes part right describes current state machine controller central denotes current position read-write head i.e. symbol right. conﬁguration would yield conﬁguration waq.rd; running again time starting waq.rd computation step would yield wq.and prescribed transition function deﬁned. constructing speciﬁed equation applying wq.ord discuss thus models symbolic computation simulate mapped piecewise aﬃne-linear systems vectorial space obtaining nonlinear dynamical automata. g¨odel encoding allows uniquely assign real number sequence space one-sided inﬁnite sequences mapped real interval completeness g¨odelization subsequently discussed alongside graphical representation provided figure g¨odel encoding maps sequences alphabet real numbers proven base-b expansion represents real number real number unique base-b representation weak condition. uniqueness g¨odel encoding sequence follows proof. figure three representations g¨odel encoding sequence. ﬁrst deﬁnition g¨odel encoding details speciﬁc choice enumerating function induction constant given alphabet sequence takes symbols. second expansion series deﬁnition. third visually conveys fractal convergent nature series highlighting relation numbers symbols color orange. level representation bottom encoding show g¨odelizing sequence resulting application push operations equivalent applying aﬃne-linear transformation original g¨odelized sequence. show dotted sequence mapped push operations proving resulting g¨odelized sequences obtained applying aﬃne-linear transformations original g¨odelized sequences. push operations parameters aﬃne-linear transformations depend number identities symbols respectively removed added beginning original sequence. particular importance framework interactive computation newly added symbol stems network’s interaction environment. accordingly symbol becomes represented linear operator acting system’s state space analogous quantum operators acting hilbert spaces shift dotted sequence shift operation shifting symbols left right positions. wαu.vwβ ˆu.ˆv substitution replacing dotted sub-sequence dotted word ˆu.ˆv straightforwardly mapped push operations showing substitutions dotted sequences mapped push operations one-sided constituents. left shift right shift dotted sequence mapped push operations one-sided constituents follows obtained composition left right shifts; composition aﬃnelinear transformations aﬃne-linear transformation g¨odelization sequence resulting composition shift operations equivalent aﬃne-linear transformation original g¨odelized sequence. thus shown dotted sequences mapped push operations one-sided inﬁnite sequences g¨odelization operations mapped aﬃne-linear transformations original sequences. symbologram substitution shift operation g¨odelized dotted sequence involves aﬃne-linear transformations acting g¨odelized g¨odelized parameters aﬃne-linear transformations depend symbols dotted sequence dotted sequences share symbols thus associated pair aﬃne-linear transformations. reason symbologram representation leads piecewise aﬃne-linear maps rectangular partitions unit square referred nonlinear dynamical automata particular interval contains dod-agreeing g¨odelized sub-sequences whereas interval contains dodagreeing g¨odelized sub-sequences. leads partition unit square number intervals equal number possible onesided sub-sequences appear left number intervals equal number possible one-sided sub-sequences appear right dod. example simulating left domain dependence dodα dotted sequences representing dependence dodβ ever contains input symbols. case number intervals becomes equal number states respectively states input dependence dodα {−−} ever contains states index tape symbols index right domain dependence dodβ always contains tape symbols. leads partition unit square number intervals equal nqns intervals equal leading total cells number symbols tape alphabet number states following section substitutions shifts sequence mapped aﬃne-linear transformations g¨odelization. reason cell partition unit square associated diﬀerent aﬃne-linear transformation parameters derived using methods outlined section therefore model computation represented means g¨odelized representation. figure connectivity neural layers within network. machine conﬁguration layer receives external input synaptically couples branch selection layer linear transformation layer feed-forwards ﬁnally recurrently feedbacks output read-out. emulate transitions cell cell suitably activating certain neural units within r-ann. achieve this propose network architecture three layers namely machine conﬁguration layer branch selection layer linear transformation layer depicted figure therefore generically deﬁne proposed follows deﬁnes network architecture discussed subsequent sections. addition generates diﬀerent neural dynamics type neural units i.e. corresponding respectively. details r-ann architecture dynamics presented. simulation orbit within r-anns distributed among ltl. since two-dimensional de-coupled discrete suggests neural units read-out layer role taken mcl. refer units figure activation functions employed network. particular heaviside function employed units branch selection layer ramp function used machine conﬁguration layer linear transformation layer. computation step stores encoding current machine conﬁguration passed units. subsequently sets units functionally switching system determines cell current machine conﬁguration belongs triggering appropriate units within sets units eﬀectively emulating application aﬃne-linear transformation encoded machine conﬁguration. action corresponds application symbolic operation original machine leading conﬁguration update. result transformation back representing conﬁguration next computation step. successive transformations eﬀectively emulate action every computational step aﬃnelinear transformation applied values encoding representation machine conﬁguration. neural units various layers make either heaviside ramp activation functions deﬁned follows time mediates computation step transmission current g¨odel encoding emulated machine’s conﬁguration units. since g¨odel encoding dotted sequence representing machine conﬁguration consists values implies solely requires neural units code current conﬁguration. consequence initialization r-anns following every computation step neural units receive inputs units subsequently activated ramp activation function y)). finally synaptically project onto neural units acts control unit enables sequential mapping orbits orbits r-anns speciﬁcally functionally embodies switching rule coordinates dynamic switching units. sequentially action dedicated emulate units single pair units become active i-th interval x-axis j-th interval y-axis. switching rule mapped follows implementation mediated sets neural units units units activated heaviside activation function receiving excitatory inputs synaptic weight layer following figure detailed feedforward connectivity weights neural network simulating nonlinear dynamical automaton branches. machine conﬁguration layer units feed-forward connect branch selection layer units weight every unit excites weights also inhibits weights relevant linear transformation layer units contained within cell cell indicates overall summed input value received unit bsl. case units cell activated overall input value zoom-in panel shows detail pair units contained within cell receives inputs units shown. addition units internal dynamics described parameter actually produce output overall input unit must overcome internal inhibition. upon activation unit’s output back paired unit weight activation units depends threshold implemented synaptic projection always-active bias unit deﬁned minimum intervals respectively units. eﬀect centering threshold towards left boundary interval therefore read-out units corresponded point unit square belonging cell would occur neurons upon excitation units synaptically project relevant units naturally inactive strong inhibitory bias magnitude speciﬁcally neural unit establishes synaptic excitatory connections units within cells also project synaptic inhibitory connections graphical depiction figure similarly projects synaptic excitatory connections units within cells also projects synaptic inhibitory connections units within cells dj−k figure combined eﬀect therefore counterbalance synaptic weights natural inhibition units cell dij. words couple units receives input triggers relevant unit reaches value selected units dij) pair units cell inhibits units excites units cell inhibits units cells units excite respectively cells inhibit cells within cell formation cross threshold value execute operation. read-out process follows results fact inputs contribute respectively half necessary excitation counterbalance ltl’s natural inhibition units also receive inputs units respectively modulated synaptic weights units cross threshold intrinsic constant neural dynamics completes desired aﬃne-linear transformation. read-out updated encoded machine conﬁguration synaptically projected back units initiating next computation step restricted ﬁrst principal axis resulting scalar variable could conceived measure overall activity neural network important scalar observables discussed literature smolensky’s harmony development biophysically inspired observation models important research ﬁeld computational neuroscience could eventually lead synthetic local ﬁeld potentials electroencephalogram event-related brain potentials shall amari’s measure derive synthetic erps follows. number sub-sequences appear respectively left right domain dependence r-ann constructed. total units units nαnβ units bias unit establish synaptic biases. point worth mentioning original formulation relied simple g¨odel encoding machine conﬁgurations subsequent work highlighted advantages using ﬂexible representation employing cylinder sets order preserve important structural relationships symbolic descriptions facilitate modeling r-ann extended incorporate cylinder encoding machine conﬁgurations simply doubling layer. important modeling issue consider halting conditions i.e. consider computation terminated. consequently model depend deﬁne explicit halting conditions. however equally reasonable choices halting conditions could employed follows. ﬁrst using homunculus external observer decides intervene computation condition second using ﬁxed point condition implementing machine halting state identity branch nda. halting conﬁguration result ﬁxed point thus halting homunculus could appropriate context interactive computation constant non-terminating interaction environment assumed cognitive modeling diﬀerent kinds ﬁxed points either desired unwanted ones required order describe sequential decision problems linguistic garden paths present examples demonstrate strength developed methodology mapping automata computation r-ann computation real-time source code examples freely accessible carmantini fsms basis many state-of-the-art approaches construction locomotion controllers articulated robots easy design implement debug relation animal gait well characterized hand recent research robot locomotion control shows increasing interest towards alternative approaches based cpgs neural networks capable producing rhythmic patterns activation absence rhythmic input sources. paper ijspeert presented beneﬁts drawbacks cpgs respect approaches robot locomotion control. brieﬂy summarize beneﬁts identiﬁed author rhythmic behavior supported cpgs robust transient perturbation state variables; cpgs well-suited distributed implementations iii) cpgs reduce dimensionality control problem introducing high-level control parameters allowing modulation locomotion; cpgs ideally suited integration sensory feedback coupling terms diﬀerential equations controller; cpgs often work well learning optimization algorithms. hand speciﬁed author cpg-based approaches still lacking sound design methodology theoretical grounding description. example presented section show mapping could design cpgs producing arbitrary patterns locomotion robots starting description desired rhythmic pattern. combining approaches design controllers beneﬁts solid theoretical grounding fsm-based locomotion ease design implementation. contextualize derived terms familiar animal locomotion qualitatively model results well-known experiment gait. seminal work shik applied diﬀerent levels electrical stimulation midbrain decerebrated cat. authors observed transitions gait animal increasing level stimulation applied eliciting ﬁrst walk trot ﬁnally gallop gait. theoretical framework qualitatively reproduce experimental observations deriving r-ann generates relevant gait patterns reproduces transition function applied stimulus strength. keep exposition simple consider walk gallop gaits transition two. study mammalian quadruped gait four legs numbered gait associated certain sequence given order legs touch ground gait cycle. left right hind legs associated respectively numbers left right fore legs associated respectively numbers gait cycle assumed start left hind touches ground. walk gait thus deﬁned sequence gallop gait deﬁned sequence high level computation carried charge producing gait patterns quadruped mammalian informally stated stimulation midbrain sequentially activate legs following pattern high sequentially activate legs pattern implement level high level stimulation input symbols construct transition function sequentially reproduce patterns switching states. thus deﬁned table table state transition table simulated central pattern generator ﬁnite-state automaton. possible observe diﬀerent input leads diﬀerent produced patterns implemented sequences states. step-by-step dynamics derived r-ann observed figure machine’s input substrate external stimulus ultimately encoded neural unit within rann shown bottom plot figure note manipulate activation gradually increase high level stimulation. introduce continuous control parameter originally pure symbolic model enabling carry bifurcation study analogy traditional coupled oscillator models stimulation rann deﬁned mapping qualitatively reproduces features involved locomotion transitions described shik particular possible observe levels stimulation elicit production walk gait cycle whereas increase level stimulation induces sudden transition gallop gait cycle. relation stimulation level computation carried network related underlying symbolic space thanks mapping depends upon informed decision gamma numbering states g¨odel encoding. fact chosen gamma numbering ensures unit square encoding machine conﬁgurations <lo> current input symbol corresponds points deﬁned equation speciﬁcally γsg− terms underlying representation increasing activation value reaches exceeds corresponds forcing encoded machine state cross boundary cells associated <lo> input symbol associated <hi> input symbol thus causing transition walk gallop gait. note example model halting conditions derived network clear halting means context computation performed cpgs. summarize derived description locomotion controller inspired results generation gait patterns midbrain. outlined design methodology cpgbased locomotion control robots suﬀer drawbacks approaches grounding description design theoretical grounding fsm-based approaches. problematic aspects methodology outlined discretetime nature mapping. fact fully realizing beneﬁts cpg-based approaches summarized beginning section requires continuous time models. notwithstanding believe proof concept provide already shows encouraging results future developments. additional remark methods describe paper ideally suited deriving neural networks implementing paradigms interactive computation demonstrate shortly. especially relevant design cpgs. fact recent research unveiled surprising degree hierarchical organization mammalian respiratory cpgs allows highly robust ﬂexible pattern production adapt variety conditions methodology easily accommodates mapping hierarchies automata hierarchically organized neural networks demonstrate next example modeling garden-path parsing concept employed language processing importantly networks automata could used design complex pattern generation modular robots figure recurrent artiﬁcial neural network functioning central pattern generator. network reproduces qualitative behavior locomotive central pattern generator described shik bottom plot level stimulation applied network neuron shown. three plots levels activation neural unit three layers shown time step. note diﬀerent patterns walk gallop generated depending level stimulation. results original ﬁnite-state machine programmed. real-world computing. classical automata theory interaction automaton external world restricted input-output relation. external world provides input automaton performs computation input returns output external world. within framework interactive computation instead automata interact external world every step computation. external forces conﬁguration automaton conﬁguration aﬀect external world. clearly framework provides much richer language describe models computation especially useful express notions compositionality concurrency. constructs essential study modern computing systems also context cognitive modeling. example build model human processing locally ambiguous sentences constructing network interactive automata. proof-of-concept want demonstrate ﬂexibility approach showing seamlessly used construct neural networks implementing interactive systems. order choose system simple enough allow clear exposition complex enough carry meaningful computation; composed range diﬀerent automata; iii) incorporates diﬀerent forms interaction automata components. garden-path sentences locally ambiguous sentences induce temporary production erroneous parse reader forced reconsider interpretation previously presented material order ﬁnally reach correct parse. consider example sentence convinced children noisy. reading sentence reader ﬁrst constructs intermediate parse children object phrase convinced. reading rest sentence reader realizes intermediate parse incorrect object convinced children noisy subordinate clause. reader thus reanalyzes sentence produce correct parse. osterhout shown reanalysis sentence garden-path associated brain reader positive deﬂection milliseconds onset garden-path word example sequentially presented sentences measured trial averaged electroencephalogram eﬀects. model implement reanalysis diagnosis repair mechanism described lewis account parser tries incrementally build parse sentence material presented. dead-end reached parser diagnoses need reanalysis search space possible continuations parse modiﬁed repair operator bridges dead-end another point search space allowing parser correctly complete processing sentence. parser model create implements mechanism process garden-path sentences local ambiguity given incorrect assignment subject object grammatical constituents. many languages native speakers shown prefer interpret ambiguous nominal constituent subject rather object. consider example following sentences extracted study ambiguous pronouns frisch german speakers. sentences start psycholinguistic study frisch shown reader ﬁrst tries apply preferred subject-object parsing strategy clause reader thus initially interprets subject clause nominative case expecting followed object accusative. upon reading however realize schmuggler nominative case instead thus subject. leads reader reconsider previous material correctly parse pronoun accusative case direct object verb sah. reanalysis observed eﬀect erp. high level abstraction capture structure sentences production rules model thus split grammar grammars gs-o go-s comprising respectively production rules reﬂecting existence strategies parsing sentences subject/object pronoun ambiguity. recognize diﬀerent sentence structures model endowed specialized tdrs constructed gs-o go-s grammars shown section initially tried input model subject-object interpretation preference. case fails garden path model acts prescribed diagnosis repair account. ﬁrst diagnoses problem arisen parsing repairs parse ﬁnally switches strategy correctly parse input. order implement diagnosis step model needs monitor state parse extract relevant diagnostic information. implement diagnosis compares current parse previous time step; parse didn’t change means parser stuck can’t process input further. case diagnosis changes state error state thus implementing diagnosis step. repair step realized introducing repair described following rewriting rule table state transition table diagnosis push-down automaton input machine parse produced top-down recognizers state input symbol stack symbol machine pushes input stack order able compare current input previous time step. particular input symbol top-of-stack blank symbols machine transitions idle state signaling nothing happening; current input previous time step diﬀerent machine transitions parsing state signaling tdrs successfully parsing input; current input previous time step parsing input stuck machine transitions error state. corresponding reanalysis ambiguous sentence terms dispreferred object-subject sentence structure. sentence reanalyzed thus parse repaired second parser proceed process input completely consumed stack emptied. order switch strategies model needs higher-level controller access diagnostic information current parse decides parsing strategy apply. particular controller ﬁrst activate preferred tdr. parser failed higher-level controller ﬁrst activate repair allow reanalysis ambiguous sentence subsequently activate tdr. implement high level controller strategy endowed capability selectively activating tdrs well repair switching internal state. machine receives diagnostic information provided diagnosis input. three states namely state repair state state. switching states activate respective automata. note form interaction deﬁned introduced section deﬁne call shifts. extending incorporate notions table state transition table strategy ﬁnite-state machine input machine diagnostic information produced diagnosis push-down state. starts state fsmqs-o. fact preferred automaton i.e. parsing strategy implemented top-down recognizer corresponding parsing subject-object sentences tried ﬁrst. fails diagnosis signals error; input sentence subject-object order switch parsing strategy needed. strategy ﬁrst changes state fsmqrepair activating repair versatile shift switch take place. repairing parse leads diagnosis signal parsing started again input strategy becomes pdaqparsing. given pdaqparsing input fsmqrepair current state moves fsmqo-s state leading activation input parsed. compositionality concurrency allow reﬁning mapping presented paper reﬂect capabilities. moment want demonstrate possibilities opened present work; reason implement subroutine capability neural network familiar mechanism already encountered previous sections ignoring momentarily missing theoretical details leaving deﬁnition future work. avoid race conditions automaton interactive network re-write symbols sub-sequence given computation step parse sub-sequence read re-written diagnosis pda. similarly diagnosis sub-sequence read re-written strategy fsm. furthermore selective activation repair operated strategy ensures given computation step automata perform symbolic re-writing input parse sub-sequences. figure interactive automata network parsing garden path sentences. ﬁgure shows complete system described section simplicity show various automata components acting conﬁgurations represented dotted sequences. dotted sub-sequences color coupled i.e. intents purposes sub-sequence. example parse dotted sub-sequence contains current stack top-down recognizers repair versatile shift time input tape diagnosis push-down automaton similarly diagnosis sub-sequence stores current state stack diagnosis time input tape strategy ﬁnite-state machine note second form interaction allowed sharing dotted sequences present automaton. particular tdrs repair activated based state strategy fsm. strategy ﬁrst converted acting dotted sequences mapped representation ﬁnally r-anns. g¨odelizations input parse strategy sub-sequences deﬁned equation gamma enumerating functions deﬁned follows enumerating states diagnosis γparse enumerating stack symbols. mapped machines r-ann derived networks components overall system architecture order simplify exposition construct overall network feature recurrent connections. endow architecture conﬁguration layers containing strategy diagnosis parse input sub-sequences. next network components derived automata connected perform part processing relevant subsequences. particular representation automaton acts dotted sequence input associated network component connected units encoding subsequences i-th whereas output connected units encoding ﬁnal connected subsequence last connected weight unit ﬁrst cl). finally implement subroutine call capabilities strategy meta branch selection layer takes strategy subsequence input connected lateral inhibition connection pattern speciﬁed section tdrs repair note creates nested structure repair functioning higher-level symbolic figure show network activation diﬀerent sentence structures presented input. particular note serial activation repair sub-networks object-subject sentence presented. mapping parser machine evolving symbolic space neural network evolving vectorial space able compute synthetic event-related potentials synth-erps trial-averages mean network activation discussed figure achieved calculating mean global network activation according amari simulation trials input stimulus random initial conditions compatible symbologram representation input prepared according beim graben brief symbologramcompatible random initial conditions generated g¨odelization sequences form wαu.vwβ dotted sequence describing figure reveals network shows p-like eﬀect processing garden-path sentences peak increased sustained activation respect control condition. simpliﬁed model garden-path processing presented allow direct quantitative comparison experiments frisch simulations could starting point detailed statistical correlation analyses future work relating computations electrophysiological measurements. study developed constructive transparent modular parsimonious mapping symbolic algorithms neural networks. ﬁrst introduced novel shift versatile shift extends generalized shift allows real-time simulation range symbolic models computation. showed represented vectorial space g¨odelization obtaining piecewise aﬃne-linear systems figure garden-path parsing network architecture. order simplify exposition construct network recurrent connection last ﬁrst layer network note parser sub-network composed top-down recognizer repair versatile shift sub-networks. arranged cells linear transformation layer selectively activated meta branch selection layer controlled strategy neural unit. figure network activation subject-object object-subject sentence presentation. notice serial activation repair sub-networks case object-subject sentence presentation longer tail activation reﬂecting additional computation needed process dispreferred input. figure synthetic event-related brain potential mean network activation random cloud initial conditions. ﬁgure show mean global network activation calculated equation time step simulations averaged trials. simulations network presenting time random inputs generated compatibly symbologram representation sequences. words noise added input that input generated g¨odelizing sequence length decoding input would yield original sequence ﬁrst symbols rest random symbolic continuation. stronger noise added instead would prevented network correctly perform computation would destroyed essential input information. blue show averaged mean activation standard deviation presented input encoding sequence s.so representing input sequence subject-object order i.e. network’s preferred order explained section note parsing completed averaged mean activation standard deviation input encoding sequence s.os representing input sequence object-subject order leading garden path parsing input. time diagnosis repair steps carried symbolic interactive system indicated arrows. also report bottom plot conﬁguration parser networks dotted sequence time step respectively garden path control condition. note garden-path processing associated strong divergence activation starting time followed longer tail network control condition. reﬂects additional computation needed network successfully resolve garden path parsing qualitatively corresponds event-related brain potential measured psycholinguistics experiments furthermore note conditions network starts returns resting state waiting input process external world implementing notion continuous computation hallmark interactive systems. unit square known nonlinear dynamical automata finally presented modular r-ann architecture simulates dynamics nda. proposed architecture consists three layers machine conﬁguration layer representing state thus symbolic data simulated automaton; branch selection layer implementing switching rule thus characterizing automaton’s decision space control; linear transformation layer implementing piecewise aﬃne-linear functions i.e. vectorial representation symbolic operations deﬁned transition table simulated automaton. additionally linear transformation layer modular operation speciﬁed transition function simulated automaton applied speciﬁc pair units layer. mapping used simulate turing machine ranns thus making architecture universal particular possible simulate -states -symbols minsky real-time r-ann consisting units -states -symbols neary woods consisting units. important analyze modeling choices made r-ann architecture described. choice worth discussing implementing biases synaptic projections always-active unit opposed implementing parameters intrinsic individual units. decided simplicity bias unit. nonetheless parameterized bias would equally reasonable. strong bearings model discussed interesting note speciﬁc choice implementation less emphasis predominantly synaptic computation versus computation distributed synaptic neuron level reﬂecting similar issues considered biological domain. second consideration concerns cell’s boundaries nda. fact distance right bound cell left bound next zero. poses challenges even extremely small noise state vector boundary lead erroneous application switching rule real state thus disruption computation. course reﬂected dynamics associated r-ann well. siegelmann sontag solve issue using cantor encoding opposed simple g¨odelization ensuring greater zero distance encoded conﬁgurations diﬀerent leading symbols. methods applied here. interestingly switching cantor encoding heaviside units layer substituted functionally equivalent ramp units r-ann would make linear units siegelmann sontag. compared eliminative approaches work allows direct interpretation representations dynamics derived network terms symbolic computation. many important consequences. first conventional neural networks trained large data sets method require training synaptic weight matrix explicitly designed machine table encoded automaton. emergent representations operations opaquely encoded several hidden layers transparently realized g¨odelization symbolic conﬁgurations. second even considering learning applications plan explore future developments derived approach could bring exciting possibility symbolic read-out learned algorithm network weights; note architecture weights necessarily ﬁxed exception connections encoding symbolic operations simulated automaton i.e. layer. third anchoring computation network well-understood computation models worthwhile tackling problems beneﬁt integration perspectives. ﬁrst example constructed r-ann performing machine computation abstracting animal locomotion. fsms widely used locomotion controllers robotics simplicity strong theoretical grounding relation animal locomotion. hand neural implementations many desirable characteristics present fsm-based implementations diﬃcult engineer. showed integrating approaches tackle problem pattern generation robotic locomotion eﬀectively. course satisfactory solution would entail continuous-time models mapping; nevertheless preliminary results already present distinct beneﬁts integration approaches compared isolation. fourth complete understanding network’s inner workings allows intelligent manipulation parameters. discussed example understanding computation carried derived network allowed introduce continuous control parameter eliciting bifurcation dynamics network present systems coupled nonlinear oscillator models widely studied literature. regards previous work transparent connectionism work advances ﬁeld several ways. ﬁrst advancement introducing able simulate broad range symbolic computation models real-time extending original work moore interestingly would straightforward deﬁne n-sided inﬁnite dotted sequences extended these. g¨odelization would obtain n-dimensional hypercube could simulated r-anns straightforward extension architecture presented work. would extend range real-time simulable computational models automata multiple tapes stacks secondly basing construction obtain architecture characterized fully distributed representation coupled granular modularity diﬀerentiating approach previous work granting series advantages. mapping transparent regards representations also regards symbolic operations deﬁned simulated computational model control clearly localizable architecture. regard advance also allows example straightforward mapping interactive automata networks r-anns. fundamental importance framework interactive computation provides rich language description many complex systems example cognitive modeling. second example constructed network interacting automata diagnosis repair model reanalysis linguistic garden path sentences. network consisted three master control program component carrying speciﬁc intelligible task overall computation. mapped network r-ann thus obtaining symbolic/connectionist implementation cognitive model. interestingly multiple levels hierarchical organization present automata network thus derived r-ann could even speculate thermodynamic limit networks number modules approaches inﬁnity presenting emergent scale-free small world properties granular modularity approach also advancement considering possibility correlational studies neurophysiological measurements. previous work showed devise large-scale biophysical observation models order correlate top-down modeling approaches neurophysiological data obtained bottom-up measurements process involves associating neural units model neuronal masses hebbian cell assemblies large-scale brain models investigated e.g. neural ﬁeld theory. setup show observational models lead improved interpretation synthetic event-related brain potentials used computational neurolinguistics studies mental/cognitive states associated metastable states dynamical system. second example presented here computed amari’s mean activation observation model diagnose repair r-ann order obtain synthetic erps qualitatively computed signal exhibited similar divergence conditions measured experiment presented frisch preliminary already encouraging results development approach direction. future work envisage possible selectively correlate electrophysiological measurements speciﬁc components derived r-ann informed suitable symbolic model computation underlying measured quantities. third point interest architecture presents clear spatial organization layout particularly level diﬀerent transformations applied based position g¨odelized automaton data unit square. r-ann architecture implemented performs form spatial pattern matching activating speciﬁc pair units lateral inhibition mechanism. considering extensions models higher complexity functionality could implemented grid units receptive ﬁelds deﬁned example self-organizing maps future work plan overcome fundamental issues current model bearing relation learning applications extension model continuous dynamics. concerns learning algorithms data current model suﬀers missing end-to-end diﬀerentiability g¨odel encodings. serious limitation prevents gradient descent methods training network’s weights. future work address limitation possibly relying methods data access manipulation akin modern r-ann approaches weston graves grefenstette joulin mikolov sukhbaatar encouraging work learning exponential state growth languages fractal learning neural networks could also inform revised trainable architecture. regards extension model continuous dynamics many ways could achieved future work. importantly mostly interested extensions continuous-time models excitable. systems trajectories perturbed away stable equilibrium come back large excursion phase space upon suﬃciently strong input; biophysical examples excitable models initiated hodgkin huxley possibility would ﬁrst extend mapping discrete-time excitable models move continuous time so-called suspension procedures. potential issues endeavor. first would crucial ﬁrst explore understand possible relationships excitable regimes neural models symbolic dynamics computation. answer question excitability property translate realm symbolic computation? think could meaningful answers question tackled framework interactive computation. another potential issue suspension process nonunique non-trivial general case; moreover guarantee excitability crucial matter dealing neural tissue lower brain structures brain stem possible neurophysiologically identify clear small neuronal networks. however neural networks models appropriate level description higher cortical structures presence large highly interconnected neuronal masses. models structures express slow large scale processes measured lfp/eeg. context alternative approach achieve continuous-time dynamics already explored extent previous work framework heteroclinic dynamics turing machine conﬁgurations interpreted metastable states attracting repelling directions framework multipletime scale dynamical systems", "year": 2016}