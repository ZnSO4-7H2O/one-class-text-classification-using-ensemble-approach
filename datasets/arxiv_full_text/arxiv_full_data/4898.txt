{"title": "Associative Adversarial Networks", "tag": ["cs.LG", "cs.AI"], "abstract": "We propose a higher-level associative memory for learning adversarial networks. Generative adversarial network (GAN) framework has a discriminator and a generator network. The generator (G) maps white noise (z) to data samples while the discriminator (D) maps data samples to a single scalar. To do so, G learns how to map from high-level representation space to data space, and D learns to do the opposite. We argue that higher-level representation spaces need not necessarily follow a uniform probability distribution. In this work, we use Restricted Boltzmann Machines (RBMs) as a higher-level associative memory and learn the probability distribution for the high-level features generated by D. The associative memory samples its underlying probability distribution and G learns how to map these samples to data space. The proposed associative adversarial networks (AANs) are generative models in the higher-levels of the learning, and use adversarial non-stochastic models D and G for learning the mapping between data and higher-level representation spaces. Experiments show the potential of the proposed networks.", "text": "propose higher-level associative memory learning adversarial networks. generative adversarial network framework discriminator generator network. generator maps white noise data samples discriminator maps data samples single scalar. learns high-level representation space data space learns opposite. argue higher-level representation spaces need necessarily follow uniform probability distribution. work restricted boltzmann machines higher-level associative memory learn probability distribution high-level features generated associative memory samples underlying probability distribution learns samples data space. proposed associative adversarial networks generative models higher-levels learning adversarial nonstochastic models learning mapping data higher-level representation spaces. experiments show potential proposed networks. generative adversarial network framework relatively type framework introduces generator discriminator often chosen type multilayer perceptron network trained adversarial manner. trained label training examples true outputs false trained maximize probability classiﬁcation errors. recent works reported models trained using framework generate excellent samples nevertheless widely reported models difﬁcult train different techniques proposed work deal difﬁculty optimizing simultaneously synchronized manner joint convergence. proposes train generator objective guides match statistics features intermediate layer discriminator. helps avoid over-training current discriminator discourages learning mappings useful generating realistic data. another major difﬁculty training gans tendency collapse converging parameter setting maps data. uses minibatch discrimination generate different gradients data samples continue guiding backpropagation force diversify outputs avoid collapsing. paper argue using noise generator contributes difﬁculty training gans. generator assigned task learning mapping input signal uniformly distributed noise data space. strong evidence presented better disentangling underlying factors variation data higher-level representation spaces might explain uniformly distributed random noise input works. however learning mapping representation space data space difﬁcult. explained example human-face image generation task. completely disentangled representation space human faces would likely consist features including pose illumination gender mood facial expressions model face etc. generator’s task good mappings representation space face images requires signiﬁcant training. difﬁcult generator network achieve goal might reasons generator collapses training. however mapping higher-level representation assume complete disentanglement factors variation mapping facial areas face image expected easier task. higher-level representation-space features correlated using associative memory generating samples memory input goal alleviate learning task. also mapping data samples lower dimensional higher-level representation space using learning associative memory model causes markov chain faster thanks uniform distribution space associative adversarial networks uses associative memory located generator discriminator networks connecting networks together. explore special case uses restricted boltzmann machine associative memory learns probability distribution features intermediate layer discriminator. gibbs sampling used sample distribution samples figure depicts example present work. main contributions work following introduce higher-level associative memory stochastic generative model connects training celeba face image dataset show convincing evidence associative memory learn probabilistic model higher-level representation space discriminator. produce samples used input generator thereby alleviating learning task. evaluate performance assoicative memory rbm’s performance neural network models generate data getting attention recently ability learn reusable feature representations large unlabeled data sets well generate realistic data samples. generative adversarial networks variational autoencoders generative maximum mean discrepeancy networks deep generative models generative moment matching networks etc. shown deep generative network learn distribution samples. speciﬁcally gans promising family generative models computer vision ﬁeld produce sharp images applying stochastic gradient descent well-deﬁned loss functions. extensions looked laplacian pyramid extensions showing higher quality images recurrent network approach de-convolution network approach demonstrating reasonable success generating natural images. several recent papers focus improving stability training quality generated samples among recent ones following stabilizing training balancing/freezing prevent generator discriminator outpace another minibatch discrimination prevent generator collapse single sample enable back-propagation gradients improve weights historical averaging technique common used game theory uses historical average parameters regularization term optimization. work build techniques. dcgan architectural suggestions proposed radford uses strided convolutions initial layers discriminator fractional-strided convolutions later layers generator discussed experiments section. proposed technique resembles learning algorithm deep belief nets presented utilizes contrastive version wake-sleep algorithm proposed deep belief nets top-level undirected associative memory together directed generative models lower levels. generate samples model gibbs sampling used top-level associative memory sample memory passed stochastically figure associative adversarial network model. intermediate layer discriminator clamped visible layer network sampled generate inputs generator network layer visible associative memory represents feature space capture latent factors variations data. following layers discriminator mostly serve classiﬁcation task embedded objective functions. however actual distinction network layers constraints imposed. three separate networks non-stochastic models stochastic generative model. networks jointly learned. directed generative connections lower layers. model learned performing contrastive version wake-sleep algorithm greedy layerwise initialization process. wake phase stochastic \"up-pass\" starts generative weights adjacent layers updated locally maximize likelihood upper layer samples generating lower level samples. sleep phase stochastic \"down-pass\" starts recognition weights adjacent layers updated similarly. proposed method uses up-pass down-pass. gans class generative models pose training process game generator network discriminator network non-stochastic models. generator network typically chosen feed forward convolutional neural network depending task. produces samples transforming vectors noise discriminator trained taking samples generator negative instances real data pdata positive instances trained distinguish generated samples real data. takes output maps binary classiﬁcation probability. generator tries trick discriminator generating fake samples. learning framework two-player game cast minmax optimization differentiable objective solved greedily iteratively performing gradient descent steps improve eventually reaching nash equilibrium problem formulated zero-sum game distinguishability game value function ﬁrst term cost function forces label real data samples second term forces label fake data samples zero. tries fake labeling output minimizes given cost function tries maximize energy based model unsupervised learning underlying undirected graph. consists layers binary stochastic units visible layer representing data hidden layer latent variables. determines strength interaction hidden visible units. energy function visible hidden variables probability partition function deﬁned below gradient log-likelihood involves positive negative term. positive gradient intractable requires summation values hidden visible variables grows exponentially. common method approximate expectation second term generating samples. contrastive divergence algorithm runs markov chain steps clamping visible layer data examples. another technique persistent chains without clamping visible layer experiments algorithm steps alternating gibbs sampling learning model. discriminator model tries discriminate real fake data. learns features explain factors variation data uses features achieve classiﬁcation goal. hand tries low-dimensional input data. contrary common approaches instead representing input space think intermediate higher-level representation corresponding intermediate layers learn distribution space. samples generated contrastive divergence updating model used input therefore connect high-level feature space. denote intermediate layer activations denotes operations remaining layers then associative memory learns distribution similar regular gans optimization becomes second expectation estimated since inputted samples associative memory. minimizing third expectation forces associative memory estimate probability distribution function trained aans large-scale celebfaces attributes mnist dataset. images dataset linearly scaled range. models trained mini-batch sizes similar used adam optimizer. learning used mini-batch size learning rate stochastic gradient descent momentum. picked momentum rate contrastive divergence steps used create negative samples. used leaky rectiﬁed linear activations exception discussed rectiﬁed linear activation activation functions. using intermediate layer leakyrelu activation visible layer requires gaussian work well practice. hence used tanh activation layer connects visible layer binary rbm. chose binary variable states similar spin states ising model expected value unit conditional probability variable sigmoid. thus expected value becomes tanh non-linearity. negative samples created rbm’s contrastive divergence learning used inputs discriminator dataset four strided convolutional layers depth ﬁrst four layers. stride size match ﬁlter width height ﬁve. last convolutional layer’s outputs reshaped one-dimensional representation fully connected layer dimension rbm’s visible layer. layer layers size ﬁnal layers thought hidden layers classiﬁcation network deﬁned section input cascaded application composes c)). deﬁnes mapping feature space model learns distribution data samples space. figure shows face images generated using negative samples generated increasing number gibbs steps ten. rbm’s markov chain initialized created real images data given leftmost column. figure uses -dimensional visible hidden layer units figure uses -dimensional visible hidden layer units. generated images using gibbs samples change slowly; gibbs steps performed facial features expressions change gradually. example pose gender change happen quickly. shows requires many steps gibbs sample jump modes distribution. however using sampling step sufﬁcient change gender race etc. seen figure shows features distributed uniformly space helping gibbs sample jump mode distribution another. compared efﬁcient packing information pass correct classiﬁcation ends learning mapping uniformly distributes mapped dataset samples space. figure gibbs steps faces face images generated model running rbm’s markov chain varying number gibbs sampling steps. original images leftmost column images generated increasing number gibbs steps last column corresponding images generated gibbs steps. note images generated reconstructed versions original images since discriminator generator networks designed satisfy constraint. images generated using -dimensional change slowly increasing number gibbs steps; facial features expression pose change smoothly. however using -dimensional generated images look correlated implying markov chain jumps mode distribution another faster compared -dimensional distribution. implicitly deﬁnes probability distribution distribution samples generates goal obtain good approximation pdata. minimax game underlines adversarial learning global optimum pdata therefore value function given optimal solution. discussed introduction section difﬁculties adversarial training improves cost signiﬁcantly faster lags behind much receive good gradients back-propagating loses game. good adversarial training would expect behind acceptable level guide analyze convergence adversarial models monitored ex∼pdata] training. ﬁrst expectation measures objective associated real images second generated images. converge global optimal solution expectations converge log/. however practice ﬁrst expectation becomes extremely close zero second expectation diverges zero. analyzed ratio ex∼pdata]/ef∼ noise sampled uniformly feed learning -dimensional image difﬁcult learning associated dimension input. hence lags behind signiﬁcantly. however using -dimensional input learning task less difﬁcult. seen figure similarly alleviates learning task producing inputs manifold lies proposed associate adversarial networks improving training generative adversarial networks gans promising class generative models. however previous works reported several issues pertaining stability training. work argue inputting noise generator makes learning task difﬁcult. instead propose using additional network associative memory network connects networks gans discriminator generator. associative memory networks learn probabilistic model using higher level representation discovered discriminator sampled produce inputs generator network. although empirically tested efﬁcacy proposed associative adversarial networks hope develop rigorous theoretical understanding future work. inspecting equation possible associative memory collapse degenerate probability distribution similar collapsing lags behind signiﬁcantly. future work planning study entropy-maximizing regularizers associative memory. another future work would study probabilistic objectives generator obtained associative network. agreement idea proposed suggest changing objective match intermediate discriminator layer’s statistics. tijmen tieleman. training restricted boltzmann machines using approximations likelihood gradient. proceedings international conference machine learning icml pages york acm.", "year": 2016}