{"title": "iBOA: The Incremental Bayesian Optimization Algorithm", "tag": ["cs.NE", "cs.AI", "I.2.6; I.2.8; G.1.6"], "abstract": "This paper proposes the incremental Bayesian optimization algorithm (iBOA), which modifies standard BOA by removing the population of solutions and using incremental updates of the Bayesian network. iBOA is shown to be able to learn and exploit unrestricted Bayesian networks using incremental techniques for updating both the structure as well as the parameters of the probabilistic model. This represents an important step toward the design of competent incremental estimation of distribution algorithms that can solve difficult nearly decomposable problems scalably and reliably.", "text": "paper proposes incremental bayesian optimization algorithm modiﬁes standard removing population solutions using incremental updates bayesian network. iboa shown able learn exploit unrestricted bayesian networks using incremental techniques updating structure well parameters probabilistic model. represents important step toward design competent incremental estimation distribution algorithms solve diﬃcult nearly decomposable problems scalably reliably. missouri estimation distribution algorithms laboratory department mathematics computer science university missouri–st. louis university blvd. louis e-mail medalcs.umsl.edu http//medal.cs.umsl.edu/ estimation distribution algorithms replace standard variation operators genetic evolutionary algorithms building sampling probabilistic models promising candidate solutions. already earliest estimation distribution algorithms completely eliminated need maintaining explicit population candidate solutions used standard evolutionary algorithms updated probabilistic model incrementally using candidate solutions time main advantage incremental edas memory complexity greatly reduced. must successful results line research application compact genetic algorithm noisy problem billion bits nonetheless incremental edas proposed past either univariate models interactions variables probabilistic models form tree. paper proposes incremental version bayesian optimization algorithm uses bayesian networks model promising solutions sample ones. proposed algorithm called incremental many ideas adopted work incremental edas design iboa poses unique challenge—how incrementally update multivariate probabilistic model without either committing highly restricted structures beginning maintain possible multivariate statistics useful throughout run? propose solution challenge outline another possible approach tackling problem. test iboa several decomposable problems verify robustness scalability boundedly diﬃcult decomposable problems. finally outline interesting topics future work area. paper starts discussing related work section section outlines standard population-based bayesian optimization algorithm section describes incremental section presents discusses experimental results. section outlines important challenges future research area. finally section summarizes concludes paper. section reviews incremental estimation distribution algorithms. throughout section assume candidate solutions represented ﬁxed-length binary strings although methods deﬁned ﬁxed-length strings ﬁnite alphabet straightforward manner. population-based incremental learning algorithm ﬁrst estimation distribution algorithms mainly inspired equilibrium genetic algorithm pbil maintains probabilistic model promising solutions form probability vector. probability vector considers univariate probabilities string position stores probability position. n-bit string probability vector thus vector probabilities encodes probability i-th position. initially entries probability vector encoding uniform distribution binary strings length iteration pbil ﬁxed number binary strings ﬁrst generated current probability vector; string string position position probability current probability vector generated solutions evaluated nbest best solutions selected solutions based results evaluation nbest selected best solutions used update probability vector. speciﬁcally selected solution probability vector updated follows learning rate typically small value. increased; otherwise decreased. rate increase decrease depends learning rate current value corresponding probability-vector entry. original work pbil nbest although pbil maintain explicit population candidate solutions learning rate used similar manner population-size parameter standard populationbased genetic evolutionary algorithms. simulate eﬀects larger populations decreased; simulate eﬀects smaller populations increased. compact genetic algorithm also maintains probability vector instead population. similarly pbil initial probability vector corresponds uniform distribution n-bit binary strings entries thus iteration generates candidate solutions current probability vector. then solutions evaluated tournament executed solutions. winner loser tournament used update probability vector. presenting update rule used discuss eﬀects steady-state update univariate probabilities probability vector population size winner replaces loser. position winner contains position loser contains position probability position would increase hand winner contains position loser contains probability position would decrease finally winner loser contain position probability position would change. update procedure simulated even without explicit population using following update rule performance expected similar pbil algorithms similarly. furthermore perform similarly simple genetic algorithm uniform crossover population size even closely resembles univariate marginal distribution algorithm equilibrium genetic algorithm optimal dependency trees dependency-tree models used thus necessary maintain univariate probabilities also pairwise probabilities pairs string positions. pairwise probabilities maintained using array contains number every pair variables every combination assignments variables. represents estimate number solutions initially entries initialized constant cinit; example cinit used marginal probabilities computed pairwise probabilities tree built using variant prim’s algorithm ﬁnding minimum spanning trees minimizing kullback-liebler divergence empirical distribution dependency-tree model denotes index root dependency tree denotes parent generation starts root value generated using univariate probabilities continues tree always generating variables parents already generated. iteration dependency-tree proceeds similarly pbil. first dependency tree built then candidate solutions generated current dependency tree nbest best solutions selected generated candidates based evaluation. selected best solutions used update entries array solution update rule executed follows pbil probabilistic model based univariate probabilities optimal dependency trees capable encoding conditional dependencies pairs string positions enabling algorithm eﬃciently solve problems intractable pbil cga. nonetheless using dependency-tree models still insuﬃcient fully cover multivariate dependencies; yield optimal dependency trees intractable many decomposable problems multivariate interactions section describes bayesian optimization algorithm first basic procedure described. next methods learning sampling bayesian networks brieﬂy reviewed. bayesian optimization algorithm evolves population candidate solutions represented ﬁxed-length vectors ﬁnite alphabet. paper assume candidate solutions represented n-bit binary strings none presented techniques limited binary alphabet. ﬁrst population candidate solutions typically generated random according uniform distribution possible strings. iteration starts selecting population promising candidate solutions current population. selection method used population-based evolutionary algorithms used; example binary tournament selection. then bayesian network built selected solutions. solutions generated sampling probability distribution encoded learned bayesian network. finally solutions incorporated original population; example done replacing entire population solutions. procedure terminated predeﬁned termination criteria reached; example solution suﬃcient quality reached population lost diversity unlikely reach better solution solution found already. procedure visualized ﬁgure structure. structure bayesian network random variables deﬁned undirected acyclic graph node corresponds random variable edge deﬁnes direct conditional dependency connected variables. subset nodes exists edge node called parents node. parents conditional probability given variable directly depends parents. hand network encodes many independence assumptions simplify joint probability distribution signiﬁcantly. bayesian networks complex decision trees discussed section allowing encode arbitrary multivariate dependencies. estimation bayesian networks algorithm learning factorized distribution algorithm also edas based bayesian network models. learn structure bayesian network greedy algorithm typically used. greedy algorithm network construction network initialized empty network edges. then iteration edge improves quality network added network cannot improved user-speciﬁed termination criteria satisﬁed. several approaches evaluating quality speciﬁc network structure. work bayesian information criterion score network structures. two-part minimum description length metric part represents model accuracy whereas part represents model complexity measured number bits required store model parameters. simplicity assume solutions binary strings ﬁxed length assigns network structure score sampling done using probabilistic logic sampling bayesian networks proceeds steps. ﬁrst step computes ancestral ordering nodes node preceded parents. second step values variables candidate solution generated according computed ordering. since algorithm generates variables according ancestral ordering algorithm attempts generate value variable parents variable must already generated. given values parents variable distribution values variable given corresponding conditional probabilities. section outlines iboa. first basic procedure iboa brieﬂy outline. next procedures used update structure parameters model described discussed combine components. finally beneﬁts costs using iboa analyzed brieﬂy. basic procedure iboa similar incremental edas. model initialized probability vector encodes uniform distribution binary strings; entries probability vector thus initialized iteration several solutions generated current model. then generated solutions evaluated. given results evaluation best worst solution generated solutions selected winner loser used update parameters model. iterations model structure updated well reﬂect dependencies supported results previous tournaments. basic iboa procedure visualized ﬁgure main diﬀerences iboa model updated. first parameters must updated incrementally iboa maintain explicit population solutions. second model structure also updated incrementally without using population strings learn structure from. remainder section discusses details iboa procedure. speciﬁcally discuss challenges must addressed design incremental version boa. then present several approaches dealing challenges detail important iboa components. parameter updates done similarly cga. speciﬁcally iboa maintains array marginal probabilities string position given positions variable depends variable depend denote winner tournament loser marginal probability order positions speciﬁed denoted figure updating parameters iboa proceeds adding marginal probability consistent winner subtracting marginal probability consistent loser. winner loser point entry probability table table remains same. update rule increases marginal probability speciﬁc instance consistent winner tournament inconsistent loser. hand instance consistent loser winner probability decreased corresponds replacing winner loser population candidate solutions. subset variables marginal probabilities change update change marginal probabilities assignments consistent either winner loser tournament. ﬁgure example iboa update rule marginal probabilities. conditional probabilities computed marginal ones. thus update rule marginal probabilities iboa maintain marginal conditional probabilities necessary sampling structural updates. straightforward initialize marginal probability assumption uniform distribution update marginal probabilities using results tournament question remains open—what marginal probabilities actually need maintain know model structures look priori? since question closely related structural updates iboa discuss next. incremental edas proposed past already beginning clear probabilities maintained. pbil probabilities maintain univariate probabilities diﬀerent string positions. dependency-tree also maintain pairwise probabilities. probabilities need maintain iboa? issue poses diﬃcult challenge know model structure priori clear conditional marginal probabilities need. ﬁrst focus structural updates assume current model probability vector decide adding ﬁrst edge based metric standard scoring metric need pairwise marginal probabilities general consider string position parents decide adding another parent current parents using metric standard scoring metric also need probabilities knew current parents variable evaluate possible edge additions variable would need marginal probability tables. overall would result marginal probability tables maintain. however since know parents variable even restricted iboa contain parents variable consider possible models possible marginal probabilities probability tables relatively large intractable even moderate values raises important question—can better store limited probability tables without sacriﬁcing model-building capabilities iboa? tackle challenge variable going maintain several probability tables. first variable maintain probability table necessary specifying conditional probabilities current model. additionally maintain probability tables become parents necessary adding parent provide iboa probabilities required sample solutions also required make edge addition ending arbitrary node network. overall number subsets probability table maintained upper bounded signiﬁcant reduction nonetheless still must resolve problem adding marginal probabilities make edge addition. speciﬁcally edge another edge ending need store probabilities denotes index variable added parent impossible obtain exact value probabilities unless would maintain beginning estimate parameters assume independence resulting following rule initialize marginal probabilities marginal probabilities initialized updated tournament using update rule presented earlier. although independence assumption hold general edge supported future instances iboa edge eventually added. approaches dealing challenge introducing marginal probabilities possible believe strategy presented provide robust performance also supported experiments presented later. time adding edge eliminate probabilities probability tables probability table necessary anymore. initially model contains edges marginal probabilities pairs variables must stored ﬁrst round structural update updated tournament. later marginal probabilities variable changed based structure model results tournaments. iboa stores marginal probabilities current structure required figure adding edge node. since edges disallowed probabilities omitted. example clarity marginal probabilities repeated diﬀerence bayesian network learned iboa. therefore sampling algorithm used iboa. speciﬁcally variables ﬁrst topologically ordered string variables generated according generated ancestral ordering using conditional probabilities stored model described section ﬁrst approach perform continuous updates structure well parameters. performing tournament probabilities updated ﬁrst structure updated adding edges lead improvement model quality. figure pseudocode incremental bayesian optimization algorithm model structure denoted marginal probabilities denoted depending variant iboa structural updates skipped. second approach attempts simulate somewhat closer updating structure iterations; probabilities sampling solutions updated every iteration iboa. signiﬁcantly reduce complexity structural updates improve overall eﬃciency. model structure updated frequently ﬁrst approach structural updates might accurate forcing metric data make adequate structural update. third approach removes steady-state component iboa updates probabilities sampling solutions well model structure every iterations. means next structural update probability distribution encoded current model remains constant changed edges added values last parameter updates. clearly main beneﬁt using iboa instead standard iboa eliminates population thus reduce memory requirements boa. especially important solving extremely diﬃcult problems populations become large. iboa also provides ﬁrst incremental capable maintaining multivariate probabilistic models built multivariate statistics. nonetheless eliminating population size also brings disadvantages. first becomes diﬃcult eﬀectively maintain diversity using niching restricted tournament selection niching techniques typically require explicit population candidate solutions. might possible design specialized niching techniques directly promote diversity modifying probabilistic model seems straightforward. second iboa reduces memory complexity eliminating population still necessary store probabilistic model including marginal probabilities required make edge additions. since marginal probability tables require even memory population itself memory savings signiﬁcant pbil. nonetheless discussed section future work problem alleviated using local structures trap-. trap- input string ﬁrst partitioned independent groups bits each. partitioning unknown algorithm change run. -bit fully deceptive trap function applied group bits contributions trap functions added together form ﬁtness. contribution group bits computed number input string bits. task maximize function. n-bit trap- function global optimum string local optima. traps order necessitate bits group treated together statistics lower order misleading. since hboa performance invariant respect ordering string positions matter partitioning -bit groups done thus make results easier understand assume trap partitions located contiguous blocks bits. number input string bits. task maximize function. n-bit trap- function global optimum string local optima. traps order necessitate bits group treated together statistics lower order misleading. although iboa maintain explicit population candidate solutions still uses parameter loosely corresponds actual population size standard populationbased boa. thus iboa population-less still need adequate population size ensure iboa ﬁnds global optimum reliably. used bisection method estimate minimum population size reliably global optimum independent runs. stable results independent bisection runs repeated problem size thus results problem size averaged successful runs. number generations upper bounded number bits based convergence theory preliminary experiments. iboa number generations deﬁned ratio number iterations divided population size. iboa number solutions tournament based preliminary experiments showed value performed well. selection similar strength used tournament selection tournament size although methods equivalent perform similarly. iboa metric used evaluate competing network structures model building maximum number parents restricted way. iboa model structure updated every iterations sampling probabilities updated iteration. finally population candidate solutions replaces entire original population; setting optimal still method choice make comparison fair iboa elitism niching either. although primary goals setting iboa make algorithms perform similarly comparison algorithms side product experiments. important goal provide empirical support ability iboa discover maintain multivariate probabilistic model without using explicit population candidate solutions. also tried original cga; however simple model form probability vector able solve problems size even extremely large populations results thus omitted. figure shows number evaluations required iboa reach global optimum concatenated traps order cases number evaluations grows low-order polynomial; trap- growth approximated whereas trap- growth approximated fact number evaluations required iboa scales worse trap- trap- seems somewhat surprising cases relatively close bound predicted scalability theory estimates growth low-order polynomial performance iboa trap- trap- provides strong empirical evidence iboa capable ﬁnding adequate problem decomposition models would fail capture important dependencies fully deceptive problems trap- trap- would fail solve problems scalably figure shows number evaluations required standard reach global optimum concatenated traps order cases number evaluations grows low-order polynomial; trap- growth approximated whereas trap- growth approximated cases performs worse predicted scalability theory likely using elitist replacement strategy signiﬁcantly alleviates necessity accurate models ﬁrst iterations potential strong pressure towards overly simple models metric score network structures. case conclude iboa keeps standard without elitist replacement strategy niching even outperforms respect order growth number function evaluations problem size. experiments conﬁrmed iboa capable learning multivariate models incrementally without using population candidate solutions advantages counterbalanced disadvantages. importantly issues need addressed future work complexity model representation improved using local structures bayesian networks default tables decision trees/graphs elitist diversity-preservation techniques incorporated iboa improve performance. without addressing diﬃculties advantages using iboa instead somewhat overshadowed disadvantages. paper proposed incremental version bayesian optimization algorithm proposed algorithm called incremental like iboa uses bayesian networks model promising solutions sample ones. however iboa maintain explicit population candidate solutions; instead iboa performs series small tournaments solutions generated current bayesian network updates model incrementally using results tournaments. structure parameters updated incrementally. main advantage using iboa rather iboa need maintain population candidate solutions memory complexity thus reduced compared boa. however without population implementing elitist diversity-preservation techniques becomes challenge. furthermore memory required store bayesian network remains signiﬁcant addressed using local structures bayesian networks represent models eﬃciently. despite diﬃculties work represents ﬁrst step toward design competent incremental edas build maintain multivariate probabilistic models without using explicit population candidate solutions reducing memory requirements standard multivariate estimation distribution algorithms. project sponsored national science foundation career grant force oﬃce scientiﬁc research force materiel command usaf grant fa--- university missouri louis high performance computing collaboratory sponsored information technology services research award research board programs. u.s. government authorized reproduce distribute reprints government purposes notwithstanding copyright notation thereon. opinions ﬁndings conclusions recommendations expressed material authors necessarily reﬂect views national science foundation force oﬃce scientiﬁc research u.s. government. experiments done using hboa software developed martin pelikan david goldberg university illinois urbana-champaign experiments performed beowulf cluster maintained university missouri louis.", "year": 2008}