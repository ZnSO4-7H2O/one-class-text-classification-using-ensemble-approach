{"title": "Harnessing Deep Neural Networks with Logic Rules", "tag": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "abstract": "Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.", "text": "combining deep neural networks structured logic rules desirable harness ﬂexibility reduce uninterpretability neural models. propose general framework capable enhancing various types neural networks declarative ﬁrst-order logic rules. speciﬁcally develop iterative distillation method transfers structured information logic rules weights neural networks. deploy framework sentiment analysis named entity recognition. highly intuitive rules obtain substantial improvements achieve state-of-the-art comparable results previous best-performing systems. deep neural networks provide powerful mechanism learning patterns massive data achieving levels performance image classiﬁcation speech recognition machine translation playing strategic board games forth. despite impressive advances widely-used methods still limitations. high predictive accuracy heavily relied large amounts labeled data; purely data-driven learning lead uninterpretable sometimes counter-intuitive results also diﬃcult encode human intention guide models capture desired patterns without expensive direct supervision ad-hoc initialization. hand cognitive process human beings indicated people learn concrete examples also diﬀerent forms general knowledge rich experiences logic rules provide ﬂexible declarative language communicating high-level cognition expressing structured knowledge. therefore desirable integrate logic rules dnns transfer human intention domain knowledge neural models regulate learning process. paper present framework capable enhancing general types neural networks convolutional networks recurrent networks various tasks logic rule knowledge. combining symbolic representations neural methods considered diﬀerent contexts. neural-symbolic systems construct network given rule execute reasoning. exploit priori knowledge general neural architectures recent work augments data instance useful features network training however still limited instance-label supervision suﬀers issues mentioned above. besides large variety structural knowledge cannot naturally encoded feature-label form. framework enables neural network learn simultaneously labeled instances well logic rules iterative rule knowledge distillation procedure transfers structured information encoded logic rules network parameters. since general logic rules complementary speciﬁc data labels natural side-product integration support semi-supervised learning unlabeled data used better absorb logical knowledge. methodologically approach seen combination knowledge distillation posterior regularization method particular iteration adapt posterior constraint principle construct rule-regularized teacher train student network interest imitate predictions teacher network. leverage soft logic support ﬂexible rule encoding. apply proposed framework deploy task sentiment analysis named entity recognition respectively. intuitive rules distilled networks joint teacher networks strongly improve basic forms achieve better comparable performance state-of-the-art models typically parameters complicated architectures. best knowledge ﬁrst work integrate logic rules general workhorse types deep neural networks principled framework. encouraging results indicate method potentially useful incorporating richer types human knowledge improving application domains. combination logic rules neural networks considered diﬀerent contexts. neural-symbolic systems kbann cilp++ construct network architectures given rules perform reasoning knowledge acquisition. related line research markov logic networks derives probabilistic graphical models rule set. recent success deep neural networks vast variety application domains increasingly desirable incorporate structured logic knowledge general types networks harness ﬂexibility reduce uninterpretability. recent work trains extra features domain knowledge producing improved results beyond data-label paradigm. kulkarni uses specialized training procedure careful ordering training instances obtain interpretable neural layer image network. karaletsos develops generative model jointly data-labels similarity knowledge expressed triplet format learn improved disentangled representations. though exist general frameworks allow encoding various structured constraints latent variable models either directly applicable case could yield inferior performance empirical study. liang transfers predictive power pre-trained structured models unstructured ones pipelined fashion. proposed approach distinct iterative rule distillation process eﬀectively transfer rich structured knowledge expressed declarative ﬁrst-order logic language parameters general neural networks. show proposed approach strongly outperforms extensive array either ad-hoc general integration methods. section present framework encapsulates logical structured knowledge neural network. achieved forcing network emulate predictions rule-regularized teacher evolving models iteratively throughout training process agnostic network architecture thus applicable general types neural models including cnns rnns. construct teacher network iteration adapting posterior regularization principle logical constraint setting formulation provides closed-form solution. figure shows overview proposed framework. figure framework overview. iteration teacher network obtained projecting student network rule-regularized subspace student network updated balance emulating teacher’s output predicting true labels assume input variable target variable clarity focus k-way classiﬁcation k-dimensional probability simplex one-hot encoding class label. however method speciﬁcation consider ﬁrst-order logic rules conﬁdences denoted rule input-target space conﬁdence level indicating hard rule i.e. groundings required given examples groundings denoted {rlg}gl practice rule grounding typically relevant single subset examples though give general form entire set. neural network deﬁnes conditional probability using softmax output layer produces k-dimensional soft prediction vector denoted network parameterized weights standard neural network training iteratively update produce correct labels training instances. integrate information encoded rules propose train network also imitate outputs rule-regularized projection denoted explicitly includes rule constraints regularization terms. iteration constructed projecting subspace constrained rules thus desirable properties. present construction next section. prediction behavior reveals information regularized subspace structured rules. emulating outputs serves transfer denotes loss function selected according speciﬁc applications soft prediction vector iteration imitation parameter calibrating relative importance objectives. itively explained analogous human education teacher aware systematic general rules instructs students providing solutions particular questions important diﬀerence previous distillation work teacher obtained beforehand student trained thereafter teacher student learned simultaneously training. though possible combine neural network rule constraints projecting network rule-regularized subspace fully trained data-label instances optimizing projected network directly found iterative teacher-student distillation approach provides much superior performance shown experiments. moreover since distills rule information weights instead relying explicit rule representations predicting examples test time rule assessment expensive even unavailable still enjoying beneﬁt integration. besides second loss term augmented rich unlabeled data addition labeled examples enables semi-supervised learning better absorbing rule knowledge. iteration index omitted clarity. adapt posterior regularization principle logic constraint setting. formulation ensures closed-form solution thus avoids signiﬁcant increases computational overhead. recall rules goal optimal rules time staying close ﬁrst property apply commonly-used strategy imposes rule constraints expectation operator. rule groundings parameter. problem seen projecting constrained subspace. problem convex eﬃciently solved dual form closed-form solutions. provide detailed derivation supplementary materials directly give solution here framework related posterior regularization method places constraints model posterior unsupervised setting. classiﬁcation optimization procedure analogous modiﬁed algorithm using crossentropy loss evaluating second loss term unlabeled data diﬀering sheds light another perspective framework would work. however found experiments produce strong performance crucial labeled data losses form direct trade-oﬀ imitating soft predictions predicting correct hard labels. training need compute soft predictions iteration straightforward direct enumeration rule constraints factored base neural model constraints introduce additional dependencies e.g. bi-gram dependency transition rule task dynamic programming eﬃcient computation. higher-order constraints position constraints span multiple instances group relevant instances minibatches joint inference note calculating soft predictions eﬃcient since forward v.s. test time test time either distilled student network teacher network ﬁnal projection. empirical results show models substantially improve base network trained data-label instances. general performs better particularly suitable logic rules introduce additional dependencies requiring joint inference. contrast mentioned above lightweight eﬃcient useful rule evaluation expensive impossible prediction time. experiments compare performance extensively. imitation strength imitation parameter balances emulating teacher soft predictions predicting true hard labels. since teacher network constructed which beginning training would produce low-quality predictions thus favor predicting true labels initial stage. training goes gradually bias towards emulating teacher predictions eﬀectively distill presented framework general enough improve various types neural networks rules easy users allowed impose knowledge intentions declarative ﬁrst-order logic. section illustrate versatility approach applying workhorse network architectures i.e. convolutional network recurrent network representative applications i.e. sentence-level sentiment analysis classiﬁcation problem named entity recognition sequence learning problem. task ﬁrst brieﬂy describe base neural network. since focusing tuning network architectures largely similar networks previous successful neural models. design linguistically-motivated rules integrated. figure left architecture sentence-level sentiment analysis. sentence representation vector followed fully-connected layer softmax output activation output sentiment predictions. right architecture bidirectional lstm recurrent network ner. extracting character representation omitted. sentence-level sentiment analysis identify sentiment underlying individual sentence. task crucial many opinion mining applications. challenging point task capture contrastive sense within sentence. base network single-channel convolutional network proposed simple model achieved compelling performance various sentiment classiﬁcation benchmarks. network contains convolutional layer word vectors given sentence followed max-over-time pooling layer fully-connected layer softmax output activation. convolution operation apply ﬁlter word windows. multiple ﬁlters varying window sizes used obtain multiple features. figure left panel shows network architecture. logic rules diﬃculty plain neural network identify contrastive sense order capture dominant sentiment precisely. conjunction word strong indicators sentiment changes sentence sentiment clauses following generally dominates. thus consider sentences abut-b structure expect sentiment whole sentence consistent sentiment clause logic rule written has-‘a-but-b’-structure indicator function takes argument true otherwise; class represents ‘positive’; element class ’+’. ‘a-but-b’ structure truth value logic rule equals locate classify elements text entity categories persons organizations. essential ﬁrst step downstream language understanding applications. task assigns word named entity format bieos entity category. valid sequence follow certain constraints deﬁnition tagging scheme. besides text structures within across sentences usually expose consistency patterns. base network base network similar architecture bi-directional lstm recurrent network proposed outperformed previous neural models. model uses pretrained word vectors capture characterword-level information respectively. features bi-directional lstm units sequence tagging. compared omit character type capitalization features well additive transition matrix output layer. figure right panel shows network architecture. logic rules base network largely makes independent tagging decisions position ignoring constraints successive labels valid sequence contrast recent work adds conditional random ﬁeld capture bi-gram dependencies outputs instead apply logic rules introduce extra parameters learn. example rule leverage list structures within across sentences documents. speciﬁcally named entities corresponding positions list likely categories. instance juventus barcelona know barcelona must organization rather location since counterpart entity juventus organization. describe simple procedure identifying lists counterparts supplementary materials. logic rule encoded bility mass labels categories single probability yielding vector length equaling number categories. distance measure closeness predictions counterpart note distance takes value proper soft truth value. list rule span multiple sentences found teacher network enables explicit joint inference provides much better performance distilled student network validate framework evaluating applications sentiment classiﬁcation named entity recognition variety public benchmarks. integrating simple eﬀective rules base networks obtain substantial improvements tasks achieve state-of-the-art comparable results previous best-performing systems. comparison diverse rule integration methods demonstrates unique eﬀectiveness framework. approach also shows promising potentials semisupervised learning sparse data context. classiﬁcation imitation parameter min{. −.t} downplay noisy listing rule. conﬁdence levels rules except hard constraints whose conﬁdence neural network conﬁguration largely followed reference work speciﬁed following respective sections. experiments performed linux machine eight .ghz cores tesla ram. implemented neural networks using theano popular deep learning platform. test method number commonly used benchmarks including stanford sentiment treebank contains classes sentences train/dev/test sets respectively. following train models sentences phrases since labels provided. one-sentence movie reviews negative positive sentiment. customer reviews various products containing classes instances. -fold cross validation previous work. three datasets around sentences contains word but. table accuracy sentiment classiﬁcation. base network corresponding cnn-non-static model rows networks enhanced framework cnn-rule-p student network cnn-rulebase neural network non-static version exact conﬁgurations. speciﬁcally word vectors initialized using wordvec ﬁne-tuned throughout training neural parameters trained using adadelta update rule table shows sentiment classiﬁcation performance. rows compare base neural model models enhanced framework but-rule method provides strong boost accuracy three datasets. teacher network improves student network though student network widely applicable certain contexts discussed sections rows show accuracy recent top-performing methods. datasets model outperforms baselines. mvcnn system shows slightly better result ours. neural network combined diverse sets pre-trained word embeddings contained neural layers parameters model. investigate eﬀectiveness framework integrating structured rule knowledge compare extensive array possible integration approaches. table lists methods performance task. that although methods lead diﬀerent degrees improvement framework outperforms competitors large margin. particular compared pipelined method analogous structure compilation work iterative distillation provides better performance. another advantage method train neural parameters opposed separate set; -project projects trained base rule-regularized subspace eq.; -opt-project directly optimizes projected cnn; -pipeline distills pre-trained -opt-project plain cnn; -rule-p -rule-q models distilled student network teacher network. note -butclause --reg ad-hoc methods applicable speciﬁcally but-rule. sets pipelined approach. distilled student network -rule-p achieves much superior accuracy compared base well -project -opt-project explicitly project rule-constrained subspace. validates distillation procedure transfers structured knowledge neural parameters eﬀectively. inferior accuracy -opt-project partially attributed poor performance neural network part achieves accuracy leads inaccurate evaluation but-rule eq.. next explore performance framework varying numbers labeled instances well eﬀect exploiting unlabeled data. intuitively less labeled examples expect general rules would contribute performance unlabeled data help better learn rules. useful property especially data sparse labels expensive obtain. table shows results. subsampling conducted sentence level. instance ﬁrst selected training sentences uniformly random trained models sentences well phrases. results verify expectations. rows give accuracy using data-label subsets training. every setting methods consistently outperform base cnn. -rule-q provides higher improvement data larger data showing promising potential sparse data context. adding unlabeled instances semi-supervised learning rows improved accuracy. -semi-pr posterior regularization imposes rule constraint unlabeled data training. distillation framework consistently provides substantially better results. table accuracy varying sizes labeled data semi-supervised learning. header percentage labeled examples training. rows supervised data. rows semi-supervised learning remaining training data used unlabeled examples. -semi-pr report projected solution performs better non-projected table performance conll-. blstm-rule-trans imposes transition rules base blstm. blstm-rules incorporates list rule report performance student model teacher model evaluate well-established conll- benchmark contains sentences tokens train/dev/test sets respectively. dataset includes categories i.e. person location organization misc. bioes tagging scheme used. around named entities occur lists. mostly conﬁgurations base blstm network except that besides slight architecture diﬀerence apply adadelta parameter updating. glove word vectors used initialize word features. table presents performance task. incorporating bi-gram transition rules joint teacher model achieves improvement score outperforms previous neural based methods including blstm-crf model applies conditional random ﬁeld blstm order capture transition patterns encourage valid sequences. contrast method implements desired constraints straightforward using declarative logic rule language time introduce extra model parameters learn. integration list rule provides second boost performance achieving score close best-performing systems including joint-ner-el probabilistic graphical model optimizing entity linking jointly massive external resources blstm-crf combination blstm parameters rule-enhanced neural networks. table accuracy joint teacher model distilled student relatively larger sentiment classiﬁcation task task used logic rules introduce extra dependencies adjacent positions well multiple instances making explicit joint inference useful fulﬁlling structured constraints. developed framework combines deep neural networks ﬁrst-order logic rules allow integrating human knowledge intentions neural models. particular proposed iterative distillation procedure transfers structured information logic rules weights neural networks. transferring done teacher network constructed using posterior regularization principle. framework general applicable various types neural architectures. intuitive rules framework signiﬁcantly improves base networks sentiment analysis named entity recognition demonstrating practical signiﬁcance approach. though focused ﬁrst-order logic rules leveraged soft logic formulation easily extended general probabilistic models expressing structured distributions performing inference reasoning plan explore diverse knowledge representations guide learning. proposed iterative distillation procedure also reveals connections recent neural autoencoders generative models encode probabilistic structures neural recognition models distill information iterative optimization", "year": 2016}