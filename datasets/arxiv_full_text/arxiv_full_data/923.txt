{"title": "Deep Online Convex Optimization with Gated Games", "tag": ["cs.LG", "cs.GT", "cs.NE", "stat.ML"], "abstract": "Methods from convex optimization are widely used as building blocks for deep learning algorithms. However, the reasons for their empirical success are unclear, since modern convolutional networks (convnets), incorporating rectifier units and max-pooling, are neither smooth nor convex. Standard guarantees therefore do not apply. This paper provides the first convergence rates for gradient descent on rectifier convnets. The proof utilizes the particular structure of rectifier networks which consists in binary active/inactive gates applied on top of an underlying linear network. The approach generalizes to max-pooling, dropout and maxout. In other words, to precisely the neural networks that perform best empirically. The key step is to introduce gated games, an extension of convex games with similar convergence properties that capture the gating function of rectifiers. The main result is that rectifier convnets converge to a critical point at a rate controlled by the gated-regret of the units in the network. Corollaries of the main result include: (i) a game-theoretic description of the representations learned by a neural network; (ii) a logarithmic-regret algorithm for training neural nets; and (iii) a formal setting for analyzing conditional computation in neural nets that can be applied to recently developed models of attention.", "text": "abstract—methods convex optimization widely used building blocks deep learning algorithms. however reasons empirical success unclear since modern convolutional networks incorporating rectiﬁer units max-pooling neither smooth convex. standard guarantees therefore apply. paper provides ﬁrst convergence rates gradient descent rectiﬁer convnets. proof utilizes particular structure rectiﬁer networks consists binary active/inactive gates applied underlying linear network. approach generalizes max-pooling dropout maxout. words precisely neural networks perform best empirically. step introduce gated games extension convex games similar convergence properties capture gating function rectiﬁers. main result rectiﬁer convnets converge critical point rate controlled gated-regret units network. corollaries main result include game-theoretic description representations learned neural network; logarithmic-regret algorithm training neural nets; formal setting analyzing conditional computation neural nets applied recently developed models attention. deep learning algorithms yielded impressive performance across range tasks including object voice recognition workhorse underlying deep learning error backpropagation decades algorithm yields state-of-the-art performance massive labeled datasets combined recent innovations rectiﬁers dropout backprop gradient descent plus chain rule. gradient descent convergence guarantees settings smooth convex both. however modern convnets neither smooth convex. although well-known convnets convex perhaps under-emphasized spectacular recent results obtained convnets benchmarks imagenet rely architectures smooth. starting alexnet every winner imagenet classiﬁcation challenge used rectiﬁer activation functions rectiﬁers max-pooling non-smooth functions used essentially modern convnets fact representational power rectiﬁer nets derives precisely nondifferentiability number nondifferentiable boundaries parameter space grows exponentially depth follows none standard convergence guarantees optimization literature apply modern convnets. paper provide ﬁrst convergence rates convolutional networks rectiﬁers max-pooling. introduce class gated games generalize convex games studied stoltz lugosi reformulate learning convnets gated games adapt results convergence correlated equilibria convex gated games. functions approximated neural networks extensively studied. early results show neural networks single hidden layer universal function approximators recently researchers focused role depth rectiﬁers function approximation standard guarantees vc-theory apply neural nets although quite loose recent work hardt shows convergence rate stochastic gradient methods implications generalization bounds convex nonconvex settings unfortunately results rely smoothness assumption hold rectiﬁers max-pooling. thus although suggestive results apply modern convnets. feng initiated promising direction based ensemble robustness although robustness cannot evaluated analytically. related problem better understand regularization methods dropout regret-bounds dropout found setting prediction expert advice however unclear extend results neural nets. third problem understand critical points found backpropagation local minima global minimum. problem challenging since neural networks convex. theoretical work studying conditions gradient descent converges local minima nonconvex problems assumptions required results quite strong include smoothness assumptions hold rectiﬁers. also observed saddles slow training even algorithm converge saddle point; designing algorithms avoid saddles area active research recent work choromanska suggests local optima neural nets error rates reasonably close global optimum searching good local optima therefore less practical importance ensuring rapid convergence. last problem focus paper understand convergence gradient-based methods neural networks. speeding training time neural nets problem enormous practical importance. although large body empirical work optimizing neural nets theoretical guarantees apply methods used train rectiﬁer convnets since neither smooth convex. recent work investigated convergence proximal methods nonconvex nonsmooth problems however computing proxoperators appears infeasible neural nets. interesting results derived variancereduced gradient optimization nonconvex setting although smoothness still required. training modern convnets rectiﬁers max-pooling entails searching rich subset universal class function approximators loss function neither smooth convex. little hope obtaining useful convergence results level generality. therefore necessary utilize structure rectiﬁer networks. strategy decompose neural nets interacting optimizers easier analyze individually whole. short strategy import techniques game theory deep learning. make observations neural network structure. ﬁrst section reformulate linear networks convex games players units network. although loss convex function weights network; convex function weights individual players. observation connects dynamics games noregret learning dynamics linear networks backpropagation. linear networks special case. natural whether neural networks nonlinearities also convex games. unfortunately answer introducing nonlinearity breaks convexity. although situation seems hopeless turns remarkably game-theoretic convergence results imported despite nonconvexity precisely nonlinearities used modern convnets. whether units active given input. unit inactive feedforward sweep also inactive backpropagation therefore update weights. motivates generalizing convex games gated games. classical game player chooses series actions round incurs convex loss. regret player difference cumulative loss cumulative would player chosen best action hindsight players implemented so-called no-regret algorithms minimize loss relative best action hindsight. precisely no-regret algorithm sublinear cumulative regret. regret round therefore vanishes asymptotically. section introduces gated games players incur convex loss rounds active. extending deﬁnitions regret correlated equilibrium gated games proposition shows players follow no-gatedregret strategies converge correlated equilibrium. gated players generalize sleepy experts introduced blum also useful technical tool path-sum games. games constructed directed acyclic graphs weighted edges. lemma shows path-sums encode dynamics feedforward feedback sweeps rectiﬁer nets. proposition shows path-sum games gated games proposition extends result convolutional networks. main contributions paper follows theorem rectiﬁer convnets converge critical point backpropagation rate controlled gatedregret units network. corollary specializes result gradient descent. best knowledge previous convergence rates applicable neural nets rectiﬁer nonlinearities max-pooling. finding conditions guarantee convergence local minima deferred future work. results derive detailed analysis internal structure rectifer nets updates backpropagation. require ideas regarding optimization general. methods provide ﬁrst rigorous explanation methods designed convex optimization improve convergence rates modern convnets. results apply neural networks hold precisely neural networks perform best empirically philosophy underlying paper decompose training neural nets distinct tasks communication optimization. communication handled backpropagation sends correct gradient information players net. optimization handled locally individual players. note although philosophy widely applied designing implementing neural nets under-utilized analysis neural nets. role players convnet encapsulated gated forecaster setting section results provide dictionary translates guarantees applicable no-regret algorithm convergence rate network whole. reformulate neural networks games. primary conceptual contribution paper connect game theory deep learning. interesting consequence main result corollary provides compact description weights learned neural network signal underlying correlated equilibrium. generally neural nets basic example game structured communication protocol determines players interact fruitful investigate broader classes structured games. suggested rectiﬁers perform well nonnegative homogeneous implications regularization robustness changes initialization results provide complementary explanation. rectiﬁers simultaneously introduce nonlinearity neural nets providing enormous representational power gates select subnetworks underlying linear neural network convex methods applicable guarantees. logarithmic regret algorithm. concrete application gated forecaster framework adapt online newton step algorithm neural nets show logarithmic-regret corollary resulting algorithm approximates newton’s method locally level individual units network rather globally network whole. local unit-wise implementation reduces computational complexity sidesteps tendency quasi-newton methods approach saddle points. conditional computation. secondary conceptual contribution introduce framework conditional computation. point assumed gate ﬁxed property game. concretely gates correspond rectiﬁers max-pooling convolutional networks baked architecture network exposed data. natural consider optimizing players gated game active section recent work along lines applied reinforcement learning algorithms data-dependent dropout policies conditional computation closely related models attention slightly aﬁeld long-short term memories gated recurrent units complicated sets sigmoid-gates control activity within memory cells unfortunately resulting architectures difﬁcult analyze; principled simpliﬁcation recurrent neural network architectures motivated similar considerations present paper. ﬁrst step towards analyzing conditional computation neural nets introduce conditional gate setting. cogs contextual bandits contextual partial monitors optimize sets units active. cogs second class players introduced neural games provide useful tool designing deep learning algorithms. neural nets typically trained minibatches sampled i.i.d. dataset. contrast analysis provides guarantees adversarial settings. results therefore conservative. extending analysis take advantage stochastic settings important open problem. however worth mentioning neural nets increasingly applied data i.i.d. sampled. example adversarially trained generative networks achieved impressive performance similarly spectacular progress applying neural nets reinforcement learning activity within neural network i.i.d. even inputs phenomenon known internal covariate shift relevant developments batch-normalization optimistic mirror descent batch normalization signiﬁcantly reduces training time neural nets actively reducing internal covariate shift. optimistic mirror descent takes advantage fact players game implementing no-regret learning speed convergence. interesting investigate whether reducing internal covariate shift understood game-theoretically whether optimistic learning algorithms adapted neural networks. number papers brought techniques convex optimization analysis neural networks. line work initiated bengio shows allowing learning algorithm choose number hidden units convert neural network optimization convex problem also convex multi-layer architecture developed although methods interesting achieved practical success convnets. paper analyze convnets rather proposing tractable potentially less useful model. game theory developed model interactions humans however directly applicable toolbox analyzing machina economicus interacting populations algorithms optimizing objective functions step further develop game-theoretic analysis internal structure backpropagation. idea decomposing deep learning algorithms cooperating modules dates back least work bottou related line work modeling biological neural networks game-theoretic perspective found paper combines disparate techniques notation game theory convex optimization deep learning therefore somewhat dense. oriented start linear neural networks. linear nets provide simple nontrivial worked example. convex. energy landscapes dynamics backpropagation extensively studied turn surprisingly intricate convenient shorthand output network simplicity suppose output layer consists single unit output scalar denote sample labeled data loss function convex ﬁrst argument. training neural network reduces solving optimization problem convex game consists loss vector player picks actions convex compact player loss convex argument. special case convex games probability simplex ﬁnite actions available agent loss multilinear. well known that even linear case loss neural network convex function weights weights individual layers. however loss linear network convex function weights individual units. observed foster vohra that players play according no-regret online learning rules average sequence plays converges correlated equilibrium proposition shows general result no-gated-regret algorithms converge correlated equilibrium rate depends gated-regret. refer coarse correlated equilibrium. ǫ-term quantiﬁes deviation coarse correlated equilibrium notion correlated equilibrium weaker nash equilibrium. correlated equilibria contains convex hull nash equilibria subset. thus perspectives linear nets networks games. train network algorithms gradient descent implemented backpropagation. play game players no-regret algorithms. sections show perspectives equivalent general setting modern convnets. particular correlated equilibria games critical points energy landscapes. strategy convert results convergence convex games correlated equilibria results convergence backpropagation neural nets. section presents detailed analysis rectiﬁer nets. observation rectiﬁers gates leads directly gated games. gates games convex. however close enough results convergence correlated equilibria easily adapted setting. main technical work section introduce notation handle interaction path-sums gates. path-sum games introduced class gated games capturing dynamics rectiﬁer nets proposition finally show extend results convnets. historically neural networks typically used sigmoid +e−a tanh ea−e−a ea+e−a nonlinearities. alternatives investigated jarrett found rectiﬁers often perform much better sigmoids practice. rectiﬁers default nonlinearity convnets many variants theme including noisy rectifers max) leaky rectifers remark neural network reformulated game treating individual units players. however general loss convex function players’ actions convergence guarantees available. main conceptual contribution paper show modern convnets form class games which although convex close enough convergence results game theory adapted setting. goal player game minimize loss unfortunately realistic since depends actions players. game repeated attainable goal players minimize regret. player’s cumulative regret difference loss incurred series plays loss would incurred player consistently chosen best play hindsight algorithm no-regret regretj grows sublinearly important note no-regret guarantees hold sequence actions players game stochastic adversarial something else. player no-regret plays optimally given actions players. examples no-regret algorithms convex losses include online gradient descent exponential weights remark player permanently inactive then trivially gatedregret. suggests loophole deﬁnition players exploit. make comments. firstly players control inactive. rather optimize rounds exposed secondly practice units rectiﬁer networks become inactive. problem mild rectiﬁer nets still outperform architectures. reducing number inactive units motivations maxout units next step extend correlated equilibrium gated games. intuition behind correlated equilibrium signal sent players guides behavior. however inactive players observe signal. signal received player active conditional distribution proposition gated game suppose players follow strategies gated-regret then empirical distribution actions played ǫ-coarse correlated equilibrium. rectiﬁers gates difference backprop linear network rectiﬁer network units zeroed forward backward sweeps. forward sweep rectiﬁers zero units would produced negative outputs; backward sweep rectiﬁer subgradients zero exact units acting indicator functions. zeroed units contribute feedforward sweep receive error signal backpropagation. effect rectiﬁers select linear subnetwork active units forward backpropagation. seen linear networks convex games. extending result rectiﬁer networks requires generalizing convex games setting subset players active round. denote powerset gated game convex game equipped gate players active. inactive players incur loss. active player incurs convex loss depends action actions active players; i.e. neural nets rectiﬁer functions max-pooling dropout forms gates control whether units actively respond input. importantly inactive units receive error backpropagation discussed section directed acyclic graph corresponding rectiﬁer neural network units input units. provide alternate description dynamics feedforward feedback sweep neural terms pathsums. deﬁnitions somewhat technical; underlying ps-pred ps-grad analogs prediction expert advice hedge setting. hedge setting players receive linear losses choose actions simplex; ps-grad players receive linear losses. results hold games although primary interest ps-pred. note ps-grad important property loss player linear function player action active remark shown convex function player action player active. note that loss player depends actions chosen players game loss convex function joint-action players. reasons game-theoretic analysis essential. proposition merely hold cases. maxout units next dropout dropconnect convolutional networks shared weights max-pooling. proposition thus applies convolutional networks used practice. finally note proposition hold leaky rectiﬁer units units piecewise linear sigmoid tanh. construct graph node input linear rectiﬁer unit; nodes maxout unit. players correspond nodes denoted greek letters. extended graph inherits edge structure connection players underlying units connected. path weights path-sums deﬁned exactly before except work instead deﬁnition active units modiﬁed follows unit thus produce output still count inactive ignored max-pooling layer effect output neural network. particular units ignored max-pooling update weights backpropagation. units convolutional layer share weights. obversely maxout units corresponds many players weight-sharing units correspond single composite player. suppose rectiﬁer units share weight vector training dropout units become inactive probability training. words stochastic component whether player active. gated games easily extended incorporate dropout allowing gates switch stochastically. gating function takes values distributions dropconnect reﬁnement dropout connections instead units dropped training dropconnect requires extending notion gate range distributions subsets edges instead subsets units composite piecewise polynomial convex function. follows no-regret algorithms either converge point gradient zero point gradient exist. thus network converges correlated equilibrium dirac distribution concentrated critical point loss function. theorem provides ﬁrst rigorous justiﬁcation applying convex methods convnets although convex individual units perform convex optimizations active. theorem provides generic conversion regretguarantees convex methods convergence rates rectiﬁer networks. corollaries provide algorithms learning rate algorithm decays according number active steps rather number steps important insight gated-game formulation learning occurs active rounds. bound function constants depend behavior units neural net. dependence arises gradient descent based algorithm weight updates depend backpropagated error. corollary precisely characterises dependence. recall correlated equilibrium requires signal guide behavior players. case rectiﬁer relevant signal emprical distribution joint actions players. corollary theorem show explore implications connection path-sum games deep learning. theorem shows convergence rates gated forecasters path-sum game controls convergence rate network whole. immediate corollary obtain ﬁrst convergence rates gradient descent rectiﬁer nets. second corollary conceptual practical importance shows signal underpinning correlated equilibrium used describe representation learned neural network. finally present algorithm logarithmic regret. main result gated-regret controls rate convergence critical points rectiﬁer convnets loss function convex output net. result holds assuming weight vectors restricted compact convex sets. weights usually hard-constrained training neural networks although frequently regularized. hinton recently argued weights rectiﬁer layers quickly stabilise similar values suggesting issue practice. important note theorem applies convolutional nets used practice. rectiﬁers replaced sigmoids nonlinearity choice consistently yield better empirical performance. loss functions convex almost applications logistic loss hinge loss meansquared error convex functions output weights. important class games introduced monderer shapley potential games. game potential game loss functions players arise single function referred potential function. rectiﬁer nets gated potential games potential function loss network. loss incurred player active loss network. potential games amenable analysis computation general games. local minima potential function pure nash equilibria. moreover simple algorithms ﬁctitious play regret-matching converge nash equilibria potential games nprop algorithm newton’s method computationally expensive since involves inverting hessian. particular online newton step scales dimension moreover quasi-newton method tends converge saddle points. naive implementation quasi-newton method neural networks based global hessian therefore problematic number parameters huge; saddle points abundant nprop algorithm sidesteps problems since implemented unit-wise. computational cost reduced since approximation local hessian computed unit. thus computational cost scales quadratically largest layer rather network. similarly since nprop implemented unit-wise newton-approximation exposed curvature neural net. instead nprop simultaneously leverages linear structure active pathsums exp-concave structure external loss. non-empty compact convex set. function α-exp-concave e−αf concave function many commonly used loss functions exp-concave including mean-squared error loss loghw logistic loss suitably signal provides compact description representations learned deep networks. thus direct connection correlated equilibria representation learning. denote empirical distribution signal gametheoretic terminology joint actions round neural network trained error backpropagation empirical signal observered player notational convenience useful incorporate learning rate number rounds initial weight vector gain deﬁne corollary succinctly describes representations learned neural network game-theoretic notation developed above. corollary eliminate complexity deep representations. rather demonstrates direct dependence empirical signal extremely complicated object. instead using function example activate linear unit equip decides round whether activate unit based context unit woken observe loss. setting generalizes apple-tasting contextual partial monitoring. example section showed model maxout unit players. adaptive-maxout unit players activates round. contextual bandit players levers; weights inputs context; loss scalar depends choice active player. currently theoretically grounded off-the shelf methods contextual partial monitoring. however efﬁcient contextual bandit algorithm stochastic setting provided recent work implemented conditional computation neural nets using reinforce train policy. although approach shown work practice currently performance guarantees available. developing solid understanding conditional computation neural nets important open problem. paper initiated game-theoretic analysis convolutional networks. observation nonlinearities found modern convnets gates control whether linear operations performed units contribute network’s output. gated games formalize role gating. path-sum games succinctly express dynamics convolutional networks. reformulating error backpropagation path-sum game yields best knowledge nprop ﬁrst logarithmicregret algorithm neural networks. nprop computationally efﬁcient order methods since require computing hessian efﬁcient ways iteratively compute without directly inverting matrix. nevertheless nprop’s memory usage computational complexity prohibitive worth investigating whether efﬁcient algorithms achieve logarithmic regret setting example based sketched online newton algorithm proposed linear runtime. finally nprop take advantage fact experts inactive round suggesting second direction improved. convnets path-sum games played gated convex players. criterion activating unit either operator random shown work well practice. nevertheless natural question whether optimal. section introduces framework tackle question. indeed analyzing optimizing gates requires kind player conditional gate controls players active. conditional gate experiences regret activing optimal subset players. precisely activates rumelhart hinton williams learning representations back-propagating errors nature vol. schmidhuber deep learning neural networks overview neural networks vol. srivastava hinton krizhevsky sutskever salakhutdinov dropout simple prevent neural networks overﬁtting jmlr vol. deng dong socher l.-j. fei-fei imagenet large-scale hierarchical image database cvpr choromanska henaff mathieu arous lecun loss surface multilayer networks journal machine learning research workshop conference proceeedings vol. choromanska lecun arous open problem landscape loss surfaces multilayer networks journal machine learning research workshop conference proceeedings vol. p.-l. bacon bengio pineau precup conditional computation neural networks using decision-theoretic approach multidisciplinary conference reinforcement learning decision making merri¨enboer gulcehre bahdanau bougares schwenk bengio learning phrase representations using encoder–decoder statistical machine translation emnlp mnih kavukcuoglu silver rusu veness bellemare graves riedmiller fidjeland ostrovski petersen beattie sadik antonoglou king kumaran wierstra legg hassabis human-level control deep reinforcement learning nature vol. silver huang maddison guez sifre antonoglou panneershelvam driessche lanctot dieleman grewe nham kalchbrenner sutskever lillicrap leach kavukcuoglu graepel hassabis mastering game deep neural networks tree search nature vol. balduzzi randomized co-training cortical neurons machine learning back again randomized methods machine learning workshop neural proc systems balduzzi vanchinathan buhmann kickback cuts backprop’s red-tape biologically plausible credit assignment neural networks aaai conference artiﬁcial intelligence", "year": 2016}