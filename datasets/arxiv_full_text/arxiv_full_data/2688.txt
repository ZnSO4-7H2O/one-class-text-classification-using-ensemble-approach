{"title": "Gradient descent GAN optimization is locally stable", "tag": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "abstract": "Despite the growing prominence of generative adversarial networks (GANs), optimization in GANs is still a poorly understood topic. In this paper, we analyze the \"gradient descent\" form of GAN optimization i.e., the natural setting where we simultaneously take small gradient steps in both generator and discriminator parameters. We show that even though GAN optimization does not correspond to a convex-concave game (even for simple parameterizations), under proper conditions, equilibrium points of this optimization procedure are still \\emph{locally asymptotically stable} for the traditional GAN formulation. On the other hand, we show that the recently proposed Wasserstein GAN can have non-convergent limit cycles near equilibrium. Motivated by this stability analysis, we propose an additional regularization term for gradient descent GAN updates, which \\emph{is} able to guarantee local stability for both the WGAN and the traditional GAN, and also shows practical promise in speeding up convergence and addressing mode collapse.", "text": "despite growing prominence generative adversarial networks optimization gans still poorly understood topic. paper analyze gradient descent form optimization i.e. natural setting simultaneously take small gradient steps generator discriminator parameters. show even though optimization correspond convex-concave game proper conditions equilibrium points optimization procedure still locally asymptotically stable traditional formulation. hand show recently proposed wasserstein non-convergent limit cycles near equilibrium. motivated stability analysis propose additional regularization term gradient descent updates able guarantee local stability wgan traditional also shows practical promise speeding convergence addressing mode collapse. since introduction years generative adversarial networks gained prominence widely used methods training deep generative models. gans successfully deployed tasks photo super-resolution object generation video prediction language modeling vocal synthesis semi-supervised learning amongst many others core methodology idea jointly training networks generator network meant produce samples distribution discriminator network attempts differentiate samples data distribution ones produced generator. problem typically written min-max optimization problem following form purposes paper shortly consider general form optimization problem also includes recent wasserstein formulation. despite prominence actual task optimizing gans remains challenging problem theoretical practical standpoint. although original paper included analysis convergence properties approach assumed updates occurred pure function space allowed arbitrarily powerful generator discriminator networks modeled resulting optimization objective convex-concave game therefore yielding well-deﬁned global convergence properties. furthermore analysis assumed discriminator network fully optimized generator updates assumption mirror practice optimization. indeed practice exist number well-documented failure modes gans mode collapse vanishing gradient problems. contributions. paper consider gradient descent formulation optimization setting generator discriminator updated simultaneously simple gradient updates; inner outer optimization loops neither generator discriminator assumed optimized convergence. despite fact that show correspond convex-concave optimization problem show that region around equilibrium point updates gradient updates converge equilibrium point exponential rate. interestingly conditions satisﬁed traditional wgan indeed show wgans non-convergent limit cycles gradient descent case. theoretical analysis also suggests natural method regularizing updates adding additional regularization term norm discriminator gradient. show addition term leads locally exponentially stable equilibria classes gans including wgans. additional penalty highly related recent proposals practical optimization unrolled improved wasserstein training practice approach simple implement preliminary experiments show helps avert mode collapse leads faster convergence. optimization theory. although theoretical analysis gans outpaced practical application notable results recent years addition aforementioned work original paper. part work entirely complementary studies different questions. arjovsky bottou provide important insights instability arises supports generated distribution true distribution disjoint. contrast paper delve equally important question whether updates stable even generator fact close true distribution arora hand explore questions relating sample complexity expressivity architecture relation existence equilibrium point. however still unknown whether given equilibrium exists update procedure converge locally. practical standpoint number papers address topic optimization gans. several methods proposed introduce objectives architectures improving stability optimization wide variety optimization heuristics architectures also proposed address challenges mode collapse proposed regularization term falls category hopefully provides context understanding methods. speciﬁcally regularization term captures degree foresight generator optimization procedure similar unrolled gans procedure indeed show gradient penalty closely related -unrolled gans also provides ﬂexibility leveraging foresight. finally gradient-based regularization explored gans recent works gulrajani though penalty discriminator rather generator case. finally several works simultaneously addressed similar issues paper. particular similarity methodology propose works roth mescheder ﬁrst present stabilizing regularizer based gradient norm gradient calculated respect datapoints. regularizer hand based norm gradient calculated respect parameters. approach strong similarities second work noted above; however authors establish disprove stability instead note presence zero eigenvalues motivation alternative optimization method. thus feel works whole quite complementary signify growing interest optimization issues. stochastic approximation algorithms analysis nonlinear systems. technical tools analyze optimization dynamics paper come ﬁelds stochastic approximation algorithm analysis nonlinear differential equations notably method analyzing convergence properties dynamical systems consider general stochastic process driven updates vector step size function martingale difference sequence fairly general conditions namely bounded second moments lipschitz continuity summable square-summable step sizes stochastic approximation algorithm converges equilibrium point ordinary differential equation thus understand stability stochastic approximation algorithm sufﬁces understand stability convergence deterministic differential equation. though analysis typically used show global asymptotic convergence stochastic approximation algorithm equilibrium point also used analyze local asymptotic stability properties stochastic approximation algorithm around equilibrium points. technique follow throughout entire work though brevity focus entirely analysis continuous time ordinary differential equation appeal standard results imply similar properties regarding discrete updates. given consideration focus proving stability dynamical system around equilbrium points i.e. points speciﬁcally appeal well known linearization theorem states jacobian dynamical system ∂h/∂θ|θ=θ evaluated equilibrium point hurwitz converge non-empty region around exponential rate. means system locally asymptotically stable precisely locally exponentially stable thus important contribution paper proof seemingly simple fact conditions jacobian dynamical system given update hurwitz matrix equilibrium trivial property show convex-concave games fact convex-concave leads substantially challenging analysis. addition this provide analysis based lyapunov’s stability theorem crux idea prove convergence sufﬁcient identify nonnegative energy function linearized system always decreases time importantly analysis provides insights dynamics lead convergence. section comprises main results paper showing proper conditions gradient descent updates gans locally exponentially stable around good equilibrium points requires loss strictly concave case wgans indeed show updates wgans cycle indeﬁnitely. leads propose simple regularization term able guarantee exponential stability concave loss including wgan rather requiring strict concavity. stochastic gradient descent objective expressed framework note local analysis show stochastic approximation algorithm necessarily converge equilibrium point still provides valuable characterization algorithm behave around points. note slightly different usage term equilibrium typically used literature refers nash equilibrium optimization problem. deﬁnitions equivalent corresponding min-max game dynamical systems meaning throughout paper point gradient update zero generator network maps latent space input space discriminator network maps input space classiﬁcation example real synthetic; concave function. recover traditional formulation taking logistic loss log); note convention slightly differs standard formulation case discriminator outputs real-valued logits loss function would implicitly scale probability. recover wasserstein simply taking assuming generator discriminator networks parameterized parameters respectively analyze simple stochastic gradient descent approach solving optimization problem. take simultaneous gradient steps method analysis leads following differential equation note alternative updates. rather updating generator discriminator according min-max problem above goodfellow also proposed modiﬁed update generator minimizes different objective −ez∼platent fact analyses consider paper apply equally case update equations jacobians equilibrium. presenting main results ﬁrst highlight understanding local stability gans non-trivial even generator discriminator simple forms. stated above optimization consists min-max game gradient descent algorithms converge game convex-concave objective must convex term minimized concave term maximized. indeed crucial assumption convergence proof original paper. however virtually parameterization real generator discriminator even representations linear objective convex-concave game proposition objective equation concave-concave objective i.e. concave respect discriminator generator parameters large part discriminator space including regions arbitrarily close equilibrium. concave inspection concave also concave reason. thus optimization involves concave minimization general difﬁcult problem. prove peculiarity linear discriminator system appendix show similar observations general parametrization also case thus major question remains whether optimization stable indeed several well-known properties optimization make seem though gradient descent optimization work theory. instance well-known optimal location pdata optimal discriminator output zero examples turn means generator distribution optimal generator. would seem imply system stable around equilibrium. however show gradient descent optimization locally asymptotically stable even natural parameterizations generator-discriminator pairs furthermore equilibrium although zero-discriminator property means generator stable independently joint dynamical system generator discriminator locally asymptotically stable around certain equilibrium points. section contains ﬁrst technical result establishing gans locally stable proper local conditions. although proofs deferred appendix elements emphasize conditions identiﬁed local stability hold. indeed proof rests conditions want highlight much possible also convey valuable intuition required convergence. formalize conditions denote support distribution probability density function supp p.d.f generator pθg. denote euclidean l-ball radius λmax denote largest smallest non-zero eigenvalues non-zero positive semideﬁnite matrix. null denote column space null space matrix respectively. finally deﬁne matrices integral analyses epdata∇t here matrices evaluated equilibrium point characterize shortly. signiﬁcance terms that proportional hessian objective respect discriminator parameters equilibrium proportional off-diagonal term hessian corresponding discriminator generator parameters. matrices also occur similar positions jacobian system equilibrium. discuss conditions guarantee exponential stability. conditions imposed equilibria small neighborhood around though state explicitly every assumption. first deﬁne good equilibria care correspond generator matches true distribution discriminator identically zero support distribution. described next implicitly also assumes discriminator generator representations powerful enough guarantee equilibria local neighborhood equilibrium. assumption generator matches true distribution rather strong assumption limits realizable case generator capable creating underlying data distribution. furthermore means discriminator powerful enough generator distribution equilibrium since typically expect case also provide alternative non-realizable assumption also sufﬁcient results i.e. system still stable. realizable non-realizable cases requirement all-zero discriminator remains. implicitly requires even generator representation rich enough discriminator identically zero generator equilibrium finally note conditions disallow equilibria outside neighborhood potentially even unstable. assumption discriminator linear parameters furthermore equilibrium point alternative assumption largely weakening assumption condition discriminator remains requirement generator give rise true distribution. however requirement discriminator linear parameters additional restriction seems unavoidable case technical reasons. further note fact generator/discriminator equilibrium still means pdata distributions indistinguishable although discriminator concerned. indeed nice characterization good equilibria discriminator cannot differentiate real generated samples. goal next identify strong curvature conditions imposed objective though locally equilibrium. first require objective strongly concave discriminator parameter space equilibrium however hand cannot objective strongly convex generator parameter space objective convex-concave even nicest scenario even arbitrarily close equilbrium. instead identify another convex function namely require strongly convex generator space equilibrium. since strong curvature assumptions allow systems locally unique equilibrium state relaxed form accommodates local subspace equilibria. furthermore state assumptions parts ﬁrst condition second condition parameter space. first condition straightforward making necessary loss concave show condition need local asymptotic convergence. assumption function satisﬁes next state conditions parameter space also allowing systems multiple equilibria locally ﬁrst deﬁne following property function speciﬁc point domain along direction either second derivative must non-zero derivatives must zero. example origin along along direction angle axis second derivative system require property formalized property convex functions whose hessians proportional dgkdg. provide intuition functions below. property satisﬁes property null i.e. \u0001θ). assumption iii. intuitive explanation non-negative functions represent relate objective. ﬁrst function function measures all-zero state second function measures true distribution; equilibrium functions zero. later given curvature ﬁrst discriminator space; similarly function given curvature second function representative curvature magnitude discriminator update generator space. intuition behind particular relation holds that moves away true distribution second function assumption increases also becomes suboptimal generator; result magnitude update increases too. note show lemma hessian functions assumption discriminator generator space respectively proportional relations involving functions objective together assumption basically allow consider systems reasonable strong curvature properties also allowing many equilibria local neighborhood speciﬁc sense. particular curvature ﬁrst function along direction perturb slightly along still ‘equilibrium discriminator’ deﬁned assumption i.e. supp similarly direction along curvature second function perturb slightly along direction remains ‘equilibrium generator’ deﬁned assumption i.e. pdata. prove formally lemma perturbations along directions yield equilibria then either longer all-zero state match true distribution. thus consider setup rank deﬁciencies dgkdg correspond equivalent equilibria ﬁnal assumption supports true generated distributions require generators sufﬁciently small neighborhood equilibrium distributions support true distribution. following this brieﬂy discuss relaxation assumption. assumption typically hold support covers whole space true distribution support smaller disjoint parts space nearby generators correspond slightly displaced versions distribution different support. latter scenario show appendix local exponential stability holds certain smoothness condition discriminator. speciﬁcally require also support small perturbations otherwise generator equilibrium. null space correspond equilibrium discriminators.) note relaxed assumption accounts larger class examples still strong also restricts certain simple systems. space constraints state discuss implications assumption greater detail appendix state main result. theorem dynamical system deﬁned objective equation updates equation locally exponentially stable respect equilibrium point assumptions hold equilibria small neighborhood around furthermore rate convergence governed eigenvalues jacobian system equilibrium strict negative real part upper bounded recall wish show hurwitz. first note negative semi-deﬁnite next crucial observation hessian term w.r.t. generator vanishes all-zero discriminator generators result objective value. fortunately means equilibrium non-convexity precluding local stability. then make crucial hurwitz provided strictly negative deﬁnite full column rank. however property holds positive deﬁnite full column rank. property recall rank deﬁciency subspace equilibria around consequently analyze stability system projected subspace orthogonal equilibria additionally also prove stability using lyapunov’s stability showing squared distance subspace equilibria always either decreases instantaneously remains constant. additional results. order illustrate assumptions theorem appendix consider simple learns multi-dimensional gaussian using quadratic discriminator linear generator. similar appendix consider case i.e. wasserstein show system perennially cycle around equilibrium point without converging. simple two-dimensional example visualized section thus gradient descent wgan optimization necessarily asymptotically stable. motivated considerations above section propose regularization penalty generator update uses term based upon gradient discriminator. crucially regularization term change parameter values equilibrium point time enhances local stability optimization procedure theory practice. although update equations require differentiate respect function another gradient term double backprop terms easily computed modern automatic differentiation tools. speciﬁcally propose regularized update local stability intuition regularizer perhaps easily understood considering changes jacobian equilibrium jacobian update although non-antisymmetric diagonal blocks block diagonal terms negative deﬁnite show theorem long choose small enough ηjdd guarantees updates locally asymptotically stable concave addition stability properties regularization term also addresses well known failure state gans called mode collapse lending foresight generator. updates provide foresight similar unrolled updates proposed metz although regularization much simpler provides ﬂexibility leverage foresight. practice method powerful complex slower -unrolled gans. discuss intuitive ways motivating regularizer appendix theorem dynamical system deﬁned objective equation updates equation locally exponentially stable equilibrium conditions theorem λmax further appropriate conditions similar these wgan system locally exponentially stable equilibrium rate convergence wgan governed eigenvalues jacobian equilibrium strict negative real part upper bounded brieﬂy present experimental results demonstrate regularization term also substantial practical promise. figure compare gradient regularization -unrolled gans architecture dataset metz system quickly spreads points instead ﬁrst exploring modes redistributing mass modes gradually. note conventional updates known enter mode collapse setup. similar results case stacked mnist dataset using dcgan i.e. three random digits mnist stacked together create distribution modes. finally figure presents streamline plots system true latent distribution uniform discriminator generator observe wgan system goes orbits expected original system converges. updates systems converge quickly true equilibrium. paper presented theoretical analysis local asymptotic stability optimization proper conditions. showed recently proposed wgan asymptotically stable conditions introduced gradient-based regularizer stabilizes traditional gans wgans improve convergence speed practice. results provide substantial insight nature optimization perhaps even offering clues methods worked well despite convex-concave. however also emphasize substantial limitations analysis directions future work. perhaps notably analysis provides understanding happens locally close equilibrium point. non-convex architectures possible seems plausible much stronger global convergence results could hold simple settings like linear quadratic second analysis show equilibrium points necessarily exist illustrates convergence exist points satisfy certain criteria existence question addressed previous work much analysis remains done here. gans rapidly becoming cornerstone deep learning methods theoretical practical understanding methods prove crucial moving ﬁeld forward. martin arjovsky soumith chintala léon bottou. wasserstein generative adversarial networks. proceedings international conference machine learning volume proceedings machine learning research pages sanjeev arora rong yingyu liang tengyu zhang. generalization equilibrium generative adversarial nets proceedings international conference machine learning volume proceedings machine learning research pages tong yanran athul paul jacob yoshua bengio wenjie mode regularized generative adversarial networks. fifth international conference learning representations ishaan gulrajani faruk ahmed martin arjovsky vincent dumoulin aaron courville. improved training wasserstein gans. thirty-ﬁrst annual conference neural information processing systems harold kushner george yin. stochastic approximation recursive algorithms applications volume stochastic modelling applied probability. springer-verlag york address christian ledig lucas theis ferenc huszar jose caballero andrew cunningham alejandro acosta andrew aitken alykhan tejani johannes totz zehan wang wenzhe shi. photo-realistic single image super-resolution using generative adversarial network. ieee conference computer vision pattern recognition july michael mathieu camille couprie yann lecun. deep multi-scale video prediction beyond mean square error. fourth international conference learning representations nguyen jeff clune yoshua bengio alexey dosovitskiy jason yosinski. plug play generative networks conditional iterative generation images latent space. ieee conference computer vision pattern recognition july alec radford luke metz soumith chintala. unsupervised representation learning deep convolutional generative adversarial networks. fourth international conference learning representations roth lucchi nowozin hofmann. stabilizing training generative adversarial networks regularization. thirty-ﬁrst annual conference neural information processing systems salimans goodfellow wojciech zaremba vicki cheung alec radford chen. improved techniques training gans. advances neural information processing systems pages jiajun chengkai zhang tianfan bill freeman josh tenenbaum. learning probabilistic latent space object shapes generative-adversarial modeling. advances neural information processing systems pages section present preliminaries non-linear systems theory particular formally deﬁne local stability dynamic systems present important theorem helps study stability non-linear systems. finally present modiﬁcation result crucial proving stability gans assumptions. consider system consisting variables whose time derivative deﬁned i.e. system stable chosen ball around equilibrium initialize system anywhere within sufﬁciently small ball around equilibrium system always stays within ball. note system either converge equilibrium orbit around equilibrium perennially within ball. contrast system unstable initializations arbitrarily close equilibrium escape \u0001-ball. finally asymptotic stability stronger notion stability implies region around equilibrium initialization within region converge equilibrium example gans always stable; however wgans stable asymptotically stable. extension multiple equilibria. note since system might multiple arbitrarily close equilibria subspace equilibria deﬁne asymptotic stability imply convergence equilibria neighborhood considered equilibrium. limt→∞ either considered equilibrium point origin equilibrium point within small neighborhood around origin. present lyapunov’s stability theorem used prove locally asymptotic stability given system. basic idea system asymptotically stable scalar energy function positive deﬁnite means positive everywhere zero equilibrium time derivative strictly negative around equilibrium. next present important tool simpliﬁes study stability non-linear systems. result linearize non-linear system near equilibrium analyze stability linearized system comment local stability original system. close equilibrium. turns hurwitz quadratic lyapunov function original system whose rate decrease also quadratic since quadratic remainder term show remainder term adds cubic term change lyapunov function. however smaller quadratic change near equilibrium therefore quadratic lyapunov function linearized system works lyapunov function original system too. analyses linearize system show jacobian hurwitz. however often useful identify quadratic lyapunov function system. unfortunately jacobians encounter hard come quadratic lyapunov function always strictly decreases. instead identify function either strictly decreases sometimes remains constant instantenously. lyapunov’s stability theorem help conclude anything stability case following corollary lasalle’s theorem sufﬁcient prove asymptotic stability case. finally prove extension linearization theorem helps deal analyzing stability special kind non-linear systems speciﬁcally multiple equilibria local neighborhood considered equilibrium. theorem though inuitively follows original linearization theorem itself standard theorem non-linear systems best knowledge. formally consider case system consists sets parameters equilibrium small perturbation along preserves equilibrium. show enough show jacobian respect hurwitz prove stability. proof. proof statement quite similar proof original theorem linearization. high level idea exponentially stable exists quadratic lyapunov function always decreasing system then show quadratic function works original non-linear system small neighborhood around equilibrium non-linear remainder terms sufﬁciently small. particular show converges zero converges value less subtle point however quadratic function would decrease within particular neighborhood around origin also particular neighborhood around origin. however within neighborhood guarantee exponentially approaches origin; might move away \u0001-neighborhood around origin does system exit system even converge carefully overcome this ﬁrst identifying ﬁrst crucial step show constant sufﬁciently small neighborhood around equilibrium show this consider taylor series expansion remainder around equilibrium. clearly expansion would constant term would linear term accounted already. finally term purely function small neighborhood around equilibrium equilibria). therefore write property soon cleverly chosen value theorem khalil positive deﬁnite symmetric matrix exists positive deﬁnite matrix then choose quadratic lyapunov function linearized system rate decrease given negative points except long ensure trajectory system remains neighborhood around origin λminθ system would exponentially converge equilibria near origin. call neighborhood i.e. within neighborhood lyapunov function strictly decreases non-linear system. brings second crucial part proof ensure always stay contain ball radius show sufﬁciently close initializations within ball radius displacement since approaches origin means system never exited bound much changes time consider taylor series expansion first constant term. next term purely function then that figure illustration theorem neighborhood within converges exponentially point γ-axis corresponds equilibrium. however initializations within preserve trajectory within lack guarantee behaves illustrated dashed trajectory. identify smaller ball within intitialization within ball well-behaved consquently ensures exponential convergence section consider general system considered main paper demonstrate gans concave-convex near equilibrium. particular consider following discriminator generator pair learning distribution clearly negative i.e. objective concave generator parameters holds parameters arbitrarily close all-zero discriminator parameter hand consider case then consider case second derivative simpliﬁes proof. derive jacobian begin subtly different algebraic form objective equation replacing term ez∼platent epθg pθgf effectively separate discriminator generator’s effects furthermore lower matrix turns zero. here implication assumption speciﬁcally generators within sufﬁciently small radius around equilibrium support therefore support. furthermore generators within radius perturbation generator going change support therefore support. show zero take vector perturbation generator space show here limit deﬁnition derivative along particular direction prove system stable need show matrix hurwitz. show later lemma furthermore full column rank indeed hurwitz. however conditions need dgkdg full rank recall discussion main paper assumption unique equilibrim locally. show case establishing relation matrices curvature functions assumption show null spaces matrices correspond subspace equilibria. then show lemma consider rotation system project space orthogonal subspace equilibria. theorem proved appendix sufﬁcient show jacobian projected system hurwitz. following discussion term equilibrium discriminator denote discriminator identically zero support equilibrium generator denote generator matches true distribution deﬁned assumption note equilibrium discriminator generator updates zero vice versa equilibrium generator. lemma dynamical system deﬁned objective equation updates equation assumptions exists unit vectors null null equilibrium point deﬁned assumption can’t immediately conclude corresponds true distribution. show discriminator update whose magnitude equal that ﬁrst note generator update zero too. therefore equilibrium point assumption conclude pdata. thus equilibrium generator i.e. paired equilibrium discriminator discriminator updates zero. summary slight perturbations along null null established discriminator generator individually satisfy requirements equilibrium discriminator generator pair therefore system equilibrium perturbations. lemma dynamical system deﬁned objective equation updates equation consider eigenvalue decompositions udλdud dgkdg colt null ugλgug colt null. consider projections tdθd tgθg. then block jacobian equilibrium corresponds projected system form proof. note columns correspond eigenvectors furthermore rows eigenvectors correspond zero eigenvalues. eigenvectors correspond local subspace equilibria lemma considers projection system space orthogonal subspace. ﬁrst address corner case either empty. case empty means discriminators neighborhood considered equilibrium identically zero support true distribution then generator discriminator update would zero time generator update would zero equilibrium discriminators. means considered point surrounded neighborhood equilibria. then system trivially exponentially stable since sufﬁciently close initialization already equilibrium. similarly empty means generators small neighborhood distribution namely true underlying distribution then generator update discriminator would zero furthermore since equilibrium generators discriminator updates would zero discriminator. thus situated neighborhood equilibria system trivially exponentially stable. handle general case. first note that jacobian block projected variables must jacobian original system derived lemma note that tdkddtt diagonal matrix positive eigenvalues. therefore since next similar manner show tgkt diagonal matrix positive eigenvalues. thus kdgtt full column rank. non-trivial step show matrix tdkdgtt fewer rows full column rank too. follow showed too. left null space subset left null space therefore projecting span hurt rank kdg. true observe lemma small perturbation along since always equilibrium discriminator i.e. true support must ut∇θddθd furthermore recall derivation jacobian ∇θgpθg outside support. then theorem dynamical system deﬁned objective equation updates equation locally exponentially stable respect equilibrium point assumptions hold equilibria small neighborhood around furthermore rate convergence governed eigenvalues jacobian system equilibrium strict negative real part upper bounded proof. lemma considered equilibrium point lies subspace equilibria small neighborhood. then lemma jacobian block corresponding subspace orthogonal this satsiﬁes properties lemma make hurwitz. conclude exponential stability system theorem eigenvalue bounds presented theorem follow lemma finally show indeed lyapunov function satisﬁes lasalle’s principle projected linearized system. fact linearized projected system jacobian decreases instantaneously remains constant. proof. note lyapunov function zero equilibrium projected system. furthermore straightforward verify rate changes given observe generator terms canceled out. clearly dkddtd positive deﬁnite; otherwise strictly negative. zero rate indeed zero ftdkdgtt term update proportional zero. again term zero full column rank. thus equilibrium means update discriminator parameters nonzero i.e. words identically stay manifold energy decrease. section relax assumption prove stability certain conditions. speciﬁcally recall originally required equilibrium generator share support perturbation generator. allow generator different supports perturbed instead impose conditions discriminator. ﬁrst condition equilibrium discriminator must zero support also supports small perturbations equilibrium slope discriminator function non-zero boundaries thus potentially encouraging generator push data points away true supp support. motivate second condition recall assumption could directions along perturb intention behind allowing directions could allow equivalent equilibrium discriminators neighborhood however relaxation assumption aiming perturbations correspond equilibrium discriminators satsify condition i.e. zero support perturbations too. need explicitly assume holds describe below. state assumption using terminology we’ve developed recall imposing property function epdata implied perturbations along directions function retains property discriminator zero support pdata functions epθg furthermore directions functions must identical perturbing along directions guarantees functions zero. then output perturbed discriminator would zero support perturbations examples. useful illustrate simple examples satisfy break conditions above clearer picture assumptions imply. first example satisﬁes conditions consider system pdata uniform generator uniform distribution interval parametrized discriminator polynomial example linear function θdx. note equilibrium then veriﬁed system hessian epdata] positive deﬁnite equilibrium thus trivially satisfying second assumption. simple example breaks assumptions speciﬁcally condition above consider system pdata point mass generator also point mass discriminator linear function θdx. again equilibrium surprisingly even though unique equilibrium hessian epdata] equilibrium turns zero. thus null space epdata] equilibrium corresponds whole parameter space. hand equilibrium epθg non-zero arbitrarily close equilibrium. thus second condition above null space ﬁrst hessian null space second hessian thereby breaking condition. shown system breaks condition fact locally exponentially stable proof. original proof holds needed equilibrium discriminator identically zero true support. prove parts proof required this. first extend lemma assumption. first observe vector null also satisﬁes null second condition assumption support then show perturbation discriminator within null space ‘equilibrium discriminator’ paired generator small neighborhood results zero updates generator. prove this recall consists around terms integrated ∇θgpθg. previous proof original version assumption used intricate fact terms. particular said generator within radius equilibrium support pdata therefore true support true support generator b\u0001g/ case weaker guarantee generator within perturbation supp. then combined support combined support generator b\u0001g/ section extend results local stability gans case true distribution represented generator generator space. hard problem general consider speciﬁc case discriminator linear parameters show system locally stable equilibrium surrounding equilibria formally consider discriminator form consider generator space necessarily contain true distribution however equilibrium point paired discriminator zero contains generator support true data generated data. must noted necessarily equilibrium discriminator. especially lies lower dimensional manifold could subspace all-zero discriminators. generator exist need words want means generated distribution true distribution representation identical. given generator space essentially restriction representation learned/chosen discriminator. richer representation computes many higher order moments data never equilibrium generator. prove theorem non-realizable case. main idea identical proof realizable case. however need careful number steps. ﬁrst prove result similar lemma derives jacobian system equilibrium. crucial step able ignore terms corresponding discriminator linear parameters i.e. thus hessian zero. terms jacobian identical realizable case assume equilibrium discriminator must identically zero. proof. proof slightly different lemma note hessian function epdata also null space hessian epdata] locally constant along unit vector null. sufﬁciently small equals value function equilibrium thus conclude support pdata assumption support generators small neighborhood identical support true distribution therefore discriminators equilibrium discriminators i.e. paired generator generator updates zero. locally equals value function equilibrium. since function proportional magnitude equilibrium discriminator’s update equals zero equilibrium. observe independent discriminator variables means generators along discriminator update must zero. words generators equilibrium generators non-realizable sense representation matches true distribution. summary slight perturbations along null null established discriminator generator individually satisfy requirements equilibrium discriminator generator pair therefore system equilibrium perturbations. order illustrate assumptions theorem consider simple learns ndimensional gaussian distribution latent variable drawn standard normal consider quadratic discriminator linear generator call resulting system proof. since system consists parameters arranged form matrices need vectorization calculus arrange parameters vector differentiate them/with respect them. verify given point indeed equilibrium look objective clearly therefore updates become zero implying equilibrium generator converged true distribution. prove locally stable need examine jacobian point. note since jacobian matrix cell pair discriminator-generator parameters need calculate second-order derivatives vectorizing parameter matrices ﬁrst calculate derivative discriminator updates respect discriminator itself. calculate derivative discriminator updates respect generator parameters. note using constant matrix matrix zeros ones deﬁned vectorization algebra; matrix vectorization equivalent transpose operator. square matrix tnnvec vec. show negative deﬁnite moment matrix negative multiplicative factor. proved theorem recall long full column rank matrix eigenvalues whose real components strictly negative. show full column rank ﬁrst observe last columns corresponding linearly independent because belongs null space observe left hand side positive semi-deﬁnite right hand side negative semideﬁnite. therefore terms must equal zero would imply i.e. thus jacobian indeed hurwitz. summary means assumption holds trivially zero eigenvalues matrices involved jacobian. means equilibria small neighborhood around considered equilibrium. therefore assumption also satisﬁed. finally since support distribution assumption also trivially satisﬁed. thus assumption holds system exponentially stable. consider speciﬁc case wgan learns zero mean gaussian distribution show exists points near certain equilibria system initialized point periodically come back initial point rather converge equilibrium. theorem wgan system learning zero mean gaussian distribution asymptotically stable equilibrium corresponding proof. order show system asymptotically stable show initializations system arbitrarily close equilibrium system goes orbits around equilibrium forever. simplicity ﬁrst prove one-dimensional gaussian later extend multi-dimensional case. quadratic discriminator linear generator wgan objective equation system system equilibria assume system initialized means system forever respective updates zero too. hence need focus variables shown initialized never becomes negative therefore focus equilibrium assuming examine changes time. rate change distance equilibrium quantity given observe term non-negative i.e. system never gets closer equilibrium. thus system half-space hope converge exit half-space becomes negative. however show exists initializations close equilibrium even exit half-space eventually re-enters going perpetual loop. speciﬁcally denote system time initialization satisfy analyze trajectory system. first note means system enters half-space immediately thus system converge considered equilibrium would reach time first observe time need time. know radius never decreased time claim system simply retraces back path along reaches time clearly claim system time described terms time prove observe statement true need show indeed true therefore applying i.e. system looped back original state following path mirrored across line since holds initializations arbitrarily close equilibrium arbitrarily close system asymptotically stable. before initialize also consider sophisticated initialization compared since positive deﬁnite uλut initialize uλaut least diagonal element positive strictly less corresponding diagonal element ﬁrst establish updates variables system remain eigenspace deﬁned point time variables expressed uλaut real diagonal matrices clearly true time assuming true arbitrary time observe updates thus true time therefore analyze system terms constant though independent -dimensional gaussian systems. then orbiting systems -dimensional updates must manifest too. speciﬁcally cycles would correspond diagonal initialized less section prove gradient-based regularizer stabilizes wgan system. besides property section provide alternative mathematical intuition based arg-max differentiation motivate regularization term. finally section discuss regularizer addresses mode collapse -unrolled updates. theorem dynamical system deﬁned objective equation updates equation locally exponentially stable equilibrium conditions theorem λmax further appropriate conditions similar these wgan system locally exponentially stable equilibrium rate convergence wgan governed eigenvalues jacobian equilibrium strict negative real part upper bounded prove result ﬁrst present jacobian system equilibrium presence gradient penalty. recall penalty basically adds extra −∇θg∇θd generator’s update. proof. note change jacobian would rows corresponding generator parameters. therefore focus additional terms rows. additional term added ∂η∇θg∇θd prove stability regularized system conventional gans. observe lemmas regarding subspace equilibria holds case too. again project system follows lemma dynamical system deﬁned objective equation updates equation consider eigenvalue decompositions udλdud dgkdg colt null ugλgug colt null. consider projections tdθd tgθg. then block jacobian equilibrium corresponds projected system form straightforward extend proof lemma prove lemma. recall theorem show hurwitz original system exponentially stable. non-regularized system showed making structure matrix. system design quadratic lyapunov function strictly decreases non-equilibria points. lemma dynamical system deﬁned objective equation updates equation λmax linearization system projected subspace orthogonal subspace equilibria exponentially stable lyapunov function where λmax therefore lyapunov function proof. note indeed positive deﬁnite. furthermore note rate decrease given pj). show strictly decreasing need show first note note choice therefore ηjdd share eigenvectors. thus null space term same speciﬁcally orthogonal words top-left block diagonal matrix strictly negative eigenvalues. similarly also know −ηtgjt diagonal matrix negative values. hence matrix negative deﬁnite. proceed wasserstain scenario. first equivalent assumptions wgan guarantee exponential stability regularized case. note even conditions unregularized update ensure asymptotic stability. first note linearity loss function necessary discriminator identically zero support system equilibrium could also constant support. thus relax assumption case accommodate this. assumption next state assumption equivalent assumption iii. recall earlier wanted epdata] satisfy property discriminator space. instead function require magnitude generator updates satisfy property discriminator space. note hessian function equilibrium kdgkt assumption note effect assumption function introduce different function here; either case original system asymptotically stable zero diagonal blocks jacobian. next retain assumption three assumptions need. begin lemma similar lemma proof. note kdgkt rium namely magnitude generator update. then assumption function locally constant along unit vector null discriminator update zero since generator matches true distribution. assumption means identical true support. then equilibrium discriminator update generator would zero. equilibrium namely magnitude function discriminator update. also sufﬁciently small discriminator update zero. furthermore generator update zero zero. thus discriminator constant throughout support. thus equilibrium point assumption conclude pdata. thus equilibrium generator update discriminator would zero. summary slight perturbations along null null established discriminator generator individually satisfy requirements equilibrium discriminator generator pair therefore system equilibrium perturbations. show system projected subspace orthogonal equilibrium subspace resulting jacobian reduced system hurwitz. earlier chose based matrix choose based lemma dynamical system deﬁned objective equation updates equation consider eigenvalue decompositions kdgkt dgkdg null ugλgug colt null. consider projections tdθd tgθg. then block jacobian equilibrium corresponds projected system form proof. observe form follows lemma substituting furthermore like seen before observe tgjt diagonal matrix positive eigenvalues therefore dgtd full column rank matrix projected subspace orthogonal null space. however need show dgtd fewer rows reduce rank. indeed true since effectively projection onto subspace orthogonal left null space. prove second part sufﬁcient show projected jacobian linearized system lemma hurwitz exponential stability original system follows theorem fact hurwitz follows usual lemma discriminator generator variables however simpliﬁed arrive given bound noting non-zero eigenvalues matrix also equal non-zero eigenvalues matrix therefore replace every occurrence kdgkt additionally show lyapunov function satisﬁes lasalle’s principle projected linearized system. fact linearized projected system jacobian decreases instantaneously remains constant. proof. note lyapunov function zero equilibrium projected system. furthermore straightforward verify rate changes given tgkt non-positive. clearly zero tgkt positive deﬁnite. rate indeed zero linearized system tgkt term becomes zero. system identically stay manifold need happens full column rank. case equilibrium. ideal world optimizer would hope access function maxθd basically optimal discriminator function generator; given this optimizer able update generator respect that. then update shown following since hope objective concave discriminator parameters approximate hessian also replacing optimal discriminator current discriminator following update rule equivalent original presented equation regularization term also natural intuitive connections important issue arises optimization called mode collapse. mode collapse situation enter irrecoverable failure state generator incorrectly assigns probability mass small region space. arises globally optimal strategy generator push mass towards single point discriminator conﬁdent real data point. overcome generator needs foresight must know collapses mass discriminator subsequently label collapsed point fake data. penalty indeed encodes foresight discriminator’s ability outdo generator quantiﬁed magnitude discriminator’s gradient. clearly generator seeks state spread data enough make sure discriminator obvious countermeasure fact show penalty term -unrolled gans similar structure intuitively provide one-step lookahead generator. precisely arrive -unrolled updates simplify updates replace unrolled η∇θd begin simplifying -unrolled updates. idea -unrolled update allow generator explicitly foresee discriminator would react update optimize accordingly ﬁrst step compute gradient respect gradients respect instances occur ﬁrst occurs second argument second occurs unrolled update ﬁrst argument. second step apply chain rule second gradient. compare updates equation show updates ﬂexible terms using lookahead. similar terms crucial difference latter every occurrence discriminator parameters additional unrolled update namely η∇θdv clearly provide power latter; however practice observe technique powerful -unrolled even -unrolled updates reason unrolled updates constrain small typically order step size. would possible increase greater magnitudes would equivalent coarse step size unrolling. method hand allows larger discriminator retained sense penalty provides extracting leveraging unrolled update ﬂexibly. stricter bound. prove bounds −λmax interested upper bound). order prove upper bound multiply equations equations respectively equations equations respectively ﬁrst step make fact third step lemma i.e. fact since λmin. upper bound using inequality? examine quadratic expression. since discriminant quadratic λmaxλmin λmin −λmin quadratic always takes sign speciﬁcally positive. next note quadratic reaches minimum −λmin. either satisfy −λmin −λmin. since former already upper bound derive upper bound latter case. interval quadratic increases therefore interval inequality rewritten plugging inside quadratic. plugging provide similar upper bound result though partially eigenvalues matrices structural properties jacobian regularized system. note upper bounds eigenvalues complex", "year": 2017}