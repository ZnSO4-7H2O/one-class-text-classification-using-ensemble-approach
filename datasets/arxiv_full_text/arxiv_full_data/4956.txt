{"title": "Selective Classification for Deep Neural Networks", "tag": ["cs.LG", "cs.AI"], "abstract": "Selective classification techniques (also known as reject option) have not yet been considered in the context of deep neural networks (DNNs). These techniques can potentially significantly improve DNNs prediction performance by trading-off coverage. In this paper we propose a method to construct a selective classifier given a trained neural network. Our method allows a user to set a desired risk level. At test time, the classifier rejects instances as needed, to grant the desired risk (with high probability). Empirical results over CIFAR and ImageNet convincingly demonstrate the viability of our method, which opens up possibilities to operate DNNs in mission-critical applications. For example, using our method an unprecedented 2% error in top-5 ImageNet classification can be guaranteed with probability 99.9%, and almost 60% test coverage.", "text": "selective classiﬁcation techniques considered context deep neural networks techniques potentially signiﬁcantly improve dnns prediction performance trading-oﬀ coverage. paper propose method construct selective classiﬁer given trained neural network. method allows user desired risk level. test time classiﬁer rejects instances needed grant desired risk empirical results cifar imagenet convincingly demonstrate viability method opens possibilities operate dnns mission-critical applications. example using method unprecedented error top- imagenet classiﬁcation guaranteed probability almost test coverage. self-awareness remains illusive hard deﬁne concept rudimentary kind selfawareness much easier grasp ability know don’t know make smarter. subﬁeld dealing capabilities machine learning called selective prediction around years main motivation selective prediction reduce error rate abstaining prediction doubt keeping coverage high possible. ultimate manifestation selective prediction classiﬁer equipped dial allows precise control desired true error rate keeping coverage classiﬁer high possible. many present future tasks performed predictive models dramatically enhanced high quality selective prediction. consider example autonomous driving. since cannot rely advent singularity superhuman must manage standard machine learning sometimes errs. deep autonomous driving network capable knowing doesn’t know respond certain situation disengaging advance alerting human driver take over? plenty mission-critical applications would likewise greatly beneﬁt eﬀective selective prediction. literature reject option quite extensive mainly discusses rejection mechanisms various hypothesis classes learning algorithms boosting nearestneighbors reject option rarely discussed context neural networks considered deep existing works consider cost-based rejection model whereby costs misclassiﬁcation abstaining must speciﬁed rejection mechanism optimized costs. proposed mechanism classiﬁcation based applying carefully selected threshold maximal neuronal response softmax layer. call mechanism softmax response cost model useful quantify involved costs many applications interest meaningful costs hard reason. consider alternative risk-coverage view selective classiﬁcation discussed ensemble techniques considered selective prediction rejection mechanisms typically based ensemble statistics however techniques presently hard realize context dnns could costly train suﬃciently many ensemble members. recently ghahramani proposed ensemble-like method measuring uncertainty dnns bypasses need train several ensemble members. method works sampling multiple dropout applications forward pass perturb network prediction randomly. monte-carlo dropout technique mentioned context selective prediction directly applied viable selective prediction method using threshold discuss here. paper consider classiﬁcation tasks goal learn selective classiﬁer standard classiﬁer rejection function. selective classiﬁer allow full guaranteed control true risk. ideal method able classify samples production desired level risk optimal coverage rate. reasonable assume optimal performance obtained pair trained together. ﬁrst step however consider simpler setting neural classiﬁer already given goal learn rejection function guarantee high probability desired error rate. consider known techniques rejection devise learning method chooses appropriate threshold ensures desired risk. given classiﬁer conﬁdence level desired risk method outputs selective classiﬁer whose test error larger probability least using well-known vgg- architecture apply method cifar- cifar- imagenet show dropout lead extremely eﬀective selective classiﬁcation. cifar datasets mechanisms achieve nearly identical results. however imagenet simpler mechanism signiﬁcantly superior. importantly show almost desirable risk level guaranteed surprisingly high coverage. example unprecedented error top- imagenet classiﬁcation guaranteed probability almost test coverage. problem setting consider standard multi-class classiﬁcation problem. feature space ﬁnite label representing classes. distribution classiﬁer function true risk w.r.t. given loss function example error. given labeled sampled i.i.d. empirical risk classiﬁer selective classiﬁer pair classiﬁer selection thus selective classiﬁer abstains prediction point performance selective classiﬁer quantiﬁed using coverage risk. fixing coverage deﬁned probability mass non-rejected region selective risk clearly risk selective classiﬁer traded-oﬀ coverage. entire performance proﬁle classiﬁer speciﬁed risk-coverage curve deﬁned risk function coverage consider following problem. given classiﬁer training sample conﬁdence parameter desired risk target goal create selection function selective risk satisﬁes section present general technique constructing selection function guaranteed performance based given classiﬁer conﬁdence-rate function assume anything interpretation rank sense conﬁdence function indicates conﬁdence prediction higher conﬁdence prediction section concerned question good goal generate selection function guaranteed performance given reminder paper loss function taken standard loss function training assumed sampled i.i.d. unknown distribution given also conﬁdence selection guaranteed risk learning algorithm appears algorithm algorithm receives input classiﬁer conﬁdence-rate function conﬁdence parameter target risk training algorithm performs binary search optimal bound guaranteeing required risk suﬃcient conﬁdence. algorithm outputs selective classiﬁer risk bound rest section analyze algorithm. make following lemma gives tightest possible numerical generalization bound single classiﬁer based test labeled sample. emphasize numerical bound lemma tightest possible setting. discussed analytic bounds derived using e.g. hoeﬀding inequality approximate numerical bound incur slack. selection function projection following theorem uniform convergence result procedure. proof sketch |gi| random variable giving size accepted examples iteration sgr. ﬁxed value lemma applied projected distribution sample consisting examples drawn product distribution sampling distribution labeled examples determined following process sample examples product distribution ﬁlter resulting number examples. therefore left-hand side equals consider classiﬁer assumed trained unknown distribution section consider conﬁdence-rate functions based previous work note ideal conﬁdence-rate function reﬂect true loss monotonicity. given would like following hold obviously cannot expect ideal given conﬁdence-rate functions useful analyze eﬀectiveness draw risk-coverage curve induced rejection function deﬁned risk-coverage curve shows relationship example figure risk-coverage curves plotted. conﬁdence-rate functions consider ideal shown empirically extremely eﬀective. ﬁrst conﬁdence-rate function consider around folklore years explicitly mentioned context reject option. function works follows given neural network classiﬁer last layer softmax denote soft response output class. conﬁdence-rate function deﬁned maxj∈y call function softmax response softmax responses often treated probabilities authors criticize approach noting that purposes ideal conﬁdence-rate function provide coherent ranking rather absolute probability values softmax responses potentially good candidates relative conﬁdence rates. familiar rigorous explanation intuitively motivated observing neuron activations. example figure depicts average response values every neuron second-to-last layer true positives false positives class mnist dataset x-axis corresponds neuron indices layer y-axis shows average responses green squares averages true positives boldface squares highlight strong responses circles correspond average response false positives. evident true positive activation response active neurons much higher false positive expected reﬂected ﬁnal softmax layer response. moreover seen large activation values spread many neurons indicating conﬁdence signal arises numerous patterns detected neurons layer. qualitatively similar behavior observed deeper layers. mc-dropout technique consider recently proposed quantify uncertainty neural networks estimate uncertainty given instance number feedforward iterations applied dropout last fully connected layer. uncertainty taken variance responses neuron corresponding probable class. consider minus uncertainty mc-dropout conﬁdence rate. models considered mc-dropout conﬁdence-rate functions induced rejection function figure present risk-coverage curves obtained three datasets. curves obtained computing validation risk coverage many values. evident risk-coverage proﬁle mc-dropout nearly identical cifar datasets. imagenet plot curves corresponding top- top- tasks dataset signiﬁcantly better mc-dropout tasks. example top- task coverage rejection error mc-dropout rejection incurs error. importantly risk-coverage curves show selective classiﬁcation potentially used dramatically reduce error three datasets. relative advantage rest experiments focus rating. selective guaranteed risk cifar- consider cifar-; details. used vgg- architecture adapted cifar- dataset adding massive dropout exactly described used data augmentation containing horizontal ﬂips vertical horizontal shifts rotations trained using momentum initial learning rate weight decay multiplicatively dropped learning rate every epochs trained epochs. setting reached validation accuracy used resulting network basis selective classiﬁer. applied algorithm conﬁdence-rating function training taken half standard cifar- validation randomly split equal parts. half consumed training reserved testing resulting bounds. thus training test sets approximately samples. applied routine several desired risk values obtained corresponding selective classiﬁer risk bound applications routine particularly small conﬁdence level applied selective classiﬁers reserved test computed selective classiﬁer test risk test coverage. results summarized table also include train risk train coverage computed selective classiﬁer training set. observing results table risk bound always close target risk moreover test risk always bounded bound required. finally possible guarantee method amazingly small error covering domain. selective guaranteed risk cifar- using architechture trained model cifar- applying data augmentation routine cifar- experiment. following precisly experimental design cfar- case obtained results table again generated tight bounds close desired target risk bounds never violated true risk. also possible dramatically reduce risk moderate compromise coverage. architecture used state-of-the coverage easily surpassed best known result cifar- currently stands using wide residual network architecture likely using wide residual network architecture could obtain signiﬁcantly better results. selective guaranteed risk imagenet used already trained image-net vgg- model based ilsvrc repeated experimental design sizes training test approximately results top- top- classiﬁcation tasks summarized tables respectively. also implemented resnet- architecture order qualitatively similar results obtained diﬀerent architecture. resnet- results imagenet top- top- classiﬁcation tasks summarized tables respectively. results show even challenging magenet resnet architectures selective classiﬁers extremely eﬀective appropriate coverage compromise classiﬁer easily surpasses best known results imagenet. surprisingly resnet known achieve better results preserves relative advantage relative values. presented algorithm learning selective classiﬁer whose risk fully controlled guaranteed high conﬁdence. empirical study validated algorithm challenging image classiﬁcation datasets showed guaranteed risk-control achievable. methods immediately used deep learning practitioners helping coping missioncritical tasks. believe work ﬁrst signiﬁcant step direction many research questions left open. starting point approach trained neural classiﬁer rejection mechanisms considered extremely eﬀective might possible identify superior mechanisms given classiﬁer believe however challenging open question would simultaneously train classiﬁer selection function optimize coverage given risk level. selective classiﬁcation intimately related active learning context linear classiﬁers would interesting explore potential relationship context neural classiﬁcation. paper studied selective classiﬁcation loss. would great importance extend techniques loss functions speciﬁcally regression fully control false-positive false-negative rates. work many applications. general classiﬁcation task controlled risk critical would beneﬁt using methods. obvious example medical applications utmost precision required rejections handled human experts. applications existence performance guarantees propose here essential. financial investment applications also obvious great many opportunities cherry-pick certain ones. futuristic application robotic sales representatives could extremely harmful would answer questions fully understand.", "year": 2017}