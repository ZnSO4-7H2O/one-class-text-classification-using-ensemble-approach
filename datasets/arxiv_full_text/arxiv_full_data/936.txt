{"title": "Deep Learning with Limited Numerical Precision", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network's behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding.", "text": "training large-scale deep neural networks often constrained available computational resources. study eﬀect limited precision data representation computation neural network training. within context low-precision ﬁxed-point computations observe rounding scheme play crucial role determining network’s behavior training. results show deep networks trained using -bit wide ﬁxed-point number representation using stochastic rounding incur little degradation classiﬁcation accuracy. also demonstrate energy-eﬃcient hardware accelerator implements low-precision ﬁxed-point arithmetic stochastic rounding. large extent success deep learning techniques contingent upon underlying hardware platform’s ability perform fast supervised training complex networks using large quantities labeled data. capability enables rapid evaluation diﬀerent network architectures thorough search space model hyperparameters. therefore come surprise recent years seen resurgence interest deploying largescale computing infrastructure designed speciﬁcally training deep neural networks. notable eﬀorts direction include distributed computing infrastructure using thousands cores high-end graphics processors combination cpus gpus scaled-up multiple nodes time natural error resiliency neural network architectures learning algorithms well-documented setting apart traditional workloads typically require precise computations number representations high dynamic range. well appreciated presence statistical approximation estimation errors high-precision computation context learning rather unnecessary moreover addition noise training shown improve neural network’s performance exception employing asynchronous version stochastic gradient descent algorithm reduce network traﬃc state-of-the-art large-scale deep learning systems fail adequately capitalize error-resiliency workloads. systems built assembling general-purpose computing hardware designed cater needs traditional workloads incurring high often unnecessary overhead required computational resources. work presented paper owes inception thinking possible leverage algorithm-level noise-tolerance relax certain constraints underlying hardware leading hardware-software co-optimized system achieves signiﬁcant improvement computational performance energy eﬃciency. allowing low-level hardware components perform approximate possibly non-deterministic computations exposing hardware-generated errors algorithm level computing stack forms ingredient developing systems. additionally low-level hardware changes need introduced manner preserves programming model beneﬁts readily absorbed application-level without incurring signiﬁcant software redevelopment costs. ﬁrst step towards achieving cross-layer codesign explore low-precision ﬁxed-point arithmetic deep neural network training special focus rounding mode adopted performing operations ﬁxed-point numbers. motivation move ﬁxed-point arithmetic twofold. firstly ﬁxed-point compute units typically faster consume less hardware resources power ﬂoating-point engines. smaller logic footprint ﬁxed-point arithmetic circuits would allow instantiation many units given area power budget. secondly low-precision data representation reduces memory footprint enabling larger models within given memory capacity. cumulatively could provide dramatically improved data-level parallelism. ﬁnding exploration deep neural networks trained using low-precision ﬁxedpoint arithmetic provided stochastic rounding scheme applied operating ﬁxed-point numbers. test validity proposed approach training deep neural networks mnist cifar image classiﬁcation tasks. deep networks trained using -bit wide ﬁxed-point stochastic rounding achieve nearly performance obtained trained using -bit ﬂoating-point computations. furthermore present hardware accelerator design prototyped fpga achieves high throughput power using large number ﬁxed-point arithmetic units dataﬂow architecture compact stochastic rounding modules. determining precision data representation compute units critical design choice hardware implementation artiﬁcial neural networks. surprisingly rich body literature exists aims quantify eﬀect choice network’s performance. however disproportionately large majority studies focused primarily implementing feed-forward stage assuming network trained oﬄine using high precision computations. recent studies embrace approach relied processor’s vector instructions perform multiple operations parallel employ reconﬁgurable hardware highthroughput energy-eﬃcient inference take route custom hardware implementations previous studies also investigated neural network training using diﬀerent number representations. iwata implements backpropagation algorithm using -bit ﬂoating-point processing units. hammerstrom presents framework on-chip learning using ﬁxed-point arithmetic. authors perform theoretical analysis understand neural network’s ability learn trained limited precision setting. results empirical evaluation simple networks indicate cases bits precision suﬃcient back-propagation learning. probabilistic rounding weight updates used reduce precision requirements gradient-based learning techniques. studies provide valuable insights behavior limited precision training neural networks networks considered often limited variants classical multilayer perceptron containing single hidden layer hidden units. extrapolating results state-of-the-art deep neural networks easily contain millions trainable parameters nontrivial. consequently need reassess impact limited precision computations within context contemporary deep neural network architectures datasets training procedures. recent work presents hardware accelerator deep neural network training employs ﬁxed-point computation units ﬁnds necessary -bit ﬁxed-point representation achieve convergence training convolutional neural network mnist dataset. contrast results show possible train networks using -bit ﬁxed-point numbers long stochastic rounding used ﬁxed-point computations. knowledge work represents ﬁrst study application stochastic rounding training deep neural networks using low-precision ﬁxed-point arithmetic. standard implementations deep neural network training back-propagation algorithm typically -bit ﬂoating-point representation real numbers data storage manipulation. instead consider generalized ﬁxed-point number representation correspond integer fractional part number respectively. number integer bits plus number fractional bits yields total number bits used represent number. product produces ﬁxed-point number format. thought temporary ﬁxed-point register enough width prevent saturation/overﬂow avoid loss precision accumulating products aibi. requirement width logd worst case. note worst case extremely rare occurs saturated either lower upper limit adopting two-step approach several advantages. firstly closely mimics behavior hardware implementation vector inner product using hardware units fpgas. units accept -bit inputs accumulate results macc operation -bit wide register. secondly invoking rounding mode accumulation sums signiﬁcantly reduce hardware overhead implementing stochastic rounding scheme. lastly adoption approach allows eﬃciently simulate ﬁxedpoint computations using cpus/gpus vendorsupplied blas libraries. instance matrix multiplication ﬁxed-point matrices simulated ﬁrst converting float matrices calling hardware-optimized sgemm routine applying convert function element resulting float matrix. evident sections follow rounding mode adopted converting number lower precision ﬁxed-point representation turns matter important consideration performing computations ﬁxed-point numbers. given number figure mnist dataset using fully connected dnns training error test error training using ﬁxed-point number representation rounding mode either round nearest stochastic rounding word length ﬁxed-point numbers kept ﬁxed bits results shown three diﬀerent fractional lengths bits. results using float also shown comparison. section present results investigation eﬀect employing limited precision data representation training deep neural networks. consider fully connected deep neural networks well convolutional neural networks present results mnist cifar datasets. baseline comparison ﬁrst evaluate network performance using conventional -bit ﬂoating-point arithmetic. subsequently constrain neural network parameters well intermediate variables generated back-propagation algorithm represented ﬁxed-point format train network starting random initialization parameters. training using ﬁxed-point diﬀerent model hyperparameters weight initialization regularization parameters learning rates etc. kept unchanged ones used fairly restrictive choice number representation important implications. perspective neural network training aggressive reduction precision parameter updates computed stored result loss gradient information updates signiﬁcantly smaller given ﬁxed-point format. consequence impede progress gradient descent algorithm worse introduce instabilities training procedure. note round-to-nearest scheme parameter update figure mnist dataset using cnns training error test error training using ﬁxed-point number representation rounding mode either round nearest stochastic rounding. word length ﬁxedpoint numbers kept ﬁxed bits results shown diﬀerent fractional lengths weights ﬁrst experiments construct fully connected neural network hidden layers containing units relu activation function train network recognize handwritten digits mnist dataset. dataset comprises training images test images image pixels containing digit pixel values normalized range. form data pre-processing augmentation performed. weights layer initialized sampling random values network trained using minibatch stochastic gradient descent minibatch size minimize cross entropy objective function. float baseline achieves test error next retrain network using ﬁxed-point computations bits. figure shows results rounding modes round-to-nearest stochastic rounding. cases allocating bits fractional part produces noticeable degradation either convergence rate classiﬁcation accuracy. reduction precision bits begins negatively impact network’s ability learn round-to-nearest scheme adopted. primarily reduced fractional precision parameter updates rounded zero. contrast stochastic rounding preserves gradient information atleast statistically network able learn bits precision without signiﬁcant loss performance. note however precision lower bits even stochastic rounding scheme unable fully prevent loss gradient information. using mnist dataset also evaluate architecture similar lenet- comprises convolutional layers ﬁlters relu activation function. ﬁrst layer feature maps second convolutional layer produces feature maps. convolutional layer followed pooling/subsampling layer. pooling layers implement pooling function non-overlapping pooling windows size output second pooling layer feeds fully connected layer consisting relu neurons connected -way softmax output layer. training network adopt exponentially decreasing learning rate scaling factor every epoch training. learning rate ﬁrst epoch momentum used speed convergence. weight decay parameter layers. figure cifar dataset using cnnstraining error test error training using ﬁxed-point number representation rounding mode either round nearest stochastic rounding. word length ﬁxedpoint numbers kept ﬁxed bits results shown diﬀerent fractional lengths weights weight updates bits. black arrows indicate epoch training carried using bits. results using float also shown comparison. trained using float network achieves test error done previously dnns retrain network using ﬁxed-point computations bits. however case saturating output convolutional layers integer value created diﬃculty jump-starting training procedure. result increase number bits allocated integer part expense representing layer outputs. figure compiles results obtained using diﬀerent rounding modes. unlike case dnns round-tonearest scheme adopted ﬁxed-point computations training procedure fails converge. stochastic rounding used achieve test error -bit -bit precision respectively corresponding slight degradation float baseline. test validity stochastic rounding approach consider another commonly used image classiﬁcation benchmark cifar. training consists images size pixels. images divided classes containing images. test images. scale image values range perform form data pre-processing augmentation. dataset construct convolutional layers followed subsampling/pooling layer. convolutional layers consist ﬁlters subsampling layers implement pooling function window size using stride pooling layer connects network training starts learning rate reduced factor epochs. using -bit ﬂoating point numbers training network conﬁguration misclassiﬁes approximately images test set. serves baseline comparing results obtained training network using ﬁxed-point computations. similar earlier experiments ﬁxed-point number test diﬀerent rounding modes fractional precision. observed previously shown figure training using ﬁxed-point round-to-nearest scheme begins collapse epochs. contrary stochastic rounding scheme appears bestow upon training procedure signiﬁcantly higher degree stability. bits fractional precision stochastic rounding scheme network’s behavior quite similar observed baseline evaluation achieves test error precision reduced convergence rate degrades learning proceeds point stops making progress. expected since reduced precision parameter updates tend become sparser perilous combination smaller gradients diminished learning rates. network’s performance suﬀers result minimum achievable test error saturates fortunately damage reversible shown figure results rapid improvement network’s performance. additional epochs training using higher precision representation test error approaches obtained using float. result reveals promising strategy deep neural network training network ﬁrst trained using low-precision ﬁxed-point arithmetic stochastic rounding. point learning shows stagnation network ﬁne-tuned using epochs higherprecision ﬁxed-point computations. concept employing mixed-precision computations explored previously context ﬂoating point arithmetic motivated largely fact modern processors achieve factor higher computational throughput single-precision ﬂoating-point compared double-precision ﬂoating-point. similar concepts conjunction stochastic rounding extended perform mixed-precision ﬁxed-point arithmetic. execution time mini-batch stochastic gradient descent algorithm dominated series gemm operations feed-forward error back-propagation weight update calculation steps. result improvement computational throughput gemm operation translates improvement training time. gpus oﬀering large number parallel vector processors high memory bandwidth therefore eﬀective accelerating workloads. section describe fpga-based hardware accelerator matrix-matrix multiplication. choice using fpgas hardware substrate motivated factors. firstly fpgas enable fast hardware development times signiﬁcantly lower costs compared asics. secondly modern while preparing paper became aware recent work shares motivations adopts orthogonal approach. authors propose dynamic ﬁxed-point training deep neural networks. however hardware implications approach immediately obvious. convolution also rewritten gemm operation application speciﬁc integrated circuits fpgas large number hard-wired ﬁxed-point units well-suited implementing ﬁxed-point arithmetic described earlier sections potentially yield gains performance power eﬃciency. however limited memory bandwidth must still carefully managed various design choices. prototype implemented oﬀ-the-shelf fpga card featuring xilinx kintext fpga memory communicating host pcie bus. fpga multiply-accumulate units almost on-chip block ram. data bandwidth oﬀ-chip memory fpga gb/s. typical dimensions input matrices preclude storing entire matrices on-chip ram. thus matrices stored memory parts matrices brought fpga performing computations. oﬀ-chip communication bandwidth limitation necessitates reuse on-chip data highest extent possible make achievable throughput measured giga-operations/second compute-bound. figure presents block diagram ﬁxedpoint matrix multiplier. units within fpga organized massively parallel dimensional systolic array size forms core multiplier described greater detail next subsection. block fpga designated cache fraction input matrices stored. read logic sends data requests memory organizes incoming data cache. write logic sends back computed results external memory. l-to-sa circuit moves relevant rows columns cache array. fifo. elements earlier cycles cascaded right corresponding partial products accumulated units. accumulation partial products output data cascaded stochastic rounding units also implemented units. rounded results stored output fifos ﬁnal readout external memory. throughput array depends number dsps available maximum operating frequency system operated without timing errors. example wavefront-type systolic array connections local i.e. neighboring dsps edge fifos limits interconnect delays improves maximum operating frequency. operation sequence multiplier follows. assume ﬁrst input matrix dimensions second input matrix dimensions initially columns matrix rows matrix largest integer choose based on-chip memory capacity constraints brought fpga compute elements result matrix. next columns matrix brought processed. continues columns matrix multiplied ﬁrst rows matrix entire sequence repeated l/pn times process rows matrix double buﬀering employed hide latency bringing subsets matrices chip. sequence operation ensures elements matrix reused times brought fpga matrix reused times. reuse allows eﬃcient bandwidth fpga memory. wavefront array depicted figure cycles corresponds inner dimension matrix multiplication macc unit accumulated partial products. point accumulated result transferred local register reset. frees receive data next matrix multiplication operation even elements completed. achieves high throughput systolic array long pipeline incoming data. word length result elements macc operations much larger word length inputs transferring output fifos result elements must trimmed stochastic rounding least signﬁcant bits truncation excess bits unit implements operations every clock cycle. elements input matrices brought l-cache staged local block units conﬁgured fifo queues. fifo contains elements either column clock cycle element read overﬂow/underﬂow). operations eﬃciently achieved using single unit output. column linear feedback shift register used generate random number whose width equal number bits rounded unit adds random number incoming result drops rounded bits. pattern-detect capabilities built used determine excess bits identical overﬂow/underﬂow condition detected result values saturated max/min complement values. result transferred output column fifos awaiting writeback external memory. overhead stochastic rounding thus logic occupied round units case units corresponding less overhead hardware resources. systolic array implemented kintexkt fpga xilinx’s vivado synthesis place-and-route tool estimated maximum circuit operation frequency power consumption translates throughput gops/s power eﬃciency g-ops/s/w. compares favorably intel i-qm nvidia gpus achieve power eﬃciency range g-ops/s/w table presents summary utilization various resources fpga. throughput numbers beneﬁt migration newer xilinx fpgas ultrascale series much higher number units potentially operate higher frequencies. direct stochastic rounding approach multibit magnitude comparison result random number followed conditional addition examining excess msbs. approach section achieves result removes ﬁrst full multi-bit comparison enabling compact implementation single unit. paper embrace top-down approach exploiting noise-tolerance deep neural networks training algorithms inﬂuence design low-level compute units. speciﬁcally substitution ﬂoating-point units ﬁxed-point arithmetic circuits comes signiﬁcant gains energy eﬃciency computational throughput potentially risking neural network’s performance. low-precision ﬁxed-point computations conventional rounding schemes fail adopting stochastic rounding deep neural network training delivers results nearly identical -bit ﬂoating-point computations. additionally implement highthroughput energy-eﬃcient architecture matrix multiplication incorporates stochastic rounding little overhead. extrapolating envision emergence hardware-software co-designed systems large-scale machine learning based relaxed inexact models computing running nondeterministic components across stack right low-level hardware circuitry.", "year": 2015}