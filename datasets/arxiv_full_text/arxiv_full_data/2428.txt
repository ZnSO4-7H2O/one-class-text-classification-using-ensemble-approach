{"title": "A Bayesian Approach to Learning Bayesian Networks with Local Structure", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Recently several researchers have investigated techniques for using data to learn Bayesian networks containing compact representations for the conditional probability distributions (CPDs) stored at each node. The majority of this work has concentrated on using decision-tree representations for the CPDs. In addition, researchers typically apply non-Bayesian (or asymptotically Bayesian) scoring functions such as MDL to evaluate the goodness-of-fit of networks to the data. In this paper we investigate a Bayesian approach to learning Bayesian networks that contain the more general decision-graph representations of the CPDs. First, we describe how to evaluate the posterior probability that is, the Bayesian score of such a network, given a database of observed cases. Second, we describe various search spaces that can be used, in conjunction with a scoring function and a search procedure, to identify one or more high-scoring networks. Finally, we present an experimental evaluation of the search spaces, using a greedy algorithm and a Bayesian scoring function.", "text": "recently several researchers investigated techniques using data learn bayesian networks containing compact representations conditional probability distributions stored node. majority work concentrated using decision-tree representations cpds. addition researchers typically apply non-bayesian scoring functions evaluate goodness-of-ﬁt networks data. paper investigate bayesian approach learning bayesian networks contain general decision-graph representations cpds. first describe evaluate posterior probability— bayesian score—of network given database observed cases. second describe various search spaces used conjunction scoring function search procedure identify high-scoring networks. finally present experimental evaluation search spaces using greedy algorithm bayesian scoring function. given observations domain common problem data analyst faces build models process generated data. last years researchers community contributed enormous body work problem using bayesian networks model choice. recent works include cooper herskovits substantial amount early work learning bayesian networks used observed data infer global independence constraints hold domain interest. global independences precisely follow missing edges within bayesian-network structure. recently researchers extended classical deﬁnition bayesian network include eﬃcient representations local constraints hold among parameters stored nodes network. notable features recent work majority eﬀort concentrated inferring decision trees structures explicitly represent parameter equality constraints researchers typically apply non-bayesian scoring functions evaluate goodness-of-ﬁt networks data. paper apply bayesian approach learning bayesian networks contain decision-graphs— generalizations decision trees encode arbitrary equality constraints—to represent conditional probability distributions nodes. section introduce notation previous relevant work. section describe evaluate bayesian score bayesian network contains decision graphs. section investigate search algorithm used conjunction scoring function identify networks data. section data various domains evaluate learning accuracy greedy search algorithm applied search spaces deﬁned section finally section conclude discussion future extensions work. section describe notation discuss previous relevant work. throughout remainder paper lower-case letters refer variables upper-case letters refer sets variables. write observe variable state observe state every variable call observations state although arguably abuse notation convenient index states variables single integer. example containing binary variables write denote section deﬁne bayesian network. section describe decision trees used represent probabilities within bayesian network. section describe decision graphs generalizations decision trees. consider domain discrete variables ﬁnite number states. bayesian network represents joint probability distribution encoding assertions conditional independence collection probability distributions. speciﬁcally bayesian network pair structure network parameters encode local probability distributions. structure components global structure local structures acyclic directed graph—dag short—that contains node variable edges denote probabilistic dependences among variables denote parent nodes refer variable corresponding node local structures mappings variable maps value parameter classical implementation bayesian network node stores distinct parameters large table. simply lookup table. note size table grows exponentially number parents often equality constraints hold among parameters researchers used mappings complete tables eﬃciently represent parameters. example consider global structure depicted figure assume nodes binary. furthermore assume value depend decision trees described detail breiman used represent sets parameters bayesian network. tree containing exactly root node every node root node exactly parent. leaf node contains table distinct parameters collectively deﬁne conditional probability distribution non-leaf node tree annotated name parent variables out-going edges node tree annotated mutually exclusive collectively exhaustive sets values variable node decision tree annotated name splits edge child annotated value child corresponding note deﬁnition edge annotations child node corresponding value unique. traverse decision tree parameter follows. first initialize root node decision tree. then long leaf node splits reset child corresponding value π—determined j—and repeat. leaf return parameter table corresponding state decision tree expressive mappings complete tables represent parameters complete table using complete decision tree. complete decision tree node tree depth every node level splits parent exactly children value follows definition complete tree distinct parameter distinct precisely behavior complete table. section describe generalization decision tree known decision graph represent much richer equality constraints among local parameters. decision graph identical decision tree except that decision graph nonroot nodes parent. consider example decision graph depicted figure decision graph represents conditional probability distribution node figure diﬀerent equality constraints tree shown figure speciﬁcally decision graph encodes equality denote decision graph node mapping node implemented instead denote mapping. decision-graph explicitly represent arbitrary equality constraints form demonstrate this consider complete tree node transform decision graph represents desired constraints simply merging together leaf nodes contain sets equal. showed section local structure node decision graph sets parameters identical derivations follow useful enumerate distinct parameter sets equivalently useful enumerate leaves decision graph. remainder section adopt following syntactic convention. referring parameter stored leaf decision graph denote node index denote parent-state index. referring parameter context speciﬁc parent state node denote node index denote parent-state index. enumerate leaves decision graph deﬁne leaf-set indices idea contains exactly parent-state index leaf graph. precisely denote number leaves deﬁned following properties assumption used derive equation closed form parameter independence assumption. simply stated assumption says given hypothesis knowledge distinct parameter give information distinct parameter set. many researchers derived bayesian measureof-ﬁt—herein called bayesian score—for network assuming equalities among parameters. friedman goldszmidt derive bayesian score structure containing decision trees. section show evaluate bayesian score structure containing decision graphs. derive bayesian score ﬁrst need make assumption process generated database particular assume database random sample unknown distribution constraints represented using network structure containing decision graphs. previous section structure imposes independence constraints must hold distribution represented using bayesian network structure. deﬁne hypothesis independence constraints imposed structure hold joint distribution database generated contains independence constraints. refer reader heckerman detailed discussion structure hypotheses. concerned relative scores various structures almost always case constant ignored. consequently extend deﬁnition bayesian score function proportional context bayesian networks containing complete tables. call bayesian scoring function uniform scoring function hyperparameters one. found prior works well practice easy implement. using additional assumptions heckerman show αabc derived prior bayesian network. idea αabc proportional prior probability obtained prior network states parameter θabc. speciﬁcally prior bayesian network single equivalent sample size used asses exponents denotes parents understood measure conﬁdence parameters call bayesian scoring function scoring function exponents assessed way. heckerman derive constraints context bayesian networks complete tables. full version paper show constraints follow using decision graphs well slight modiﬁcations additional assumptions. given scoring function evaluates merit bayesian-network structure learning bayesian networks data reduces search structures high score. chickering shows ﬁnding optimal structure containing complete tables mappings np-hard using bayesian scoring function. given result seems reasonable assume allowing decision-graph mappings problem remains hard consequently appropriate apply heuristic search techniques. using assumptions derive bayesian score structure contains decision graphs following completely analogous method heckerman showing result must deﬁne inverse function denote arbitrary parameter function denotes index triples maps determine counts nabc node follows. first initialize counts nabc zero. then case database denote value case respectively increment count nabc corresponding parameter θabc da). parameter found eﬃciently traversing root. scoring function node decomposable factored product functions depend node parents. node decomposability useful eﬃciently searching space global-network structures. note equation node decomposable long node decomposable. consider node-decomposable distributions perhaps simplest distribution assume uniform prior network structures. constant equation simple prior experiments described section another approach favor networks fewer parameters. example section deﬁne search space decisiongraph structures within single node assuming parent ﬁxed. space deﬁned apply space number well-known search algorithms. experiments described section example apply greedy search. section assume states search space correspond possible decision graphs node order search algorithm traverse space must deﬁne operators transform state another. pre-condition operator must change parameter constraints implied decision graph. would allow example complete split figure children would correspond impossible states third child would correspond original constraints single node generate complete decision tree repeatedly applying complete splits. discussed previous section represent parameter-set equalities merging leaves complete decision tree. consequently starting graph containing node exists series operators result possible parameter-set equalities. note also repeatedly merge leaves decision graph until single parameter resulting graph equivalent graph containing single node. therefore operators suﬃcient moving parameter constraints parameter constraints. although discuss here methods simplify decision graphs represent parameter constraints. complete-split operator actually needed ensure parameter equalities reached complete split replaced series binary splits resulting parameter-set constraints identical. included complete-split operator hopes would help lead search algorithm better structures. section compare greedy search performance various search spaces deﬁned including subsets operators. suppose decision-graph node non-leaf node annotated parent case independent given parents remove without violating decomposition given equation thus given ﬁxed structure learn local decision graphs nodes delete parents independent. also consider adding edges follows. node non-descendants learn decision graph delete parents contained decision graph. figure shows greedy algorithm uses combines ideas. experiments started algorithm structure contains edges graph consists single root node. note result merge operator decision graph rendered independent parents even contains node annotated simple example could repeatedly merge leaves single leaf node resulting graph implies depend parents. found experimentally that—when using algorithm figure —this phenomenon rare. testing parent deletions expensive chose check another greedy approach learning structures containing decision trees explored friedman goldszmidt idea score edge operations applying operation greedily learning local decision trees nodes who’s parents changed result operation. full version paper compare approach theirs. section investigate varying allowed operators aﬀects performance greedy search. disallowing merge operator search algorithms identify decision-tree local structures bayesian network. consequently learning accuracy changes context greedy search generalize local structures decision trees decision graphs. experiments described section measure learning accuracy posterior probability identiﬁed structure hypotheses. researchers often criteria predictive accuracy holdout structural diﬀerence generative model. reason criteria evaluating well search algorithm performs various search spaces goal search algorithm maximize scoring function. evaluating well bayesian scoring functions approximate criteria. ﬁrst experiment consider promoter gene sequences database irvine collection consisting cases. variables domain. variables represent base-pair values sequence four possible values. variable promoter binary indicates whether sequence promoter activity. goal learning domain build accurate model distribution consequently reasonable consider static graphical structure search decision graph node promoter. table shows relative bayesian scores best decision graph learned using greedy search various parameter priors search spaces. searches started decision graph containing single node current best operator applied step operator increased score current state. column corresponds diﬀerent restriction search space described section labels indicate operators greedy search allowed denotes complete splits denotes binary splits denotes merges. column labeled example shows results greedy search used binary splits merges complete splits. corresponds diﬀerent parameter-prior bayesian scoring function. u-pn scoring function special case scoring function prior network imposes uniform distribution variables. number following u-pn labels indicates equivalent-sample size results uniform prior structure hypotheses. value zero table denotes hypothesis lowest probability identiﬁed using given parameter prior. values denote natural logarithm many times likely identiﬁed hypothesis lowest probability. comparing relative values searches merges searches don’t merges without exception adding merge operator results signiﬁcantly probable structure hypothesis. therefore conclude greedy search decision graphs results better solutions greedy search decision trees. interesting observation complete-split operator actually reduces solution quality restrict search decision trees. performed identical experiment another classiﬁcation problem simplicity present results uniform scoring function. recall section uniform scoring function hyperparameters αabc one. second experiment splice-junction gene sequences database irvine repository. database also contains sequence problem predict whether position middle sequence intron-exon boundary exon-intron boundary neither. results given table used uniform prior structure hypotheses. ﬁnal experiments done alarm domain well-known benchmark bayesiannetwork learning algorithms. alarm network described beinlich handconstructed bayesian network used diagnosis medical domain. parameters network stored using complete tables. ﬁrst experiment alarm domain demonstrate ﬁxed global structure hypothesis identiﬁed searching local decision graphs nodes signiﬁcantly better hypothesis corresponding complete tables nodes. ﬁrst generated cases alarm network computed uniform bayesian score alarm network assuming parameter mappings complete tables. expect posterior model quite good we’re evaluating generative model structure. next using uniform scoring function applied greedy searches previous experiments identify good decision graphs nodes network. kept global structure ﬁxed identical global structure alarm network. results shown table values semantics previous tables. score given ﬁrst column labeled comp score complete-table model. table demonstrates search performance using decision graphs identify signiﬁcantly better models using decision trees. fact complete-table model attains score surprising upon examination probability tables stored table performance greedy algorithm combines local global structure search using diﬀerent sets operators alarm domain. also included result greedy algorithm searches global structure assuming complete tables. table performance restricted version greedy algorithm using diﬀerent sets operators alarm domain. also included result greedy algorithm initialized global structure alarm network searches global structure assuming complete tables. next experiment used alarm domain test structure-learning algorithm given section generated database cases used uniform scoring function uniform prior structure hypotheses. versions algorithm corresponding possible sets local-structure operators previous experiments. also greedy structure-search algorithm assumes complete tables nodes. initialized search global network structure edges operators single-edge modiﬁcations graph deletion addition reversal. table show results. column labeled comp corresponds greedy search structures complete tables. again note allow nodes contain decision graphs signiﬁcant improvement solution quality. note search complete-table structures out-performed algorithm restricted algorithm search decision trees containing either complete splits complete splits binary splits. ﬁnal experiment repeated previous experiment except allowed algorithm parents descendants generative model. restricted global search dags violate partial ordering alarm network. also greedy structure-search algorithm searches structures complete tables except initialized search alarm network. results experiment shown table table constrained searches exhibit relative behavior unconstrained searches. experiment alarm domain values presented measure performance search relative worst performance experiment. table compare search performance across experiments alarm domain. value zero table corresponds experiment operators learned hypothesis lowest posterior probability experiments operator restrictions considered alarm domain. values given table relative posterior probability. labels correspond experiment denotes ﬁrst experiment performed local searches static global structure denotes second experiment performed unconstrained structural searches denotes ﬁnal experiment performed constrained structural search. rather surprising hypothesis learned using global-structure search decision graphs higher posterior every hypothesis learned using generative static structures. paper showed derive bayesian score network structure contains parameter maps implemented decision graphs. deﬁned search space learning individual decision graphs within static global structure deﬁned greedy algorithm searches global local structure simultaneously. demonstrated experimentally greedy search structures containing decision graphs signiﬁcantly outperforms greedy search structures containing complete tables structures containing decision trees. consider extension decision graph mentioned section recall decision graph parameter sets stored table within leaves. decision graphs implemented parameter θabc must belong exactly parameter set. important heckerman geiger chickering learning bayesian networks combination knowledge statistical data. proceedings tenth conference uncertainty artiﬁcial intelligence seattle pages morgan kaufman. thiesson score information recursive exponential models incomplete data. technical report institute electronic systems aalborg university aalborg denmark. consequence property priors parameter sets dirichlet posterior distributions dirichlet well. dirichlet distribution conjugate respect likelihood observed data. result easy derive bayesian scoring function closed form. allow nodes within decision graph split node represent arbitrary parameter constraints form example consider baysian network two-variable domain parent decision graph splits represent constraint unfortunately allow types constraints dirichlet distribution longer conjugate respect likelihood data parameter independence assumption violated. consequently derivation described section apply. conjugate priors decision graph splits node exist however full version paper weaker version parameter independence derive bayesian score graphs closed form. conclude noting easy extend definition network structure represent constraints parameters diﬀerent nodes net= buntine work e.g. thiesson consider types constraints. bayesian score structures derived simple modiﬁcations approach described paper.", "year": 2013}