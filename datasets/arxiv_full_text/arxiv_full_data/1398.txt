{"title": "Deep Convolutional Neural Networks and Data Augmentation for  Environmental Sound Classification", "tag": ["cs.SD", "cs.CV", "cs.LG", "cs.NE"], "abstract": "The ability of deep convolutional neural networks (CNN) to learn discriminative spectro-temporal patterns makes them well suited to environmental sound classification. However, the relative scarcity of labeled data has impeded the exploitation of this family of high-capacity models. This study has two primary contributions: first, we propose a deep convolutional neural network architecture for environmental sound classification. Second, we propose the use of audio data augmentation for overcoming the problem of data scarcity and explore the influence of different augmentations on the performance of the proposed CNN architecture. Combined with data augmentation, the proposed model produces state-of-the-art results for environmental sound classification. We show that the improved performance stems from the combination of a deep, high-capacity model and an augmented training set: this combination outperforms both the proposed CNN without augmentation and a \"shallow\" dictionary learning model with augmentation. Finally, we examine the influence of each augmentation on the model's classification accuracy for each class, and observe that the accuracy for each class is influenced differently by each augmentation, suggesting that the performance of the model could be improved further by applying class-conditional data augmentation.", "text": "abstract—the ability deep convolutional neural networks learn discriminative spectro-temporal patterns makes well suited environmental sound classiﬁcation. however relative scarcity labeled data impeded exploitation family high-capacity models. study primary contributions ﬁrst propose deep convolutional neural network architecture environmental sound classiﬁcation. second propose audio data augmentation overcoming problem data scarcity explore inﬂuence different augmentations performance proposed architecture. combined data augmentation proposed model produces state-of-the-art results environmental sound classiﬁcation. show improved performance stems combination deep high-capacity model augmented training combination outperforms proposed without augmentation shallow dictionary learning model augmentation. finally examine inﬂuence augmentation model’s classiﬁcation accuracy class observe accuracy class inﬂuenced differently augmentation suggesting performance model could improved applying class-conditional data augmentation. cation received increasing attention research community recent years. applications range context aware computing surveillance noise mitigation enabled smart acoustic sensor networks date variety signal processing machine learning techniques applied problem including matrix factorization dictionary learning wavelet ﬁlterbanks recently deep neural networks reviews existing approaches. particular deep convolutional neural networks principle well suited problem environmental sound classiﬁcation ﬁrst capable capturing energy modulation patterns across time frequency applied spectrogram-like inputs shown important trait distinguishing different often noise-like sounds engines jackhammers second using convolutional kernels small salamon music audio research laboratory center urban science progress york university usa. bello music audio research laboratory york university usa. receptive ﬁeld network should principle able successfully learn later identify spectro-temporal patterns representative different sound classes even part sound masked sources traditional audio features melfrequency cepstral coefﬁcients fail application cnns environmental sound classiﬁcation limited date. instance proposed obtained comparable results yielded dictionary learning approach improve upon deep neural networks high model capacity particularly dependent availability large quantities training data order learn non-linear function input output generalizes well yields high classiﬁcation accuracy unseen data. possible explanation limited exploration cnns difﬁculty improve simpler models relative scarcity labeled data environmental sound classiﬁcation. several datasets released recent years still considerably smaller datasets available research example image classiﬁcation elegant solution problem data augmentation application deformations collection annotated training samples result additional training data concept data augmentation deformations applied labeled data change semantic meaning labels. taking example computer vision rotated translated mirrored scaled image would still coherent image thus possible apply deformations produce additional training data maintaining semantic validity label. training network additional deformed data hope network becomes invariant deformations generalizes better unseen data. semantics-preserving deformations also proposed audio domain shown increase model accuracy music classiﬁcation tasks however case environmental sound classiﬁcation application data augmentation relatively limited author reporting simple augmentation techniques proved unsatisfactory urbansoundk dataset given considerable increase training time generated negligible impact model accuracy. paper present deep convolutional neural network architecture localized kernels environmental sound classiﬁcation. furthermore propose data augmentation overcome problem data scarcity explore different types audio deformations inﬂuence model’s performance. show proposed architecture combination audio data augmentation yields state-of-the-art performance environmental sound classiﬁcation. deep convolutional neural network architecture proposed study comprised convolutional layers interleaved pooling operations followed fully connected layers. similar previously proposed feature learning approaches applied environmental sound classiﬁcation input network consists timefrequency patches taken log-scaled melspectrogram representation audio signal. speciﬁcally essentia extract log-scaled mel-spectrograms components covering audible frequency range using window size size duration. since excerpts evaluation dataset varying duration size input tf-patch seconds i.e. tfpatches extracted randomly full log-melspectrogram audio excerpt training described down. given input network trained learn parameters composite nonlinear function maps output -dimensional input tensor consisting feature maps collection -dimensional kernels represents valid convolution vector bias term point-wise activation function. thus shapes respectively. note ﬁrst layer network i.e. dimensions input tf-patch. apply strided max-pooling ﬁrst convolutional layers using stride size equal pooling dimensions reduces dimensions output feature maps consequently speeds training builds scale invariance network. ﬁnal layers fully-connected consist matrix product rather convolution proposed architecture parameterized follows ﬁlters receptive ﬁeld i.e. shape followed strided maxpooling last dimensions rectiﬁed linear unit activation function max. note small receptive ﬁeld compared input dimensions designed allow network learn small localized patterns fused subsequent layers gather evidence support larger time-frequency signatures indicative presence/absence different sound classes even spectro-temporal masking interfering sources. training model optimizes cross-entropy loss mini-batch stochastic gradient descent batch consists tf-patches randomly selected training data tf-patch taken random position time full log-mel-spectrogram representation training sample. constant learning rate dropout applied input last layers probability lregularization applied weights last layers penalty factor model trained epochs checkpointed epoch trained random minibatches training data exhausted validation used identify parameter setting achieving highest classiﬁcation accuracy prediction performed slicing test sample overlapping tf-patches making prediction tf-patch ﬁnally choosing samplelevel prediction class highest mean ouptut activation frames. implemented python lasagne used pescador manage multiplex data streams training. experiment different audio data augmentations resulting augmentation sets detailed below. deformation applied directly audio signal prior converting input representation used train network note augmentation important choose deformation time stretching slow speed audio sample sample time stretched factors pitch shifting raise lower pitch audio sample sample pitch shifted values pitch shifting since initial experiments indicated pitch shifting particularly beneﬁcial augmentation decided create second augmentation set. time sample pitch shifted larger values {−.−. dynamic range compression compress dynamic range sample using parameterizations taken dolby standard icecast online radio streaming server {music standard standard speech radio}. background noise sample another recording containing background sounds different types acoustic scenes. sample mixed acoustic scenes {street-workers street-trafﬁc streetpeople park}. generated using ·x+w·y audio signal original sample signal background scene weighting parameter chosen randomly uniform distribution range augmentations applied using muda library reader referred details implementation deformation. muda takes audio corresponding annotation jams format outputs deformed audio together enhanced jams containing parameters used deformation. ported original annotations provided dataset used evaluation study jams ﬁles made available online along post-deformation jams ﬁles. evaluate proposed architecture inﬂuence different augmentation sets urbansoundk dataset dataset comprised sound clips duration taken ﬁeld recordings. clips span environmental sound classes conditioner horn children playing bark drilling engine idling shot jackhammer siren street music. using dataset compare results study previously published approaches evaluated data including dictionary learning approach proposed proposed different architecture employ augmentation training. piczakcnn fig. left dashed line classiﬁcation accuracy without augmentation dictionary learning piczak’s proposed model right dashed line classiﬁcation accuracy sb-cnn augmentation. fig. confusion matrix proposed sb-cnn model augmentation. difference confusion matrices yielded sb-cnn without augmentation negative values diagonal mean confusion reduced augmentation positive values diagonal mean confusion increased augmentation. positive values along diagonal indicate overall classiﬁcation accuracy improved classes augmentation. convolutional layers followed dense layers ﬁlters ﬁrst layer tall span almost entire frequency dimension input network operates input channels mel-spectra deltas. proposed approach used comparison study evaluated terms classiﬁcation accuracy. dataset comes sorted stratiﬁed folds models evaluated using -fold cross validation report results plot generated accuracy scores folds. training proposed architecture training folds split validation identifying training epoch yields best model parameters training remaining folds. classiﬁcation accuracy proposed model presented figure left dashed line present performance proposed model original datast without augmentation. comparison also provide accuracy obtained dataset dictionary learning approach proposed proposed piczak proposed author). right dashed line provide performance figure provide difference confusion matrices yielded proposed model without augmentation. latter overall classiﬁcation accuracy improved classes augmentation. however observe augmentation also detrimental effect confusion speciﬁc pairs classes. instance note confusion conditioner drilling classes reduced augmentation confusion conditioner engine idling classes increased. gain insight inﬂuence augmentation performance proposed model sound class figure present difference classiﬁcation accuracy adding augmentation compared using original training broken sound class. bottom plot provide delta scores classes combined. classes affected positively augmentation types clear exceptions. particular conditioner class negatively affected augmentations. given sound class characterized continuous sound often background makes sense addition background noise mask presence class deteriorate performance model. general pitch augmentations greatest positive impact performance augmentation sets negative impact classes. half classes beneﬁt applying augmentations combined would application subset augmentations. suggests performance model could improved application class-conditional augmentation training could validation identify augmentations improve model’s classiﬁcation accuracy class selectively augment training data accordingly. intend explore idea future work. article proposed deep convolutional neural combination network architecture which audio data augmentations produces state-of-the-art results environmental sound classiﬁcation. showed improved performance stems combination deep high-capacity model augmented training combination outperformed proposed without augmentation shallow dictionary learning model augmentation. finally examined inﬂuence augmentation model’s classiﬁcation accuracy. observed performance model sound class inﬂuenced differently augmentation suggesting performance model could improved applying class-conditional data augmentation. authors would like thank brian mcfee eric humphrey valuable feedback karol piczak providing details results reported work partially supported award fig. difference classiﬁcation accuracy class function augmentation applied time shift pitch shift dynamic range compression background noise combined proposed sb-cnn performs comparably piczakcnn training original dataset without augmentation original dataset large/varied enough convolutional model outperform shallow approach. however increase size/variance dataset means proposed augmentations performance proposed model increases signiﬁcantly yielding mean accuracy corresponding per-class accuracies importantly note proposed approach performs comparably shallow learning approach original dataset signiﬁcantly outperforms using augmented training set. furthermore increasing capacity model yield improvement classiﬁcation accuracy. indicates superior performance proposed sb-cnn augmented training rather thanks combination augmented training increased capacity representational power deep learning model. simard steinkraus platt best practices convolutional neural networks applied visual document analysis. international conference document analysis recognition vol. edinburgh scottland aug. parascandolo huttunen virtanen recurrent neural networks polyphonic sound event detection real life recordings international conference acoustics speech signal processing shanghai china mar. bogdanov wack g´omez gulati herrera mayor roma salamon zapata serra essentia audio analysis library music information retrieval int. soc. music info. retrieval conf. curitiba brazil nov. bottou large-scale machine learning stochastic gradient descent international conference computational statistics paris france aug. available http//dx.doi.org/./---- dieleman schl¨uter raffel olson sønderby nouri maturana thoma battenberg kelly lasagne first release https//github.com/lasagne/lasagne available http//dx.doi.org/./zenodo. humphrey salamon nieto forsyth bittner bello jams json annotated music speciﬁcation reproducible research int. soc. music info. retrieval conf. taipei taiwan oct. mesaros heittola dikmen virtanen sound event detection real life recordings using coupled matrix factorization spectral representations class activity annotations international conference acoustics speech signal processing brisbane australia apr. benetos lafay lagrange plumbley detection overlapping acoustic events using temporally-constrained probabilistic model ieee international conference acoustics speech signal processing shanghai china mar. bisot serizel essid richard acoustic scene classiﬁcation matrix factorization unsupervised feature learning ieee international conference acoustics speech signal processing shanghai china mar. salamon bello unsupervised feature learning urban sound classiﬁcation ieee int. conf. acoustics speech signal processing brisbane australia apr. geiger helwani improving event detection audio surveillance using gabor ﬁlterbank features european signal processing conference nice france aug. cakir heittola huttunen virtanen polyphonic sound label deep neural networks event detection using multi international joint conference neural networks july piczak environmental sound classiﬁcation convolutional neural networks international workshop machine learning signal processing boston sep. giannoulis benetos stowell rossignol lagrange plumbley detection classiﬁcation acoustic scenes events ieee aasp challenge ieee workshop applications signal processing audio acoustics paltz oct. sigtia stark krstulovic plumbley automatic environmental sound recognition performance versus computational cost ieee/acm transactions audio speech language processing vol. mesaros fagerlund hiltunen heittola virtanen sound events development dataset available online http//dx.doi.org/./zenodo. available http//dx.doi.org/./zenodo.", "year": 2016}