{"title": "A Slice Sampler for Restricted Hierarchical Beta Process with  Applications to Shared Subspace Learning", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Hierarchical beta process has found interesting applications in recent years. In this paper we present a modified hierarchical beta process prior with applications to hierarchical modeling of multiple data sources. The novel use of the prior over a hierarchical factor model allows factors to be shared across different sources. We derive a slice sampler for this model, enabling tractable inference even when the likelihood and the prior over parameters are non-conjugate. This allows the application of the model in much wider contexts without restrictions. We present two different data generative models a linear GaussianGaussian model for real valued data and a linear Poisson-gamma model for count data. Encouraging transfer learning results are shown for two real world applications text modeling and content based image retrieval.", "text": "hierarchical beta process found interesting applications recent years. paper present modiﬁed hierarchical beta process prior applications hierarchical modeling multiple data sources. novel prior hierarchical factor model allows factors shared across different sources. derive slice sampler model enabling tractable inference even likelihood prior parameters non-conjugate. allows application model much wider contexts without restrictions. present different data generative models linear gaussiangaussian model real valued data linear poisson-gamma model count data. encouraging transfer learning results shown real world applications text modeling content based image retrieval. hierarchical modeling becoming increasingly popular bayesian statistics allows joint modeling multiple data sources permitting shared patterns. successful example hierarchy text modeling hierarchical dirichlet process used prior mixture model allowing shared mixture components across multiple data sources. data point data source modeled using -out-of-k topics shared across different sources. many applications easy separate data mutually exclusive groups appropriate approach factor analysis data point modeled using p-out-of-k factors. beneﬁts joint modeling still retained sharing factors across different sources. prior inﬁnite binary matrices similar works differing inference methods proposed signiﬁcant contribution thibaux jordan uncovers predictive process underlying beta process concentration parameter equaling one. addition propose hierarchical beta process model demonstrate document classiﬁcation single data source. extended modeling multiple data sources utilizing nonparametric prior factor models similar used nonparametric prior mixture models. however combining factor model inference often intractable. mainly need integrate parameters invoking factors according prior. efﬁcient computation integration conjugacy required parameter priors likelihood. however requirement conjugacy restricts applicability model. approach deal non-conjugacy issues explicit representation samples drawn beta process. requires representation inﬁnite atoms practically impossible task. circumvent problem possible solution truncate beta process. however pre-speciﬁed truncation level arbitrary. avoid arbitrary truncation based factor model slice sampling technique turns inﬁnite representation problem ﬁnite representation problem. similar approach dirichlet processes taken walker however slice sampler based factor model developed. attempt nonparametric hierarchical factor analysis using made however inference model many approximation steps risk sampling incorrect posterior distributions. thus accurate inference methods need developed. paper take prior replace upper beta process layer prior i.e. concentration parameter underlying beta process ﬁxed one. refer modiﬁed hierarchical prior restricted hierarchical beta process modiﬁcation allows stick breaking construction explicitly represent samples underlying beta process. handle resultant inﬁnite representation problem present novel slice sampler proposed r-hbp deriving inﬁnite limit necessary tractable posterior stick-weights ibp. data source level still retain beta processes arbitrary concentration parameters allowing arbitrary sharing data sources. hierarchical factor analysis propose different data models linear gaussian data model real-valued data poisson-gamma model count data. linear poisson-gamma model present novel inference using auxiliary variables. synthetic experiments illustrate behavior model demonstrating correctly discovers parameters including dimensionalities factor subspaces. demonstrate proposed models real world tasks text modeling content based image retrieval. text modeling demonstrated nips dataset successfully show beneﬁts transfer learning. addition show linear poisson-gamma model achieves much better perplexity models linear gaussian model mixture model. content based image retrieval using nus-wide animal dataset demonstrate method outperforms recent state-of-the-art methods. rest paper organized follows section presents brief background related nonparametric priors. section describes modeling distributions nonparametric hierarchical factor analysis. section presents inference covering novel slice sampling along gibbs sampling. section demonstrates experiments using synthetic real-world text image data. section concludes paper. positive concentration parameter base measure beta process completely random measure implying disjoint sets measures independent random variables draws beta process discrete probability one. using property draw beta process written βkδφk atom drawn random weight assigned according following ξkδφk continuous i.i.d. draws poisson process product space lévy measure building hierarchy beta processes thibaux jordan introduced hierarchical beta process prior allows sharing atoms across multiple data sources. hierarchical beta process constructed following draw beta process base measure emanate child beta processes using base measure finally used parametrize bernoulli process denoted denotes i-th nary matrix support random measure base measure passed allowing sharing atoms across j-sources. indian buffet process nonparametric predictive prior developed generative model inﬁnite binary matrices. property exchangeable i.e. permutation {··· given exchangeability property interesting underlying finetti stochastic process investigating question thibaux jordan discovered underlying stochastic process beta process concentration parameter equal models construct binary matrix sequential process followed. however keeps underlying beta process hidden. many applications required represent underlying beta process explicitly. applications needs know sample beta process underlying ibp. solution problem proposed derived stick-breaking construction related scheme similar stick breaking construction dirichlet processes. denote beta process underlying assume using inﬁconsider joint factor analysis rhbp prior infer dimensionalities factor subspaces. goal model multiple data matrices ...xj jointly using factor matrix denotes k-th factor. factors shared amongst various data sources whereas others factors speciﬁc individual sources. allow number factors grow inﬁnitely data observed. infer value represent matrix element-wise multiplication matrices i.e. implies presence factor i-th data point source represents corresponding coefﬁcient weight factor collection matrices modeled using r-hbp prior. particular model i-th draw bernoulli proj cess parametrized beta process i.e. deﬁned note sampling according allows sharing factors φk’s across data sources. stickibp used data sources. factor analysis parameters propose different models gaussiangaussian model real-valued data poissongamma model count data. whole model summarized restricted hierarchical beta process deﬁned hierarchical beta process parent beta process concentration parameter equal one. words parent beta process r-hbp beta process predictive process indian buffet process. formally nhfa model addition introduce auxiliary variable purpose auxiliary variable turn inﬁnite representation beta process ﬁnite representation. integrate sample notation-wise superscript attached variable followed sign means excluding variable indexed superscript. following describe inference steps. details derivations provided supplementary material joint distribution written denote index active features index index also represented although inactive feature. sample according value extend stick breaking representation extended representation sampled prior distributions sampled following conditional distribution sampling obtained using adaptive rejection sampling procedure distribution log-concave sampling auxiliary variable straight-forward sampling discrete distribution ﬁnite support. experiments show correct recovery parameters. next demonstrate usefulness model real-world tasks text modeling content based image retrieval. synthetic real-world tasks priors hyperparameters chosen following gamma nhfa-ggm shape scale parameters gamma priors nhfa-pgm since expect results sparse shape parameters hyperparameters whereas scale parameters sampled gamma supplementary material provides details. experiments-i synthetic data create synthetic dataset similar show beneﬁts model vis-à-vis model considered dataset create twelve binary factors distribute across data sources termed ﬁrst four factors used data points next four factors used data points last four factors shared data points across generated mixing conﬁguration matrices randomly discrete support weight/coefﬁcient matrices sources i.e. sampled gamma distribution shape scale parameters respectively. using parameters along noise data generated poisson distributions according generative model described section verify correctness inference slice sampler along gibbs updates detailed section starting value one. observe sampler converges true value number factors less iterations. however sampler longer verify mode posterior number factors remains true value. seen figure model correctly learns factors scores automatically. compare computational efﬁciency slice sampler approximate gibbs sampler methods need sample zφw}. sampling remain identical cases. sampling differ. case approximate gibbs sampler conditioned {mlz} drawn beta distribution whereas slice sampler conditioned {ρmlz} uses adaptive rejection sampling although fast sampling beta distribution quite efﬁcient sampling dimensional space. comparing sampling slice sampler efﬁcient approximate gibbs sampler. main difference lies sampling number factors gibbs sampling updates parameters remains updates described sampling again gibbs sampling updates remains updates described hyperparameter controls variation around vary high value concentration random distribution increases. since random distribution shared across different data sources increases probability sharing factors. predictive likelihood denote test data j-th source matrix corresponding matrix factorization monte carlo approximation predictive likelihood written carry variety experiments demonstrate effectiveness proposed nhfa model. illustrate behavior model ﬁrst perform experiments synthetic dataset true dimensionality subspaces parameters known. figure synthetic data experiments true factors inferred factors number active factors gibbs iterations true factor scores source- true factor scores source- inferred factor scores source- inferred factor scores source-; easy comparison columns permuted according mapping i-th data point j-th source slice sampling grows gradually based slice variable requires sampling variables using ars. hand approximate gibbs sampler sample approximates intractable integral using monte carlo samples. synthetic experiments described above slice sampler takes iterations converge runs minutes. hand approximate gibbs sampler requires iterations converge true number factors taking totally minutes. timing analysis performed using windows intel ram. dataset intend show strength model transfer learning. this choose section target data source sections auxiliary data sources. follow scheme reasons. first believe although underlying sharing across different sections section focus differs distribution. nhfa exploits sharing across different sections still retaining focus individual section maintaining hierarchy. second allows compare results baselines dataset similar settings. results using nips dataset ﬁrst real-world dataset nips dataset contains articles proceedings neural information processing systems conference published dataset text articles divided nine different sections/tracks plus miscellaneous section/track. work nine sections cognitive science neuroscience learning theory algorithms architecture implementations speech signal processing visual processing applications control navigation planning treat section data source generate nine different term-document matrices. ensure matrices common dictionary terms. articles total across nine sections randomly select articles section training. similar test chosen section kept ﬁxed throughexperimentation nips dataset. average number words article approximately compute perplexity test report performance terms perplexity document. given training data test j-th data source perplexity document deﬁned figure perplexity results using nips dataset perplexity test data section number training documents baseline-a baseline-b proposed nhfa-pgm mean average perplexity test data section versus different nips auxiliary sections using proposed nhfa-pgm. baseline-b baseline linear poisson-gamma based factor analysis model similar ﬁrst baseline also uses auxiliary data simply augmenting target data maintain hierarchy. baseline- baseline hierarchical model recently proposed gupta model attempts learn shared individual subspace using hierarchical model. data distributions model similar nhfa-ggm model proposed section however uses series approximations inference. experimental results proposed nhfapgm baseline models compute perplexity values independent runs. select section target source sections auxiliary compute perplexity values increasing number target training articles varying step this intention point auxiliary data ceases improve generalization performance. figure depicts perplexity results proposed model compares baseline-a baseline-b. seen figure proposed nhfa-pgm model achieves signiﬁcantly lower perplexity compared baselines irrespective number training articles section. comparing results between baseline-a baseline long number training documents section less performance baseline-b better baseline goes according intuition auxiliary source help learning patterns. soon enough training articles section simply combining auxiliary data help performance baseline-b goes lower baseline-a. clearly shows baseline-b prone negative transfer learning whereas true proposed nhfa-pgm model retains perplexity values compared baseline-a. figure focuses results nhfa-pgm model provides mean average perplexity auxiliary section seen ﬁgure three auxiliary sections help maximum generalization decreasing order performance. table lists comparison proposed nhfa-pgm nhfa-ggm models baseline- baseline-. clearly since nips data matrices count data nhfa-pgm baseline- achieve signiﬁcantly lower perplexity compared nhfa-ggm baseline-. proposed nhfa-pgm clearly outperforms baseline- suggesting poisson-gamma model better count data topic models. comparing perplexity values baseline nhfa-ggm nhfa-ggm achieves better performance. gain performance attributed exact inference made possible slice sampling. table comparison image retrieval results recent stateof-the multi-view learning hierarchical modeling techniques using nus-wide animal dataset. presented novel slice sampler restricted hierarchical beta process feature proposed sampler keeps inference tractable even prior likelihood non-conjugate therefore applied joint modeling multiple data sources elaborate settings. another feature sampler offers exact inference need series approximations used standard gibbs samplers combined sampler hierarchical factor analysis data models linear poisson-gamma model linear gaussian-gaussian model modeling count data real-valued data respectively. show utility proposed models applied learning shared individual subspaces across multiple data sources unlike parametric models subspace dimensionalities learned automatically. using real world datasets demonstrated method outperforms recent state-of-the-art methods text modeling content based image retrieval. further slice sampling derived paper general utilized matrix factorizations. second dataset subset nus-wide dataset. subset comprises images involving animals. different low-level features namely color histogram color correlogram edge direction histogram wavelet texture blockwise color moments. construct real-valued feature matrix animal category treat separate data source hierarchical framework. done belief features different animal categories vary distributions. images images used training remaining images held testing. done comparing identical training test settings used. unlike nips data feature matrices nuswide images real valued. therefore nhfaggm model learn factor matrix learnt matrix construct test matrix test data compute subspace coefﬁcient matrix retrieve similar ages r-th test image cosine similarity subspace coefﬁcient vector subspace coefﬁcient matrices training data computed. retrieved images ranked decreasing order similarity values. evaluation measures baselines evaluate performance proposed method standard precision-scope curve mean average precision compare result proposed nhfa-ggm recent state-of-the-art multi-view learning techniques nhfa-ggm model experimental results table compares proposed nhfa-ggm recent works image retrieval using nus-wide animal dataset. comparison based mean average precision values presented note dataset used generating results identical. works values reported using topics. nhfa-ggm model nonparametric model reports total number shared individual factors varying contrary proposed nhfa-ggm exact version nhfa-ggm uses factors typically. clearly seen table nhfa-ggm outperforms baselines. focusing exact approximate models report precision scope curve shows nhfa-ggm model achieves clearly higher precision initial scope levels often knowles ghahramani. inﬁnite sparse factor analysis inﬁnite independent components analysis. independent component analysis signal separation pages yang e.x. ping a.g. hauptmann. harmonium models semantic video representation classiﬁcation. proceedings siam international conference data mining pages", "year": 2012}