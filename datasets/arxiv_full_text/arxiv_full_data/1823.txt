{"title": "Adversarial Feature Matching for Text Generation", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "The Generative Adversarial Network (GAN) has achieved great success in generating realistic (real-valued) synthetic data. However, convergence issues and difficulties dealing with discrete data hinder the applicability of GAN to text. We propose a framework for generating realistic text via adversarial training. We employ a long short-term memory network as generator, and a convolutional network as discriminator. Instead of using the standard objective of GAN, we propose matching the high-dimensional latent feature distributions of real and synthetic sentences, via a kernelized discrepancy metric. This eases adversarial training by alleviating the mode-collapsing problem. Our experiments show superior performance in quantitative evaluation, and demonstrate that our model can generate realistic-looking sentences.", "text": "generative adversarial network achieved great success generating realistic synthetic data. however convergence issues difﬁculties dealing discrete data hinder applicability text. propose framework generating realistic text adversarial training. employ long shortterm memory network generator convolutional network discriminator. instead using standard objective propose matching high-dimensional latent feature distributions real synthetic sentences kernelized discrepancy metric. eases adversarial training alleviating mode-collapsing problem. experiments show superior performance quantitative evaluation demonstrate model generate realistic-looking sentences. generating meaningful coherent sentences central many natural language processing applications. general idea estimate distribution sentences corpus sample realistic-looking sentences. task important enables generation novel sentences preserve semantic syntactic properties real-world sentences potentially different examples used estimate model. instance context dialog generation desirable generate answers diverse less generic simple approach consists ﬁrst learning latent space represent sentences using encoderdecoder framework based recurrent neural networks generate synthetic sentences decoding random samples latent space. however approach often fails generate realistic sentences arbitrary latent representations. reason that mapping sentences latent representations using autoencoder mappings usually cover small structured region latent space corresponds manifold embedding practice regions latent space necessarily realistic sentences. consequently randomly sampling latent representations often yields nonsensical sentences. recent work bowman attempted generate diverse sentences rnn-based variational autoencoders. however address fundamental problem posterior distribution latent variables appropriately cover latent space. another underlying challenge generating realistic text relates nature rnn. inference generates words sequence previously generated words contrary learning ground-truth words used every time. result error accumulates proportional length sequence i.e. ﬁrst words look reasonable however quality deteriorates quickly sentence progresses. bengio coined phenomenon exposure bias. toward addressing problem bengio proposed scheduled sampling approach. however huszár showed scheduled sampling fundamentally inconsistent training strategy produces largely unstable results practice. generative adversarial network appealing natural answer issues. matches distributions synthetic real data introducing adversarial game generator discriminator. objective seeks constitute generator functionally maps samples given prior distribution synthetic data appear realistic. setup explicitly seeks latent representations real data distributed manner consistent speciﬁed prior nature adversarial training discriminator compares real synthetic sentences rather individual words principle alleviate exposure-bias issue. recent work incorporated additional discriminator train sequence-to-sequence language model better preserves effort also made generate realistic-looking sentences adversarial training. instance borrowing ideas reinforcement learning treat sentence generation sequential decision making process. despite success methods fundamental problems framework limit practice generator tends produce single observation multiple latent representations i.e. mode collapsing generator’s contribution learning signal insubstantial discriminator close local optimum i.e. vanishing gradient behavior paper propose framework textgan alleviate problems associated generating realisticlooking sentences gan. speciﬁcally long shortterm memory used generator convolutional neural network used discriminator. consider kernel-based moment-matching scheme reproducing kernel hilbert space force empirical distributions real synthetic sentences matched moments latent-feature space. consequence approach ameliorates mode-collapsing issue associated standard training. strategy encourages model learn representations informative original sentences discriminative w.r.t. synthetic sentences also propose several complementary techniques including initialization strategies discretization approximations ease training achieve superior performance compared related approaches. aims obtain equilibrium following optimization objective lgan ex∼px ez∼pz log)] lgan maximized w.r.t. minimized w.r.t. note ﬁrst term depend observed data sampled empirical distribution latent code feeds generator drawn simple prior distribution discriminator optimal solving adversarial game equivalent minimizing jenson-shannon divergence real data distribution synthetic data distribution however cases saddle-point solution objective intractable. therefore procedure figure model scheme textgan. latent codes generator produce synthetic sentence synthetic real sentences binary discriminator real fake prediction also latent code reconstruction represent features respectively. iteratively update often applied. arjovsky bottou pointed standard objective suffers unstably weak learning signal discriminator gets close local optimal gradient-vanishing effect. implied original loss becomes constant share support thus minimizing yields learning signal. problem also exists recently proposed energy-based distance metric implied ebgan total variance distance issue w.r.t. shown arjovsky textgan given sentence corpus instead directly optimizing objective standard adopt approach similar feature matching scheme salimans speciﬁcally consider objective iteratively maximized w.r.t minimized w.r.t. respectively. lgan standard objective lrecon euclidean distance reconstructed latent code original code drawn prior distribution denote synthetic sentences represents maximum mean discrepancy empirical distribution sentence embeddings synthetic real data respectively. model framework illustrated figure detailed below. ﬁrst consider generator attempts adjust produce synthetic sentence features concisely measures mean squared difference sets samples {xi}i=nx {yi}i=ny dimensionality samples sample sizes respectively. metric characterizes differences reproducing kernel hilbert space associated kernel function kernel written inner product k·)h denoted feature mapping formally empirical distributions given ||ex∼x ey∼y φ||h ex∼x ex∼x ex∼x ey∼y ey∼yey∼y match exactly. example polynomial kernel minimizing understood matching moments empirical distributions order universal kernel like gaussian kernel bandwidth minimizing objective match moments orders here match empirical distribution using gaussian kernel. adversarial discriminator associated loss aims produce sentence features discriminative representative challenging. aims explicitly represented three components namely lgan requires discriminative real synthesized sentences; lrecon requires preserve maximum reconstruction information latent code generates synthetic sentences; forces select challenging features generator match. situation simple features enough discrimination/reconstruction task additional loss seeks estimate complex features difﬁcult current generator thus improving terms generation ability. experience reconstruction loss serve regularizer binary classiﬁcation loss adding losses discriminator features tend spread-out feature space. summary adversarial game associated following attempts select informative sentence features aims match features. parameters trade-off discrimoriginal objective shown prone mode collapsing especially so-called alternative generator loss used i.e. replacing second term −ez∼pz log)]. used fake-looking samples penalized severely less diverse samples thus grossly underestimating variance latent features. loss hand forces generator produce highly diverse sentences match variation real sentences latent moment matching thus alleviating mode-collapsing problem. believe leveraging general enough useful framework data domains e.g. images. presumably discrete nature text data makes standard prone mode-collapsing. manifested close neighbors latent code space producing text output. approach feature matching introduced alleviate mode collapsing text data motivating domain. however whether objective free convergence issues standard vanishing gradient generator known problem speciﬁc arjovsky bottou demonstrated yields weak gradient signals real synthetic data apart. deliver stable gradients smoother distance metric data domain required. essentially employing neural network embedding gaussian kernel matching i.e. denotes embedding maps data feature domain. assumption bijective mapping i.e. distinct sentences different embedded feature vectors supplementary material prove original kernel function universal composed kernel also universal. shown gretton proper metric kernel universal. fact kernel function universal metric worse terms vanishing gradients however bandwidth kernel small much smaller average distance data points vanishing gradient problem remains additionally seeking match sentence features provides achievable informative objective directly trying mislead discriminator standard gan. speciﬁcally loss implies clearer generator requires matching latent features opposed uniquely trying fake binary classiﬁer. note latent features real synthetic data similar distributions unlikely discriminator uses features inputs able tell apart. implementation-wise updating signal generator need propagate back discriminator rather directly features layer thus less prone fading. believe possible approaches text generation using however hope provide ﬁrst attempt toward overcoming difﬁculties associated limitation proposed approach dimensionality features could much larger size subset data used learning hence empirical distribution sufﬁciently representative. fact reliable gaussian kernel twosample test generally requires size minibatch proportional number dimensions alleviate issue consider strategies. compressing network lowerdimensional feature space using compressing network fully connected layers also learned sensible discriminator still encourage challenging features abstracted original features approach provides signiﬁcant computational savings computation scales denotes dimensionality feature vector. however lower-dimensional mapping miss valuable information. besides ﬁnding optimal mapping dimension difﬁcult practice. exists tradeoff fast estimation richer feature vector setting appropriately. gaussian covariance matching could also avoid using kernel trick used instead replace accumulate sufﬁcient statistics multiple minibatches thus alleviating inadequate-minibatch-size issue. specifically represent covariance matrices synthetic real sentence feature vectors respectively. denote mean vectors respectively. setting reduces ﬁrst-moment feature matching technique salimans note loss upper bound between multivariate gaussian distribution tractable directly minimizing jsd. feature vectors used neural outputs applying non-linear activation function. note gaussian assumption still strong many cases. practice moving average recent minibatches estimating sufﬁcient statistics further initialized prevent numerical problems. denote t-th word sentence word embedded k-dimensional word vector rk×v word embedding matrix vocabulary size notation denotes v-th column matrix discriminator architecture collobert sentence encoding. consists convolution layer max-pooling operation entire sentence feature map. sentence length represented matrix rk×t concatenating word embeddings columns i.e. t-th column shown figure convolution operation involves ﬁlter rk×h applied window words produce feature. following collobert induce latent feature rt−h+ nonlinear activation function rt−h+ bias vector synthetic sentence deterministically obtained given concatenating generated words. experiments transition function implemented lstm details provided supplementary material. training techniques soft-argmax approximation train generator contains discrete variables direct application gradient estimation difﬁcult scorefunction-based approaches reinforce algorithm achieve unbiased gradient estimation discrete variables using monte carlo estimation. however experiments found variance gradient estimation large consistent maddison consider soft-argmax operator i.e. performing learning approximation pre-training previous literature discussed fundamental difﬁculty training gans using gradient-based methods. general gradient descent optimization schemes fail converge equilibrium moving along orbit trajectory among saddle points intuitively good initialization facilitate convergence. toward initialize lstm parameters generator pre-training standard cnn-lstm autoencoder discriminator/encoder initialization permutation training strategy. sentence corpus randomly swap words construct slightly tweaked sentence counterpart. discriminator pre-trained distinguish tweaked sentences true sentences. swapping operation preferred constitutes much challenging task discriminator learn compared adding deleting words structure real sentences strongly disrupted thus making easier discriminator. permutation pre-training important requires discriminator learn features characteristic sentences’ long dependencies. empirically found provides better initialization discriminator learn good features. denotes convolutional operator. apply max-over-time pooling operation feature take maximum value i.e. max{c} feature corresponding particular ﬁlter. convolving ﬁlter h-gram every position sentence allows features extracted independently position sentence. pooling scheme tries capture salient feature i.e. highest value feature effectively ﬁltering less informative compositions words. further pooling scheme also guarantees extracted features independent length input sentence. process describes feature extracted ﬁlter. practice model uses multiple ﬁlters varying window sizes. ﬁlter considered linguistic feature detector learns recognize speciﬁc class h-grams. assume window sizes window size ﬁlters obtain mp-dimensional vector represent sentence. mp-dimensional feature vector specify softmax layer input sentence output representing probability data distribution rather adversarial generator lstm generator specify lstm generator translate latent code vector synthetic sentence illustrated figure probability length-t sentence given encoded feature vector deﬁned denotes t-th generated token. speciﬁcally generate ﬁrst word deterministically argmax tanh. bias terms omitted simplicity. words sentence sequentially generated using based previously generated words end-sentence symbol generated. t-th word generated argmax hidden units recursively updated weight matrix used computing distribution words. input t-th step embedding vector previous generated word i.e. generative moment matching networks closely related approach. however methods either directly match empirical distribution data domain extract features using pre-trained autoencoder goal perform matching data domain generating sentences dimensionality input data would note minibatch size required obtain reasonable statistical power grows linearly number dimension computational cost grows quadratically size data points. therefore directly applying gmmns often computationally prohibitive. furthermore directly matching data domain gmmns implies word-by-word discrepancy yields less smooth gradients. happens word-by-word discrepancy ignores sentence structure. example sentences swimming swimming apart word-by-word metric indeed close sentence-by-sentence feature space. two-step method feature encoder generated ﬁrst helps alleviate problems above. however feature encoder ﬁxed pre-trained limiting potential adjust features training phase. alternatively approach matches real synthetic data sentence feature space features dynamically adversarially adapted focus challenging features generator mimic. addition features designed maintain discrimination reconstruction ability instead merely focusing reconstruction recent work considered combining autoencoders variational autoencoders demonstrated superior performance image generation. approach similar approaches; however attempt learn reconstruction latent code instead input data donahue dumoulin learned reverse mapping data space latent space. approach enforce discriminator encoder share latent structure learning representation discrimination latent code reconstruction. chen maximized mutual information generated data latent codes leveraging network-adapted variational proposal distribution. case minimize distance original reconstructed latent codes. stein discrepancy shown computationally tractable maintaining statistical power. leave investigation using stein moment matching promising future direction. wasserstein considers earth-mover distance real data synthetic data distribution instead standard zhao metric yields stable gradients thus avoiding collapsing mode vanishing gradient problem latter two. note approach equivalent minimizing loss data domain however nnbased embedded gaussian kernel. shown arjovsky proper metric kernel universal. similarity conditions approach enjoys advantages wasserstein namely ameliorating gradient vanishing problems. experiments data experimental setup model trained using combination datasets bookcorpus dataset consists million sentences books; arxiv dataset consists million sentences abstracts papers various subjects obtained arxiv website. motivation merging different corpora investigate whether model generate sentences integrate scientiﬁc informal writing styles. randomly choose million sentences bookcorpus million sentences arxiv construct training validation sets i.e. million sentences each. testing randomly select sentences corpus total sentences. train generator discriminator/encoder iteratively. provided lstm generator typically involves parameters difﬁcult train discriminator perform optimization step discriminator every steps generator. mixture isotropic gaussian kernels different bandwidths bandwidth parameters selected close median distance feature vectors encoded real sentences. selected based performance validation set. validation performance evaluated loss generator corpus-level bleu score described below. discriminator/encoder ﬁlter windows sizes feature maps each hence sentence represented -dimensional vector. dimensionality also feature vector fully connected network discriminator encoder sigmoid figure moment matching comparison. left expectations latent features real synthetic data. right elements σijf ˜σij real synthetic data respectively. activation units connecting intermediate layers softmax/tanh units layer discriminator/encoder. observe performance changes adding dropout. lstm sentence generator hidden layer units. generator discriminator pre-trained using strategies described section also employed warm-up training ﬁrst epochs found improves convergence initial stage learning. speciﬁcally mean-matching objective generator loss i.e. ||ef ˜f|| salimans details experimental design provided supplementary material. experiments implemented theano using nvidia geforce titan memory. model trained epochs roughly days. learning curves shown supplementary material. matching feature distributions ﬁrst examine generator’s ability produce synthetic features similar obtained real data. purpose calculate empirical expectation -dimensional sentence feature vector real sentences synthetic sentences. shown figure expectation feature dimensions synthetic sentences matches well feature expectation real sentences. also compared estimated covariance matrix elements ˜σijf real data covariance matrix elements ˜σij estimated synthetic data figure observe covariance structure -dimensional features real synthetic sentences general match well. full covariance matrices real synthetic sentences provided quantitative comparison evaluate generatedsentence quality using bleu score kernel density estimation goodfellow nowozin comparison consider textgan different loss objectives mean matching salimans covariance matching compressed network mapping original -dimensional features -dimensional described section also compare baseline autoencoder model. uses encoder lstm decoder lstm network structures identical lstm used textgan. ﬁnally consider variational autoencoder implemented bowman train model annealing gradually increase divergence prior approximated posterior. details provided supplementary material. also compare seqgan seqgan follow authors’ guidelines running pre-training epochs followed discriminator training epochs generate sentences. textgan ﬁrst uniformly sample latent codes latent code space corresponding generator generate sentences. bleu score evaluation follow strategy using entire test reference. evaluation lengths generated sentences different thus ﬁrst embed sentences dimensional vector. since standard sentence encoder available encoder learned covariance matrix parzen kernel covariance feature vectors real tested sentences. despite fact approach log-likelihood estimator tends high variance score tracks well bleu score evaluation. results shown table mmd-l generally score higher sentences quality. mmd-l seems better capturing -grams outperforms mmd-l -grams also observed using generated sentences tend shorter show joint likelihood estimator problem achieves less interesting choices convergence guarantees turing machine learning hidden markov relational spaces random walk feature decomposition unique generalized parametric mappings. primitives specifying deterministic probabilistic machine learning algorithm wanted alone gene expression dataset form phantom action values opposite fuzzy modelling algorithm pruning performed using template representing network structures generated sentences table shows sentences generated textgan. note generated sentences seem able produce novel phrases imagining concept combinations e.g. table borrow words different corpus compose novel sentences e.g. table many cases learns automatically match parentheses quotation marks e.g. table synthesize relatively long sentences e.g. general synthetic sentences seem syntactically reasonable. however semantic meaning less well preserved especially sentence words e.g. table observe discriminator still sufﬁciently distinguish synthetic sentences real ones even synthetic sentences seems perserve reasonable grammatical structure proper wording. likely able accurately characterize semantic meaning differentiate sentences generator trapped local optimum slight modiﬁcation would result higher loss generator. presumably long-range distance features difﬁcult abstract discriminator/encoder however less likely imitated generator. promising direction leverage reinforcement learning strategies updating lstm effectively steered. nevertheless investigation improve long-range behavior left interesting future work. latent feature space trajectories following bowman empirically evaluate whether latent variable space densely encode sentences. visualize transition sentence another constructing linear path randomly selected points latent feature space generate intermediate sentences along linear trajectory. comparison baseline autoencoder trained epochs. results textgan presented table compared sentences produced textgan generally methods apply novel approaches solve modeling methods apply different approaches solve computing methods achieves different approaches solve computing methods achieves best expert structure detection methods different related tasks minimum syntactically semantically reasonable. transition suggest smoothness interpretability however wording choices sentence structure showed dramatic changes regions latent feature space. seems indicate local transition smoothness varies region region. introduced novel approach text generation using adversarial training termed textgan discussed several techniques specify train model. demonstrated proposed model delivers superior performance compared related approaches produce realistic sentences learned latent representation space smoothly encode plausible sentences. quantitatively evaluate proposed methods baseline models existing methods. results indicate superior performance textgan. future work attempt apply conditional models disentangle latent representations different writing styles. would enable smooth lexical grammatical transition different writing styles. would also interesting generate text conditioning observed images addition plan leverage additional reﬁning stage reverse-order lstm applied sentence ﬁrst generated produce sentences better long-term semantical interpretation. chen duan houthooft rein schulman john sutskever ilya abbeel pieter. infogan interpretable representation learning information maximizing generative adversarial nets. nips merriënboer gulcehre bahdanau bougares schwenk bengio learning phrase representations using encoder-decoder statistical machine translation. emnlp dumoulin vincent belghazi ishmael poole lamb alex arjovsky martin mastropietro olivier courville arxiv preprint aaron. adversarially learned inference. arxiv. yunchen henao ricardo chunyuan xiaodong carin lawrence. unsupervised learning sentence representations using convolutional neural networks. arxiv preprint arxiv. goodfellow pouget-abadie jean mirza mehdi bing warde-farley david ozair sherjil courville aaron bengio yoshua. generative adversarial nets. nips lamb alex goyal anirudh goyal alias parth zhang ying zhang saizheng courville aaron bengio yoshua. professor forcing algorithm training recurrent networks. nips larsen anders boesen lindbo sønderby søren kaae larochelle hugo winther ole. autoencoding beyond pixels using learned similarity metric. icml mescheder lars nowozin sebastian geiger andreas. adversarial variational bayes unifying variational autoencoders generative adversarial networks. icml ramdas aaditya reddi sashank poczos barnabas singh aarti wasserman larry. high-dimensional power linear-time kernel two-sample testing mean-difference alternatives. arxiv. kiros zemel salakhutdinov urtasun torralba fidler aligning books movies towards story-like visual explanations watching movies reading books. iccv", "year": 2017}