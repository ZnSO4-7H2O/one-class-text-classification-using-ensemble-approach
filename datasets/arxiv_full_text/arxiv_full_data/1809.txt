{"title": "Learning Latent Representations for Speech Generation and Transformation", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "An ability to model a generative process and learn a latent representation for speech in an unsupervised fashion will be crucial to process vast quantities of unlabelled speech data. Recently, deep probabilistic generative models such as Variational Autoencoders (VAEs) have achieved tremendous success in modeling natural images. In this paper, we apply a convolutional VAE to model the generative process of natural speech. We derive latent space arithmetic operations to disentangle learned latent representations. We demonstrate the capability of our model to modify the phonetic content or the speaker identity for speech segments using the derived operations, without the need for parallel supervisory data.", "text": "experiment conducted measure ability modify speaker characteristics without changing linguistic content vice versa. addition perform analysis evaluate model’s ability generate speech segments different durations. rest paper organized follows. section brieﬂy discuss related work. models analysis latent representations detailed section data preparation explained section section show experimental results. finally conclude work discuss future research plans topic section recent research speech audio generation made remarkable progress directly utilizing time-domain speech signals. wavenet introduces one-dimensional dilated causal convolutional model effective receptive ﬁeld grows exponentially wide depth using exponentially growing dilation factors depth. different model called samplernn presented multi-scale recurrent neural network layer operated different clock rates sample generated conditioned previous samples. models focused generating high quality audio segments predicting next sample given preceding samples instead learning latent representations entire audio segments using probabilistic generative models. vaes widely applied image generation less speech research topic. vaebased framework used extract frame-level utterance-level features used combination features robust speech recognition. fully-connected used learn frame-level latent representation evaluated using gaussian diffusion process generate concatenate multiple samples varied smoothly time. variational autoencoders deﬁne probabilistic generative process observation latent variable follows prior conditional likelihood probability distribution family parameterized unsupervised setting given dataset {x}n true value well latent variable observation process unknown. often interested knowing marginal likelihood data posterior however ability model generative process learn latent representation speech unsupervised fashion crucial process vast quantities unlabelled speech data. recently deep probabilistic generative models variational autoencoders achieved tremendous success modeling natural images. paper apply convolutional model generative process natural speech. derive latent space arithmetic operations disentangle learned latent representations. demonstrate capability model modify phonetic content speaker identity speech segments using derived operations without need parallel supervisory data. index terms unsupervised learning variational autoencoder speech generation speech transformation voice conversion speech waveforms complex distributions exhibit high variance factors include linguistic content speaking style dialect speaker identity emotional state environment channel effects etc. understanding inﬂuence factors speech signal important problem used wide variety applications including limited adaptation data augmentation speech recognition voice conversion speech compression however previous research focused handcrafting features capture factors rather learning factors automatically probabilistic generative process. recently signiﬁcant interest deep probabilistic generative models variational autoencoders generative adversarial nets particularly addresses intractability issue occurs even moderately complicated models restricted boltzmann machines applied voice conversion provides efﬁcient approximated posterior inference latent factors. many works investigating generative models natural images little work done learning speech generation deep probabilistic generative models paper adopt framework propose convolutional architecture model probabilistic generative process speech learn latent representation. present simple arithmetic operations latent space demonstrate operations decompose latent representation different attributes speaker identity linguistic content. manipulating latent representation also demonstrate ability perturb aspect surface speech segment example speaker identity keeping remaining attributes ﬁxed quantify behavior latent representation modiﬁcations since mean log-variance unbounded layers tanh unbounded rectiﬁer linear units overﬂow kl-divergence conditional likelihood. batch normalization applied every layer except gaussian parameter layer. ﬁrst assumption make conditioning attribute phone /ae/ prior distribution also gaussian; words therefore deﬁne latent attribute representation subset attribute instance estimate follows results averaging sampled latent representations instance furthermore since log-concave function guaranteed minzi therefore able generate reasonable speech-like segment high values. make second assumption independent attributes affect realization speech attribute modeled using subspace k=zak hence tent representation decomposed orthogonal latent attribute representations zak. combining aforementioned assumption conditioned prior next derive latent space arithmetic operations modify speech attributes. suppose want modify attribute example speaker identity speech segment speaker speaker given latent attribute representations speaker respectively latent attribute shift vrs→rt computed vrs→rt µrs. modify speech follows variational lower bound want optimize respect framework consider here recognition model generative model parameterized using diagonal gaussian distributions mean covariance computed neural network. prior assumed centered isotropic multivariate gaussian free parameters. practice expectation approximated ﬁrst drawing samples computing eqφ] yield differentiable network sampling reparameterization trick used. suppose reparameterizing denotes element-wise product vector sampled treated additional input. work goal learn latent representations speech segments model generation process. observed data sequence frames ﬁxed length. learned latent variable therefore supposed encode factors result variability speech segments content spoken speaker identity channel effect. mentioned earlier composed networks recognition network generative network. recognition network takes speech segment input predicts mean log-variance parameterize posterior distribution speech segment treated dimensional image width height however unlike natural images speech segments translational invariant time axis. therefore similar -by-f ﬁlters applied ﬁrst convolutional layer w-by- ﬁlters following layers. suggested instead pooling stride size down-sampling along time axis. output last convolutional layer ﬂattened fully connected layers going gaussian parameter layer modeling latent variable table summary. generative network takes sampled input predicts mean well log-variance observed data. symmetric architectures corresponding recognition network. table recognition network architecture. conv refers convolutional layers refers fully connected layers gauss refers gaussian parametric layer modeling timit acoustic-phonetic corpus contains broadband recordings phonetically-balanced read speech. total utterances presented sentences speakers approximately male female. utterance comes manually time-aligned phonetic word transcriptions well -bit speech waveform ﬁle. follow kaldi’s timit recipe split train/dev/test sets exclude dialect sentences non-overlapping speakers respectively. phonetic transcriptions based phones excluding silence phones. consider types frame representations magnitude spectrum ﬁlter banks features ﬁrst apply hanning window shift compute short time fourier transform coefﬁcients ﬂooring -db. fbank features mel-scale ﬁlter banks match human perceptual sensitivity applied preserves detail lower frequency regions. models trained stochastic gradient descent using mini-batch size without clipping minimize negative variational lower bound plus l-regularization weight adam optimizer used initial learning rate training terminated lower bound development improve epochs. compare also train autoencoder proposed model architecture except gaussian latent variable layer replaced fully-connected layer hidden units. figure show results reconstructing latent attribute representations three phones /ae/ /th/ using respectively based derivation section baseline also show results averaging ﬁlter bank features. preserves harmonic structure clearer spectral envelope fbank blurred. worth noting also shows unnatural frequent vertical stripe artifacts. assess orthogonality-between-attributes assumption sampled speakers three males three females denoted {mf} phones including vowels stops fricatives nasals compute three latent speaker representations latent phone representations. figure plots cosine similarities representations. ﬁgure observe off-diagonal blocks cosine similarities indicates latent speaker representations latent phone representations reside orthogonal latent subspaces. second different latent phone representations also cluster according phonetic characteristics suggests latent phone subspace divided. next explored modifying phone speaker attributes using derived operations section figure shows example drawing instances phone /aa/ transforming /ae/ using latent attribute shift vaa→ae. clearly observe second formant marked boxes instance goes modiﬁcation changed back vowel front vowel. hand harmonics instance closely related speaker identity maintain roughly same. attempt quantify latent attribute perturbation trained convolutional phonetic speaker classiﬁers could measure difference posterior attribute modiﬁcation. -class phone classiﬁer achieves test accuracy -class speaker classiﬁer achieves test accuracy shifts posterior distributions phone speaker classiﬁcations modiﬁed data shown table upper half table contains results speech segments transformed /aa/ /ae/. ﬁrst shows average /aa/ posterior average correct speaker posterior second shows modiﬁcation /ae/ average phone posteriors shift dramatically /ae/ slightly degrading average correct speaker posterior. lower part table shows results speech segments speaker identity modiﬁed speaker ‘falk’ ‘madc’. third shows average speaker posterior ‘falk’ unmodiﬁed samples average correct phone posterior modiﬁcation average speaker posterior shifted ‘madc’ slightly degrading average correct phone posterior. advantages vaes prior assumed centered isotropic gaussian enables sample latent vectors reconstruct speech-like segments. here investigate syllable-level word-level datasets. figure shows random samples syllablelevel model look sound reasonable; however observe random samples drawn word-level model less natural excessive closures shown figure failure drawing random samples implies discrepancy assumed prior true prior. hypothesize per-dimension kl-divergence values computed correlations among dimensions penalized covariance matrix true prior diagonal. estimate covariance matrix true prior sampling latent representations entire test compute full covariance matrix. figure compares syllable model word model off-diagonal covariance scale dimension. observe word-level model higher correlations different dimensions syllable-level model. finally explore operation interpolation latent space speech segments. since log-concave interpolated zint would therefore also generate reasonable speech-like segments. figure shows transition male /ey/ female /ay/ using respectively. clearly observe pitch shifting formant contour transforming; however akin interpolation feature space magnitude segment goes goes paper present convolutional model speech generation process learn latent representations speech unsupervised framework. abilities decompose learned latent representations modify attributes speech segments demonstrated qualitatively quantitatively. future work plan extend hierarchical recurrent models order capture information different time scales generate speech variable lengths. toda ohtani shikan eigenvoice conversion based gaussian mixture model interspeech wong juang cheng very data rate speech compression vector matrix quantization acoustics speech signal processing ieee international conference icassp’. vol. nakashika takiguchi ariki voice conversion using speaker-dependent conditional restricted boltzmann machine eurasip journal audio speech music processing vol. nakashika takiguchi minami nakashika takiguchi minami non-parallel training voice conversion using adaptive restricted boltzmann machine ieee/acm trans. audio speech lang. proc. vol. nov. c.-c. h.-t. hwang y.-c. tsao h.-m. wang voice conversion non-parallel corpora using variational auto-encoder asia-paciﬁc signal information processing association annual summit conference ieee mehri kumar gulrajani kumar jain sotelo courville bengio samplernn unconditional end-to-end neural audio generation model arxiv preprint arxiv. learning utterance-level normalisation using variational autoencoders robust automatic speech recognition spoken language technology workshop ieee. garofolo lamel fisher fiscus pallett darpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc nasa sti/recon technical report vol.", "year": 2017}