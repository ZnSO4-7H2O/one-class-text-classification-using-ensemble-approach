{"title": "Stochastic Function Norm Regularization of Deep Networks", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Deep neural networks have had an enormous impact on image analysis. State-of-the-art training methods, based on weight decay and DropOut, result in impressive performance when a very large training set is available. However, they tend to have large problems overfitting to small data sets. Indeed, the available regularization methods deal with the complexity of the network function only indirectly. In this paper, we study the feasibility of directly using the $L_2$ function norm for regularization. Two methods to integrate this new regularization in the stochastic backpropagation are proposed. Moreover, the convergence of these new algorithms is studied. We finally show that they outperform the state-of-the-art methods in the low sample regime on benchmark datasets (MNIST and CIFAR10). The obtained results demonstrate very clear improvement, especially in the context of small sample regimes with data laying in a low dimensional manifold. Source code of the method can be found at \\url{https://github.com/AmalRT/DNN_Reg}.", "text": "deep neural networks enormous impact image analysis. state-ofthe-art training methods based weight decay dropout result impressive performance large training available. however tend large problems overﬁtting small data sets. indeed available regularization methods deal complexity network function indirectly. paper study feasibility directly using function norm regularization. methods integrate regularization stochastic backpropagation proposed. moreover convergence algorithms studied. ﬁnally show outperform state-of-the-art methods sample regime benchmark datasets obtained results demonstrate clear improvement especially context small sample regimes data laying dimensional manifold. source code method found https//github.com/amalrt/dnn_reg. deep neural networks shown powerful tool learning large datasets many domains. however require large number training samples order reasonable generalization power. indeed fact tends limit application type images easy construct large labeled database natural images collection done image sharing websites labels obtained cost crowdsourcing. labeled samples available medical imaging dnns tend overﬁt data competitive methods known tractable methods stronger regularization available e.g. kernel methods. generally learning theory wants puts outputs respectively. however generally unknown empirical risk accessible. statistical learning theory gives conditions empirical risk good approximation statistical risk giving bound form increasing function vapnik–chervonenkis dimension measure depends function set. small minimizer empirical risk gives small statistical risk even small. activation function weights dnns index efﬁciently large function space parameters however dimension function tends high. indeed many researchers studied vc-dimension simple neural networks shown that example networks sigmoidal activation functions optimal bound dimension number weights. authors also show deep network implement functions higher complexity shallow network number weights proving many advantages dnns. nevertheless dnns become popular recently tend high number weights makes generalization power decrease signiﬁcantly small databases. reason appropriate regularization required. training regularization solving following problem much research approached idea regularized training. however regularization tends poor compared examples regularization kernel methods common idea behind regularization application occam’s razor several solutions exist given problem favor simplest solution. notion simplicity employed typically determined prior knowledge computational considerations. categories regularization distinguished existing literature regularizing controlling magnitude weights regularization regularizing controlling network architecture dropconnect order make regularization stronger study paper feasibility using function norm exact value norm importantly gradient accessible good approximation needed. approximations considered. ﬁrst idea consider function norm respect probability measure easily valid function norm furthermore weighted canonical function norm regularization higher effect data higher probability observed. second idea markov-chain sampling method numerical integration here idea generate samples probability density proportional norm) approximate integral positive weights samples generated samples concentrated around higher values section detail methods describe algorithms order integrate gradient descent. section describe experiments using mnist cifar databases report results. finally section results discussed. unbiased estimate. samples outside training u-statistic order asymptotic gaussian distribution ﬁnite sample estimates converge quickly. samples images needed labels particularly interesting applications medical imaging labeling samples require expert intervention usually highly expensive. therefore problem solve written following markov-chain sampling second idea stochastic integration methods approximate integral simple apply methods draw samples uniform distribution domain function approximate integral average value function points. type methods known monte-carlo quasi-monte-carlo methods however working high dimension type numerical integration appropriate suffers curse dimensionality results sample concentration independent characteristics function want integrate i.e. function real non-negative consider sampling distribution density proportional function itself using markov-chain methods gibbs metropolis–hastings sampling. gibbs sampling requires approximate marginal distribution metropolis-hastings sampling depending choice proposal distribution generate samples using slice sampling method draw samples uniformly hypervolume n-dimensional graph function iterating following steps choose initial sample draw uniformly distributed sample find hyperrectangle around contained slice draw samples uniformly hyperrectangle; back second step iterate collecting wanted number samples. many methods deﬁne hyperrectangle third step proposed. used method called \"stepping-out shrinking-in\". method consist ﬁnding neighborhood increasing progressively outside slice drawing samples increased neighborhood. drawn sample outside slice used shrink neighborhood. samples drawn using procedure concentration depends variations concentrated around regions values high. effect curse dimensionality limited. moreover means regularization higher effect regions samples concentrated i.e. regions values function high resulting less complex function. integral approximated cases consider objective minimize form methods ﬁrst case replaced drawn distribution; second case replaced drawn data distribution. suppose permute integration derivation straightforward show updates obey condition much research considers question convergence procedure. many based results nemirovsky showed convex gradient lipshitz continuous bounded optimal convergence rate obtained case speed decrease best compromise speed convergence control variance introduced using sample rather samples approximate gradient. bottou shown property holds less constraining conditions studied particular type objective wants minimize neural networks. objectives convex admit several minimums. shows ∃cmin∀w cmin long converges converges using sgd. know second condition holds cases study. thus methods knowing conditions hold need prove still hold adding regularization term. cmin parameters ensures ﬁrst third conditions know ﬁrst term bounded suppose bounded. reasonable hypothesis want control norm moreover bounded condition veriﬁed many popular loss functions cross-entropy loss. experiments conducted loss. thus bounds second term third term bounded λaet finally sufﬁces positive ﬁnish proof third condition second case case rewrite objective note depends also however positive expected value equal thus ﬁrst condition holds sketch proof third condition holds thus consider learning rate obeys fourth condition ensures convergence considered objectives tuned correctly. practice parameter tuned empirically. training algorithm used epoch rather using single samples mini-batch approach popular used library programming algorithm denote network nreg training regularization note n∗.l layers. training network differs regularization network last layer computes loss function. denotes batch size number patches regularization variable mode\" either train\" test\" rreg contain output network subscript designs error. regularization step backpropagation initialized value output forward pass. difference proposed methods deﬁnition regularization data test algorithms series experiments using different databases considered mnist cifar. experiments conducted using matconvnet databases convolutional neural network lenet used. training algorithms provided cited library modiﬁed according described algorithms. algorithms used weight decay. results algorithms compared with training weight decay training dropout weight decay. extensive experimental results found supplementary material qualitatively follow results reported here. paragraph report results tests conducted algorithms mnist. mnist data base composed samples training samples test. experiments conducted using intel core processor. experiments test ﬁrst algorithm reference methods listed using decreasing number samples mnist training data set. remaining training samples used regularization. figure shows evolution error training weight decay weight decay+ dropout rate weight decay+ dropout rate approximate function norm using data distribution. error displayed test samples. test compare proposed algorithms. consider test problem classiﬁcation using samples.in figure display test error evolution function training time. number indicates ratio table shows accuracy obtained test training. reference note best accuracy obtained training data used paragraph report results algorithms applied cifar used cifar database composed samples training samples test. experiments conducted using machine equipped core geforce experiments test algorithms weight decay weight decay dropout decreasing number samples cifar training data set. remaining training samples used regularization. figure shows error test function number epochs using respectively training samples. figure shows evolution test top-error training time using samples training. table shows accuracy obtained test training. reference note best accuracy obtained training data used mnist experiments show feasibility efﬁciency regularization based function norm. indeed figure shows function norm regularization sampling data distribution behaves least well state-of-the methods high number samples used outperforms decrease number samples. figure shows proposed methods converge similar errors different computation time. indeed case data distribution network simple enough ensure equally high accuracy approximation function norm using data distribution slice sampling. proposed methods resulted accuracy high training samples. mentioned above case mnist main difference sampling methods computation time. point view using data distribution fastest needs read data memory slice sampling approach require generation samples. however methods preferred ﬁrst algorithm situations number available samples small medical imaging data collection expensive. experiments conducted cifar show idea regularization also feasible complicated data complicated network. curves accuracy rates show performance method always higher state-of-the-art methods methods gave accuracy close best accuracy obtained using samples training samples. also robust decrease number samples. even obtained results method sample optimal error still lower methods. another important observation slice sampling seems robust using data distribution database. explained fact cifar’s distribution complicated available samples regularization enough capture variations distribution. however cases slice sampling captures well characteristics function variations. paper methods introduce function norm regularization neural network training developed. demonstrated theoretically empirically integration stochastic backpropagation converges. efﬁciency showed mnist data complicated data cifar. experiments suggest developed methods high quality small databases dimensional manifolds. complicated data small number samples results methods outperform state-of-the-art regularization methods available literature. bianchini scarselli. complexity neural network classiﬁers comparison shallow deep architectures. neural networks learning systems ieee transactions ization. advances neural information processing systems neal. slice sampling. annals statistics nemirovski juditsky shapiro. robust stochastic approximation approach paragraph show test top-errors obtained applying function norm regularization using data distribution weight decay weight decay dropout mnist data decreasing sizes. figure shows test top-error samples used. corresponding top-error showed main text. figure shows test errors using samples. figure shows evolution test top-error training time using samples. experiment show effect choice tested function norm regularization using data distribution different decreasing factor figure display validation top-error. figure display validation top-error. ﬁgures show left curves corresponding values. right remove value procedure fails order better visualization values. among test samples taken randomly order validation. curves constructed using samples. samples used compute accuracies displayed table experiments shows long enough trained network good performance. indeed except values result curves errors hardly classiﬁable. note value gives best accuracy test correspond best top-error best top-error. paragraph show test top-errors obtained applying function norm regularization using data distribution slice sampling weight decay weight decay dropout cifar data decreasing sizes. figure shows test top-error samples used. corresponding top-error showed main text. figure shows", "year": 2016}